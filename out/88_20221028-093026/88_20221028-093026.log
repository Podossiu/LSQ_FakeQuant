2022-10-28 09:30:26,538 - INFO  - Log file for this run: /home/ilena7440/LSQFakeQuant/out/88_20221028-093026/88_20221028-093026.log
2022-10-28 09:30:28,264 - INFO  - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
2022-10-28 09:30:28,298 - INFO  - Created `resnet20` model
          Use pre-trained model = True
2022-10-28 09:30:28,464 - INFO  - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.001
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 0.0001
           )
2022-10-28 09:30:28,464 - INFO  - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.001

2022-10-28 09:30:29,716 - INFO  - >>>>>> Epoch -1 (pre-trained model evaluation)
2022-10-28 09:30:29,716 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-10-28 09:30:32,570 - INFO  - Validation [   20/   40]   Loss 2.337042   Top1 9.882812   Top5 49.296875   BatchTime 0.140769   
2022-10-28 09:30:34,064 - INFO  - Validation [   40/   40]   Loss 2.335632   Top1 10.000000   Top5 49.510000   BatchTime 0.107733   
2022-10-28 09:30:34,126 - INFO  - ==> Top1: 10.000    Top5: 49.510    Loss: 2.336

2022-10-28 09:30:34,126 - INFO  - Scoreboard best 1 ==> Epoch [-1][Top1: 10.000   Top5: 49.510]
2022-10-28 09:30:34,126 - INFO  - >>>>>> Epoch   0
2022-10-28 09:30:34,126 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-10-28 09:30:36,806 - INFO  - Training [0][   20/  196]   Loss 2.286533   Top1 11.992188   Top5 52.148438   BatchTime 0.133946   LR 0.001000   
2022-10-28 09:30:38,944 - INFO  - Training [0][   40/  196]   Loss 2.294559   Top1 10.615234   Top5 50.751953   BatchTime 0.120429   LR 0.001000   
2022-10-28 09:30:41,088 - INFO  - Training [0][   60/  196]   Loss 2.297234   Top1 10.332031   Top5 50.611979   BatchTime 0.116019   LR 0.001000   
2022-10-28 09:30:43,228 - INFO  - Training [0][   80/  196]   Loss 2.298572   Top1 10.234375   Top5 50.336914   BatchTime 0.113765   LR 0.001000   
2022-10-28 09:30:45,369 - INFO  - Training [0][  100/  196]   Loss 2.299374   Top1 10.382812   Top5 50.414062   BatchTime 0.112425   LR 0.001000   
2022-10-28 09:30:47,510 - INFO  - Training [0][  120/  196]   Loss 2.299910   Top1 10.419922   Top5 50.341797   BatchTime 0.111528   LR 0.001000   
2022-10-28 09:30:49,652 - INFO  - Training [0][  140/  196]   Loss 2.300292   Top1 10.354353   Top5 50.153460   BatchTime 0.110891   LR 0.001000   
2022-10-28 09:30:51,794 - INFO  - Training [0][  160/  196]   Loss 2.300578   Top1 10.336914   Top5 50.217285   BatchTime 0.110420   LR 0.001000   
2022-10-28 09:30:53,913 - INFO  - Training [0][  180/  196]   Loss 2.300801   Top1 10.249566   Top5 50.188802   BatchTime 0.109922   LR 0.001000   
2022-10-28 09:30:55,642 - INFO  - ==> Top1: 10.214    Top5: 50.228    Loss: 2.301

2022-10-28 09:30:55,762 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-10-28 09:30:57,407 - INFO  - Validation [0][   20/   40]   Loss 2.302585   Top1 10.078125   Top5 50.019531   BatchTime 0.082237   
2022-10-28 09:30:58,489 - INFO  - Validation [0][   40/   40]   Loss 2.302585   Top1 10.000000   Top5 50.000000   BatchTime 0.068157   
2022-10-28 09:30:58,557 - INFO  - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303

2022-10-28 09:30:58,557 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-10-28 09:31:00,232 - INFO  - Validation [0][   20/   40]   Loss 2.582605   Top1 11.660156   Top5 51.386719   BatchTime 0.083704   
2022-10-28 09:31:01,172 - INFO  - Validation [0][   40/   40]   Loss 2.578381   Top1 11.470000   Top5 51.580000   BatchTime 0.065341   
2022-10-28 09:31:01,263 - INFO  - ==> Top1: 11.470    Top5: 51.580    Loss: 2.578

2022-10-28 09:31:01,263 - INFO  - Scoreboard best 1 ==> Epoch [0][Top1: 11.470   Top5: 51.580]
2022-10-28 09:31:01,263 - INFO  - Scoreboard best 2 ==> Epoch [-1][Top1: 10.000   Top5: 49.510]
2022-10-28 09:31:02,926 - INFO  - Saving checkpoint to:
            Current: /home/ilena7440/LSQFakeQuant/out/88_20221028-093026/88_checkpoint.pth.tar
                Best: /home/ilena7440/LSQFakeQuant/out/88_20221028-093026/88_best.pth.tar
save quantized models...
2022-10-28 09:31:02,927 - INFO  - >>>>>> Epoch   1
2022-10-28 09:31:02,927 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-10-28 09:31:05,682 - INFO  - Training [1][   20/  196]   Loss 2.302585   Top1 9.804688   Top5 49.941406   BatchTime 0.137732   LR 0.001000   
2022-10-28 09:31:07,824 - INFO  - Training [1][   40/  196]   Loss 2.302585   Top1 9.765625   Top5 49.882812   BatchTime 0.122420   LR 0.001000   
2022-10-28 09:31:09,968 - INFO  - Training [1][   60/  196]   Loss 2.302585   Top1 9.882812   Top5 50.045573   BatchTime 0.117345   LR 0.001000   
2022-10-28 09:31:12,114 - INFO  - Training [1][   80/  196]   Loss 2.302585   Top1 10.092773   Top5 50.039062   BatchTime 0.114826   LR 0.001000   
2022-10-28 09:31:14,259 - INFO  - Training [1][  100/  196]   Loss 2.302585   Top1 10.152344   Top5 49.890625   BatchTime 0.113313   LR 0.001000   
2022-10-28 09:31:16,404 - INFO  - Training [1][  120/  196]   Loss 2.302585   Top1 10.104167   Top5 50.039062   BatchTime 0.112298   LR 0.001000   
2022-10-28 09:31:18,557 - INFO  - Training [1][  140/  196]   Loss 2.302585   Top1 10.147879   Top5 50.203683   BatchTime 0.111633   LR 0.001000   
