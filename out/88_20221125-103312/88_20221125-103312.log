2022-11-25 10:33:12,827 - INFO  - Log file for this run: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103312/88_20221125-103312.log
2022-11-25 10:33:17,626 - INFO  - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
2022-11-25 10:33:19,712 - INFO  - Created `MobileNetv2` model
          Use pre-trained model = True
2022-11-25 10:33:20,542 - INFO  - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
2022-11-25 10:33:20,542 - INFO  - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005

2022-11-25 10:33:20,841 - INFO  - >>>>>> Epoch   0
2022-11-25 10:33:20,843 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-11-25 10:33:30,659 - INFO  - Training [0][   20/  196]   Loss 1.606806   Top1 53.378906   Top5 89.277344   BatchTime 0.490717   LR 0.004999   
2022-11-25 10:33:39,546 - INFO  - Training [0][   40/  196]   Loss 1.528117   Top1 52.968750   Top5 89.716797   BatchTime 0.467525   LR 0.004995   
2022-11-25 10:33:48,172 - INFO  - Training [0][   60/  196]   Loss 1.429541   Top1 55.143229   Top5 90.976562   BatchTime 0.455448   LR 0.004989   
2022-11-25 10:33:56,515 - INFO  - Training [0][   80/  196]   Loss 1.381899   Top1 56.523438   Top5 91.655273   BatchTime 0.445875   LR 0.004980   
2022-11-25 10:34:05,737 - INFO  - Training [0][  100/  196]   Loss 1.335057   Top1 57.757812   Top5 92.136719   BatchTime 0.448915   LR 0.004968   
2022-11-25 10:34:13,944 - INFO  - Training [0][  120/  196]   Loss 1.300319   Top1 58.841146   Top5 92.496745   BatchTime 0.442488   LR 0.004954   
2022-11-25 10:34:20,336 - INFO  - Training [0][  140/  196]   Loss 1.270602   Top1 59.757254   Top5 92.832031   BatchTime 0.424935   LR 0.004938   
2022-11-25 10:34:26,924 - INFO  - Training [0][  160/  196]   Loss 1.250472   Top1 60.388184   Top5 93.027344   BatchTime 0.412989   LR 0.004919   
2022-11-25 10:34:35,266 - INFO  - Training [0][  180/  196]   Loss 1.229401   Top1 61.032986   Top5 93.211806   BatchTime 0.413446   LR 0.004897   
2022-11-25 10:34:41,909 - INFO  - ==> Top1: 61.696    Top5: 93.414    Loss: 1.210

2022-11-25 10:34:42,135 - INFO  - Created `MobileNetv2` model
          Use pre-trained model = False
2022-11-25 10:34:43,622 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-11-25 10:34:46,319 - INFO  - Validation [0][   20/   40]   Loss 0.927601   Top1 70.507812   Top5 97.656250   BatchTime 0.134762   
2022-11-25 10:34:48,977 - INFO  - Validation [0][   40/   40]   Loss 0.946534   Top1 69.940000   Top5 97.800000   BatchTime 0.133834   
2022-11-25 10:34:49,369 - INFO  - ==> Top1: 69.940    Top5: 97.800    Loss: 0.947

2022-11-25 10:34:49,369 - INFO  - ==> Sparsity : 0.306

2022-11-25 10:34:49,370 - INFO  - Scoreboard best 1 ==> Epoch [0][Top1: 69.940   Top5: 97.800]
2022-11-25 10:34:55,846 - INFO  - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103312/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103312/_best.pth.tar
save quantized models...
2022-11-25 10:34:55,850 - INFO  - >>>>>> Epoch   1
2022-11-25 10:34:55,852 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-11-25 10:35:03,251 - INFO  - Training [1][   20/  196]   Loss 1.037892   Top1 67.050781   Top5 95.000000   BatchTime 0.369812   LR 0.004853   
2022-11-25 10:35:09,301 - INFO  - Training [1][   40/  196]   Loss nan   Top1 44.033203   Top5 77.246094   BatchTime 0.336154   LR 0.004825   
2022-11-25 10:35:17,216 - INFO  - Training [1][   60/  196]   Loss nan   Top1 32.669271   Top5 68.567708   BatchTime 0.356024   LR 0.004794   
2022-11-25 10:35:24,799 - INFO  - Training [1][   80/  196]   Loss nan   Top1 27.016602   Top5 63.789062   BatchTime 0.361802   LR 0.004761   
2022-11-25 10:35:31,935 - INFO  - Training [1][  100/  196]   Loss nan   Top1 23.597656   Top5 61.246094   BatchTime 0.360803   LR 0.004725   
2022-11-25 10:35:38,898 - INFO  - Training [1][  120/  196]   Loss nan   Top1 21.282552   Top5 59.368490   BatchTime 0.358691   LR 0.004687   
2022-11-25 10:35:45,766 - INFO  - Training [1][  140/  196]   Loss nan   Top1 19.578683   Top5 57.801339   BatchTime 0.356509   LR 0.004647   
2022-11-25 10:35:52,640 - INFO  - Training [1][  160/  196]   Loss nan   Top1 18.400879   Top5 56.914062   BatchTime 0.354908   LR 0.004605   
2022-11-25 10:35:59,775 - INFO  - Training [1][  180/  196]   Loss nan   Top1 17.439236   Top5 56.128472   BatchTime 0.355107   LR 0.004560   
2022-11-25 10:36:05,877 - INFO  - ==> Top1: 16.860    Top5: 55.650    Loss: nan

2022-11-25 10:36:06,173 - INFO  - Created `MobileNetv2` model
          Use pre-trained model = False
2022-11-25 10:36:07,314 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-11-25 10:36:09,572 - INFO  - Validation [1][   20/   40]   Loss 33.458102   Top1 10.253906   Top5 50.351562   BatchTime 0.112825   
2022-11-25 10:36:10,654 - INFO  - Validation [1][   40/   40]   Loss 33.625710   Top1 10.000000   Top5 50.000000   BatchTime 0.083462   
2022-11-25 10:36:10,854 - INFO  - ==> Top1: 10.000    Top5: 50.000    Loss: 33.626

2022-11-25 10:36:10,854 - INFO  - ==> Sparsity : 0.119

2022-11-25 10:36:10,854 - INFO  - Scoreboard best 1 ==> Epoch [0][Top1: 69.940   Top5: 97.800]
2022-11-25 10:36:10,855 - INFO  - Scoreboard best 2 ==> Epoch [1][Top1: 10.000   Top5: 50.000]
2022-11-25 10:36:11,043 - INFO  - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103312/_checkpoint.pth.tar

2022-11-25 10:36:11,045 - INFO  - >>>>>> Epoch   2
2022-11-25 10:36:11,047 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-11-25 10:36:20,605 - INFO  - Training [2][   20/  196]   Loss nan   Top1 9.843750   Top5 50.468750   BatchTime 0.477763   LR 0.004477   
2022-11-25 10:36:27,992 - INFO  - Training [2][   40/  196]   Loss nan   Top1 9.951172   Top5 49.677734   BatchTime 0.423563   LR 0.004426   
2022-11-25 10:36:34,052 - INFO  - Training [2][   60/  196]   Loss nan   Top1 10.019531   Top5 49.928385   BatchTime 0.383363   LR 0.004374   
2022-11-25 10:36:39,725 - INFO  - Training [2][   80/  196]   Loss nan   Top1 9.960938   Top5 49.794922   BatchTime 0.358440   LR 0.004320   
2022-11-25 10:36:47,443 - INFO  - Training [2][  100/  196]   Loss nan   Top1 9.925781   Top5 49.734375   BatchTime 0.363928   LR 0.004264   
2022-11-25 10:36:54,942 - INFO  - Training [2][  120/  196]   Loss nan   Top1 10.084635   Top5 49.798177   BatchTime 0.365763   LR 0.004206   
2022-11-25 10:37:01,966 - INFO  - Training [2][  140/  196]   Loss nan   Top1 10.119978   Top5 49.860491   BatchTime 0.363684   LR 0.004146   
2022-11-25 10:37:09,136 - INFO  - Training [2][  160/  196]   Loss nan   Top1 10.065918   Top5 50.004883   BatchTime 0.363038   LR 0.004085   
2022-11-25 10:37:16,391 - INFO  - Training [2][  180/  196]   Loss nan   Top1 10.080295   Top5 50.095486   BatchTime 0.363002   LR 0.004022   
2022-11-25 10:37:22,424 - INFO  - ==> Top1: 10.074    Top5: 50.064    Loss: nan

2022-11-25 10:37:22,682 - INFO  - Created `MobileNetv2` model
          Use pre-trained model = False
2022-11-25 10:37:24,318 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-11-25 10:37:26,588 - INFO  - Validation [2][   20/   40]   Loss 37.371957   Top1 10.078125   Top5 50.195312   BatchTime 0.113418   
2022-11-25 10:37:27,711 - INFO  - Validation [2][   40/   40]   Loss 37.485639   Top1 10.000000   Top5 50.000000   BatchTime 0.084801   
2022-11-25 10:37:27,924 - INFO  - ==> Top1: 10.000    Top5: 50.000    Loss: 37.486

2022-11-25 10:37:27,924 - INFO  - ==> Sparsity : 0.148

2022-11-25 10:37:27,924 - INFO  - Scoreboard best 1 ==> Epoch [0][Top1: 69.940   Top5: 97.800]
2022-11-25 10:37:27,924 - INFO  - Scoreboard best 2 ==> Epoch [2][Top1: 10.000   Top5: 50.000]
2022-11-25 10:37:27,925 - INFO  - Scoreboard best 3 ==> Epoch [1][Top1: 10.000   Top5: 50.000]
2022-11-25 10:37:28,047 - INFO  - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103312/_checkpoint.pth.tar

2022-11-25 10:37:28,048 - INFO  - >>>>>> Epoch   3
2022-11-25 10:37:28,050 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-11-25 10:37:36,740 - INFO  - Training [3][   20/  196]   Loss nan   Top1 11.230469   Top5 51.289062   BatchTime 0.434356   LR 0.003907   
2022-11-25 10:37:44,405 - INFO  - Training [3][   40/  196]   Loss nan   Top1 10.332031   Top5 49.707031   BatchTime 0.408808   LR 0.003840   
2022-11-25 10:37:51,258 - INFO  - Training [3][   60/  196]   Loss nan   Top1 10.130208   Top5 49.576823   BatchTime 0.386751   LR 0.003771   
2022-11-25 10:37:57,609 - INFO  - Training [3][   80/  196]   Loss nan   Top1 10.180664   Top5 49.960938   BatchTime 0.369446   LR 0.003701   
2022-11-25 10:38:04,065 - INFO  - Training [3][  100/  196]   Loss nan   Top1 10.273438   Top5 50.148438   BatchTime 0.360122   LR 0.003630   
2022-11-25 10:38:11,117 - INFO  - Training [3][  120/  196]   Loss nan   Top1 10.224609   Top5 49.980469   BatchTime 0.358864   LR 0.003558   
2022-11-25 10:38:18,604 - INFO  - Training [3][  140/  196]   Loss nan   Top1 10.136719   Top5 50.013951   BatchTime 0.361077   LR 0.003484   
