2022-11-25 08:42:57,759 - INFO  - Log file for this run: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-084257/88_20221125-084257.log
2022-11-25 08:43:01,712 - INFO  - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
2022-11-25 08:43:03,573 - INFO  - Created `MobileNetv2` model
          Use pre-trained model = True
2022-11-25 08:43:04,336 - INFO  - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
2022-11-25 08:43:04,337 - INFO  - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005

2022-11-25 08:43:04,619 - INFO  - >>>>>> Epoch   0
2022-11-25 08:43:04,620 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-11-25 08:43:13,912 - INFO  - Training [0][   20/  196]   Loss 1.929112   Top1 53.613281   Top5 89.140625   BatchTime 0.464439   LR 0.004999   
2022-11-25 08:43:22,158 - INFO  - Training [0][   40/  196]   Loss 1.986952   Top1 52.832031   Top5 89.667969   BatchTime 0.438366   LR 0.004995   
2022-11-25 08:43:29,291 - INFO  - Training [0][   60/  196]   Loss 1.917989   Top1 54.856771   Top5 90.872396   BatchTime 0.411136   LR 0.004989   
2022-11-25 08:43:34,474 - INFO  - Training [0][   80/  196]   Loss 1.861403   Top1 56.308594   Top5 91.586914   BatchTime 0.373135   LR 0.004980   
2022-11-25 08:43:40,078 - INFO  - Training [0][  100/  196]   Loss nan   Top1 54.656250   Top5 89.578125   BatchTime 0.354542   LR 0.004968   
2022-11-25 08:43:45,690 - INFO  - Training [0][  120/  196]   Loss nan   Top1 47.060547   Top5 82.903646   BatchTime 0.342219   LR 0.004954   
2022-11-25 08:43:51,149 - INFO  - Training [0][  140/  196]   Loss nan   Top1 41.668527   Top5 78.030134   BatchTime 0.332326   LR 0.004938   
2022-11-25 08:43:56,685 - INFO  - Training [0][  160/  196]   Loss nan   Top1 37.766113   Top5 74.416504   BatchTime 0.325381   LR 0.004919   
2022-11-25 08:44:02,627 - INFO  - Training [0][  180/  196]   Loss nan   Top1 34.676649   Top5 71.612413   BatchTime 0.322240   LR 0.004897   
2022-11-25 08:44:06,575 - INFO  - ==> Top1: 32.736    Top5: 69.876    Loss: nan

2022-11-25 08:44:06,779 - INFO  - Created `MobileNetv2` model
          Use pre-trained model = False
2022-11-25 08:44:08,030 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-11-25 08:44:10,160 - INFO  - Validation [0][   20/   40]   Loss 2.642930   Top1 10.019531   Top5 49.902344   BatchTime 0.106397   
2022-11-25 08:44:11,259 - INFO  - Validation [0][   40/   40]   Loss 2.644470   Top1 10.000000   Top5 50.000000   BatchTime 0.080698   
2022-11-25 08:44:11,415 - INFO  - ==> Top1: 10.000    Top5: 50.000    Loss: 2.644

2022-11-25 08:44:11,415 - INFO  - ==> Sparsity : 0.236

2022-11-25 08:44:11,416 - INFO  - Scoreboard best 1 ==> Epoch [0][Top1: 10.000   Top5: 50.000]
2022-11-25 08:44:16,626 - INFO  - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-084257/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-084257/_best.pth.tar
save quantized models...
2022-11-25 08:44:16,628 - INFO  - >>>>>> Epoch   1
2022-11-25 08:44:16,631 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-11-25 08:44:23,303 - INFO  - Training [1][   20/  196]   Loss nan   Top1 10.058594   Top5 49.570312   BatchTime 0.333435   LR 0.004853   
