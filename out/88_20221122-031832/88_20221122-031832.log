2022-11-22 03:18:32,356 - INFO  - Log file for this run: /home/ilena7440/LSQ_FakeQuant/out/88_20221122-031832/88_20221122-031832.log
2022-11-22 03:18:37,967 - INFO  - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
2022-11-22 03:18:38,047 - INFO  - Created `once_for_all` model
          Use pre-trained model = True
2022-11-22 03:18:38,335 - INFO  - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               differentiable: False
               foreach: None
               lr: 0.0001
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 0.0005
           )
2022-11-22 03:18:38,335 - INFO  - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.0001

2022-11-22 03:18:40,194 - INFO  - >>>>>> Epoch -1 (pre-trained model evaluation)
2022-11-22 03:18:40,194 - INFO  - Validation: 10000 samples (128 per mini-batch)
2022-11-22 03:18:43,818 - INFO  - Validation [   20/   79]   Loss 8.111629   Top1 0.000000   Top5 0.000000   BatchTime 0.181094   
2022-11-22 03:18:45,097 - INFO  - Validation [   40/   79]   Loss 8.152248   Top1 0.000000   Top5 0.000000   BatchTime 0.122503   
2022-11-22 03:18:46,121 - INFO  - Validation [   60/   79]   Loss 8.178463   Top1 0.000000   Top5 0.000000   BatchTime 0.098746   
2022-11-22 03:18:47,123 - INFO  - ==> Top1: 0.000    Top5: 0.000    Loss: 8.187

2022-11-22 03:18:47,124 - INFO  - Scoreboard best 1 ==> Epoch [-1][Top1: 0.000   Top5: 0.000]
2022-11-22 03:18:47,124 - INFO  - >>>>>> Epoch   0
2022-11-22 03:18:47,124 - INFO  - Training: 50000 samples (128 per mini-batch)
2022-11-22 03:18:50,167 - INFO  - Training [0][   20/  391]   Loss 10.595572   Top1 0.078125   Top5 0.234375   BatchTime 0.151844   LR 0.000100   
2022-11-22 03:18:52,718 - INFO  - Training [0][   40/  391]   Loss 10.320716   Top1 0.039062   Top5 0.234375   BatchTime 0.139696   LR 0.000100   
2022-11-22 03:18:55,050 - INFO  - Training [0][   60/  391]   Loss 10.111725   Top1 0.052083   Top5 0.260417   BatchTime 0.132002   LR 0.000100   
2022-11-22 03:18:57,407 - INFO  - Training [0][   80/  391]   Loss 9.858017   Top1 0.068359   Top5 0.322266   BatchTime 0.128455   LR 0.000100   
2022-11-22 03:18:59,892 - INFO  - Training [0][  100/  391]   Loss 9.586921   Top1 0.062500   Top5 0.437500   BatchTime 0.127618   LR 0.000100   
2022-11-22 03:19:02,242 - INFO  - Training [0][  120/  391]   Loss 9.331902   Top1 0.143229   Top5 0.618490   BatchTime 0.125928   LR 0.000100   
2022-11-22 03:19:04,573 - INFO  - Training [0][  140/  391]   Loss 9.089702   Top1 0.178571   Top5 0.820312   BatchTime 0.124592   LR 0.000100   
2022-11-22 03:19:07,220 - INFO  - Training [0][  160/  391]   Loss 8.954418   Top1 0.219727   Top5 1.059570   BatchTime 0.125558   LR 0.000100   
2022-11-22 03:19:09,926 - INFO  - Training [0][  180/  391]   Loss nan   Top1 0.577257   Top5 2.812500   BatchTime 0.126642   LR 0.000100   
2022-11-22 03:19:12,405 - INFO  - Training [0][  200/  391]   Loss nan   Top1 1.546875   Top5 7.515625   BatchTime 0.126370   LR 0.000100   
2022-11-22 03:19:14,783 - INFO  - Training [0][  220/  391]   Loss nan   Top1 2.322443   Top5 11.239347   BatchTime 0.125691   LR 0.000100   
2022-11-22 03:19:17,135 - INFO  - Training [0][  240/  391]   Loss nan   Top1 2.906901   Top5 14.339193   BatchTime 0.125019   LR 0.000100   
