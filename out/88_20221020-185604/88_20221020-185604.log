2022-10-20 18:56:04,028 - INFO  - Log file for this run: /home/ilena7440/LSQFakeQuant/out/88_20221020-185604/88_20221020-185604.log
2022-10-20 18:56:05,214 - INFO  - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
2022-10-20 18:56:05,250 - INFO  - Created `resnet20` model
          Use pre-trained model = True
2022-10-20 18:56:05,377 - INFO  - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.001
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 0.0001
           )
2022-10-20 18:56:05,377 - INFO  - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.001

2022-10-20 18:56:06,523 - INFO  - >>>>>> Epoch -1 (pre-trained model evaluation)
2022-10-20 18:56:06,523 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-10-20 18:56:07,953 - INFO  - Validation [   20/   40]   Loss 4.050640   Top1 15.332031   Top5 67.363281   BatchTime 0.069629   
2022-10-20 18:56:08,604 - INFO  - Validation [   40/   40]   Loss 4.047568   Top1 15.560000   Top5 67.030000   BatchTime 0.051086   
2022-10-20 18:56:08,672 - INFO  - ==> Top1: 15.560    Top5: 67.030    Loss: 4.048

2022-10-20 18:56:08,672 - INFO  - Scoreboard best 1 ==> Epoch [-1][Top1: 15.560   Top5: 67.030]
2022-10-20 18:56:08,672 - INFO  - >>>>>> Epoch   0
2022-10-20 18:56:08,672 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-10-20 18:56:10,791 - INFO  - Training [0][   20/  196]   Loss 1.234248   Top1 67.109375   Top5 94.531250   BatchTime 0.105917   LR 0.001000   
2022-10-20 18:56:12,356 - INFO  - Training [0][   40/  196]   Loss 1.037877   Top1 70.302734   Top5 95.244141   BatchTime 0.092073   LR 0.001000   
2022-10-20 18:56:13,911 - INFO  - Training [0][   60/  196]   Loss 0.942039   Top1 72.623698   Top5 95.800781   BatchTime 0.087305   LR 0.001000   
2022-10-20 18:56:15,467 - INFO  - Training [0][   80/  196]   Loss 0.875961   Top1 73.999023   Top5 96.171875   BatchTime 0.084919   LR 0.001000   
2022-10-20 18:56:17,039 - INFO  - Training [0][  100/  196]   Loss 0.829173   Top1 74.980469   Top5 96.472656   BatchTime 0.083653   LR 0.001000   
2022-10-20 18:56:18,594 - INFO  - Training [0][  120/  196]   Loss 0.795500   Top1 75.751953   Top5 96.757812   BatchTime 0.082670   LR 0.001000   
2022-10-20 18:56:20,150 - INFO  - Training [0][  140/  196]   Loss 0.767503   Top1 76.453683   Top5 96.925223   BatchTime 0.081973   LR 0.001000   
2022-10-20 18:56:21,705 - INFO  - Training [0][  160/  196]   Loss 0.743721   Top1 76.984863   Top5 97.060547   BatchTime 0.081447   LR 0.001000   
2022-10-20 18:56:23,254 - INFO  - Training [0][  180/  196]   Loss 0.725530   Top1 77.391493   Top5 97.167969   BatchTime 0.081002   LR 0.001000   
2022-10-20 18:56:24,539 - INFO  - ==> Top1: 77.620    Top5: 97.270    Loss: 0.714

2022-10-20 18:56:24,606 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-10-20 18:56:25,663 - INFO  - Validation [0][   20/   40]   Loss 0.628909   Top1 79.921875   Top5 98.144531   BatchTime 0.052787   
2022-10-20 18:56:26,200 - INFO  - Validation [0][   40/   40]   Loss 0.633792   Top1 79.750000   Top5 98.170000   BatchTime 0.039824   
2022-10-20 18:56:26,271 - INFO  - ==> Top1: 79.750    Top5: 98.170    Loss: 0.634

2022-10-20 18:56:26,271 - INFO  - Validation: 10000 samples (256 per mini-batch)
2022-10-20 18:56:27,825 - INFO  - Validation [0][   20/   40]   Loss 0.630610   Top1 80.195312   Top5 98.027344   BatchTime 0.077686   
2022-10-20 18:56:28,622 - INFO  - Validation [0][   40/   40]   Loss 0.636266   Top1 80.080000   Top5 98.170000   BatchTime 0.058766   
2022-10-20 18:56:28,700 - INFO  - ==> Top1: 80.080    Top5: 98.170    Loss: 0.636

2022-10-20 18:56:28,700 - INFO  - Scoreboard best 1 ==> Epoch [0][Top1: 80.080   Top5: 98.170]
2022-10-20 18:56:28,700 - INFO  - Scoreboard best 2 ==> Epoch [-1][Top1: 15.560   Top5: 67.030]
2022-10-20 18:56:30,456 - INFO  - Saving checkpoint to:
            Current: /home/ilena7440/LSQFakeQuant/out/88_20221020-185604/88_checkpoint.pth.tar
                Best: /home/ilena7440/LSQFakeQuant/out/88_20221020-185604/88_best.pth.tar
save quantized models...
2022-10-20 18:56:30,456 - INFO  - >>>>>> Epoch   1
2022-10-20 18:56:30,456 - INFO  - Training: 50000 samples (256 per mini-batch)
2022-10-20 18:56:32,599 - INFO  - Training [1][   20/  196]   Loss 0.535381   Top1 82.011719   Top5 98.222656   BatchTime 0.107092   LR 0.001000   
2022-10-20 18:56:34,155 - INFO  - Training [1][   40/  196]   Loss 0.523712   Top1 82.412109   Top5 98.378906   BatchTime 0.092452   LR 0.001000   
2022-10-20 18:56:35,712 - INFO  - Training [1][   60/  196]   Loss 0.526441   Top1 82.376302   Top5 98.398438   BatchTime 0.087582   LR 0.001000   
2022-10-20 18:56:37,268 - INFO  - Training [1][   80/  196]   Loss 0.518855   Top1 82.617188   Top5 98.461914   BatchTime 0.085136   LR 0.001000   
2022-10-20 18:56:38,825 - INFO  - Training [1][  100/  196]   Loss 0.512730   Top1 82.726562   Top5 98.507812   BatchTime 0.083680   LR 0.001000   
2022-10-20 18:56:40,387 - INFO  - Training [1][  120/  196]   Loss 0.514920   Top1 82.652995   Top5 98.505859   BatchTime 0.082750   LR 0.001000   
2022-10-20 18:56:41,939 - INFO  - Training [1][  140/  196]   Loss 0.512061   Top1 82.739955   Top5 98.479353   BatchTime 0.082012   LR 0.001000   
