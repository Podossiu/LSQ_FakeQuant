Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
tensor(1.9141, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.9297, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.7598, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.7568, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(2.0430, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.9092, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.9355, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.7725, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 77, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader,qat_model, teacher_model,criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 52, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 180, in train_one_epoch_slsq
    scaler.scale(loss).backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Expected dY.scalar_type() == ScalarType::Float to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)
0.00000000
tensor(1.8945, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.7236, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.00000000
tensor(1.6660, device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)
0.96194512
tensor(2.3056, device='cuda:0', grad_fn=<AddBackward0>)