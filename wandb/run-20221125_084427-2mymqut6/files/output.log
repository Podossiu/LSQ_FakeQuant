Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.00000000
0.00000000
0.95438832
0.95018005
0.93024260
0.92081916
0.90632784
INFO - Training [0][   20/  196]   Loss 1.595880   Top1 53.964844   Top5 89.179688   BatchTime 0.333362   LR 0.004999
0.89132160
0.89215451
0.88584572
0.87861520
0.87224853
0.86893654
0.86833733
0.86819482
0.86693752
0.86694592
0.86758357
0.86845356
0.86967206
0.87237293
0.87511605
0.87697083
0.87936437
0.87952304
0.88150823
0.88070160
0.88043803
0.87752306
0.87273484
INFO - Training [0][   40/  196]   Loss 1.518910   Top1 52.695312   Top5 89.375000   BatchTime 0.298575   LR 0.004995
0.86985487
0.86716861
0.86466432
0.86820173
0.86885625
0.87012774
0.87143391
0.87266392
0.87435603
0.87560385
0.87713689
0.87650949
0.87919098
0.88127381
0.88358790
0.88536489
0.88820535
0.88805258
0.88874006
0.88895148
INFO - Training [0][   60/  196]   Loss 1.417363   Top1 54.993490   Top5 90.729167   BatchTime 0.299000   LR 0.004989
0.88463247
0.87321305
0.85830843
0.85380101
0.86229622
0.86671001
0.86703849
0.87076443
0.87623096
0.88207895
0.88599807
0.88900489
0.89001578
0.89023679
0.89124453
0.89219701
0.89221889
0.89257014
0.89241344
0.89290124
0.89272052
INFO - Training [0][   80/  196]   Loss 1.351063   Top1 56.660156   Top5 91.630859   BatchTime 0.295567   LR 0.004980
0.89247352
0.89237750
0.89170611
0.89201301
0.89045423
0.89309013
0.89436394
0.89521015
0.89656115
0.89740604
0.89854711
0.89828873
0.89861304
0.89895600
0.89927107
0.89921010
INFO - Training [0][  100/  196]   Loss 1.292502   Top1 58.398438   Top5 92.402344   BatchTime 0.290297   LR 0.004968
0.89857459
0.89808023
0.89786339
0.89716011
0.89596462
0.89438498
0.89363211
0.89283508
0.89244771
0.89174932
0.89223266
0.89237261
0.89240378
0.89301002
0.89578199
0.89902836
0.90128058
0.90278882
0.90345287
0.90354025
0.90357721
0.90315562
0.90274608
INFO - Training [0][  120/  196]   Loss 1.245921   Top1 59.866536   Top5 92.903646   BatchTime 0.286386   LR 0.004954
0.90233958
0.90167695
0.90111929
0.90094519
0.90082830
0.90013444
0.89957267
0.89925534
0.89828873
0.89611828
0.89252639
0.89164150
0.89012539
0.88976216
0.89058316
0.88806605
0.88620734
0.88504457
0.88390887
0.88194573
0.88085139
0.87923336
INFO - Training [0][  140/  196]   Loss 1.213286   Top1 60.831473   Top5 93.275670   BatchTime 0.285964   LR 0.004938
0.87773281
0.87574822
0.87486893
0.87617272
0.87532979
0.87309247
0.87049633
0.86936015
0.86899537
0.86756861
0.86744231
0.86599797
0.86352044
0.85934156
0.85863477
0.85890186
0.85928041
0.85870719
0.85836107
INFO - Training [0][  160/  196]   Loss 1.188838   Top1 61.557617   Top5 93.537598   BatchTime 0.288356   LR 0.004919
0.85772771
0.85766482
0.85790771
0.85805041
0.85858524
0.85933757
0.86014587
0.86124915
0.86222512
0.86353427
0.86476743
0.86639810
0.86567032
0.86443949
0.86513901
0.86532420
0.86519414
0.86439109
0.86422026
0.86594886
INFO - Training [0][  180/  196]   Loss 1.163300   Top1 62.319878   Top5 93.704427   BatchTime 0.290068   LR 0.004897
0.86604553
0.86644065
0.86694920
0.86738831
0.86776781
0.86811018
0.86861598
0.86910206
0.86920393
0.86930352
0.87013716
0.87108397
0.87180704
0.87248999
INFO - ==> Top1: 62.854    Top5: 93.844    Loss: 1.146
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87232530
0.87183809
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [0][   20/   40]   Loss 0.849221   Top1 72.675781   Top5 97.617188   BatchTime 0.110771
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0595)
features.2.conv.0 tensor(0.0443)
features.2.conv.3 tensor(0.0679)
features.2.conv.6 tensor(0.0894)
features.3.conv.0 tensor(0.0454)
features.3.conv.3 tensor(0.0594)
features.3.conv.6 tensor(0.0640)
features.4.conv.0 tensor(0.0617)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.1105)
features.5.conv.0 tensor(0.0568)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.1115)
features.6.conv.0 tensor(0.0529)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0898)
features.7.conv.0 tensor(0.0912)
features.7.conv.3 tensor(0.1036)
features.7.conv.6 tensor(0.2940)
features.8.conv.0 tensor(0.1216)
features.8.conv.3 tensor(0.1010)
features.8.conv.6 tensor(0.1434)
features.9.conv.0 tensor(0.1125)
features.9.conv.3 tensor(0.1262)
features.9.conv.6 tensor(0.1383)
features.10.conv.0 tensor(0.0850)
features.10.conv.3 tensor(0.0856)
features.10.conv.6 tensor(0.1104)
features.11.conv.0 tensor(0.1532)
features.11.conv.3 tensor(0.0926)
features.11.conv.6 tensor(0.1785)
features.12.conv.0 tensor(0.1750)
features.12.conv.3 tensor(0.0860)
features.12.conv.6 tensor(0.3211)
features.13.conv.0 tensor(0.1804)
features.13.conv.3 tensor(0.1235)
features.13.conv.6 tensor(0.1124)
features.14.conv.0 tensor(0.7814)
features.14.conv.3 tensor(0.0841)
features.14.conv.6 tensor(0.3480)
features.15.conv.0 tensor(0.8836)
features.15.conv.3 tensor(0.0718)
features.15.conv.6 tensor(0.8676)
features.16.conv.0 tensor(0.0445)
features.16.conv.3 tensor(0.0856)
features.16.conv.6 tensor(0.0972)
conv.0 tensor(0.0806)
tensor(617155.) 2188896.0
INFO - Validation [0][   40/   40]   Loss 0.853023   Top1 72.310000   Top5 97.650000   BatchTime 0.084276
INFO - ==> Top1: 72.310    Top5: 97.650    Loss: 0.853
INFO - ==> Sparsity : 0.282
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 72.310   Top5: 97.650]
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-084426/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-084426/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.87048113
0.86985081
0.86871403
0.86758751
0.86850709
0.86972833
0.87045145
0.87179929
0.87476832
0.87466401
0.87423605
0.87387556
0.87351608
0.87324381
0.87240165
0.87229049
0.87243134
0.87201375
0.87204629
0.87163311
0.87159884
0.87128985
INFO - Training [1][   20/  196]   Loss 0.970853   Top1 68.320312   Top5 95.429688   BatchTime 0.333525   LR 0.004853
0.87104964
0.87084895
0.87083787
0.87053359
0.87036067
0.87040722
0.87048060
0.87050349
0.87035233
0.86994004
0.86931139
0.86760080
0.86554515
0.86166090
0.86293066
0.86120325
INFO - Training [1][   40/  196]   Loss 0.953599   Top1 68.935547   Top5 95.693359   BatchTime 0.294079   LR 0.004825
0.86104876
0.86049736
0.85875344
0.85828489
0.85755509
0.85486072
0.84750974
0.83888501
0.83558893
0.84035468
0.84607762
0.84980208
0.85044938
0.85234767
0.85293746
0.85365862
0.85634434
0.85884923
0.86072260
0.86139005
INFO - Training [1][   60/  196]   Loss 0.940917   Top1 69.264323   Top5 95.820312   BatchTime 0.293189   LR 0.004794
0.86183000
0.86234397
0.86211878
0.86178356
0.86094433
0.86000085
0.86070693
0.86209005
0.86144215
0.86081266
0.86021364
0.85916489
0.85845220
0.85766441
0.85728508
0.85695571
0.85658479
0.85590988
0.85510164
INFO - Training [1][   80/  196]   Loss 0.932867   Top1 69.521484   Top5 95.903320   BatchTime 0.300537   LR 0.004761
0.85366678
0.85234416
0.85306674
0.85158843
0.84115738
0.82643950
0.82658249
0.82717007
0.82743335
0.82756728
0.82746750
0.82781541
0.82809025
0.82845813
0.82915473
0.83199358
0.83554542
0.83709371
0.83665842
0.83583909
0.83577496
INFO - Training [1][  100/  196]   Loss 0.920586   Top1 69.933594   Top5 96.082031   BatchTime 0.296336   LR 0.004725
0.83779109
0.83869839
0.83901381
0.83963811
0.84136009
0.84239346
0.84315795
0.84331512
0.84225720
0.84127933
0.84033132
0.83844566
0.83361644
0.83389461
0.83191401
0.82884443
0.82822025
0.83076483
0.83027947
0.83230728
INFO - Training [1][  120/  196]   Loss 0.908631   Top1 70.390625   Top5 96.210938   BatchTime 0.298244   LR 0.004687
0.83406323
0.83338547
0.83285570
0.83262891
0.83160084
0.83053565
0.82992303
0.83012366
0.83139586
0.83455199
0.83732545
0.84079784
0.84319496
0.84526139
0.84601063
0.84531373
0.84487700
0.84490132
0.84587574
0.84607929
0.84567720
INFO - Training [1][  140/  196]   Loss 0.899962   Top1 70.708705   Top5 96.308594   BatchTime 0.297243   LR 0.004647
0.84529352
0.84232801
0.83981335
0.83775216
0.83651125
0.83570200
0.83526248
0.83460134
0.83419079
0.83332115
0.83267581
0.83241314
0.83196622
0.83130705
0.83120459
0.83141148
0.83146566
0.83196312
INFO - Training [1][  160/  196]   Loss 0.895171   Top1 70.837402   Top5 96.323242   BatchTime 0.300646   LR 0.004605
0.83206332
0.83227688
0.83240372
0.83237928
0.83274794
0.83396441
0.83633798
0.83934897
0.84298730
0.84815973
0.85409909
0.85923254
0.86218971
0.86256677
0.86213899
0.86217201
0.86269432
0.86440909
0.86483759
0.86506778
INFO - Training [1][  180/  196]   Loss 0.885859   Top1 71.132812   Top5 96.317274   BatchTime 0.300138   LR 0.004560
0.86553609
0.86561316
0.86554539
0.86549497
0.86543596
0.86554110
0.86559099
0.86558145
0.86539912
0.86531323
0.86543155
0.86556220
0.86564076
0.86545616
0.86530572
0.86489081
INFO - ==> Top1: 71.286    Top5: 96.332    Loss: 0.883
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.86484224
0.86458433
0.86429131
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.631686   Top1 79.121094   Top5 98.613281   BatchTime 0.108214
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.1934)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0564)
features.2.conv.0 tensor(0.0448)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.0998)
features.3.conv.0 tensor(0.0414)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0744)
features.4.conv.0 tensor(0.0535)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.1038)
features.5.conv.0 tensor(0.0493)
features.5.conv.3 tensor(0.0637)
features.5.conv.6 tensor(0.1110)
features.6.conv.0 tensor(0.0448)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0894)
features.7.conv.0 tensor(0.1030)
features.7.conv.3 tensor(0.1024)
features.7.conv.6 tensor(0.1259)
features.8.conv.0 tensor(0.1019)
features.8.conv.3 tensor(0.1024)
features.8.conv.6 tensor(0.1384)
features.9.conv.0 tensor(0.1033)
features.9.conv.3 tensor(0.1354)
features.9.conv.6 tensor(0.1367)
features.10.conv.0 tensor(0.0764)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.1108)
features.11.conv.0 tensor(0.1898)
features.11.conv.3 tensor(0.0961)
features.11.conv.6 tensor(0.1928)
features.12.conv.0 tensor(0.1641)
features.12.conv.3 tensor(0.1051)
features.12.conv.6 tensor(0.5034)
features.13.conv.0 tensor(0.1968)
features.13.conv.3 tensor(0.1319)
features.13.conv.6 tensor(0.1107)
features.14.conv.0 tensor(0.8803)
features.14.conv.3 tensor(0.0847)
features.14.conv.6 tensor(0.3984)
features.15.conv.0 tensor(0.9145)
features.15.conv.3 tensor(0.0729)
features.15.conv.6 tensor(0.8757)
features.16.conv.0 tensor(0.0923)
features.16.conv.3 tensor(0.0808)
features.16.conv.6 tensor(0.1397)
conv.0 tensor(0.1090)
tensor(686324.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 0.634027   Top1 78.760000   Top5 98.690000   BatchTime 0.080337
INFO - ==> Top1: 78.760    Top5: 98.690    Loss: 0.634
INFO - ==> Sparsity : 0.314
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 78.760   Top5: 98.690]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 72.310   Top5: 97.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-084426/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-084426/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.86381680
0.86349261
0.86311442
0.86288625
0.86247045
0.86227185
0.86195409
0.86153805
0.86117905
0.86062866
0.86004859
0.85951865
0.85925347
0.85909384
0.85875398
0.85828358
0.85810900
0.85754597
0.85713500
0.85673398
INFO - Training [2][   20/  196]   Loss 0.857629   Top1 71.796875   Top5 96.015625   BatchTime 0.331564   LR 0.004477
0.85595310
0.85561585
0.85502356
0.85458189
0.85418463
0.85344785
0.85299116
0.85336143
0.85397738
0.85463911
0.85425752
0.85436779
0.85350531
0.85193175
0.85126090
0.85010642
0.84986675
0.84914601
0.84757632
0.84539771
INFO - Training [2][   40/  196]   Loss 0.846850   Top1 72.480469   Top5 96.201172   BatchTime 0.316977   LR 0.004426
0.83812082
0.84231526
0.83773750
0.83049899
0.83855003
0.84419018
0.84739262
0.84958684
0.85131747
0.85130656
0.85156041
0.85206288
0.85293120
0.85346198
0.85357755
0.85387570
0.85433513
0.85515338
0.85595202
0.85681528
INFO - Training [2][   60/  196]   Loss 0.831316   Top1 73.046875   Top5 96.425781   BatchTime 0.312726   LR 0.004374
0.85780078
0.85935110
0.87252516
0.87275469
0.87242913
0.87196773
0.87133890
0.87111282
0.87065428
0.87012154
0.86970043
0.86777139
0.86446190
0.86575645
0.86413127
0.86085182
0.85974056
0.86411542
0.86697233
0.86637175
0.86616719
INFO - Training [2][   80/  196]   Loss 0.818986   Top1 73.442383   Top5 96.562500   BatchTime 0.305458   LR 0.004320
0.86584234
0.86637515
0.86618197
0.86623776
0.86649370
0.86690378
0.86687827
0.86688870
0.86693418
0.86628687
0.86522615
0.86529833
0.86559284
0.86641645
0.86947674
0.87347978
0.87335891
0.87333924
0.87328696
0.87347436
0.87373006
0.87395614
INFO - Training [2][  100/  196]   Loss 0.813500   Top1 73.652344   Top5 96.589844   BatchTime 0.299965   LR 0.004264
0.87412357
0.87417763
0.87430811
0.87441581
0.87494177
0.87585312
0.87674898
0.87761712
0.87754148
0.87727946
0.87598664
0.87521321
0.87457532
nan
nan
nan
nan
nan
nan
INFO - Training [2][  120/  196]   Loss nan   Top1 72.369792   Top5 95.543620   BatchTime 0.300951   LR 0.004206
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [2][  140/  196]   Loss nan   Top1 63.417969   Top5 88.892299   BatchTime 0.299363   LR 0.004146
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [2][  160/  196]   Loss nan   Top1 56.738281   Top5 83.930664   BatchTime 0.297486   LR 0.004085
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [2][  180/  196]   Loss nan   Top1 51.601562   Top5 80.334201   BatchTime 0.294207   LR 0.004022
nan
nan
nan
nan
nan
nan
Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/wandb/sdk/lib/exit_hooks.py", line 54, in exc_handler
    traceback.print_exception(exc_type, exc, tb)
  File "/usr/lib/python3.8/traceback.py", line 103, in print_exception
    for line in TracebackException(
  File "/usr/lib/python3.8/traceback.py", line 509, in __init__
    self.stack = StackSummary.extract(
  File "/usr/lib/python3.8/traceback.py", line 353, in extract
    linecache.lazycache(filename, f.f_globals)
  File "/home/ilena7440/qilbertenv/lib/python3.8/linecache.py", line 175, in lazycache
    cache[filename] = (get_lines,)
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 77, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader,qat_model, teacher_model,criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 186, in train_one_epoch_slsq
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt