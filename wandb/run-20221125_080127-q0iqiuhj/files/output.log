
Files already downloaded and verified
Files already downloaded and verified
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.0005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.0005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.95131010
0.89207035
0.92038947
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.96072030
0.96080691
0.96077305
0.96136993
0.96106428
0.96217829
0.96302491
0.96310860
0.96267980
0.96238619
INFO - Training [0][   20/  196]   Loss 1.756723   Top1 39.238281   Top5 83.417969   BatchTime 0.608207   LR 0.000500
0.95804965
0.95492387
0.95357132
0.95131391
0.94974935
0.94735694
0.94516742
0.94339639
0.94223708
0.94101012
0.93996823
0.93824118
0.93698782
0.93546611
0.93380660
0.93238330
0.93114758
0.93032986
INFO - Training [0][   40/  196]   Loss 1.680889   Top1 41.474609   Top5 85.273438   BatchTime 0.585397   LR 0.000500
0.92942005
0.92920619
0.92877603
0.92816263
0.92796469
0.92806953
0.92793268
0.92758811
0.92682493
0.92632508
0.92447746
0.92312962
0.92094415
0.92147547
0.92341429
0.92272359
0.92222393
0.92161685
0.92029434
0.91915143
0.91764182
0.91747934
INFO - Training [0][   60/  196]   Loss 1.595841   Top1 44.342448   Top5 87.154948   BatchTime 0.575022   LR 0.000499
0.91663063
0.91573566
0.91526860
0.91405123
0.91226041
0.91124451
0.91114473
0.91084951
0.91000038
0.90877062
0.90764451
0.90682715
0.90570641
0.90466666
0.90399534
0.90324223
0.90219969
0.90068257
INFO - Training [0][   80/  196]   Loss 1.531653   Top1 46.811523   Top5 88.417969   BatchTime 0.569673   LR 0.000498
0.90003139
0.89905477
0.89814234
0.89746845
0.89676815
0.89618164
0.89571738
0.89534163
0.89509165
0.89486271
0.89432913
0.89417434
0.89432365
0.89400172
0.89390057
0.89362949
0.89349324
0.89345610
0.89270186
0.89258784
INFO - Training [0][  100/  196]   Loss 1.477730   Top1 48.851562   Top5 89.242188   BatchTime 0.557684   LR 0.000497
0.89231217
0.89193541
0.89182347
0.89189970
0.89203900
0.89189231
0.89186591
0.89166838
0.89148849
0.89172387
0.89176530
0.89174771
0.89132071
0.89134681
0.89090526
0.89028788
0.88984460
0.88969129
INFO - Training [0][  120/  196]   Loss 1.431413   Top1 50.527344   Top5 89.853516   BatchTime 0.552339   LR 0.000495
0.88919294
0.88901669
0.88865399
0.88837409
0.88800645
0.88694322
0.88676590
0.88567722
0.88509536
0.88483095
0.88457358
0.88433266
0.88490587
0.88489103
0.88485223
0.88596159
0.88697702
0.88670969
0.88664800
0.88686293
INFO - Training [0][  140/  196]   Loss 1.397987   Top1 51.693638   Top5 90.343192   BatchTime 0.550604   LR 0.000494
0.88663727
0.88630664
0.88587940
0.88581234
0.88500923
0.88471735
0.88445646
0.88426834
0.88357365
0.88349932
0.88314581
0.88309860
0.88318366
0.88298053
0.88274360
0.88299817
0.88353986
0.88398325
0.88380587
0.88457537
0.88484484
0.88443357
0.88432199
INFO - Training [0][  160/  196]   Loss 1.376194   Top1 52.500000   Top5 90.688477   BatchTime 0.534755   LR 0.000492
0.88396198
0.88410556
0.88366908
0.88349313
0.88296831
0.88277650
0.88213563
0.88188416
0.88194710
0.88174999
0.88165855
0.88107187
0.88092250
0.88095093
0.88140851
0.88180465
0.88185334
0.88175559
0.88190424
INFO - Training [0][  180/  196]   Loss 1.350738   Top1 53.424479   Top5 91.015625   BatchTime 0.531329   LR 0.000490
0.88165051
0.88154817
0.88132060
0.88104022
0.88102478
0.88094699
0.88085240
0.88065946
0.88055152
0.87927562
0.87827390
0.87761307
0.87722683
0.87737972
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 53.996    Top5: 91.238    Loss: 1.334
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3368)
features.0.conv.3 tensor(0.1191)
features.1.conv.0 tensor(0.0625)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0729)
features.2.conv.0 tensor(0.1565)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.1895)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0926)
features.3.conv.6 tensor(0.0953)
features.4.conv.0 tensor(0.1167)
features.4.conv.3 tensor(0.3015)
features.4.conv.6 tensor(0.1698)
features.5.conv.0 tensor(0.2541)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.1123)
features.6.conv.0 tensor(0.0563)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0878)
features.7.conv.0 tensor(0.1567)
features.7.conv.3 tensor(0.4537)
features.7.conv.6 tensor(0.1859)
features.8.conv.0 tensor(0.4731)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.1451)
features.9.conv.0 tensor(0.5011)
features.9.conv.3 tensor(0.5703)
features.9.conv.6 tensor(0.1398)
features.10.conv.0 tensor(0.0853)
features.10.conv.3 tensor(0.1131)
features.10.conv.6 tensor(0.1076)
features.11.conv.0 tensor(0.4943)
features.11.conv.3 tensor(0.6516)
features.11.conv.6 tensor(0.7153)
features.12.conv.0 tensor(0.6024)
features.12.conv.3 tensor(0.6958)
features.12.conv.6 tensor(0.6797)
features.13.conv.0 tensor(0.2221)
features.13.conv.3 tensor(0.4985)
features.13.conv.6 tensor(0.0893)
features.14.conv.0 tensor(0.4633)
features.14.conv.3 tensor(0.8361)
features.14.conv.6 tensor(0.1454)
features.15.conv.0 tensor(0.4490)
features.15.conv.3 tensor(0.8904)
features.15.conv.6 tensor(0.9003)
features.16.conv.0 tensor(0.5142)
features.16.conv.3 tensor(0.8159)
features.16.conv.6 tensor(0.0648)
conv.0 tensor(0.0440)
tensor(668453.) 2188896.0
INFO - Validation [0][   20/   40]   Loss 1.211298   Top1 61.015625   Top5 94.960938   BatchTime 0.106739
INFO - Validation [0][   40/   40]   Loss 1.231139   Top1 60.440000   Top5 94.980000   BatchTime 0.082128
INFO - ==> Top1: 60.440    Top5: 94.980    Loss: 1.231
INFO - ==> Sparsity : 0.305
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 60.440   Top5: 94.980]
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.87708145
0.87665623
0.87633920
0.87474227
0.87444699
0.87449145
0.87393582
0.87322384
0.87391281
0.87777495
0.87797326
0.87786168
0.87779385
0.87821895
0.87795001
0.87777293
0.87707549
0.87720299
0.87705886
0.87768823
0.87741840
0.87737215
INFO - Training [1][   20/  196]   Loss 1.158925   Top1 58.984375   Top5 93.515625   BatchTime 0.501157   LR 0.000485
0.87722033
0.87794322
0.87720704
0.87680978
0.87672204
0.87669712
0.87703758
0.87725818
0.87712824
0.87710124
0.87715429
0.87731349
0.87741435
0.87732041
0.87715060
0.87704253
0.87728465
0.87734449
0.87711984
0.87720424
0.87696278
INFO - Training [1][   40/  196]   Loss 1.162281   Top1 59.482422   Top5 92.900391   BatchTime 0.477862   LR 0.000482
0.87704080
0.87685841
0.87721092
0.87716442
0.87705475
0.87691218
0.87686539
0.87706542
0.87613380
0.87651223
0.87699944
0.87709481
0.87711191
0.87714118
0.87685502
0.87702280
0.87693620
0.87666082
0.87670791
0.87665671
INFO - Training [1][   60/  196]   Loss 1.137966   Top1 60.130208   Top5 93.404948   BatchTime 0.462121   LR 0.000479
0.87651587
0.87641114
0.87644750
0.87626940
0.87632459
0.87635130
0.87532097
0.87541145
0.87556380
0.87603819
0.87671256
0.87703806
0.87720007
0.87728029
0.87710583
0.87711674
0.87677658
0.87662369
0.87656111
0.87651610
INFO - Training [1][   80/  196]   Loss 1.125479   Top1 60.708008   Top5 93.769531   BatchTime 0.441355   LR 0.000476
0.87656099
0.87651747
0.87614125
0.87587893
0.87604982
0.87639368
0.87622082
0.87618291
0.87601382
0.87576210
0.87560910
0.87558085
0.87560296
0.87573755
0.87553024
0.87576181
0.87518555
INFO - Training [1][  100/  196]   Loss 1.109344   Top1 61.292969   Top5 94.062500   BatchTime 0.447630   LR 0.000473
0.87428629
0.87434566
0.87448835
0.87444204
0.87490487
0.87561679
0.87541932
0.87546593
0.87545252
0.87489438
0.87482661
0.87467855
0.87463617
0.87428564
0.87452298
0.87476385
0.87475890
0.87495047
0.87468362
0.87443596
0.87417656
0.87384027
0.87309414
INFO - Training [1][  120/  196]   Loss 1.109810   Top1 61.429036   Top5 93.854167   BatchTime 0.446778   LR 0.000469
0.87300789
0.87316245
0.87271267
0.87230635
0.87195480
0.87187111
0.87232059
0.87291235
0.87330955
0.87336785
0.87328470
0.87364465
0.87375021
0.87362081
0.87310332
0.87301403
0.87292725
0.87300664
0.87277657
INFO - Training [1][  140/  196]   Loss 1.101380   Top1 61.729911   Top5 94.054129   BatchTime 0.443489   LR 0.000465
0.87299967
0.87307060
0.87280935
0.87301713
0.87331492
0.87345415
0.87391073
0.87442970
0.87412113
0.87396723
0.87469369
0.87569267
0.87608194
0.87581939
0.87591195
0.87603754
0.87591207
0.87569833
0.87580997
0.87595052
0.87599015
INFO - Training [1][  160/  196]   Loss 1.094991   Top1 61.926270   Top5 94.157715   BatchTime 0.436337   LR 0.000460
0.87590307
0.87583512
0.87589073
0.87567019
0.87554318
0.87581402
0.87613320
0.87544590
0.87541908
0.87530822
0.87529045
0.87558240
0.87569314
0.87537628
0.87522620
0.87536353
0.87542826
0.87524492
0.87504464
INFO - Training [1][  180/  196]   Loss 1.082352   Top1 62.339410   Top5 94.223090   BatchTime 0.434395   LR 0.000456
0.87504476
0.87540460
0.87542397
0.87551421
0.87532467
0.87529665
0.87571293
0.87595087
0.87577528
0.87550300
0.87582994
0.87594885
0.87574875
0.87545925
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 62.640    Top5: 94.280    Loss: 1.075
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3924)
features.0.conv.3 tensor(0.1504)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0734)
features.2.conv.0 tensor(0.0940)
features.2.conv.3 tensor(0.3341)
features.2.conv.6 tensor(0.1814)
features.3.conv.0 tensor(0.0561)
features.3.conv.3 tensor(0.0910)
features.3.conv.6 tensor(0.0985)
features.4.conv.0 tensor(0.0633)
features.4.conv.3 tensor(0.3131)
features.4.conv.6 tensor(0.1525)
features.5.conv.0 tensor(0.1383)
features.5.conv.3 tensor(0.4334)
features.5.conv.6 tensor(0.1263)
features.6.conv.0 tensor(0.0558)
features.6.conv.3 tensor(0.0648)
features.6.conv.6 tensor(0.0903)
features.7.conv.0 tensor(0.1220)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.1829)
features.8.conv.0 tensor(0.4723)
features.8.conv.3 tensor(0.5469)
features.8.conv.6 tensor(0.1467)
features.9.conv.0 tensor(0.5015)
features.9.conv.3 tensor(0.5680)
features.9.conv.6 tensor(0.1329)
features.10.conv.0 tensor(0.0840)
features.10.conv.3 tensor(0.1123)
features.10.conv.6 tensor(0.1036)
features.11.conv.0 tensor(0.4269)
features.11.conv.3 tensor(0.6501)
features.11.conv.6 tensor(0.7599)
features.12.conv.0 tensor(0.5588)
features.12.conv.3 tensor(0.6921)
features.12.conv.6 tensor(0.7863)
features.13.conv.0 tensor(0.1946)
features.13.conv.3 tensor(0.4936)
features.13.conv.6 tensor(0.0939)
features.14.conv.0 tensor(0.4403)
features.14.conv.3 tensor(0.8343)
features.14.conv.6
INFO - Validation [1][   20/   40]   Loss 0.760946   Top1 74.804688   Top5 97.929688   BatchTime 0.106678
features.14.conv.6 tensor(0.1543)
features.15.conv.0 tensor(0.4203)
features.15.conv.3 tensor(0.8973)
features.15.conv.6 tensor(0.9320)
features.16.conv.0 tensor(0.5026)
features.16.conv.3 tensor(0.8159)
features.16.conv.6 tensor(0.0759)
conv.0 tensor(0.0523)
tensor(670353.) 2188896.0
INFO - ==> Top1: 74.520    Top5: 98.100    Loss: 0.754
INFO - ==> Sparsity : 0.306
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 74.520   Top5: 98.100]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 60.440   Top5: 94.980]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.87505364
0.87413985
0.87532294
0.87532753
0.87543195
0.87568343
0.87554771
0.87529755
0.87508798
0.87513733
0.87549919
0.87551296
0.87546587
0.87537861
0.87571156
0.87548035
0.87521923
0.87494874
0.87511867
0.87519324
0.87546170
0.87552345
0.87563169
0.87564659
INFO - Training [2][   20/  196]   Loss 0.998265   Top1 64.921875   Top5 94.667969   BatchTime 0.508656   LR 0.000448
0.87555528
0.87549329
0.87535030
0.87534666
0.87526846
0.87512082
0.87530500
0.87581611
0.87598723
0.87614650
0.87594682
0.87595624
0.87592047
0.87572652
0.87554032
0.87566024
0.87559354
INFO - Training [2][   40/  196]   Loss 0.978283   Top1 65.722656   Top5 94.882812   BatchTime 0.490181   LR 0.000443
0.87570167
0.87538797
0.87557006
0.87528086
0.87491375
0.87533146
0.87504399
0.87569445
0.87667251
0.87644923
0.87660718
0.87662488
0.87654233
0.87634087
0.87633198
0.87657636
0.87700582
0.87693584
0.87696618
0.87671614
0.87641078
0.87657994
INFO - Training [2][   60/  196]   Loss 0.964718   Top1 66.289062   Top5 95.292969   BatchTime 0.483079   LR 0.000437
0.87690496
0.87666017
0.87659091
0.87652051
0.87639672
0.87695485
0.87671357
0.87665474
0.87659192
0.87655616
0.87637693
0.87602049
0.87555438
0.87596422
0.87582976
0.87553191
0.87541896
0.87611604
0.87585187
INFO - Training [2][   80/  196]   Loss 0.957221   Top1 66.503906   Top5 95.454102   BatchTime 0.463262   LR 0.000432
0.87538433
0.87489361
0.87517053
0.87563729
0.87548381
0.87568539
0.87579721
0.87578553
0.87558037
0.87550992
0.87553298
0.87541211
0.87518084
0.87508237
0.87533516
0.87522227
0.87548834
0.87591076
0.87557423
0.87546432
0.87505919
INFO - Training [2][  100/  196]   Loss 0.949910   Top1 66.847656   Top5 95.546875   BatchTime 0.466875   LR 0.000426
0.87512696
0.87512696
0.87556314
0.87546217
0.87545395
0.87547129
0.87521553
0.87530780
0.87511897
0.87503827
0.87538105
0.87538123
0.87542158
0.87541175
0.87543350
0.87541604
0.87529564
0.87528980
INFO - Training [2][  120/  196]   Loss 0.940283   Top1 67.180990   Top5 95.670573   BatchTime 0.464906   LR 0.000421
0.87522942
0.87506115
0.87506342
0.87514383
0.87531310
0.87491602
0.87479025
0.87458074
0.87389159
0.87328279
0.87275898
0.87249106
0.87209678
0.87193239
0.87145054
0.87116069
0.87050879
0.86961800
0.86884195
0.86794525
0.86667669
0.86476666
0.86373287
0.86223704
INFO - Training [2][  140/  196]   Loss 0.939159   Top1 67.318638   Top5 95.728237   BatchTime 0.458964   LR 0.000415
0.86012417
0.85911179
0.85720426
0.86094952
0.86049920
0.86059737
0.86135113
0.86210424
0.86288428
0.86322606
0.86563122
0.86970520
0.87634504
0.87628555
0.87661660
0.87617826
INFO - Training [2][  160/  196]   Loss 0.938490   Top1 67.333984   Top5 95.727539   BatchTime 0.449189   LR 0.000409
0.87609935
0.87610948
0.87625927
0.87656808
0.87637025
0.87592608
0.87595987
0.87637144
0.87573797
0.87643784
0.87631422
0.87632549
0.87665546
0.87650311
0.87678427
0.87655640
0.87645942
0.87634969
0.87643540
0.87645507
0.87618107
0.87611324
0.87646180
INFO - Training [2][  180/  196]   Loss 0.932204   Top1 67.619358   Top5 95.724826   BatchTime 0.447066   LR 0.000402
0.87613225
0.87590671
0.87627780
0.87622190
0.87585944
0.87542552
0.87577617
0.87585932
0.87564021
0.87556440
0.87563807
0.87528938
********************pre-trained*****************
INFO - ==> Top1: 67.762    Top5: 95.756    Loss: 0.927
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0803)
features.2.conv.0 tensor(0.0666)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.1921)
features.3.conv.0 tensor(0.0648)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.0996)
features.4.conv.0 tensor(0.0531)
features.4.conv.3 tensor(0.3148)
features.4.conv.6 tensor(0.1538)
features.5.conv.0 tensor(0.1227)
features.5.conv.3 tensor(0.4311)
features.5.conv.6 tensor(0.1222)
features.6.conv.0 tensor(0.0576)
features.6.conv.3 tensor(0.0596)
features.6.conv.6 tensor(0.0879)
features.7.conv.0 tensor(0.1066)
features.7.conv.3 tensor(0.4563)
features.7.conv.6 tensor(0.1927)
features.8.conv.0 tensor(0.4732)
features.8.conv.3 tensor(0.5495)
features.8.conv.6 tensor(0.1546)
features.9.conv.0 tensor(0.4838)
features.9.conv.3 tensor(0.5622)
features.9.conv.6 tensor(0.1475)
features.10.conv.0 tensor(0.0704)
features.10.conv.3 tensor(0.1157)
features.10.conv.6 tensor(0.1049)
features.11.conv.0 tensor(0.4276)
features.11.conv.3 tensor(0.6481)
features.11.conv.6 tensor(0.7725)
features.12.conv.0 tensor(0.5194)
features.12.conv.3 tensor(0.6844)
features.12.conv.6 tensor(0.7783)
features.13.conv.0 tensor(0.1984)
features.13.conv.3 tensor(0.4942)
features.13.conv.6 tensor(0.0977)
features.14.conv.0 tensor(0.3851)
features.14.conv.3 tensor(0.8304)
features.14.conv.6 tensor(0.1618)
features.15.conv.0 tensor(0.4030)
features.15.conv.3 tensor(0.8995)
features.15.conv.6 tensor(0.9436)
features.16.conv.0 tensor(0.4783)
features.16.conv.3 tensor(0.8138)
features.16.conv.6 tensor(0.0883)
conv.0 tensor(0.0555)
tensor(661674.) 2188896.0
INFO - Validation [2][   20/   40]   Loss 0.739541   Top1 74.960938   Top5 98.242188   BatchTime 0.104705
INFO - Validation [2][   40/   40]   Loss 0.734424   Top1 74.600000   Top5 98.280000   BatchTime 0.080754
INFO - ==> Top1: 74.600    Top5: 98.280    Loss: 0.734
INFO - ==> Sparsity : 0.302
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 74.600   Top5: 98.280]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 74.520   Top5: 98.100]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 60.440   Top5: 94.980]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.87538862
0.87564123
0.87574363
0.87566602
0.87599188
0.87545323
0.87501901
0.87507576
0.87506813
0.87486875
0.87489879
0.87494254
0.87508035
0.87532580
0.87520832
0.87534976
0.87511241
0.87507391
INFO - Training [3][   20/  196]   Loss 0.894892   Top1 69.121094   Top5 95.605469   BatchTime 0.517197   LR 0.000391
0.87529457
0.87523323
0.87540382
0.87537628
0.87535238
0.87526929
0.87534398
0.87505490
0.87508005
0.87510806
0.87518251
0.87498856
0.87510324
0.87521964
0.87522084
0.87509483
0.87519509
0.87489688
0.87456602
0.87496513
0.87503254
0.87466699
INFO - Training [3][   40/  196]   Loss 0.886723   Top1 69.384766   Top5 95.888672   BatchTime 0.490944   LR 0.000384
0.87473077
0.87473959
0.87492901
0.87464702
0.87478775
0.87433505
0.87409651
0.87404382
0.87354481
0.87334973
0.87367207
0.87343293
0.87299222
0.87247932
0.87312180
0.87375134
0.87461668
0.87835091
0.88144296
0.88175225
INFO - Training [3][   60/  196]   Loss 0.882534   Top1 69.602865   Top5 95.983073   BatchTime 0.457685   LR 0.000377
0.88168389
0.88155985
0.88159972
0.88127506
0.88104135
0.88091666
0.88142240
0.88143897
0.88142461
0.88167840
0.88179028
0.88151783
0.88169444
0.88197440
0.88229847
0.88208753
0.88215709
0.88203484
INFO - Training [3][   80/  196]   Loss 0.870171   Top1 69.960938   Top5 96.181641   BatchTime 0.455917   LR 0.000370
0.88239813
0.88232368
0.88240290
0.88246763
0.88278300
0.88277411
0.88271463
0.88258082
0.88252413
0.88261336
0.88205725
0.88149184
0.88134283
0.88072664
0.88086212
0.88112569
0.88131952
0.88084096
0.88220787
0.88195443
0.88218367
INFO - Training [3][  100/  196]   Loss 0.857157   Top1 70.480469   Top5 96.292969   BatchTime 0.459174   LR 0.000363
0.88227183
0.88221866
0.88201946
0.88208908
0.88183898
0.88184774
0.88193184
0.88177443
0.88158369
0.88150346
0.88175285
0.88174313
0.88174856
0.88161433
0.88172621
0.88153929
0.88116288
INFO - Training [3][  120/  196]   Loss 0.850783   Top1 70.621745   Top5 96.422526   BatchTime 0.458027   LR 0.000356
0.88067967
0.88060087
0.88067740
0.88061869
0.87981915
0.87934434
0.87907511
0.87911910
0.87915742
0.87938094
0.87993103
0.88038647
0.88119948
0.88199753
0.88316017
0.88326800
0.88333124
INFO - Training [3][  140/  196]   Loss 0.847998   Top1 70.597098   Top5 96.487165   BatchTime 0.453383   LR 0.000348
0.88316447
0.88297969
0.88289106
0.88299507
0.88348538
0.88311785
0.88270277
0.88180590
0.88156945
0.88279039
0.88259417
0.88253224
0.88257629
0.88270617
0.88285267
0.88296252
0.88288444
0.88264781
0.88250208
0.88241851
0.88255668
0.88252044
0.88238537
0.88235712
0.88245124
INFO - Training [3][  160/  196]   Loss 0.846622   Top1 70.622559   Top5 96.467285   BatchTime 0.449101   LR 0.000341
0.88224363
0.88228887
0.88241726
0.88238221
0.88241225
0.88206995
0.88214695
0.88228095
0.88240641
0.88236582
0.88247043
0.88240528
0.88184434
0.88177502
0.88137585
0.88063961
0.88048160
0.87984782
INFO - Training [3][  180/  196]   Loss 0.841073   Top1 70.813802   Top5 96.436632   BatchTime 0.450204   LR 0.000333
0.87968355
0.87966537
0.87960142
0.87974483
0.87986833
0.87986350
0.87973052
0.87965220
0.87934190
0.87902719
0.87858808
0.87852627
0.87824643
0.87768972
0.87738669
0.87834704
0.87910652
0.87916327
0.87927854
0.87871867
********************pre-trained*****************
INFO - ==> Top1: 70.910    Top5: 96.442    Loss: 0.838
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.604618   Top1 79.882812   Top5 98.652344   BatchTime 0.181784
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0410)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0440)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1973)
features.3.conv.0 tensor(0.0645)
features.3.conv.3 tensor(0.0887)
features.3.conv.6 tensor(0.1013)
features.4.conv.0 tensor(0.0503)
features.4.conv.3 tensor(0.3229)
features.4.conv.6 tensor(0.1427)
features.5.conv.0 tensor(0.1227)
features.5.conv.3 tensor(0.4225)
features.5.conv.6 tensor(0.2059)
features.6.conv.0 tensor(0.0529)
features.6.conv.3 tensor(0.0648)
features.6.conv.6 tensor(0.0830)
features.7.conv.0 tensor(0.1067)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.1916)
features.8.conv.0 tensor(0.4765)
features.8.conv.3 tensor(0.5463)
features.8.conv.6 tensor(0.1625)
features.9.conv.0 tensor(0.4702)
features.9.conv.3 tensor(0.5613)
features.9.conv.6 tensor(0.1485)
features.10.conv.0 tensor(0.0671)
features.10.conv.3 tensor(0.1111)
features.10.conv.6 tensor(0.1064)
features.11.conv.0 tensor(0.3526)
features.11.conv.3 tensor(0.6476)
features.11.conv.6 tensor(0.7633)
features.12.conv.0 tensor(0.5122)
features.12.conv.3 tensor(0.6821)
features.12.conv.6 tensor(0.7777)
features.13.conv.0 tensor(0.1969)
features.13.conv.3 tensor(0.4948)
features.13.conv.6 tensor(0.0986)
features.14.conv.0 tensor(0.3880)
features.14.conv.3 tensor(0.8281)
features.14.conv.6 tensor(0.1718)
features.15.conv.0 tensor(0.4048)
features.15.conv.3 tensor(0.9000)
features.15.conv.6 tensor(0.9460)
features.16.conv.0 tensor(0.4540)
features.16.conv.3 tensor(0.8144)
features.16.conv.6 tensor(0.0968)
conv.0 tensor(0.0598)
tensor(660008.) 2188896.0
INFO - Validation [3][   40/   40]   Loss 0.602723   Top1 79.940000   Top5 98.740000   BatchTime 0.122518
INFO - ==> Top1: 79.940    Top5: 98.740    Loss: 0.603
INFO - ==> Sparsity : 0.302
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 79.940   Top5: 98.740]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 74.600   Top5: 98.280]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 74.520   Top5: 98.100]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.87861735
0.87864727
0.87876940
0.87861079
0.87866527
0.87905520
0.87920392
0.87960905
0.88015163
0.88054502
0.88164991
0.88191807
0.88163763
0.88142639
0.88138872
0.88139379
0.88140953
INFO - Training [4][   20/  196]   Loss 0.818975   Top1 71.816406   Top5 95.996094   BatchTime 0.502043   LR 0.000320
0.88144940
0.88172859
0.88212633
0.88208055
0.88178921
0.88171196
0.88158375
0.88162577
0.88155031
0.88151801
0.88157654
0.88187206
0.88133246
0.88116330
0.88054794
0.87979454
0.87950027
0.87942725
0.87912071
0.87922019
0.87940174
0.88032520
0.87980306
0.87969929
INFO - Training [4][   40/  196]   Loss 0.811680   Top1 72.070312   Top5 96.328125   BatchTime 0.464527   LR 0.000312
0.87849987
0.87817830
0.87782198
0.87779754
0.87839687
0.87822199
0.87797409
0.87782985
0.87772155
0.87736565
0.87704086
0.87668920
0.87668502
0.87596697
0.87549376
0.87509102
INFO - Training [4][   60/  196]   Loss 0.814411   Top1 72.031250   Top5 96.438802   BatchTime 0.436342   LR 0.000304
0.87456578
0.87423068
0.87391907
0.87359786
0.87337857
0.87322038
0.87296295
0.87210488
0.87233663
0.87263954
0.87387681
0.87392575
0.87355191
0.87345731
0.87308556
0.87294424
0.87264097
0.87248689
0.87198603
INFO - Training [4][   80/  196]   Loss 0.810213   Top1 72.197266   Top5 96.567383   BatchTime 0.433307   LR 0.000296
0.87234455
0.87225211
0.87222570
0.87188882
0.87158161
0.87135118
0.87113625
0.87122703
0.87120944
0.87069327
0.87060905
0.87003160
0.87014109
0.87001854
0.86970353
0.86928278
0.86913365
0.86922240
0.86940718
0.86918300
0.86919695
0.86914152
INFO - Training [4][  100/  196]   Loss 0.795983   Top1 72.722656   Top5 96.722656   BatchTime 0.437556   LR 0.000289
0.86898899
0.86918360
0.86936432
0.86905432
0.86890131
0.86899519
0.86911184
0.86914235
0.86910814
0.86929661
0.86950123
0.86976504
0.86988789
0.86963809
0.86999995
0.87053925
0.87078071
0.87181485
0.87352002
INFO - Training [4][  120/  196]   Loss 0.788919   Top1 72.965495   Top5 96.754557   BatchTime 0.440421   LR 0.000281
0.87438029
0.87533033
0.87550682
0.87621444
0.87671471
0.87681329
0.87648660
0.87690467
0.87715495
0.87708771
0.87717021
0.87722260
0.87691194
0.87706959
0.87714058
0.87682801
0.87635636
0.87606114
INFO - Training [4][  140/  196]   Loss 0.786767   Top1 73.074777   Top5 96.813616   BatchTime 0.438152   LR 0.000273
0.87590140
0.87565374
0.87596446
0.87613755
0.87595135
0.87578511
0.87591326
0.87585229
0.87562370
0.87516451
0.87532109
0.87516981
0.87497824
0.87504727
0.87522745
0.87520921
0.87554830
0.87503517
0.87439650
0.87412393
0.87410206
0.87409413
INFO - Training [4][  160/  196]   Loss 0.786211   Top1 73.107910   Top5 96.816406   BatchTime 0.439397   LR 0.000265
0.87406594
0.87388778
0.87381202
0.87370050
0.87298924
0.87359440
0.87371022
0.87365282
0.87369704
0.87360138
0.87350273
0.87327933
0.87321883
0.87312621
0.87316412
0.87308186
0.87303144
0.87267554
0.87238884
0.87148702
0.87153363
0.87356555
INFO - Training [4][  180/  196]   Loss 0.780215   Top1 73.276910   Top5 96.803385   BatchTime 0.441683   LR 0.000257
0.87339044
0.87342924
0.87324739
0.87296730
0.87263668
0.87288439
0.87317479
0.87313664
0.87294447
0.87326431
0.87328595
0.87320638
0.87343389
INFO - ==> Top1: 73.452    Top5: 96.814    Loss: 0.777
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87279683
0.87274623
0.87239522
0.87187696
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.543893   Top1 82.128906   Top5 99.121094   BatchTime 0.127224
INFO - Validation [4][   40/   40]   Loss 0.533895   Top1 82.190000   Top5 99.190000   BatchTime 0.090337
INFO - ==> Top1: 82.190    Top5: 99.190    Loss: 0.534
INFO - ==> Sparsity : 0.311
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 82.190   Top5: 99.190]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 79.940   Top5: 98.740]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 74.600   Top5: 98.280]
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1406)
features.1.conv.0 tensor(0.0417)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0781)
features.2.conv.0 tensor(0.0561)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.2069)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0965)
features.3.conv.6 tensor(0.1057)
features.4.conv.0 tensor(0.0511)
features.4.conv.3 tensor(0.3183)
features.4.conv.6 tensor(0.1444)
features.5.conv.0 tensor(0.1300)
features.5.conv.3 tensor(0.4334)
features.5.conv.6 tensor(0.1271)
features.6.conv.0 tensor(0.0498)
features.6.conv.3 tensor(0.0625)
features.6.conv.6 tensor(0.0829)
features.7.conv.0 tensor(0.1009)
features.7.conv.3 tensor(0.4552)
features.7.conv.6 tensor(0.1931)
features.8.conv.0 tensor(0.4703)
features.8.conv.3 tensor(0.5469)
features.8.conv.6 tensor(0.1624)
features.9.conv.0 tensor(0.4698)
features.9.conv.3 tensor(0.5639)
features.9.conv.6 tensor(0.1430)
features.10.conv.0 tensor(0.0603)
features.10.conv.3 tensor(0.1102)
features.10.conv.6 tensor(0.1072)
features.11.conv.0 tensor(0.3503)
features.11.conv.3 tensor(0.6507)
features.11.conv.6 tensor(0.7610)
features.12.conv.0 tensor(0.4981)
features.12.conv.3 tensor(0.6838)
features.12.conv.6 tensor(0.7777)
features.13.conv.0 tensor(0.1968)
features.13.conv.3 tensor(0.4919)
features.13.conv.6 tensor(0.0987)
features.14.conv.0 tensor(0.3762)
features.14.conv.3 tensor(0.8270)
features.14.conv.6 tensor(0.1580)
features.15.conv.0 tensor(0.5727)
features.15.conv.3 tensor(0.9019)
features.15.conv.6 tensor(0.9474)
features.16.conv.0 tensor(0.4459)
features.16.conv.3 tensor(0.8133)
features.16.conv.6 tensor(0.1017)
conv.0 tensor(0.0614)
tensor(681171.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.87157750
0.87167728
0.87178856
0.87181699
0.87171566
0.87164271
0.87140852
0.87117410
0.87113506
0.87070465
0.87052673
0.87068385
0.87011248
0.86968386
0.86874270
0.86880720
0.86865538
0.86814916
0.86758244
INFO - Training [5][   20/  196]   Loss 0.751215   Top1 73.808594   Top5 96.660156   BatchTime 0.500776   LR 0.000242
0.86725485
0.86663848
0.86663187
0.86634105
0.86638069
0.86585373
0.86577570
0.86542094
0.86530399
0.86510658
0.86496150
0.86502290
0.86512268
0.86498374
0.86483282
0.86377466
0.86317927
INFO - Training [5][   40/  196]   Loss 0.771900   Top1 73.134766   Top5 96.875000   BatchTime 0.477268   LR 0.000234
0.86294019
0.86331147
0.86317396
0.86303735
0.86326915
0.86288661
0.86277610
0.86230284
0.86224538
0.86217588
0.86204541
0.86182570
0.86159331
0.86157089
0.86162567
0.86140305
0.86176085
0.86198843
0.86198354
0.86206347
0.86196113
INFO - Training [5][   60/  196]   Loss 0.760513   Top1 73.834635   Top5 96.757812   BatchTime 0.443104   LR 0.000226
0.86184180
0.86176747
0.86180609
0.86195803
0.86208826
0.86211413
0.86205018
0.86202431
0.86222613
0.86224478
0.86250335
0.86259460
0.86277312
0.86314911
0.86412627
0.86698705
0.87537628
0.88162458
0.88275993
0.88266075
INFO - Training [5][   80/  196]   Loss 0.746277   Top1 74.345703   Top5 96.855469   BatchTime 0.433175   LR 0.000218
0.88244385
0.88257420
0.88249409
0.88272172
0.88262653
0.88265085
0.88267457
0.88287508
0.88268554
0.88266903
0.88231862
0.88179123
0.88135755
0.88138974
0.88133645
0.88241088
0.88226992
0.88244379
0.88265204
0.88272297
0.88209784
0.88238674
INFO - Training [5][  100/  196]   Loss 0.738873   Top1 74.539062   Top5 97.000000   BatchTime 0.440014   LR 0.000210
0.88283473
0.88269830
0.88267666
0.88249189
0.88256502
0.88252509
0.88275194
0.88267577
0.88246274
0.88246077
0.88244170
0.88256019
0.88203770
0.88158989
0.88068604
0.88025230
0.87994140
0.87914699
INFO - Training [5][  120/  196]   Loss 0.730844   Top1 74.869792   Top5 97.099609   BatchTime 0.440614   LR 0.000202
0.87961388
0.87976366
0.87981415
0.87986642
0.87962013
0.87945801
0.87937146
0.87961882
0.87982690
0.87988526
0.88166624
0.88198334
0.88180465
0.88163984
0.88153607
0.88131148
0.88112545
0.88120830
0.88102257
INFO - Training [5][  140/  196]   Loss 0.727073   Top1 74.988839   Top5 97.165179   BatchTime 0.435590   LR 0.000195
0.88108331
0.88120663
0.88124156
0.88124949
0.88082641
0.88025624
0.88000530
0.88046342
0.88067102
0.88099593
0.88167530
0.88205636
0.88411129
0.88441497
0.88460147
0.88492268
0.88456815
0.88453114
0.88438135
0.88427621
0.88457400
INFO - Training [5][  160/  196]   Loss 0.726427   Top1 75.021973   Top5 97.165527   BatchTime 0.430576   LR 0.000187
0.88481724
0.88471299
0.88454473
0.88462001
0.88439137
0.88461435
0.88482511
0.88492012
0.88478124
0.88472730
0.88461155
0.88429505
0.88418764
0.88424212
0.88420320
0.88406396
0.88391280
INFO - Training [5][  180/  196]   Loss 0.722170   Top1 75.206163   Top5 97.113715   BatchTime 0.433220   LR 0.000179
0.88409936
0.88425863
0.88419241
0.88371450
0.88401347
0.88393658
0.88384038
0.88322836
0.88433921
0.88432008
0.88429821
0.88418901
0.88429797
0.88445872
0.88451260
0.88443345
0.88447189
0.88435751
INFO - ==> Top1: 75.310    Top5: 97.106    Loss: 0.720
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.88407248
0.88403088
0.88393837
0.88389993
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.497901   Top1 83.300781   Top5 99.121094   BatchTime 0.105560
INFO - Validation [5][   40/   40]   Loss 0.494858   Top1 83.050000   Top5 99.240000   BatchTime 0.081796
INFO - ==> Top1: 83.050    Top5: 99.240    Loss: 0.495
INFO - ==> Sparsity : 0.304
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 83.050   Top5: 99.240]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 82.190   Top5: 99.190]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 79.940   Top5: 98.740]
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0573)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.0613)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1887)
features.3.conv.0 tensor(0.0625)
features.3.conv.3 tensor(0.0949)
features.3.conv.6 tensor(0.1085)
features.4.conv.0 tensor(0.0511)
features.4.conv.3 tensor(0.3212)
features.4.conv.6 tensor(0.1444)
features.5.conv.0 tensor(0.1315)
features.5.conv.3 tensor(0.4277)
features.5.conv.6 tensor(0.1253)
features.6.conv.0 tensor(0.0540)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0830)
features.7.conv.0 tensor(0.0994)
features.7.conv.3 tensor(0.4540)
features.7.conv.6 tensor(0.1909)
features.8.conv.0 tensor(0.4723)
features.8.conv.3 tensor(0.5448)
features.8.conv.6 tensor(0.1573)
features.9.conv.0 tensor(0.4729)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.1472)
features.10.conv.0 tensor(0.0646)
features.10.conv.3 tensor(0.1117)
features.10.conv.6 tensor(0.1080)
features.11.conv.0 tensor(0.3528)
features.11.conv.3 tensor(0.6495)
features.11.conv.6 tensor(0.7592)
features.12.conv.0 tensor(0.4959)
features.12.conv.3 tensor(0.6834)
features.12.conv.6 tensor(0.7758)
features.13.conv.0 tensor(0.2012)
features.13.conv.3 tensor(0.4907)
features.13.conv.6 tensor(0.1080)
features.14.conv.0 tensor(0.3899)
features.14.conv.3 tensor(0.8272)
features.14.conv.6 tensor(0.1852)
features.15.conv.0 tensor(0.4159)
features.15.conv.3 tensor(0.9016)
features.15.conv.6 tensor(0.9473)
features.16.conv.0 tensor(0.4480)
features.16.conv.3 tensor(0.8133)
features.16.conv.6 tensor(0.0996)
conv.0 tensor(0.0628)
tensor(664571.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.88372713
0.88335854
0.88336319
0.88364053
0.88337880
0.88366419
0.88376224
0.88365895
0.88337427
0.88292658
0.88372242
0.88349050
0.88352066
0.88310015
0.88318115
0.88333809
0.88331449
0.88313442
INFO - Training [6][   20/  196]   Loss 0.717043   Top1 75.605469   Top5 96.660156   BatchTime 0.502568   LR 0.000166
0.88295549
0.88281131
0.88261265
0.88280350
0.88280302
0.88311744
0.88279253
0.88275760
0.88262546
0.88251632
0.88249665
0.88253373
0.88221061
0.88203669
0.88176513
0.88237810
0.88233924
INFO - Training [6][   40/  196]   Loss 0.714775   Top1 75.322266   Top5 96.992188   BatchTime 0.481904   LR 0.000158
0.88258600
0.88259542
0.88231570
0.88172424
0.88159484
0.88180900
0.88136810
0.88107228
0.88091862
0.88110888
0.88209718
0.88205004
0.88216299
0.88220811
0.88226044
0.88205302
0.88177735
0.88151079
0.88087004
0.88062680
INFO - Training [6][   60/  196]   Loss 0.698202   Top1 75.846354   Top5 97.174479   BatchTime 0.452771   LR 0.000151
0.88018978
0.88059300
0.88064146
0.88039845
0.88002115
0.88011146
0.87990963
0.87964398
0.87921399
0.87900984
0.87853181
0.87819409
0.87822497
0.87797636
0.87795460
0.87710381
0.87680900
0.87745452
0.87817460
0.87935746
INFO - Training [6][   80/  196]   Loss 0.684578   Top1 76.289062   Top5 97.416992   BatchTime 0.439944   LR 0.000143
0.88160217
0.88132590
0.88175249
0.88233793
0.88255161
0.88282281
0.88282901
0.88272053
0.88252246
0.88279748
0.88277727
0.88237995
0.88231903
0.88248700
0.88245183
0.88240296
0.88234270
0.88217330
0.88188231
0.88118619
0.88177925
INFO - Training [6][  100/  196]   Loss 0.676652   Top1 76.484375   Top5 97.488281   BatchTime 0.448237   LR 0.000136
0.88194865
0.88194817
0.88175005
0.88147485
0.88105446
0.88110852
0.88114434
0.88064438
0.88046092
0.88021964
0.88036448
0.88034320
0.88024569
0.88028502
0.88020480
0.87980258
0.87988245
0.88018858
0.87985045
0.87942809
0.87912154
0.87971145
INFO - Training [6][  120/  196]   Loss 0.675020   Top1 76.627604   Top5 97.568359   BatchTime 0.450867   LR 0.000129
0.88017803
0.88048047
0.88204002
0.88222843
0.88238066
0.88230604
0.88201237
0.88184196
0.88152766
0.88138908
0.88132745
0.88113743
0.88112950
0.88104790
0.88097119
0.88110274
0.88125473
0.88119990
0.88114816
INFO - Training [6][  140/  196]   Loss 0.673545   Top1 76.623884   Top5 97.622768   BatchTime 0.444180   LR 0.000122
0.88091993
0.88131195
0.88150531
0.88115942
0.88167483
0.88143587
0.88101542
0.88088405
0.88076627
0.88059801
0.88065028
0.88067341
0.88038957
0.88067412
0.88062131
0.88079244
0.88047165
0.88025594
0.87998998
INFO - Training [6][  160/  196]   Loss 0.675955   Top1 76.567383   Top5 97.573242   BatchTime 0.442840   LR 0.000115
0.87994814
0.88017690
0.88011056
0.88011348
0.88007051
0.88012910
0.88018847
0.88016611
0.88016486
0.88008696
0.88016802
0.88032210
0.88019073
0.88011897
0.87991333
0.87984586
0.87955368
0.87969363
0.87926733
0.87934625
0.87964934
0.87972164
INFO - Training [6][  180/  196]   Loss 0.673085   Top1 76.627604   Top5 97.523872   BatchTime 0.444265   LR 0.000108
0.87966210
0.87954342
0.87956971
0.88049644
0.88055742
0.88029814
0.88025701
0.88028932
0.88017726
0.87961251
0.87907892
0.87892354
0.87887561
INFO - ==> Top1: 76.634    Top5: 97.530    Loss: 0.672
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87886900
0.87883872
0.87879127
0.87878722
0.87852699
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0430)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0816)
features.2.conv.0 tensor(0.0689)
features.2.conv.3 tensor(0.3380)
features.2.conv.6 tensor(0.1907)
features.3.conv.0 tensor(0.0550)
features.3.conv.3 tensor(0.0941)
features.3.conv.6 tensor(0.1079)
features.4.conv.0 tensor(0.0506)
features.4.conv.3 tensor(0.3264)
features.4.conv.6 tensor(0.1367)
features.5.conv.0 tensor(0.1307)
features.5.conv.3 tensor(0.4288)
features.5.conv.6 tensor(0.1261)
features.6.conv.0 tensor(0.0537)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0845)
features.7.conv.0 tensor(0.1013)
features.7.conv.3 tensor(0.4569)
features.7.conv.6 tensor(0.1952)
features.8.conv.0 tensor(0.4589)
features.8.conv.3 tensor(0.5460)
features.8.conv.6 tensor(0.1564)
features.9.conv.0 tensor(0.4278)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.1491)
features.10.conv.0 tensor(0.0634)
features.10.conv.3 tensor(0.1120)
features.10.conv.6 tensor(0.1056)
features.11.conv.0 tensor(0.3497)
features.11.conv.3 tensor(0.6508)
features.11.conv.6 tensor(0.7453)
features.12.conv.0 tensor(0.4931)
features.12.conv.3 tensor(0.6831)
features.12.conv.6 tensor(0.7725)
features.13.conv.0 tensor(0.2019)
features.13.conv.3 tensor(0.4921)
features.13.conv.6 tensor(0.1054)
features.14.conv.0 tensor(0.4031)
features.14.conv.3 tensor(0.8274)
features.14.conv.6 tensor(0.1990)
features.15.conv.0 tensor(0.4279)
features.15.conv.3 tensor(0.9014)
features.15.conv.6 tensor(0.9477)
features.16.conv.0 tensor(0.4487)
features.16.conv.3 tensor(0.8134)
features.16.conv.6 tensor(0.1001)
conv.0 tensor(0.0634)
tensor(668236.) 2188896.0
INFO - Validation [6][   20/   40]   Loss 0.469119   Top1 84.121094   Top5 99.140625   BatchTime 0.110576
INFO - Validation [6][   40/   40]   Loss 0.460309   Top1 84.360000   Top5 99.320000   BatchTime 0.079808
INFO - ==> Top1: 84.360    Top5: 99.320    Loss: 0.460
INFO - ==> Sparsity : 0.305
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 84.360   Top5: 99.320]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 83.050   Top5: 99.240]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 82.190   Top5: 99.190]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.87854671
0.87801576
0.87714010
0.87686795
0.87689662
0.87692720
0.87748438
0.87726939
0.87744403
0.87719411
0.87706500
0.87730724
0.87708682
0.87680233
0.87685037
0.87670273
0.87633282
0.87599927
0.87602508
0.87598848
INFO - Training [7][   20/  196]   Loss 0.664385   Top1 76.386719   Top5 97.226562   BatchTime 0.526019   LR 0.000097
0.87585664
0.87566721
0.87594765
0.87607342
0.87599134
0.87598467
0.87712657
0.87723601
0.87741292
0.87740403
0.87697476
0.87683052
0.87652069
0.87647313
0.87639570
0.87636715
0.87632966
0.87610132
INFO - Training [7][   40/  196]   Loss 0.656855   Top1 77.031250   Top5 97.587891   BatchTime 0.491999   LR 0.000091
0.87594312
0.87620980
0.87597752
0.87582326
0.87568432
0.87576789
0.87559319
0.87572694
0.87606263
0.87576586
0.87548167
0.87554753
0.87549067
0.87501788
0.87486559
0.87487274
0.87497228
0.87538534
INFO - Training [7][   60/  196]   Loss 0.649390   Top1 77.389323   Top5 97.571615   BatchTime 0.469287   LR 0.000085
0.87539250
0.87514710
0.87474209
0.87478429
0.87466204
0.87480026
0.87484413
0.87450081
0.87441659
0.87455255
0.87434244
0.87415516
0.87393469
0.87341183
0.87296206
0.87264931
0.87224883
0.87216485
0.87228698
0.87233788
INFO - Training [7][   80/  196]   Loss 0.648636   Top1 77.553711   Top5 97.626953   BatchTime 0.449798   LR 0.000079
0.87230790
0.87236804
0.87253952
0.87249786
0.87257713
0.87259203
0.87268120
0.87257248
0.87264407
0.87263626
0.87256485
0.87272048
0.87272972
0.87248379
0.87181127
0.87130761
0.87129450
0.87125719
0.87140858
INFO - Training [7][  100/  196]   Loss 0.639756   Top1 77.785156   Top5 97.617188   BatchTime 0.448891   LR 0.000073
0.87127221
0.87111801
0.87116915
0.87116808
0.87128395
0.87105572
0.87096214
0.87112260
0.87112540
0.87127560
0.87131882
0.87136269
0.87113011
0.87109506
0.87136835
0.87106055
0.87084901
0.87171072
0.87179554
0.87203521
0.87177807
0.87163502
0.87162375
INFO - Training [7][  120/  196]   Loss 0.632325   Top1 78.004557   Top5 97.731120   BatchTime 0.446704   LR 0.000067
0.87151009
0.87151122
0.87142044
0.87145853
0.87177873
0.87176824
0.87178510
0.87146169
0.87131649
0.87132412
0.87125641
0.87131596
0.87122720
0.87131935
0.87108207
0.87089139
0.87076348
0.87078613
0.87066346
0.87059891
INFO - Training [7][  140/  196]   Loss 0.630969   Top1 78.055246   Top5 97.792969   BatchTime 0.437280   LR 0.000062
0.87070608
0.87059212
0.87080616
0.87061661
0.87071431
0.87075508
0.87062532
0.87067622
0.87096596
0.87072200
0.87066698
0.87041563
0.87011957
0.87006086
0.86994904
0.86984646
0.86962682
0.86900222
0.86840260
INFO - Training [7][  160/  196]   Loss 0.631378   Top1 78.027344   Top5 97.800293   BatchTime 0.438137   LR 0.000057
0.86857891
0.86840045
0.86858314
0.86886805
0.86900789
0.86863333
0.86858088
0.86777908
0.86775845
0.86757308
0.86764675
0.86748439
0.86741740
0.86754686
0.86768609
0.86751378
0.86718321
0.86691368
0.86684376
0.86662811
0.86642152
0.86660081
INFO - Training [7][  180/  196]   Loss 0.631349   Top1 77.990451   Top5 97.753906   BatchTime 0.439628   LR 0.000052
0.86678529
0.86664414
0.86633325
0.86621767
0.86607707
0.86610556
0.86602449
0.86599666
0.86566079
0.86521524
0.86530602
0.86516118
0.86575323
INFO - ==> Top1: 78.044    Top5: 97.764    Loss: 0.631
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.86564559
0.86476183
0.86415708
0.86391586
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.436409   Top1 85.468750   Top5 99.160156   BatchTime 0.108979
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0423)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0790)
features.2.conv.0 tensor(0.0694)
features.2.conv.3 tensor(0.3395)
features.2.conv.6 tensor(0.1913)
features.3.conv.0 tensor(0.0553)
features.3.conv.3 tensor(0.0910)
features.3.conv.6 tensor(0.1053)
features.4.conv.0 tensor(0.0544)
features.4.conv.3 tensor(0.3264)
features.4.conv.6 tensor(0.1354)
features.5.conv.0 tensor(0.1304)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.1291)
features.6.conv.0 tensor(0.0553)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0811)
features.7.conv.0 tensor(0.1055)
features.7.conv.3 tensor(0.4543)
features.7.conv.6 tensor(0.1941)
features.8.conv.0 tensor(0.4598)
features.8.conv.3 tensor(0.5457)
features.8.conv.6 tensor(0.1577)
features.9.conv.0 tensor(0.4287)
features.9.conv.3 tensor(0.5596)
features.9.conv.6 tensor(0.1500)
features.10.conv.0 tensor(0.0596)
features.10.conv.3 tensor(0.1082)
features.10.conv.6 tensor(0.1066)
features.11.conv.0 tensor(0.3499)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.7463)
features.12.conv.0 tensor(0.5891)
features.12.conv.3 tensor(0.6836)
features.12.conv.6 tensor(0.7702)
features.13.conv.0 tensor(0.2018)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.1056)
features.14.conv.0 tensor(0.4304)
features.14.conv.3 tensor(0.8275)
features.14.conv.6 tensor(0.1871)
features.15.conv.0 tensor(0.5079)
features.15.conv.3 tensor(0.9014)
features.15.conv.6 tensor(0.9486)
features.16.conv.0 tensor(0.4487)
features.16.conv.3 tensor(0.8134)
features.16.conv.6 tensor(0.1009)
conv.0 tensor(0.0628)
tensor(688373.) 2188896.0
INFO - Validation [7][   40/   40]   Loss 0.426040   Top1 85.550000   Top5 99.370000   BatchTime 0.083836
INFO - ==> Top1: 85.550    Top5: 99.370    Loss: 0.426
INFO - ==> Sparsity : 0.314
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 85.550   Top5: 99.370]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 84.360   Top5: 99.320]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 83.050   Top5: 99.240]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
0.86317199
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.86232823
0.86161143
0.86177301
0.86114091
0.86041653
0.85984319
0.85904640
0.85884619
0.85903424
0.85892749
0.85890663
0.85897988
0.85893673
0.85907251
0.85956001
0.85979235
0.85985631
0.85973549
0.85950220
0.85941988
INFO - Training [8][   20/  196]   Loss 0.604084   Top1 79.140625   Top5 97.226562   BatchTime 0.548760   LR 0.000043
0.85932732
0.85890883
0.85896885
0.85897583
0.85894960
0.85888886
0.85898173
0.85917974
0.85896796
0.85863447
0.85845822
0.85865921
0.85865611
0.85849917
0.85855657
0.85841864
0.85806674
0.85791272
INFO - Training [8][   40/  196]   Loss 0.628656   Top1 78.320312   Top5 97.431641   BatchTime 0.502320   LR 0.000039
0.85803390
0.85775596
0.85770935
0.85736901
0.85726827
0.85737377
0.85748482
0.85739964
0.85735583
0.85717827
0.85699624
0.85690892
0.85679281
0.85645348
0.85637009
0.85634124
0.85629523
0.85608292
0.85593462
INFO - Training [8][   60/  196]   Loss 0.623821   Top1 78.346354   Top5 97.447917   BatchTime 0.475334   LR 0.000035
0.85584235
0.85578346
0.85574871
0.85566986
0.85541910
0.85518271
0.85516858
0.85505497
0.85507941
0.85514438
0.85507876
0.85495901
0.85473567
0.85487056
0.85473233
0.85480344
0.85483468
0.85475677
INFO - Training [8][   80/  196]   Loss 0.625890   Top1 78.461914   Top5 97.539062   BatchTime 0.469998   LR 0.000031
0.85460436
0.85429871
0.85437745
0.85470921
0.85454208
0.85478359
0.85481656
0.85469770
0.85453433
0.85402811
0.85431141
0.85457331
0.85452509
0.85447121
0.85435593
0.85385311
0.85398453
0.85421783
0.85427737
0.85422170
0.85430419
0.85458589
INFO - Training [8][  100/  196]   Loss 0.620282   Top1 78.554688   Top5 97.652344   BatchTime 0.466452   LR 0.000027
0.85459518
0.85434705
0.85413104
0.85405797
0.85403723
0.85409158
0.85408068
0.85442293
0.85478389
0.85511464
0.85501719
0.85459137
0.85504878
0.85531455
0.85513079
0.85470980
0.85426629
0.85423952
0.85421199
INFO - Training [8][  120/  196]   Loss 0.612652   Top1 78.873698   Top5 97.718099   BatchTime 0.459335   LR 0.000023
0.85416317
0.85413033
0.85419321
0.85429895
0.85414070
0.85417801
0.85399795
0.85379112
0.85381246
0.85391057
0.85394299
0.85384804
0.85391271
0.85425669
0.85462773
0.85439008
0.85412937
0.85394388
INFO - Training [8][  140/  196]   Loss 0.609457   Top1 78.922991   Top5 97.781808   BatchTime 0.457385   LR 0.000020
0.85377812
0.85383487
0.85388231
0.85385138
0.85370570
0.85360068
0.85338169
0.85333937
0.85359716
0.85377294
0.85388160
0.85389930
0.85405284
0.85399830
0.85402375
0.85424680
0.85401165
0.85389137
0.85373253
0.85367411
0.85347253
0.85325104
INFO - Training [8][  160/  196]   Loss 0.610522   Top1 78.837891   Top5 97.792969   BatchTime 0.457875   LR 0.000017
0.85333371
0.85362107
0.85362530
0.85359889
0.85394317
0.85422957
0.85471332
0.85438055
0.85430777
0.85420114
0.85394782
0.85367519
0.85371488
0.85323572
0.85319847
0.85351527
0.85366875
0.85357440
0.85391313
0.85418963
0.85442084
0.85458755
INFO - Training [8][  180/  196]   Loss 0.607749   Top1 78.856337   Top5 97.773438   BatchTime 0.457978   LR 0.000014
0.85479790
0.85469919
0.85469139
0.85451943
0.85461313
0.85472792
0.85481542
0.85508317
0.85505217
0.85477257
0.85482281
0.85470587
0.85481846
0.85498083
0.85481834
0.85502487
INFO - ==> Top1: 78.902    Top5: 97.784    Loss: 0.605
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.85497051
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [8][   20/   40]   Loss 0.421748   Top1 85.429688   Top5 99.316406   BatchTime 0.136360
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0436)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0799)
features.2.conv.0 tensor(0.0732)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.1930)
features.3.conv.0 tensor(0.0573)
features.3.conv.3 tensor(0.0926)
features.3.conv.6 tensor(0.1074)
features.4.conv.0 tensor(0.0540)
features.4.conv.3 tensor(0.3252)
features.4.conv.6 tensor(0.1372)
features.5.conv.0 tensor(0.1312)
features.5.conv.3 tensor(0.4306)
features.5.conv.6 tensor(0.1281)
features.6.conv.0 tensor(0.0527)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0820)
features.7.conv.0 tensor(0.1078)
features.7.conv.3 tensor(0.4543)
features.7.conv.6 tensor(0.1938)
features.8.conv.0 tensor(0.4609)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.1562)
features.9.conv.0 tensor(0.4288)
features.9.conv.3 tensor(0.5602)
features.9.conv.6 tensor(0.1500)
features.10.conv.0 tensor(0.0592)
features.10.conv.3 tensor(0.1079)
features.10.conv.6 tensor(0.1054)
features.11.conv.0 tensor(0.3504)
features.11.conv.3 tensor(0.6503)
features.11.conv.6 tensor(0.7457)
features.12.conv.0 tensor(0.5916)
features.12.conv.3 tensor(0.6829)
features.12.conv.6 tensor(0.7689)
features.13.conv.0 tensor(0.2011)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.1054)
features.14.conv.0 tensor(0.4409)
features.14.conv.3 tensor(0.8273)
features.14.conv.6 tensor(0.1935)
features.15.conv.0 tensor(0.5398)
features.15.conv.3 tensor(0.9014)
features.15.conv.6 tensor(0.9479)
features.16.conv.0 tensor(0.4492)
features.16.conv.3 tensor(0.8128)
features.16.conv.6 tensor(0.0999)
conv.0 tensor(0.0625)
tensor(695461.) 2188896.0
INFO - Validation [8][   40/   40]   Loss 0.412027   Top1 85.930000   Top5 99.500000   BatchTime 0.097413
INFO - ==> Top1: 85.930    Top5: 99.500    Loss: 0.412
INFO - ==> Sparsity : 0.318
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 85.550   Top5: 99.370]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 84.360   Top5: 99.320]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.85484743
0.85465556
0.85461622
0.85473603
0.85463983
0.85451150
0.85468787
0.85477608
0.85466075
0.85438883
0.85419190
0.85426617
0.85441375
0.85426915
0.85423225
0.85399312
0.85390627
0.85420787
0.85409683
0.85425240
0.85437280
0.85414797
INFO - Training [9][   20/  196]   Loss 0.593533   Top1 79.257812   Top5 97.285156   BatchTime 0.513960   LR 0.000010
0.85414624
0.85413337
0.85394996
0.85390955
0.85427427
0.85423183
0.85404301
0.85405976
0.85415715
0.85377723
0.85373306
0.85382318
0.85401160
0.85421830
INFO - Training [9][   40/  196]   Loss 0.616935   Top1 78.457031   Top5 97.353516   BatchTime 0.471145   LR 0.000008
0.85439348
0.85449177
0.85425645
0.85421652
0.85408181
0.85414463
0.85369980
0.85355121
0.85356569
0.85341543
0.85342956
0.85340589
0.85363412
0.85332733
0.85357881
0.85356915
0.85365850
0.85367507
0.85327810
0.85334069
INFO - Training [9][   60/  196]   Loss 0.604807   Top1 79.095052   Top5 97.558594   BatchTime 0.452787   LR 0.000006
0.85362071
0.85365963
0.85382241
0.85387075
0.85370827
0.85359818
0.85323846
0.85309178
0.85301769
0.85297787
0.85298306
0.85279489
0.85277164
0.85302782
0.85332912
0.85361648
0.85385096
0.85371041
0.85352314
0.85331190
0.85318404
INFO - Training [9][   80/  196]   Loss 0.608481   Top1 78.867188   Top5 97.709961   BatchTime 0.455040   LR 0.000004
0.85317367
0.85315174
0.85295635
0.85296106
0.85288483
0.85288709
0.85288811
0.85290349
0.85301834
0.85301214
0.85306984
0.85306013
0.85324079
0.85330212
0.85342044
0.85329759
0.85325086
0.85326427
INFO - Training [9][  100/  196]   Loss 0.600106   Top1 79.218750   Top5 97.789062   BatchTime 0.448489   LR 0.000003
0.85340935
0.85311240
0.85300463
0.85296923
0.85285240
0.85275656
0.85264069
0.85265565
0.85265338
0.85282552
0.85279703
0.85270602
0.85283726
0.85282576
0.85265070
0.85264242
0.85245883
0.85242838
0.85268402
0.85276729
0.85258830
0.85257947
INFO - Training [9][  120/  196]   Loss 0.592776   Top1 79.482422   Top5 97.858073   BatchTime 0.438934   LR 0.000002
0.85243237
0.85254097
0.85280812
0.85328841
0.85328507
0.85303688
0.85274708
0.85281014
0.85284215
0.85278410
0.85265768
0.85256726
0.85262525
0.85272115
0.85257912
0.85233128
0.85231644
0.85209250
0.85206753
0.85218847
0.85206932
0.85198772
INFO - Training [9][  140/  196]   Loss 0.591060   Top1 79.567522   Top5 97.890625   BatchTime 0.441918   LR 0.000001
0.85214603
0.85218382
0.85218906
0.85221964
0.85228407
0.85215956
0.85215622
0.85223615
0.85205323
0.85187131
0.85156327
0.85178518
0.85195118
0.85225654
0.85223359
0.85211968
0.85213876
INFO - Training [9][  160/  196]   Loss 0.597290   Top1 79.306641   Top5 97.854004   BatchTime 0.444815   LR 0.000000
0.85190594
0.85195202
0.85164320
0.85125834
0.85097706
0.85073674
0.85063654
0.85075343
0.85078239
0.85057044
0.85030800
0.85034353
0.85030514
0.85054600
0.85074097
0.85068256
0.85054654
0.85013044
0.84980255
0.84946674
0.84907252
INFO - Training [9][  180/  196]   Loss 0.594514   Top1 79.396701   Top5 97.871094   BatchTime 0.447095   LR 0.000000
0.84865677
0.84813887
0.84776205
0.84753793
0.84728140
0.84681302
0.84658891
0.84621882
0.84591424
0.84554130
0.84491748
0.84367520
0.84258974
0.84181696
0.84120601
0.84069455
0.84031123
0.84003013
INFO - ==> Top1: 79.456    Top5: 97.898    Loss: 0.592
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83990157
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.421077   Top1 85.839844   Top5 99.277344   BatchTime 0.119607
INFO - Validation [9][   40/   40]   Loss 0.411815   Top1 85.920000   Top5 99.450000   BatchTime 0.087898
INFO - ==> Top1: 85.920    Top5: 99.450    Loss: 0.412
INFO - ==> Sparsity : 0.318
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.550   Top5: 99.370]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0430)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0790)
features.2.conv.0 tensor(0.0709)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1918)
features.3.conv.0 tensor(0.0579)
features.3.conv.3 tensor(0.0934)
features.3.conv.6 tensor(0.1068)
features.4.conv.0 tensor(0.0544)
features.4.conv.3 tensor(0.3241)
features.4.conv.6 tensor(0.1374)
features.5.conv.0 tensor(0.1307)
features.5.conv.3 tensor(0.4317)
features.5.conv.6 tensor(0.1276)
features.6.conv.0 tensor(0.0527)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0824)
features.7.conv.0 tensor(0.1076)
features.7.conv.3 tensor(0.4537)
features.7.conv.6 tensor(0.1934)
features.8.conv.0 tensor(0.4612)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.1558)
features.9.conv.0 tensor(0.4286)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.1502)
features.10.conv.0 tensor(0.0590)
features.10.conv.3 tensor(0.1082)
features.10.conv.6 tensor(0.1054)
features.11.conv.0 tensor(0.3502)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.7456)
features.12.conv.0 tensor(0.5900)
features.12.conv.3 tensor(0.6827)
features.12.conv.6 tensor(0.7690)
features.13.conv.0 tensor(0.2012)
features.13.conv.3 tensor(0.4919)
features.13.conv.6 tensor(0.1055)
features.14.conv.0 tensor(0.4441)
features.14.conv.3 tensor(0.8274)
features.14.conv.6 tensor(0.1933)
features.15.conv.0 tensor(0.5447)
features.15.conv.3 tensor(0.9014)
features.15.conv.6 tensor(0.9479)
features.16.conv.0 tensor(0.4493)
features.16.conv.3 tensor(0.8128)
features.16.conv.6 tensor(0.1000)
conv.0 tensor(0.0623)
tensor(696494.) 2188896.0
0.83989614
0.85307568
0.85595202
0.85998559
0.86222476
0.86604035
0.86720389
0.86717755
0.86700606
0.86699420
0.86664933
0.86689192
0.86677235
0.86660415
0.86712986
0.86728102
0.86825842
INFO - Training [10][   20/  196]   Loss 0.661484   Top1 77.031250   Top5 97.343750   BatchTime 0.503612   LR 0.000250
0.86918569
0.86989713
0.87309718
0.87361628
0.87303221
0.87284684
0.87200820
0.87133431
0.87093103
0.87018740
0.87030107
0.87041295
0.87058192
0.87139547
0.87194961
0.87245935
0.87287176
0.87332761
0.87344825
INFO - Training [10][   40/  196]   Loss 0.668617   Top1 76.630859   Top5 97.402344   BatchTime 0.464064   LR 0.000250
0.87387913
0.87439454
0.87703127
0.87772328
0.87816328
0.87833667
0.87876779
0.87896079
0.87922859
0.87936074
0.87956613
0.87942654
0.88001186
0.88805890
0.88878322
0.88948929
0.88930088
0.88915706
0.88979363
0.89015681
INFO - Training [10][   60/  196]   Loss 0.674830   Top1 76.555990   Top5 97.441406   BatchTime 0.443412   LR 0.000250
0.89036918
0.89106941
0.89154065
0.89189798
0.89225781
0.89278907
0.89310575
0.89289600
0.89326650
0.89323348
0.89308751
0.89305317
0.89285827
0.89310825
0.89300662
0.89291650
0.89275008
0.89274585
0.89278555
0.89263833
0.89258748
INFO - Training [10][   80/  196]   Loss 0.677465   Top1 76.479492   Top5 97.465820   BatchTime 0.455393   LR 0.000250
0.89277774
0.89254630
0.89254779
0.89246726
0.89233524
0.89204919
0.89209032
0.89197415
0.89173889
0.89178556
0.89146829
0.89136213
0.89114797
0.89101362
0.89068133
0.89022672
0.88978487
0.88950258
0.88915318
0.88895512
0.88886631
INFO - Training [10][  100/  196]   Loss 0.675092   Top1 76.542969   Top5 97.433594   BatchTime 0.457472   LR 0.000250
0.88838369
0.88831222
0.88808870
0.88774711
0.88758731
0.88752151
0.88757372
0.88707483
0.88700795
0.88680357
0.88697588
0.88709766
0.88695103
0.88680995
0.88688707
0.88666272
0.88638389
0.88577527
0.88589317
0.88568938
0.88525838
INFO - Training [10][  120/  196]   Loss 0.675161   Top1 76.604818   Top5 97.460938   BatchTime 0.445908   LR 0.000249
0.88504779
0.88497323
0.88472795
0.88448536
0.88446176
0.88448197
0.88458699
0.88417870
0.88413924
0.88396508
0.88385165
0.88381636
0.88326591
0.88327765
0.88291860
0.88273537
INFO - Training [10][  140/  196]   Loss 0.670755   Top1 76.863839   Top5 97.569754   BatchTime 0.450549   LR 0.000249
0.88242918
0.88251036
0.88257611
0.88253146
0.88306248
0.88273698
0.88270205
0.88257843
0.88261920
0.88260156
0.88237816
0.88221306
0.88218397
0.88249451
0.88257748
0.88252169
0.88260806
0.88214022
0.88242882
0.88247085
0.88267744
0.88294590
INFO - Training [10][  160/  196]   Loss 0.673062   Top1 76.743164   Top5 97.521973   BatchTime 0.453192   LR 0.000249
0.88329428
0.88359696
0.88403970
0.88395458
0.88445532
0.88481474
0.88473094
0.88491708
0.88501203
0.88507283
0.88514566
0.88540739
0.88489109
0.88473397
0.88454080
0.88456160
0.88435954
0.88443351
0.88426900
0.88425815
0.88415241
0.88389456
INFO - Training [10][  180/  196]   Loss 0.673877   Top1 76.681858   Top5 97.460938   BatchTime 0.453055   LR 0.000249
0.88403428
0.88391519
0.88391739
0.88420063
0.88434064
0.88412452
0.88399863
0.88391668
0.88425380
0.88519406
0.88530833
0.88530499
0.88524979
INFO - ==> Top1: 76.570    Top5: 97.454    Loss: 0.678
0.88514668
0.88484037
0.88455033
0.88449800
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [10][   20/   40]   Loss 0.514240   Top1 82.578125   Top5 99.355469   BatchTime 0.112689
INFO - Validation [10][   40/   40]   Loss 0.501352   Top1 82.650000   Top5 99.300000   BatchTime 0.084496
INFO - ==> Top1: 82.650    Top5: 99.300    Loss: 0.501
INFO - ==> Sparsity : 0.304
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.550   Top5: 99.370]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0781)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1973)
features.3.conv.0 tensor(0.0663)
features.3.conv.3 tensor(0.0941)
features.3.conv.6 tensor(0.1126)
features.4.conv.0 tensor(0.0441)
features.4.conv.3 tensor(0.3258)
features.4.conv.6 tensor(0.1553)
features.5.conv.0 tensor(0.1310)
features.5.conv.3 tensor(0.4236)
features.5.conv.6 tensor(0.1351)
features.6.conv.0 tensor(0.0544)
features.6.conv.3 tensor(0.0637)
features.6.conv.6 tensor(0.0786)
features.7.conv.0 tensor(0.1042)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.2078)
features.8.conv.0 tensor(0.4481)
features.8.conv.3 tensor(0.5440)
features.8.conv.6 tensor(0.1568)
features.9.conv.0 tensor(0.3007)
features.9.conv.3 tensor(0.5454)
features.9.conv.6 tensor(0.1562)
features.10.conv.0 tensor(0.0617)
features.10.conv.3 tensor(0.1091)
features.10.conv.6 tensor(0.0619)
features.11.conv.0 tensor(0.3519)
features.11.conv.3 tensor(0.6501)
features.11.conv.6 tensor(0.7247)
features.12.conv.0 tensor(0.4467)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.4607)
features.13.conv.0 tensor(0.2028)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.1081)
features.14.conv.0 tensor(0.4012)
features.14.conv.3 tensor(0.8262)
features.14.conv.6 tensor(0.1739)
features.15.conv.0 tensor(0.5957)
features.15.conv.3 tensor(0.9035)
features.15.conv.6 tensor(0.9538)
features.16.conv.0 tensor(0.4434)
features.16.conv.3 tensor(0.8139)
features.16.conv.6 tensor(0.1072)
conv.0 tensor(0.0618)
tensor(666447.) 2188896.0
0.88405502
0.88357848
0.88306093
0.88286930
0.88269001
0.88236731
0.88219780
0.88244289
0.88209587
0.88207662
0.88176960
0.88118720
0.88224900
0.88224071
0.88228840
INFO - Training [11][   20/  196]   Loss 0.710347   Top1 75.117188   Top5 96.855469   BatchTime 0.508284   LR 0.000248
0.88236243
0.88237876
0.88242579
0.88256741
0.88244671
0.88368529
0.88898295
0.89720219
0.89831132
0.89789176
0.89782703
0.89792579
0.89748830
0.89775056
0.89752245
0.89750952
0.89732164
0.89736891
0.89734727
0.89750808
0.89729434
0.89739048
0.89764339
INFO - Training [11][   40/  196]   Loss 0.700384   Top1 75.683594   Top5 97.138672   BatchTime 0.463417   LR 0.000248
0.89759290
0.89758545
0.89793658
0.89837772
0.89775008
0.89792955
0.89811152
0.89806896
0.89797419
0.89792204
0.89791358
0.89808410
0.89794564
0.89763868
0.89776951
0.89781880
0.89759171
0.89752662
0.89750212
0.89747864
INFO - Training [11][   60/  196]   Loss 0.692517   Top1 75.944010   Top5 97.141927   BatchTime 0.441980   LR 0.000247
0.89730275
0.89741820
0.89748520
0.89757782
0.89752656
0.89741665
0.89717823
0.89728874
0.89710003
0.89717180
0.89736587
0.89764416
0.89721185
0.89659935
0.89605147
0.89681059
0.89739722
0.89772034
0.89758021
0.89764333
INFO - Training [11][   80/  196]   Loss 0.690220   Top1 75.991211   Top5 97.270508   BatchTime 0.435810   LR 0.000247
0.89764273
0.89765954
0.89762652
0.89739496
0.89743841
0.89728475
0.89737767
0.89726639
0.89700794
0.89698905
0.89695221
0.89691210
0.89690989
0.89715934
0.89688724
0.89709413
0.89688146
0.89677912
INFO - Training [11][  100/  196]   Loss 0.682409   Top1 76.218750   Top5 97.312500   BatchTime 0.441110   LR 0.000247
0.89717340
0.89698869
0.89673138
0.89671046
0.89680630
0.89694846
0.89675117
0.89678407
0.89699638
0.89674020
0.89696419
0.89729899
0.89721352
0.89731610
0.89690733
0.89615244
0.89650255
0.89689875
0.89671671
0.89673841
0.89667898
0.89657170
0.89637619
INFO - Training [11][  120/  196]   Loss 0.675004   Top1 76.539714   Top5 97.457682   BatchTime 0.438895   LR 0.000246
0.89610326
0.89614427
0.89588636
0.89595783
0.89605248
0.89625978
0.89589638
0.89580530
0.89586675
0.89585763
0.89575386
0.89564478
0.89574409
0.89560825
0.89579380
0.89580935
0.89565337
INFO - Training [11][  140/  196]   Loss 0.670644   Top1 76.768973   Top5 97.511161   BatchTime 0.441043   LR 0.000246
0.89561558
0.89562160
0.89567626
0.89578915
0.89575386
0.89581603
0.89601225
0.89659983
0.89707059
0.89740026
0.89766490
0.89767313
0.89752102
0.89777112
0.89765471
0.89786321
0.89785385
0.89789838
0.89804810
0.89743549
0.89711398
0.89722282
INFO - Training [11][  160/  196]   Loss 0.673112   Top1 76.601562   Top5 97.492676   BatchTime 0.444338   LR 0.000245
0.89728481
0.89736360
0.89729285
0.89740270
0.89749223
0.89749658
0.89718956
0.89736480
0.89715618
0.89712769
0.89687186
0.89693022
0.89700818
0.89689124
0.89707398
0.89698559
0.89694881
INFO - Training [11][  180/  196]   Loss 0.672718   Top1 76.655816   Top5 97.519531   BatchTime 0.445824   LR 0.000244
0.89727741
0.89735019
0.89708114
0.89691353
0.89678878
0.89654851
0.89666492
0.89680755
0.89673358
0.89651132
0.89668047
0.89636493
0.89617521
0.89662242
0.89609176
0.89625907
0.89633012
0.89639562
INFO - ==> Top1: 76.666    Top5: 97.506    Loss: 0.672
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.89678425
0.89649892
0.89655888
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.542410   Top1 82.070312   Top5 98.984375   BatchTime 0.116545
INFO - Validation [11][   40/   40]   Loss 0.539754   Top1 81.950000   Top5 99.180000   BatchTime 0.086581
INFO - ==> Top1: 81.950    Top5: 99.180    Loss: 0.540
INFO - ==> Sparsity : 0.293
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.550   Top5: 99.370]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.1445)
features.1.conv.0 tensor(0.0430)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0812)
features.2.conv.0 tensor(0.0541)
features.2.conv.3 tensor(0.3403)
features.2.conv.6 tensor(0.2002)
features.3.conv.0 tensor(0.0564)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.1152)
features.4.conv.0 tensor(0.0444)
features.4.conv.3 tensor(0.3177)
features.4.conv.6 tensor(0.1546)
features.5.conv.0 tensor(0.1305)
features.5.conv.3 tensor(0.4277)
features.5.conv.6 tensor(0.1309)
features.6.conv.0 tensor(0.0571)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0807)
features.7.conv.0 tensor(0.0944)
features.7.conv.3 tensor(0.4525)
features.7.conv.6 tensor(0.1970)
features.8.conv.0 tensor(0.4411)
features.8.conv.3 tensor(0.5466)
features.8.conv.6 tensor(0.1653)
features.9.conv.0 tensor(0.3013)
features.9.conv.3 tensor(0.5454)
features.9.conv.6 tensor(0.1556)
features.10.conv.0 tensor(0.0610)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0655)
features.11.conv.0 tensor(0.3488)
features.11.conv.3 tensor(0.6528)
features.11.conv.6 tensor(0.7150)
features.12.conv.0 tensor(0.4452)
features.12.conv.3 tensor(0.6671)
features.12.conv.6 tensor(0.4785)
features.13.conv.0 tensor(0.2042)
features.13.conv.3 tensor(0.4934)
features.13.conv.6 tensor(0.1084)
features.14.conv.0 tensor(0.4037)
features.14.conv.3 tensor(0.8255)
features.14.conv.6 tensor(0.1796)
features.15.conv.0 tensor(0.4005)
features.15.conv.3 tensor(0.9045)
features.15.conv.6 tensor(0.9540)
features.16.conv.0 tensor(0.4405)
features.16.conv.3 tensor(0.8122)
features.16.conv.6 tensor(0.1149)
conv.0 tensor(0.0649)
tensor(640906.) 2188896.0
0.89618099
0.89612484
0.89593297
0.89640003
0.89574707
0.89593995
0.89579940
0.89550501
0.89547074
0.89538288
0.89528519
0.89547938
0.89561766
0.89562792
0.89546305
0.89514369
INFO - Training [12][   20/  196]   Loss 0.692576   Top1 76.152344   Top5 97.050781   BatchTime 0.493343   LR 0.000243
0.89500290
0.89499050
0.89481819
0.89499843
0.89502156
0.89485264
0.89477503
0.89482456
0.89481109
0.89470518
0.89474726
0.89442903
0.89454257
0.89476323
0.89467645
0.89400899
0.89421690
0.89405733
0.89369130
0.89414656
0.89418834
0.89415419
INFO - Training [12][   40/  196]   Loss 0.681886   Top1 76.621094   Top5 97.392578   BatchTime 0.476386   LR 0.000243
0.89421743
0.89439917
0.89428890
0.89432102
0.89406276
0.89380139
0.89372158
0.89381033
0.89372027
0.89370084
0.89336419
0.89331448
0.89318675
0.89293605
0.89299536
0.89314830
0.89310557
INFO - Training [12][   60/  196]   Loss 0.679653   Top1 76.555990   Top5 97.571615   BatchTime 0.468525   LR 0.000242
0.89302987
0.89282680
0.89266676
0.89244545
0.89184850
0.89216757
0.89277458
0.89300615
0.89314890
0.89390820
0.89369315
0.89356476
0.89324832
0.89336383
0.89310974
0.89314324
0.89294153
0.89290088
0.89294803
0.89277393
0.89289302
0.89302206
0.89303547
0.89298707
INFO - Training [12][   80/  196]   Loss 0.677997   Top1 76.557617   Top5 97.583008   BatchTime 0.458253   LR 0.000241
0.89318597
0.89307982
0.89328086
0.89307028
0.89262122
0.89270645
0.89262486
0.89272887
0.89255673
0.89240640
0.89227229
0.89199954
0.89183402
0.89170402
0.89154220
0.89137441
0.89119202
INFO - Training [12][  100/  196]   Loss 0.669648   Top1 76.746094   Top5 97.593750   BatchTime 0.458327   LR 0.000240
0.89127964
0.89136571
0.89125913
0.89108866
0.89037824
0.89024705
0.88993937
0.88932687
0.88907778
0.88909441
0.88842118
0.88780403
0.88805693
0.88744158
0.88646346
0.88597029
0.88549286
0.88531637
0.88483489
0.88460708
0.88549286
0.88539720
0.88528061
INFO - Training [12][  120/  196]   Loss 0.661080   Top1 77.070312   Top5 97.659505   BatchTime 0.452778   LR 0.000240
0.88541859
0.88512796
0.88524032
0.88536733
0.88587523
0.88606465
0.88614249
0.88645935
0.88678640
0.88695759
0.88744289
0.88838351
0.89004439
0.89138222
0.89551896
0.89620793
0.89605474
0.89592743
0.89594001
0.89580536
INFO - Training [12][  140/  196]   Loss 0.658135   Top1 77.248884   Top5 97.714844   BatchTime 0.446492   LR 0.000239
0.89584762
0.89583319
0.89581567
0.89609897
0.89626569
0.89611953
0.89598238
0.89598721
0.89606553
0.89613307
0.89619339
0.89625221
0.89611036
0.89594138
0.89607656
0.89590490
0.89589816
INFO - Training [12][  160/  196]   Loss 0.661284   Top1 77.126465   Top5 97.668457   BatchTime 0.447842   LR 0.000238
0.89605016
0.89598423
0.89595103
0.89564818
0.89553326
0.89530575
0.89522898
0.89497375
0.89459711
0.89415467
0.89481694
0.89509642
0.89520633
0.89546406
0.89593250
0.89553553
0.89542276
0.89501816
0.89502633
0.89487374
0.89445770
0.89474916
INFO - Training [12][  180/  196]   Loss 0.659502   Top1 77.163628   Top5 97.632378   BatchTime 0.448722   LR 0.000237
0.89476281
0.89446718
0.89453518
0.89455605
0.89454859
0.89441693
0.89418614
0.89454442
0.89466274
0.89477706
0.89483696
0.89464378
0.89436698
0.89454079
INFO - ==> Top1: 77.188    Top5: 97.670    Loss: 0.658
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.89438003
0.89420116
0.89436024
0.89442170
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.483217   Top1 83.925781   Top5 99.179688   BatchTime 0.108971
INFO - Validation [12][   40/   40]   Loss 0.479403   Top1 83.760000   Top5 99.330000   BatchTime 0.105195
INFO - ==> Top1: 83.760    Top5: 99.330    Loss: 0.479
INFO - ==> Sparsity : 0.295
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.550   Top5: 99.370]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0825)
features.2.conv.0 tensor(0.0446)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1962)
features.3.conv.0 tensor(0.0651)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1152)
features.4.conv.0 tensor(0.0557)
features.4.conv.3 tensor(0.3212)
features.4.conv.6 tensor(0.1523)
features.5.conv.0 tensor(0.1309)
features.5.conv.3 tensor(0.4282)
features.5.conv.6 tensor(0.1257)
features.6.conv.0 tensor(0.0601)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0811)
features.7.conv.0 tensor(0.0960)
features.7.conv.3 tensor(0.4531)
features.7.conv.6 tensor(0.1913)
features.8.conv.0 tensor(0.4399)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.1588)
features.9.conv.0 tensor(0.3031)
features.9.conv.3 tensor(0.5443)
features.9.conv.6 tensor(0.1687)
features.10.conv.0 tensor(0.0588)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0643)
features.11.conv.0 tensor(0.3514)
features.11.conv.3 tensor(0.6495)
features.11.conv.6 tensor(0.7111)
features.12.conv.0 tensor(0.4455)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.5133)
features.13.conv.0 tensor(0.2051)
features.13.conv.3 tensor(0.4942)
features.13.conv.6 tensor(0.1070)
features.14.conv.0 tensor(0.4029)
features.14.conv.3 tensor(0.8247)
features.14.conv.6 tensor(0.1943)
features.15.conv.0 tensor(0.3946)
features.15.conv.3 tensor(0.9049)
features.15.conv.6 tensor(0.9567)
features.16.conv.0 tensor(0.4380)
features.16.conv.3 tensor(0.8133)
features.16.conv.6 tensor(0.1199)
conv.0 tensor(0.0665)
tensor(646152.) 2188896.0
0.89420116
0.89451754
0.89431584
0.89406163
0.89395463
0.89397234
0.89405829
0.89394152
0.89382714
0.89381683
0.89373219
0.89309269
0.89363885
0.89309442
0.89368325
0.89385200
0.89364600
INFO - Training [13][   20/  196]   Loss 0.676676   Top1 76.425781   Top5 96.835938   BatchTime 0.521901   LR 0.000235
0.89303637
0.89145893
0.88987446
0.88935435
0.88998830
0.89062041
0.89111513
0.89355826
0.89512902
0.89519888
0.89540929
0.89498538
0.89478737
0.89538956
0.89570576
0.89554399
0.89538854
0.89556128
0.89540488
0.89536166
0.89538145
0.89526606
INFO - Training [13][   40/  196]   Loss 0.669506   Top1 76.855469   Top5 97.294922   BatchTime 0.497154   LR 0.000235
0.89514184
0.89536083
0.89508379
0.89544296
0.89533383
0.89489704
0.89505219
0.89506847
0.89479411
0.89490545
0.89485556
0.89521348
0.89519316
0.89543420
0.89538932
0.89548737
INFO - Training [13][   60/  196]   Loss 0.652823   Top1 77.337240   Top5 97.506510   BatchTime 0.482096   LR 0.000234
0.89556390
0.89543581
0.89573276
0.89563316
0.89556617
0.89521915
0.89458185
0.89524740
0.89589262
0.89596009
0.89578241
0.89604646
0.89606786
0.89625812
0.89635330
0.89619380
0.89634943
0.89623004
0.89608890
0.89629763
INFO - Training [13][   80/  196]   Loss 0.644737   Top1 77.763672   Top5 97.602539   BatchTime 0.468049   LR 0.000233
0.89623612
0.89630610
0.89635515
0.89631844
0.89627463
0.89593822
0.89578342
0.89606524
0.89583677
0.89542055
0.89564753
0.89555717
0.89534700
0.89529496
0.89526844
0.89490199
0.89523399
0.89557892
0.89521968
0.89526260
0.89504838
0.89445364
INFO - Training [13][  100/  196]   Loss 0.638664   Top1 77.808594   Top5 97.660156   BatchTime 0.465845   LR 0.000232
0.89501166
0.89526355
0.89502335
0.89502823
0.89474261
0.89460546
0.89440256
0.89408380
0.89429748
0.89440030
0.89429444
0.89395982
0.89362550
0.89359349
0.89324725
0.89271903
0.89201444
0.89113820
INFO - Training [13][  120/  196]   Loss 0.633939   Top1 78.011068   Top5 97.740885   BatchTime 0.459555   LR 0.000230
0.88952637
0.88651884
0.88429695
0.88335711
0.88306081
0.88258880
0.88313335
0.88343388
0.88391757
0.88472992
0.88556510
0.88626838
0.88718027
0.88838762
0.89035642
0.89453959
0.90275371
0.90651047
0.90696865
0.90802509
0.90880942
INFO - Training [13][  140/  196]   Loss 0.632444   Top1 77.996652   Top5 97.773438   BatchTime 0.450607   LR 0.000229
0.90936548
0.90981859
0.91013008
0.91048104
0.91064107
0.91044432
0.91075999
0.91078657
0.91113687
0.91117972
0.91129941
0.91157371
0.91127294
0.91123748
0.91100353
0.91101557
0.91086173
0.91093582
0.91104722
0.91060448
0.91061044
0.91075420
INFO - Training [13][  160/  196]   Loss 0.632023   Top1 78.051758   Top5 97.768555   BatchTime 0.450770   LR 0.000228
0.91072983
0.91061246
0.91058791
0.91063154
0.91042733
0.91029334
0.91028756
0.91020620
0.91014212
0.90993410
0.91027194
0.91027308
0.91018444
0.91043293
0.91020405
0.91001582
0.90986079
0.90995371
INFO - Training [13][  180/  196]   Loss 0.631800   Top1 78.038194   Top5 97.708333   BatchTime 0.451733   LR 0.000227
0.90998566
0.90969938
0.90937310
0.90946299
0.90943772
0.90933281
0.90929806
0.90940106
0.90932131
0.90929550
0.90910852
0.90897137
0.90913719
0.90909451
0.90864187
0.90790635
0.90782142
INFO - ==> Top1: 78.026    Top5: 97.714    Loss: 0.632
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.90780437
0.90839845
0.90972906
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.452013   Top1 84.394531   Top5 99.140625   BatchTime 0.125276
INFO - Validation [13][   40/   40]   Loss 0.437059   Top1 84.990000   Top5 99.390000   BatchTime 0.091725
INFO - ==> Top1: 84.990    Top5: 99.390    Loss: 0.437
INFO - ==> Sparsity : 0.288
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.550   Top5: 99.370]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0833)
features.2.conv.0 tensor(0.0425)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1930)
features.3.conv.0 tensor(0.0608)
features.3.conv.3 tensor(0.0903)
features.3.conv.6 tensor(0.1155)
features.4.conv.0 tensor(0.0490)
features.4.conv.3 tensor(0.3189)
features.4.conv.6 tensor(0.1566)
features.5.conv.0 tensor(0.1309)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.1242)
features.6.conv.0 tensor(0.0560)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0819)
features.7.conv.0 tensor(0.1030)
features.7.conv.3 tensor(0.4563)
features.7.conv.6 tensor(0.2010)
features.8.conv.0 tensor(0.3732)
features.8.conv.3 tensor(0.5370)
features.8.conv.6 tensor(0.1593)
features.9.conv.0 tensor(0.3014)
features.9.conv.3 tensor(0.5446)
features.9.conv.6 tensor(0.1661)
features.10.conv.0 tensor(0.0602)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0653)
features.11.conv.0 tensor(0.3492)
features.11.conv.3 tensor(0.6508)
features.11.conv.6 tensor(0.3512)
features.12.conv.0 tensor(0.4445)
features.12.conv.3 tensor(0.6676)
features.12.conv.6 tensor(0.5024)
features.13.conv.0 tensor(0.2062)
features.13.conv.3 tensor(0.4929)
features.13.conv.6 tensor(0.1082)
features.14.conv.0 tensor(0.4136)
features.14.conv.3 tensor(0.8251)
features.14.conv.6 tensor(0.1842)
features.15.conv.0 tensor(0.4096)
features.15.conv.3 tensor(0.9054)
features.15.conv.6 tensor(0.9582)
features.16.conv.0 tensor(0.4390)
features.16.conv.3 tensor(0.8115)
features.16.conv.6 tensor(0.1302)
conv.0 tensor(0.0677)
tensor(630738.) 2188896.0
0.90959495
0.90908408
0.90937048
0.90960073
0.90951717
0.90931726
0.90919828
0.90918529
0.90901238
0.90884167
0.90871328
0.90839767
0.90844983
0.90845674
0.90838891
INFO - Training [14][   20/  196]   Loss 0.637591   Top1 77.382812   Top5 97.128906   BatchTime 0.541247   LR 0.000225
0.90822369
0.90827709
0.90781838
0.90743411
0.90716553
0.90674108
0.90669686
0.90677679
0.90673870
0.90641004
0.90618795
0.90559804
0.90476286
0.90412349
0.90393281
0.90365785
0.90328288
0.90250748
0.90192205
0.90087861
0.89971745
0.89880490
INFO - Training [14][   40/  196]   Loss 0.629865   Top1 77.656250   Top5 97.509766   BatchTime 0.505541   LR 0.000224
0.89791983
0.89717436
0.89627683
0.89607900
0.89578396
0.89483577
0.89376706
0.89315730
0.89279646
0.89235520
0.89192921
0.89198732
0.89154625
0.89131367
0.89110816
0.89114666
0.89102769
0.89084184
INFO - Training [14][   60/  196]   Loss 0.629810   Top1 77.766927   Top5 97.643229   BatchTime 0.478318   LR 0.000223
0.89073652
0.89035583
0.88996154
0.88966519
0.88918811
0.88882118
0.88867325
0.88875312
0.88867718
0.88853222
0.88860196
0.88796401
0.88779509
0.88751101
0.88696009
0.88625956
0.88579059
0.88508379
0.88474256
0.88455814
0.88382900
INFO - Training [14][   80/  196]   Loss 0.619774   Top1 78.232422   Top5 97.783203   BatchTime 0.458812   LR 0.000221
0.88338047
0.88318688
0.88340962
0.88324124
0.88303542
0.88278514
0.88256311
0.88268644
0.88243151
0.88241494
0.88200355
0.88177812
0.88197333
0.88184756
0.88270372
0.88253361
0.88259095
0.88246202
0.88212651
0.88231629
0.88246655
INFO - Training [14][  100/  196]   Loss 0.617780   Top1 78.355469   Top5 97.808594   BatchTime 0.459700   LR 0.000220
0.88212001
0.88166887
0.88082701
0.88037896
0.88009804
0.88038385
0.88137400
0.88442665
0.88690543
0.89368474
0.89874279
0.89860415
0.89836568
0.89789838
0.89760160
0.89796978
0.89822024
0.89835197
0.89869428
0.89853936
0.89845091
0.89872605
INFO - Training [14][  120/  196]   Loss 0.612767   Top1 78.512370   Top5 97.949219   BatchTime 0.458995   LR 0.000219
0.89840358
0.89803666
0.89829612
0.89875287
0.89867640
0.89893347
0.89919502
0.89988214
0.90035558
0.90088725
0.90180695
0.90297496
0.90377718
0.90424836
0.90504569
0.90538466
0.90621734
0.90697247
0.90704507
0.90690768
INFO - Training [14][  140/  196]   Loss 0.613490   Top1 78.537946   Top5 97.991071   BatchTime 0.451695   LR 0.000217
0.90687877
0.90703058
0.90669000
0.90657985
0.90634865
0.90610069
0.90619713
0.90614176
0.90619886
0.90607417
0.90590882
0.90588868
0.90575218
0.90550762
0.90508819
0.90506095
0.90492314
0.90489525
0.90460211
INFO - Training [14][  160/  196]   Loss 0.614320   Top1 78.566895   Top5 97.958984   BatchTime 0.449037   LR 0.000216
0.90456200
0.90443975
0.90384996
0.90359455
0.90331274
0.90314609
0.90295184
0.90303087
0.90307873
0.90259641
0.90252858
0.90221250
0.90205538
0.90132821
0.90116596
0.90171623
0.90155971
0.90144670
INFO - Training [14][  180/  196]   Loss 0.616044   Top1 78.502604   Top5 97.886285   BatchTime 0.449172   LR 0.000215
0.90156597
0.90144318
0.90116298
0.90130866
0.90084535
0.90049320
0.90020102
0.89968586
0.90025038
0.89998394
0.89970636
0.89958227
0.89946032
0.89933711
0.89889169
0.89860231
0.89853942
0.89939296
0.89966989
0.89948595
INFO - ==> Top1: 78.568    Top5: 97.862    Loss: 0.616
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [14][   20/   40]   Loss 0.433461   Top1 85.605469   Top5 99.414062   BatchTime 0.119837
INFO - Validation [14][   40/   40]   Loss 0.423750   Top1 85.690000   Top5 99.540000   BatchTime 0.089688
INFO - ==> Top1: 85.690    Top5: 99.540    Loss: 0.424
INFO - ==> Sparsity : 0.301
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 85.690   Top5: 99.540]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.1406)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0838)
features.2.conv.0 tensor(0.0353)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1973)
features.3.conv.0 tensor(0.0642)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1157)
features.4.conv.0 tensor(0.0522)
features.4.conv.3 tensor(0.3142)
features.4.conv.6 tensor(0.1532)
features.5.conv.0 tensor(0.1317)
features.5.conv.3 tensor(0.4248)
features.5.conv.6 tensor(0.1294)
features.6.conv.0 tensor(0.0549)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0808)
features.7.conv.0 tensor(0.0898)
features.7.conv.3 tensor(0.4534)
features.7.conv.6 tensor(0.2001)
features.8.conv.0 tensor(0.3742)
features.8.conv.3 tensor(0.5388)
features.8.conv.6 tensor(0.1696)
features.9.conv.0 tensor(0.3001)
features.9.conv.3 tensor(0.5457)
features.9.conv.6 tensor(0.1785)
features.10.conv.0 tensor(0.0594)
features.10.conv.3 tensor(0.1085)
features.10.conv.6 tensor(0.0650)
features.11.conv.0 tensor(0.3474)
features.11.conv.3 tensor(0.6503)
features.11.conv.6 tensor(0.4187)
features.12.conv.0 tensor(0.4461)
features.12.conv.3 tensor(0.6676)
features.12.conv.6 tensor(0.4934)
features.13.conv.0 tensor(0.2058)
features.13.conv.3 tensor(0.4936)
features.13.conv.6 tensor(0.1080)
features.14.conv.0 tensor(0.4395)
features.14.conv.3 tensor(0.8247)
features.14.conv.6 tensor(0.2005)
features.15.conv.0 tensor(0.5185)
features.15.conv.3 tensor(0.9057)
features.15.conv.6 tensor(0.9584)
features.16.conv.0 tensor(0.4370)
features.16.conv.3 tensor(0.8113)
features.16.conv.6 tensor(0.1318)
conv.0 tensor(0.0684)
tensor(657833.) 2188896.0
0.89980197
0.89972734
0.89977634
0.89962614
0.89962441
0.89958459
0.89945155
0.89928478
0.89909035
0.89861792
0.89832300
0.89811665
0.89801586
0.89799345
0.89759058
0.89752477
0.89747632
INFO - Training [15][   20/  196]   Loss 0.611148   Top1 78.808594   Top5 97.656250   BatchTime 0.531293   LR 0.000212
0.89769286
0.89749134
0.89747149
0.89703697
0.89706242
0.89658087
0.89643794
0.89634264
0.89625371
0.89605510
0.89575577
0.89553553
0.89578146
0.89535403
0.89483875
0.89430714
0.89400959
0.89344561
0.89307058
0.89270687
0.89264405
0.89266193
INFO - Training [15][   40/  196]   Loss 0.615226   Top1 78.515625   Top5 97.832031   BatchTime 0.499233   LR 0.000211
0.89262080
0.89263421
0.89250398
0.89213228
0.89169413
0.89103150
0.89063239
0.89022160
0.88989449
0.88961077
0.88916117
0.88860023
0.88836575
0.88770902
0.88750029
0.88744074
0.88696796
INFO - Training [15][   60/  196]   Loss 0.605550   Top1 78.997396   Top5 97.910156   BatchTime 0.483076   LR 0.000209
0.88641846
0.88557297
0.88582569
0.88635129
0.88620365
0.88522774
0.88440675
0.88335562
0.88232589
0.88152725
0.88073599
0.88008654
0.87969869
0.87873626
0.87788349
0.87667948
0.87578356
0.87496334
0.87448907
INFO - Training [15][   80/  196]   Loss 0.608347   Top1 78.916016   Top5 97.915039   BatchTime 0.468613   LR 0.000208
0.87412924
0.87379676
0.87348992
0.87296587
0.87221783
0.87189317
0.87134212
0.87095195
0.87043738
0.86996675
0.86915588
0.86853182
0.86806536
0.86793047
0.86711794
0.86659497
0.86630082
0.86590195
0.86580992
0.86551726
0.86561722
0.86512476
INFO - Training [15][  100/  196]   Loss 0.600407   Top1 79.144531   Top5 97.988281   BatchTime 0.469826   LR 0.000206
0.86497074
0.86492360
0.86477339
0.86478716
0.86475229
0.86525059
0.86557502
0.86558473
0.86589938
0.86636978
0.86750066
0.86905110
0.87160379
0.87871981
0.88780051
0.88768142
0.88768804
0.88776261
0.88778383
0.88783127
0.88823676
0.88803619
INFO - Training [15][  120/  196]   Loss 0.597104   Top1 79.277344   Top5 98.098958   BatchTime 0.466549   LR 0.000205
0.88814265
0.88828427
0.88874400
0.88907391
0.88901800
0.88885093
0.88913208
0.88909918
0.88914567
0.88896936
0.88911086
0.88913453
0.88929522
0.88941979
0.88955623
0.88945347
0.88963604
0.88936168
INFO - Training [15][  140/  196]   Loss 0.595757   Top1 79.428013   Top5 98.116629   BatchTime 0.462274   LR 0.000203
0.88923556
0.88912332
0.88922727
0.88928682
0.88922727
0.88919353
0.88950878
0.88919020
0.88885176
0.88867533
0.88915163
0.88932502
0.88923520
0.88893002
0.88890254
0.88885212
0.88875097
0.88879955
0.88883847
0.88874811
INFO - Training [15][  160/  196]   Loss 0.599289   Top1 79.291992   Top5 98.083496   BatchTime 0.454536   LR 0.000201
0.88882917
0.88883859
0.88853532
0.88848299
0.88845581
0.88864505
0.88815993
0.88781095
0.88725817
0.88704991
0.88679075
0.88672179
0.88704777
0.88669038
0.88709921
0.88724589
0.88767558
0.88733023
0.88692296
0.88682872
INFO - Training [15][  180/  196]   Loss 0.599916   Top1 79.199219   Top5 98.036024   BatchTime 0.449818   LR 0.000200
0.88716239
0.88765693
0.88745701
0.88748515
0.88771045
0.88766032
0.88762361
0.88764346
0.88780338
0.88763374
0.88794261
0.88773859
0.88736570
0.88742012
0.88751262
0.88753819
0.88739383
INFO - ==> Top1: 79.212    Top5: 98.014    Loss: 0.600
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.88735425
0.88742268
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.431292   Top1 85.195312   Top5 99.531250   BatchTime 0.118873
INFO - Validation [15][   40/   40]   Loss 0.425619   Top1 85.250000   Top5 99.540000   BatchTime 0.088118
INFO - ==> Top1: 85.250    Top5: 99.540    Loss: 0.426
INFO - ==> Sparsity : 0.314
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 85.690   Top5: 99.540]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1445)
features.1.conv.0 tensor(0.0417)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0838)
features.2.conv.0 tensor(0.0411)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.1962)
features.3.conv.0 tensor(0.0657)
features.3.conv.3 tensor(0.0841)
features.3.conv.6 tensor(0.1211)
features.4.conv.0 tensor(0.0519)
features.4.conv.3 tensor(0.3113)
features.4.conv.6 tensor(0.1536)
features.5.conv.0 tensor(0.1318)
features.5.conv.3 tensor(0.4259)
features.5.conv.6 tensor(0.1258)
features.6.conv.0 tensor(0.0518)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0774)
features.7.conv.0 tensor(0.0883)
features.7.conv.3 tensor(0.4546)
features.7.conv.6 tensor(0.2024)
features.8.conv.0 tensor(0.3709)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.1658)
features.9.conv.0 tensor(0.3014)
features.9.conv.3 tensor(0.5475)
features.9.conv.6 tensor(0.1765)
features.10.conv.0 tensor(0.0597)
features.10.conv.3 tensor(0.1076)
features.10.conv.6 tensor(0.0642)
features.11.conv.0 tensor(0.3484)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.4719)
features.12.conv.0 tensor(0.4499)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.5150)
features.13.conv.0 tensor(0.2055)
features.13.conv.3 tensor(0.4934)
features.13.conv.6 tensor(0.1090)
features.14.conv.0 tensor(0.4658)
features.14.conv.3 tensor(0.8245)
features.14.conv.6 tensor(0.1924)
features.15.conv.0 tensor(0.6449)
features.15.conv.3 tensor(0.9060)
features.15.conv.6 tensor(0.9570)
features.16.conv.0 tensor(0.4380)
features.16.conv.3 tensor(0.8115)
features.16.conv.6 tensor(0.1370)
conv.0 tensor(0.0700)
tensor(686488.) 2188896.0
0.88759273
0.88773584
0.88779980
0.88760579
0.88737935
0.88719571
0.88714015
0.88706005
0.88719916
0.88716292
0.88722986
0.88695335
0.88673258
0.88664347
0.88640678
0.88655424
INFO - Training [16][   20/  196]   Loss 0.615906   Top1 79.023438   Top5 97.363281   BatchTime 0.541012   LR 0.000197
0.88662583
0.88670629
0.88669920
0.88677484
0.88693386
0.88707161
0.88713479
0.88691950
0.88676143
0.88671070
0.88661176
0.88675302
0.88669908
0.88662392
0.88625240
0.88628346
0.88603795
0.88584775
0.88596278
0.88586628
0.88616335
0.88626796
INFO - Training [16][   40/  196]   Loss 0.628267   Top1 78.242188   Top5 97.529297   BatchTime 0.500609   LR 0.000195
0.88596970
0.88537967
0.88571161
0.88563764
0.88583839
0.88595051
0.88623524
0.88600403
0.88573343
0.88580245
0.88537085
0.88533032
0.88559276
0.88523716
0.88517863
0.88529748
0.88523513
INFO - Training [16][   60/  196]   Loss 0.617269   Top1 78.632812   Top5 97.630208   BatchTime 0.481082   LR 0.000194
0.88519007
0.88509732
0.88482779
0.88468772
0.88450497
0.88435709
0.88369894
0.88329673
0.88217503
0.88143194
0.88078499
0.87970424
0.87891203
0.87823844
0.87751263
0.87724966
0.87672585
0.87635696
0.87615192
0.87581038
0.87524694
0.87481540
0.87420207
0.87358236
0.87283754
INFO - Training [16][   80/  196]   Loss 0.613447   Top1 78.681641   Top5 97.773438   BatchTime 0.466007   LR 0.000192
0.87233222
0.87185287
0.87147373
0.87102622
0.87079579
0.87053663
0.87050641
0.87026459
0.87079626
0.87091678
0.87074244
0.87061357
0.87053645
0.87059277
0.87062472
0.87041813
0.87018824
INFO - Training [16][  100/  196]   Loss 0.605386   Top1 78.941406   Top5 97.855469   BatchTime 0.466034   LR 0.000190
0.86993986
0.86968225
0.86938977
0.86923736
0.86914766
0.86934000
0.87043166
0.87171787
0.87275875
0.87360257
0.87447506
0.87516111
0.87647492
0.87976372
0.89423859
0.89450556
0.89475071
0.89448249
0.89446700
0.89414221
0.89402735
INFO - Training [16][  120/  196]   Loss 0.598868   Top1 79.195964   Top5 97.955729   BatchTime 0.465054   LR 0.000188
0.89398319
0.89403212
0.89388376
0.89368254
0.89375216
0.89399981
0.89362580
0.89349961
0.89329392
0.89319247
0.89277095
0.89262885
0.89264590
0.89243191
0.89246064
0.89215916
0.89205033
0.89185196
0.89172375
0.89148295
0.89152223
0.89103472
INFO - Training [16][  140/  196]   Loss 0.594805   Top1 79.375000   Top5 98.052455   BatchTime 0.466402   LR 0.000187
0.89082628
0.89087182
0.89100593
0.89069420
0.89055300
0.89021635
0.89037031
0.89013630
0.89018720
0.88972884
0.88963497
0.88945973
0.88962466
0.88962382
0.88918114
0.88893139
0.88863373
INFO - Training [16][  160/  196]   Loss 0.594236   Top1 79.458008   Top5 98.010254   BatchTime 0.464842   LR 0.000185
0.88865930
0.88838571
0.88818884
0.88769048
0.88743585
0.88749158
0.88710636
0.88675445
0.88639176
0.88605362
0.88585329
0.88542736
0.88598961
0.88676578
0.88698727
0.88697380
0.88674623
0.88670284
0.88661188
0.88640028
INFO - Training [16][  180/  196]   Loss 0.592776   Top1 79.472656   Top5 97.990451   BatchTime 0.458592   LR 0.000183
0.88634300
0.88615012
0.88607144
0.88616902
0.88613522
0.88636595
0.88658834
0.88647646
0.88627499
0.88599479
0.88599485
0.88637984
0.88644058
0.88629496
0.88627613
0.88655245
0.88684636
INFO - ==> Top1: 79.592    Top5: 97.990    Loss: 0.589
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.88660961
0.88631040
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.420612   Top1 86.035156   Top5 99.296875   BatchTime 0.116209
INFO - Validation [16][   40/   40]   Loss 0.408377   Top1 86.250000   Top5 99.420000   BatchTime 0.083131
INFO - ==> Top1: 86.250    Top5: 99.420    Loss: 0.408
INFO - ==> Sparsity : 0.312
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 86.250   Top5: 99.420]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
features.0.conv.0 tensor(0.5868)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0404)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0816)
features.2.conv.0 tensor(0.0486)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1942)
features.3.conv.0 tensor(0.0587)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1202)
features.4.conv.0 tensor(0.0524)
features.4.conv.3 tensor(0.3131)
features.4.conv.6 tensor(0.1527)
features.5.conv.0 tensor(0.1309)
features.5.conv.3 tensor(0.4230)
features.5.conv.6 tensor(0.1245)
features.6.conv.0 tensor(0.0537)
features.6.conv.3 tensor(0.0584)
features.6.conv.6 tensor(0.0822)
features.7.conv.0 tensor(0.0933)
features.7.conv.3 tensor(0.4560)
features.7.conv.6 tensor(0.2053)
features.8.conv.0 tensor(0.3717)
features.8.conv.3 tensor(0.5391)
features.8.conv.6 tensor(0.1632)
features.9.conv.0 tensor(0.3014)
features.9.conv.3 tensor(0.5466)
features.9.conv.6 tensor(0.1752)
features.10.conv.0 tensor(0.0568)
features.10.conv.3 tensor(0.1091)
features.10.conv.6 tensor(0.0639)
features.11.conv.0 tensor(0.3507)
features.11.conv.3 tensor(0.6493)
features.11.conv.6 tensor(0.4849)
features.12.conv.0 tensor(0.4471)
features.12.conv.3 tensor(0.6672)
features.12.conv.6 tensor(0.5104)
features.13.conv.0 tensor(0.2048)
features.13.conv.3 tensor(0.4938)
features.13.conv.6 tensor(0.1101)
features.14.conv.0 tensor(0.4604)
features.14.conv.3 tensor(0.8234)
features.14.conv.6 tensor(0.1994)
features.15.conv.0 tensor(0.6289)
features.15.conv.3 tensor(0.9057)
features.15.conv.6 tensor(0.9569)
features.16.conv.0 tensor(0.4335)
features.16.conv.3 tensor(0.8116)
features.16.conv.6 tensor(0.1329)
conv.0 tensor(0.0701)
tensor(682920.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
0.88639861
0.88618839
0.88625282
0.88684899
0.88704538
0.88757288
0.88769376
0.88831949
0.88852906
0.88873506
0.88835728
0.88916320
0.88961560
0.89018464
0.89022583
0.89060915
0.89064556
INFO - Training [17][   20/  196]   Loss 0.599696   Top1 79.121094   Top5 97.695312   BatchTime 0.545236   LR 0.000180
0.89083517
0.89084309
0.89061219
0.89069706
0.89076221
0.89086378
0.89109921
0.89148068
0.89156353
0.89194417
0.89226335
0.89257973
0.89265805
0.89290619
0.89340657
0.89396542
0.89453763
0.89541817
0.89622933
0.89639854
0.89677441
0.89736515
INFO - Training [17][   40/  196]   Loss 0.579031   Top1 80.195312   Top5 97.968750   BatchTime 0.500523   LR 0.000178
0.89733243
0.89776832
0.89798981
0.89804047
0.89819896
0.89836186
0.89851409
0.89867151
0.89868021
0.89868087
0.89870602
0.89856714
0.89874417
0.89833790
0.89834774
0.89817256
INFO - Training [17][   60/  196]   Loss 0.580911   Top1 80.052083   Top5 97.884115   BatchTime 0.462127   LR 0.000176
0.89806932
0.89821088
0.89825147
0.89813381
0.89832008
0.89837718
0.89830279
0.89843673
0.89828128
0.89800221
0.89763844
0.89735478
0.89698905
0.89688367
0.89688390
0.89700752
0.89716691
0.89800477
0.89851284
0.89820284
0.89800030
0.89787060
INFO - Training [17][   80/  196]   Loss 0.580328   Top1 79.921875   Top5 97.958984   BatchTime 0.462731   LR 0.000175
0.89757025
0.89764833
0.89768469
0.89749908
0.89747167
0.89752543
0.89736235
0.89746362
0.89782387
0.89790106
0.89759827
0.89731562
0.89669102
0.89648253
0.89591461
0.89555675
0.89519894
0.89544261
0.89572167
0.89517963
INFO - Training [17][  100/  196]   Loss 0.573352   Top1 80.175781   Top5 98.093750   BatchTime 0.472818   LR 0.000173
0.89474028
0.89426625
0.89437258
0.89416903
0.89380151
0.89389646
0.89390999
0.89427185
0.89438868
0.89444202
0.89421362
0.89388680
0.89368224
0.89344895
0.89307243
0.89254987
0.89216280
0.89200103
0.89179641
0.89157051
0.89154071
INFO - Training [17][  120/  196]   Loss 0.566540   Top1 80.494792   Top5 98.170573   BatchTime 0.470703   LR 0.000171
0.89108914
0.89062494
0.89017361
0.89028406
0.88990963
0.89024687
0.89024174
0.89002913
0.88998097
0.88971353
0.88972002
0.88915306
0.88881409
0.88849598
0.88818955
0.88755774
0.88689506
0.88670027
0.88603896
0.88535798
0.88498813
INFO - Training [17][  140/  196]   Loss 0.564850   Top1 80.541295   Top5 98.194754   BatchTime 0.473294   LR 0.000169
0.88450295
0.88403618
0.88368189
0.88322574
0.88255358
0.88277107
0.88237882
0.88210934
0.88163620
0.88090324
0.88020664
0.87969846
0.87919271
0.87876135
0.87843424
0.87818921
0.87788141
0.87772161
0.87768722
0.87801552
INFO - Training [17][  160/  196]   Loss 0.566160   Top1 80.485840   Top5 98.159180   BatchTime 0.464441   LR 0.000167
0.87772036
0.87736076
0.87716550
0.87687188
0.87670273
0.87661290
0.87617815
0.87561893
0.87561530
0.87581116
0.87547314
0.87527329
0.87553048
0.87566060
0.87518716
0.87477845
0.87471539
0.87433583
INFO - Training [17][  180/  196]   Loss 0.566262   Top1 80.449219   Top5 98.140191   BatchTime 0.462766   LR 0.000165
0.87410372
0.87397468
0.87389582
0.87414080
0.87390822
0.87407583
0.87406629
0.87402469
0.87411749
0.87400281
0.87413508
0.87404811
0.87428147
0.87421954
0.87412530
0.87429607
0.87436318
INFO - ==> Top1: 80.504    Top5: 98.154    Loss: 0.565
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87412262
0.87409598
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [17][   20/   40]   Loss 0.429308   Top1 85.273438   Top5 99.335938   BatchTime 0.117619
INFO - Validation [17][   40/   40]   Loss 0.410662   Top1 85.810000   Top5 99.490000   BatchTime 0.086775
INFO - ==> Top1: 85.810    Top5: 99.490    Loss: 0.411
INFO - ==> Sparsity : 0.327
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 86.250   Top5: 99.420]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.930   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 85.920   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-080126/_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5938)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0799)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.3526)
features.2.conv.6 tensor(0.1973)
features.3.conv.0 tensor(0.0666)
features.3.conv.3 tensor(0.0918)
features.3.conv.6 tensor(0.1189)
features.4.conv.0 tensor(0.0472)
features.4.conv.3 tensor(0.3148)
features.4.conv.6 tensor(0.1549)
features.5.conv.0 tensor(0.1364)
features.5.conv.3 tensor(0.4248)
features.5.conv.6 tensor(0.1253)
features.6.conv.0 tensor(0.0501)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0815)
features.7.conv.0 tensor(0.0962)
features.7.conv.3 tensor(0.4549)
features.7.conv.6 tensor(0.2042)
features.8.conv.0 tensor(0.3728)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.1568)
features.9.conv.0 tensor(0.2990)
features.9.conv.3 tensor(0.5469)
features.9.conv.6 tensor(0.1722)
features.10.conv.0 tensor(0.0657)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0645)
features.11.conv.0 tensor(0.3494)
features.11.conv.3 tensor(0.6485)
features.11.conv.6 tensor(0.4694)
features.12.conv.0 tensor(0.4478)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.3545)
features.13.conv.0 tensor(0.2052)
features.13.conv.3 tensor(0.4946)
features.13.conv.6 tensor(0.1089)
features.14.conv.0 tensor(0.6994)
features.14.conv.3 tensor(0.8237)
features.14.conv.6 tensor(0.2004)
features.15.conv.0 tensor(0.6478)
features.15.conv.3 tensor(0.9051)
features.15.conv.6 tensor(0.9540)
features.16.conv.0 tensor(0.4353)
features.16.conv.3 tensor(0.8118)
features.16.conv.6 tensor(0.1393)
conv.0 tensor(0.0700)
tensor(714866.) 2188896.0
0.87419808
0.87433004
0.87426871
0.87426043
0.87427860
0.87415719
0.87410802
0.87413448
0.87422544
0.87418431
0.87429065
0.87441391
0.87443060
0.87460893
0.87468243
0.87468600
0.87509733
INFO - Training [18][   20/  196]   Loss 0.571755   Top1 79.824219   Top5 97.910156   BatchTime 0.518739   LR 0.000162
0.87511873
0.87505871
0.87486279
0.87664080
0.88870519
0.89489961
0.89477134
0.89471591
0.89438415
0.89420503
0.89422214
0.89348680
0.89327258
0.89297611
0.89264542
0.89231783
0.89218086
0.89188510
0.89217985
0.89231426
0.89251381
0.89242631
INFO - Training [18][   40/  196]   Loss 0.561873   Top1 80.380859   Top5 97.998047   BatchTime 0.486806   LR 0.000160
0.89231557
0.89202338
0.89199805
0.89186984
0.89156508
0.89150989
0.89155072
0.89146578
0.89165759
0.89165962
0.89160132
0.89156026
0.89101225
0.89190531
0.89208525
0.89240658
0.89301163
0.89321572
0.89366812
0.89434767
0.89510280
INFO - Training [18][   60/  196]   Loss 0.563088   Top1 80.136719   Top5 98.040365   BatchTime 0.453423   LR 0.000158
0.89515150
0.89587468
0.90772587
0.91424811
0.91403919
0.91383666
0.91368675
0.91400236
0.91413307
0.91395372
0.91386890
0.91392070
0.91367203
0.91343427
0.91324449
0.91357547
0.91363221
0.91344666
0.91349262
0.91360676
INFO - Training [18][   80/  196]   Loss 0.561817   Top1 80.288086   Top5 98.178711   BatchTime 0.467616   LR 0.000156
0.91367161
0.91348320
0.91345924
0.91348761
0.91353184
0.91353875
0.91356105
0.91353637
0.91356057
0.91348392
0.91374993
0.91375875
0.91342771
0.91334456
0.91302150
0.91278857
0.91256726
0.91249943
0.91275394
INFO - Training [18][  100/  196]   Loss 0.552046   Top1 80.628906   Top5 98.218750   BatchTime 0.470535   LR 0.000154
0.91343218
0.91359192
0.91395372
0.91411251
0.91401547
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    optimizer, lr_scheduler, args.epochs, monitors, args, init_qparams = False, hard_pruning = True)
  File "main_slsq.py", line 77, in main
    logger.info(('Optimizer: %s' % optimizer).replace('\n', '\n' + ' ' * 11))
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 186, in train_one_epoch_slsq
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
0.91398680