Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
*************soft_pruning_mode*******************
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.95438832
0.95000303
0.92791492
0.91658413
0.91714948
0.90908009
0.89835936
0.90029627
0.89170969
0.87887233
INFO - Training [0][   20/  196]   Loss 1.571111   Top1 53.398438   Top5 89.101562   BatchTime 0.452160   LR 0.004999
0.87738210
0.87074333
0.87055385
0.87808472
0.88178813
0.88505703
0.88598442
0.88603413
0.88658559
0.88730466
0.88787526
0.88784510
0.88825673
0.88996148
0.89193702
0.89321184
0.89434618
0.89554560
0.89689636
INFO - Training [0][   40/  196]   Loss 1.487792   Top1 52.285156   Top5 89.384766   BatchTime 0.435084   LR 0.004995
0.89835101
0.89938009
0.90021896
0.90142184
0.90192264
0.90285426
0.90399742
0.90467280
0.90537131
0.90585566
0.90667713
0.90715510
0.90760696
0.90838581
0.90883929
0.90956342
0.91054511
0.91110253
0.91168392
0.91251117
INFO - Training [0][   60/  196]   Loss 1.384029   Top1 54.459635   Top5 90.644531   BatchTime 0.425966   LR 0.004989
0.91307008
0.91370434
0.91430360
0.91572040
0.91675913
0.91841424
0.91990143
0.92179984
0.92316937
0.92489821
0.92737031
0.93026024
0.93340099
0.93623042
0.93881458
0.94128555
0.94320160
0.94428301
INFO - Training [0][   80/  196]   Loss 1.314619   Top1 56.499023   Top5 91.586914   BatchTime 0.428628   LR 0.004980
0.94458401
0.94483578
0.94438529
0.93852639
0.94464213
0.94479483
0.94475162
0.94489151
0.94513500
0.94518685
0.94520849
0.94527775
0.94541818
0.94546193
0.94550818
0.94581097
0.94594890
0.94594079
0.94576579
0.94596583
0.94593149
0.94616902
0.94611907
INFO - Training [0][  100/  196]   Loss 1.253574   Top1 58.363281   Top5 92.371094   BatchTime 0.429371   LR 0.004968
0.94593406
0.94587225
0.94591141
0.94618249
0.94624847
0.94627178
0.94622141
0.94457000
0.94048142
0.94454420
0.94614589
0.94618553
0.94609642
0.94622612
0.94636065
0.94627315
0.94620055
0.94625801
0.94642168
0.94638330
INFO - Training [0][  120/  196]   Loss 1.205866   Top1 59.889323   Top5 92.858073   BatchTime 0.425933   LR 0.004954
0.94657111
0.94674349
0.94647151
0.94667166
0.94694877
0.94690728
0.94688672
0.94681066
0.94700307
0.94706303
0.94688570
0.94715232
0.94699770
0.94676030
0.94646770
0.94675332
0.94686955
0.94685328
0.94698960
INFO - Training [0][  140/  196]   Loss 1.191195   Top1 60.424107   Top5 92.938058   BatchTime 0.425025   LR 0.004938
0.94687879
0.94673985
0.94710517
0.94707769
0.94727236
0.94709927
0.94736075
0.94751024
0.94750887
0.94758081
0.94722623
0.94742012
0.94739264
0.94740051
0.94756657
0.94762266
0.94773167
0.94767267
0.94763702
0.94753391
0.94773406
INFO - Training [0][  160/  196]   Loss 1.170753   Top1 61.083984   Top5 93.112793   BatchTime 0.420730   LR 0.004919
0.94757462
0.94751382
0.94745880
0.94732702
0.94746667
0.94763744
0.94774091
0.94786489
0.94768208
0.94795954
0.94803250
0.94801825
0.94809902
0.94742721
0.94662565
0.94577092
0.94643289
0.94748867
0.94880790
INFO - Training [0][  180/  196]   Loss 1.147664   Top1 61.773003   Top5 93.337674   BatchTime 0.408791   LR 0.004897
0.94839072
0.94849193
0.94860840
0.94842035
0.94832367
0.94823301
0.94832677
0.94855928
0.94885725
0.94884068
0.94838333
0.94842619
0.92744672
0.92102045
INFO - ==> Top1: 62.382    Top5: 93.476    Loss: 1.128
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.92630923
0.94413894
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [0][   20/   40]   Loss 0.705497   Top1 76.328125   Top5 98.554688   BatchTime 0.109939
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.2207)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.1100)
features.1.conv.6 tensor(0.0707)
features.2.conv.0 tensor(0.0408)
features.2.conv.3 tensor(0.0733)
features.2.conv.6
INFO - Validation [0][   40/   40]   Loss 0.698075   Top1 76.410000   Top5 98.560000   BatchTime 0.081944
INFO - ==> Top1: 76.410    Top5: 98.560    Loss: 0.698
INFO - ==> Sparsity : 0.152
features.5.conv.3 tensor(0.0700)
features.3.conv.0 tensor(0.1481)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0681)
features.4.conv.0 tensor(0.0589)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0952)
features.5.conv.0 tensor(0.0526)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.0897)
features.6.conv.0 tensor(0.0490)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0719)
features.7.conv.0 tensor(0.0699)
features.7.conv.3 tensor(0.1010)
features.7.conv.6 tensor(0.1207)
features.8.conv.0 tensor(0.1848)
features.8.conv.3 tensor(0.1033)
features.8.conv.6 tensor(0.1280)
features.9.conv.0 tensor(0.1061)
features.9.conv.3 tensor(0.1230)
features.9.conv.6 tensor(0.1226)
features.10.conv.0 tensor(0.0562)
features.10.conv.3 tensor(0.0877)
features.10.conv.6 tensor(0.1071)
features.11.conv.0 tensor(0.1072)
features.11.conv.3 tensor(0.0947)
features.11.conv.6 tensor(0.1889)
features.12.conv.0 tensor(0.4314)
features.12.conv.3 tensor(0.0947)
features.12.conv.6 tensor(0.1636)
features.13.conv.0 tensor(0.0831)
features.13.conv.3 tensor(0.1264)
features.13.conv.6 tensor(0.1116)
features.14.conv.0 tensor(0.1431)
features.14.conv.3 tensor(0.0822)
features.14.conv.6 tensor(0.5399)
features.15.conv.0 tensor(0.0515)
features.15.conv.3 tensor(0.0700)
features.15.conv.6 tensor(0.2433)
features.16.conv.0 tensor(0.0821)
features.16.conv.3 tensor(0.0817)
features.16.conv.6 tensor(0.1049)
conv.0 tensor(0.0962)
tensor(332347.) 2188896.0
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.94512022
0.94863939
0.94937718
0.94918174
0.94910544
0.94924682
0.94946849
0.94960940
0.94973731
0.94970524
0.94968301
0.94969761
0.94967407
0.94967920
0.94987386
0.94971770
0.94975752
0.94969851
0.94981140
0.94994980
0.94996375
0.95005441
INFO - Training [1][   20/  196]   Loss 0.930762   Top1 68.046875   Top5 95.703125   BatchTime 0.467105   LR 0.004853
0.95006317
0.94991416
0.94951957
0.94954616
0.94972599
0.94984370
0.94989562
0.94991338
0.95007259
0.95002836
0.94989169
0.94995636
0.95016873
0.94995725
0.95007795
0.94996071
INFO - Training [1][   40/  196]   Loss 0.949319   Top1 67.724609   Top5 95.673828   BatchTime 0.420673   LR 0.004825
0.94974190
0.95018715
0.95003575
0.95003647
0.95041478
0.95024985
0.95014101
0.95017570
0.95000267
0.94994289
0.95014763
0.95032871
0.95045280
0.95024961
0.95028281
0.95025039
0.95045114
0.95050669
0.95051819
0.95093131
0.95061642
0.95046383
INFO - Training [1][   60/  196]   Loss 0.954075   Top1 67.356771   Top5 95.416667   BatchTime 0.404604   LR 0.004794
0.95068944
0.95051801
0.94753712
0.94422936
0.94261307
0.94404739
0.94719517
0.95160192
0.95156616
0.95115525
0.95127732
0.95113832
0.95121789
0.95120633
0.95119321
0.95136082
0.95138562
0.95138437
0.95163167
0.95174587
0.95185608
INFO - Training [1][   80/  196]   Loss 0.938361   Top1 67.880859   Top5 95.654297   BatchTime 0.396599   LR 0.004761
0.95176655
0.95144004
0.95171231
0.95170772
0.95142031
0.95150310
0.95125377
0.95116055
0.95124102
0.95144027
0.95152819
0.95156908
0.95129800
0.95132101
0.95109153
0.95129442
0.95116389
0.95103544
0.95095962
0.95098901
0.95126027
INFO - Training [1][  100/  196]   Loss 0.919990   Top1 68.632812   Top5 95.769531   BatchTime 0.393839   LR 0.004725
0.95108378
0.95088756
0.95120353
0.95129496
0.95122933
0.95134825
0.95147550
0.95146358
0.95140773
0.95129824
0.95154697
0.95127487
0.95121765
0.95141208
0.95117825
0.95170110
INFO - Training [1][  120/  196]   Loss 0.906355   Top1 69.169922   Top5 95.979818   BatchTime 0.390213   LR 0.004687
0.95157397
0.95161593
0.95168549
0.95166105
0.95173401
0.95184475
0.95179951
0.95198590
0.95221734
0.95203781
0.95220619
0.95226479
0.95250255
0.95260769
0.95256895
0.95259565
0.95253229
0.95228732
0.95234704
0.95225883
0.95228565
0.95226151
INFO - Training [1][  140/  196]   Loss 0.892732   Top1 69.592634   Top5 96.110491   BatchTime 0.388033   LR 0.004647
0.95229024
0.95237035
0.95234525
0.95259666
0.95277417
0.95261538
0.95274711
0.95295602
0.95309585
0.95343739
0.95345855
0.95359838
0.95382982
0.95379984
0.95386529
0.95388603
0.95394963
0.95393348
INFO - Training [1][  160/  196]   Loss 0.884936   Top1 69.836426   Top5 96.113281   BatchTime 0.380013   LR 0.004605
0.95384139
0.95384622
0.95381534
0.95381761
0.95376444
0.95371449
0.95410585
0.95393801
0.95393139
0.95268720
0.95236832
0.95293200
0.95382172
0.95390457
0.95390946
0.95401001
0.95407361
0.95411187
0.95402133
0.95399576
0.95381367
0.95388007
0.95404780
0.95391822
0.95367026
INFO - Training [1][  180/  196]   Loss 0.870368   Top1 70.301649   Top5 96.178385   BatchTime 0.373906   LR 0.004560
0.95316631
0.95245004
0.95127833
0.95209616
0.95284474
0.95318884
0.95361865
0.95357561
0.95346731
0.95359379
0.95352477
INFO - ==> Top1: 70.542    Top5: 96.206    Loss: 0.864
0.95360905
0.95340705
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [1][   20/   40]   Loss 0.628708   Top1 78.886719   Top5 98.574219   BatchTime 0.113983
INFO - Validation [1][   40/   40]   Loss 0.634339   Top1 78.610000   Top5 98.600000   BatchTime 0.084898
INFO - ==> Top1: 78.610    Top5: 98.600    Loss: 0.634
INFO - ==> Sparsity : 0.150
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 78.610   Top5: 98.600]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.410   Top5: 98.560]
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.1007)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0318)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0747)
features.3.conv.0 tensor(0.0370)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0662)
features.4.conv.0 tensor(0.0537)
features.4.conv.3 tensor(0.1013)
features.4.conv.6 tensor(0.0924)
features.5.conv.0 tensor(0.0487)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.1073)
features.6.conv.0 tensor(0.0350)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0707)
features.7.conv.0 tensor(0.0448)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.1270)
features.8.conv.0 tensor(0.1451)
features.8.conv.3 tensor(0.1111)
features.8.conv.6 tensor(0.1264)
features.9.conv.0 tensor(0.0997)
features.9.conv.3 tensor(0.1282)
features.9.conv.6 tensor(0.1214)
features.10.conv.0 tensor(0.0487)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0932)
features.11.conv.0 tensor(0.1318)
features.11.conv.3 tensor(0.1049)
features.11.conv.6 tensor(0.1859)
features.12.conv.0 tensor(0.2767)
features.12.conv.3 tensor(0.1015)
features.12.conv.6 tensor(0.1769)
features.13.conv.0 tensor(0.0817)
features.13.conv.3 tensor(0.1427)
features.13.conv.6 tensor(0.1151)
features.14.conv.0 tensor(0.1540)
features.14.conv.3 tensor(0.0854)
features.14.conv.6 tensor(0.4407)
features.15.conv.0 tensor(0.0654)
features.15.conv.3 tensor(0.0722)
features.15.conv.6 tensor(0.2654)
features.16.conv.0 tensor(0.0411)
features.16.conv.3 tensor(0.0943)
features.16.conv.6 tensor(0.1436)
conv.0 tensor(0.1142)
tensor(328469.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.95364696
0.95334297
0.95314175
0.95330900
0.95342839
0.95342344
0.95347506
0.95328927
0.95340526
0.95322162
0.95297199
0.95292765
0.95308077
0.95313770
0.95332313
0.95321298
0.95323718
0.95325536
0.95346606
0.95359355
0.95361888
0.95361531
INFO - Training [2][   20/  196]   Loss 0.825666   Top1 71.445312   Top5 96.015625   BatchTime 0.438727   LR 0.004477
0.95355493
0.95373446
0.95350987
0.95360363
0.95366263
0.95336962
0.95327443
0.95343477
0.95363057
0.95329511
0.95320737
0.95290846
0.95052856
0.95252430
0.95337272
0.95338780
INFO - Training [2][   40/  196]   Loss 0.807187   Top1 72.392578   Top5 96.386719   BatchTime 0.402768   LR 0.004426
0.95321769
0.95336175
0.95366871
0.95351267
0.95308542
0.95340133
0.95350736
0.95334971
0.95336097
0.95366043
0.95361620
0.95352209
0.95359343
0.95367098
0.95358390
0.95363772
0.95381838
0.95398194
0.95401484
0.95399958
0.95388961
0.95383972
INFO - Training [2][   60/  196]   Loss 0.792815   Top1 72.701823   Top5 96.595052   BatchTime 0.392487   LR 0.004374
0.95374352
0.95383388
0.95388842
0.95382118
0.95378852
0.95391250
0.95400822
0.95397747
0.95399666
0.95399612
0.95391452
0.95403826
0.95420432
0.95410538
0.95417470
0.95422220
INFO - Training [2][   80/  196]   Loss 0.780326   Top1 73.168945   Top5 96.679688   BatchTime 0.383784   LR 0.004320
0.95444334
0.95438921
0.95408958
0.95418155
0.95413005
0.95402604
0.95430362
0.95411438
0.95398879
0.95426446
0.95428222
0.95416600
0.95392376
0.95383310
0.95382255
0.95363033
0.95358032
0.95348740
0.95363963
0.95378280
0.95388389
0.95386058
0.95391601
INFO - Training [2][  100/  196]   Loss 0.769531   Top1 73.550781   Top5 96.683594   BatchTime 0.378485   LR 0.004264
0.95378077
0.95381367
0.95374793
0.95352691
0.95340943
0.95333570
0.95324999
0.95341796
0.95360619
0.95380849
0.95400101
0.95410895
0.95419556
0.95427370
0.95424360
0.95412940
0.95407242
0.95418435
0.95408225
0.95401955
0.95388007
0.95417851
INFO - Training [2][  120/  196]   Loss 0.760677   Top1 73.811849   Top5 96.777344   BatchTime 0.376165   LR 0.004206
0.95401829
0.95407641
0.95423943
0.95424616
0.95432699
0.95431775
0.95127094
0.94064105
0.95094198
0.95451152
0.95450145
0.95441461
0.95467615
0.95457888
0.95455843
0.95468867
0.95466208
INFO - Training [2][  140/  196]   Loss 0.761168   Top1 73.761161   Top5 96.830357   BatchTime 0.370984   LR 0.004146
0.95469511
0.95484453
0.95499039
0.95500427
0.95492756
0.95514905
0.95514435
0.95491093
0.95416945
0.95007741
0.93749875
0.92900479
0.92763722
0.92821825
0.93021655
0.93843752
0.94842744
0.95274276
0.95438844
0.95457023
INFO - Training [2][  160/  196]   Loss 0.764036   Top1 73.698730   Top5 96.831055   BatchTime 0.362705   LR 0.004085
0.95453417
0.95467985
0.95488584
0.95482570
0.95513982
0.95521659
0.95483553
0.95471931
0.95493340
0.95461768
0.95465606
0.95447898
0.95460320
0.95477206
0.95461643
0.95453912
0.95455933
0.95463818
0.95474726
0.95489872
0.95478159
0.95472562
INFO - Training [2][  180/  196]   Loss 0.759822   Top1 73.891059   Top5 96.783854   BatchTime 0.363634   LR 0.004022
0.95458335
0.95465046
0.95465750
0.95465636
0.95479560
0.95477384
0.95492274
0.95511460
0.95489365
0.95523858
0.95528096
0.95507330
0.95519459
0.95526683
0.95536572
0.95569813
INFO - ==> Top1: 74.052    Top5: 96.800    Loss: 0.755
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.585192   Top1 81.230469   Top5 98.867188   BatchTime 0.208207
INFO - Validation [2][   40/   40]   Loss 0.579532   Top1 80.980000   Top5 98.980000   BatchTime 0.136489
INFO - ==> Top1: 80.980    Top5: 98.980    Loss: 0.580
INFO - ==> Sparsity : 0.168
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 80.980   Top5: 98.980]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 78.610   Top5: 98.600]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 76.410   Top5: 98.560]
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.1953)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0411)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0790)
features.3.conv.0 tensor(0.0292)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0603)
features.4.conv.0 tensor(0.0420)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.0994)
features.5.conv.0 tensor(0.0464)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1076)
features.6.conv.0 tensor(0.0324)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0688)
features.7.conv.0 tensor(0.0603)
features.7.conv.3 tensor(0.1071)
features.7.conv.6 tensor(0.1218)
features.8.conv.0 tensor(0.0950)
features.8.conv.3 tensor(0.1073)
features.8.conv.6 tensor(0.1304)
features.9.conv.0 tensor(0.0786)
features.9.conv.3 tensor(0.1363)
features.9.conv.6 tensor(0.1238)
features.10.conv.0 tensor(0.0472)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0925)
features.11.conv.0 tensor(0.1414)
features.11.conv.3 tensor(0.1084)
features.11.conv.6 tensor(0.1965)
features.12.conv.0 tensor(0.2512)
features.12.conv.3 tensor(0.1084)
features.12.conv.6 tensor(0.1775)
features.13.conv.0 tensor(0.0684)
features.13.conv.3 tensor(0.1773)
features.13.conv.6 tensor(0.0928)
features.14.conv.0 tensor(0.1857)
features.14.conv.3 tensor(0.0873)
features.14.conv.6 tensor(0.4743)
features.15.conv.0 tensor(0.0651)
features.15.conv.3 tensor(0.0722)
features.15.conv.6 tensor(0.2636)
features.16.conv.0 tensor(0.0536)
features.16.conv.3 tensor(0.0955)
features.16.conv.6 tensor(0.2311)
conv.0 tensor(0.1286)
tensor(368548.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.95564169
0.95553946
0.95543569
0.95543516
0.95547038
0.95551848
0.95534706
0.95522076
0.95512772
0.95531183
0.95547253
0.95532924
0.95536679
0.95547026
0.95582706
0.95575863
0.95602733
INFO - Training [3][   20/  196]   Loss 0.747987   Top1 75.000000   Top5 96.406250   BatchTime 0.438523   LR 0.003907
0.95617741
0.95598197
0.95578194
0.95568174
0.95579964
0.95578486
0.95597887
0.95608097
0.95586741
0.95579642
0.95590031
0.95564896
0.95556897
0.95550984
0.95559174
0.95568669
0.95567399
0.95577699
0.95568043
0.95568490
0.95578992
INFO - Training [3][   40/  196]   Loss 0.737081   Top1 75.009766   Top5 96.650391   BatchTime 0.407136   LR 0.003840
0.95570666
0.95567739
0.95551211
0.95558596
0.95570689
0.95555764
0.95558178
0.95590729
0.95605582
0.95597637
0.95573962
0.95555115
0.95559651
0.95564741
0.95567232
0.95552111
0.95570397
0.95574838
0.95582539
0.95583588
0.95557427
0.95543057
INFO - Training [3][   60/  196]   Loss 0.728883   Top1 75.104167   Top5 96.842448   BatchTime 0.397124   LR 0.003771
0.95536321
0.95524079
0.95521885
0.95514500
0.95523328
0.95474571
0.94786072
0.93200433
0.92765248
0.92881829
0.93854123
0.95108056
0.95486116
0.95497006
0.95508260
0.95535815
INFO - Training [3][   80/  196]   Loss 0.718088   Top1 75.537109   Top5 97.045898   BatchTime 0.390349   LR 0.003701
0.95548427
0.95548475
0.95555192
0.95560074
0.95540619
0.95526326
0.95523107
0.95528466
0.95534474
0.95545936
0.95568424
0.95571786
0.95573872
0.95583582
0.95577419
0.95556796
0.95549482
0.95551044
0.95527673
0.95552242
0.95565861
0.95562363
INFO - Training [3][  100/  196]   Loss 0.704388   Top1 76.093750   Top5 97.039062   BatchTime 0.384614   LR 0.003630
0.95552379
0.95548987
0.95537543
0.95512861
0.95511675
0.95511019
0.95491105
0.95521379
0.95513874
0.95512682
0.95502812
0.95503628
0.95498097
0.95500118
0.95511043
0.95511675
0.95529896
0.95537442
INFO - Training [3][  120/  196]   Loss 0.697632   Top1 76.279297   Top5 97.135417   BatchTime 0.374574   LR 0.003558
0.95515889
0.95505548
0.95551568
0.95536894
0.95543289
0.95557022
0.95555049
0.95572609
0.95567065
0.95563620
0.95566469
0.95572495
0.95563263
0.95548630
0.95533079
0.95567322
0.95581526
0.95587540
0.95584184
INFO - Training [3][  140/  196]   Loss 0.692705   Top1 76.409040   Top5 97.195871   BatchTime 0.367060   LR 0.003484
0.95595413
0.95596135
0.95575333
0.95546454
0.95567369
0.95496190
0.95387703
0.95300430
0.95246911
0.95379525
0.95606333
0.95578831
0.95567638
0.95570219
0.95509875
0.95558077
0.95561939
0.95556146
0.95576793
0.95569396
0.95559198
0.95600265
0.95565659
INFO - Training [3][  160/  196]   Loss 0.692183   Top1 76.469727   Top5 97.207031   BatchTime 0.365929   LR 0.003410
0.95551950
0.95546734
0.95548326
0.95546407
0.95550448
0.95571315
0.95562947
0.95565897
0.95579392
0.95585656
0.95567816
0.95576656
0.95588267
0.95563161
0.95553857
0.95572686
0.95558536
0.95559424
0.95555472
0.95567435
0.95556885
INFO - Training [3][  180/  196]   Loss 0.690635   Top1 76.425781   Top5 97.170139   BatchTime 0.366766   LR 0.003335
0.95545793
0.95556462
0.95569789
0.95555520
0.95553714
0.95577568
0.95565259
0.95555103
0.95553106
0.95561749
0.95560926
0.95552623
0.95545298
0.95558977
0.95563954
0.95541722
INFO - ==> Top1: 76.556    Top5: 97.170    Loss: 0.687
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95565754
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.508516   Top1 82.949219   Top5 99.082031   BatchTime 0.117351
INFO - Validation [3][   40/   40]   Loss 0.505483   Top1 82.810000   Top5 99.300000   BatchTime 0.091874
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.2148)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0686)
features.2.conv.0 tensor(0.0339)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0775)
features.3.conv.0 tensor(0.0275)
features.3.conv.3 tensor(0.0540)
features.3.conv.6 tensor(0.0616)
features.4.conv.0 tensor(0.0399)
features.4.conv.3 tensor(0.1019)
features.4.conv.6 tensor(0.0951)
features.5.conv.0 tensor(0.0456)
features.5.conv.3 tensor(0.0602)
features.5.conv.6 tensor(0.1094)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0666)
features.7.conv.0 tensor(0.0483)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.1250)
features.8.conv.0 tensor(0.0945)
features.8.conv.3 tensor(0.1088)
features.8.conv.6 tensor(0.1338)
INFO - ==> Top1: 82.810    Top5: 99.300    Loss: 0.505
INFO - ==> Sparsity : 0.172
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 82.810   Top5: 99.300]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 80.980   Top5: 98.980]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 78.610   Top5: 98.600]
features.9.conv.0 tensor(0.0741)
features.9.conv.3 tensor(0.1369)
features.9.conv.6 tensor(0.1187)
features.10.conv.0 tensor(0.0461)
features.10.conv.3 tensor(0.1027)
features.10.conv.6 tensor(0.0819)
features.11.conv.0 tensor(0.1302)
features.11.conv.3 tensor(0.1134)
features.11.conv.6 tensor(0.1983)
features.12.conv.0 tensor(0.2376)
features.12.conv.3 tensor(0.1115)
features.12.conv.6 tensor(0.1705)
features.13.conv.0 tensor(0.0612)
features.13.conv.3 tensor(0.1806)
features.13.conv.6 tensor(0.0967)
features.14.conv.0 tensor(0.1955)
features.14.conv.3 tensor(0.0872)
features.14.conv.6 tensor(0.4698)
features.15.conv.0 tensor(0.0737)
features.15.conv.3 tensor(0.0738)
features.15.conv.6 tensor(0.3332)
features.16.conv.0 tensor(0.0494)
features.16.conv.3 tensor(0.1058)
features.16.conv.6 tensor(0.2519)
conv.0 tensor(0.1114)
tensor(377572.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.95559222
0.95558673
0.95547754
0.95545822
0.95537478
0.95519120
0.95514715
0.95513397
0.95511472
0.95511222
0.95527482
0.95516813
0.95517033
0.95517373
0.95527202
0.95534158
0.95543110
INFO - Training [4][   20/  196]   Loss 0.669720   Top1 76.992188   Top5 96.718750   BatchTime 0.446066   LR 0.003200
0.95535797
0.95541888
0.95547396
0.95531732
0.95507038
0.95489132
0.95512885
0.95532405
0.95520896
0.95515162
0.95524329
0.95527023
0.95509517
0.95534694
0.95514411
0.95509923
0.95530599
0.95554960
0.95540082
0.95528448
0.95539182
0.95557046
INFO - Training [4][   40/  196]   Loss 0.652614   Top1 77.656250   Top5 97.050781   BatchTime 0.408171   LR 0.003122
0.95542181
0.95527512
0.95530611
0.95536220
0.95551366
0.95549887
0.95551479
0.95559156
0.95548505
0.95541763
0.95540875
0.95552403
0.95558143
0.95548350
0.95549506
0.95540011
INFO - Training [4][   60/  196]   Loss 0.650810   Top1 77.701823   Top5 97.259115   BatchTime 0.391582   LR 0.003044
0.95524216
0.95526862
0.95532453
0.95538408
0.95537049
0.95535904
0.95558965
0.95550805
0.95553833
0.95566517
0.95563561
0.95560759
0.95552021
0.95544499
0.95512480
0.95487583
0.95474380
0.95468187
0.95464438
0.95365894
0.95158333
0.94237900
INFO - Training [4][   80/  196]   Loss 0.652861   Top1 77.685547   Top5 97.338867   BatchTime 0.388322   LR 0.002965
0.94320536
0.94829470
0.95313758
0.95527953
0.95499849
0.95484871
0.95493633
0.95504808
0.95512635
0.95509517
0.95506668
0.95505387
0.95501328
0.95501775
0.95487124
0.95487660
0.95490962
0.95491457
0.95481908
0.95495379
0.95511842
0.95427108
0.95353556
INFO - Training [4][  100/  196]   Loss 0.646316   Top1 77.910156   Top5 97.402344   BatchTime 0.379518   LR 0.002886
0.95496595
0.95520681
0.95506287
0.95470494
0.95494461
0.95501101
0.95465535
0.95461386
0.95460522
0.95450324
0.95439351
0.95451313
0.95428532
0.95431262
0.95432001
0.95426226
0.95403808
0.95312929
0.95185626
INFO - Training [4][  120/  196]   Loss 0.636955   Top1 78.229167   Top5 97.500000   BatchTime 0.369305   LR 0.002806
0.95323449
0.95447797
0.95430040
0.95447695
0.95473278
0.95455742
0.95463371
0.95428109
0.95352566
0.95407742
0.95433897
0.95458841
0.95458806
0.95472962
0.95475727
0.95481962
INFO - Training [4][  140/  196]   Loss 0.635842   Top1 78.250558   Top5 97.544643   BatchTime 0.367702   LR 0.002726
0.95464998
0.95466995
0.95481020
0.95474702
0.95482230
0.95481348
0.95477790
0.95466119
0.95471829
0.95476377
0.95463508
0.95466536
0.95467401
0.95454520
0.95466369
0.95457321
0.95465326
0.95478463
0.95473886
0.95480484
0.95483005
INFO - Training [4][  160/  196]   Loss 0.636823   Top1 78.234863   Top5 97.565918   BatchTime 0.371052   LR 0.002646
0.95499736
0.95498681
0.95465314
0.95454699
0.95492107
0.95523500
0.95513040
0.95503592
0.95516700
0.95489556
0.95483983
0.95492578
0.95497298
0.95510942
0.95510620
0.95533776
0.95553142
0.95573580
0.95567816
0.95555496
0.95550269
INFO - Training [4][  180/  196]   Loss 0.632375   Top1 78.387587   Top5 97.526042   BatchTime 0.371356   LR 0.002566
0.95536131
0.95536196
0.95528078
0.95518440
0.95496505
0.95489550
0.95514727
0.95542675
0.95543289
0.95564306
0.95550245
0.95542639
0.95555753
0.95573819
0.95561755
0.95547241
0.95538217
INFO - ==> Top1: 78.444    Top5: 97.530    Loss: 0.630
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95547038
0.95518136
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 0.473075   Top1 84.707031   Top5 99.199219   BatchTime 0.137057
INFO - Validation [4][   40/   40]   Loss 0.463924   Top1 84.500000   Top5 99.260000   BatchTime 0.106911
INFO - ==> Top1: 84.500    Top5: 99.260    Loss: 0.464
INFO - ==> Sparsity : 0.178
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 84.500   Top5: 99.260]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 82.810   Top5: 99.300]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 80.980   Top5: 98.980]
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0365)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0252)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0632)
features.4.conv.0 tensor(0.0308)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0993)
features.5.conv.0 tensor(0.0446)
features.5.conv.3 tensor(0.0573)
features.5.conv.6 tensor(0.1024)
features.6.conv.0 tensor(0.0280)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0647)
features.7.conv.0 tensor(0.0516)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.1208)
features.8.conv.0 tensor(0.0943)
features.8.conv.3 tensor(0.1105)
features.8.conv.6 tensor(0.1299)
features.9.conv.0 tensor(0.1153)
features.9.conv.3 tensor(0.1412)
features.9.conv.6 tensor(0.1156)
features.10.conv.0 tensor(0.0365)
features.10.conv.3 tensor(0.1056)
features.10.conv.6 tensor(0.0811)
features.11.conv.0 tensor(0.1166)
features.11.conv.3 tensor(0.1169)
features.11.conv.6 tensor(0.2154)
features.12.conv.0 tensor(0.2631)
features.12.conv.3 tensor(0.1121)
features.12.conv.6 tensor(0.1707)
features.13.conv.0 tensor(0.0476)
features.13.conv.3 tensor(0.1784)
features.13.conv.6 tensor(0.0870)
features.14.conv.0 tensor(0.1998)
features.14.conv.3 tensor(0.0905)
features.14.conv.6 tensor(0.4911)
features.15.conv.0 tensor(0.0802)
features.15.conv.3 tensor(0.0744)
features.15.conv.6 tensor(0.3439)
features.16.conv.0 tensor(0.0542)
features.16.conv.3 tensor(0.1015)
features.16.conv.6 tensor(0.2688)
conv.0 tensor(0.1101)
tensor(390073.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.95535707
0.95533508
0.95505434
0.95486587
0.95478398
0.95470160
0.95462900
0.95470721
0.95492417
0.95489347
0.95499623
0.95480120
0.95505470
0.95526659
0.95506972
INFO - Training [5][   20/  196]   Loss 0.622209   Top1 78.710938   Top5 97.031250   BatchTime 0.427934   LR 0.002424
0.95493823
0.95479852
0.95485932
0.95489174
0.95483130
0.95470464
0.95479161
0.95489287
0.95479989
0.95481324
0.95473605
0.95452285
0.95446754
0.95444733
0.95448101
0.95446563
0.95458031
0.95459187
0.95468867
0.95471138
0.95491105
0.95498288
INFO - Training [5][   40/  196]   Loss 0.637261   Top1 78.154297   Top5 97.167969   BatchTime 0.395935   LR 0.002343
0.95498276
0.95505750
0.95530432
0.95548874
0.95511693
0.95506734
0.95511985
0.95503193
0.95496851
0.95484632
0.95481580
0.95472944
0.95475131
0.95477486
0.95477903
0.95496422
0.95511776
0.95505852
0.95493585
0.95499712
0.95510441
0.95512313
INFO - Training [5][   60/  196]   Loss 0.621658   Top1 78.860677   Top5 97.239583   BatchTime 0.385154   LR 0.002263
0.95500541
0.95515591
0.95488280
0.95493585
0.95502919
0.95520514
0.95513201
0.95501190
0.95504534
0.95516342
0.95533627
0.95518249
0.95517427
0.95501482
0.95493597
0.95488662
0.95497286
INFO - Training [5][   80/  196]   Loss 0.607964   Top1 79.296875   Top5 97.421875   BatchTime 0.377656   LR 0.002183
0.95504737
0.95509899
0.95523709
0.95537734
0.95501149
0.95510763
0.95519638
0.95520264
0.95489287
0.95474863
0.95479226
0.95504290
0.95500499
0.95508921
0.95485419
0.95460802
0.95453417
0.95454764
0.95448202
INFO - Training [5][  100/  196]   Loss 0.595441   Top1 79.695312   Top5 97.589844   BatchTime 0.363506   LR 0.002104
0.95446372
0.95455664
0.95455652
0.95462471
0.95474827
0.95468342
0.95470667
0.95483780
0.95502269
0.95483464
0.95517015
0.95515126
0.95512545
0.95507163
0.95496851
0.95495337
0.95494115
0.95488751
0.95493227
0.95490468
0.95489347
INFO - Training [5][  120/  196]   Loss 0.588734   Top1 80.006510   Top5 97.643229   BatchTime 0.349905   LR 0.002024
0.95496279
0.95504504
0.95523506
0.95527112
0.95519215
0.95536852
0.95498973
0.95498425
0.95502585
0.95500183
0.95504338
0.95496696
0.95496839
0.95489883
0.95478565
0.95484430
0.95491815
0.95494640
0.95507669
0.95523196
0.95547521
0.95543593
0.95556325
INFO - Training [5][  140/  196]   Loss 0.584039   Top1 80.094866   Top5 97.695312   BatchTime 0.354141   LR 0.001946
0.95542169
0.95543963
0.95525604
0.95517594
0.95513612
0.95519793
0.95505309
0.95494372
0.95514345
0.95544624
0.95575535
0.95559692
0.95554900
0.95552993
0.95549810
0.95556015
INFO - Training [5][  160/  196]   Loss 0.587042   Top1 79.960938   Top5 97.707520   BatchTime 0.354884   LR 0.001868
0.95549977
0.95559084
0.95563126
0.95561624
0.95556587
0.95562398
0.95569253
0.95541519
0.95553547
0.95531642
0.95510113
0.95501161
0.95468295
0.95386368
0.95387113
0.95511067
0.95497423
0.95476264
0.95506042
0.95530474
0.95530039
INFO - Training [5][  180/  196]   Loss 0.586003   Top1 80.008681   Top5 97.693142   BatchTime 0.356406   LR 0.001790
0.95520931
0.95509583
0.95500869
0.95508677
0.95517528
0.95521885
0.95508552
0.95498896
0.95491385
0.95480001
0.95481068
0.95489848
0.95493543
0.95484722
0.95480776
0.95483285
0.95496917
0.95515579
INFO - ==> Top1: 80.130    Top5: 97.692    Loss: 0.583
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95525795
0.95520759
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.458791   Top1 84.531250   Top5 99.257812   BatchTime 0.119751
INFO - Validation [5][   40/   40]   Loss 0.454969   Top1 84.550000   Top5 99.320000   BatchTime 0.087532
INFO - ==> Top1: 84.550    Top5: 99.320    Loss: 0.455
INFO - ==> Sparsity : 0.177
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 84.550   Top5: 99.320]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 84.500   Top5: 99.260]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 82.810   Top5: 99.300]
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.2090)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0712)
features.2.conv.0 tensor(0.0353)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0816)
features.3.conv.0 tensor(0.0205)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0627)
features.4.conv.0 tensor(0.0259)
features.4.conv.3 tensor(0.1007)
features.4.conv.6 tensor(0.0990)
features.5.conv.0 tensor(0.0399)
features.5.conv.3 tensor(0.0584)
features.5.conv.6 tensor(0.1064)
features.6.conv.0 tensor(0.0244)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0682)
features.7.conv.0 tensor(0.0484)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.1190)
features.8.conv.0 tensor(0.0887)
features.8.conv.3 tensor(0.1120)
features.8.conv.6 tensor(0.1307)
features.9.conv.0 tensor(0.0817)
features.9.conv.3 tensor(0.1418)
features.9.conv.6 tensor(0.1107)
features.10.conv.0 tensor(0.0304)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0875)
features.11.conv.0 tensor(0.1171)
features.11.conv.3 tensor(0.1184)
features.11.conv.6 tensor(0.2112)
features.12.conv.0 tensor(0.2635)
features.12.conv.3 tensor(0.1121)
features.12.conv.6 tensor(0.1710)
features.13.conv.0 tensor(0.0520)
features.13.conv.3 tensor(0.1806)
features.13.conv.6 tensor(0.0840)
features.14.conv.0 tensor(0.2065)
features.14.conv.3 tensor(0.0902)
features.14.conv.6 tensor(0.4893)
features.15.conv.0 tensor(0.0813)
features.15.conv.3 tensor(0.0769)
features.15.conv.6 tensor(0.3508)
features.16.conv.0 tensor(0.0524)
features.16.conv.3 tensor(0.1041)
features.16.conv.6 tensor(0.2765)
conv.0 tensor(0.0977)
tensor(387773.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.95502228
0.95490468
0.95482433
0.95484072
0.95503098
0.95508057
0.95512962
0.95509565
0.95526582
0.95540756
0.95542616
0.95543516
0.95550358
0.95562565
0.95558953
INFO - Training [6][   20/  196]   Loss 0.588356   Top1 79.765625   Top5 97.558594   BatchTime 0.437094   LR 0.001655
0.95565158
0.95556235
0.95566112
0.95557314
0.95557451
0.95572239
0.95533776
0.95531261
0.95541430
0.95555824
0.95555180
0.95546061
0.95529377
0.95522374
0.95526606
0.95519614
0.95550179
0.95534021
0.95529354
0.95546770
0.95548493
0.95557898
INFO - Training [6][   40/  196]   Loss 0.580159   Top1 79.941406   Top5 97.539062   BatchTime 0.398785   LR 0.001580
0.95567840
0.95561028
0.95560974
0.95573425
0.95577466
0.95551366
0.95568234
0.95587057
0.95582873
0.95586932
0.95587766
0.95590895
0.95591468
0.95569366
0.95539385
0.95521086
0.95525026
0.95539904
0.95543820
0.95558470
0.95535725
INFO - Training [6][   60/  196]   Loss 0.563744   Top1 80.625000   Top5 97.636719   BatchTime 0.390875   LR 0.001506
0.95517707
0.95516068
0.95530677
0.95527101
0.95558143
0.95547891
0.95531893
0.95506614
0.95519435
0.95527446
0.95533675
0.95531934
0.95529062
0.95560455
0.95552647
0.95563823
0.95555395
INFO - Training [6][   80/  196]   Loss 0.553489   Top1 80.805664   Top5 97.768555   BatchTime 0.381903   LR 0.001432
0.95551085
0.95547181
0.95544952
0.95540595
0.95538574
0.95546025
0.95553768
0.95545822
0.95547843
0.95548630
0.95529556
0.95545655
0.95537996
0.95540047
0.95551187
0.95541245
0.95535088
0.95527673
0.95525396
0.95541048
INFO - Training [6][  100/  196]   Loss 0.542126   Top1 81.218750   Top5 97.851562   BatchTime 0.365499   LR 0.001360
0.95540810
0.95532572
0.95526135
0.95526731
0.95547241
0.95552891
0.95556563
0.95554882
0.95567197
0.95553929
0.95562410
0.95550311
0.95542324
0.95544356
0.95530790
0.95532215
0.95542490
0.95543724
0.95561266
0.95551211
0.95574504
INFO - Training [6][  120/  196]   Loss 0.537309   Top1 81.455078   Top5 97.952474   BatchTime 0.369915   LR 0.001289
0.95566905
0.95550448
0.95542258
0.95529443
0.95528370
0.95515466
0.95512164
0.95513612
0.95510441
0.95513994
0.95504558
0.95497984
0.95538384
0.95540911
0.95516127
0.95507675
0.95527214
0.95532364
0.95543325
0.95536131
0.95538086
0.95555317
INFO - Training [6][  140/  196]   Loss 0.537946   Top1 81.473214   Top5 97.996652   BatchTime 0.369389   LR 0.001220
0.95557404
0.95566684
0.95588541
0.95573807
0.95557123
0.95552689
0.95550919
0.95564717
0.95556360
0.95528668
0.95522457
0.95519692
0.95541608
0.95555162
0.95554888
0.95563084
0.95565999
0.95562351
0.95563954
0.95567816
0.95546496
INFO - Training [6][  160/  196]   Loss 0.537774   Top1 81.464844   Top5 97.971191   BatchTime 0.370727   LR 0.001151
0.95543808
0.95549035
0.95548403
0.95549887
0.95546198
0.95559829
0.95555341
0.95572519
0.95594609
0.95564651
0.95562607
0.95562387
0.95569432
0.95564157
0.95545059
0.95548743
0.95555943
INFO - Training [6][  180/  196]   Loss 0.535772   Top1 81.486545   Top5 97.881944   BatchTime 0.370043   LR 0.001084
0.95563573
0.95551312
0.95547652
0.95557225
0.95553154
0.95544904
0.95547014
0.95530534
0.95531005
0.95524979
0.95540136
0.95587224
0.95593601
0.95598549
0.95595807
0.95563555
INFO - ==> Top1: 81.500    Top5: 97.894    Loss: 0.536
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95558828
0.95557821
0.95541775
0.95534199
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.412898   Top1 86.582031   Top5 99.296875   BatchTime 0.122505
INFO - Validation [6][   40/   40]   Loss 0.401779   Top1 86.590000   Top5 99.500000   BatchTime 0.092804
INFO - ==> Top1: 86.590    Top5: 99.500    Loss: 0.402
INFO - ==> Sparsity : 0.176
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 84.550   Top5: 99.320]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 84.500   Top5: 99.260]
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.2051)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0359)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0868)
features.3.conv.0 tensor(0.0211)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0625)
features.4.conv.0 tensor(0.0278)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0967)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0556)
features.5.conv.6 tensor(0.1094)
features.6.conv.0 tensor(0.0228)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0643)
features.7.conv.0 tensor(0.0448)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1171)
features.8.conv.0 tensor(0.0804)
features.8.conv.3 tensor(0.1091)
features.8.conv.6 tensor(0.1290)
features.9.conv.0 tensor(0.0771)
features.9.conv.3 tensor(0.1427)
features.9.conv.6 tensor(0.1102)
features.10.conv.0 tensor(0.0352)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.0855)
features.11.conv.0 tensor(0.1158)
features.11.conv.3 tensor(0.1235)
features.11.conv.6 tensor(0.2049)
features.12.conv.0 tensor(0.2653)
features.12.conv.3 tensor(0.1092)
features.12.conv.6 tensor(0.1659)
features.13.conv.0 tensor(0.0533)
features.13.conv.3 tensor(0.1786)
features.13.conv.6 tensor(0.0812)
features.14.conv.0 tensor(0.2082)
features.14.conv.3 tensor(0.0883)
features.14.conv.6 tensor(0.4927)
features.15.conv.0 tensor(0.0777)
features.15.conv.3 tensor(0.0738)
features.15.conv.6 tensor(0.3516)
features.16.conv.0 tensor(0.0503)
features.16.conv.3 tensor(0.1020)
features.16.conv.6 tensor(0.2824)
conv.0 tensor(0.0899)
tensor(385061.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.95551556
0.95553207
0.95568240
0.95572829
0.95601135
0.95596921
0.95599860
0.95569384
0.95560634
0.95561802
0.95559233
0.95558369
0.95563495
0.95612973
0.95580941
0.95565808
0.95565772
0.95572436
0.95576042
INFO - Training [7][   20/  196]   Loss 0.522886   Top1 81.503906   Top5 97.636719   BatchTime 0.450552   LR 0.000969
0.95586252
0.95582479
0.95590085
0.95589918
0.95591110
0.95591927
0.95570821
0.95570421
0.95562810
0.95571899
0.95586181
0.95577610
0.95575780
0.95574927
0.95566785
0.95580649
INFO - Training [7][   40/  196]   Loss 0.519403   Top1 81.865234   Top5 97.968750   BatchTime 0.407998   LR 0.000907
0.95598269
0.95581549
0.95565462
0.95570552
0.95561332
0.95569748
0.95583636
0.95560724
0.95568693
0.95579463
0.95594072
0.95606804
0.95581311
0.95565945
0.95591784
0.95573336
0.95567650
0.95565945
0.95566267
0.95546287
0.95544422
0.95553857
0.95567018
INFO - Training [7][   60/  196]   Loss 0.511440   Top1 82.298177   Top5 97.955729   BatchTime 0.386318   LR 0.000845
0.95579869
0.95581526
0.95595598
0.95580584
0.95581436
0.95582491
0.95558548
0.95544076
0.95538676
0.95536739
0.95537859
0.95547283
0.95567030
0.95549929
0.95547777
0.95550269
0.95540774
0.95545757
0.95550895
0.95551831
0.95552331
INFO - Training [7][   80/  196]   Loss 0.508347   Top1 82.280273   Top5 98.085938   BatchTime 0.362541   LR 0.000786
0.95539182
0.95527649
0.95513839
0.95541877
0.95563978
0.95577085
0.95563555
0.95555443
0.95567077
0.95550269
0.95530027
0.95544165
0.95554870
0.95555812
0.95564830
0.95597100
0.95601565
0.95580631
0.95599490
0.95596761
INFO - Training [7][  100/  196]   Loss 0.501622   Top1 82.402344   Top5 98.125000   BatchTime 0.367897   LR 0.000728
0.95596629
0.95599300
0.95579642
0.95570046
0.95574176
0.95560813
0.95562351
0.95572144
0.95585096
0.95572156
0.95557618
0.95561534
0.95563799
0.95577776
0.95577484
0.95608264
0.95566761
INFO - Training [7][  120/  196]   Loss 0.498359   Top1 82.591146   Top5 98.212891   BatchTime 0.369087   LR 0.000673
0.95539832
0.95543694
0.95550805
0.95547283
0.95545894
0.95546544
0.95542461
0.95547509
0.95557833
0.95565450
0.95561564
0.95578790
0.95581323
0.95563656
0.95552444
0.95563418
0.95538467
0.95525831
0.95517707
0.95522183
0.95544255
INFO - Training [7][  140/  196]   Loss 0.498819   Top1 82.566964   Top5 98.250558   BatchTime 0.368231   LR 0.000619
0.95563954
0.95561004
0.95561749
0.95560199
0.95559067
0.95563573
0.95556134
0.95569205
0.95593148
0.95596290
0.95584065
0.95566088
0.95556325
0.95537794
0.95546407
0.95532787
0.95532632
0.95533216
0.95534337
0.95534247
0.95531172
0.95525968
INFO - Training [7][  160/  196]   Loss 0.499913   Top1 82.551270   Top5 98.234863   BatchTime 0.369327   LR 0.000567
0.95520097
0.95525080
0.95539731
0.95546138
0.95564181
0.95541000
0.95543110
0.95529163
0.95526618
0.95523185
0.95505881
0.95513737
0.95507658
0.95515686
0.95532745
0.95571327
0.95598084
INFO - Training [7][  180/  196]   Loss 0.501711   Top1 82.539062   Top5 98.185764   BatchTime 0.368240   LR 0.000517
0.95603138
0.95587045
0.95569688
0.95556122
0.95549113
0.95537591
0.95533866
0.95546097
0.95553726
0.95561463
0.95552200
0.95553273
0.95564616
0.95571631
0.95581168
0.95601326
INFO - ==> Top1: 82.642    Top5: 98.204    Loss: 0.499
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95612651
0.95588440
0.95601702
0.95587021
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.499425   Top1 83.710938   Top5 98.886719   BatchTime 0.168645
INFO - Validation [7][   40/   40]   Loss 0.492437   Top1 83.750000   Top5 99.030000   BatchTime 0.112458
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0625)
features.2.conv.0 tensor(0.0307)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0883)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0614)
features.4.conv.0 tensor(0.0260)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0988)
features.5.conv.0 tensor(0.0439)
features.5.conv.3 tensor(0.0596)
features.5.conv.6 tensor(0.1076)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0659)
features.7.conv.0 tensor(0.0404)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1152)
features.8.conv.0 tensor(0.0752)
features.8.conv.3 tensor(0.1085)
features.8.conv.6 tensor(0.1264)
features.9.conv.0 tensor(0.0739)
features.9.conv.3 tensor(0.1453)
features.9.conv.6 tensor(0.1088)
features.10.conv.0 tensor(0.0324)
features.10.conv.3 tensor(0.1004)
INFO - ==> Top1: 83.750    Top5: 99.030    Loss: 0.492
INFO - ==> Sparsity : 0.176
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 84.550   Top5: 99.320]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 84.500   Top5: 99.260]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
features.10.conv.6 tensor(0.0849)
features.11.conv.0 tensor(0.1141)
features.11.conv.3 tensor(0.1225)
features.11.conv.6 tensor(0.2039)
features.12.conv.0 tensor(0.2692)
features.12.conv.3 tensor(0.1096)
features.12.conv.6 tensor(0.1648)
features.13.conv.0 tensor(0.0540)
features.13.conv.3 tensor(0.1819)
features.13.conv.6 tensor(0.0840)
features.14.conv.0 tensor(0.2122)
features.14.conv.3 tensor(0.0890)
features.14.conv.6 tensor(0.5037)
features.15.conv.0 tensor(0.0796)
features.15.conv.3 tensor(0.0726)
features.15.conv.6 tensor(0.3519)
features.16.conv.0 tensor(0.0500)
features.16.conv.3 tensor(0.1020)
features.16.conv.6 tensor(0.2768)
conv.0 tensor(0.0878)
tensor(384843.) 2188896.0
0.95565301
0.95562947
0.95569420
0.95555091
0.95552480
0.95571947
0.95562935
0.95574099
0.95614600
0.95599478
0.95601743
0.95587337
0.95585972
0.95582086
0.95579338
0.95577532
INFO - Training [8][   20/  196]   Loss 0.501600   Top1 82.539062   Top5 97.910156   BatchTime 0.448752   LR 0.000434
0.95589143
0.95586354
0.95602059
0.95599365
0.95604503
0.95610583
0.95585442
0.95549637
0.95553422
0.95562744
0.95542842
0.95552301
0.95558548
0.95572978
0.95583880
0.95601350
0.95586115
0.95569342
0.95574152
0.95591468
0.95568424
INFO - Training [8][   40/  196]   Loss 0.495571   Top1 82.812500   Top5 98.056641   BatchTime 0.413577   LR 0.000389
0.95567995
0.95568645
0.95582467
0.95588458
0.95574504
0.95592779
0.95571667
0.95584637
0.95590478
0.95553654
0.95536882
0.95530117
0.95563775
0.95572495
0.95590109
0.95589983
0.95585477
0.95582098
0.95581967
0.95563114
0.95577103
0.95558143
0.95549828
INFO - Training [8][   60/  196]   Loss 0.495668   Top1 82.825521   Top5 98.033854   BatchTime 0.391660   LR 0.000347
0.95549124
0.95554519
0.95570552
0.95585555
0.95577288
0.95563847
0.95541167
0.95544904
0.95557338
0.95562530
0.95574939
0.95578665
0.95548093
0.95543694
INFO - Training [8][   80/  196]   Loss 0.494184   Top1 82.729492   Top5 98.154297   BatchTime 0.364419   LR 0.000308
0.95553195
0.95551324
0.95558143
0.95572293
0.95591748
0.95596719
0.95577216
0.95552582
0.95528615
0.95530665
0.95530319
0.95530635
0.95530307
0.95524365
0.95546788
0.95567375
0.95575958
0.95577675
0.95582849
0.95579684
0.95573401
0.95545578
0.95575333
0.95562720
INFO - Training [8][  100/  196]   Loss 0.485922   Top1 83.011719   Top5 98.203125   BatchTime 0.365807   LR 0.000270
0.95585495
0.95585901
0.95565188
0.95557427
0.95562881
0.95551515
0.95556462
0.95556414
0.95557940
0.95564508
0.95562184
0.95585275
0.95578903
0.95554936
0.95539230
0.95539814
0.95539051
0.95538950
0.95563263
0.95589787
INFO - Training [8][  120/  196]   Loss 0.478787   Top1 83.238932   Top5 98.307292   BatchTime 0.368971   LR 0.000235
0.95594543
0.95583040
0.95582908
0.95600218
0.95606893
0.95604861
0.95581591
0.95582736
0.95589471
0.95552748
0.95541626
0.95545542
0.95536715
0.95525986
0.95523620
0.95512849
0.95522904
0.95556033
0.95591354
0.95597166
0.95572013
INFO - Training [8][  140/  196]   Loss 0.472439   Top1 83.526786   Top5 98.367746   BatchTime 0.369888   LR 0.000202
0.95552319
0.95554620
0.95562947
0.95541900
0.95532000
0.95542961
0.95540845
0.95571291
0.95559424
0.95567942
0.95552510
0.95543212
0.95539069
0.95546299
0.95539206
0.95542866
0.95533258
INFO - Training [8][  160/  196]   Loss 0.476108   Top1 83.356934   Top5 98.356934   BatchTime 0.369873   LR 0.000172
0.95522779
0.95540196
0.95538568
0.95534223
0.95508891
0.95520175
0.95482481
0.95461935
0.95448595
0.95427006
0.95398128
0.95402300
0.95410281
0.95413184
0.95389873
0.95370913
0.95363224
0.95352173
0.95364052
0.95376611
0.95368642
0.95376587
INFO - Training [8][  180/  196]   Loss 0.474571   Top1 83.348524   Top5 98.281250   BatchTime 0.368955   LR 0.000143
0.95376968
0.95363629
0.95370841
0.95375210
0.95378923
0.95378852
0.95398289
0.95400846
0.95417517
0.95391709
0.95384175
0.95387459
0.95376676
0.95380437
0.95369172
0.95383900
INFO - ==> Top1: 83.420    Top5: 98.304    Loss: 0.474
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95393282
0.95394337
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [8][   20/   40]   Loss 0.774238   Top1 74.667969   Top5 98.183594   BatchTime 0.121249
INFO - Validation [8][   40/   40]   Loss 0.777028   Top1 74.440000   Top5 98.300000   BatchTime 0.087580
INFO - ==> Top1: 74.440    Top5: 98.300    Loss: 0.777
INFO - ==> Sparsity : 0.177
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 84.550   Top5: 99.320]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 84.500   Top5: 99.260]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.2129)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0625)
features.2.conv.0 tensor(0.0286)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0880)
features.3.conv.0 tensor(0.0156)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0645)
features.4.conv.0 tensor(0.0270)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0965)
features.5.conv.0 tensor(0.0402)
features.5.conv.3 tensor(0.0579)
features.5.conv.6 tensor(0.1086)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0645)
features.7.conv.0 tensor(0.0410)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1136)
features.8.conv.0 tensor(0.0736)
features.8.conv.3 tensor(0.1117)
features.8.conv.6 tensor(0.1256)
features.9.conv.0 tensor(0.0741)
features.9.conv.3 tensor(0.1473)
features.9.conv.6 tensor(0.1100)
features.10.conv.0 tensor(0.0331)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0851)
features.11.conv.0 tensor(0.1133)
features.11.conv.3 tensor(0.1219)
features.11.conv.6 tensor(0.2042)
features.12.conv.0 tensor(0.2697)
features.12.conv.3 tensor(0.1086)
features.12.conv.6 tensor(0.1631)
features.13.conv.0 tensor(0.0547)
features.13.conv.3 tensor(0.1827)
features.13.conv.6 tensor(0.0845)
features.14.conv.0 tensor(0.2121)
features.14.conv.3 tensor(0.0892)
features.14.conv.6 tensor(0.5035)
features.15.conv.0 tensor(0.0815)
features.15.conv.3 tensor(0.0720)
features.15.conv.6 tensor(0.3474)
features.16.conv.0 tensor(0.0499)
features.16.conv.3 tensor(0.1019)
features.16.conv.6 tensor(0.2756)
conv.0 tensor(0.0962)
tensor(387411.) 2188896.0
0.95386213
0.95381951
0.95382690
0.95368105
0.95351857
0.95347965
0.95346886
0.95344251
0.95354295
0.95369923
0.95385718
0.95387495
0.95386302
0.95395166
0.95386606
0.95389229
0.95395607
INFO - Training [9][   20/  196]   Loss 0.490995   Top1 82.773438   Top5 97.519531   BatchTime 0.442594   LR 0.000100
0.95419616
0.95396411
0.95397705
0.95407498
0.95414138
0.95409822
0.95413321
0.95411950
0.95436555
0.95400196
0.95420861
0.95423597
0.95421803
0.95391935
0.95382744
0.95373178
0.95370930
0.95373863
0.95377755
0.95382667
0.95404154
INFO - Training [9][   40/  196]   Loss 0.491700   Top1 82.529297   Top5 97.792969   BatchTime 0.407834   LR 0.000079
0.95404863
0.95390636
0.95391542
0.95387763
0.95380670
0.95369315
0.95355427
0.95340896
0.95325607
0.95328701
0.95313197
0.95273083
0.95261598
0.95245093
0.95246190
0.95241231
0.95236778
0.95234454
0.95253730
0.95248568
0.95242244
0.95235568
INFO - Training [9][   60/  196]   Loss 0.484203   Top1 82.812500   Top5 98.020833   BatchTime 0.394727   LR 0.000060
0.95241815
0.95235974
0.95227981
0.95225525
0.95221025
0.95213115
0.95206410
0.95234555
0.95255917
0.95232648
0.95221239
0.95215875
0.95206439
0.95213497
0.95224088
0.95198542
0.95177919
0.95168430
0.95155561
0.95144922
INFO - Training [9][   80/  196]   Loss 0.482824   Top1 82.983398   Top5 98.095703   BatchTime 0.371776   LR 0.000044
0.95130867
0.95126814
0.95128024
0.95134670
0.95139313
0.95168775
0.95190454
0.95160830
0.95156556
0.95146906
0.95150286
0.95142633
0.95127535
0.95111454
0.95131356
0.95123595
0.95110726
0.95100135
0.95099908
0.95120877
0.95114136
INFO - Training [9][  100/  196]   Loss 0.475672   Top1 83.308594   Top5 98.199219   BatchTime 0.376161   LR 0.000030
0.95111400
0.95107228
0.95075583
0.95059609
0.95055187
0.95050114
0.95043820
0.95046616
0.95054334
0.95061135
0.95054817
0.95046729
0.95049995
0.95055145
0.95054066
INFO - Training [9][  120/  196]   Loss 0.468628   Top1 83.499349   Top5 98.297526   BatchTime 0.375887   LR 0.000019
0.95063132
0.95097262
0.95136219
0.95120770
0.95083517
0.95073217
0.95068043
0.95060307
0.95054168
0.95048088
0.95041710
0.95034343
0.95034456
0.95027989
0.95027608
0.95019948
0.95020902
0.95027035
0.95035642
0.95034140
0.95040131
0.95036846
INFO - Training [9][  140/  196]   Loss 0.466614   Top1 83.549107   Top5 98.351004   BatchTime 0.375923   LR 0.000010
0.95039481
0.95028228
0.95028955
0.95028484
0.95031190
0.95025790
0.95029640
0.95024288
0.95027429
0.95045483
0.95072311
0.95015872
0.95004231
0.95002288
0.94982105
0.94955218
0.94936144
0.94929695
0.94954783
0.94952077
0.94933254
INFO - Training [9][  160/  196]   Loss 0.468050   Top1 83.493652   Top5 98.347168   BatchTime 0.375377   LR 0.000004
0.94923538
0.94916648
0.94907135
0.94901299
0.94907212
0.94913685
0.94917184
0.94908321
0.94897360
0.94879669
0.94875056
0.94880271
0.94887823
0.94871086
0.94875449
0.94842899
0.94824982
INFO - Training [9][  180/  196]   Loss 0.467216   Top1 83.565538   Top5 98.289931   BatchTime 0.374281   LR 0.000001
0.94798619
0.94813246
0.94793433
0.94763410
0.94755095
0.94774282
0.94779992
0.94780779
0.94773787
0.94761926
0.94749695
0.94738030
0.94721133
0.94705468
0.94690663
0.94680876
INFO - ==> Top1: 83.578    Top5: 98.280    Loss: 0.468
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.94670206
0.94664347
0.94661027
0.94659287
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.386020   Top1 87.519531   Top5 99.355469   BatchTime 0.118915
INFO - Validation [9][   40/   40]   Loss 0.376680   Top1 87.580000   Top5 99.450000   BatchTime 0.085786
INFO - ==> Top1: 87.580    Top5: 99.450    Loss: 0.377
INFO - ==> Sparsity : 0.178
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 84.550   Top5: 99.320]
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.2129)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0616)
features.2.conv.0 tensor(0.0292)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0880)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0645)
features.4.conv.0 tensor(0.0270)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.0965)
features.5.conv.0 tensor(0.0402)
features.5.conv.3 tensor(0.0584)
features.5.conv.6 tensor(0.1094)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0646)
features.7.conv.0 tensor(0.0410)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.1139)
features.8.conv.0 tensor(0.0738)
features.8.conv.3 tensor(0.1105)
features.8.conv.6 tensor(0.1259)
features.9.conv.0 tensor(0.0736)
features.9.conv.3 tensor(0.1473)
features.9.conv.6 tensor(0.1099)
features.10.conv.0 tensor(0.0331)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0852)
features.11.conv.0 tensor(0.1135)
features.11.conv.3 tensor(0.1211)
features.11.conv.6 tensor(0.2033)
features.12.conv.0 tensor(0.2699)
features.12.conv.3 tensor(0.1080)
features.12.conv.6 tensor(0.1678)
features.13.conv.0 tensor(0.0551)
features.13.conv.3 tensor(0.1836)
features.13.conv.6 tensor(0.0847)
features.14.conv.0 tensor(0.2125)
features.14.conv.3 tensor(0.0890)
features.14.conv.6 tensor(0.5033)
features.15.conv.0 tensor(0.0815)
features.15.conv.3 tensor(0.0718)
features.15.conv.6 tensor(0.3503)
features.16.conv.0 tensor(0.0499)
features.16.conv.3 tensor(0.1019)
features.16.conv.6 tensor(0.2756)
conv.0 tensor(0.1009)
tensor(390061.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.94659692
0.95671809
0.95772797
0.95763016
0.95658785
0.95578903
0.95736247
0.95754802
0.95738029
0.95708489
0.95697087
0.95697337
0.95646423
0.95662647
0.95693588
0.95711082
0.95708263
INFO - Training [10][   20/  196]   Loss 0.517963   Top1 81.894531   Top5 97.714844   BatchTime 0.451520   LR 0.002500
0.95698905
0.95696348
0.95678890
0.95700502
0.95712382
0.95679027
0.95685285
0.95679742
0.95684117
0.95672315
0.95674080
0.95670354
0.95668143
0.95663756
0.95659906
0.95680481
0.95671260
0.95672381
0.95652062
0.95650685
0.95632309
INFO - Training [10][   40/  196]   Loss 0.533947   Top1 81.308594   Top5 97.812500   BatchTime 0.407435   LR 0.002499
0.95689458
0.95691210
0.95685971
0.95688921
0.95662534
0.95670903
0.95676470
0.95645726
0.95641381
0.95659447
0.95661885
0.95656037
0.95656675
0.95659983
0.95675659
0.95682591
0.95670980
0.95668143
0.95680475
0.95677882
INFO - Training [10][   60/  196]   Loss 0.546993   Top1 81.035156   Top5 97.825521   BatchTime 0.371411   LR 0.002499
0.95690829
0.95685142
0.95651489
0.95662218
0.95672673
0.95654345
0.95641351
0.95635581
0.95645523
0.95662445
0.95645165
0.95680392
0.95683032
0.95677811
0.95660692
0.95675343
0.95668399
0.95678735
0.95685691
0.95701319
0.95673370
INFO - Training [10][   80/  196]   Loss 0.573931   Top1 80.214844   Top5 97.226562   BatchTime 0.377123   LR 0.002497
0.95675367
0.95674235
0.95668751
0.95668155
0.95662206
0.95642608
0.95641440
0.95650607
0.95660377
0.95679599
0.95676446
0.95664191
0.95662934
0.95671362
0.95656723
0.95670372
0.95662916
0.95650381
0.95648158
0.95640665
0.95634907
INFO - Training [10][  100/  196]   Loss 0.566310   Top1 80.488281   Top5 97.398438   BatchTime 0.377738   LR 0.002496
0.95630634
0.95632440
0.95644033
0.95628875
0.95643580
0.95639557
0.95651680
0.95644087
0.95655644
0.95654118
0.95627683
0.95617831
0.95624679
0.95631814
0.95641553
0.95638901
INFO - Training [10][  120/  196]   Loss 0.563703   Top1 80.634766   Top5 97.522786   BatchTime 0.377894   LR 0.002494
0.95640522
0.95648777
0.95636749
0.95635021
0.95641935
0.95640248
0.95638704
0.95637184
0.95657092
0.95646578
0.95649159
0.95643944
0.95637167
0.95623833
0.95615107
0.95636636
0.95601803
0.95601809
0.95599037
0.95598376
0.95607644
0.95604932
INFO - Training [10][  140/  196]   Loss 0.561949   Top1 80.683594   Top5 97.653460   BatchTime 0.376375   LR 0.002492
0.95590794
0.95597547
0.95596987
0.95612627
0.95594800
0.95593119
0.95603055
0.95598829
0.95588630
0.95584255
0.95595777
0.95605469
0.95620906
0.95610374
0.95600241
0.95608586
0.95600849
0.95584959
0.95593846
0.95582896
0.95574725
INFO - Training [10][  160/  196]   Loss 0.562308   Top1 80.720215   Top5 97.663574   BatchTime 0.375833   LR 0.002490
0.95590949
0.95593578
0.95586711
0.95575637
0.95593476
0.95571989
0.95564234
0.95542753
0.95553297
0.95554519
0.95543355
0.95525348
0.95517933
0.95533574
0.95535523
0.95541626
0.95541406
INFO - Training [10][  180/  196]   Loss 0.562681   Top1 80.640191   Top5 97.615017   BatchTime 0.374466   LR 0.002487
0.95521140
0.95508891
0.95497447
0.95504761
0.95494944
0.95490974
0.95497727
0.95491016
0.95490581
0.95495695
0.95494217
0.95492798
0.95500922
0.95527434
0.95542562
0.95554316
INFO - ==> Top1: 80.610    Top5: 97.622    Loss: 0.563
0.95542258
0.95544893
0.95554721
0.95531589
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.572363   Top1 81.152344   Top5 98.671875   BatchTime 0.120091
INFO - Validation [10][   40/   40]   Loss 0.571272   Top1 80.930000   Top5 98.740000   BatchTime 0.088472
INFO - ==> Top1: 80.930    Top5: 98.740    Loss: 0.571
INFO - ==> Sparsity : 0.181
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 84.550   Top5: 99.320]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0221)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0690)
features.2.conv.0 tensor(0.0289)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0569)
features.4.conv.0 tensor(0.0299)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.0936)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.1043)
features.6.conv.0 tensor(0.0226)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0627)
features.7.conv.0 tensor(0.0499)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.0993)
features.8.conv.0 tensor(0.0651)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.1280)
features.9.conv.0 tensor(0.0764)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.0957)
features.10.conv.0 tensor(0.0396)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.0830)
features.11.conv.0 tensor(0.0942)
features.11.conv.3 tensor(0.1271)
features.11.conv.6 tensor(0.2167)
features.12.conv.0 tensor(0.2231)
features.12.conv.3 tensor(0.1128)
features.12.conv.6 tensor(0.1654)
features.13.conv.0 tensor(0.0559)
features.13.conv.3 tensor(0.1819)
features.13.conv.6 tensor(0.0876)
features.14.conv.0 tensor(0.2100)
features.14.conv.3 tensor(0.0890)
features.14.conv.6 tensor(0.5293)
features.15.conv.0 tensor(0.0824)
features.15.conv.3 tensor(0.0721)
features.15.conv.6 tensor(0.3393)
features.16.conv.0 tensor(0.0690)
features.16.conv.3 tensor(0.1053)
features.16.conv.6 tensor(0.2602)
conv.0 tensor(0.1223)
tensor(395863.) 2188896.0
0.95525879
0.95526552
0.95533901
0.95521241
0.95532215
0.95485091
0.95324886
0.95112967
0.94185805
0.92828840
0.93950921
0.95389533
0.95483387
0.95466107
0.95457011
0.95453221
0.95467085
0.95475322
INFO - Training [11][   20/  196]   Loss 0.564188   Top1 80.820312   Top5 97.031250   BatchTime 0.458285   LR 0.002481
0.95481175
0.95469362
0.95466119
0.95464337
0.95464653
0.95469248
0.95462763
0.95453691
0.95454383
0.95473212
0.95473975
0.95483714
0.95492756
0.95491904
0.95498300
0.95488113
0.95463830
0.95460576
0.95465380
0.95462978
0.95463294
0.95455527
0.95450795
INFO - Training [11][   40/  196]   Loss 0.564672   Top1 80.634766   Top5 97.431641   BatchTime 0.409809   LR 0.002478
0.95458895
0.95481378
0.95488000
0.95463139
0.95481640
0.95503438
0.95494807
0.95494688
0.95503718
0.95464480
0.95456910
0.95455104
0.95452410
0.95435131
INFO - Training [11][   60/  196]   Loss 0.563780   Top1 80.520833   Top5 97.643229   BatchTime 0.368773   LR 0.002474
0.95467323
0.95476466
0.95511311
0.95512325
0.95505482
0.95506185
0.95504153
0.95490700
0.95507407
0.95506626
0.95488220
0.95482814
0.95490605
0.95494258
0.95489377
0.95471728
0.95494562
0.95504683
0.95498186
0.95488405
0.95489007
0.95488256
0.95400554
INFO - Training [11][   80/  196]   Loss 0.565260   Top1 80.507812   Top5 97.719727   BatchTime 0.367164   LR 0.002470
0.95142031
0.95219177
0.95469666
0.95474970
0.95466381
0.95473796
0.95509225
0.95484185
0.95490211
0.95503145
0.95499802
0.95485240
0.95482951
0.95501059
0.95484757
0.95487505
0.95482278
0.95466477
0.95484972
0.95491320
0.95487618
INFO - Training [11][  100/  196]   Loss 0.556834   Top1 80.839844   Top5 97.750000   BatchTime 0.368200   LR 0.002465
0.95480907
0.95484823
0.95468098
0.95468205
0.95476687
0.95469946
0.95467401
0.95468205
0.95477241
0.95491558
0.95475441
0.95475221
0.95488715
0.95502228
0.95500600
0.95497245
0.95500296
0.95489019
0.95490605
0.95498592
0.95500422
0.95494229
INFO - Training [11][  120/  196]   Loss 0.551547   Top1 81.031901   Top5 97.858073   BatchTime 0.369399   LR 0.002460
0.95478576
0.95481783
0.95477450
0.95417899
0.95350403
0.95339739
0.95340055
0.95343488
0.95336682
0.95298904
0.95265567
0.95210582
0.95038402
0.95078176
0.95162964
0.95102578
INFO - Training [11][  140/  196]   Loss 0.555537   Top1 80.965402   Top5 97.890625   BatchTime 0.369035   LR 0.002455
0.95141739
0.95075810
0.95202076
0.95276934
0.95441282
0.95446664
0.95469308
0.95430148
0.95429856
0.95428008
0.95424831
0.95429569
0.95432419
0.95432013
0.95443308
0.95459211
0.95484591
0.95474041
0.95487314
0.95482749
0.95488918
0.95475072
INFO - Training [11][  160/  196]   Loss 0.558978   Top1 80.761719   Top5 97.907715   BatchTime 0.369660   LR 0.002450
0.95474672
0.95474648
0.95472807
0.95454001
0.95439214
0.95436341
0.95441484
0.95431936
0.95430601
0.95427018
0.95435780
0.95440674
0.95453507
0.95453185
0.95455337
0.95458692
0.95438230
0.95441765
0.95437396
0.95436883
0.95443994
INFO - Training [11][  180/  196]   Loss 0.557656   Top1 80.839844   Top5 97.849392   BatchTime 0.369657   LR 0.002444
0.95438880
0.95431215
0.95442444
0.95454419
0.95440149
0.95431596
0.95428693
0.95431900
0.95436859
0.95431566
0.95432979
0.95436633
INFO - ==> Top1: 80.830    Top5: 97.852    Loss: 0.557
0.95459771
0.95465022
0.95448887
0.95455998
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [11][   20/   40]   Loss 0.449047   Top1 85.292969   Top5 99.121094   BatchTime 0.121785
INFO - Validation [11][   40/   40]   Loss 0.445876   Top1 85.410000   Top5 99.250000   BatchTime 0.090332
INFO - ==> Top1: 85.410    Top5: 99.250    Loss: 0.446
INFO - ==> Sparsity : 0.182
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 85.410   Top5: 99.250]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1738)
features.1.conv.0 tensor(0.0221)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0677)
features.2.conv.0 tensor(0.0286)
features.2.conv.3 tensor(0.0687)
features.2.conv.6 tensor(0.0862)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0566)
features.4.conv.0 tensor(0.0226)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0905)
features.5.conv.0 tensor(0.0492)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1016)
features.6.conv.0 tensor(0.0264)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0619)
features.7.conv.0 tensor(0.0498)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.1101)
features.8.conv.0 tensor(0.0698)
features.8.conv.3 tensor(0.1186)
features.8.conv.6 tensor(0.1350)
features.9.conv.0 tensor(0.0886)
features.9.conv.3 tensor(0.1609)
features.9.conv.6 tensor(0.1069)
features.10.conv.0 tensor(0.0313)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0770)
features.11.conv.0 tensor(0.0923)
features.11.conv.3 tensor(0.1356)
features.11.conv.6 tensor(0.2168)
features.12.conv.0 tensor(0.2440)
features.12.conv.3 tensor(0.1125)
features.12.conv.6 tensor(0.1593)
features.13.conv.0 tensor(0.0573)
features.13.conv.3 tensor(0.1786)
features.13.conv.6 tensor(0.0861)
features.14.conv.0 tensor(0.1963)
features.14.conv.3 tensor(0.0897)
features.14.conv.6 tensor(0.5144)
features.15.conv.0 tensor(0.0920)
features.15.conv.3 tensor(0.0750)
features.15.conv.6 tensor(0.3491)
features.16.conv.0 tensor(0.0556)
features.16.conv.3 tensor(0.1106)
features.16.conv.6 tensor(0.3060)
conv.0 tensor(0.1014)
tensor(399418.) 2188896.0
0.95452714
0.95443714
0.95433199
0.95424896
0.95446104
0.95440918
0.95423508
0.95429277
0.95438915
0.95441854
0.95434624
0.95440674
0.95434964
0.95428723
0.95421398
0.95422870
0.95430923
0.95462835
INFO - Training [12][   20/  196]   Loss 0.556617   Top1 80.332031   Top5 97.304688   BatchTime 0.477032   LR 0.002433
0.95452285
0.95436633
0.95451975
0.95453811
0.95461601
0.95456874
0.95448875
0.95447856
0.95442706
0.95455116
0.95486027
0.95463437
0.95457965
0.95456684
0.95464593
0.95475948
0.95474792
0.95464414
0.95453238
0.95457637
INFO - Training [12][   40/  196]   Loss 0.556030   Top1 80.634766   Top5 97.607422   BatchTime 0.430270   LR 0.002426
0.95478034
0.95474064
0.95457023
0.95450503
0.95454562
0.95454699
0.95455056
0.95447487
0.95446563
0.95461679
0.95483565
0.95480090
0.95476556
0.95485181
0.95464504
0.95466727
0.95462114
0.95465457
0.95464730
INFO - Training [12][   60/  196]   Loss 0.552248   Top1 80.852865   Top5 97.688802   BatchTime 0.392158   LR 0.002419
0.95466435
0.95464772
0.95470315
0.95467132
0.95464551
0.95462674
0.95461184
0.95466459
0.95470417
0.95461690
0.95464569
0.95471406
0.95479369
0.95475119
0.95470577
0.95453870
0.95446283
0.95460266
0.95462763
INFO - Training [12][   80/  196]   Loss 0.550545   Top1 80.976562   Top5 97.783203   BatchTime 0.376279   LR 0.002412
0.95452523
0.95470768
0.95455348
0.95445746
0.95446217
0.95419514
0.95420152
0.95418370
0.95394057
0.95366335
0.95410335
0.95401055
0.95359766
0.95349187
0.95308304
0.95276237
0.95198452
0.95026171
0.94847244
0.94886065
INFO - Training [12][  100/  196]   Loss 0.542433   Top1 81.347656   Top5 97.855469   BatchTime 0.383647   LR 0.002404
0.95090485
0.95295042
0.95491546
0.95489120
0.95478517
0.95468891
0.95461899
0.95451087
0.95446420
0.95447785
0.95457321
0.95463943
0.95464754
0.95483637
0.95506805
0.95509046
0.95490646
0.95484656
0.95468408
0.95452929
0.95448011
INFO - Training [12][  120/  196]   Loss 0.538308   Top1 81.497396   Top5 97.958984   BatchTime 0.382419   LR 0.002396
0.95454496
0.95456082
0.95455956
0.95458180
0.95462865
0.95465225
0.95451647
0.95448011
0.95454544
0.95462978
0.95480323
0.95486790
0.95480257
0.95480525
0.95350164
0.94862008
0.93413508
0.95465010
0.95459151
0.95464861
0.95463192
0.95468801
INFO - Training [12][  140/  196]   Loss 0.538150   Top1 81.478795   Top5 98.013393   BatchTime 0.379579   LR 0.002388
0.95467341
0.95454025
0.95448023
0.95449764
0.95457458
0.95470876
0.95478261
0.95434237
0.95443636
0.95448440
0.95451200
0.95454150
0.95450324
0.95442557
0.95441538
0.95444936
0.95444131
0.95453411
0.95459771
0.95469564
0.95476800
INFO - Training [12][  160/  196]   Loss 0.539351   Top1 81.508789   Top5 97.990723   BatchTime 0.378610   LR 0.002380
0.95473254
0.95478183
0.95490444
0.95485532
0.95484185
0.95471305
0.95445567
0.95447093
0.95472956
0.95467502
0.95460546
0.95473582
0.95495933
0.95496434
0.95505953
0.95501977
0.95485902
INFO - Training [12][  180/  196]   Loss 0.540192   Top1 81.438802   Top5 97.929688   BatchTime 0.377407   LR 0.002371
0.95493966
0.95497602
0.95475042
0.95160049
0.94591522
0.93504536
0.92688525
0.92902100
0.93012863
0.92701745
0.92675048
0.92665380
0.92669773
0.92696160
0.92675120
0.92670721
0.92682385
INFO - ==> Top1: 81.538    Top5: 97.952    Loss: 0.538
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.92691714
0.92695463
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [12][   20/   40]   Loss 0.515171   Top1 82.734375   Top5 99.121094   BatchTime 0.127792
INFO - Validation [12][   40/   40]   Loss 0.523212   Top1 82.690000   Top5 99.160000   BatchTime 0.095055
INFO - ==> Top1: 82.690    Top5: 99.160    Loss: 0.523
INFO - ==> Sparsity : 0.240
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 85.410   Top5: 99.250]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1719)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0625)
features.2.conv.0 tensor(0.0315)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0786)
features.4.conv.0 tensor(0.0177)
features.4.conv.3 tensor(0.1036)
features.4.conv.6 tensor(0.0854)
features.5.conv.0 tensor(0.0381)
features.5.conv.3 tensor(0.0804)
features.5.conv.6 tensor(0.1045)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0594)
features.7.conv.0 tensor(0.0406)
features.7.conv.3 tensor(0.1108)
features.7.conv.6 tensor(0.1140)
features.8.conv.0 tensor(0.0705)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.1341)
features.9.conv.0 tensor(0.0734)
features.9.conv.3 tensor(0.1586)
features.9.conv.6 tensor(0.1022)
features.10.conv.0 tensor(0.0307)
features.10.conv.3 tensor(0.1045)
features.10.conv.6 tensor(0.0784)
features.11.conv.0 tensor(0.0904)
features.11.conv.3 tensor(0.1381)
features.11.conv.6 tensor(0.2097)
features.12.conv.0 tensor(0.2493)
features.12.conv.3 tensor(0.1119)
features.12.conv.6 tensor(0.1579)
features.13.conv.0 tensor(0.0612)
features.13.conv.3 tensor(0.1863)
features.13.conv.6 tensor(0.0873)
features.14.conv.0 tensor(0.2130)
features.14.conv.3 tensor(0.0914)
features.14.conv.6 tensor(0.5524)
features.15.conv.0 tensor(0.9856)
features.15.conv.3 tensor(0.0693)
features.15.conv.6 tensor(0.3196)
features.16.conv.0 tensor(0.0786)
features.16.conv.3 tensor(0.1097)
features.16.conv.6 tensor(0.2824)
conv.0 tensor(0.0773)
tensor(526395.) 2188896.0
0.92707080
0.92700541
0.92698431
0.92725098
0.92723334
0.92721188
0.92728972
0.92721033
0.92729342
0.92724240
0.92721981
0.92725587
0.92731667
0.92742628
0.92801780
0.92862451
INFO - Training [13][   20/  196]   Loss 0.536611   Top1 81.738281   Top5 97.480469   BatchTime 0.453824   LR 0.002355
0.92918962
0.92951542
0.92966729
0.93038827
0.93113035
0.93169338
0.93266064
0.93332750
0.93393779
0.93455541
0.93435919
0.93444341
0.93446779
0.93455333
0.93511873
0.93522686
0.93528122
0.93559128
0.93595695
0.93628198
0.93659782
INFO - Training [13][   40/  196]   Loss 0.547074   Top1 81.269531   Top5 97.656250   BatchTime 0.413492   LR 0.002345
0.93672216
0.93692970
0.93721020
0.93741554
0.93748832
0.93750542
0.93763214
0.93768930
0.93775558
0.93775421
0.93770742
0.93746513
0.93737137
0.93757224
0.93773437
0.93779224
0.93790966
0.93760353
0.93721735
0.93702948
0.93693602
0.93677837
0.93677998
0.93671030
INFO - Training [13][   60/  196]   Loss 0.531500   Top1 81.731771   Top5 97.792969   BatchTime 0.386525   LR 0.002336
0.93741852
0.93729925
0.93760872
0.93777168
0.93797630
0.93810558
0.93807977
0.93808270
0.93835181
0.93860400
0.93874079
0.93864769
0.93844497
0.93793690
INFO - Training [13][   80/  196]   Loss 0.533389   Top1 81.582031   Top5 97.875977   BatchTime 0.363100   LR 0.002325
0.93813974
0.93807310
0.93785143
0.93768078
0.93768269
0.93773055
0.93749899
0.93746185
0.93744618
0.93730557
0.93722188
0.93719888
0.93752015
0.93744874
0.93731266
0.93723732
0.93709809
0.93711478
0.93715554
0.93712384
0.93717152
0.93722188
0.93710732
0.93680698
0.93649250
INFO - Training [13][  100/  196]   Loss 0.528688   Top1 81.667969   Top5 97.914062   BatchTime 0.354452   LR 0.002315
0.93630463
0.93640393
0.93665707
0.93649411
0.93663514
0.93653452
0.93638366
0.93650419
0.93649197
0.93652880
0.93690199
0.93693733
0.93695104
0.93694943
0.93732309
0.93743390
INFO - Training [13][  120/  196]   Loss 0.528830   Top1 81.663411   Top5 97.975260   BatchTime 0.357479   LR 0.002304
0.93758637
0.93796724
0.93794543
0.93780369
0.93793535
0.93792850
0.93786979
0.93792725
0.93778712
0.93719953
0.93755490
0.93755442
0.93734777
0.93779081
0.93814987
0.93827194
0.93817610
0.93805087
0.93795371
0.93801785
0.93820089
0.93807781
0.93794608
INFO - Training [13][  140/  196]   Loss 0.529205   Top1 81.623884   Top5 98.046875   BatchTime 0.358072   LR 0.002293
0.93799871
0.93780464
0.93768549
0.93764246
0.93736786
0.93720436
0.93712819
0.93708092
0.93694472
0.93669713
0.93648636
0.93657422
0.93647510
0.93645304
0.93631864
0.93597549
0.93553597
0.93491423
0.93421459
0.93317199
0.93116701
INFO - Training [13][  160/  196]   Loss 0.530151   Top1 81.596680   Top5 98.076172   BatchTime 0.360497   LR 0.002282
0.92144787
0.92070341
0.92897213
0.93255514
0.93516976
0.93539935
0.93535942
0.93531811
0.93550467
0.93516880
0.93510240
0.93502909
0.93512702
0.93512112
0.93506056
0.93524575
INFO - Training [13][  180/  196]   Loss 0.529692   Top1 81.612413   Top5 98.020833   BatchTime 0.361710   LR 0.002271
0.93547225
0.93557346
0.93548781
0.93536997
0.93549287
0.93555826
0.93547213
0.93532461
0.93528664
0.93521392
0.93518281
0.93531203
0.93525720
0.93520069
0.93533492
0.93547034
INFO - ==> Top1: 81.724    Top5: 98.002    Loss: 0.528
0.93570703
0.93572283
0.93561691
0.93562984
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.407291   Top1 86.796875   Top5 99.375000   BatchTime 0.121900
INFO - Validation [13][   40/   40]   Loss 0.395959   Top1 86.630000   Top5 99.450000   BatchTime 0.089319
INFO - ==> Top1: 86.630    Top5: 99.450    Loss: 0.396
INFO - ==> Sparsity : 0.223
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 86.630   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1719)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0556)
features.2.conv.0 tensor(0.0315)
features.2.conv.3 tensor(0.0702)
features.2.conv.6 tensor(0.0813)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0532)
features.4.conv.0 tensor(0.0229)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0868)
features.5.conv.0 tensor(0.0358)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.1076)
features.6.conv.0 tensor(0.0244)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0567)
features.7.conv.0 tensor(0.0362)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.1122)
features.8.conv.0 tensor(0.0741)
features.8.conv.3 tensor(0.1264)
features.8.conv.6 tensor(0.1311)
features.9.conv.0 tensor(0.0693)
features.9.conv.3 tensor(0.1589)
features.9.conv.6 tensor(0.1043)
features.10.conv.0 tensor(0.0358)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0812)
features.11.conv.0 tensor(0.0939)
features.11.conv.3 tensor(0.1337)
features.11.conv.6 tensor(0.2153)
features.12.conv.0 tensor(0.2465)
features.12.conv.3 tensor(0.1225)
features.12.conv.6 tensor(0.1875)
features.13.conv.0 tensor(0.0587)
features.13.conv.3 tensor(0.1879)
features.13.conv.6 tensor(0.0945)
features.14.conv.0 tensor(0.2452)
features.14.conv.3 tensor(0.0949)
features.14.conv.6 tensor(0.4718)
features.15.conv.0 tensor(0.6631)
features.15.conv.3 tensor(0.0836)
features.15.conv.6 tensor(0.3679)
features.16.conv.0 tensor(0.0665)
features.16.conv.3 tensor(0.1109)
features.16.conv.6 tensor(0.2833)
conv.0 tensor(0.1028)
tensor(488342.) 2188896.0
0.93561631
0.93544269
0.93529832
0.93534470
0.93530667
0.93509346
0.93535817
0.93492621
0.93469286
0.93420327
0.93367380
0.93353575
0.93297511
0.93198752
0.93054295
0.92777681
0.92531866
0.92203760
0.92456144
INFO - Training [14][   20/  196]   Loss 0.512762   Top1 81.796875   Top5 97.617188   BatchTime 0.461522   LR 0.002250
0.92414194
0.92767918
0.93074608
0.93258137
0.93346518
0.93428510
0.93461144
0.93450421
0.93435806
0.93428856
0.93407059
0.93390137
0.93402481
0.93383205
0.93402672
0.93406069
0.93403286
0.93400782
0.93380350
0.93401235
0.93404132
INFO - Training [14][   40/  196]   Loss 0.523407   Top1 81.523438   Top5 97.822266   BatchTime 0.414627   LR 0.002238
0.93384296
0.93379366
0.93376970
0.93370497
0.93361092
0.93359399
0.93358600
0.93379110
0.93381268
0.93359822
0.93355900
0.93347508
0.93350011
0.93336868
0.93301004
0.93302351
INFO - Training [14][   60/  196]   Loss 0.517091   Top1 81.842448   Top5 97.988281   BatchTime 0.400102   LR 0.002225
0.93298334
0.93282169
0.93272787
0.93273008
0.93268281
0.93258584
0.93253255
0.93247038
0.93263829
0.93257385
0.93264174
0.93265420
0.93298519
0.93304694
0.93287587
0.93287045
0.93311846
0.93318504
0.93318784
0.93354762
0.93369329
0.93368173
INFO - Training [14][   80/  196]   Loss 0.514132   Top1 82.021484   Top5 98.076172   BatchTime 0.391048   LR 0.002213
0.93378168
0.93402570
0.93393528
0.93367827
0.93373454
0.93369335
0.93390328
0.93392467
0.93393815
0.93409449
0.93410331
0.93401933
0.93406945
0.93422890
0.93402696
0.93412763
0.93417412
0.93436652
0.93456078
0.93454719
0.93464160
0.93457276
0.93445295
INFO - Training [14][  100/  196]   Loss 0.508854   Top1 82.289062   Top5 98.121094   BatchTime 0.384675   LR 0.002200
0.93425834
0.93355632
0.92788035
0.91547889
0.92574555
0.93358380
0.93427932
0.93432492
0.93430573
0.93430692
0.93431330
0.93420446
0.93414581
0.93396276
0.93390262
INFO - Training [14][  120/  196]   Loss 0.503825   Top1 82.454427   Top5 98.206380   BatchTime 0.366930   LR 0.002186
0.93384415
0.93377167
0.93370831
0.93364370
0.93352610
0.93372130
0.93369901
0.93339825
0.93339854
0.93337101
0.93301171
0.93293405
0.93266129
0.93251991
0.93245012
0.93236387
0.93225670
0.93214470
0.93196136
0.93190891
0.93174255
0.93169826
INFO - Training [14][  140/  196]   Loss 0.504416   Top1 82.488839   Top5 98.261719   BatchTime 0.367059   LR 0.002173
0.93179882
0.93168926
0.93138719
0.93151498
0.93164277
0.93162650
0.93154109
0.93152529
0.93146539
0.93152708
0.93169934
0.93186241
0.93189454
0.93210793
0.93197215
0.93216568
0.93221968
0.93227065
0.93227404
0.93222195
0.93230236
0.93226683
INFO - Training [14][  160/  196]   Loss 0.506122   Top1 82.426758   Top5 98.215332   BatchTime 0.367416   LR 0.002159
0.93205720
0.93237430
0.93232423
0.93238968
0.93241584
0.93222225
0.93236369
0.93241519
0.93250799
0.93283170
0.93285435
0.93290567
0.93295312
0.93307823
0.93284249
0.93294454
INFO - Training [14][  180/  196]   Loss 0.507927   Top1 82.348090   Top5 98.142361   BatchTime 0.367193   LR 0.002145
0.93300104
0.93300092
0.93308449
0.93308061
0.93303192
0.93311119
0.93296176
0.93299425
0.93306851
0.93276697
0.93257010
0.93266624
0.93274570
0.93286180
0.93269914
0.93263620
0.93275499
INFO - ==> Top1: 82.416    Top5: 98.120    Loss: 0.506
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.93273216
0.93269253
0.93270820
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 0.450185   Top1 84.921875   Top5 99.257812   BatchTime 0.129363
INFO - Validation [14][   40/   40]   Loss 0.450278   Top1 85.040000   Top5 99.310000   BatchTime 0.091536
INFO - ==> Top1: 85.040    Top5: 99.310    Loss: 0.450
INFO - ==> Sparsity : 0.230
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 86.630   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1758)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0564)
features.2.conv.0 tensor(0.0321)
features.2.conv.3 tensor(0.0764)
features.2.conv.6 tensor(0.0816)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0527)
features.4.conv.0 tensor(0.0249)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0869)
features.5.conv.0 tensor(0.0365)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.1045)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0597)
features.7.conv.0 tensor(0.0408)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.1156)
features.8.conv.0 tensor(0.0677)
features.8.conv.3 tensor(0.1253)
features.8.conv.6 tensor(0.1276)
features.9.conv.0 tensor(0.0733)
features.9.conv.3 tensor(0.1577)
features.9.conv.6 tensor(0.1032)
features.10.conv.0 tensor(0.0328)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0792)
features.11.conv.0 tensor(0.0944)
features.11.conv.3 tensor(0.1408)
features.11.conv.6 tensor(0.2090)
features.12.conv.0 tensor(0.2478)
features.12.conv.3 tensor(0.1173)
features.12.conv.6 tensor(0.1853)
features.13.conv.0 tensor(0.0607)
features.13.conv.3 tensor(0.1869)
features.13.conv.6 tensor(0.1011)
features.14.conv.0 tensor(0.2672)
features.14.conv.3 tensor(0.0934)
features.14.conv.6 tensor(0.5091)
features.15.conv.0 tensor(0.6940)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.4147)
features.16.conv.0 tensor(0.0708)
features.16.conv.3 tensor(0.1089)
features.16.conv.6 tensor(0.2726)
conv.0 tensor(0.0920)
tensor(502564.) 2188896.0
0.93272793
0.93292511
0.93254012
0.93252164
0.93260038
0.93257040
0.93251264
0.93245620
0.93245429
0.93264037
0.93233567
0.93211019
0.93209022
0.93206501
0.93212247
0.93185997
0.93182904
0.93176514
INFO - Training [15][   20/  196]   Loss 0.522956   Top1 81.855469   Top5 97.675781   BatchTime 0.485490   LR 0.002120
0.93183428
0.93184626
0.93159753
0.93165791
0.93148810
0.93170744
0.93185830
0.93208706
0.93229371
0.93233234
0.93222845
0.93202144
0.93201500
0.93189532
0.93178087
0.93166339
0.93163788
0.93156672
0.93156147
0.93171072
0.93151861
0.93158573
INFO - Training [15][   40/  196]   Loss 0.563451   Top1 80.087891   Top5 96.748047   BatchTime 0.426464   LR 0.002106
0.93154001
0.93161362
0.93158567
0.93162841
0.93167329
0.93145072
0.93124944
0.93108582
0.93099821
0.93095368
0.93121183
0.93114758
0.93115801
0.93114066
0.93120676
0.93119580
INFO - Training [15][   60/  196]   Loss 0.548002   Top1 80.781250   Top5 97.180990   BatchTime 0.407845   LR 0.002091
0.93106055
0.93086964
0.93113667
0.93103886
0.93081909
0.93076313
0.93091762
0.93082589
0.93075448
0.93034208
0.93025577
0.93013322
0.92983013
0.92934000
0.92970395
0.92948377
0.92922586
0.92887223
0.92884868
0.92882514
0.92868763
0.92860472
INFO - Training [15][   80/  196]   Loss 0.537687   Top1 81.191406   Top5 97.524414   BatchTime 0.399645   LR 0.002076
0.92851788
0.92828709
0.92795473
0.92748791
0.92672628
0.92573643
0.92488754
0.92394149
0.92263478
0.92141241
0.91953903
0.91830045
0.91779548
0.91648555
0.91517103
0.91350567
0.91037565
0.90652919
0.90615261
0.90632373
INFO - Training [15][  100/  196]   Loss 0.519075   Top1 81.753906   Top5 97.699219   BatchTime 0.398325   LR 0.002061
0.90695971
0.90692180
0.90587306
0.90503949
0.90523696
0.90504062
0.90529972
0.90474534
0.90481335
0.90459210
0.90411544
0.90383226
0.90417767
0.90452373
0.90446591
0.90382797
0.90318823
0.90286303
0.90262729
0.90268975
0.90270907
INFO - Training [15][  120/  196]   Loss 0.513213   Top1 82.021484   Top5 97.858073   BatchTime 0.380038   LR 0.002045
0.90267551
0.90264213
0.90257448
0.90271878
0.90251726
0.90255445
0.90247458
0.90239745
0.90260416
0.90246278
0.90241957
0.90239477
0.90264928
0.90222585
0.90214527
0.90241867
0.90281808
0.90263736
INFO - Training [15][  140/  196]   Loss 0.509251   Top1 82.142857   Top5 97.968750   BatchTime 0.373266   LR 0.002030
0.90247750
0.90260422
0.90274519
0.90268403
0.90317702
0.90343618
0.90403110
0.90462726
0.90541917
0.90633279
0.90788287
0.90977335
0.91056675
0.91238427
0.91481960
0.91690916
0.91986519
0.92288816
0.92493391
0.92622733
0.92772275
INFO - Training [15][  160/  196]   Loss 0.508816   Top1 82.158203   Top5 97.956543   BatchTime 0.372132   LR 0.002014
0.92862314
0.92878193
0.92889345
0.92857778
0.92810428
0.92791343
0.92792815
0.92813694
0.92835671
0.92859143
0.92876762
0.92882401
0.92889941
0.92903954
0.92912412
0.92928463
0.92931223
0.92937136
INFO - Training [15][  180/  196]   Loss 0.506452   Top1 82.287326   Top5 97.903646   BatchTime 0.370491   LR 0.001998
0.92929149
0.92909980
0.92907041
0.92888874
0.92880744
0.92868489
0.92876989
0.92847025
0.92865807
0.92932802
0.92944115
0.92967337
0.92990428
0.92987692
0.92967987
0.92980444
INFO - ==> Top1: 82.386    Top5: 97.948    Loss: 0.504
0.92995495
0.93013090
0.93014866
0.93019331
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.410619   Top1 86.289062   Top5 99.335938   BatchTime 0.126526
INFO - Validation [15][   40/   40]   Loss 0.400925   Top1 86.320000   Top5 99.530000   BatchTime 0.093336
INFO - ==> Top1: 86.320    Top5: 99.530    Loss: 0.401
INFO - ==> Sparsity : 0.235
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 86.630   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 86.590   Top5: 99.500]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0590)
features.2.conv.0 tensor(0.0252)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0859)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0503)
features.4.conv.0 tensor(0.0171)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0373)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.1017)
features.6.conv.0 tensor(0.0192)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0560)
features.7.conv.0 tensor(0.0481)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.1169)
features.8.conv.0 tensor(0.0664)
features.8.conv.3 tensor(0.1175)
features.8.conv.6 tensor(0.1292)
features.9.conv.0 tensor(0.0712)
features.9.conv.3 tensor(0.1623)
features.9.conv.6 tensor(0.0986)
features.10.conv.0 tensor(0.0353)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0769)
features.11.conv.0 tensor(0.0853)
features.11.conv.3 tensor(0.1476)
features.11.conv.6 tensor(0.2098)
features.12.conv.0 tensor(0.2513)
features.12.conv.3 tensor(0.1217)
features.12.conv.6 tensor(0.1933)
features.13.conv.0 tensor(0.0599)
features.13.conv.3 tensor(0.1867)
features.13.conv.6 tensor(0.1026)
features.14.conv.0 tensor(0.3181)
features.14.conv.3 tensor(0.0970)
features.14.conv.6 tensor(0.5068)
features.15.conv.0 tensor(0.7330)
features.15.conv.3 tensor(0.0855)
features.15.conv.6 tensor(0.3820)
features.16.conv.0 tensor(0.0718)
features.16.conv.3 tensor(0.1130)
features.16.conv.6 tensor(0.2796)
conv.0 tensor(0.0935)
tensor(514240.) 2188896.0
0.92998481
0.93000257
0.93005270
0.93001485
0.92993587
0.93009287
0.93020505
0.93010557
0.93019754
0.93023795
0.92994028
0.92975056
0.92987198
0.92984599
0.92992640
0.92997891
0.93011600
0.93006909
0.93011284
INFO - Training [16][   20/  196]   Loss 0.489181   Top1 83.261719   Top5 97.949219   BatchTime 0.445562   LR 0.001969
0.92996246
0.93031538
0.93043250
0.93049085
0.93051422
0.93062264
0.93058687
0.93072927
0.93089700
0.93095070
0.93099070
0.93105930
0.93108618
0.93101567
0.93076873
0.93079096
0.93095452
0.93101770
0.93106151
0.93130356
0.93144190
0.93161750
INFO - Training [16][   40/  196]   Loss 0.486889   Top1 83.203125   Top5 98.115234   BatchTime 0.411014   LR 0.001953
0.93150902
0.93147689
0.93153071
0.93153071
0.93148923
0.93161225
0.93180710
0.93166709
0.93173534
0.93188053
0.93216014
0.93268347
0.93280894
0.93291205
0.93296933
0.93298596
INFO - Training [16][   60/  196]   Loss 0.480535   Top1 83.229167   Top5 98.216146   BatchTime 0.398080   LR 0.001936
0.93306386
0.93318123
0.93301255
0.93314916
0.93311536
0.93310457
0.93299574
0.93291950
0.93326527
0.93282396
0.93276876
0.93278503
0.93280709
0.93271828
0.93254977
0.93260294
0.93241209
0.93227273
0.93219328
0.93232197
0.93203169
INFO - Training [16][   80/  196]   Loss 0.483181   Top1 83.300781   Top5 98.247070   BatchTime 0.393561   LR 0.001919
0.93221539
0.93219948
0.93215102
0.93202746
0.93196893
0.93187529
0.93180329
0.93155235
0.93192863
0.93212861
0.93212467
0.93210876
0.93220139
0.93219489
0.93219310
0.93206614
0.93242902
0.93236047
0.93229353
0.93253231
0.93270034
INFO - Training [16][  100/  196]   Loss 0.476425   Top1 83.515625   Top5 98.269531   BatchTime 0.391748   LR 0.001902
0.93266338
0.93257713
0.93254524
0.93272740
0.93282920
0.93256360
0.93257147
0.93247372
0.93245536
0.93227202
0.93217289
0.93203247
0.93210506
0.93215144
0.93185335
0.93178439
0.93189508
0.93173969
INFO - Training [16][  120/  196]   Loss 0.469690   Top1 83.785807   Top5 98.349609   BatchTime 0.380242   LR 0.001885
0.93163282
0.93152004
0.93140852
0.93136764
0.93125290
0.93124479
0.93098825
0.93095905
0.93080276
0.93065560
0.93089342
0.93079334
0.93066752
0.93054378
0.93033612
0.93015683
0.93020594
0.93031365
0.93055391
0.93086523
0.93088943
0.93098789
0.93080395
0.93061167
INFO - Training [16][  140/  196]   Loss 0.467008   Top1 83.842076   Top5 98.370536   BatchTime 0.374390   LR 0.001867
0.93031168
0.92797416
0.92385310
0.92736328
0.92961884
0.93049037
0.93051869
0.93040317
0.93034387
0.93025035
0.93020904
0.93013954
0.92991847
0.93002212
0.92975688
0.92960519
INFO - Training [16][  160/  196]   Loss 0.468821   Top1 83.842773   Top5 98.354492   BatchTime 0.373315   LR 0.001850
0.92965341
0.92954570
0.92965549
0.92976719
0.93000340
0.92991602
0.92986959
0.92989022
0.92975551
0.92973483
0.93004787
0.93070346
0.93075532
0.93069804
0.93099535
0.93098909
0.93096393
0.93049580
0.93070209
0.93099976
0.93087125
0.93110532
INFO - Training [16][  180/  196]   Loss 0.468975   Top1 83.843316   Top5 98.281250   BatchTime 0.372875   LR 0.001832
0.93103850
0.93082231
0.93064684
0.93074220
0.93086976
0.93099666
0.93038338
0.93056434
0.93031555
0.93037438
0.93040466
0.93013865
0.93011922
0.93015492
0.93011940
0.93005753
INFO - ==> Top1: 83.872    Top5: 98.292    Loss: 0.469
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.93007356
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [16][   20/   40]   Loss 0.391671   Top1 87.128906   Top5 99.570312   BatchTime 0.129019
INFO - Validation [16][   40/   40]   Loss 0.385998   Top1 86.890000   Top5 99.660000   BatchTime 0.092817
INFO - ==> Top1: 86.890    Top5: 99.660    Loss: 0.386
INFO - ==> Sparsity : 0.238
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 86.890   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 86.630   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.1065)
features.1.conv.6 tensor(0.0595)
features.2.conv.0 tensor(0.0234)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0880)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0493)
features.4.conv.0 tensor(0.0229)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0920)
features.5.conv.0 tensor(0.0376)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.1051)
features.6.conv.0 tensor(0.0244)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0532)
features.7.conv.0 tensor(0.0479)
features.7.conv.3 tensor(0.1155)
features.7.conv.6 tensor(0.1141)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1183)
features.8.conv.6 tensor(0.1254)
features.9.conv.0 tensor(0.0580)
features.9.conv.3 tensor(0.1617)
features.9.conv.6 tensor(0.1002)
features.10.conv.0 tensor(0.0364)
features.10.conv.3 tensor(0.1047)
features.10.conv.6 tensor(0.0726)
features.11.conv.0 tensor(0.1048)
features.11.conv.3 tensor(0.1464)
features.11.conv.6 tensor(0.2024)
features.12.conv.0 tensor(0.2520)
features.12.conv.3 tensor(0.1177)
features.12.conv.6 tensor(0.1923)
features.13.conv.0 tensor(0.0543)
features.13.conv.3 tensor(0.1877)
features.13.conv.6 tensor(0.0990)
features.14.conv.0 tensor(0.3426)
features.14.conv.3 tensor(0.0976)
features.14.conv.6 tensor(0.4931)
features.15.conv.0 tensor(0.7417)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.4024)
features.16.conv.0 tensor(0.0799)
features.16.conv.3 tensor(0.1153)
features.16.conv.6 tensor(0.2844)
conv.0 tensor(0.0917)
tensor(521804.) 2188896.0
0.93002194
0.92992783
0.92995143
0.92991024
0.92985302
0.92983538
0.92965293
0.92975742
0.92990613
0.92980373
0.92983586
0.92980218
0.92955780
0.92969024
0.92938131
0.92930925
INFO - Training [17][   20/  196]   Loss 0.500389   Top1 82.246094   Top5 97.714844   BatchTime 0.451877   LR 0.001800
0.92938077
0.92920673
0.92930180
0.92928034
0.92936575
0.92901289
0.92908436
0.92907810
0.92890823
0.92882067
0.92873460
0.92869127
0.92847097
0.92859232
0.92854184
0.92822009
0.92820632
0.92833656
0.92815000
0.92814261
0.92802948
0.92790842
INFO - Training [17][   40/  196]   Loss 0.488721   Top1 82.910156   Top5 97.910156   BatchTime 0.411632   LR 0.001782
0.92768234
0.92757255
0.92749649
0.92718083
0.92696619
0.92635417
0.92616946
0.92598087
0.92556715
0.92541045
0.92519176
0.92482781
0.92454731
0.92407185
0.92417902
0.92379808
0.92308289
0.92241412
0.92162561
0.92095667
0.92054284
0.92003113
INFO - Training [17][   60/  196]   Loss 0.484718   Top1 83.118490   Top5 97.988281   BatchTime 0.397442   LR 0.001764
0.91986901
0.91977400
0.91893029
0.91842169
0.91819161
0.91829747
0.91837037
0.91818386
0.91809118
0.91780430
0.91730899
0.91653085
0.91642815
0.91646069
0.91561359
0.91501653
0.91432023
0.91455621
0.91603720
0.91672683
0.91677147
INFO - Training [17][   80/  196]   Loss 0.479648   Top1 83.295898   Top5 98.100586   BatchTime 0.392687   LR 0.001746
0.91650146
0.91612470
0.91571724
0.91554761
0.91504246
0.91438901
0.91414189
0.91402119
0.91485745
0.91553038
0.91567791
0.91590446
0.91576511
0.91562694
0.91549796
0.91531867
INFO - Training [17][  100/  196]   Loss 0.472954   Top1 83.550781   Top5 98.117188   BatchTime 0.386579   LR 0.001727
0.91510260
0.91513836
0.91524255
0.91535258
0.91498053
0.91461015
0.91425169
0.91399395
0.91361368
0.91333753
0.91329920
0.91327697
0.91314483
0.91279185
0.91238296
0.91223490
0.91267192
0.91292709
INFO - Training [17][  120/  196]   Loss 0.467143   Top1 83.779297   Top5 98.216146   BatchTime 0.375942   LR 0.001708
0.91293716
0.91305125
0.91271096
0.91228539
0.91176438
0.91145080
0.91123104
0.91104984
0.91084230
0.91080010
0.91081828
0.91006845
0.90872431
0.90794742
0.90585089
0.90548450
0.90569389
0.90525043
0.90446907
0.90447122
0.90443796
0.90451324
0.90476841
0.90460467
0.90440941
INFO - Training [17][  140/  196]   Loss 0.462165   Top1 83.981585   Top5 98.311942   BatchTime 0.369030   LR 0.001690
0.90453196
0.90447283
0.90457743
0.90459681
0.90446889
0.90426826
0.90410542
0.90416121
0.90425706
0.90421271
0.90434659
0.90438288
0.90439755
0.90403485
0.90396762
0.90406716
0.90422058
INFO - Training [17][  160/  196]   Loss 0.464014   Top1 83.930664   Top5 98.337402   BatchTime 0.368411   LR 0.001671
0.90426219
0.90424222
0.90446311
0.90466309
0.90482891
0.90518212
0.90548098
0.90582323
0.90649384
0.90668499
0.90743834
0.90848321
0.90905958
0.91022885
0.90995711
0.90966141
0.90963715
0.90968770
0.91007262
0.91011792
0.91010630
0.91037166
INFO - Training [17][  180/  196]   Loss 0.463011   Top1 83.936632   Top5 98.289931   BatchTime 0.366921   LR 0.001652
0.91041577
0.91064370
0.91070616
0.91105896
0.91115093
0.91123527
0.91154027
0.91184652
0.91199291
0.91253501
0.91305536
0.91341007
0.91400373
0.91405678
0.91454709
0.91530824
0.91608465
INFO - ==> Top1: 83.974    Top5: 98.290    Loss: 0.461
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [17][   20/   40]   Loss 0.350459   Top1 88.828125   Top5 99.589844   BatchTime 0.125686
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.1076)
features.1.conv.6 tensor(0.0582)
features.2.conv.0 tensor(0.0211)
features.2.conv.3 tensor(0.0664)
features.2.conv.6 tensor(0.0819)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0460)
features.4.conv.0 tensor(0.0223)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0928)
features.5.conv.0 tensor(0.0369)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1048)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0530)
features.7.conv.0 tensor(0.0434)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.1139)
features.8.conv.0 tensor(0.0656)
features.8.conv.3 tensor(0.1192)
features.8.conv.6 tensor(0.1248)
features.9.conv.0 tensor(0.0629)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.1020)
features.10.conv.0 tensor(0.0356)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0710)
features.11.conv.0 tensor(0.0977)
features.11.conv.3 tensor(0.1480)
features.11.conv.6 tensor(0.1973)
features.12.conv.0 tensor(0.2498)
features.12.conv.3 tensor(0.1192)
features.12.conv.6 tensor(0.1910)
features.13.conv.0 tensor(0.0560)
features.13.conv.3 tensor(0.1838)
features.13.conv.6 tensor(0.0956)
features.14.conv.0 tensor(0.4978)
features.14.conv.3 tensor(0.0997)
features.14.conv.6 tensor(0.5342)
features.15.conv.0 tensor(0.7635)
features.15.conv.3 tensor(0.0829)
features.15.conv.6 tensor(0.4872)
features.16.conv.0 tensor(0.0863)
features.16.conv.3 tensor(0.1139)
features.16.conv.6 tensor(0.2933)
conv.0 tensor(0.1448)
tensor(592570.) 2188896.0
INFO - Validation [17][   40/   40]   Loss 0.342301   Top1 88.680000   Top5 99.700000   BatchTime 0.088627
INFO - ==> Top1: 88.680    Top5: 99.700    Loss: 0.342
INFO - ==> Sparsity : 0.271
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 86.890   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.91600138
0.91619277
0.91632432
0.91635376
0.91632897
0.91608286
0.91593218
0.91548234
0.91546208
0.91511875
0.91547227
0.91556460
0.91600609
0.91679054
0.91743851
0.91770679
0.91799712
INFO - Training [18][   20/  196]   Loss 0.435858   Top1 84.355469   Top5 98.007812   BatchTime 0.418628   LR 0.001618
0.91826141
0.91843057
0.91835225
0.91811639
0.91799688
0.91787010
0.91800743
0.91789311
0.91776276
0.91718608
0.91690916
0.91682035
0.91663486
0.91612333
0.91529763
0.91434520
0.91370952
0.91283351
0.91133165
0.90954864
0.90777487
0.90636122
INFO - Training [18][   40/  196]   Loss 0.453117   Top1 83.925781   Top5 98.183594   BatchTime 0.394299   LR 0.001599
0.90563327
0.90447563
0.90312421
0.90177774
0.90142399
0.90162152
0.90167302
0.90211868
0.90265340
0.90386021
0.90572757
0.90745711
0.90927655
0.91031188
0.91154742
0.91195476
0.91188598
0.91141063
0.91114908
0.91114897
0.91045517
INFO - Training [18][   60/  196]   Loss 0.451213   Top1 84.192708   Top5 98.300781   BatchTime 0.388365   LR 0.001579
0.91001797
0.90929192
0.90783715
0.90618843
0.90533495
0.90453345
0.90385109
0.90264404
0.90223962
0.90176654
0.90178287
0.90062958
0.90010357
0.89925432
0.89835036
0.89783752
0.89788699
0.89762080
0.89763004
0.89817977
0.89868248
INFO - Training [18][   80/  196]   Loss 0.449178   Top1 84.238281   Top5 98.413086   BatchTime 0.388515   LR 0.001560
0.89952558
0.90119588
0.90364033
0.90536410
0.90510207
0.90625817
0.90836579
0.91092199
0.91292191
0.91497183
0.91509110
0.91443115
0.91352636
0.91271818
0.91240096
0.91304666
0.91363972
0.91427857
INFO - Training [18][  100/  196]   Loss 0.447833   Top1 84.320312   Top5 98.421875   BatchTime 0.377724   LR 0.001540
0.91534400
0.91642934
0.91779262
0.91937143
0.92038685
0.92105585
0.92151010
0.92209023
0.92255002
0.92286682
0.92317533
0.92335457
0.92339224
0.92335111
0.92339402
0.92349577
0.92384595
0.92419940
INFO - Training [18][  120/  196]   Loss 0.443207   Top1 84.479167   Top5 98.473307   BatchTime 0.371444   LR 0.001521
0.92399853
0.92409998
0.92406857
0.92394692
0.92385036
0.92368162
0.92345303
0.92322427
0.92306024
0.92307228
0.92312199
0.92319775
0.92314553
0.92310125
0.92303413
0.92308229
0.92311490
0.92309624
0.92326319
0.92306072
0.92297024
0.92300308
INFO - Training [18][  140/  196]   Loss 0.439506   Top1 84.628906   Top5 98.546317   BatchTime 0.371532   LR 0.001501
0.92293280
0.92305350
0.92278916
0.92262483
0.92243016
0.92233074
0.92182463
0.92178011
0.92173624
0.92168581
0.92128551
0.92081136
0.92061692
0.92046756
0.92093599
0.92059696
0.92034322
0.92037159
0.92073679
0.92062420
0.92049092
0.92022830
INFO - Training [18][  160/  196]   Loss 0.444701   Top1 84.477539   Top5 98.508301   BatchTime 0.370629   LR 0.001482
0.92005295
0.91966969
0.91940939
0.91901308
0.91916680
0.91877306
0.91845572
0.91829640
0.91783214
0.91740888
0.91710621
0.91669708
0.91626716
0.91590583
0.91557044
0.91516656
INFO - Training [18][  180/  196]   Loss 0.445179   Top1 84.450955   Top5 98.424479   BatchTime 0.371007   LR 0.001462
0.91430408
0.91368830
0.91340947
0.91276789
0.91270047
0.91278327
0.91300678
0.91301394
0.91321063
0.91319329
0.91323590
0.91296542
0.91241014
0.91204733
0.91197979
0.91179734
INFO - ==> Top1: 84.506    Top5: 98.420    Loss: 0.445
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.91143072
0.91108972
0.91056287
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [18][   20/   40]   Loss 0.350026   Top1 88.203125   Top5 99.453125   BatchTime 0.130054
INFO - Validation [18][   40/   40]   Loss 0.342978   Top1 88.590000   Top5 99.550000   BatchTime 0.092467
INFO - ==> Top1: 88.590    Top5: 99.550    Loss: 0.343
INFO - ==> Sparsity : 0.270
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 88.590   Top5: 99.550]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 87.580   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5208)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0538)
features.2.conv.0 tensor(0.0223)
features.2.conv.3 tensor(0.0671)
features.2.conv.6 tensor(0.0926)
features.3.conv.0 tensor(0.0211)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0443)
features.4.conv.0 tensor(0.0273)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0968)
features.5.conv.0 tensor(0.0381)
features.5.conv.3 tensor(0.0799)
features.5.conv.6 tensor(0.1047)
features.6.conv.0 tensor(0.0254)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0496)
features.7.conv.0 tensor(0.0488)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.1141)
features.8.conv.0 tensor(0.0650)
features.8.conv.3 tensor(0.1172)
features.8.conv.6 tensor(0.1233)
features.9.conv.0 tensor(0.0659)
features.9.conv.3 tensor(0.1641)
features.9.conv.6 tensor(0.1017)
features.10.conv.0 tensor(0.0317)
features.10.conv.3 tensor(0.1073)
features.10.conv.6 tensor(0.0715)
features.11.conv.0 tensor(0.1029)
features.11.conv.3 tensor(0.1508)
features.11.conv.6 tensor(0.1977)
features.12.conv.0 tensor(0.2381)
features.12.conv.3 tensor(0.1190)
features.12.conv.6 tensor(0.1906)
features.13.conv.0 tensor(0.0586)
features.13.conv.3 tensor(0.1833)
features.13.conv.6 tensor(0.0947)
features.14.conv.0 tensor(0.7291)
features.14.conv.3 tensor(0.1012)
features.14.conv.6 tensor(0.5253)
features.15.conv.0 tensor(0.7631)
features.15.conv.3 tensor(0.0823)
features.15.conv.6 tensor(0.3865)
features.16.conv.0 tensor(0.0840)
features.16.conv.3 tensor(0.1197)
features.16.conv.6 tensor(0.2945)
conv.0 tensor(0.0966)
tensor(591415.) 2188896.0
0.90979344
0.90884930
0.90817362
0.90754002
0.90724117
0.90722364
0.90789425
0.90834332
0.90846807
0.90872383
0.90897214
0.90924299
0.90939230
0.90932834
0.90927416
0.90905797
0.90867031
0.90834969
0.90823752
INFO - Training [19][   20/  196]   Loss 0.448960   Top1 84.062500   Top5 97.792969   BatchTime 0.445630   LR 0.001427
0.90823323
0.90836132
0.90842324
0.90835810
0.90850341
0.90848535
0.90836978
0.90834361
0.90826350
0.90836483
0.90836102
0.90849984
0.90841943
0.90843928
0.90846086
0.90852952
0.90858495
0.90865004
0.90850079
0.90865350
0.90870130
0.90878946
INFO - Training [19][   40/  196]   Loss 0.440002   Top1 84.423828   Top5 98.027344   BatchTime 0.407232   LR 0.001407
0.90888935
0.90884167
0.90897912
0.90899962
0.90898854
0.90913230
0.90913230
0.90910774
0.90929490
0.90934825
0.90977037
0.90954345
0.90949768
0.90944856
0.90940779
INFO - Training [19][   60/  196]   Loss 0.436829   Top1 84.667969   Top5 98.131510   BatchTime 0.398870   LR 0.001387
0.90957057
0.90968347
0.90959334
0.90951502
0.90949970
0.90978897
0.90963084
0.90953022
0.90926135
0.90920919
0.90918171
0.90920293
0.90918732
0.90911829
0.90908802
0.90896231
0.90889192
0.90889317
0.90886307
0.90904009
0.90911210
0.90918052
INFO - Training [19][   80/  196]   Loss 0.433709   Top1 84.941406   Top5 98.242188   BatchTime 0.391998   LR 0.001367
0.90909946
0.90901542
0.90921086
0.90940034
0.90933943
0.90929997
0.90932465
0.90927666
0.90939385
0.90907770
0.90890205
0.90881461
0.90884638
0.90883827
0.90886426
0.90909666
0.90907216
0.90898985
INFO - Training [19][  100/  196]   Loss 0.426942   Top1 85.242188   Top5 98.277344   BatchTime 0.379450   LR 0.001347
0.90904909
0.90897757
0.90892756
0.90873963
0.90863484
0.90856373
0.90885061
0.90886372
0.90860051
0.90830499
0.90822768
0.90820551
0.90814883
0.90810472
0.90794706
0.90784127
0.90780091
0.90778440
0.90787923
0.90795380
0.90812516
0.90799671
0.90799218
0.90806282
0.90806872
INFO - Training [19][  120/  196]   Loss 0.423125   Top1 85.292969   Top5 98.398438   BatchTime 0.370065   LR 0.001327
0.90807682
0.90802741
0.90830803
0.90851563
0.90841997
0.90866846
0.90877974
0.90882236
0.90874022
0.90866876
0.90856200
0.90875763
0.90888202
0.90894008
0.90871227
0.90858775
INFO - Training [19][  140/  196]   Loss 0.422605   Top1 85.320871   Top5 98.457031   BatchTime 0.370270   LR 0.001307
0.90861547
0.90885091
0.90887874
0.90887278
0.90875244
0.90860546
0.90845674
0.90841001
0.90836209
0.90827638
0.90805960
0.90795475
0.90787518
0.90796751
0.90807557
0.90809417
0.90797693
0.90820116
0.90788168
0.90772897
0.90768343
0.90763414
INFO - Training [19][  160/  196]   Loss 0.427759   Top1 85.163574   Top5 98.432617   BatchTime 0.369000   LR 0.001287
0.90756875
0.90752411
0.90763187
0.90761948
0.90778607
0.90782958
0.90818399
0.90834385
0.90863270
0.90861779
0.90831542
0.90820521
0.90798783
0.90795696
0.90796971
0.90796286
0.90796727
0.90796465
0.90809941
0.90825051
0.90844226
0.90880114
INFO - Training [19][  180/  196]   Loss 0.426509   Top1 85.210503   Top5 98.411458   BatchTime 0.368648   LR 0.001266
0.90913445
0.90901154
0.90887678
0.90872848
0.90879971
0.90872729
0.90861917
0.90865403
0.90860647
0.90860891
0.90844893
0.90843493
0.90860534
0.90890324
0.90876102
INFO - ==> Top1: 85.250    Top5: 98.416    Loss: 0.424
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [19][   20/   40]   Loss 0.347589   Top1 88.984375   Top5 99.355469   BatchTime 0.117534
INFO - Validation [19][   40/   40]   Loss 0.334810   Top1 88.830000   Top5 99.570000   BatchTime 0.086397
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0995)
features.1.conv.6 tensor(0.0538)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0874)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0454)
features.4.conv.0 tensor(0.0233)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0923)
features.5.conv.0 tensor(0.0324)
features.5.conv.3 tensor(0.0816)
features.5.conv.6 tensor(0.1056)
features.6.conv.0 tensor(0.0270)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0507)
features.7.conv.0 tensor(0.0422)
features.7.conv.3 tensor(0.1111)
features.7.conv.6 tensor(0.1148)
features.8.conv.0 tensor(0.0590)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.1229)
features.9.conv.0 tensor(0.0693)
features.9.conv.3 tensor(0.1696)
features.9.conv.6 tensor(0.1017)
features.10.conv.0 tensor(0.0314)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0715)
features.11.conv.0 tensor(0.1000)
features.11.conv.3 tensor(0.1480)
features.11.conv.6 tensor(0.1968)
features.12.conv.0 tensor(0.2404)
features.12.conv.3 tensor(0.1184)
features.12.conv.6 tensor(0.1905)
features.13.conv.0 tensor(0.0560)
features.13.conv.3 tensor(0.1798)
features.13.conv.6 tensor(0.0930)
features.14.conv.0 tensor(0.7344)
features.14.conv.3 tensor(0.1000)
features.14.conv.6 tensor(0.5419)
features.15.conv.0 tensor(0.7693)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.3888)
features.16.conv.0 tensor(0.0723)
features.16.conv.3 tensor(0.1179)
features.16.conv.6 tensor(0.3078)
conv.0 tensor(0.0995)
tensor(598796.) 2188896.0
INFO - ==> Top1: 88.830    Top5: 99.570    Loss: 0.335
INFO - ==> Sparsity : 0.274
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.830   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 88.590   Top5: 99.550]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.90889871
0.90903807
0.90910357
0.90894789
0.90891606
0.90892357
0.90900981
0.90878665
0.90914285
0.90949720
0.90943575
0.90939867
0.90929377
0.90927392
0.90930688
0.90923983
0.90912771
INFO - Training [20][   20/  196]   Loss 0.423713   Top1 85.292969   Top5 97.988281   BatchTime 0.421182   LR 0.001231
0.90903091
0.90900916
0.90888035
0.90873528
0.90854251
0.90851456
0.90866214
0.90861666
0.90845197
0.90828639
0.90846086
0.90849859
0.90849590
0.90856069
0.90841877
0.90844268
0.90840322
0.90843290
0.90849376
0.90863067
INFO - Training [20][   40/  196]   Loss 0.424622   Top1 85.244141   Top5 98.300781   BatchTime 0.412064   LR 0.001211
0.90878534
0.90872318
0.90877110
0.90904897
0.90904576
0.90889978
0.90880847
0.90870821
0.90877074
0.90857923
0.90842503
0.90847123
0.90839016
0.90836674
0.90843987
0.90842986
0.90849495
0.90834183
0.90835243
0.90831399
0.90860289
INFO - Training [20][   60/  196]   Loss 0.428108   Top1 84.986979   Top5 98.346354   BatchTime 0.400591   LR 0.001191
0.90840977
0.90868175
0.90876049
0.90877026
0.90868443
0.90852624
0.90851009
0.90832883
0.90826619
0.90821445
0.90815789
0.90825170
0.90838182
0.90847123
0.90841091
0.90839857
0.90851218
0.90839642
INFO - Training [20][   80/  196]   Loss 0.423461   Top1 85.170898   Top5 98.491211   BatchTime 0.385030   LR 0.001171
0.90818697
0.90817761
0.90811211
0.90823323
0.90841240
0.90824318
0.90842569
0.90801835
0.90820402
0.90808481
0.90797192
0.90788960
0.90792412
0.90816653
0.90841877
0.90825510
0.90837812
0.90852582
0.90863746
0.90849531
0.90853047
0.90846854
0.90836906
0.90830332
0.90833992
0.90844429
INFO - Training [20][  100/  196]   Loss 0.419144   Top1 85.292969   Top5 98.496094   BatchTime 0.369754   LR 0.001151
0.90861624
0.90839779
0.90852314
0.90838462
0.90843391
0.90854442
0.90851110
0.90858823
0.90863198
0.90854579
0.90876788
0.90859312
0.90861613
0.90873861
0.90872931
0.90868115
0.90869975
INFO - Training [20][  120/  196]   Loss 0.414065   Top1 85.465495   Top5 98.551432   BatchTime 0.366412   LR 0.001131
0.90874302
0.90854609
0.90865535
0.90871620
0.90860003
0.90852177
0.90857160
0.90859085
0.90826851
0.90824717
0.90821010
0.90821731
0.90828896
0.90836132
0.90826744
0.90820062
0.90841472
0.90846252
0.90825790
0.90815544
INFO - Training [20][  140/  196]   Loss 0.413471   Top1 85.521763   Top5 98.610491   BatchTime 0.368750   LR 0.001111
0.90799731
0.90805107
0.90811455
0.90826261
0.90863764
0.90872532
0.90853649
0.90850532
0.90863848
0.90876925
0.90877479
0.90874642
0.90867710
0.90852571
0.90856320
0.90831119
0.90834725
INFO - Training [20][  160/  196]   Loss 0.412492   Top1 85.593262   Top5 98.620605   BatchTime 0.367595   LR 0.001091
0.90821928
0.90808594
0.90790433
0.90775776
0.90783376
0.90762407
0.90776342
0.90757889
0.90742314
0.90726769
0.90707642
0.90705085
0.90714121
0.90744972
0.90754265
0.90729064
0.90700775
0.90704888
0.90704191
0.90727925
0.90822947
INFO - Training [20][  180/  196]   Loss 0.412976   Top1 85.557726   Top5 98.580729   BatchTime 0.368853   LR 0.001071
0.90873313
0.90872288
0.90868974
0.90870363
0.90834057
0.90834010
0.90836096
0.90827394
0.90826184
0.90805584
0.90802610
0.90828514
0.90818149
0.90789318
0.90772188
0.90774399
0.90764314
INFO - ==> Top1: 85.680    Top5: 98.600    Loss: 0.410
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.90764785
0.90761042
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 0.345906   Top1 88.535156   Top5 99.570312   BatchTime 0.120563
INFO - Validation [20][   40/   40]   Loss 0.337411   Top1 88.500000   Top5 99.620000   BatchTime 0.088286
INFO - ==> Top1: 88.500    Top5: 99.620    Loss: 0.337
INFO - ==> Sparsity : 0.273
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.830   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 88.590   Top5: 99.550]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0984)
features.1.conv.6 tensor(0.0538)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0802)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0625)
features.3.conv.6 tensor(0.0425)
features.4.conv.0 tensor(0.0218)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0902)
features.5.conv.0 tensor(0.0317)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.1076)
features.6.conv.0 tensor(0.0195)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0534)
features.7.conv.0 tensor(0.0425)
features.7.conv.3 tensor(0.1120)
features.7.conv.6 tensor(0.1140)
features.8.conv.0 tensor(0.0503)
features.8.conv.3 tensor(0.1192)
features.8.conv.6 tensor(0.1238)
features.9.conv.0 tensor(0.0608)
features.9.conv.3 tensor(0.1672)
features.9.conv.6 tensor(0.0999)
features.10.conv.0 tensor(0.0342)
features.10.conv.3 tensor(0.1071)
features.10.conv.6 tensor(0.0698)
features.11.conv.0 tensor(0.0961)
features.11.conv.3 tensor(0.1493)
features.11.conv.6 tensor(0.1934)
features.12.conv.0 tensor(0.2221)
features.12.conv.3 tensor(0.1198)
features.12.conv.6 tensor(0.1889)
features.13.conv.0 tensor(0.0557)
features.13.conv.3 tensor(0.1790)
features.13.conv.6 tensor(0.0929)
features.14.conv.0 tensor(0.7478)
features.14.conv.3 tensor(0.0985)
features.14.conv.6 tensor(0.5414)
features.15.conv.0 tensor(0.7791)
features.15.conv.3 tensor(0.0829)
features.15.conv.6 tensor(0.3975)
features.16.conv.0 tensor(0.0762)
features.16.conv.3 tensor(0.1182)
features.16.conv.6 tensor(0.3072)
conv.0 tensor(0.0885)
tensor(597532.) 2188896.0
0.90773344
0.90803057
0.90806252
0.90821606
0.90845692
0.90862781
0.90865582
0.90854794
0.90818495
0.90814155
0.90826124
0.90821224
0.90835756
0.90815723
0.90797579
0.90797377
0.90788007
INFO - Training [21][   20/  196]   Loss 0.417183   Top1 84.902344   Top5 98.183594   BatchTime 0.431960   LR 0.001036
0.90786141
0.90791279
0.90780902
0.90768176
0.90762079
0.90766293
0.90766531
0.90740943
0.90755975
0.90770823
0.90745246
0.90768778
0.90746933
0.90734452
0.90725005
0.90711272
0.90708846
0.90744585
0.90743816
0.90728682
0.90712792
INFO - Training [21][   40/  196]   Loss 0.424433   Top1 85.224609   Top5 98.369141   BatchTime 0.407523   LR 0.001016
0.90692127
0.90667266
0.90636837
0.90632844
0.90623891
0.90591127
0.90593928
0.90593517
0.90602124
0.90618438
0.90619600
0.90628713
0.90610433
0.90599477
0.90601969
0.90588242
0.90591848
0.90583420
0.90575713
0.90563035
0.90553308
INFO - Training [21][   60/  196]   Loss 0.414145   Top1 85.442708   Top5 98.430990   BatchTime 0.399250   LR 0.000996
0.90516037
0.90500051
0.90481031
0.90468794
0.90447271
0.90436119
0.90397966
0.90349233
0.90306115
0.90250689
0.90206414
0.90128845
0.90041912
0.89990681
0.89914912
0.89806926
0.89690977
0.89523047
INFO - Training [21][   80/  196]   Loss 0.412483   Top1 85.566406   Top5 98.520508   BatchTime 0.384455   LR 0.000976
0.89451182
0.89448297
0.89428562
0.89397204
0.89329642
0.89252907
0.89228278
0.89179701
0.89006704
0.88930643
0.88796759
0.88622880
0.88555259
0.88477075
0.88383532
0.88282371
0.88222301
0.88202053
0.88334996
INFO - Training [21][  100/  196]   Loss 0.406650   Top1 85.746094   Top5 98.554688   BatchTime 0.367108   LR 0.000957
0.88484657
0.88612801
0.88782680
0.88981718
0.89186996
0.89366019
0.89512634
0.89647251
0.89703625
0.89801472
0.89847690
0.89937699
0.89981890
0.90001267
0.90197104
0.90251833
0.90303355
0.90342408
0.90388083
0.90424335
0.90436453
0.90444905
0.90463930
0.90472883
0.90496242
INFO - Training [21][  120/  196]   Loss 0.400720   Top1 85.979818   Top5 98.636068   BatchTime 0.361035   LR 0.000937
0.90513891
0.90534818
0.90531665
0.90556693
0.90571278
0.90586901
0.90604967
0.90603977
0.90584731
0.90566307
0.90585977
0.90585476
0.90583259
0.90567809
0.90549684
0.90547562
INFO - Training [21][  140/  196]   Loss 0.398220   Top1 86.085379   Top5 98.696987   BatchTime 0.362409   LR 0.000918
0.90545136
0.90528232
0.90527970
0.90504652
0.90506893
0.90485817
0.90482593
0.90483606
0.90469295
0.90481317
0.90444005
0.90420455
0.90404570
0.90394706
0.90384090
0.90380770
0.90364575
0.90350920
0.90313083
0.90275556
0.90265501
0.90236813
INFO - Training [21][  160/  196]   Loss 0.401855   Top1 85.922852   Top5 98.696289   BatchTime 0.361617   LR 0.000899
0.90185213
0.90149081
0.90078217
0.90005326
0.89942837
0.89810014
0.89651984
0.89506930
0.89397007
0.89268529
0.89044917
0.88845563
0.88698351
0.88523918
0.88431966
0.88352281
0.88281763
0.88225955
0.88170427
0.88182002
0.88133103
0.88107413
INFO - Training [21][  180/  196]   Loss 0.399460   Top1 86.004774   Top5 98.656684   BatchTime 0.363558   LR 0.000879
0.88071525
0.88058168
0.88036692
0.88010252
0.87970513
0.87954712
0.87960559
0.87969071
0.87957245
0.87974316
0.88008094
0.88124132
0.88349414
0.88679290
0.89127499
INFO - ==> Top1: 86.024    Top5: 98.644    Loss: 0.397
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [21][   20/   40]   Loss 0.389234   Top1 87.343750   Top5 99.238281   BatchTime 0.165126
INFO - Validation [21][   40/   40]   Loss 0.387840   Top1 87.130000   Top5 99.430000   BatchTime 0.108189
INFO - ==> Top1: 87.130    Top5: 99.430    Loss: 0.388
INFO - ==> Sparsity : 0.282
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.830   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 88.590   Top5: 99.550]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1895)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0543)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0830)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0438)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0378)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1047)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0541)
features.7.conv.0 tensor(0.0402)
features.7.conv.3 tensor(0.1082)
features.7.conv.6 tensor(0.1123)
features.8.conv.0 tensor(0.0481)
features.8.conv.3 tensor(0.1155)
features.8.conv.6 tensor(0.1197)
features.9.conv.0 tensor(0.0618)
features.9.conv.3 tensor(0.1696)
features.9.conv.6 tensor(0.0987)
features.10.conv.0 tensor(0.0370)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.0708)
features.11.conv.0 tensor(0.1002)
features.11.conv.3 tensor(0.1495)
features.11.conv.6 tensor(0.1945)
features.12.conv.0 tensor(0.2194)
features.12.conv.3 tensor(0.1173)
features.12.conv.6 tensor(0.1885)
features.13.conv.0 tensor(0.0599)
features.13.conv.3 tensor(0.1860)
features.13.conv.6 tensor(0.0913)
features.14.conv.0 tensor(0.7332)
features.14.conv.3 tensor(0.0970)
features.14.conv.6 tensor(0.5507)
features.15.conv.0 tensor(0.7970)
features.15.conv.3 tensor(0.0851)
features.15.conv.6 tensor(0.5271)
features.16.conv.0 tensor(0.0730)
features.16.conv.3 tensor(0.1161)
features.16.conv.6 tensor(0.3070)
conv.0 tensor(0.0867)
tensor(618211.) 2188896.0
0.89780849
0.90338731
0.90674055
0.90827006
0.90821010
0.90825129
0.90845847
0.90851694
0.90844470
0.90835458
0.90823895
0.90821785
0.90815014
0.90812230
0.90813076
0.90806144
0.90793866
INFO - Training [22][   20/  196]   Loss 0.431418   Top1 85.214844   Top5 97.832031   BatchTime 0.424831   LR 0.000846
0.90805018
0.90808743
0.90802306
0.90821195
0.90845346
0.90844840
0.90830296
0.90824717
0.90841758
0.90829045
0.90823132
0.90829235
0.90847641
0.90859085
0.90863234
0.90845817
0.90844524
0.90831959
0.90826607
0.90813828
0.90814650
INFO - Training [22][   40/  196]   Loss 0.438579   Top1 85.068359   Top5 98.242188   BatchTime 0.405096   LR 0.000827
0.90829492
0.90808904
0.90803581
0.90806556
0.90811843
0.90838891
0.90838623
0.90850788
0.90883946
0.90875894
0.90849459
0.90870059
0.90880066
0.90889412
0.90883100
0.90881628
0.90859848
0.90856910
0.90869105
0.90881842
INFO - Training [22][   60/  196]   Loss 0.433268   Top1 85.253906   Top5 98.320312   BatchTime 0.399455   LR 0.000808
0.90894210
0.90889525
0.90833646
0.90838349
0.90831119
0.90831381
0.90801412
0.90803730
0.90706277
0.90692949
0.90829062
0.90824902
0.90839958
0.90832418
0.90816635
0.90819335
0.90825033
0.90821385
INFO - Training [22][   80/  196]   Loss 0.427742   Top1 85.307617   Top5 98.481445   BatchTime 0.382742   LR 0.000789
0.90814435
0.90816969
0.90828258
0.90836573
0.90825146
0.90846783
0.90832800
0.90829444
0.90815312
0.90797240
0.90806133
0.90820229
0.90827101
0.90853006
0.90844917
0.90885067
0.90861082
0.90843427
0.90837777
0.90827233
INFO - Training [22][  100/  196]   Loss 0.416202   Top1 85.593750   Top5 98.531250   BatchTime 0.364109   LR 0.000770
0.90819061
0.90812045
0.90806764
0.90802234
0.90797198
0.90812731
0.90819877
0.90800947
0.90825498
0.90830892
0.90839130
0.90826792
0.90833676
0.90841734
0.90850383
0.90859848
0.90852511
0.90841973
0.90843064
0.90836877
0.90831476
INFO - Training [22][  120/  196]   Loss 0.408707   Top1 85.764974   Top5 98.603516   BatchTime 0.354770   LR 0.000752
0.90838569
0.90837079
0.90839452
0.90847194
0.90879613
0.90888602
0.90886074
0.90843034
0.90821314
0.90817851
0.90825772
0.90819031
0.90809578
0.90802902
0.90798455
0.90815324
0.90805387
0.90815490
0.90816182
0.90820611
0.90803283
0.90799713
INFO - Training [22][  140/  196]   Loss 0.406001   Top1 85.856585   Top5 98.655134   BatchTime 0.355976   LR 0.000734
0.90796763
0.90791559
0.90804285
0.90802777
0.90811270
0.90822738
0.90811384
0.90824729
0.90830731
0.90838891
0.90827751
0.90808177
0.90810466
0.90812916
0.90813786
0.90806890
0.90801716
0.90793341
0.90801442
0.90811735
0.90799928
0.90810812
INFO - Training [22][  160/  196]   Loss 0.403733   Top1 85.898438   Top5 98.691406   BatchTime 0.357038   LR 0.000715
0.90790379
0.90783536
0.90771765
0.90766370
0.90760440
0.90773368
0.90771943
0.90757114
0.90748173
0.90733248
0.90716451
0.90714759
0.90716052
0.90718073
0.90719461
0.90730715
INFO - Training [22][  180/  196]   Loss 0.402635   Top1 85.930990   Top5 98.648003   BatchTime 0.358564   LR 0.000697
0.90747398
0.90735716
0.90751797
0.90753180
0.90743625
0.90744299
0.90760839
0.90808159
0.90798759
0.90831333
0.90825182
0.90844530
0.90836656
0.90799361
0.90803719
0.90808094
INFO - ==> Top1: 86.072    Top5: 98.632    Loss: 0.399
0.90791500
0.90794170
0.90811390
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.361764   Top1 88.437500   Top5 99.472656   BatchTime 0.130258
INFO - Validation [22][   40/   40]   Loss 0.349713   Top1 88.570000   Top5 99.610000   BatchTime 0.091375
INFO - ==> Top1: 88.570    Top5: 99.610    Loss: 0.350
INFO - ==> Sparsity : 0.276
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.830   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 88.590   Top5: 99.550]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.1934)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0530)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0434)
features.4.conv.0 tensor(0.0317)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0933)
features.5.conv.0 tensor(0.0374)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1009)
features.6.conv.0 tensor(0.0270)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0530)
features.7.conv.0 tensor(0.0452)
features.7.conv.3 tensor(0.1120)
features.7.conv.6 tensor(0.1123)
features.8.conv.0 tensor(0.0522)
features.8.conv.3 tensor(0.1178)
features.8.conv.6 tensor(0.1205)
features.9.conv.0 tensor(0.0579)
features.9.conv.3 tensor(0.1664)
features.9.conv.6 tensor(0.0976)
features.10.conv.0 tensor(0.0349)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0703)
features.11.conv.0 tensor(0.0939)
features.11.conv.3 tensor(0.1501)
features.11.conv.6 tensor(0.1940)
features.12.conv.0 tensor(0.2223)
features.12.conv.3 tensor(0.1140)
features.12.conv.6 tensor(0.1885)
features.13.conv.0 tensor(0.0578)
features.13.conv.3 tensor(0.1811)
features.13.conv.6 tensor(0.0918)
features.14.conv.0 tensor(0.7481)
features.14.conv.3 tensor(0.0970)
features.14.conv.6 tensor(0.5397)
features.15.conv.0 tensor(0.7981)
features.15.conv.3 tensor(0.0840)
features.15.conv.6 tensor(0.3806)
features.16.conv.0 tensor(0.0707)
features.16.conv.3 tensor(0.1146)
features.16.conv.6 tensor(0.3019)
conv.0 tensor(0.1126)
tensor(604879.) 2188896.0
0.90815073
0.90839845
0.90822703
0.90801060
0.90805084
0.90793306
0.90793544
0.90775007
0.90668756
0.90614277
0.90612143
0.90629846
0.90817630
0.90799761
0.90805030
0.90808064
0.90809089
0.90801865
0.90780944
0.90770268
0.90789646
INFO - Training [23][   20/  196]   Loss 0.405433   Top1 85.839844   Top5 98.105469   BatchTime 0.433053   LR 0.000666
0.90823609
0.90825611
0.90808070
0.90827465
0.90826100
0.90830189
0.90800613
0.90827876
0.90823489
0.90811735
0.90801436
0.90806937
0.90803641
0.90794605
0.90808225
0.90828258
0.90803331
0.90820330
0.90814155
0.90798110
INFO - Training [23][   40/  196]   Loss 0.396814   Top1 86.201172   Top5 98.388672   BatchTime 0.417662   LR 0.000648
0.90805310
0.90818417
0.90808475
0.90799296
0.90795261
0.90798807
0.90804201
0.90800816
0.90784621
0.90771139
0.90759134
0.90758938
0.90767795
0.90762138
0.90763146
0.90751636
INFO - Training [23][   60/  196]   Loss 0.395245   Top1 86.230469   Top5 98.496094   BatchTime 0.401890   LR 0.000630
0.90751112
0.90751344
0.90732890
0.90752536
0.90756798
0.90773839
0.90773869
0.90807581
0.90763116
0.90730828
0.90714699
0.90704352
0.90699065
0.90703011
0.90709561
0.90707523
0.90726483
0.90722847
0.90723866
0.90723091
0.90715384
0.90700626
INFO - Training [23][   80/  196]   Loss 0.392717   Top1 86.347656   Top5 98.627930   BatchTime 0.392411   LR 0.000613
0.90700734
0.90706813
0.90696031
0.90703362
0.90705431
0.90694565
0.90686578
0.90683663
0.90697223
0.90698087
0.90699250
0.90678620
0.90676987
0.90686071
0.90710038
0.90744191
0.90739536
0.90755928
0.90738511
0.90734839
0.90724993
INFO - Training [23][  100/  196]   Loss 0.383977   Top1 86.636719   Top5 98.667969   BatchTime 0.371114   LR 0.000596
0.90710610
0.90736580
0.90745872
0.90709192
0.90683478
0.90688783
0.90722901
0.90733910
0.90725952
0.90705121
0.90682977
0.90678650
0.90689528
0.90680051
0.90686578
0.90692657
0.90667319
0.90676087
0.90688032
0.90685284
0.90682054
INFO - Training [23][  120/  196]   Loss 0.379518   Top1 86.780599   Top5 98.730469   BatchTime 0.357320   LR 0.000579
0.90646493
0.90633076
0.90648168
0.90650851
0.90634918
0.90643740
0.90666527
0.90678543
0.90672195
0.90654868
0.90667152
0.90664673
0.90623134
0.90627283
0.90629572
0.90633112
INFO - Training [23][  140/  196]   Loss 0.375014   Top1 86.972656   Top5 98.814174   BatchTime 0.359044   LR 0.000562
0.90631962
0.90618283
0.90588719
0.90586036
0.90595585
0.90577132
0.90570784
0.90561372
0.90570337
0.90569264
0.90562385
0.90547335
0.90527427
0.90526420
0.90559632
0.90566981
0.90556091
0.90553385
0.90531641
0.90513021
0.90513575
INFO - Training [23][  160/  196]   Loss 0.377461   Top1 86.943359   Top5 98.813477   BatchTime 0.360796   LR 0.000545
0.90510845
0.90506333
0.90529925
0.90535510
0.90538979
0.90503049
0.90493238
0.90509611
0.90526766
0.90525663
0.90515286
0.90525752
0.90509838
0.90503198
0.90509528
0.90508342
0.90475422
0.90448678
0.90427679
0.90423357
0.90422404
0.90412277
0.90385938
INFO - Training [23][  180/  196]   Loss 0.378125   Top1 86.872830   Top5 98.778212   BatchTime 0.360578   LR 0.000529
0.90403819
0.90429842
0.90434915
0.90415460
0.90410131
0.90416437
0.90404564
0.90373331
0.90366858
0.90348077
0.90343612
0.90346450
0.90329945
0.90330499
0.90337318
********************pre-trained*****************
INFO - ==> Top1: 86.876    Top5: 98.764    Loss: 0.377
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.350628   Top1 88.515625   Top5 99.472656   BatchTime 0.133416
INFO - Validation [23][   40/   40]   Loss 0.343191   Top1 88.550000   Top5 99.600000   BatchTime 0.097517
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1953)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0984)
features.1.conv.6 tensor(0.0473)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0664)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0205)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0447)
features.4.conv.0 tensor(0.0252)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0916)
features.5.conv.0 tensor(0.0353)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1025)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0516)
features.7.conv.0 tensor(0.0445)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1120)
features.8.conv.0 tensor(0.0524)
features.8.conv.3 tensor(0.1166)
features.8.conv.6 tensor(0.1202)
features.9.conv.0 tensor(0.0577)
features.9.conv.3 tensor(0.1670)
features.9.conv.6 tensor(0.0984)
features.10.conv.0 tensor(0.0355)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0704)
features.11.conv.0 tensor(0.0949)
features.11.conv.3 tensor(0.1499)
features.11.conv.6 tensor(0.1896)
features.12.conv.0 tensor(0.2293)
features.12.conv.3 tensor(0.1154)
features.12.conv.6 tensor(0.1844)
features.13.conv.0 tensor(0.0574)
features.13.conv.3 tensor(0.1823)
features.13.conv.6 tensor(0.0929)
features.14.conv.0 tensor(0.7628)
features.14.conv.3 tensor(0.0959)
features.14.conv.6 tensor(0.5463)
features.15.conv.0 tensor(0.8057)
features.15.conv.3 tensor(0.0808)
features.15.conv.6 tensor(0.4860)
features.16.conv.0 tensor(0.0749)
features.16.conv.3 tensor(0.1148)
features.16.conv.6 tensor(0.3039)
conv.0 tensor(0.0862)
tensor(615900.) 2188896.0
INFO - ==> Top1: 88.550    Top5: 99.600    Loss: 0.343
INFO - ==> Sparsity : 0.281
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.830   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 88.590   Top5: 99.550]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.90315437
0.90339327
0.90305901
0.90307128
0.90311748
0.90274656
0.90256488
0.90249527
0.90208387
0.90165526
0.90148550
0.90159303
0.90170187
0.90187132
0.90155625
0.90107179
0.90071321
0.90059328
INFO - Training [24][   20/  196]   Loss 0.398007   Top1 86.113281   Top5 98.007812   BatchTime 0.450251   LR 0.000500
0.90036517
0.89994848
0.89972252
0.89942580
0.89917457
0.89890504
0.89820838
0.89747655
0.89666772
0.89587939
0.89485127
0.89438301
0.89400417
0.89361733
0.89331418
0.89267647
0.89211708
0.89161855
0.89127403
0.89099270
0.89084798
INFO - Training [24][   40/  196]   Loss 0.392696   Top1 86.386719   Top5 98.369141   BatchTime 0.423427   LR 0.000484
0.89061427
0.89011222
0.88932335
0.88855708
0.88789177
0.88746822
0.88701004
0.88641256
0.88534236
0.88580263
0.88680750
0.88682324
0.88685375
0.88658881
0.88644880
0.88586009
0.88560897
0.88520259
0.88461840
0.88384491
0.88315791
INFO - Training [24][   60/  196]   Loss 0.384339   Top1 86.510417   Top5 98.457031   BatchTime 0.404846   LR 0.000468
0.88293558
0.88218993
0.88180804
0.88126326
0.88087690
0.88038629
0.88026464
0.88058126
0.88055664
0.88014489
0.87960857
0.87930453
0.87898207
0.87874728
0.87878704
0.87908238
INFO - Training [24][   80/  196]   Loss 0.383355   Top1 86.445312   Top5 98.540039   BatchTime 0.396877   LR 0.000453
0.87912905
0.87922782
0.87915242
0.87926114
0.87946677
0.87968600
0.87980139
0.87988979
0.88016844
0.88020927
0.88007581
0.88007575
0.88010329
0.88022077
0.88053071
0.88086128
0.88064659
0.88052565
0.88060421
0.88069880
0.88089842
0.88108778
0.88135338
0.88191426
0.88204700
0.88274741
INFO - Training [24][  100/  196]   Loss 0.374235   Top1 86.851562   Top5 98.621094   BatchTime 0.378979   LR 0.000437
0.88337749
0.88428891
0.88521081
0.88641685
0.88765168
0.88785297
0.88832527
0.88888472
0.88939732
0.89004415
0.89045244
0.89052773
0.89068270
0.89062881
0.89073002
INFO - Training [24][  120/  196]   Loss 0.368319   Top1 87.119141   Top5 98.707682   BatchTime 0.361679   LR 0.000422
0.89079452
0.89086467
0.89081359
0.89065897
0.89043260
0.89025939
0.89019984
0.89032465
0.89018607
0.89007300
0.88984728
0.88957089
0.88961685
0.88955915
0.88944280
0.88956207
0.88969636
0.88967955
0.88973582
0.88939226
0.88926464
0.88932174
0.88951236
0.88940054
INFO - Training [24][  140/  196]   Loss 0.367162   Top1 87.126116   Top5 98.775112   BatchTime 0.357467   LR 0.000407
0.88933736
0.88893777
0.88903844
0.88873100
0.88872147
0.88855809
0.88825256
0.88784128
0.88746804
0.88744205
0.88664472
0.88579488
0.88558221
0.88537192
0.88500202
0.88481659
INFO - Training [24][  160/  196]   Loss 0.367895   Top1 87.121582   Top5 98.754883   BatchTime 0.359791   LR 0.000392
0.88462156
0.88470715
0.88458633
0.88405871
0.88385451
0.88355958
0.88359815
0.88333338
0.88315022
0.88296175
0.88278151
0.88252598
0.88218272
0.88169062
0.88155174
0.88149792
0.88201392
0.88213253
0.88152862
0.88109142
0.88091445
0.88068157
INFO - Training [24][  180/  196]   Loss 0.368346   Top1 87.113715   Top5 98.730469   BatchTime 0.360696   LR 0.000378
0.88056350
0.88071591
0.88086033
0.88108689
0.88125634
0.88136488
0.88144392
0.88146400
0.88165271
0.88155931
0.88144839
0.88131166
0.88147205
0.88170916
0.88217241
0.88224304
INFO - ==> Top1: 87.090    Top5: 98.714    Loss: 0.369
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.88220340
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 0.300338   Top1 90.117188   Top5 99.531250   BatchTime 0.125406
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1934)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0495)
features.2.conv.0 tensor(0.0223)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0419)
features.4.conv.0 tensor(0.0272)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0907)
features.5.conv.0 tensor(0.0363)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.1021)
features.6.conv.0 tensor(0.0259)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0514)
features.7.conv.0 tensor(0.0408)
features.7.conv.3 tensor(0.1097)
features.7.conv.6 tensor(0.1121)
features.8.conv.0 tensor(0.0556)
features.8.conv.3 tensor(0.1160)
features.8.conv.6 tensor(0.1182)
features.9.conv.0 tensor(0.0593)
features.9.conv.3 tensor(0.1675)
features.9.conv.6 tensor(0.0996)
features.10.conv.0 tensor(0.0331)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0692)
features.11.conv.0 tensor(0.0915)
features.11.conv.3 tensor(0.1507)
features.11.conv.6 tensor(0.1862)
features.12.conv.0 tensor(0.2325)
features.12.conv.3 tensor(0.1130)
features.12.conv.6 tensor(0.1817)
features.13.conv.0 tensor(0.0605)
features.13.conv.3 tensor(0.1786)
features.13.conv.6 tensor(0.0929)
features.14.conv.0 tensor(0.7690)
features.14.conv.3 tensor(0.0951)
features.14.conv.6 tensor(0.5400)
features.15.conv.0 tensor(0.8176)
features.15.conv.3 tensor(0.0817)
features.15.conv.6 tensor(0.8074)
features.16.conv.0 tensor(0.0733)
features.16.conv.3 tensor(0.1138)
features.16.conv.6 tensor(0.3030)
conv.0 tensor(0.1056)
tensor(674278.) 2188896.0
INFO - Validation [24][   40/   40]   Loss 0.294660   Top1 90.230000   Top5 99.680000   BatchTime 0.089238
INFO - ==> Top1: 90.230    Top5: 99.680    Loss: 0.295
INFO - ==> Sparsity : 0.308
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 88.830   Top5: 99.570]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 88.680   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.88189095
0.88180673
0.88159603
0.88118058
0.88119465
0.88117808
0.88101578
0.88065934
0.88047367
0.88060278
0.88016677
0.87989986
0.87969488
0.87982202
0.87962097
0.87947750
0.87936932
0.87931067
INFO - Training [25][   20/  196]   Loss 0.380148   Top1 85.976562   Top5 98.437500   BatchTime 0.439670   LR 0.000353
0.87899256
0.87878501
0.87865168
0.87887633
0.87935704
0.87910026
0.87917000
0.87941337
0.87938553
0.88064396
0.88054413
0.88065398
0.88084656
0.88090640
0.88084322
0.88086319
0.88080263
0.88080585
0.88104677
0.88136953
INFO - Training [25][   40/  196]   Loss 0.382920   Top1 86.357422   Top5 98.525391   BatchTime 0.418254   LR 0.000339
0.88149410
0.88135517
0.88162541
0.88176662
0.88179982
0.88195676
0.88191980
0.88217962
0.88224715
0.88237309
0.88220793
0.88181531
0.88178444
0.88160205
0.88134807
0.88123858
0.88127863
0.88111305
0.88115937
0.88129008
0.88121271
INFO - Training [25][   60/  196]   Loss 0.377134   Top1 86.569010   Top5 98.626302   BatchTime 0.403316   LR 0.000325
0.88107562
0.88089687
0.88056046
0.88038301
0.88039505
0.88036972
0.88081169
0.88079029
0.88066059
0.88077050
0.88069552
0.88041931
0.88021702
0.88018519
0.88018036
0.88008302
0.88001657
0.88012505
INFO - Training [25][   80/  196]   Loss 0.371502   Top1 86.811523   Top5 98.715820   BatchTime 0.386395   LR 0.000312
0.87984484
0.87966120
0.87973648
0.87984735
0.87976319
0.87973821
0.87978882
0.87974697
0.87970120
0.87976110
0.87966871
0.87970579
0.87977082
0.87949330
0.87929189
0.87901342
0.87887615
0.87876749
0.87859941
0.87853229
INFO - Training [25][  100/  196]   Loss 0.363710   Top1 87.148438   Top5 98.734375   BatchTime 0.369158   LR 0.000299
0.87840229
0.87838250
0.87822509
0.87825608
0.87844706
0.87863487
0.87867844
0.87901008
0.87907732
0.87886858
0.87842649
0.87850237
0.87855238
0.87839544
0.87832361
0.87823164
0.87816292
0.87828439
0.87819451
0.87796265
INFO - Training [25][  120/  196]   Loss 0.358412   Top1 87.304688   Top5 98.792318   BatchTime 0.358400   LR 0.000286
0.87809044
0.87825376
0.87823933
0.87812036
0.87819427
0.87809163
0.87786549
0.87778139
0.87767941
0.87771243
0.87827790
0.87914324
0.87928319
0.87919766
0.87931466
0.87895072
0.87878245
0.87885094
0.87878066
0.87867343
0.87861568
0.87885529
INFO - Training [25][  140/  196]   Loss 0.356718   Top1 87.433036   Top5 98.833705   BatchTime 0.361109   LR 0.000273
0.87905747
0.87976766
0.87963569
0.87915474
0.87973201
0.87979203
0.87981105
0.87957269
0.87901574
0.87852663
0.87857622
0.87847906
0.87868994
0.87884289
0.87905604
0.87942171
0.88057381
0.88074899
0.88075733
0.88061106
0.88020396
INFO - Training [25][  160/  196]   Loss 0.357010   Top1 87.500000   Top5 98.793945   BatchTime 0.363201   LR 0.000261
0.88010824
0.87990910
0.87997925
0.87997746
0.87995809
0.88017428
0.88032627
0.88049847
0.88088173
0.88093132
0.88079256
0.88051838
0.88056284
0.88083935
0.88071972
0.88077331
0.88078368
0.88089871
0.88075244
0.88069934
0.88069451
0.88052768
INFO - Training [25][  180/  196]   Loss 0.357298   Top1 87.532552   Top5 98.806424   BatchTime 0.363882   LR 0.000248
0.88033056
0.88022900
0.88016963
0.88024801
0.88031578
0.88039756
0.88042998
0.88058519
0.88072443
0.88082719
0.88051087
0.88030267
0.87936831
0.87922466
********************pre-trained*****************
INFO - ==> Top1: 87.628    Top5: 98.808    Loss: 0.355
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 0.314922   Top1 89.902344   Top5 99.609375   BatchTime 0.128375
INFO - Validation [25][   40/   40]   Loss 0.302855   Top1 90.050000   Top5 99.690000   BatchTime 0.092989
INFO - ==> Top1: 90.050    Top5: 99.690    Loss: 0.303
INFO - ==> Sparsity : 0.308
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 88.830   Top5: 99.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0490)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0421)
features.4.conv.0 tensor(0.0256)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0900)
features.5.conv.0 tensor(0.0361)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.1029)
features.6.conv.0 tensor(0.0228)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0430)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1121)
features.8.conv.0 tensor(0.0554)
features.8.conv.3 tensor(0.1149)
features.8.conv.6 tensor(0.1187)
features.9.conv.0 tensor(0.0595)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.0999)
features.10.conv.0 tensor(0.0314)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0693)
features.11.conv.0 tensor(0.0904)
features.11.conv.3 tensor(0.1501)
features.11.conv.6 tensor(0.1854)
features.12.conv.0 tensor(0.2341)
features.12.conv.3 tensor(0.1130)
features.12.conv.6 tensor(0.1780)
features.13.conv.0 tensor(0.0642)
features.13.conv.3 tensor(0.1784)
features.13.conv.6 tensor(0.0923)
features.14.conv.0 tensor(0.7750)
features.14.conv.3 tensor(0.0956)
features.14.conv.6 tensor(0.5415)
features.15.conv.0 tensor(0.8273)
features.15.conv.3 tensor(0.0811)
features.15.conv.6 tensor(0.8349)
features.16.conv.0 tensor(0.0729)
features.16.conv.3 tensor(0.1137)
features.16.conv.6 tensor(0.3005)
conv.0 tensor(0.0913)
tensor(674277.) 2188896.0
0.87935346
0.87955666
0.87969869
0.87974507
0.87971169
0.87954468
0.87945151
0.87959146
0.88174689
0.88168126
0.88182503
0.88211441
0.88217646
0.88186753
0.88185632
0.88181525
0.88156599
0.88149220
0.88147211
0.88131154
INFO - Training [26][   20/  196]   Loss 0.361853   Top1 86.972656   Top5 98.398438   BatchTime 0.443384   LR 0.000228
0.88108367
0.88134396
0.88144583
0.88158190
0.88156420
0.88145542
0.88165700
0.88177639
0.88188171
0.88162953
0.88167721
0.88155109
0.88157654
0.88147050
0.88124615
0.88101697
0.88095075
0.88108963
0.88101351
0.88087851
INFO - Training [26][   40/  196]   Loss 0.356325   Top1 87.324219   Top5 98.476562   BatchTime 0.422914   LR 0.000216
0.88093424
0.88106132
0.88100231
0.88069409
0.88060367
0.88062519
0.88041914
0.88025832
0.88009715
0.87963527
0.87951225
0.87951839
0.87954569
0.87948531
0.87936205
0.87928492
0.87935418
0.87931585
0.87922657
0.87914300
INFO - Training [26][   60/  196]   Loss 0.349969   Top1 87.643229   Top5 98.580729   BatchTime 0.411256   LR 0.000205
0.87886572
0.87866014
0.87841481
0.87824452
0.87840164
0.87836301
0.87840497
0.87831587
0.87806523
0.87816948
0.87824970
0.87812179
0.87804240
0.87792039
0.87724274
0.87719405
0.87729752
0.87740874
0.87753642
0.87752968
0.87754273
0.87752515
INFO - Training [26][   80/  196]   Loss 0.349557   Top1 87.739258   Top5 98.715820   BatchTime 0.402411   LR 0.000194
0.87749100
0.87751251
0.87746751
0.87728679
0.87689382
0.87665737
0.87658006
0.87669402
0.87670022
0.87648964
0.87643445
0.87624121
0.87750584
0.87744254
0.87742430
0.87732148
0.87697566
0.87694407
0.87712580
0.87714392
INFO - Training [26][  100/  196]   Loss 0.345801   Top1 87.847656   Top5 98.738281   BatchTime 0.381954   LR 0.000183
0.87705761
0.87706172
0.87717110
0.87744993
0.87797737
0.87858337
0.87874812
0.87868655
0.87864631
0.87851155
0.87852597
0.87863910
0.87880945
0.87903148
0.87888092
INFO - Training [26][  120/  196]   Loss 0.340274   Top1 88.020833   Top5 98.798828   BatchTime 0.366340   LR 0.000173
0.87903357
0.87930644
0.87919372
0.87938362
0.87944478
0.87939525
0.87931681
0.87938344
0.87924683
0.87911850
0.87915194
0.87930399
0.87923777
0.87917835
0.87922227
0.87904382
0.87933475
0.87928432
0.87935358
0.87919056
0.87911272
INFO - Training [26][  140/  196]   Loss 0.339477   Top1 88.077567   Top5 98.850446   BatchTime 0.368922   LR 0.000163
0.87914670
0.87909180
0.87895453
0.87887031
0.87887180
0.87904251
0.87923127
0.87929177
0.87931824
0.87936747
0.87901491
0.87920803
0.87923670
0.87893057
0.87867427
0.87853462
0.87854713
0.87870520
0.87860972
0.87859875
0.87846619
INFO - Training [26][  160/  196]   Loss 0.341855   Top1 88.039551   Top5 98.830566   BatchTime 0.369967   LR 0.000153
0.87859446
0.87836522
0.87822288
0.87824541
0.87816894
0.87820315
0.87817204
0.87820774
0.87816107
0.87822622
0.87831318
0.87840772
0.87842691
0.87819290
0.87790847
0.87787604
0.87780547
0.87779737
0.87786633
0.87779385
0.87783772
INFO - Training [26][  180/  196]   Loss 0.342392   Top1 88.027344   Top5 98.799913   BatchTime 0.371334   LR 0.000144
0.87800211
0.87797964
0.87789947
0.87771058
0.87762332
0.87767738
0.87763637
0.87787879
0.87771213
0.87769240
0.87769526
0.87766826
0.87758130
0.87750584
0.87750322
0.87746960
INFO - ==> Top1: 88.038    Top5: 98.806    Loss: 0.343
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.300569   Top1 90.664062   Top5 99.531250   BatchTime 0.124396
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1895)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0486)
features.2.conv.0 tensor(0.0208)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0799)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0412)
features.4.conv.0 tensor(0.0259)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0905)
features.5.conv.0 tensor(0.0366)
features.5.conv.3 tensor(0.0839)
features.5.conv.6 tensor(0.1007)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0519)
features.7.conv.0 tensor(0.0421)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.1121)
features.8.conv.0 tensor(0.0576)
features.8.conv.3 tensor(0.1175)
features.8.conv.6 tensor(0.1184)
features.9.conv.0 tensor(0.0612)
features.9.conv.3 tensor(0.1675)
features.9.conv.6 tensor(0.0996)
features.10.conv.0 tensor(0.0329)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0691)
features.11.conv.0 tensor(0.0931)
features.11.conv.3 tensor(0.1493)
features.11.conv.6 tensor(0.1861)
features.12.conv.0 tensor(0.2438)
features.12.conv.3 tensor(0.1127)
features.12.conv.6 tensor(0.1788)
features.13.conv.0 tensor(0.0632)
features.13.conv.3 tensor(0.1775)
features.13.conv.6 tensor(0.0925)
features.14.conv.0 tensor(0.7783)
features.14.conv.3 tensor(0.0956)
features.14.conv.6 tensor(0.5397)
features.15.conv.0 tensor(0.8338)
features.15.conv.3 tensor(0.0824)
features.15.conv.6 tensor(0.8767)
features.16.conv.0 tensor(0.0733)
features.16.conv.3 tensor(0.1125)
features.16.conv.6 tensor(0.3019)
conv.0 tensor(0.0933)
tensor(684106.) 2188896.0
INFO - Validation [26][   40/   40]   Loss 0.293244   Top1 90.420000   Top5 99.690000   BatchTime 0.089301
INFO - ==> Top1: 90.420    Top5: 99.690    Loss: 0.293
INFO - ==> Sparsity : 0.313
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.050   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
0.87725818
0.87729770
0.87747991
0.87777227
0.87816238
0.87823343
0.87828654
0.87799340
0.87758994
0.87728256
0.87703723
0.87675005
0.87661946
0.87633342
0.87622374
0.87622792
0.87611198
0.87604165
0.87611234
0.87612534
0.87598133
0.87593615
INFO - Training [27][   20/  196]   Loss 0.381619   Top1 86.953125   Top5 98.496094   BatchTime 0.455533   LR 0.000128
0.87579972
0.87592942
0.87612712
0.87596047
0.87583286
0.87576967
0.87578571
0.87558544
0.87537557
0.87535727
0.87549448
0.87533748
0.87525696
0.87535435
0.87535703
0.87518954
INFO - Training [27][   40/  196]   Loss 0.369554   Top1 87.148438   Top5 98.574219   BatchTime 0.415129   LR 0.000119
0.87523079
0.87551105
0.87539226
0.87533462
0.87518102
0.87524599
0.87528390
0.87524337
0.87494141
0.87487715
0.87497944
0.87521207
0.87528294
0.87544942
0.87523854
0.87529129
0.87529105
0.87520051
0.87504184
0.87511438
0.87496030
INFO - Training [27][   60/  196]   Loss 0.359126   Top1 87.259115   Top5 98.710938   BatchTime 0.402511   LR 0.000111
0.87493259
0.87486649
0.87490577
0.87478334
0.87471360
0.87475151
0.87490821
0.87508780
0.87519246
0.87527770
0.87526661
0.87538099
0.87535340
0.87529635
0.87536198
0.87563515
0.87540358
0.87535411
0.87521046
0.87499934
INFO - Training [27][   80/  196]   Loss 0.351470   Top1 87.646484   Top5 98.789062   BatchTime 0.374247   LR 0.000102
0.87481171
0.87474531
0.87475365
0.87482488
0.87474620
0.87487447
0.87478560
0.87478369
0.87472165
0.87465870
0.87448311
0.87443900
0.87456399
0.87459254
0.87456185
0.87460417
0.87465537
0.87472087
0.87455851
0.87458986
0.87445217
INFO - Training [27][  100/  196]   Loss 0.346001   Top1 87.824219   Top5 98.832031   BatchTime 0.358506   LR 0.000095
0.87436908
0.87426770
0.87415606
0.87405789
0.87397420
0.87392491
0.87390804
0.87418330
0.87431931
0.87422663
0.87427050
0.87434751
0.87435830
0.87432700
0.87432653
0.87445426
0.87442207
0.87432641
0.87430149
0.87417960
0.87398458
INFO - Training [27][  120/  196]   Loss 0.340646   Top1 88.040365   Top5 98.886719   BatchTime 0.361645   LR 0.000087
0.87371111
0.87348390
0.87336385
0.87343425
0.87342799
0.87333876
0.87364954
0.87383491
0.87383056
0.87367046
0.87357861
0.87347001
0.87328535
0.87319505
0.87321270
0.87317139
INFO - Training [27][  140/  196]   Loss 0.339588   Top1 88.108259   Top5 98.939732   BatchTime 0.363124   LR 0.000080
0.87312913
0.87312877
0.87312496
0.87317234
0.87301177
0.87293994
0.87293637
0.87289929
0.87281370
0.87281799
0.87285286
0.87288046
0.87288147
0.87296498
0.87329054
0.87323517
0.87329769
0.87332457
0.87310052
0.87302929
0.87306315
0.87312847
INFO - Training [27][  160/  196]   Loss 0.340131   Top1 88.117676   Top5 98.920898   BatchTime 0.363711   LR 0.000073
0.87314820
0.87315160
0.87319428
0.87301135
0.87293220
0.87277657
0.87275994
0.87272918
0.87256384
0.87239760
0.87233806
0.87221181
0.87198800
0.87181693
0.87181282
0.87188053
0.87186253
0.87180287
0.87173724
0.87169152
0.87166518
0.87156194
INFO - Training [27][  180/  196]   Loss 0.341687   Top1 88.064236   Top5 98.884549   BatchTime 0.363912   LR 0.000066
0.87147665
0.87155926
0.87144989
0.87089807
0.87071103
0.87063807
0.87065828
0.87092185
0.87068683
0.87064606
0.87186116
0.87195235
0.87214828
0.87144500
0.87045699
********************pre-trained*****************
INFO - ==> Top1: 88.100    Top5: 98.882    Loss: 0.341
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [27][   20/   40]   Loss 0.317384   Top1 89.941406   Top5 99.511719   BatchTime 0.132130
INFO - Validation [27][   40/   40]   Loss 0.311145   Top1 89.920000   Top5 99.630000   BatchTime 0.093559
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0486)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0417)
features.4.conv.0 tensor(0.0275)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0381)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.1024)
features.6.conv.0 tensor(0.0226)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0521)
features.7.conv.0 tensor(0.0413)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1129)
features.8.conv.0 tensor(0.0560)
features.8.conv.3 tensor(0.1157)
features.8.conv.6 tensor(0.1191)
features.9.conv.0 tensor(0.0610)
features.9.conv.3 tensor(0.1675)
features.9.conv.6 tensor(0.0994)
features.10.conv.0 tensor(0.0329)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0692)
features.11.conv.0 tensor(0.0936)
features.11.conv.3 tensor(0.1503)
features.11.conv.6 tensor(0.1861)
features.12.conv.0 tensor(0.2497)
features.12.conv.3 tensor(0.1128)
features.12.conv.6 tensor(0.1783)
features.13.conv.0 tensor(0.0626)
features.13.conv.3 tensor(0.1773)
features.13.conv.6 tensor(0.0923)
features.14.conv.0 tensor(0.7819)
features.14.conv.3 tensor(0.0954)
features.14.conv.6 tensor(0.5462)
features.15.conv.0 tensor(0.8400)
features.15.conv.3 tensor(0.0811)
features.15.conv.6 tensor(0.9050)
features.16.conv.0 tensor(0.0729)
features.16.conv.3 tensor(0.1133)
features.16.conv.6 tensor(0.3018)
conv.0 tensor(0.0994)
tensor(693605.) 2188896.0
INFO - ==> Top1: 89.920    Top5: 99.630    Loss: 0.311
INFO - ==> Sparsity : 0.317
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.050   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
0.87049007
0.87033993
0.87038124
0.87058306
0.87046403
0.87030286
0.87028098
0.87029833
0.87015671
0.87005460
0.86991411
0.86991334
0.86987668
0.86987895
0.86988127
0.86984843
0.86991370
0.86989939
INFO - Training [28][   20/  196]   Loss 0.352678   Top1 87.109375   Top5 98.593750   BatchTime 0.442463   LR 0.000055
0.86982566
0.86972916
0.86965781
0.86969280
0.86967224
0.86952960
0.86954516
0.86953312
0.86959076
0.86962503
0.87011504
0.87136728
0.87147021
0.87147880
0.87141532
0.87148327
0.87150538
0.87146503
0.87140536
0.87132835
INFO - Training [28][   40/  196]   Loss 0.350223   Top1 87.294922   Top5 98.681641   BatchTime 0.425807   LR 0.000050
0.87125725
0.87122196
0.87122601
0.87129748
0.87134445
0.87141758
0.87137026
0.87137800
0.87153065
0.87143928
0.87172687
0.87171644
0.87163132
0.87159854
0.87159812
0.87160277
0.87163711
0.87177408
0.87186271
0.87145883
0.87138683
INFO - Training [28][   60/  196]   Loss 0.346677   Top1 87.643229   Top5 98.678385   BatchTime 0.407737   LR 0.000044
0.87138158
0.87142712
0.87149465
0.87150955
0.87140995
0.87150055
0.87157100
0.87147439
0.87145817
0.87158614
0.87163699
0.87142271
0.87139589
0.87135738
0.87141377
0.87139475
0.87140131
0.87134606
0.87126160
0.87133902
INFO - Training [28][   80/  196]   Loss 0.344476   Top1 87.836914   Top5 98.759766   BatchTime 0.382364   LR 0.000039
0.87139988
0.87132823
0.87113190
0.87093306
0.87081611
0.87084502
0.87085682
0.87083107
0.87049454
0.87029201
0.87033868
0.87014902
0.87023681
0.87014854
0.87010247
0.87004858
0.86997253
0.86987221
0.86976689
0.86966163
0.86965722
0.86979544
0.86982834
INFO - Training [28][  100/  196]   Loss 0.341457   Top1 87.917969   Top5 98.816406   BatchTime 0.376711   LR 0.000034
0.86971277
0.86967695
0.86958796
0.86958641
0.86974955
0.86986172
0.86981612
0.86997843
0.86979759
0.86955655
0.86947548
0.86950177
0.86941683
0.86930209
0.86920619
0.86912507
INFO - Training [28][  120/  196]   Loss 0.336224   Top1 88.144531   Top5 98.889974   BatchTime 0.374764   LR 0.000030
0.86883527
0.86933899
0.86919045
0.86902249
0.86894947
0.86872536
0.86815786
0.86771619
0.86757320
0.86755788
0.86757046
0.86758775
0.86758643
0.86755359
0.86751413
0.86754912
0.86755520
0.86755461
0.86751592
0.86753136
0.86745185
INFO - Training [28][  140/  196]   Loss 0.333111   Top1 88.306362   Top5 98.945312   BatchTime 0.374987   LR 0.000026
0.86743790
0.86762595
0.86771065
0.86751550
0.86757934
0.86757356
0.86755168
0.86752343
0.86748582
0.86747271
0.86748463
0.86745757
0.86742991
0.86739480
0.86736131
0.86733180
0.86735302
0.86737311
0.86738652
0.86741245
0.86737543
0.86735237
INFO - Training [28][  160/  196]   Loss 0.338621   Top1 88.105469   Top5 98.950195   BatchTime 0.374772   LR 0.000022
0.86736506
0.86742145
0.86739022
0.86735576
0.86733562
0.86738253
0.86738425
0.86737406
0.86735892
0.86734772
0.86732674
0.86728615
0.86729467
0.86732012
0.86733764
0.86733478
INFO - Training [28][  180/  196]   Loss 0.340463   Top1 88.046875   Top5 98.882378   BatchTime 0.373245   LR 0.000018
0.86732137
0.86730266
0.86725384
0.86729777
0.86730564
0.86729819
0.86729044
0.86728245
0.86728847
0.86723113
0.86722904
0.86720675
0.86723822
0.86724126
0.86723238
0.86725670
0.86721271
INFO - ==> Top1: 88.180    Top5: 98.886    Loss: 0.338
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.86718744
0.86721808
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.305847   Top1 90.253906   Top5 99.609375   BatchTime 0.144640
INFO - Validation [28][   40/   40]   Loss 0.294123   Top1 90.450000   Top5 99.730000   BatchTime 0.096932
INFO - ==> Top1: 90.450    Top5: 99.730    Loss: 0.294
INFO - ==> Sparsity : 0.317
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0961)
features.1.conv.6 tensor(0.0486)
features.2.conv.0 tensor(0.0205)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0807)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0417)
features.4.conv.0 tensor(0.0282)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0890)
features.5.conv.0 tensor(0.0369)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.1047)
features.6.conv.0 tensor(0.0226)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0414)
features.7.conv.3 tensor(0.1111)
features.7.conv.6 tensor(0.1121)
features.8.conv.0 tensor(0.0556)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.1200)
features.9.conv.0 tensor(0.0610)
features.9.conv.3 tensor(0.1652)
features.9.conv.6 tensor(0.0989)
features.10.conv.0 tensor(0.0333)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0693)
features.11.conv.0 tensor(0.0938)
features.11.conv.3 tensor(0.1507)
features.11.conv.6 tensor(0.1870)
features.12.conv.0 tensor(0.2494)
features.12.conv.3 tensor(0.1127)
features.12.conv.6 tensor(0.1799)
features.13.conv.0 tensor(0.0631)
features.13.conv.3 tensor(0.1773)
features.13.conv.6 tensor(0.0925)
features.14.conv.0 tensor(0.7844)
features.14.conv.3 tensor(0.0954)
features.14.conv.6 tensor(0.5516)
features.15.conv.0 tensor(0.8438)
features.15.conv.3 tensor(0.0817)
features.15.conv.6 tensor(0.9065)
features.16.conv.0 tensor(0.0730)
features.16.conv.3 tensor(0.1135)
features.16.conv.6 tensor(0.3014)
conv.0 tensor(0.0972)
tensor(694811.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
0.86725503
0.86725205
0.86724204
0.86721611
0.86720783
0.86720455
0.86721092
0.86721385
0.86724353
0.86724859
0.86723173
0.86718225
0.86713600
0.86716217
0.86717951
0.86719924
0.86716861
0.86717302
0.86715180
0.86717808
0.86713082
INFO - Training [29][   20/  196]   Loss 0.354711   Top1 87.343750   Top5 98.574219   BatchTime 0.475524   LR 0.000013
0.86716777
0.86721009
0.86721164
0.86722016
0.86723900
0.86724395
0.86723888
0.86720330
0.86721808
0.86720979
0.86721253
0.86717021
0.86717182
0.86719537
0.86720490
0.86719513
0.86718196
INFO - Training [29][   40/  196]   Loss 0.354919   Top1 87.382812   Top5 98.681641   BatchTime 0.411197   LR 0.000010
0.86718810
0.86716598
0.86715573
0.86715937
0.86717302
0.86717731
0.86713499
0.86713338
0.86712110
0.86712545
0.86708981
0.86712188
0.86713940
0.86718088
0.86718565
0.86721551
0.86720937
0.86715794
0.86716336
0.86714703
INFO - Training [29][   60/  196]   Loss 0.360857   Top1 87.180990   Top5 98.743490   BatchTime 0.371521   LR 0.000008
0.86710840
0.86711764
0.86709768
0.86712837
0.86714590
0.86712700
0.86711842
0.86711967
0.86714798
0.86713737
0.86714429
0.86714923
0.86713517
0.86708933
0.86705148
0.86705267
0.86703539
0.86702466
0.86703253
INFO - Training [29][   80/  196]   Loss 0.358517   Top1 87.353516   Top5 98.862305   BatchTime 0.361480   LR 0.000005
0.86705029
0.86704874
0.86704916
0.86702698
0.86702818
0.86705017
0.86703932
0.86704278
0.86703873
0.86703074
0.86703569
0.86703265
0.86704123
0.86705381
0.86704320
0.86703986
0.86696637
0.86695492
0.86696982
0.86698049
0.86700433
0.86699986
INFO - Training [29][  100/  196]   Loss 0.348958   Top1 87.832031   Top5 98.914062   BatchTime 0.362155   LR 0.000004
0.86698949
0.86700284
0.86699957
0.86702681
0.86704218
0.86702967
0.86704087
0.86703199
0.86701500
0.86698586
0.86699921
0.86700559
0.86703014
0.86702752
0.86704117
0.86701900
0.86701924
0.86698675
0.86700904
0.86701339
0.86697185
INFO - Training [29][  120/  196]   Loss 0.344342   Top1 87.998047   Top5 98.932292   BatchTime 0.364305   LR 0.000002
0.86698622
0.86698061
0.86700016
0.86699903
0.86702704
0.86701983
0.86704457
0.86702538
0.86701703
0.86700207
0.86697441
0.86695671
0.86696881
0.86695170
0.86694705
0.86696917
0.86698079
0.86699057
0.86696190
0.86697811
0.86696422
0.86695874
INFO - Training [29][  140/  196]   Loss 0.343746   Top1 88.035714   Top5 98.959263   BatchTime 0.364538   LR 0.000001
0.86700094
0.86700749
0.86699295
0.86696750
0.86696994
0.86694425
0.86696047
0.86697036
0.86696583
0.86694258
0.86693710
0.86697102
0.86695933
0.86694831
0.86695516
0.86698467
0.86699045
INFO - Training [29][  160/  196]   Loss 0.345787   Top1 87.924805   Top5 98.947754   BatchTime 0.364486   LR 0.000001
0.86697400
0.86695814
0.86698735
0.86702204
0.86701280
0.86700171
0.86701852
0.86700106
0.86700428
0.86701059
0.86699396
0.86700231
0.86698985
0.86700392
0.86701226
0.86704093
0.86701423
0.86697745
0.86698508
0.86694902
0.86695677
INFO - Training [29][  180/  196]   Loss 0.345840   Top1 87.936198   Top5 98.871528   BatchTime 0.365734   LR 0.000000
0.86693966
0.86693233
0.86694926
0.86693943
0.86694121
0.86694384
0.86696452
0.86696297
0.86695570
0.86693525
0.86695015
0.86698568
0.86695939
0.86698449
0.86697084
0.86698067
INFO - ==> Top1: 87.960    Top5: 98.852    Loss: 0.345
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [29][   20/   40]   Loss 0.311966   Top1 89.863281   Top5 99.453125   BatchTime 0.122293
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0208)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0813)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0417)
features.4.conv.0 tensor(0.0285)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0895)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.1048)
features.6.conv.0 tensor(0.0228)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0415)
features.7.conv.3 tensor(0.1105)
features.7.conv.6 tensor(0.1120)
features.8.conv.0 tensor(0.0555)
features.8.conv.3 tensor(0.1157)
features.8.conv.6 tensor(0.1193)
features.9.conv.0 tensor(0.0611)
features.9.conv.3 tensor(0.1661)
features.9.conv.6 tensor(0.0989)
features.10.conv.0 tensor(0.0333)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0693)
features.11.conv.0 tensor(0.0938)
features.11.conv.3 tensor(0.1503)
features.11.conv.6 tensor(0.1882)
features.12.conv.0 tensor(0.2492)
features.12.conv.3 tensor(0.1134)
features.12.conv.6 tensor(0.1808)
features.13.conv.0 tensor(0.0631)
features.13.conv.3 tensor(0.1769)
features.13.conv.6 tensor(0.0924)
features.14.conv.0 tensor(0.7844)
features.14.conv.3 tensor(0.0958)
features.14.conv.6 tensor(0.5545)
features.15.conv.0 tensor(0.8437)
features.15.conv.3 tensor(0.0823)
features.15.conv.6 tensor(0.9047)
features.16.conv.0 tensor(0.0730)
features.16.conv.3 tensor(0.1140)
features.16.conv.6 tensor(0.3016)
conv.0 tensor(0.0985)
tensor(695702.) 2188896.0
INFO - Validation [29][   40/   40]   Loss 0.305643   Top1 90.010000   Top5 99.610000   BatchTime 0.089609
INFO - ==> Top1: 90.010    Top5: 99.610    Loss: 0.306
INFO - ==> Sparsity : 0.318
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
0.86694598
0.87528414
0.87481999
0.87751198
0.87787247
0.87800497
0.87785935
0.87776142
0.87786293
0.87789333
0.87794739
0.87842274
0.87943596
0.88197285
0.88321251
0.88547313
0.88770860
0.89011121
0.89283669
0.89569408
INFO - Training [30][   20/  196]   Loss 0.374096   Top1 87.070312   Top5 98.164062   BatchTime 0.406990   LR 0.001250
0.89839894
0.90065587
0.90251476
0.90401655
0.90549457
0.90633351
0.90645307
0.90664768
0.90675825
0.90670657
0.90652031
0.90650994
0.90652645
0.90646237
0.90646917
0.90647936
0.90666062
0.90666723
0.90673786
0.90676677
0.90673411
INFO - Training [30][   40/  196]   Loss 0.380244   Top1 86.816406   Top5 98.476562   BatchTime 0.398596   LR 0.001250
0.90651208
0.90641207
0.90625530
0.90641403
0.90643293
0.90653741
0.90674943
0.90678608
0.90677232
0.90681052
0.90676945
0.90676337
0.90680826
0.90706491
0.90690267
0.90677899
0.90795481
0.90815026
0.90815717
INFO - Training [30][   60/  196]   Loss 0.383836   Top1 86.660156   Top5 98.522135   BatchTime 0.369919   LR 0.001250
0.90828580
0.90834922
0.90827256
0.90845066
0.90868908
0.90878361
0.90879631
0.90873313
0.90862072
0.90861356
0.90845805
0.90831375
0.90841329
0.90830332
0.90847450
0.90860206
0.90879744
0.90916085
0.90950823
0.90946054
0.90956384
INFO - Training [30][   80/  196]   Loss 0.387005   Top1 86.689453   Top5 98.662109   BatchTime 0.372157   LR 0.001250
0.90923917
0.90926993
0.90927315
0.90912932
0.90905440
0.90916109
0.90924567
0.90936452
0.90928310
0.90928948
0.90928495
0.90920675
0.90925860
0.90928960
0.90923178
0.90930748
INFO - Training [30][  100/  196]   Loss 0.384448   Top1 86.757812   Top5 98.679688   BatchTime 0.372134   LR 0.001250
0.90946531
0.90952694
0.90939242
0.90928411
0.90893209
0.90891039
0.90874630
0.90869433
0.90870333
0.90851408
0.90839005
0.90799201
0.90837014
0.90868163
0.90898967
0.90891302
0.90907758
0.90901965
0.90901393
0.90886170
0.90883595
INFO - Training [30][  120/  196]   Loss 0.382455   Top1 86.842448   Top5 98.756510   BatchTime 0.373650   LR 0.001249
0.90920132
0.90934086
0.90917408
0.90918410
0.90922320
0.90946430
0.90943038
0.90924162
0.90924442
0.90907937
0.90904528
0.90901953
0.90884262
0.90886271
0.90891302
0.90888345
0.90883803
0.90900326
0.90904987
0.90894860
0.90886551
0.90882295
0.90897751
INFO - Training [30][  140/  196]   Loss 0.382134   Top1 86.880580   Top5 98.797433   BatchTime 0.371235   LR 0.001249
0.90901059
0.90900278
0.90897620
0.90898848
0.90891862
0.90895444
0.90892267
0.90882748
0.90881866
0.90883553
0.90938568
0.90928948
0.90906686
0.90924418
0.90909040
0.90883946
INFO - Training [30][  160/  196]   Loss 0.386555   Top1 86.687012   Top5 98.784180   BatchTime 0.370120   LR 0.001249
0.90876174
0.90859336
0.90827560
0.90780258
0.90715617
0.90699822
0.90687096
0.90907973
0.90899152
0.90888846
0.90872157
0.90876251
0.90888935
0.90903157
0.90927613
0.90914559
0.90899229
0.90895766
0.90885901
0.90897483
0.90883005
0.90878743
INFO - Training [30][  180/  196]   Loss 0.388489   Top1 86.540799   Top5 98.747830   BatchTime 0.370572   LR 0.001248
0.90851635
0.90776396
0.90733868
0.90719497
0.90713757
0.90714532
0.90702003
0.90761286
0.90864104
0.90864521
0.90857452
0.90849710
0.90848482
0.90825456
0.90823108
0.90837318
INFO - ==> Top1: 86.496    Top5: 98.710    Loss: 0.390
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.90707773
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.390860   Top1 87.871094   Top5 99.550781   BatchTime 0.120659
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.1953)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0464)
features.2.conv.0 tensor(0.0191)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0764)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0373)
features.4.conv.0 tensor(0.0270)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0352)
features.5.conv.3 tensor(0.0822)
features.5.conv.6 tensor(0.0973)
features.6.conv.0 tensor(0.0260)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0234)
features.7.conv.3 tensor(0.1108)
features.7.conv.6 tensor(0.1125)
features.8.conv.0 tensor(0.0641)
features.8.conv.3 tensor(0.1126)
features.8.conv.6 tensor(0.1177)
features.9.conv.0 tensor(0.4006)
features.9.conv.3 tensor(0.1429)
features.9.conv.6 tensor(0.0967)
features.10.conv.0 tensor(0.0373)
features.10.conv.3 tensor(0.1045)
features.10.conv.6 tensor(0.0671)
features.11.conv.0 tensor(0.0934)
features.11.conv.3 tensor(0.1510)
features.11.conv.6 tensor(0.1846)
features.12.conv.0 tensor(0.1043)
features.12.conv.3 tensor(0.1181)
features.12.conv.6 tensor(0.1824)
features.13.conv.0 tensor(0.0485)
features.13.conv.3 tensor(0.1757)
features.13.conv.6 tensor(0.0903)
features.14.conv.0 tensor(0.7612)
features.14.conv.3 tensor(0.1002)
features.14.conv.6 tensor(0.5610)
features.15.conv.0 tensor(0.8207)
features.15.conv.3 tensor(0.0841)
features.15.conv.6 tensor(0.4257)
features.16.conv.0 tensor(0.0724)
features.16.conv.3 tensor(0.1154)
features.16.conv.6 tensor(0.2909)
conv.0 tensor(0.0762)
tensor(602306.) 2188896.0
INFO - Validation [30][   40/   40]   Loss 0.377415   Top1 87.790000   Top5 99.570000   BatchTime 0.088511
INFO - ==> Top1: 87.790    Top5: 99.570    Loss: 0.377
INFO - ==> Sparsity : 0.275
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
0.90381736
0.90573490
0.90622443
0.90788609
0.90844935
0.90843832
0.90859342
0.90854681
0.90862119
0.90869272
0.90884167
0.90855485
0.90844250
0.90853894
0.90836883
0.90807307
0.90778702
INFO - Training [31][   20/  196]   Loss 0.420186   Top1 84.960938   Top5 98.320312   BatchTime 0.451096   LR 0.001248
0.90810496
0.90810806
0.90786237
0.90739411
0.90795690
0.90827459
0.90832096
0.90851885
0.90880418
0.90924650
0.90952545
0.90977806
0.90972400
0.90985733
0.90996295
0.90994447
0.90994972
0.90999806
0.90990144
0.90990424
0.90978038
0.90983307
0.90978736
0.91000450
0.90998429
INFO - Training [31][   40/  196]   Loss 0.418933   Top1 85.380859   Top5 98.466797   BatchTime 0.384989   LR 0.001247
0.91085714
0.91070503
0.91100073
0.91111922
0.91137457
0.91139024
0.91144228
0.91135865
0.91155648
0.91161519
0.91217846
0.91247123
0.91276044
0.91298914
0.91312844
0.91406298
0.91445005
0.91459984
0.91476631
0.91480368
INFO - Training [31][   60/  196]   Loss 0.408934   Top1 85.677083   Top5 98.580729   BatchTime 0.388840   LR 0.001247
0.91479653
0.91475064
0.91481256
0.91472566
0.91467983
0.91459465
0.91455281
0.91441077
0.91436160
0.91432148
0.91426307
0.91434437
0.91445351
0.91448849
0.91438371
0.91422665
INFO - Training [31][   80/  196]   Loss 0.403531   Top1 85.937500   Top5 98.657227   BatchTime 0.388155   LR 0.001246
0.91410077
0.91410571
0.91352075
0.91337502
0.91341788
0.91363269
0.91372222
0.91353911
0.91332436
0.91302574
0.91284144
0.91270208
0.91246450
0.91199672
0.91161460
0.91153961
0.91135687
0.91117483
0.91079354
0.91022408
0.91054398
INFO - Training [31][  100/  196]   Loss 0.395764   Top1 86.101562   Top5 98.683594   BatchTime 0.383782   LR 0.001246
0.91112775
0.91094351
0.91073352
0.91084784
0.91062665
0.91008985
0.90981275
0.90947330
0.90912712
0.90883607
0.90857440
0.90834874
0.90830702
0.90853590
0.90816170
0.90767497
0.90761405
0.90761530
0.90733594
0.90698433
0.90675610
0.90649116
INFO - Training [31][  120/  196]   Loss 0.390820   Top1 86.269531   Top5 98.727214   BatchTime 0.380018   LR 0.001245
0.90621775
0.90580136
0.90560406
0.90541893
0.90533823
0.90497178
0.90458906
0.90357840
0.90242350
0.90150291
0.90053219
0.89896417
0.89780349
0.89706904
0.89515555
0.88956940
0.87224770
INFO - Training [31][  140/  196]   Loss 0.389034   Top1 86.336496   Top5 98.775112   BatchTime 0.378421   LR 0.001244
0.86797303
0.86706901
0.86650276
0.86642224
0.86794698
0.87670869
0.88786113
0.88940346
0.88879246
0.88785881
0.88665926
0.88610834
0.88588178
0.88546520
0.88521099
0.88487995
0.88456583
0.88443571
0.88417262
0.88384908
0.88401210
0.88465339
INFO - Training [31][  160/  196]   Loss 0.392096   Top1 86.279297   Top5 98.762207   BatchTime 0.376181   LR 0.001244
0.88547921
0.88625824
0.88659680
0.88668805
0.88687474
0.88697803
0.88710713
0.88789672
0.88914371
0.88973475
0.89012313
0.89021444
0.89012784
0.89051992
0.89065635
0.89027995
0.88946050
0.88908023
0.88851124
0.88733584
0.88673043
0.88720429
INFO - Training [31][  180/  196]   Loss 0.392710   Top1 86.215278   Top5 98.721788   BatchTime 0.375298   LR 0.001243
0.88614511
0.88541615
0.88466191
0.88431019
0.88425994
0.88415766
0.88365626
0.88239342
0.88210523
0.88191605
0.88239402
0.88252550
0.88278455
0.88309163
********************pre-trained*****************
INFO - ==> Top1: 86.218    Top5: 98.730    Loss: 0.393
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [31][   20/   40]   Loss 0.354286   Top1 88.476562   Top5 99.433594   BatchTime 0.138976
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1973)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0995)
features.1.conv.6 tensor(0.0508)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0796)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0408)
features.4.conv.0 tensor(0.0210)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0361)
features.5.conv.3 tensor(0.0799)
features.5.conv.6 tensor(0.1001)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0502)
features.7.conv.0 tensor(0.0261)
features.7.conv.3 tensor(0.1120)
features.7.conv.6 tensor(0.1098)
features.8.conv.0 tensor(0.0540)
features.8.conv.3 tensor(0.1195)
features.8.conv.6 tensor(0.1152)
features.9.conv.0 tensor(0.0487)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.0963)
features.10.conv.0 tensor(0.0408)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0664)
features.11.conv.0 tensor(0.0930)
features.11.conv.3 tensor(0.1524)
features.11.conv.6 tensor(0.1805)
features.12.conv.0 tensor(0.1108)
features.12.conv.3 tensor(0.1190)
features.12.conv.6 tensor(0.1793)
features.13.conv.0 tensor(0.0529)
features.13.conv.3 tensor(0.1788)
features.13.conv.6 tensor(0.0908)
features.14.conv.0 tensor(0.7016)
features.14.conv.3 tensor(0.1009)
features.14.conv.6 tensor(0.5763)
features.15.conv.0 tensor(0.8337)
features.15.conv.3 tensor(0.0856)
features.15.conv.6 tensor(0.8813)
features.16.conv.0 tensor(0.0527)
features.16.conv.3 tensor(0.1153)
features.16.conv.6 tensor(0.2718)
conv.0 tensor(0.0895)
tensor(655445.) 2188896.0
INFO - Validation [31][   40/   40]   Loss 0.349807   Top1 88.420000   Top5 99.530000   BatchTime 0.098215
INFO - ==> Top1: 88.420    Top5: 99.530    Loss: 0.350
INFO - ==> Sparsity : 0.299
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
0.88341558
0.88391840
0.88438946
0.88489479
0.88560915
0.88675511
0.88748509
0.88850796
0.88903314
0.88918382
0.88936460
0.88934475
0.88972932
0.88950723
0.88924938
0.89068979
0.89129859
0.89242333
0.89285439
0.89357615
0.89409953
INFO - Training [32][   20/  196]   Loss 0.416367   Top1 85.878906   Top5 97.968750   BatchTime 0.400085   LR 0.001242
0.89514023
0.89551514
0.89579517
0.89575469
0.89565790
0.89511836
0.89525789
0.89539552
0.89518034
0.89441162
0.89360237
0.89243305
0.89113599
0.88985157
0.88867211
0.88775820
0.88723874
0.88658512
0.88528955
INFO - Training [32][   40/  196]   Loss 0.403604   Top1 86.113281   Top5 98.378906   BatchTime 0.366712   LR 0.001241
0.88443071
0.88389701
0.88401163
0.88401848
0.88380462
0.88376355
0.88371462
0.88431638
0.88450855
0.88414723
0.88404149
0.88369060
0.88388842
0.88398665
0.88396454
0.88383383
0.88406307
0.88366342
0.88317752
INFO - Training [32][   60/  196]   Loss 0.409419   Top1 85.859375   Top5 98.430990   BatchTime 0.381375   LR 0.001240
0.88326651
0.88397372
0.88475811
0.88480306
0.88491976
0.88495374
0.88476968
0.88487202
0.88484341
0.88465804
0.88445717
0.88411731
0.88398683
0.88392246
0.88383436
0.88295430
0.88379604
0.88363498
0.88361013
0.88385898
0.88430738
INFO - Training [32][   80/  196]   Loss 0.408052   Top1 85.820312   Top5 98.530273   BatchTime 0.378453   LR 0.001239
0.88456601
0.88458902
0.88468832
0.88472325
0.88469023
0.88481122
0.88499385
0.88518876
0.88511652
0.88539177
0.88600630
0.88629490
0.88618267
0.88662106
0.88679010
0.88679594
0.88724798
0.88806868
0.89042646
0.89325356
0.89617014
0.89956856
INFO - Training [32][  100/  196]   Loss 0.400917   Top1 85.945312   Top5 98.636719   BatchTime 0.376833   LR 0.001238
0.90129548
0.90264612
0.90302891
0.90354776
0.90414828
0.90466583
0.90321898
0.90276664
0.89513069
0.88983953
0.89087093
0.89836782
0.90405905
0.90644580
0.90749878
0.90785521
INFO - Training [32][  120/  196]   Loss 0.396198   Top1 86.116536   Top5 98.704427   BatchTime 0.378451   LR 0.001237
0.90822995
0.90902334
0.90940130
0.90984231
0.91006535
0.91000628
0.90997654
0.90987384
0.90990567
0.90987182
0.90984255
0.91002625
0.91009116
0.90989232
0.91024321
0.90987736
0.90970570
0.90963662
0.90918076
0.90864277
INFO - Training [32][  140/  196]   Loss 0.391509   Top1 86.291853   Top5 98.775112   BatchTime 0.378892   LR 0.001236
0.90846986
0.90867096
0.90932435
0.90911126
0.90893960
0.90933794
0.90911132
0.90877026
0.90976208
0.90973192
0.90970445
0.91002744
0.90984017
0.90971231
0.90972370
0.90976626
0.90968502
0.90967989
0.90966862
0.90963745
0.90976602
0.90979844
INFO - Training [32][  160/  196]   Loss 0.393831   Top1 86.225586   Top5 98.752441   BatchTime 0.378248   LR 0.001235
0.90983909
0.90971494
0.90960652
0.90938866
0.90930086
0.90923458
0.90935683
0.90955299
0.90960288
0.90945393
0.90947139
0.90955991
0.90960169
0.90953392
0.90937471
0.90931243
0.90925670
0.90926719
0.90929431
0.90910923
0.90880930
0.90864003
INFO - Training [32][  180/  196]   Loss 0.394217   Top1 86.219618   Top5 98.704427   BatchTime 0.377373   LR 0.001234
0.90854788
0.90815955
0.90782589
0.90695930
0.90670407
0.90712464
0.90729225
0.90761101
0.90721023
0.90612811
0.88533556
0.88175535
0.88149077
0.88129508
********************pre-trained*****************
INFO - ==> Top1: 86.182    Top5: 98.692    Loss: 0.394
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [32][   20/   40]   Loss 0.345877   Top1 88.496094   Top5 99.492188   BatchTime 0.129828
INFO - Validation [32][   40/   40]   Loss 0.338766   Top1 88.310000   Top5 99.570000   BatchTime 0.092758
features.0.conv.0 tensor(0.4479)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.1019)
features.1.conv.6 tensor(0.0495)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0836)
features.3.conv.0 tensor(0.0220)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0388)
features.4.conv.0 tensor(0.0212)
features.4.conv.3 tensor(0.0926)
features.4.conv.6 tensor(0.0861)
features.5.conv.0 tensor(0.0365)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.1001)
features.6.conv.0 tensor(0.0277)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0505)
features.7.conv.0 tensor(0.0286)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.1075)
features.8.conv.0 tensor(0.0596)
features.8.conv.3 tensor(0.1181)
features.8.conv.6 tensor(0.1144)
features.9.conv.0 tensor(0.0562)
features.9.conv.3 tensor(0.1698)
features.9.conv.6 tensor(0.0966)
features.10.conv.0 tensor(0.0346)
features.10.conv.3 tensor(0.1076)
features.10.conv.6 tensor(0.0653)
features.11.conv.0 tensor(0.0933)
features.11.conv.3 tensor(0.1535)
features.11.conv.6 tensor(0.1786)
features.12.conv.0 tensor(0.1077)
features.12.conv.3 tensor(0.1236)
features.12.conv.6 tensor(0.1823)
features.13.conv.0 tensor(0.0558)
features.13.conv.3 tensor(0.1821)
features.13.conv.6 tensor(0.0835)
features.14.conv.0 tensor(0.7120)
features.14.conv.3 tensor(0.1045)
features.14.conv.6 tensor(0.9994)
features.15.conv.0 tensor(0.8379)
features.15.conv.3 tensor(0.0843)
features.15.conv.6 tensor(0.3866)
features.16.conv.0 tensor(0.0589)
features.16.conv.3 tensor(0.1178)
features.16.conv.6 tensor(0.2707)
conv.0 tensor(0.1053)
tensor(653401.) 2188896.0
INFO - ==> Top1: 88.310    Top5: 99.570    Loss: 0.339
INFO - ==> Sparsity : 0.299
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
0.88133490
0.88125265
0.88123208
0.88114405
0.88118601
0.88122994
0.88167751
0.88157535
0.88144863
0.88141811
0.88144630
0.88147920
0.88165367
0.88214511
0.88311768
0.88482702
0.88742596
0.89464313
INFO - Training [33][   20/  196]   Loss 0.415006   Top1 85.546875   Top5 98.339844   BatchTime 0.358763   LR 0.001232
0.90394652
0.91070849
0.91069847
0.91047311
0.91042280
0.91047108
0.91063350
0.91066432
0.91037828
0.91007876
0.90998167
0.91010755
0.91009152
0.90993613
0.91009742
0.90996289
0.91010952
0.91007191
0.90991586
0.90979832
0.90978265
0.91018760
INFO - Training [33][   40/  196]   Loss 0.410317   Top1 85.527344   Top5 98.378906   BatchTime 0.362390   LR 0.001230
0.90995461
0.90985096
0.90977275
0.91003865
0.90981978
0.90964437
0.90975779
0.90969723
0.90939450
0.90941215
0.90945917
0.90962255
0.90964681
0.90972441
0.90987700
0.90972531
0.90955567
0.90949333
0.90946412
0.90947467
INFO - Training [33][   60/  196]   Loss 0.404388   Top1 85.820312   Top5 98.509115   BatchTime 0.377899   LR 0.001229
0.90945357
0.90929031
0.90933871
0.90948850
0.90959728
0.90954697
0.90959126
0.90950257
0.90926886
0.90910327
0.90929842
0.90920186
0.90926611
0.90921676
0.90922731
0.90920728
0.90895134
0.90877491
0.90896362
0.90894240
0.90855980
0.90854329
INFO - Training [33][   80/  196]   Loss 0.404855   Top1 85.917969   Top5 98.579102   BatchTime 0.374439   LR 0.001228
0.90836948
0.90832859
0.90778631
0.90813595
0.90792459
0.90772223
0.90741038
0.90722519
0.90685529
0.90697974
0.90705484
0.90699786
0.90717733
0.90699017
0.90695381
0.90689540
INFO - Training [33][  100/  196]   Loss 0.400162   Top1 86.050781   Top5 98.625000   BatchTime 0.372948   LR 0.001226
0.90646946
0.90629548
0.90604478
0.90598857
0.90586758
0.90578026
0.90561730
0.90550113
0.90555125
0.90548670
0.90543783
0.90570849
0.90537935
0.90508479
0.90467572
0.90432584
0.90396816
0.90372539
0.90362674
0.90320909
0.90296018
INFO - Training [33][  120/  196]   Loss 0.395840   Top1 86.266276   Top5 98.697917   BatchTime 0.374352   LR 0.001225
0.90262938
0.90185463
0.90066415
0.89942640
0.89810550
0.89604080
0.89349747
0.89172024
0.89045578
0.88945121
0.88885307
0.88830233
0.88704729
0.88626629
0.88478166
0.88405138
0.88368911
0.88362473
0.88360256
0.88321966
0.88355869
INFO - Training [33][  140/  196]   Loss 0.395179   Top1 86.314174   Top5 98.730469   BatchTime 0.375417   LR 0.001224
0.88347971
0.88413006
0.88509762
0.88513237
0.88480467
0.88447595
0.88367218
0.88275975
0.88233960
0.88236493
0.88250721
0.88223010
0.88208377
0.88213384
0.88236463
0.88255614
0.88277596
0.88324797
0.88292485
0.88238347
0.88217628
0.88193995
INFO - Training [33][  160/  196]   Loss 0.395463   Top1 86.325684   Top5 98.757324   BatchTime 0.374682   LR 0.001222
0.88163114
0.88111639
0.88125521
0.88147724
0.88097966
0.88099217
0.88069540
0.88055325
0.88050318
0.88120025
0.88182157
0.88249272
0.88367915
0.88530046
0.88705248
0.88892835
INFO - Training [33][  180/  196]   Loss 0.398062   Top1 86.226128   Top5 98.695747   BatchTime 0.373726   LR 0.001221
0.89047098
0.89156687
0.89214402
0.89342052
0.89444226
0.89479476
0.89559531
0.89650708
0.89620304
0.89597458
0.89491487
0.89383209
0.89351875
0.89268267
0.89107251
0.89015728
0.88971287
INFO - ==> Top1: 86.210    Top5: 98.684    Loss: 0.397
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.88953269
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [33][   20/   40]   Loss 0.333697   Top1 89.648438   Top5 99.550781   BatchTime 0.123267
INFO - Validation [33][   40/   40]   Loss 0.319785   Top1 89.600000   Top5 99.720000   BatchTime 0.092539
INFO - ==> Top1: 89.600    Top5: 99.720    Loss: 0.320
INFO - ==> Sparsity : 0.291
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1895)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0984)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0217)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0787)
features.3.conv.0 tensor(0.0205)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0369)
features.4.conv.0 tensor(0.0202)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0850)
features.5.conv.0 tensor(0.0361)
features.5.conv.3 tensor(0.0793)
features.5.conv.6 tensor(0.0986)
features.6.conv.0 tensor(0.0153)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0535)
features.7.conv.0 tensor(0.0317)
features.7.conv.3 tensor(0.1131)
features.7.conv.6 tensor(0.1045)
features.8.conv.0 tensor(0.0642)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.1145)
features.9.conv.0 tensor(0.0554)
features.9.conv.3 tensor(0.1652)
features.9.conv.6 tensor(0.0969)
features.10.conv.0 tensor(0.0323)
features.10.conv.3 tensor(0.1088)
features.10.conv.6 tensor(0.0671)
features.11.conv.0 tensor(0.0918)
features.11.conv.3 tensor(0.1530)
features.11.conv.6 tensor(0.1782)
features.12.conv.0 tensor(0.1032)
features.12.conv.3 tensor(0.1285)
features.12.conv.6 tensor(0.1911)
features.13.conv.0 tensor(0.0508)
features.13.conv.3 tensor(0.1788)
features.13.conv.6 tensor(0.0876)
features.14.conv.0 tensor(0.7242)
features.14.conv.3 tensor(0.1031)
features.14.conv.6 tensor(0.5539)
features.15.conv.0 tensor(0.8317)
features.15.conv.3 tensor(0.0833)
features.15.conv.6 tensor(0.7122)
features.16.conv.0 tensor(0.0630)
features.16.conv.3 tensor(0.1170)
features.16.conv.6 tensor(0.2732)
conv.0 tensor(0.1036)
tensor(636885.) 2188896.0
0.88816392
0.88590127
0.88417816
0.88325399
0.88282484
0.88234419
0.88176078
0.88174587
0.88227105
0.88271588
0.88324344
0.88390040
0.88392079
0.88357693
0.88365781
0.88341731
0.88313639
0.88253069
0.88215333
INFO - Training [34][   20/  196]   Loss 0.390714   Top1 86.562500   Top5 97.988281   BatchTime 0.406774   LR 0.001218
0.88198203
0.88247275
0.88219506
0.88226110
0.88203543
0.88147938
0.88253880
0.88290203
0.88336271
0.88380373
0.88377267
0.88353896
0.88316762
0.88308907
0.88282114
0.88254887
0.88243765
0.88237596
0.88246733
0.88243610
0.88238245
0.88259298
INFO - Training [34][   40/  196]   Loss 0.394000   Top1 86.201172   Top5 98.242188   BatchTime 0.385376   LR 0.001216
0.88237810
0.88209355
0.88212389
0.88182247
0.88142335
0.88114601
0.88109535
0.88155055
0.88224757
0.88298935
0.88369322
0.88472342
0.88657916
0.88895774
0.89115578
0.89264208
0.89463609
0.89666432
0.89860559
INFO - Training [34][   60/  196]   Loss 0.390526   Top1 86.269531   Top5 98.378906   BatchTime 0.393327   LR 0.001215
0.89965588
0.90037209
0.90059632
0.90088946
0.90062732
0.90047467
0.90039402
0.90010726
0.89972991
0.89946020
0.89908487
0.89813018
0.89698523
0.89537495
0.89488858
0.89447320
0.89454156
0.89455563
0.89380699
0.89345908
0.89259529
0.89203882
INFO - Training [34][   80/  196]   Loss 0.390811   Top1 86.318359   Top5 98.525391   BatchTime 0.390166   LR 0.001213
0.89065397
0.88817132
0.88892311
0.88728130
0.88641798
0.88632345
0.88481575
0.88431740
0.88368046
0.88292378
0.88229519
0.88206547
0.88211811
0.88241732
0.88298690
0.88334805
INFO - Training [34][  100/  196]   Loss 0.383641   Top1 86.558594   Top5 98.589844   BatchTime 0.387237   LR 0.001211
0.88442385
0.88653845
0.88815361
0.88949543
0.88979477
0.88982832
0.88948095
0.88921732
0.88911510
0.88889402
0.88921463
0.88881874
0.88738948
0.88576084
0.88495439
0.88389754
0.88278323
0.88199002
0.88139713
0.88086480
0.88094985
INFO - Training [34][  120/  196]   Loss 0.376850   Top1 86.793620   Top5 98.704427   BatchTime 0.385568   LR 0.001209
0.88062119
0.88041914
0.88029689
0.88018471
0.87971616
0.87984794
0.87991780
0.87989193
0.87973809
0.87940842
0.87941849
0.87950379
0.87955683
0.87955654
0.87963086
0.88144690
0.88699877
0.89986819
0.90753937
0.90758073
0.90772331
0.90777862
INFO - Training [34][  140/  196]   Loss 0.375706   Top1 86.883371   Top5 98.738839   BatchTime 0.383164   LR 0.001208
0.90786225
0.90790087
0.90796077
0.90792543
0.90771508
0.90774149
0.90772396
0.90758944
0.90745634
0.90737534
0.90734988
0.90738928
0.90752834
0.90763760
0.90767568
0.90762746
0.90761852
0.90757596
0.90742308
0.90751255
0.90756607
INFO - Training [34][  160/  196]   Loss 0.381995   Top1 86.701660   Top5 98.715820   BatchTime 0.382061   LR 0.001206
0.90746135
0.90754443
0.90764457
0.90783721
0.90761632
0.90782660
0.90804899
0.90786290
0.90793145
0.90821099
0.90807569
0.90817612
0.90817571
0.90811819
0.90814692
0.90821230
INFO - Training [34][  180/  196]   Loss 0.381852   Top1 86.710069   Top5 98.667535   BatchTime 0.381553   LR 0.001204
0.90799564
0.90799063
0.90789038
0.90782917
0.90774220
0.90769881
0.90781486
0.90788317
0.90814000
0.90803391
0.90801769
0.90816361
0.90822512
0.90828168
0.90827399
0.90830809
0.90831107
INFO - ==> Top1: 86.698    Top5: 98.656    Loss: 0.382
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.90841037
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.363096   Top1 88.750000   Top5 99.453125   BatchTime 0.169272
INFO - Validation [34][   40/   40]   Loss 0.355020   Top1 88.690000   Top5 99.560000   BatchTime 0.124230
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.1953)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0473)
features.2.conv.0 tensor(0.0252)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0764)
features.3.conv.0 tensor(0.0197)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0341)
features.4.conv.0 tensor(0.0181)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0887)
features.5.conv.0 tensor(0.0303)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0942)
features.6.conv.0 tensor(0.0187)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0505)
features.7.conv.0 tensor(0.0328)
features.7.conv.3 tensor(0.1155)
features.7.conv.6 tensor(0.1036)
features.8.conv.0 tensor(0.0645)
features.8.conv.3 tensor(0.1221)
features.8.conv.6 tensor(0.1112)
features.9.conv.0 tensor(0.0538)
features.9.conv.3 tensor(0.1672)
features.9.conv.6 tensor(0.0930)
features.10.conv.0 tensor(0.0321)
features.10.conv.3 tensor(0.1062)
features.10.conv.6 tensor(0.0665)
features.11.conv.0 tensor(0.0977)
features.11.conv.3 tensor(0.1543)
features.11.conv.6 tensor(0.1763)
features.12.conv.0 tensor(0.0954)
features.12.conv.3 tensor(0.1269)
features.12.conv.6 tensor(0.1859)
features.13.conv.0 tensor(0.0508)
features.13.conv.3 tensor(0.1809)
features.13.conv.6 tensor(0.0837)
features.14.conv.0 tensor(0.7123)
features.14.conv.3 tensor(0.1016)
features.14.conv.6 tensor(0.5553)
features.15.conv.0 tensor(0.8476)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.3984)
features.16.conv.0 tensor(0.0645)
features.16.conv.3 tensor(0.1245)
features.16.conv.6 tensor(0.2624)
conv.0 tensor(0.1220)
tensor(592857.) 2188896.0
INFO - ==> Top1: 88.690    Top5: 99.560    Loss: 0.355
INFO - ==> Sparsity : 0.271
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
0.90842855
0.90842456
0.90826702
0.90841985
0.90848374
0.90850216
0.90854740
0.90832496
0.90822899
0.90834612
0.90836763
0.90814358
0.90828651
0.90833318
0.90636843
0.90821695
0.90806645
0.90805393
0.90805793
0.90804070
INFO - Training [35][   20/  196]   Loss 0.385291   Top1 86.777344   Top5 98.417969   BatchTime 0.424419   LR 0.001201
0.90803146
0.90794027
0.90797704
0.90801615
0.90809023
0.90800500
0.90799958
0.90808243
0.90832585
0.90807790
0.90795326
0.90791458
0.90784007
0.90785104
0.90774143
0.90755457
0.90737700
0.90754980
0.90774208
0.90781635
0.90796113
INFO - Training [35][   40/  196]   Loss 0.394820   Top1 86.230469   Top5 98.398438   BatchTime 0.402067   LR 0.001199
0.90793711
0.90786016
0.90785754
0.90778673
0.90768838
0.90774238
0.90779930
0.90787047
0.90791661
0.90781659
0.90775216
0.90753484
0.90758514
0.90727609
0.90728718
0.90734988
0.90728384
0.90718424
0.90711957
0.90712190
INFO - Training [35][   60/  196]   Loss 0.395083   Top1 86.276042   Top5 98.457031   BatchTime 0.407607   LR 0.001197
0.90710258
0.90712893
0.90711534
0.90710419
0.90708530
0.90719765
0.90733421
0.90735894
0.90721840
0.90711218
0.90707552
0.90734357
0.90720779
0.90701431
0.90706205
0.90704441
INFO - Training [35][   80/  196]   Loss 0.391563   Top1 86.538086   Top5 98.554688   BatchTime 0.394644   LR 0.001195
0.90683281
0.90691668
0.90645975
0.90653157
0.90662915
0.90681159
0.90610212
0.90526378
0.90501291
0.90692800
0.90688378
0.90694463
0.90699691
0.90700203
0.90698785
0.90684241
0.90678144
0.90664810
0.90649319
0.90641111
0.90629059
0.90620583
INFO - Training [35][  100/  196]   Loss 0.383093   Top1 86.847656   Top5 98.632812   BatchTime 0.390640   LR 0.001192
0.90626860
0.90609378
0.90568024
0.90491152
0.90518337
0.90524036
0.90513682
0.90562648
0.90589863
0.90603101
0.90622956
0.90604949
0.90585274
0.90585148
0.90583622
0.90585852
0.90587431
0.90594327
0.90499699
0.90617591
0.90622956
0.90629917
INFO - Training [35][  120/  196]   Loss 0.378060   Top1 86.988932   Top5 98.694661   BatchTime 0.386065   LR 0.001190
0.90611500
0.90605873
0.90615118
0.90626907
0.90615696
0.90614152
0.90615076
0.90622950
0.90599972
0.90579349
0.90564555
0.90548187
0.90527582
0.90505713
0.90484083
0.90435749
INFO - Training [35][  140/  196]   Loss 0.376868   Top1 87.047991   Top5 98.758371   BatchTime 0.383457   LR 0.001188
0.90396321
0.90342385
0.90275210
0.90235263
0.90141779
0.90016085
0.89912224
0.89777023
0.89619005
0.89487869
0.89339763
0.89173788
0.89127111
0.89137101
0.89115840
0.88993466
0.88928860
0.88892519
0.88892692
0.88832551
0.88735843
0.88704908
INFO - Training [35][  160/  196]   Loss 0.380277   Top1 86.914062   Top5 98.757324   BatchTime 0.382295   LR 0.001186
0.88652223
0.88828969
0.88773316
0.88704962
0.88619971
0.88502526
0.88411373
0.88296777
0.88205546
0.88138986
0.88094771
0.88072348
0.88053185
0.88031143
0.87996519
0.87965697
0.87958711
0.87976682
0.87968534
0.87957424
0.87939143
INFO - Training [35][  180/  196]   Loss 0.381185   Top1 86.931424   Top5 98.691406   BatchTime 0.380910   LR 0.001184
0.87920350
0.87916434
0.87940699
0.87942934
0.87904817
0.87836480
0.87766623
0.87752956
0.87726796
0.87744713
0.87853593
0.87836105
0.87827271
INFO - ==> Top1: 86.964    Top5: 98.676    Loss: 0.380
0.87827110
0.87840056
0.87829888
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.328358   Top1 89.257812   Top5 99.531250   BatchTime 0.145478
INFO - Validation [35][   40/   40]   Loss 0.324260   Top1 89.320000   Top5 99.630000   BatchTime 0.099444
INFO - ==> Top1: 89.320    Top5: 99.630    Loss: 0.324
INFO - ==> Sparsity : 0.308
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0995)
features.1.conv.6 tensor(0.0443)
features.2.conv.0 tensor(0.0205)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0764)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0334)
features.4.conv.0 tensor(0.0225)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.0882)
features.5.conv.0 tensor(0.0311)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.0924)
features.6.conv.0 tensor(0.0190)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0500)
features.7.conv.0 tensor(0.0354)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.1017)
features.8.conv.0 tensor(0.0571)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.1076)
features.9.conv.0 tensor(0.0575)
features.9.conv.3 tensor(0.1690)
features.9.conv.6 tensor(0.1026)
features.10.conv.0 tensor(0.0328)
features.10.conv.3 tensor(0.1079)
features.10.conv.6 tensor(0.0648)
features.11.conv.0 tensor(0.0886)
features.11.conv.3 tensor(0.1578)
features.11.conv.6 tensor(0.1802)
features.12.conv.0 tensor(0.1041)
features.12.conv.3 tensor(0.1260)
features.12.conv.6 tensor(0.1802)
features.13.conv.0 tensor(0.0523)
features.13.conv.3 tensor(0.1804)
features.13.conv.6 tensor(0.0860)
features.14.conv.0 tensor(0.7130)
features.14.conv.3 tensor(0.1023)
features.14.conv.6 tensor(0.5771)
features.15.conv.0 tensor(0.8583)
features.15.conv.3 tensor(0.0900)
features.15.conv.6 tensor(0.9281)
features.16.conv.0 tensor(0.0655)
features.16.conv.3 tensor(0.1265)
features.16.conv.6 tensor(0.2557)
conv.0 tensor(0.1152)
tensor(674877.) 2188896.0
0.87812567
0.87822092
0.87850446
0.87965673
0.88156569
0.88392603
0.88643378
0.88967228
0.89411175
0.89869666
0.90370643
0.90815949
0.90815139
0.90812504
0.90840757
0.90820843
0.90826589
0.90833515
0.90837848
0.90828913
0.90832669
INFO - Training [36][   20/  196]   Loss 0.397306   Top1 85.742188   Top5 98.339844   BatchTime 0.455159   LR 0.001180
0.90821755
0.90832114
0.90852612
0.90844077
0.90875435
0.90884674
0.90897131
0.90914458
0.90920806
0.90937603
0.90933955
0.90929389
0.90916002
0.90917647
0.90899044
0.90885121
0.90864080
0.90865487
0.90850753
0.90833914
INFO - Training [36][   40/  196]   Loss 0.404043   Top1 85.898438   Top5 98.417969   BatchTime 0.422389   LR 0.001177
0.90805167
0.90804660
0.90823215
0.90825379
0.90827906
0.90824121
0.90831238
0.90818542
0.90807694
0.90775567
0.90777761
0.90794510
0.90774858
0.90766883
0.90738779
0.90728682
0.90720022
0.90717846
0.90723068
0.90730590
0.90737629
INFO - Training [36][   60/  196]   Loss 0.397824   Top1 85.957031   Top5 98.606771   BatchTime 0.407140   LR 0.001175
0.90738767
0.90748394
0.90736163
0.90726256
0.90709722
0.90710473
0.90713322
0.90735394
0.90716916
0.90727186
0.90729380
0.90682745
0.90711689
0.90729105
0.90712672
0.90684992
0.90690875
INFO - Training [36][   80/  196]   Loss 0.397894   Top1 85.898438   Top5 98.681641   BatchTime 0.397802   LR 0.001173
0.90688705
0.90693182
0.90689534
0.90687710
0.90687066
0.90687299
0.90695608
0.90680182
0.90667409
0.90653074
0.90638363
0.90632969
0.90646297
0.90629089
0.90632886
0.90613323
0.90514046
0.90489244
0.90474826
0.90445954
0.90406495
INFO - Training [36][  100/  196]   Loss 0.386453   Top1 86.394531   Top5 98.730469   BatchTime 0.393501   LR 0.001170
0.90475589
0.90552586
0.90682065
0.90698642
0.90698063
0.90682673
0.90686029
0.90664381
0.90635949
0.90623856
0.90622902
0.90619528
0.90603769
0.90605187
0.90609759
0.90579742
0.90527660
0.90418208
0.90519714
0.90506411
0.90489483
INFO - Training [36][  120/  196]   Loss 0.380343   Top1 86.588542   Top5 98.792318   BatchTime 0.391802   LR 0.001168
0.90475231
0.90462053
0.90444940
0.90461057
0.90414852
0.90369684
0.90376055
0.90387738
0.90410966
0.90431160
0.90455532
0.90477139
0.90488458
0.90480936
0.90466243
0.90449816
INFO - Training [36][  140/  196]   Loss 0.378163   Top1 86.682478   Top5 98.864397   BatchTime 0.388354   LR 0.001165
0.90438867
0.90419567
0.90405285
0.90408754
0.90424991
0.90422595
0.90401334
0.90411729
0.90439826
0.90453416
0.90457302
0.90444452
0.90449136
0.90457547
0.90474278
0.90487123
0.90473288
0.90372431
0.90332836
0.90325195
0.90414578
0.90404785
INFO - Training [36][  160/  196]   Loss 0.381428   Top1 86.582031   Top5 98.823242   BatchTime 0.385359   LR 0.001163
0.90402633
0.90385824
0.90384835
0.90369171
0.90328586
0.90317041
0.90299666
0.90196776
0.90135086
0.90319079
0.90306526
0.90291393
0.90276474
0.90267175
0.90260023
0.90275824
0.90265298
0.90263665
0.90229422
0.90222585
INFO - Training [36][  180/  196]   Loss 0.380305   Top1 86.627604   Top5 98.789062   BatchTime 0.376178   LR 0.001160
0.90179592
0.90136921
0.90099490
0.90051430
0.90019929
0.89999586
0.89983696
0.89949799
0.89928675
0.89886904
0.89859116
0.89835703
0.89798522
0.89708173
0.89619720
INFO - ==> Top1: 86.682    Top5: 98.800    Loss: 0.378
0.89553887
0.89424413
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 0.690736   Top1 78.632812   Top5 98.593750   BatchTime 0.126154
INFO - Validation [36][   40/   40]   Loss 0.684880   Top1 78.790000   Top5 98.700000   BatchTime 0.089924
INFO - ==> Top1: 78.790    Top5: 98.700    Loss: 0.685
INFO - ==> Sparsity : 0.290
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.1042)
features.1.conv.6 tensor(0.0451)
features.2.conv.0 tensor(0.0260)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0747)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0332)
features.4.conv.0 tensor(0.0197)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.0854)
features.5.conv.0 tensor(0.0291)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.0954)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0524)
features.7.conv.0 tensor(0.0325)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1028)
features.8.conv.0 tensor(0.0459)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.1121)
features.9.conv.0 tensor(0.0562)
features.9.conv.3 tensor(0.1696)
features.9.conv.6 tensor(0.0953)
features.10.conv.0 tensor(0.0357)
features.10.conv.3 tensor(0.1079)
features.10.conv.6 tensor(0.0655)
features.11.conv.0 tensor(0.0927)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.1783)
features.12.conv.0 tensor(0.1071)
features.12.conv.3 tensor(0.1264)
features.12.conv.6 tensor(0.1750)
features.13.conv.0 tensor(0.0509)
features.13.conv.3 tensor(0.1811)
features.13.conv.6 tensor(0.0828)
features.14.conv.0 tensor(0.7829)
features.14.conv.3 tensor(0.1028)
features.14.conv.6 tensor(0.5701)
features.15.conv.0 tensor(0.8568)
features.15.conv.3 tensor(0.0935)
features.15.conv.6 tensor(0.6192)
features.16.conv.0 tensor(0.0663)
features.16.conv.3 tensor(0.1257)
features.16.conv.6 tensor(0.2581)
conv.0 tensor(0.1118)
tensor(635709.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
0.89278656
0.89117485
0.88972580
0.88779390
0.88584936
0.88473994
0.88402313
0.88376093
0.88324368
0.88311779
0.88266128
0.88223785
0.88210464
0.88204002
0.88193655
0.88221103
0.88226074
0.88216954
0.88203532
0.88181990
0.88145792
INFO - Training [37][   20/  196]   Loss 0.373928   Top1 86.660156   Top5 98.203125   BatchTime 0.431506   LR 0.001155
0.88100284
0.88028097
0.88018864
0.88064206
0.88131160
0.88200629
0.88256902
0.88317543
0.88342506
0.88406193
0.88432080
0.88518661
0.88580185
0.88613278
0.88663274
0.88681436
0.88726968
INFO - Training [37][   40/  196]   Loss 0.386980   Top1 86.328125   Top5 98.466797   BatchTime 0.398718   LR 0.001153
0.88884693
0.89092433
0.89249063
0.89418268
0.89504719
0.89618909
0.89658356
0.89696908
0.89707559
0.89732438
0.89744604
0.89703941
0.89681172
0.89634490
0.89593673
0.89538407
0.89455068
0.89413667
0.89324439
0.89223093
0.89097482
0.89016759
0.88935620
0.88805920
0.88653082
INFO - Training [37][   60/  196]   Loss 0.382098   Top1 86.608073   Top5 98.541667   BatchTime 0.399898   LR 0.001150
0.88535559
0.88450766
0.88471931
0.88426679
0.88354838
0.88260877
0.88190442
0.88156015
0.88094717
0.88079631
0.88064009
0.88080788
0.88061291
0.88092285
0.88158405
0.88209462
INFO - Training [37][   80/  196]   Loss 0.382769   Top1 86.679688   Top5 98.715820   BatchTime 0.394014   LR 0.001147
0.88329339
0.88351315
0.88447535
0.88539577
0.88618183
0.88733941
0.88846338
0.88967961
0.89082354
0.89172536
0.89283991
0.89378160
0.89454049
0.89524055
0.89484936
0.89462799
0.89441365
0.89427346
0.89374018
0.89286506
0.89230102
INFO - Training [37][  100/  196]   Loss 0.377446   Top1 86.855469   Top5 98.750000   BatchTime 0.388294   LR 0.001144
0.89157254
0.89118171
0.89134121
0.89122480
0.89081270
0.89053589
0.88968736
0.88888824
0.88826603
0.88782948
0.88716471
0.88663656
0.88592070
0.88579309
0.88560861
0.88518041
0.88459945
0.88397169
0.88352323
0.88329083
0.88294685
0.88266748
INFO - Training [37][  120/  196]   Loss 0.373448   Top1 86.917318   Top5 98.837891   BatchTime 0.384926   LR 0.001142
0.88229388
0.88198781
0.88166511
0.88115686
0.88054478
0.88020933
0.87998939
0.88041884
0.88095790
0.88105917
0.88114101
0.88093162
0.88046986
0.88042301
0.87993801
0.87963492
INFO - Training [37][  140/  196]   Loss 0.372505   Top1 86.944754   Top5 98.883929   BatchTime 0.382794   LR 0.001139
0.87869591
0.87847483
0.87888086
0.87956041
0.87995231
0.88048792
0.88118887
0.88202554
0.88288814
0.88350749
0.88459563
0.88561809
0.88666302
0.88748372
0.88844985
0.88878870
0.88837320
0.88805336
0.88766813
0.88722962
0.88656133
0.88618785
0.88555676
INFO - Training [37][  160/  196]   Loss 0.374759   Top1 86.928711   Top5 98.845215   BatchTime 0.379262   LR 0.001136
0.88496715
0.88438439
0.88411003
0.88315320
0.88254416
0.88202399
0.88180602
0.88190424
0.88172203
0.88170040
0.88188809
0.88193524
0.88188869
0.88206124
0.88195330
0.88166773
0.88151580
0.88132679
0.88122898
0.88106430
0.88087106
INFO - Training [37][  180/  196]   Loss 0.375870   Top1 86.907552   Top5 98.773872   BatchTime 0.369785   LR 0.001133
0.88064981
0.88041759
0.88057566
0.88092113
0.88102096
0.88123018
0.88141125
0.88213325
0.88308024
0.88369411
0.88473576
0.88576066
0.88674164
0.88738883
********************pre-trained*****************
INFO - ==> Top1: 86.918    Top5: 98.768    Loss: 0.375
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.348761   Top1 88.886719   Top5 99.589844   BatchTime 0.137375
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0464)
features.2.conv.0 tensor(0.0229)
features.2.conv.3 tensor(0.0664)
features.2.conv.6 tensor(0.0747)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0352)
features.4.conv.0 tensor(0.0220)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0817)
features.5.conv.0 tensor(0.0348)
features.5.conv.3 tensor(0.0822)
features.5.conv.6 tensor(0.0965)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0267)
features.7.conv.3 tensor(0.1065)
features.7.conv.6 tensor(0.1010)
features.8.conv.0 tensor(0.0536)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.1086)
features.9.conv.0 tensor(0.0552)
features.9.conv.3 tensor(0.1670)
features.9.conv.6 tensor(0.0924)
features.10.conv.0 tensor(0.0325)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0661)
features.11.conv.0 tensor(0.0955)
features.11.conv.3 tensor(0.1568)
features.11.conv.6 tensor(0.1750)
features.12.conv.0 tensor(0.1111)
features.12.conv.3 tensor(0.1267)
features.12.conv.6 tensor(0.1811)
features.13.conv.0 tensor(0.0510)
features.13.conv.3 tensor(0.1753)
features.13.conv.6 tensor(0.0822)
features.14.conv.0 tensor(0.7062)
features.14.conv.3 tensor(0.1053)
features.14.conv.6 tensor(0.5848)
features.15.conv.0 tensor(0.8590)
features.15.conv.3 tensor(0.0913)
features.15.conv.6 tensor(0.6878)
features.16.conv.0 tensor(0.0659)
features.16.conv.3 tensor(0.1221)
features.16.conv.6 tensor(0.2516)
conv.0 tensor(0.1950)
tensor(669256.) 2188896.0
INFO - Validation [37][   40/   40]   Loss 0.336401   Top1 89.040000   Top5 99.660000   BatchTime 0.097307
INFO - ==> Top1: 89.040    Top5: 99.660    Loss: 0.336
INFO - ==> Sparsity : 0.306
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
0.88771969
0.88844013
0.88881373
0.88933617
0.88997722
0.89063346
0.89053547
0.89058608
0.89021081
0.88996786
0.88901323
0.88798261
0.88692427
0.88610101
0.88535702
0.88495350
0.88496006
0.88512087
0.88405025
0.88271391
INFO - Training [38][   20/  196]   Loss 0.390799   Top1 86.347656   Top5 98.320312   BatchTime 0.433324   LR 0.001128
0.88190645
0.88186759
0.88168281
0.88098347
0.88032216
0.87970668
0.87931263
0.88034034
0.88167971
0.88298267
0.88434303
0.88666087
0.88950020
0.89367515
0.89793265
0.90023255
0.90276259
0.90451133
0.90475744
0.90527958
0.90516078
INFO - Training [38][   40/  196]   Loss 0.395379   Top1 86.318359   Top5 98.388672   BatchTime 0.409630   LR 0.001125
0.90604091
0.90623701
0.90647292
0.90659094
0.90682292
0.90678692
0.90672594
0.90681332
0.90696245
0.90693295
0.90680790
0.90680808
0.90677971
0.90686321
0.90677941
0.90670854
0.90686077
0.90672189
0.90661877
INFO - Training [38][   60/  196]   Loss 0.383668   Top1 86.595052   Top5 98.561198   BatchTime 0.410133   LR 0.001122
0.90663993
0.90664536
0.90654576
0.90649086
0.90648597
0.90635908
0.90650278
0.90630478
0.90630555
0.90614045
0.90625054
0.90626210
0.90598780
0.90601110
0.90597838
0.90576249
0.90579170
0.90570301
0.90536129
0.90550709
0.90540469
0.90557897
INFO - Training [38][   80/  196]   Loss 0.380314   Top1 86.801758   Top5 98.627930   BatchTime 0.400117   LR 0.001119
0.90537375
0.90527201
0.90462863
0.90356594
0.90562123
0.90548801
0.90521026
0.90494055
0.90485245
0.90480638
0.90474957
0.90465623
0.90458757
0.90471160
0.90474457
0.90475762
INFO - Training [38][  100/  196]   Loss 0.376625   Top1 86.980469   Top5 98.683594   BatchTime 0.393341   LR 0.001116
0.90470684
0.90453762
0.90457892
0.90461320
0.90463090
0.90456158
0.90473121
0.90472806
0.90467513
0.90486765
0.90496308
0.90494692
0.90486693
0.90488482
0.90503824
0.90500516
0.90474600
0.90456039
0.90462738
0.90465367
0.90476656
0.90501219
INFO - Training [38][  120/  196]   Loss 0.370082   Top1 87.259115   Top5 98.746745   BatchTime 0.387756   LR 0.001112
0.90496320
0.90490627
0.90465772
0.90464747
0.90457815
0.90446347
0.90433908
0.90407753
0.90350252
0.90335411
0.90356815
0.90472525
0.90470445
0.90487224
0.90474755
0.90495181
0.90488940
0.90516865
0.90508062
0.90523058
0.90527320
0.90540570
INFO - Training [38][  140/  196]   Loss 0.368699   Top1 87.248884   Top5 98.803013   BatchTime 0.384085   LR 0.001109
0.90530884
0.90521860
0.90503466
0.90477842
0.90473759
0.90432113
0.90393859
0.90362102
0.90361321
0.90367365
0.90363407
0.90352619
0.90333587
0.90309906
0.90268213
0.90233380
0.90186083
0.90166497
0.90140396
INFO - Training [38][  160/  196]   Loss 0.371290   Top1 87.172852   Top5 98.818359   BatchTime 0.377139   LR 0.001106
0.90102440
0.90028244
0.90005469
0.89986116
0.89957952
0.89903927
0.89871532
0.89816624
0.89791292
0.89765865
0.89699930
0.89649075
0.89553499
0.89363313
0.89207894
0.89017093
0.88852179
0.88643491
0.88583565
0.88542819
INFO - Training [38][  180/  196]   Loss 0.371978   Top1 87.118056   Top5 98.793403   BatchTime 0.369148   LR 0.001103
0.88431638
0.88309109
0.88260621
0.88159156
0.88145381
0.88089740
0.88052136
0.87937313
0.87809497
0.87648761
0.87440968
0.87224644
0.86875492
0.86421520
0.85984004
INFO - ==> Top1: 87.196    Top5: 98.782    Loss: 0.369
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [38][   20/   40]   Loss 0.366080   Top1 88.261719   Top5 99.472656   BatchTime 0.120862
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0425)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0744)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0332)
features.4.conv.0 tensor(0.0161)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0811)
features.5.conv.0 tensor(0.0348)
features.5.conv.3 tensor(0.0804)
features.5.conv.6 tensor(0.0960)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0531)
features.7.conv.0 tensor(0.0338)
features.7.conv.3 tensor(0.1123)
features.7.conv.6 tensor(0.1029)
features.8.conv.0 tensor(0.0483)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.1088)
features.9.conv.0 tensor(0.0562)
features.9.conv.3 tensor(0.1693)
features.9.conv.6 tensor(0.0912)
features.10.conv.0 tensor(0.0373)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.0667)
features.11.conv.0 tensor(0.1017)
features.11.conv.3 tensor(0.1568)
features.11.conv.6 tensor(0.5478)
features.12.conv.0 tensor(0.1069)
features.12.conv.3 tensor(0.1264)
features.12.conv.6
INFO - Validation [38][   40/   40]   Loss 0.364134   Top1 88.160000   Top5 99.540000   BatchTime 0.090459
INFO - ==> Top1: 88.160    Top5: 99.540    Loss: 0.364
INFO - ==> Sparsity : 0.316
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
features.12.conv.6 tensor(0.1817)
features.13.conv.0 tensor(0.0549)
features.13.conv.3 tensor(0.1777)
features.13.conv.6 tensor(0.0838)
features.14.conv.0 tensor(0.7262)
features.14.conv.3 tensor(0.1061)
features.14.conv.6 tensor(0.6033)
features.15.conv.0 tensor(0.8825)
features.15.conv.3 tensor(0.0918)
features.15.conv.6 tensor(0.8056)
features.16.conv.0 tensor(0.0663)
features.16.conv.3 tensor(0.1241)
features.16.conv.6 tensor(0.2617)
conv.0 tensor(0.1212)
tensor(691165.) 2188896.0
0.86638057
0.88473648
0.89561021
0.89736164
0.89842999
0.89919245
0.89991724
0.90061086
0.90110487
0.90153337
0.90180701
0.90291041
0.90296829
0.90224230
0.90278029
0.90265590
0.90132713
0.90273869
0.90262389
INFO - Training [39][   20/  196]   Loss 0.377316   Top1 86.640625   Top5 98.457031   BatchTime 0.438919   LR 0.001097
0.90241188
0.90221024
0.90209669
0.90203142
0.90199834
0.90205872
0.90178823
0.90191036
0.90184742
0.90172523
0.90209460
0.90187550
0.90193164
0.90205103
0.90194356
0.90196693
0.90194434
0.90175456
0.90168440
0.90153462
INFO - Training [39][   40/  196]   Loss 0.374212   Top1 86.953125   Top5 98.564453   BatchTime 0.417084   LR 0.001094
0.90103602
0.90062064
0.90033454
0.90018678
0.90047747
0.90082538
0.90059090
0.90024936
0.89993858
0.89993244
0.89990604
0.89997298
0.89956862
0.89955544
0.89955497
0.89948618
0.89927685
0.89922458
0.89918387
0.89883053
0.89832968
INFO - Training [39][   60/  196]   Loss 0.379316   Top1 86.816406   Top5 98.593750   BatchTime 0.407681   LR 0.001090
0.89819413
0.89807183
0.89828074
0.89864832
0.89762294
0.89721745
0.89630699
0.89538705
0.89519715
0.89538467
0.89550233
0.89592487
0.89583677
0.89520496
0.89409792
0.89331001
0.89221245
0.89118451
0.89069456
0.89021051
0.89022237
INFO - Training [39][   80/  196]   Loss 0.380287   Top1 86.870117   Top5 98.691406   BatchTime 0.399029   LR 0.001087
0.89019138
0.89014578
0.88958889
0.88905048
0.88826805
0.88731033
0.88663876
0.88587809
0.88504076
0.88411063
0.88316905
0.88297355
0.88287479
0.88277513
0.88298762
0.88291961
0.88279355
0.88262290
0.88266981
0.88233304
INFO - Training [39][  100/  196]   Loss 0.375913   Top1 87.000000   Top5 98.757812   BatchTime 0.395335   LR 0.001084
0.88178384
0.88139361
0.88144594
0.88136828
0.88098782
0.88068330
0.88091236
0.88133985
0.88107252
0.88103890
0.88128906
0.88112080
0.88115841
0.88149446
0.88230121
0.88358289
INFO - Training [39][  120/  196]   Loss 0.371519   Top1 87.190755   Top5 98.808594   BatchTime 0.393584   LR 0.001080
0.88441938
0.88613486
0.88809079
0.89001733
0.89246368
0.89458799
0.89599288
0.89787102
0.89945620
0.90018207
0.90060294
0.90076500
0.90141207
0.90190214
0.90442735
0.90512902
0.90574849
0.90601289
0.90635055
0.90718901
0.90699714
INFO - Training [39][  140/  196]   Loss 0.369202   Top1 87.184710   Top5 98.847656   BatchTime 0.388869   LR 0.001077
0.90691972
0.90649468
0.90583795
0.90633392
0.90657902
0.90626156
0.90614319
0.90629089
0.90623051
0.90591675
0.90624386
0.90584630
0.90574986
0.90566725
0.90557694
0.90521270
0.90489805
0.90482509
0.90495288
0.90484661
INFO - Training [39][  160/  196]   Loss 0.372955   Top1 87.001953   Top5 98.818359   BatchTime 0.377604   LR 0.001073
0.90473831
0.90465313
0.90457737
0.90402102
0.90386766
0.90372568
0.90329391
0.90280133
0.90261173
0.90250307
0.90212590
0.90183514
0.90133196
0.90065694
0.89965373
0.89943582
0.89900529
0.89815086
0.89733082
0.89611292
0.89409369
0.89241242
0.89097172
INFO - Training [39][  180/  196]   Loss 0.373383   Top1 86.955295   Top5 98.776042   BatchTime 0.374868   LR 0.001070
0.88885325
0.88674664
0.88575512
0.88513142
0.88494426
0.88388175
0.88343841
0.88390857
0.88415676
0.88480335
0.88559788
0.88639981
0.88776046
0.88829100
0.88875365
********************pre-trained*****************
INFO - ==> Top1: 86.998    Top5: 98.760    Loss: 0.371
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [39][   20/   40]   Loss 0.330430   Top1 89.316406   Top5 99.550781   BatchTime 0.125945
INFO - Validation [39][   40/   40]   Loss 0.321867   Top1 89.190000   Top5 99.660000   BatchTime 0.088526
INFO - ==> Top1: 89.190    Top5: 99.660    Loss: 0.322
INFO - ==> Sparsity : 0.292
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0425)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0715)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0352)
features.4.conv.0 tensor(0.0212)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0846)
features.5.conv.0 tensor(0.0363)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.0955)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0539)
features.7.conv.0 tensor(0.0382)
features.7.conv.3 tensor(0.1117)
features.7.conv.6 tensor(0.0989)
features.8.conv.0 tensor(0.0505)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.1079)
features.9.conv.0 tensor(0.0615)
features.9.conv.3 tensor(0.1655)
features.9.conv.6 tensor(0.0926)
features.10.conv.0 tensor(0.0332)
features.10.conv.3 tensor(0.1045)
features.10.conv.6 tensor(0.0660)
features.11.conv.0 tensor(0.1022)
features.11.conv.3 tensor(0.1566)
features.11.conv.6 tensor(0.1814)
features.12.conv.0 tensor(0.1005)
features.12.conv.3 tensor(0.1275)
features.12.conv.6 tensor(0.1746)
features.13.conv.0 tensor(0.0573)
features.13.conv.3 tensor(0.1719)
features.13.conv.6 tensor(0.0838)
features.14.conv.0 tensor(0.7405)
features.14.conv.3 tensor(0.1079)
features.14.conv.6 tensor(0.5599)
features.15.conv.0 tensor(0.8700)
features.15.conv.3 tensor(0.0924)
features.15.conv.6 tensor(0.6593)
features.16.conv.0 tensor(0.0652)
features.16.conv.3 tensor(0.1275)
features.16.conv.6 tensor(0.2565)
conv.0 tensor(0.1213)
tensor(639945.) 2188896.0
0.88909692
0.88903177
0.88882101
0.88823992
0.88784975
0.88727343
0.88659716
0.88582981
0.88508844
0.88451922
0.88405049
0.88298821
0.88204288
0.88205868
0.88250124
0.88301277
0.88346368
0.88415122
0.88446957
INFO - Training [40][   20/  196]   Loss 0.367029   Top1 87.558594   Top5 98.007812   BatchTime 0.465382   LR 0.001064
0.88399470
0.88333374
0.88252753
0.88300180
0.88356137
0.88339096
0.88299149
0.88256913
0.88239968
0.88201880
0.88226557
0.88230807
0.88243306
0.88260740
0.88249367
0.88255411
0.88264453
0.88252407
0.88202149
0.88087547
0.88010925
INFO - Training [40][   40/  196]   Loss 0.375690   Top1 87.070312   Top5 98.369141   BatchTime 0.427476   LR 0.001060
0.87957275
0.87983948
0.87952459
0.87949091
0.87926817
0.87923890
0.87985057
0.88015628
0.88027221
0.87993252
0.87957394
0.87975425
0.87956095
0.87938094
0.87903309
0.87742674
0.87714332
0.87699670
0.87906492
0.87900680
INFO - Training [40][   60/  196]   Loss 0.372996   Top1 87.102865   Top5 98.489583   BatchTime 0.415819   LR 0.001056
0.87894660
0.87910992
0.87893689
0.87885273
0.87880474
0.87862766
0.87867177
0.87866884
0.87860894
0.87858963
0.87871188
0.87864715
0.87902063
0.87901884
0.87913477
0.87900263
INFO - Training [40][   80/  196]   Loss 0.372834   Top1 87.099609   Top5 98.632812   BatchTime 0.404997   LR 0.001053
0.87888968
0.87897056
0.87893313
0.87904531
0.87914246
0.87918782
0.87936491
0.87938994
0.87939763
0.87938035
0.87948263
0.87967795
0.87984234
0.88015062
0.88031679
0.88042891
0.88060379
0.88070923
0.88095433
0.88097751
0.88098562
0.88130975
INFO - Training [40][  100/  196]   Loss 0.370261   Top1 87.164062   Top5 98.707031   BatchTime 0.397781   LR 0.001049
0.88162863
0.88220865
0.88234711
0.88255626
0.88249719
0.88262290
0.88269138
0.88288957
0.88302070
0.88308102
0.88359761
0.88361639
0.88376808
0.88377136
0.88379669
0.88373941
0.88361579
0.88352740
0.88331503
0.88312858
0.88310045
INFO - Training [40][  120/  196]   Loss 0.368724   Top1 87.180990   Top5 98.750000   BatchTime 0.393791   LR 0.001045
0.88312703
0.88328439
0.88327509
0.88331211
0.88316196
0.88336903
0.88391000
0.88368249
0.88337708
0.88335294
0.88328850
0.88303626
0.88288724
0.88264477
0.88253796
0.88243920
0.88239270
0.88239086
0.88228643
INFO - Training [40][  140/  196]   Loss 0.365147   Top1 87.324219   Top5 98.780692   BatchTime 0.384200   LR 0.001042
0.88220704
0.88212639
0.88209933
0.88213146
0.88222849
0.88225383
0.88227719
0.88215327
0.88209492
0.88186884
0.88165474
0.88155472
0.88149309
0.88162667
0.88176209
0.88182813
0.88219929
0.88188881
INFO - Training [40][  160/  196]   Loss 0.368683   Top1 87.214355   Top5 98.781738   BatchTime 0.377377   LR 0.001038
0.88088697
0.88034034
0.88018638
0.87971622
0.87971652
0.88005638
0.88112456
0.88091326
0.88099658
0.88095444
0.88091141
0.88108891
0.88108569
0.88094169
0.88091135
0.88104182
0.88107443
0.88119906
0.88099122
0.88096911
0.88101918
0.88105667
INFO - Training [40][  180/  196]   Loss 0.368523   Top1 87.194010   Top5 98.745660   BatchTime 0.375755   LR 0.001034
0.88121468
0.88123745
0.88129920
0.88130206
0.88135630
0.88104856
0.88095093
0.88071007
0.88053566
0.88045365
0.88012308
0.88000727
0.87982625
0.87975383
0.87978864
0.88002938
0.88018054
INFO - ==> Top1: 87.186    Top5: 98.746    Loss: 0.368
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.88042539
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.329519   Top1 88.867188   Top5 99.648438   BatchTime 0.125030
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0473)
features.2.conv.0 tensor(0.0226)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.0706)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0308)
features.4.conv.0 tensor(0.0197)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.0877)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0964)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0526)
features.7.conv.0 tensor(0.0384)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.0964)
features.8.conv.0 tensor(0.0596)
features.8.conv.3 tensor(0.1175)
features.8.conv.6 tensor(0.1106)
features.9.conv.0 tensor(0.0599)
features.9.conv.3 tensor(0.1658)
features.9.conv.6 tensor(0.0905)
features.10.conv.0 tensor(0.0347)
features.10.conv.3 tensor(0.1047)
features.10.conv.6 tensor(0.0670)
features.11.conv.0 tensor(0.1119)
features.11.conv.3 tensor(0.1564)
features.11.conv.6 tensor(0.1753)
features.12.conv.0 tensor(0.1071)
features.12.conv.3 tensor(0.1310)
features.12.conv.6 tensor(0.1732)
features.13.conv.0 tensor(0.0627)
features.13.conv.3 tensor(0.1719)
features.13.conv.6 tensor(0.0813)
features.14.conv.0 tensor(0.7440)
features.14.conv.3 tensor(0.1067)
features.14.conv.6 tensor(0.5683)
features.15.conv.0 tensor(0.8717)
features.15.conv.3 tensor(0.0907)
features.15.conv.6 tensor(0.8287)
features.16.conv.0 tensor(0.0691)
features.16.conv.3 tensor(0.1289)
features.16.conv.6 tensor(0.2521)
conv.0 tensor(0.1342)
tensor(673297.) 2188896.0
INFO - Validation [40][   40/   40]   Loss 0.321460   Top1 89.180000   Top5 99.690000   BatchTime 0.088950
INFO - ==> Top1: 89.180    Top5: 99.690    Loss: 0.321
INFO - ==> Sparsity : 0.308
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
0.88040030
0.88055116
0.88087267
0.88076502
0.88078421
0.88053894
0.88076121
0.88088304
0.88101500
0.88080376
0.88081491
0.88068962
0.88075334
0.88051420
0.88045961
0.88025975
0.87996477
0.87985891
INFO - Training [41][   20/  196]   Loss 0.377069   Top1 86.660156   Top5 98.457031   BatchTime 0.447462   LR 0.001027
0.87989372
0.87995219
0.88014704
0.88015497
0.88013560
0.88002574
0.87990242
0.87976772
0.87981182
0.87979543
0.87985015
0.87960893
0.87945938
0.87948316
0.87933308
0.87934035
0.87925678
0.87918091
0.87882751
0.87891155
0.87898237
0.87888682
INFO - Training [41][   40/  196]   Loss 0.371509   Top1 86.875000   Top5 98.535156   BatchTime 0.406123   LR 0.001023
0.87872750
0.87876588
0.87888294
0.87883902
0.87892771
0.87895977
0.87935078
0.87964994
0.87987161
0.87990516
0.87992632
0.88037062
0.88069451
0.88046765
0.88035291
0.88036013
0.88031995
0.88036925
0.88065094
0.88079560
0.88079852
INFO - Training [41][   60/  196]   Loss 0.364331   Top1 87.226562   Top5 98.626302   BatchTime 0.395772   LR 0.001020
0.88084894
0.88095433
0.88108683
0.88115275
0.88136458
0.88151711
0.88139236
0.88142741
0.88167059
0.88180649
0.88150710
0.88129508
0.88135022
0.88131934
0.88123703
0.88142157
INFO - Training [41][   80/  196]   Loss 0.357141   Top1 87.539062   Top5 98.740234   BatchTime 0.389123   LR 0.001016
0.88159698
0.88128424
0.88125849
0.88142753
0.88159889
0.88159347
0.88127381
0.88096863
0.88069922
0.88011819
0.88042986
0.88029408
0.88024461
0.88015932
0.87984395
0.87932134
0.87928581
0.87915087
0.87892640
0.87889493
0.87879384
0.87877423
INFO - Training [41][  100/  196]   Loss 0.352533   Top1 87.675781   Top5 98.730469   BatchTime 0.384683   LR 0.001012
0.87866437
0.87860888
0.87859964
0.87860298
0.87838978
0.87818050
0.87817591
0.87837386
0.87853456
0.87877119
0.87866360
0.87862808
0.87925375
0.88004452
0.88000298
0.87984800
0.87954581
0.87936348
0.87933618
0.87920427
0.87896693
0.87827694
INFO - Training [41][  120/  196]   Loss 0.349725   Top1 87.724609   Top5 98.789062   BatchTime 0.381919   LR 0.001008
0.87807447
0.87761927
0.87679023
0.87576276
0.87496346
0.87557942
0.87667626
0.87768078
0.87819058
0.87844700
0.87849551
0.87843913
0.87855721
0.87829357
0.87844276
0.87853932
0.87864107
0.87844771
0.87831336
0.87828177
INFO - Training [41][  140/  196]   Loss 0.349292   Top1 87.756696   Top5 98.842076   BatchTime 0.369770   LR 0.001004
0.87830240
0.87847239
0.87817413
0.87821549
0.87802392
0.87784094
0.87768155
0.87752163
0.87727857
0.87770021
0.87825632
0.87809426
0.87822205
0.87828386
0.87866455
0.87893981
0.87881637
INFO - Training [41][  160/  196]   Loss 0.352539   Top1 87.644043   Top5 98.823242   BatchTime 0.368334   LR 0.001000
0.87866455
0.87883806
0.87864470
0.87866646
0.87916744
0.87899792
0.87898755
0.87878263
0.87847787
0.87852454
0.87864494
0.87854195
0.87818962
0.87745130
0.87685901
0.87685776
0.87754285
0.87777907
0.87794501
0.87792784
0.87778556
0.87670285
INFO - Training [41][  180/  196]   Loss 0.353503   Top1 87.610677   Top5 98.773872   BatchTime 0.368187   LR 0.000996
0.87613624
0.87569630
0.87569487
0.87760532
0.87724280
0.87701392
0.87690407
0.87664282
0.87666476
0.87674582
0.87684941
0.87714070
0.87733400
0.87733579
0.87753230
0.87734812
INFO - ==> Top1: 87.684    Top5: 98.770    Loss: 0.352
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [41][   20/   40]   Loss 0.326083   Top1 89.765625   Top5 99.550781   BatchTime 0.135462
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1758)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.1030)
features.1.conv.6 tensor(0.0486)
features.2.conv.0 tensor(0.0231)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0720)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0345)
features.4.conv.0 tensor(0.0184)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.0866)
features.5.conv.0 tensor(0.0397)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0942)
features.6.conv.0 tensor(0.0173)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0534)
features.7.conv.0 tensor(0.0345)
features.7.conv.3 tensor(0.1065)
features.7.conv.6 tensor(0.0972)
features.8.conv.0 tensor(0.0681)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.1102)
features.9.conv.0 tensor(0.0588)
features.9.conv.3 tensor(0.1658)
features.9.conv.6 tensor(0.0912)
features.10.conv.0 tensor(0.0330)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.0672)
INFO - Validation [41][   40/   40]   Loss 0.314493   Top1 89.870000   Top5 99.650000   BatchTime 0.098862
INFO - ==> Top1: 89.870    Top5: 99.650    Loss: 0.314
INFO - ==> Sparsity : 0.314
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
features.11.conv.0 tensor(0.1072)
features.11.conv.3 tensor(0.1584)
features.11.conv.6 tensor(0.1752)
features.12.conv.0 tensor(0.1111)
features.12.conv.3 tensor(0.1318)
features.12.conv.6 tensor(0.1722)
features.13.conv.0 tensor(0.0606)
features.13.conv.3 tensor(0.1732)
features.13.conv.6 tensor(0.0807)
features.14.conv.0 tensor(0.7723)
features.14.conv.3 tensor(0.1090)
features.14.conv.6 tensor(0.5849)
features.15.conv.0 tensor(0.8794)
features.15.conv.3 tensor(0.0909)
features.15.conv.6 tensor(0.8319)
features.16.conv.0 tensor(0.0655)
features.16.conv.3 tensor(0.1255)
features.16.conv.6 tensor(0.2663)
conv.0 tensor(0.1386)
tensor(687270.) 2188896.0
0.87701046
0.87687325
0.87680811
0.87657607
0.87631935
0.87623876
0.87664002
0.87661511
0.87603927
0.87649828
0.87693226
0.87711769
0.87711793
0.87713009
0.87708789
0.87706870
0.87675405
0.87658340
0.87715888
0.87707883
0.87715739
INFO - Training [42][   20/  196]   Loss 0.372776   Top1 87.402344   Top5 97.988281   BatchTime 0.466169   LR 0.000988
0.87721622
0.87719399
0.87714171
0.87679935
0.87681639
0.87717944
0.87725431
0.87709159
0.87699956
0.87674004
0.87659723
0.87650990
0.87638646
0.87611616
0.87607086
0.87659627
0.87667882
INFO - Training [42][   40/  196]   Loss 0.375396   Top1 87.158203   Top5 98.339844   BatchTime 0.416357   LR 0.000984
0.87664551
0.87664908
0.87631071
0.87605363
0.87581229
0.87552762
0.87536812
0.87519819
0.87516928
0.87538266
0.87516230
0.87503541
0.87486279
0.87474102
0.87459427
0.87426466
0.87426269
0.87413841
0.87390316
0.87356114
0.87329745
INFO - Training [42][   60/  196]   Loss 0.368708   Top1 87.317708   Top5 98.496094   BatchTime 0.402770   LR 0.000980
0.87291366
0.87264651
0.87230951
0.87161762
0.87103873
0.87056690
0.87037587
0.87004864
0.86954927
0.86958343
0.86967814
0.86914951
0.86883128
0.86834711
0.86774254
0.86790544
0.86798233
0.86714935
0.86613327
0.86553550
0.86540478
0.86536926
0.86589152
INFO - Training [42][   80/  196]   Loss 0.364356   Top1 87.451172   Top5 98.676758   BatchTime 0.391302   LR 0.000976
0.86590636
0.86613613
0.86584109
0.86544365
0.86461258
0.86370027
0.86275917
0.86204231
0.86107683
0.85990560
0.85869235
0.85675657
0.85541523
0.85413855
0.85319757
INFO - Training [42][  100/  196]   Loss 0.360604   Top1 87.554688   Top5 98.703125   BatchTime 0.389897   LR 0.000972
0.85330993
0.85340619
0.85385567
0.85426646
0.85491735
0.85512108
0.85549653
0.85638285
0.85702920
0.85731882
0.85589892
0.85762286
0.85800850
0.85953355
0.86100179
0.86227405
0.86300212
0.86307865
0.86332726
0.86349040
0.86453617
0.86584079
0.86766869
0.86851078
0.86960924
INFO - Training [42][  120/  196]   Loss 0.353191   Top1 87.766927   Top5 98.805339   BatchTime 0.377812   LR 0.000968
0.87040627
0.87118596
0.87165964
0.87187797
0.87213778
0.87258440
0.87321037
0.87366444
0.87387305
0.87346798
0.87290162
0.87208968
0.87135863
0.87094772
0.87075758
0.87013179
0.86983979
0.86992449
0.86953986
0.86947542
INFO - Training [42][  140/  196]   Loss 0.352277   Top1 87.795759   Top5 98.867188   BatchTime 0.366765   LR 0.000964
0.86980790
0.87028074
0.87212813
0.87230837
0.87192357
0.87171119
0.87154859
0.87190884
0.87178677
0.87169909
0.87154388
0.87138897
0.87139410
0.87128180
0.87088335
0.87048936
0.86973262
INFO - Training [42][  160/  196]   Loss 0.352075   Top1 87.783203   Top5 98.872070   BatchTime 0.363951   LR 0.000959
0.86912310
0.86869478
0.86824000
0.86775625
0.86707550
0.86606234
0.86551988
0.86498857
0.86475289
0.86369818
0.86337841
0.86284083
0.86194700
0.86140126
0.86021352
0.85967273
0.85883749
0.85886520
0.85868049
0.85875732
0.85840774
0.85854769
INFO - Training [42][  180/  196]   Loss 0.352068   Top1 87.760417   Top5 98.843316   BatchTime 0.364616   LR 0.000955
0.85842896
0.85802513
0.85810763
0.85846949
0.85859054
0.85861725
0.85822546
0.85803920
0.85790366
0.85797393
0.85813957
0.85860306
0.85900199
0.85939842
0.85938948
********************pre-trained*****************
INFO - ==> Top1: 87.746    Top5: 98.860    Loss: 0.351
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [42][   20/   40]   Loss 0.482005   Top1 84.863281   Top5 99.218750   BatchTime 0.122672
INFO - Validation [42][   40/   40]   Loss 0.478968   Top1 84.730000   Top5 99.270000   BatchTime 0.092128
INFO - ==> Top1: 84.730    Top5: 99.270    Loss: 0.479
INFO - ==> Sparsity : 0.324
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.230   Top5: 99.680]
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1621)
features.1.conv.0 tensor(0.0111)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0430)
features.2.conv.0 tensor(0.0211)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0683)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0297)
features.4.conv.0 tensor(0.0181)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0825)
features.5.conv.0 tensor(0.0477)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0923)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0377)
features.7.conv.3 tensor(0.1082)
features.7.conv.6 tensor(0.0950)
features.8.conv.0 tensor(0.0646)
features.8.conv.3 tensor(0.1166)
features.8.conv.6 tensor(0.1088)
features.9.conv.0 tensor(0.0568)
features.9.conv.3 tensor(0.1658)
features.9.conv.6 tensor(0.0894)
features.10.conv.0 tensor(0.0320)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.0668)
features.11.conv.0 tensor(0.1109)
features.11.conv.3 tensor(0.1590)
features.11.conv.6 tensor(0.1753)
features.12.conv.0 tensor(0.1075)
features.12.conv.3 tensor(0.1296)
features.12.conv.6 tensor(0.1729)
features.13.conv.0 tensor(0.0576)
features.13.conv.3 tensor(0.1790)
features.13.conv.6 tensor(0.0797)
features.14.conv.0 tensor(0.7342)
features.14.conv.3 tensor(0.1088)
features.14.conv.6 tensor(0.7867)
features.15.conv.0 tensor(0.8830)
features.15.conv.3 tensor(0.0905)
features.15.conv.6 tensor(0.8302)
features.16.conv.0 tensor(0.0667)
features.16.conv.3 tensor(0.1204)
features.16.conv.6 tensor(0.2476)
conv.0 tensor(0.1422)
tensor(708114.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
0.85890836
0.85889983
0.85894966
0.85900211
0.85929728
0.85938960
0.85945129
0.85945183
0.85945684
0.86014420
0.86066753
0.86142373
0.86262184
0.86321616
0.86451215
0.86556125
0.86612296
0.86666584
INFO - Training [43][   20/  196]   Loss 0.386105   Top1 86.289062   Top5 98.398438   BatchTime 0.467295   LR 0.000947
0.86706871
0.86782247
0.86837643
0.86874193
0.86927873
0.86954856
0.86993998
0.87048340
0.87114900
0.87180781
0.87181294
0.87193221
0.87179220
0.87156308
0.87153870
0.87125069
0.87119633
0.87106103
0.87083638
0.87091243
0.87080956
INFO - Training [43][   40/  196]   Loss 0.373754   Top1 86.513672   Top5 98.564453   BatchTime 0.423569   LR 0.000943
0.87079924
0.87063462
0.87046570
0.87023354
0.86982292
0.86946070
0.86914879
0.86896372
0.86879188
0.86843020
0.86798716
0.86758226
0.86712509
0.86708760
0.86698711
0.86675906
0.86592752
0.86597437
0.86751449
0.86753827
0.86713517
INFO - Training [43][   60/  196]   Loss 0.367416   Top1 86.861979   Top5 98.697917   BatchTime 0.408793   LR 0.000939
0.86703950
0.86671680
0.86618870
0.86585170
0.86524922
0.86461687
0.86383206
0.86357164
0.86370933
0.86363083
0.86345696
0.86308330
0.86304116
0.86289632
0.86196303
0.86111462
0.86023074
0.85945767
0.85869175
0.85810369
0.85788065
0.85799211
INFO - Training [43][   80/  196]   Loss 0.362364   Top1 87.231445   Top5 98.750000   BatchTime 0.400125   LR 0.000934
0.85784453
0.85743302
0.85739797
0.85739350
0.85754955
0.85764539
0.85757291
0.85744721
0.85710752
0.85701436
0.85697174
0.85734779
0.85785472
0.85824686
0.85906219
INFO - Training [43][  100/  196]   Loss 0.353818   Top1 87.539062   Top5 98.828125   BatchTime 0.395065   LR 0.000930
0.86039811
0.86187136
0.86360425
0.86530781
0.86684048
0.86807656
0.86909384
0.87036252
0.87141734
0.87252998
0.87349397
0.87416041
0.87416697
0.87458885
0.87468374
0.87471873
0.87489271
0.87500548
0.87526351
0.87508559
0.87507933
0.87514406
0.87528354
0.87570876
INFO - Training [43][  120/  196]   Loss 0.349894   Top1 87.679036   Top5 98.909505   BatchTime 0.387155   LR 0.000926
0.87633491
0.87602669
0.87620926
0.87652773
0.87671638
0.87665808
0.87662971
0.87662715
0.87640649
0.87642378
0.87647766
0.87657154
0.87664944
0.87670177
0.87615448
0.87610209
0.87599170
0.87586260
0.87565911
0.87569690
INFO - Training [43][  140/  196]   Loss 0.349389   Top1 87.745536   Top5 98.906250   BatchTime 0.374589   LR 0.000921
0.87560803
0.87554556
0.87545216
0.87524360
0.87539530
0.87533748
0.87533087
0.87520981
0.87498355
0.87477177
0.87436730
0.87427074
0.87402558
0.87374067
0.87357253
0.87351942
INFO - Training [43][  160/  196]   Loss 0.353492   Top1 87.636719   Top5 98.869629   BatchTime 0.373043   LR 0.000917
0.87349892
0.87294513
0.87360829
0.87318558
0.87298876
0.87249339
0.87192219
0.87143999
0.87118274
0.87104887
0.87041640
0.87013000
0.86995572
0.86962318
0.86940002
0.86914402
0.86884969
0.86896563
0.86890119
0.86840010
0.86808270
0.86750829
INFO - Training [43][  180/  196]   Loss 0.353617   Top1 87.610677   Top5 98.838976   BatchTime 0.375097   LR 0.000912
0.86684090
0.86591834
0.86492831
0.86430401
0.86322671
0.86199081
0.86066091
0.85949302
0.85842746
0.85756642
0.85693508
0.85662234
0.85641152
0.85633802
0.85573858
INFO - ==> Top1: 87.652    Top5: 98.846    Loss: 0.352
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.85510558
0.85475349
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [43][   20/   40]   Loss 0.310600   Top1 90.117188   Top5 99.472656   BatchTime 0.128757
INFO - Validation [43][   40/   40]   Loss 0.299237   Top1 90.240000   Top5 99.590000   BatchTime 0.096178
INFO - ==> Top1: 90.240    Top5: 99.590    Loss: 0.299
INFO - ==> Sparsity : 0.335
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.240   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0689)
features.3.conv.0 tensor(0.0197)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0310)
features.4.conv.0 tensor(0.0156)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0846)
features.5.conv.0 tensor(0.0435)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0913)
features.6.conv.0 tensor(0.0259)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0540)
features.7.conv.0 tensor(0.0308)
features.7.conv.3 tensor(0.1047)
features.7.conv.6 tensor(0.0936)
features.8.conv.0 tensor(0.0589)
features.8.conv.3 tensor(0.1186)
features.8.conv.6 tensor(0.1078)
features.9.conv.0 tensor(0.0573)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.0896)
features.10.conv.0 tensor(0.0339)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0680)
features.11.conv.0 tensor(0.1108)
features.11.conv.3 tensor(0.1578)
features.11.conv.6 tensor(0.1716)
features.12.conv.0 tensor(0.1064)
features.12.conv.3 tensor(0.1302)
features.12.conv.6 tensor(0.1744)
features.13.conv.0 tensor(0.0555)
features.13.conv.3 tensor(0.1784)
features.13.conv.6 tensor(0.0822)
features.14.conv.0 tensor(0.7491)
features.14.conv.3 tensor(0.1086)
features.14.conv.6 tensor(0.9270)
features.15.conv.0 tensor(0.8814)
features.15.conv.3 tensor(0.0903)
features.15.conv.6 tensor(0.8389)
features.16.conv.0 tensor(0.0694)
features.16.conv.3 tensor(0.1231)
features.16.conv.6 tensor(0.2412)
conv.0 tensor(0.1476)
tensor(733327.) 2188896.0
0.85389429
0.85338140
0.85384184
0.85413349
0.85407645
0.85393101
0.85410607
0.85384846
0.85435146
0.85450637
0.85476589
0.85485017
0.85483313
0.85479027
0.85473984
0.85488564
0.85500282
0.85512316
0.85516125
0.85560721
INFO - Training [44][   20/  196]   Loss 0.367454   Top1 87.285156   Top5 98.300781   BatchTime 0.469888   LR 0.000904
0.85611868
0.85652840
0.85657275
0.85639405
0.85598093
0.85573816
0.85543776
0.85491711
0.85452843
0.85469836
0.85422057
0.85487580
0.85539907
0.85639095
0.85793203
0.85968441
0.86112326
0.86137718
0.86201280
0.86087525
0.85851860
INFO - Training [44][   40/  196]   Loss 0.360439   Top1 87.343750   Top5 98.398438   BatchTime 0.424377   LR 0.000900
0.85689789
0.85545474
0.85517722
0.85556036
0.85734928
0.85769308
0.85840476
0.85931611
0.86031210
0.86151218
0.86324137
0.86528724
0.86701447
0.86845642
0.86996001
0.87143499
INFO - Training [44][   60/  196]   Loss 0.351409   Top1 87.656250   Top5 98.541667   BatchTime 0.406935   LR 0.000895
0.87278342
0.87367755
0.87460965
0.87532371
0.87575817
0.87613958
0.87654549
0.87683296
0.87718022
0.87749535
0.87788355
0.87840927
0.87983882
0.88096225
0.88104820
0.88099426
0.88104564
0.88102877
0.88107485
0.88120526
0.88114572
0.88088351
INFO - Training [44][   80/  196]   Loss 0.350056   Top1 87.817383   Top5 98.657227   BatchTime 0.395774   LR 0.000891
0.88053101
0.87957555
0.88044131
0.88061476
0.88054454
0.88054121
0.88058430
0.88060516
0.88025713
0.87886006
0.87890136
0.87898290
0.88137871
0.88144559
0.88127548
0.88129842
0.88130152
0.88123727
0.88124251
0.88112801
0.88115555
0.88117737
INFO - Training [44][  100/  196]   Loss 0.347377   Top1 87.933594   Top5 98.750000   BatchTime 0.389620   LR 0.000886
0.88100845
0.88092190
0.88080662
0.88077241
0.88078076
0.88078701
0.88079733
0.88088828
0.88124806
0.88110805
0.88093394
0.88093007
0.88092387
0.88069713
0.88067299
0.88068348
0.88117343
0.88125712
0.88113809
0.88121760
0.88122267
INFO - Training [44][  120/  196]   Loss 0.341361   Top1 88.141276   Top5 98.811849   BatchTime 0.372830   LR 0.000882
0.88127983
0.88118547
0.88114524
0.88107848
0.88090277
0.88105226
0.88126117
0.88142079
0.88144022
0.88100851
0.88112235
0.88108850
0.88091236
0.88052434
0.88035506
0.88040495
INFO - Training [44][  140/  196]   Loss 0.341100   Top1 88.138951   Top5 98.909040   BatchTime 0.372238   LR 0.000877
0.88046747
0.88064539
0.88083845
0.88073552
0.88081598
0.88073426
0.88050342
0.88048297
0.88059175
0.88034761
0.88034725
0.87996101
0.88055533
0.87982213
0.88088232
0.88087767
0.88095373
0.88070345
0.88047820
0.88057381
0.88070536
INFO - Training [44][  160/  196]   Loss 0.344931   Top1 87.971191   Top5 98.911133   BatchTime 0.373152   LR 0.000873
0.88085312
0.88071829
0.88054878
0.88040614
0.88046080
0.88018328
0.87987781
0.87981784
0.87982088
0.87981862
0.87986416
0.87988925
0.88010657
0.88021141
0.88022113
0.88021863
0.88017744
0.88009048
0.88008004
0.88009912
0.88001972
0.88034856
INFO - Training [44][  180/  196]   Loss 0.345812   Top1 87.916667   Top5 98.851997   BatchTime 0.372352   LR 0.000868
0.88073444
0.88072515
0.88068378
0.88059241
0.88045913
0.88048422
0.88044316
0.88043010
0.88049090
0.88061512
0.88081789
0.88075763
0.88070333
0.88063186
0.88045865
********************pre-trained*****************
INFO - ==> Top1: 87.918    Top5: 98.858    Loss: 0.345
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [44][   20/   40]   Loss 0.315935   Top1 90.019531   Top5 99.550781   BatchTime 0.138258
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1602)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0408)
features.2.conv.0 tensor(0.0179)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0694)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0321)
features.4.conv.0 tensor(0.0181)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0830)
features.5.conv.0 tensor(0.0407)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0908)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0521)
features.7.conv.0 tensor(0.0297)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.0921)
features.8.conv.0 tensor(0.0630)
features.8.conv.3 tensor(0.1178)
features.8.conv.6 tensor(0.1064)
features.9.conv.0 tensor(0.0559)
features.9.conv.3 tensor(0.1623)
features.9.conv.6 tensor(0.0896)
features.10.conv.0 tensor(0.0404)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0670)
features.11.conv.0 tensor(0.1127)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.1626)
features.12.conv.0 tensor(0.1110)
features.12.conv.3 tensor(0.1294)
features.12.conv.6 tensor(0.1785)
features.13.conv.0 tensor(0.0636)
features.13.conv.3 tensor(0.1784)
features.13.conv.6 tensor(0.0814)
features.14.conv.0 tensor(0.7462)
features.14.conv.3 tensor(0.1083)
features.14.conv.6 tensor(0.5982)
features.15.conv.0 tensor(0.8963)
features.15.conv.3 tensor(0.0907)
features.15.conv.6 tensor(0.8557)
features.16.conv.0 tensor(0.0788)
features.16.conv.3 tensor(0.1241)
features.16.conv.6 tensor(0.2368)
conv.0 tensor(0.1499)
tensor(688838.) 2188896.0
INFO - Validation [44][   40/   40]   Loss 0.304914   Top1 89.940000   Top5 99.680000   BatchTime 0.098497
INFO - ==> Top1: 89.940    Top5: 99.680    Loss: 0.305
INFO - ==> Sparsity : 0.315
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.240   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
0.88066012
0.88034600
0.88017106
0.88022977
0.88005835
0.87978607
0.87981087
0.87964886
0.87959671
0.87968791
0.87962818
0.87963402
0.87968278
0.87972581
0.87962967
0.87936366
0.87941539
0.87944973
0.87935483
0.87919313
0.87899679
INFO - Training [45][   20/  196]   Loss 0.352060   Top1 87.656250   Top5 98.496094   BatchTime 0.485857   LR 0.000860
0.87922335
0.87950629
0.87977815
0.87985772
0.87975669
0.87980872
0.87967592
0.87932205
0.87921196
0.87921613
0.87888235
0.87873650
0.87858570
0.87845707
0.87831408
0.87822038
0.87799942
0.87791842
0.87785536
0.87785876
0.87785351
INFO - Training [45][   40/  196]   Loss 0.355112   Top1 87.812500   Top5 98.515625   BatchTime 0.434325   LR 0.000855
0.87780857
0.87759995
0.87740910
0.87728649
0.87722045
0.87700325
0.87696177
0.87692750
0.87659287
0.87584645
0.87625682
0.87630242
0.87616253
0.87586248
0.87533015
0.87574756
0.87596774
INFO - Training [45][   60/  196]   Loss 0.347808   Top1 87.923177   Top5 98.632812   BatchTime 0.409518   LR 0.000850
0.87570149
0.87296736
0.87029356
0.86841011
0.86650854
0.86497825
0.86390048
0.86327147
0.86274463
0.86300224
0.86282748
0.86297554
0.86308622
0.86321408
0.86334068
0.86345875
0.86345440
0.86358768
0.86336416
0.86359906
0.86348319
0.86341804
INFO - Training [45][   80/  196]   Loss 0.347478   Top1 87.939453   Top5 98.759766   BatchTime 0.396941   LR 0.000846
0.86246920
0.86218125
0.86221606
0.86217588
0.86232847
0.86235148
0.86261517
0.86265320
0.86261666
0.86224419
0.86228609
0.86202294
0.86170965
0.86152452
0.86150593
0.86172158
0.86210382
0.86195523
0.86158788
0.86149967
INFO - Training [45][  100/  196]   Loss 0.341360   Top1 88.128906   Top5 98.804688   BatchTime 0.378549   LR 0.000841
0.86126351
0.86100745
0.86119896
0.86112690
0.86084622
0.86083865
0.86064935
0.86074674
0.86099821
0.86125207
0.86146522
0.86190718
0.86188442
0.86219472
0.86245084
0.86282033
INFO - Training [45][  120/  196]   Loss 0.338411   Top1 88.229167   Top5 98.883464   BatchTime 0.375648   LR 0.000836
0.86307609
0.86350179
0.86393452
0.86462831
0.86536425
0.86577123
0.86600858
0.86574048
0.86561722
0.86425072
0.86404532
0.86615211
0.86611992
0.86624002
0.86620295
0.86591125
0.86589867
0.86557841
0.86554956
0.86542177
0.86550152
0.86511087
INFO - Training [45][  140/  196]   Loss 0.337914   Top1 88.250558   Top5 98.936942   BatchTime 0.374564   LR 0.000832
0.86477101
0.86455458
0.86444229
0.86437869
0.86425787
0.86401469
0.86428982
0.86467177
0.86450201
0.86452347
0.86399204
0.86385417
0.86387533
0.86342561
0.86338294
0.86334634
0.86369771
0.86363178
0.86355400
0.86337334
0.86315048
0.86261517
0.86203569
INFO - Training [45][  160/  196]   Loss 0.339075   Top1 88.232422   Top5 98.955078   BatchTime 0.371586   LR 0.000827
0.86166596
0.86146021
0.86122435
0.86049396
0.86132294
0.86164570
0.86139071
0.86182570
0.86195815
0.86250645
0.86271900
0.86278248
0.86250889
0.86230552
0.86231309
0.86202687
0.86162198
INFO - Training [45][  180/  196]   Loss 0.339574   Top1 88.240017   Top5 98.912760   BatchTime 0.369002   LR 0.000822
0.86115521
0.86122245
0.86096340
0.86076027
0.86045474
0.85978466
0.85889447
0.85788816
0.85720366
0.85635042
0.85647964
0.85634679
0.85644585
0.85633582
0.85587466
0.85526174
0.85521483
INFO - ==> Top1: 88.320    Top5: 98.926    Loss: 0.337
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.336531   Top1 89.296875   Top5 99.550781   BatchTime 0.121876
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0694)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0297)
features.4.conv.0 tensor(0.0192)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0841)
features.5.conv.0 tensor(0.0409)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0902)
features.6.conv.0 tensor(0.0195)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0495)
features.7.conv.0 tensor(0.0324)
features.7.conv.3 tensor(0.1111)
features.7.conv.6 tensor(0.0913)
features.8.conv.0 tensor(0.0585)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.1046)
features.9.conv.0 tensor(0.0548)
features.9.conv.3 tensor(0.1626)
features.9.conv.6 tensor(0.0902)
features.10.conv.0 tensor(0.0343)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0677)
features.11.conv.0 tensor(0.1103)
features.11.conv.3 tensor(0.1557)
features.11.conv.6 tensor(0.1645)
features.12.conv.0 tensor(0.1080)
features.12.conv.3 tensor(0.1321)
features.12.conv.6 tensor(0.1823)
features.13.conv.0 tensor(0.0568)
features.13.conv.3 tensor(0.1809)
features.13.conv.6 tensor(0.0824)
features.14.conv.0 tensor(0.7813)
features.14.conv.3 tensor(0.1025)
features.14.conv.6 tensor(0.8428)
features.15.conv.0 tensor(0.8825)
features.15.conv.3 tensor(0.0894)
features.15.conv.6 tensor(0.8496)
features.16.conv.0 tensor(0.0793)
features.16.conv.3 tensor(0.1287)
features.16.conv.6 tensor(0.2251)
conv.0 tensor(0.1454)
tensor(722770.) 2188896.0
INFO - Validation [45][   40/   40]   Loss 0.315002   Top1 89.550000   Top5 99.640000   BatchTime 0.090011
INFO - ==> Top1: 89.550    Top5: 99.640    Loss: 0.315
INFO - ==> Sparsity : 0.330
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.240   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
0.85487783
0.85455906
0.85452616
0.85409421
0.85466355
0.85522854
0.85547560
0.85554564
0.85569048
0.85574239
0.85559911
0.85542226
0.85542363
0.85549891
0.85545081
0.85556424
0.85563529
0.85566419
0.85589945
0.85628211
0.85645241
INFO - Training [46][   20/  196]   Loss 0.349147   Top1 87.480469   Top5 98.437500   BatchTime 0.479370   LR 0.000814
0.85642058
0.85640562
0.85632777
0.85627866
0.85630721
0.85602647
0.85603595
0.85643411
0.85714245
0.85757369
0.85808194
0.85951471
0.86084396
0.86212808
0.86250359
0.86364019
0.86424381
INFO - Training [46][   40/  196]   Loss 0.348809   Top1 87.587891   Top5 98.593750   BatchTime 0.426822   LR 0.000809
0.86525220
0.86627305
0.86751616
0.86882526
0.86996186
0.87095946
0.87185675
0.87266821
0.87319338
0.87359977
0.87391275
0.87459624
0.87513864
0.87568337
0.87616742
0.87648076
0.87667680
0.87696022
0.87737054
0.87765986
0.87818009
INFO - Training [46][   60/  196]   Loss 0.342520   Top1 87.910156   Top5 98.710938   BatchTime 0.407140   LR 0.000804
0.87799972
0.87832683
0.87849742
0.87856275
0.87925959
0.88033497
0.88016105
0.88010269
0.87999660
0.87991893
0.88003618
0.87966472
0.87956035
0.87959683
0.87949502
0.87935907
0.87928253
0.87925875
INFO - Training [46][   80/  196]   Loss 0.342078   Top1 87.963867   Top5 98.798828   BatchTime 0.385759   LR 0.000799
0.87923539
0.87926120
0.87906438
0.87867051
0.87845284
0.87814444
0.87777019
0.87789017
0.87787527
0.87791783
0.87808633
0.87802023
0.87817204
0.87873071
0.87899119
0.87891835
0.87900227
0.87910867
0.87910318
0.87918460
INFO - Training [46][  100/  196]   Loss 0.340709   Top1 88.011719   Top5 98.839844   BatchTime 0.372645   LR 0.000794
0.87911439
0.87890518
0.87865692
0.87862724
0.87862432
0.87855703
0.87840736
0.87842929
0.87867665
0.87884212
0.87897527
0.87914985
0.87902695
0.87911779
0.87937975
0.87962711
0.87984872
0.87999475
0.87976891
0.87968737
0.87967020
INFO - Training [46][  120/  196]   Loss 0.338138   Top1 88.037109   Top5 98.912760   BatchTime 0.372425   LR 0.000789
0.87968421
0.87964690
0.87954271
0.87950885
0.87941527
0.87924451
0.87946278
0.87960202
0.87979853
0.87995398
0.88020897
0.88023847
0.88008797
0.87991840
0.87982363
0.87975514
0.87971824
0.87970465
0.87963456
0.87985593
0.87983948
0.88004154
INFO - Training [46][  140/  196]   Loss 0.337978   Top1 88.116629   Top5 98.922991   BatchTime 0.372184   LR 0.000785
0.88033444
0.88035619
0.88033730
0.88028866
0.88029689
0.88026899
0.88018936
0.88009900
0.88020504
0.88015556
0.88026029
0.88053936
0.88054281
0.88062441
0.88071305
0.88055211
0.88074726
INFO - Training [46][  160/  196]   Loss 0.343523   Top1 87.905273   Top5 98.891602   BatchTime 0.370563   LR 0.000780
0.88082451
0.88093287
0.88095927
0.88118923
0.88146490
0.88150036
0.88111836
0.88106221
0.88081127
0.88076073
0.88078785
0.88102418
0.88100827
0.88097703
0.88088185
0.88116455
0.88078052
0.88074112
0.88060558
0.88064408
0.88051230
0.88044494
INFO - Training [46][  180/  196]   Loss 0.343437   Top1 87.892795   Top5 98.836806   BatchTime 0.370481   LR 0.000775
0.88037753
0.88030201
0.88044482
0.88044810
0.88048464
0.88048983
0.88086510
0.88102567
0.88048065
0.88022256
0.88017768
0.88019013
0.88005280
0.87993181
0.87975407
0.87966311
INFO - ==> Top1: 87.970    Top5: 98.838    Loss: 0.341
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87970936
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [46][   20/   40]   Loss 0.311422   Top1 89.804688   Top5 99.667969   BatchTime 0.199448
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0723)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.0284)
features.4.conv.0 tensor(0.0177)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0747)
features.5.conv.0 tensor(0.0412)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.0910)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0530)
features.7.conv.0 tensor(0.0326)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.0892)
features.8.conv.0 tensor(0.0628)
features.8.conv.3 tensor(0.1155)
features.8.conv.6 tensor(0.1033)
features.9.conv.0 tensor(0.0531)
features.9.conv.3 tensor(0.1681)
features.9.conv.6 tensor(0.0879)
features.10.conv.0 tensor(0.0326)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0678)
features.11.conv.0 tensor(0.1156)
features.11.conv.3 tensor(0.1545)
features.11.conv.6 tensor(0.1625)
features.12.conv.0 tensor(0.1109)
features.12.conv.3 tensor(0.1310)
features.12.conv.6 tensor(0.1850)
features.13.conv.0 tensor(0.0553)
features.13.conv.3 tensor(0.1782)
features.13.conv.6 tensor(0.0831)
features.14.conv.0 tensor(0.7754)
features.14.conv.3 tensor(0.1038)
features.14.conv.6 tensor(0.6160)
features.15.conv.0 tensor(0.8808)
features.15.conv.3 tensor(0.0913)
features.15.conv.6 tensor(0.8562)
features.16.conv.0 tensor(0.0724)
features.16.conv.3 tensor(0.1286)
features.16.conv.6 tensor(0.2213)
conv.0 tensor(0.1552)
tensor(689925.) 2188896.0
INFO - Validation [46][   40/   40]   Loss 0.296332   Top1 90.200000   Top5 99.740000   BatchTime 0.125712
INFO - ==> Top1: 90.200    Top5: 99.740    Loss: 0.296
INFO - ==> Sparsity : 0.315
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.240   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
0.87962151
0.87944198
0.87931097
0.87914670
0.87885815
0.87865335
0.87845486
0.87821501
0.87793666
0.87773144
0.87795234
0.87825251
0.87840283
0.87815517
0.87791240
0.87771285
0.87772936
0.87763339
0.87765819
INFO - Training [47][   20/  196]   Loss 0.352909   Top1 87.187500   Top5 98.476562   BatchTime 0.437609   LR 0.000766
0.87759507
0.87740821
0.87746954
0.87755448
0.87748921
0.87749946
0.87762439
0.87750661
0.87721455
0.87695390
0.87678242
0.87649822
0.87650448
0.87628835
0.87605983
0.87603313
0.87587863
0.87558556
0.87544668
0.87535000
INFO - Training [47][   40/  196]   Loss 0.349304   Top1 87.617188   Top5 98.623047   BatchTime 0.412503   LR 0.000761
0.87505376
0.87506461
0.87515730
0.87521416
0.87487125
0.87445563
0.87393707
0.87328285
0.87204784
0.87100941
0.86977220
0.86853135
0.86838645
0.86867648
0.86955971
0.86995155
0.87009782
INFO - Training [47][   60/  196]   Loss 0.350239   Top1 87.701823   Top5 98.717448   BatchTime 0.388274   LR 0.000756
0.86963284
0.86896706
0.86835462
0.86724526
0.86616820
0.86531138
0.86456889
0.86385334
0.86322260
0.86305302
0.86261171
0.86239403
0.86209118
0.86202198
0.86172444
0.86141121
0.86143863
0.86173016
0.86160856
0.86134946
INFO - Training [47][   80/  196]   Loss 0.344311   Top1 87.954102   Top5 98.828125   BatchTime 0.366743   LR 0.000752
0.86127007
0.86166269
0.86167502
0.86133182
0.86131793
0.86156183
0.86134410
0.86121416
0.86088961
0.86029488
0.85963923
0.85928500
0.85920137
0.85907686
0.85917467
0.85929221
0.85925412
0.85908276
0.85887325
0.85845399
0.85840565
INFO - Training [47][  100/  196]   Loss 0.336070   Top1 88.320312   Top5 98.843750   BatchTime 0.353170   LR 0.000747
0.85793221
0.85873657
0.85876232
0.85870618
0.85826051
0.85802579
0.85789895
0.85770565
0.85759532
0.85701519
0.85633528
0.85629278
0.85612637
0.85609215
0.85569960
0.85581213
0.85571760
0.85561204
0.85557860
0.85544622
0.85535192
0.85558748
INFO - Training [47][  120/  196]   Loss 0.330925   Top1 88.486328   Top5 98.896484   BatchTime 0.355934   LR 0.000742
0.85558492
0.85393196
0.85609651
0.85570353
0.85545969
0.85551399
0.85532624
0.85527825
0.85570151
0.85550976
0.85563636
0.85532999
0.85500824
0.85501558
0.85489446
0.85459071
0.85428584
0.85414499
0.85405761
0.85405958
0.85444945
INFO - Training [47][  140/  196]   Loss 0.328752   Top1 88.585379   Top5 98.956473   BatchTime 0.358147   LR 0.000737
0.85478544
0.85455567
0.85516822
0.85526234
0.85517085
0.85521048
0.85532725
0.85547751
0.85581273
0.85634345
0.85687262
0.85734504
0.85820419
0.85926777
0.85993779
0.86028004
0.86066395
0.86088789
0.86101592
0.86153632
0.86195266
0.86207885
INFO - Training [47][  160/  196]   Loss 0.330113   Top1 88.503418   Top5 98.947754   BatchTime 0.359849   LR 0.000732
0.86217964
0.86238050
0.86274195
0.86310458
0.86363935
0.86386591
0.86430055
0.86498749
0.86575371
0.86672944
0.86769050
0.86862171
0.86949730
0.87033200
0.87067753
0.87081921
INFO - Training [47][  180/  196]   Loss 0.329397   Top1 88.491753   Top5 98.940972   BatchTime 0.360715   LR 0.000727
0.87071472
0.87084436
0.87085223
0.87100846
0.87102759
0.87125105
0.87095040
0.87068623
0.87058270
0.87076569
0.87073672
0.87066597
0.87058169
0.87061816
0.87074494
0.87093240
INFO - ==> Top1: 88.546    Top5: 98.964    Loss: 0.328
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87088925
0.87078202
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [47][   20/   40]   Loss 0.328639   Top1 89.804688   Top5 99.492188   BatchTime 0.118406
INFO - Validation [47][   40/   40]   Loss 0.313807   Top1 90.020000   Top5 99.650000   BatchTime 0.085916
INFO - ==> Top1: 90.020    Top5: 99.650    Loss: 0.314
INFO - ==> Sparsity : 0.315
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.240   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.1621)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0391)
features.2.conv.0 tensor(0.0188)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0694)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0308)
features.4.conv.0 tensor(0.0184)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0767)
features.5.conv.0 tensor(0.0330)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.0866)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0544)
features.7.conv.0 tensor(0.0362)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.0896)
features.8.conv.0 tensor(0.0621)
features.8.conv.3 tensor(0.1192)
features.8.conv.6 tensor(0.1030)
features.9.conv.0 tensor(0.0536)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.0864)
features.10.conv.0 tensor(0.0356)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0683)
features.11.conv.0 tensor(0.1088)
features.11.conv.3 tensor(0.1586)
features.11.conv.6 tensor(0.1610)
features.12.conv.0 tensor(0.1113)
features.12.conv.3 tensor(0.1302)
features.12.conv.6 tensor(0.1838)
features.13.conv.0 tensor(0.0564)
features.13.conv.3 tensor(0.1765)
features.13.conv.6 tensor(0.0843)
features.14.conv.0 tensor(0.7667)
features.14.conv.3 tensor(0.1020)
features.14.conv.6 tensor(0.6626)
features.15.conv.0 tensor(0.8893)
features.15.conv.3 tensor(0.0933)
features.15.conv.6 tensor(0.8642)
features.16.conv.0 tensor(0.0721)
features.16.conv.3 tensor(0.1294)
features.16.conv.6 tensor(0.2075)
conv.0 tensor(0.1450)
tensor(689659.) 2188896.0
0.87082237
0.87096113
0.87109894
0.87101591
0.87111741
0.87120616
0.87129265
0.87136024
0.87161762
0.87184501
0.87182075
0.87133539
0.87068009
0.86987203
0.86927527
0.86889380
0.86866242
INFO - Training [48][   20/  196]   Loss 0.355345   Top1 87.656250   Top5 98.437500   BatchTime 0.430321   LR 0.000718
0.86874616
0.86871874
0.86851019
0.86830193
0.86859852
0.86874199
0.86841440
0.86800802
0.86733598
0.86647111
0.86565048
0.86484689
0.86439532
0.86399800
0.86321563
0.86217856
0.86304945
0.86238903
0.86206019
0.86184943
0.86137986
INFO - Training [48][   40/  196]   Loss 0.353757   Top1 87.656250   Top5 98.681641   BatchTime 0.405275   LR 0.000713
0.86114079
0.86099434
0.86093980
0.86110163
0.86122429
0.86128438
0.86157888
0.86179638
0.86189419
0.86210418
0.86225128
0.86234915
0.86234200
0.86181563
0.86305112
0.86359644
0.86349320
0.86315429
0.86263466
0.86238092
0.86203229
INFO - Training [48][   60/  196]   Loss 0.349571   Top1 87.786458   Top5 98.684896   BatchTime 0.394796   LR 0.000708
0.86177784
0.86173874
0.86165303
0.86161268
0.86184448
0.86214107
0.86228532
0.86214268
0.86203384
0.86161280
0.86138397
0.86104000
0.86063749
0.86045414
0.86018282
0.85969049
0.85982448
0.85979992
0.85944724
INFO - Training [48][   80/  196]   Loss 0.343966   Top1 87.988281   Top5 98.789062   BatchTime 0.375667   LR 0.000703
0.85912675
0.85889310
0.85854155
0.85834122
0.85776448
0.85836691
0.85839409
0.85842246
0.85796481
0.85761946
0.85735613
0.85706943
0.85720253
0.85698217
0.85670906
0.85652882
0.85640550
0.85582399
0.85545969
0.85502088
0.85432792
0.85418266
0.85406190
INFO - Training [48][  100/  196]   Loss 0.338404   Top1 88.214844   Top5 98.828125   BatchTime 0.370785   LR 0.000698
0.85387129
0.85316914
0.85265535
0.85228336
0.85214132
0.85191101
0.85164249
0.85140353
0.85139185
0.85156202
0.85177028
0.85159302
0.85146034
0.85166001
0.85164613
0.85158193
0.85150844
INFO - Training [48][  120/  196]   Loss 0.332552   Top1 88.408203   Top5 98.902995   BatchTime 0.368760   LR 0.000693
0.85139060
0.85128641
0.85079688
0.85071230
0.85074300
0.85064292
0.85071528
0.85101551
0.85093474
0.85082138
0.85103017
0.85083526
0.85066521
0.85063618
0.85062534
0.85017341
0.84993500
0.84916282
0.84909958
0.84995425
0.85033202
INFO - Training [48][  140/  196]   Loss 0.324725   Top1 88.657924   Top5 99.006696   BatchTime 0.369414   LR 0.000688
0.85033315
0.85025209
0.85023552
0.84910274
0.84839094
0.84843951
0.85054964
0.85044301
0.85045904
0.85029060
0.85033381
0.85035247
0.85042471
0.85025460
0.85005867
0.84990555
0.85014236
0.85049438
0.85086083
0.85077494
0.85017043
0.85085756
INFO - Training [48][  160/  196]   Loss 0.328088   Top1 88.518066   Top5 98.969727   BatchTime 0.370259   LR 0.000683
0.85064369
0.85076863
0.85136133
0.85145718
0.85149342
0.85133463
0.85104752
0.85074663
0.85060269
0.85039544
0.85003948
0.85001802
0.85006845
0.85134608
0.85136324
0.85103118
0.85109025
0.85108310
0.85087508
0.85085618
INFO - Training [48][  180/  196]   Loss 0.327456   Top1 88.517795   Top5 98.925781   BatchTime 0.372277   LR 0.000678
0.85072136
0.85058129
0.85042775
0.85054177
0.85053027
0.85065991
0.85057092
0.85043895
0.85043770
0.85074747
0.85050964
0.85048538
0.85044527
0.85033852
0.85020703
********************pre-trained*****************
INFO - ==> Top1: 88.536    Top5: 98.918    Loss: 0.327
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [48][   20/   40]   Loss 0.313849   Top1 89.882812   Top5 99.648438   BatchTime 0.123391
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.1582)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0438)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0651)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0386)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0160)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0783)
features.5.conv.0 tensor(0.0326)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0879)
features.6.conv.0 tensor(0.0282)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0339)
features.7.conv.3 tensor(0.1047)
features.7.conv.6 tensor(0.0877)
features.8.conv.0 tensor(0.0551)
features.8.conv.3 tensor(0.1175)
features.8.conv.6 tensor(0.1032)
features.9.conv.0 tensor(0.0583)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.0853)
features.10.conv.0 tensor(0.0369)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0671)
features.11.conv.0 tensor(0.1072)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.1616)
features.12.conv.0 tensor(0.1145)
features.12.conv.3 tensor(0.1281)
features.12.conv.6 tensor(0.1841)
features.13.conv.0 tensor(0.0511)
features.13.conv.3 tensor(0.1780)
features.13.conv.6 tensor(0.0817)
features.14.conv.0 tensor(0.7737)
features.14.conv.3 tensor(0.1037)
features.14.conv.6 tensor(0.9678)
features.15.conv.0 tensor(0.8910)
features.15.conv.3 tensor(0.0934)
features.15.conv.6 tensor(0.8777)
features.16.conv.0 tensor(0.0706)
features.16.conv.3 tensor(0.1270)
features.16.conv.6 tensor(0.2064)
conv.0 tensor(0.1838)
tensor(754642.) 2188896.0
INFO - Validation [48][   40/   40]   Loss 0.300851   Top1 90.320000   Top5 99.690000   BatchTime 0.089952
INFO - ==> Top1: 90.320    Top5: 99.690    Loss: 0.301
INFO - ==> Sparsity : 0.345
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 90.320   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
0.85003650
0.84986621
0.84984410
0.84993988
0.84968662
0.84942305
0.84936750
0.84930813
0.84925717
0.84915632
0.84894711
0.84886914
0.84906441
0.84937996
0.84947199
0.84982806
0.84965771
0.84946376
0.85007614
INFO - Training [49][   20/  196]   Loss 0.324772   Top1 88.593750   Top5 98.613281   BatchTime 0.429458   LR 0.000669
0.85017926
0.85027939
0.85036105
0.85014814
0.85006744
0.85001117
0.84995741
0.84944588
0.84936708
0.84917569
0.84884018
0.84935617
0.85014147
0.85036111
0.85039419
0.85036236
0.85025549
0.85008389
0.85003716
0.84996784
0.85006106
INFO - Training [49][   40/  196]   Loss 0.338666   Top1 88.271484   Top5 98.671875   BatchTime 0.404206   LR 0.000664
0.84982401
0.84949160
0.84936690
0.84934372
0.84940666
0.84960628
0.85001361
0.84996235
0.84982693
0.84992516
0.84997672
0.84989035
0.84992421
0.84981120
0.84977490
0.84979266
0.84971672
0.84962475
0.84963953
INFO - Training [49][   60/  196]   Loss 0.337918   Top1 88.209635   Top5 98.645833   BatchTime 0.374830   LR 0.000659
0.84970796
0.84975588
0.84982878
0.84985644
0.84961557
0.84943414
0.84937763
0.84948403
0.84941423
0.84941864
0.84947300
0.84943879
0.84944642
0.84949821
0.84928137
0.84917831
0.84913480
0.84884101
0.84866095
0.84870106
0.84868199
INFO - Training [49][   80/  196]   Loss 0.338267   Top1 88.129883   Top5 98.793945   BatchTime 0.352957   LR 0.000654
0.84862643
0.84851700
0.84844548
0.84835964
0.84804124
0.84791428
0.84772545
0.84767151
0.84774452
0.84794217
0.84804904
0.84796739
0.84796828
0.84797347
0.84785998
0.84767926
0.84713930
0.84733570
0.84750491
INFO - Training [49][  100/  196]   Loss 0.331626   Top1 88.386719   Top5 98.875000   BatchTime 0.347711   LR 0.000649
0.84860700
0.84850943
0.84840780
0.84886813
0.84908050
0.84902459
0.84920049
0.84925538
0.84941685
0.84981680
0.84995562
0.85009122
0.84959918
0.84937721
0.84945631
0.84949124
0.84940630
0.84933770
0.84926492
0.84955877
0.84968996
INFO - Training [49][  120/  196]   Loss 0.323847   Top1 88.645833   Top5 98.945312   BatchTime 0.351371   LR 0.000644
0.84977037
0.84993732
0.84989625
0.84977144
0.84969574
0.84957951
0.84951746
0.84949630
0.84927225
0.84907585
0.84924972
0.84938824
0.84931809
0.84898293
0.84930164
0.84947830
0.84928501
0.84907907
0.84928805
0.84913456
0.84907275
0.84895897
INFO - Training [49][  140/  196]   Loss 0.322210   Top1 88.755580   Top5 99.009487   BatchTime 0.353356   LR 0.000639
0.84881848
0.84887552
0.84893787
0.84890223
0.84867543
0.84856021
0.84869695
0.84880102
0.84892642
0.84898239
0.84909123
0.84900492
0.84897137
0.84914321
0.84905040
0.84900194
0.84899676
INFO - Training [49][  160/  196]   Loss 0.326869   Top1 88.591309   Top5 98.972168   BatchTime 0.353532   LR 0.000634
0.84923798
0.84952265
0.84927738
0.84924269
0.84917158
0.84920454
0.84925038
0.84931147
0.84927011
0.84912848
0.84904438
0.84922755
0.84944630
0.84948349
0.84958333
0.84959304
0.84979713
0.84991664
0.84987026
0.84991282
0.84977269
INFO - Training [49][  180/  196]   Loss 0.328834   Top1 88.485243   Top5 98.904080   BatchTime 0.357788   LR 0.000629
0.84993804
0.84957147
0.84945732
0.84927469
0.84941328
0.84945232
0.84956056
0.84947300
0.84955329
0.84973484
0.84962142
0.84954602
0.84949887
0.84949380
0.84955645
INFO - ==> Top1: 88.528    Top5: 98.908    Loss: 0.328
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84973574
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [49][   20/   40]   Loss 0.311574   Top1 89.687500   Top5 99.628906   BatchTime 0.126254
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.1543)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0430)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0660)
features.3.conv.0 tensor(0.0211)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0291)
features.4.conv.0 tensor(0.0164)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0750)
features.5.conv.0 tensor(0.0334)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0889)
features.6.conv.0 tensor(0.0270)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0535)
features.7.conv.0 tensor(0.0345)
features.7.conv.3 tensor(0.1082)
features.7.conv.6 tensor(0.0874)
features.8.conv.0 tensor(0.0566)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.1016)
features.9.conv.0 tensor(0.0604)
features.9.conv.3 tensor(0.1594)
features.9.conv.6 tensor(0.0856)
features.10.conv.0 tensor(0.0359)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0669)
features.11.conv.0 tensor(0.1063)
features.11.conv.3 tensor(0.1566)
features.11.conv.6 tensor(0.1610)
features.12.conv.0 tensor(0.1239)
features.12.conv.3 tensor(0.1302)
features.12.conv.6 tensor(0.1929)
features.13.conv.0 tensor(0.0559)
features.13.conv.3 tensor(0.1780)
features.13.conv.6 tensor(0.0829)
features.14.conv.0 tensor(0.7977)
features.14.conv.3 tensor(0.1027)
features.14.conv.6 tensor(0.9612)
features.15.conv.0 tensor(0.8801)
features.15.conv.3 tensor(0.0917)
features.15.conv.6 tensor(0.8358)
features.16.conv.0 tensor(0.0702)
features.16.conv.3 tensor(0.1278)
features.16.conv.6 tensor(0.2086)
conv.0 tensor(0.1399)
tensor(733189.) 2188896.0
INFO - Validation [49][   40/   40]   Loss 0.304218   Top1 90.020000   Top5 99.700000   BatchTime 0.092511
INFO - ==> Top1: 90.020    Top5: 99.700    Loss: 0.304
INFO - ==> Sparsity : 0.335
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 90.320   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
0.84951532
0.84937888
0.84941435
0.84958172
0.84953779
0.84938377
0.84951347
0.84960121
0.84983039
0.84964699
0.84965670
0.84961915
0.84964722
0.84924078
0.84920502
0.84919637
0.84929341
0.84935397
INFO - Training [50][   20/  196]   Loss 0.333950   Top1 87.929688   Top5 98.554688   BatchTime 0.435202   LR 0.000620
0.84938127
0.84926957
0.84930813
0.84937197
0.84934491
0.84915340
0.84908646
0.84928334
0.84926981
0.84949028
0.84948707
0.84930432
0.84917408
0.84917879
0.84916598
0.84937274
0.84938997
0.84927452
0.84949124
0.84935367
0.84956205
0.84944451
INFO - Training [50][   40/  196]   Loss 0.326458   Top1 88.593750   Top5 98.710938   BatchTime 0.401128   LR 0.000615
0.84932572
0.84908205
0.84880441
0.84861636
0.84853727
0.84841770
0.84831858
0.84844095
0.84837902
0.84831357
0.84808326
0.84819096
0.84754217
0.84710306
0.84683967
0.84681058
INFO - Training [50][   60/  196]   Loss 0.330654   Top1 88.352865   Top5 98.769531   BatchTime 0.387035   LR 0.000610
0.84707475
0.84834725
0.84849066
0.84851974
0.84858924
0.84863836
0.84846729
0.84846330
0.84841520
0.84836119
0.84831244
0.84837931
0.84849495
0.84837687
0.84827471
0.84846205
0.84869373
0.84892321
0.84895974
0.84910440
0.84892410
0.84907347
0.84880155
0.84877408
0.84875888
0.84854078
INFO - Training [50][   80/  196]   Loss 0.325271   Top1 88.608398   Top5 98.901367   BatchTime 0.369658   LR 0.000605
0.84833026
0.84833163
0.84847581
0.84831274
0.84832406
0.84816080
0.84819680
0.84817934
0.84820694
0.84809875
0.84811848
0.84817880
0.84680218
0.84668028
0.84658724
INFO - Training [50][  100/  196]   Loss 0.318301   Top1 88.800781   Top5 98.898438   BatchTime 0.371150   LR 0.000600
0.84550846
0.84446460
0.84522718
0.84731054
0.84701401
0.84670788
0.84646910
0.84653002
0.84633619
0.84758943
0.84762299
0.84769416
0.84800088
0.84794927
0.84800506
0.84796679
0.84791410
0.84773582
0.84767038
0.84765142
0.84774357
0.84778327
0.84800512
INFO - Training [50][  120/  196]   Loss 0.312810   Top1 89.010417   Top5 98.958333   BatchTime 0.368646   LR 0.000595
0.84812719
0.84851480
0.84851187
0.84851408
0.84863865
0.84888279
0.84885585
0.84873384
0.84839302
0.84836936
0.84822828
0.84812069
0.84801537
0.84796327
0.84815162
0.84825188
0.84836656
0.84841907
0.84851897
0.84881055
0.84884286
INFO - Training [50][  140/  196]   Loss 0.311285   Top1 89.098772   Top5 99.006696   BatchTime 0.370847   LR 0.000590
0.84889734
0.84909970
0.84931928
0.84940279
0.84944880
0.84948587
0.84979403
0.84991038
0.85013247
0.85019457
0.85040164
0.85020351
0.84998435
0.84983957
0.84991962
0.84997851
0.84986800
0.84963644
0.84950894
0.84958398
INFO - Training [50][  160/  196]   Loss 0.315258   Top1 88.945312   Top5 99.016113   BatchTime 0.373467   LR 0.000585
0.84940577
0.84936213
0.84946686
0.84945667
0.84942639
0.84947801
0.84956092
0.84956551
0.84971750
0.85013890
0.85034573
0.85029387
0.85010165
0.84985542
0.84974754
0.84974617
0.84971088
INFO - Training [50][  180/  196]   Loss 0.315999   Top1 88.925781   Top5 98.986545   BatchTime 0.372096   LR 0.000580
0.84964424
0.84970587
0.84972328
0.84977835
0.84957141
0.84935904
0.84924626
0.84935820
0.84935480
0.84917223
0.84917307
0.84914619
0.84906685
0.84902912
0.84911281
0.84925383
INFO - ==> Top1: 88.932    Top5: 98.976    Loss: 0.316
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84933335
0.84939414
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [50][   20/   40]   Loss 0.311220   Top1 90.214844   Top5 99.687500   BatchTime 0.122008
INFO - Validation [50][   40/   40]   Loss 0.297568   Top1 90.560000   Top5 99.750000   BatchTime 0.089278
INFO - ==> Top1: 90.560    Top5: 99.750    Loss: 0.298
INFO - ==> Sparsity : 0.337
INFO - Scoreboard best 1 ==> Epoch [50][Top1: 90.560   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0408)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0689)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0151)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0713)
features.5.conv.0 tensor(0.0353)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0902)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0542)
features.7.conv.0 tensor(0.0337)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.0890)
features.8.conv.0 tensor(0.0564)
features.8.conv.3 tensor(0.1189)
features.8.conv.6 tensor(0.1005)
features.9.conv.0 tensor(0.0665)
features.9.conv.3 tensor(0.1603)
features.9.conv.6 tensor(0.0868)
features.10.conv.0 tensor(0.0388)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0670)
features.11.conv.0 tensor(0.1079)
features.11.conv.3 tensor(0.1559)
features.11.conv.6 tensor(0.1596)
features.12.conv.0 tensor(0.1214)
features.12.conv.3 tensor(0.1298)
features.12.conv.6 tensor(0.1894)
features.13.conv.0 tensor(0.0538)
features.13.conv.3 tensor(0.1811)
features.13.conv.6 tensor(0.0821)
features.14.conv.0 tensor(0.8180)
features.14.conv.3 tensor(0.1020)
features.14.conv.6 tensor(0.9591)
features.15.conv.0 tensor(0.8933)
features.15.conv.3 tensor(0.0914)
features.15.conv.6 tensor(0.8477)
features.16.conv.0 tensor(0.0657)
features.16.conv.3 tensor(0.1279)
features.16.conv.6 tensor(0.2071)
conv.0 tensor(0.1370)
tensor(737206.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
0.84942079
0.84934300
0.84933239
0.84949505
0.84948426
0.84922594
0.84919727
0.84906566
0.84871119
0.84884578
0.84877276
0.84894782
0.84896582
0.84908420
0.84921962
0.84923387
0.84961581
0.84970826
0.84991509
0.85008520
INFO - Training [51][   20/  196]   Loss 0.326406   Top1 88.515625   Top5 98.652344   BatchTime 0.439958   LR 0.000571
0.84988803
0.84982479
0.84973687
0.84984404
0.84996396
0.84970653
0.84957415
0.84985042
0.84987915
0.84982222
0.84999925
0.85014451
0.84975743
0.84967804
0.84971482
0.84931624
INFO - Training [51][   40/  196]   Loss 0.328424   Top1 88.457031   Top5 98.710938   BatchTime 0.393662   LR 0.000566
0.84885889
0.85027868
0.85038656
0.85047090
0.85042757
0.85021347
0.85022086
0.85006982
0.85008168
0.85017651
0.85019934
0.85031945
0.85055941
0.85056835
0.85045087
0.85028023
0.85050708
0.85063642
0.85053241
0.85045522
0.85054892
0.85064167
0.85061812
0.85073143
0.85093057
0.85096413
INFO - Training [51][   60/  196]   Loss 0.325013   Top1 88.639323   Top5 98.802083   BatchTime 0.366320   LR 0.000561
0.85072649
0.85062981
0.85036725
0.85053432
0.85059005
0.85046041
0.85035044
0.85030442
0.85037130
0.85037810
0.85031599
0.85039359
0.85043615
0.85042810
0.85048252
0.85049969
0.85046971
INFO - Training [51][   80/  196]   Loss 0.321815   Top1 88.730469   Top5 98.896484   BatchTime 0.365069   LR 0.000556
0.85071361
0.85083967
0.85086000
0.85089499
0.85072255
0.85078645
0.85084152
0.85078454
0.85076189
0.85100210
0.85122371
0.85137916
0.85092646
0.85081732
0.85095799
0.85072351
0.85058707
0.85060459
0.85050577
0.85051298
0.85060078
INFO - Training [51][  100/  196]   Loss 0.317942   Top1 88.824219   Top5 98.925781   BatchTime 0.367168   LR 0.000551
0.85069436
0.85083431
0.85084796
0.85094661
0.85099614
0.85090953
0.85099304
0.85111010
0.85112584
0.85118687
0.85113996
0.85099930
0.85090321
0.85063756
0.85060549
0.85062611
0.85087258
0.85078204
0.85069513
0.85064274
INFO - Training [51][  120/  196]   Loss 0.313185   Top1 88.968099   Top5 98.987630   BatchTime 0.372170   LR 0.000546
0.85038775
0.85036290
0.85008645
0.85009396
0.84993523
0.84967732
0.84974241
0.85000110
0.85024321
0.84996885
0.84983021
0.84964991
0.84961206
0.84893072
0.84852374
0.84870350
0.84890604
0.84893066
0.84899700
0.84900552
0.84898770
0.84902221
INFO - Training [51][  140/  196]   Loss 0.311412   Top1 89.090402   Top5 99.031808   BatchTime 0.372254   LR 0.000541
0.84886903
0.84873426
0.84860659
0.84861869
0.84911042
0.84928447
0.84955674
0.84984910
0.85004497
0.84990650
0.85001093
0.85028392
0.85008168
0.85007215
0.85002953
0.84990329
INFO - Training [51][  160/  196]   Loss 0.315215   Top1 89.001465   Top5 99.023438   BatchTime 0.371678   LR 0.000536
0.84993970
0.85001522
0.85018873
0.85033852
0.84969956
0.84962857
0.84980923
0.84980142
0.85010111
0.85034889
0.85093790
0.85123497
0.85141635
0.85244381
0.85339195
0.85450608
0.85488778
0.85410756
0.85339296
0.85319883
0.85249794
0.85207516
INFO - Training [51][  180/  196]   Loss 0.317936   Top1 88.851997   Top5 98.977865   BatchTime 0.371848   LR 0.000531
0.85184902
0.85147232
0.85123360
0.85116535
0.85206032
0.85240585
0.85229570
0.85230589
0.85184240
0.85173702
0.85172117
0.85193360
0.85181701
0.85160214
0.85156018
0.85148066
INFO - ==> Top1: 88.936    Top5: 98.988    Loss: 0.316
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [51][   20/   40]   Loss 0.329768   Top1 90.156250   Top5 99.589844   BatchTime 0.124689
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.1758)
features.1.conv.0 tensor(0.0098)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0391)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0645)
features.3.conv.0 tensor(0.0223)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0151)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0723)
features.5.conv.0 tensor(0.0350)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0892)
features.6.conv.0 tensor(0.0277)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0544)
features.7.conv.0 tensor(0.0315)
features.7.conv.3 tensor(0.1042)
features.7.conv.6 tensor(0.0868)
features.8.conv.0 tensor(0.0573)
features.8.conv.3 tensor(0.1166)
features.8.conv.6 tensor(0.0983)
features.9.conv.0 tensor(0.0647)
features.9.conv.3 tensor(0.1667)
features.9.conv.6 tensor(0.0859)
features.10.conv.0 tensor(0.0378)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0696)
features.11.conv.0 tensor(0.1080)
features.11.conv.3 tensor(0.1562)
features.11.conv.6 tensor(0.1609)
features.12.conv.0 tensor(0.1272)
features.12.conv.3 tensor(0.1283)
features.12.conv.6 tensor(0.1888)
features.13.conv.0 tensor(0.0544)
features.13.conv.3 tensor(0.1802)
features.13.conv.6 tensor(0.0808)
features.14.conv.0 tensor(0.7675)
features.14.conv.3 tensor(0.0992)
features.14.conv.6 tensor(0.9603)
features.15.conv.0 tensor(0.8934)
features.15.conv.3 tensor(0.0899)
features.15.conv.6 tensor(0.8341)
features.16.conv.0 tensor(0.0695)
features.16.conv.3 tensor(0.1289)
features.16.conv.6 tensor(0.2066)
conv.0 tensor(0.1375)
tensor(728305.) 2188896.0
INFO - Validation [51][   40/   40]   Loss 0.325619   Top1 90.180000   Top5 99.660000   BatchTime 0.089527
INFO - ==> Top1: 90.180    Top5: 99.660    Loss: 0.326
INFO - ==> Sparsity : 0.333
INFO - Scoreboard best 1 ==> Epoch [50][Top1: 90.560   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.420   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
0.85165936
0.85146922
0.85157633
0.85112154
0.85095841
0.85098851
0.85123873
0.85134405
0.85133129
0.85128546
0.85119969
0.85126835
0.85104442
0.85108757
0.85094970
0.85074455
0.85091567
0.85109657
INFO - Training [52][   20/  196]   Loss 0.332933   Top1 87.968750   Top5 98.632812   BatchTime 0.447645   LR 0.000523
0.85099196
0.85088426
0.85090029
0.85098749
0.85104501
0.85127664
0.85101265
0.85050583
0.85025352
0.85011858
0.85023892
0.85007107
0.85004592
0.85016990
0.85015118
0.85010988
0.85004902
0.84918040
0.84925854
0.84917438
0.84885174
0.84864312
0.84935236
0.84981209
INFO - Training [52][   40/  196]   Loss 0.332060   Top1 88.193359   Top5 98.720703   BatchTime 0.387441   LR 0.000518
0.85014462
0.85041726
0.85152841
0.85199738
0.85190415
0.85191590
0.85198796
0.85178715
0.85165167
0.85161197
0.85150033
0.85140836
0.85134441
0.85096169
0.85104585
0.85109824
0.85105056
0.85113436
INFO - Training [52][   60/  196]   Loss 0.323317   Top1 88.561198   Top5 98.789062   BatchTime 0.372998   LR 0.000513
0.85131061
0.85134369
0.85140747
0.85148466
0.85139978
0.85103500
0.85103565
0.85098773
0.85101944
0.85071748
0.85055530
0.85055733
0.85044932
0.85047877
0.85056907
0.85080135
0.85108441
0.85102326
0.85091555
0.85101783
0.85075527
0.85044044
INFO - Training [52][   80/  196]   Loss 0.326685   Top1 88.500977   Top5 98.891602   BatchTime 0.370016   LR 0.000508
0.85030341
0.85041672
0.85049182
0.85055947
0.85062027
0.85066646
0.85058075
0.85082585
0.85059685
0.85073233
0.85080618
0.85043257
0.85042131
0.85012203
0.84994268
INFO - Training [52][  100/  196]   Loss 0.320396   Top1 88.734375   Top5 98.914062   BatchTime 0.371260   LR 0.000503
0.84975672
0.84958404
0.84948868
0.84963107
0.84980464
0.85000300
0.85011268
0.85007781
0.85001755
0.85005403
0.84963161
0.84952700
0.84936219
0.84938025
0.84948909
0.84937245
0.84921300
0.84940571
0.84925103
0.84917182
0.84937781
INFO - Training [52][  120/  196]   Loss 0.313564   Top1 89.010417   Top5 98.974609   BatchTime 0.375642   LR 0.000498
0.84943235
0.84963167
0.84973353
0.84976500
0.84996039
0.85028595
0.85030550
0.85017306
0.85016507
0.85003799
0.84995127
0.84970909
0.84981972
0.85001487
0.84996420
0.84935206
0.84866560
0.84830165
0.84782892
0.84823394
0.84933352
INFO - Training [52][  140/  196]   Loss 0.312537   Top1 89.095982   Top5 99.015067   BatchTime 0.374672   LR 0.000493
0.84966063
0.84940875
0.84926564
0.84878141
0.84835809
0.84780490
0.84782571
0.84878659
0.84892416
0.84881073
0.84871835
0.84850544
0.84859562
0.84843987
0.84854686
0.84869808
0.84858739
0.84857804
0.84857333
0.84849924
0.84873986
0.84886324
INFO - Training [52][  160/  196]   Loss 0.313886   Top1 89.003906   Top5 99.047852   BatchTime 0.373911   LR 0.000488
0.84870976
0.84859324
0.84840977
0.84853667
0.84852093
0.84872299
0.84860647
0.84833091
0.84804398
0.84757239
0.84784585
0.84868395
0.84861118
0.84877139
0.84863406
0.84855354
INFO - Training [52][  180/  196]   Loss 0.316200   Top1 88.936632   Top5 98.993056   BatchTime 0.373610   LR 0.000483
0.84835190
0.84825408
0.84844613
0.84829527
0.84854454
0.84863102
0.84851837
0.84836733
0.84832346
0.84833741
0.84816402
0.84851044
0.84876657
0.84854764
0.84835815
0.84878141
0.84891093
0.84898663
0.84899825
********************pre-trained*****************
INFO - ==> Top1: 89.032    Top5: 98.980    Loss: 0.314
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [52][   20/   40]   Loss 0.301404   Top1 90.683594   Top5 99.687500   BatchTime 0.125403
INFO - Validation [52][   40/   40]   Loss 0.293243   Top1 90.640000   Top5 99.790000   BatchTime 0.088780
INFO - ==> Top1: 90.640    Top5: 99.790    Loss: 0.293
INFO - ==> Sparsity : 0.343
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.640   Top5: 99.790]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 90.560   Top5: 99.750]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 90.450   Top5: 99.730]
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0637)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0295)
features.4.conv.0 tensor(0.0155)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0741)
features.5.conv.0 tensor(0.0358)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0887)
features.6.conv.0 tensor(0.0275)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0535)
features.7.conv.0 tensor(0.0336)
features.7.conv.3 tensor(0.1042)
features.7.conv.6 tensor(0.0864)
features.8.conv.0 tensor(0.0571)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.0971)
features.9.conv.0 tensor(0.0664)
features.9.conv.3 tensor(0.1626)
features.9.conv.6 tensor(0.0865)
features.10.conv.0 tensor(0.0415)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0673)
features.11.conv.0 tensor(0.1060)
features.11.conv.3 tensor(0.1545)
features.11.conv.6 tensor(0.1578)
features.12.conv.0 tensor(0.1221)
features.12.conv.3 tensor(0.1287)
features.12.conv.6 tensor(0.1866)
features.13.conv.0 tensor(0.0533)
features.13.conv.3 tensor(0.1827)
features.13.conv.6 tensor(0.0807)
features.14.conv.0 tensor(0.7919)
features.14.conv.3 tensor(0.0988)
features.14.conv.6 tensor(0.9654)
features.15.conv.0 tensor(0.8980)
features.15.conv.3 tensor(0.0904)
features.15.conv.6 tensor(0.9376)
features.16.conv.0 tensor(0.0692)
features.16.conv.3 tensor(0.1295)
features.16.conv.6 tensor(0.2121)
conv.0 tensor(0.1399)
tensor(751374.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
0.84840387
0.84841615
0.84832627
0.84823453
0.84803301
0.84813273
0.84840822
0.84857541
0.84875447
0.84874409
0.84887111
0.84876770
0.84874105
0.84862864
0.84872943
0.84881687
0.84882009
0.84894878
0.84906101
INFO - Training [53][   20/  196]   Loss 0.323890   Top1 89.101562   Top5 98.652344   BatchTime 0.393824   LR 0.000474
0.84938020
0.84951836
0.84958017
0.84949338
0.84912825
0.84898531
0.84913313
0.84917259
0.84926730
0.84953642
0.84946215
0.84938282
0.84929872
0.84928292
0.84924245
0.84915310
0.84917933
0.84940541
0.84948224
0.84939319
0.84959251
0.84974617
INFO - Training [53][   40/  196]   Loss 0.325114   Top1 88.974609   Top5 98.652344   BatchTime 0.384522   LR 0.000470
0.84978294
0.84950680
0.84947944
0.84950221
0.84939510
0.84942967
0.84970850
0.84967637
0.84954882
0.84932923
0.84923816
0.84903252
0.84888923
0.84885788
0.84898371
0.84898669
INFO - Training [53][   60/  196]   Loss 0.319186   Top1 89.108073   Top5 98.743490   BatchTime 0.378386   LR 0.000465
0.84905875
0.84905088
0.84909433
0.84898138
0.84889400
0.84888363
0.84899205
0.84888518
0.84903318
0.84890443
0.84902120
0.84903330
0.84905529
0.84897166
0.84890944
0.84873974
0.84867913
0.84869695
0.84900445
0.84926379
0.84921956
0.84911221
0.84913152
0.84914553
0.84906870
INFO - Training [53][   80/  196]   Loss 0.317430   Top1 89.077148   Top5 98.852539   BatchTime 0.384579   LR 0.000460
0.84930897
0.84992456
0.84971416
0.84969521
0.84971076
0.84978968
0.84983033
0.84995776
0.85009140
0.85002238
0.84979677
0.84978700
0.84986579
0.84955966
0.84931588
0.84934783
INFO - Training [53][  100/  196]   Loss 0.307161   Top1 89.359375   Top5 98.898438   BatchTime 0.383705   LR 0.000455
0.84829324
0.84813768
0.84806907
0.84802580
0.84801179
0.84802294
0.84810686
0.84828508
0.84783131
0.84751815
0.84731513
0.84724265
0.84715730
0.84688526
0.84664685
0.84661764
0.84676683
0.84674323
0.84682405
0.84686369
0.84683877
0.84667766
INFO - Training [53][  120/  196]   Loss 0.304860   Top1 89.427083   Top5 98.945312   BatchTime 0.380709   LR 0.000450
0.84653240
0.84641576
0.84635550
0.84659219
0.84675747
0.84691691
0.84690082
0.84690219
0.84676689
0.84667319
0.84668481
0.84673131
0.84668815
0.84681672
0.84674352
0.84685719
0.84695041
0.84666175
0.84661990
0.84659290
0.84660083
0.84676337
INFO - Training [53][  140/  196]   Loss 0.302258   Top1 89.556362   Top5 98.978795   BatchTime 0.379094   LR 0.000445
0.84678996
0.84680974
0.84708220
0.84697902
0.84685463
0.84703761
0.84726059
0.84709108
0.84674698
0.84680176
0.84694678
0.84676480
0.84681928
0.84695941
0.84695774
0.84688264
INFO - Training [53][  160/  196]   Loss 0.301964   Top1 89.492188   Top5 98.981934   BatchTime 0.377024   LR 0.000441
0.84678018
0.84722292
0.84736961
0.84711266
0.84720170
0.84848845
0.84842008
0.84828728
0.84819859
0.84815109
0.84825033
0.84847587
0.84845722
0.84848064
0.84847295
0.84860039
0.84876335
0.84896344
0.84904248
0.84952694
0.84945720
0.84914124
INFO - Training [53][  180/  196]   Loss 0.304615   Top1 89.405382   Top5 98.964844   BatchTime 0.376501   LR 0.000436
0.84901977
0.84907281
0.84914559
0.84912032
0.84925991
0.84935516
0.84917611
0.84939152
0.84971958
0.84929127
0.84907037
0.84922177
0.84917784
0.84912938
0.84930021
0.84946251
INFO - ==> Top1: 89.394    Top5: 98.970    Loss: 0.305
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [53][   20/   40]   Loss 0.310916   Top1 90.097656   Top5 99.687500   BatchTime 0.138913
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0443)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0634)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0158)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.0750)
features.5.conv.0 tensor(0.0343)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.0913)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0316)
features.7.conv.3 tensor(0.1027)
features.7.conv.6 tensor(0.0860)
features.8.conv.0 tensor(0.0628)
features.8.conv.3 tensor(0.1160)
features.8.conv.6 tensor(0.0978)
features.9.conv.0 tensor(0.0651)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.0848)
features.10.conv.0 tensor(0.0391)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0681)
features.11.conv.0 tensor(0.1100)
features.11.conv.3 tensor(0.1539)
features.11.conv.6 tensor(0.1578)
features.12.conv.0 tensor(0.1253)
features.12.conv.3 tensor(0.1291)
features.12.conv.6 tensor(0.1909)
features.13.conv.0 tensor(0.0537)
features.13.conv.3 tensor(0.1852)
features.13.conv.6 tensor(0.0806)
features.14.conv.0 tensor(0.7882)
features.14.conv.3 tensor(0.0985)
features.14.conv.6 tensor(0.9629)
features.15.conv.0 tensor(0.8922)
features.15.conv.3 tensor(0.0916)
features.15.conv.6 tensor(0.8770)
features.16.conv.0 tensor(0.0711)
features.16.conv.3 tensor(0.1303)
features.16.conv.6 tensor(0.2130)
conv.0 tensor(0.1390)
tensor(741036.) 2188896.0
INFO - Validation [53][   40/   40]   Loss 0.289665   Top1 90.630000   Top5 99.740000   BatchTime 0.101681
INFO - ==> Top1: 90.630    Top5: 99.740    Loss: 0.290
INFO - ==> Sparsity : 0.339
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.640   Top5: 99.790]
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.630   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.560   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
0.84957749
0.84957975
0.84962791
0.84965879
0.84972650
0.84923571
0.84917170
0.84934968
0.84946811
0.84943491
0.84928322
0.84931856
0.84918821
0.84900361
0.84911972
0.84908521
0.84902334
0.84899521
INFO - Training [54][   20/  196]   Loss 0.305657   Top1 89.042969   Top5 98.613281   BatchTime 0.435474   LR 0.000427
0.84908140
0.84905732
0.84920901
0.84927291
0.84933823
0.84939271
0.84951907
0.84935176
0.84938109
0.84941798
0.84981900
0.84986943
0.85014129
0.84994864
0.84990770
0.84987754
0.84974313
0.84951037
0.84940779
0.84934175
0.84911960
INFO - Training [54][   40/  196]   Loss 0.314835   Top1 88.662109   Top5 98.750000   BatchTime 0.410540   LR 0.000423
0.84906119
0.84907323
0.84915513
0.84924960
0.84950817
0.84936291
0.84919679
0.84907609
0.84882993
0.84859997
0.84880406
0.84918427
0.84913319
0.84914500
0.84888524
0.84941494
0.84938776
0.84920603
0.84894055
INFO - Training [54][   60/  196]   Loss 0.310241   Top1 88.990885   Top5 98.808594   BatchTime 0.411733   LR 0.000418
0.84889871
0.84883898
0.84901577
0.84893519
0.84881490
0.84842682
0.84874868
0.84866750
0.84869123
0.84856665
0.84866089
0.84861636
0.84854209
0.84875542
0.84868962
0.84861225
0.84838408
0.84763402
0.84702605
0.84668595
0.84611785
INFO - Training [54][   80/  196]   Loss 0.309441   Top1 89.082031   Top5 98.935547   BatchTime 0.403344   LR 0.000413
0.84621328
0.84617883
0.84622812
0.84606999
0.84609950
0.84640580
0.84646505
0.84659392
0.84626603
0.84594852
0.84638405
0.84620643
0.84605545
0.84606254
0.84635276
0.84669834
0.84689391
0.84680605
0.84714180
0.84897178
0.84888333
INFO - Training [54][  100/  196]   Loss 0.303255   Top1 89.457031   Top5 98.988281   BatchTime 0.399172   LR 0.000408
0.84883362
0.84904337
0.84907222
0.84908473
0.84903002
0.84888321
0.84884608
0.84914118
0.84946746
0.84924436
0.84934181
0.84951252
0.84939384
0.84904706
0.84901464
0.84939098
0.84922653
0.84917915
0.84929645
0.84888279
0.84850615
INFO - Training [54][  120/  196]   Loss 0.298314   Top1 89.622396   Top5 99.023438   BatchTime 0.395022   LR 0.000404
0.84779096
0.84719682
0.84696031
0.84681386
0.84703469
0.84697098
0.84687573
0.84714812
0.84747183
0.84797806
0.84806484
0.84805489
0.84846711
0.84881049
0.84841704
0.84840560
0.84823239
INFO - Training [54][  140/  196]   Loss 0.296163   Top1 89.690290   Top5 99.065290   BatchTime 0.391957   LR 0.000399
0.84802103
0.84787643
0.84757382
0.84756142
0.84764528
0.84776473
0.84781289
0.84800887
0.84817272
0.84857994
0.84872746
0.84867167
0.84865177
0.84871024
0.84876186
0.84882557
0.84889722
0.84909123
0.84912157
0.84971392
0.84979045
INFO - Training [54][  160/  196]   Loss 0.299771   Top1 89.555664   Top5 99.064941   BatchTime 0.389847   LR 0.000394
0.84971029
0.84971464
0.84971058
0.84941930
0.84924573
0.84909284
0.84898835
0.84915549
0.84930152
0.84911656
0.84915453
0.84936392
0.84926486
0.84926879
0.84955841
0.84967053
0.84971911
0.85022151
0.85010326
0.85007858
0.85013157
0.84999800
INFO - Training [54][  180/  196]   Loss 0.299822   Top1 89.542101   Top5 99.042969   BatchTime 0.386444   LR 0.000390
0.84997863
0.84975934
0.84960240
0.84965068
0.84973329
0.84975100
0.84972346
0.84959954
0.84960079
0.84952790
0.84923393
0.84906423
0.84909523
0.84919095
INFO - ==> Top1: 89.586    Top5: 99.046    Loss: 0.298
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84935188
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [54][   20/   40]   Loss 0.293608   Top1 90.937500   Top5 99.687500   BatchTime 0.151453
INFO - Validation [54][   40/   40]   Loss 0.281860   Top1 91.100000   Top5 99.740000   BatchTime 0.100207
INFO - ==> Top1: 91.100    Top5: 99.740    Loss: 0.282
INFO - ==> Sparsity : 0.339
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [52][Top1: 90.640   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.630   Top5: 99.740]
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.1621)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0408)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0671)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0164)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0728)
features.5.conv.0 tensor(0.0355)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.0894)
features.6.conv.0 tensor(0.0278)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0531)
features.7.conv.0 tensor(0.0334)
features.7.conv.3 tensor(0.1033)
features.7.conv.6 tensor(0.0865)
features.8.conv.0 tensor(0.0664)
features.8.conv.3 tensor(0.1163)
features.8.conv.6 tensor(0.0974)
features.9.conv.0 tensor(0.0680)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.0858)
features.10.conv.0 tensor(0.0355)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0668)
features.11.conv.0 tensor(0.1135)
features.11.conv.3 tensor(0.1555)
features.11.conv.6 tensor(0.1568)
features.12.conv.0 tensor(0.1258)
features.12.conv.3 tensor(0.1329)
features.12.conv.6 tensor(0.1962)
features.13.conv.0 tensor(0.0588)
features.13.conv.3 tensor(0.1800)
features.13.conv.6 tensor(0.0803)
features.14.conv.0 tensor(0.7997)
features.14.conv.3 tensor(0.0972)
features.14.conv.6 tensor(0.9636)
features.15.conv.0 tensor(0.8993)
features.15.conv.3 tensor(0.0921)
features.15.conv.6 tensor(0.8572)
features.16.conv.0 tensor(0.0755)
features.16.conv.3 tensor(0.1293)
features.16.conv.6 tensor(0.2128)
conv.0 tensor(0.1382)
tensor(742050.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
0.84935033
0.84922981
0.84928095
0.84938902
0.84919548
0.84889710
0.84860665
0.84863812
0.84906882
0.84938544
0.84942335
0.84937894
0.84969079
0.84977812
0.84962028
0.84956539
0.84959000
0.84966934
INFO - Training [55][   20/  196]   Loss 0.318323   Top1 88.496094   Top5 98.496094   BatchTime 0.465324   LR 0.000381
0.84976244
0.84968042
0.84969443
0.84965020
0.84992296
0.85000455
0.85011566
0.84982520
0.84953278
0.84950954
0.84947419
0.84920001
0.84921044
0.84905475
0.84897184
0.84897166
0.84924048
0.84908980
0.84946454
0.84948170
0.84932023
INFO - Training [55][   40/  196]   Loss 0.320513   Top1 88.613281   Top5 98.632812   BatchTime 0.424001   LR 0.000377
0.84927446
0.84952331
0.84939635
0.84914470
0.84907317
0.84929407
0.84911615
0.84897316
0.84894723
0.84894150
0.84906292
0.84897345
0.84923214
0.84897816
0.84891725
0.84883982
0.84839451
0.84799552
0.84780556
0.84786975
0.84781837
INFO - Training [55][   60/  196]   Loss 0.307855   Top1 89.160156   Top5 98.736979   BatchTime 0.408375   LR 0.000372
0.84776694
0.84754163
0.84777564
0.84790009
0.84789854
0.84944206
0.84909558
0.84922481
0.84931666
0.84932643
0.84914064
0.84909976
0.84922820
0.84911549
0.84896022
0.84902889
0.84903049
0.84909791
0.84928089
0.84960353
0.84982806
INFO - Training [55][   80/  196]   Loss 0.306266   Top1 89.316406   Top5 98.847656   BatchTime 0.399439   LR 0.000368
0.85001236
0.84957314
0.84949785
0.84932554
0.84919256
0.84931010
0.84946150
0.84957123
0.84952784
0.85001975
0.84975439
0.84972149
0.84972942
0.84951395
0.84924465
0.84913927
0.84894228
INFO - Training [55][  100/  196]   Loss 0.300377   Top1 89.527344   Top5 98.898438   BatchTime 0.392721   LR 0.000363
0.84870797
0.84882498
0.84866720
0.84840453
0.84830880
0.84829700
0.84837586
0.84841740
0.84835923
0.84860027
0.84884083
0.84985977
0.85029525
0.85028070
0.85027319
0.85011762
0.84973460
0.84961122
0.84942871
0.84934300
0.84917474
INFO - Training [55][  120/  196]   Loss 0.295620   Top1 89.667969   Top5 98.964844   BatchTime 0.391094   LR 0.000358
0.84916890
0.84927446
0.84936982
0.84928459
0.84914452
0.84900343
0.84900677
0.84888381
0.84879470
0.84879225
0.84888804
0.84884876
0.84893030
0.84908777
0.84909683
0.84887248
0.84897083
0.84888494
0.84897995
0.84869623
0.84834093
0.84816235
INFO - Training [55][  140/  196]   Loss 0.295459   Top1 89.681920   Top5 99.029018   BatchTime 0.388414   LR 0.000354
0.84812886
0.84795725
0.84777272
0.84771317
0.84771717
0.84766114
0.84798610
0.84800071
0.84772569
0.84758151
0.84773749
0.84796506
0.84833109
0.84830713
0.84843212
0.84823203
INFO - Training [55][  160/  196]   Loss 0.296664   Top1 89.592285   Top5 99.020996   BatchTime 0.383391   LR 0.000349
0.84791648
0.84761101
0.84750605
0.84749603
0.84749413
0.84755570
0.84770364
0.84767765
0.84769684
0.84792310
0.84785289
0.84773678
0.84753954
0.84730566
0.84632516
0.84606063
0.84659213
0.84662044
0.84618831
0.84556782
0.84539467
INFO - Training [55][  180/  196]   Loss 0.296958   Top1 89.572483   Top5 98.984375   BatchTime 0.372932   LR 0.000345
0.84505820
0.84459418
0.84429145
0.84436876
0.84438306
0.84486002
0.84540939
0.84556812
0.84567505
0.84560049
0.84548807
0.84546989
0.84565401
0.84728903
0.84739852
0.84751374
0.84733570
0.84760565
INFO - ==> Top1: 89.668    Top5: 98.976    Loss: 0.295
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [55][   20/   40]   Loss 0.303196   Top1 90.820312   Top5 99.707031   BatchTime 0.121842
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0399)
features.2.conv.0 tensor(0.0150)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0642)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0319)
features.4.conv.0 tensor(0.0161)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0732)
features.5.conv.0 tensor(0.0381)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0895)
features.6.conv.0 tensor(0.0259)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0529)
features.7.conv.0 tensor(0.0338)
features.7.conv.3 tensor(0.1004)
features.7.conv.6 tensor(0.0863)
features.8.conv.0 tensor(0.0658)
features.8.conv.3 tensor(0.1172)
features.8.conv.6 tensor(0.0981)
features.9.conv.0 tensor(0.0664)
features.9.conv.3 tensor(0.1638)
features.9.conv.6 tensor(0.0845)
features.10.conv.0 tensor(0.0365)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0668)
features.11.conv.0 tensor(0.1117)
features.11.conv.3 tensor(0.1557)
features.11.conv.6 tensor(0.1571)
features.12.conv.0 tensor(0.1266)
features.12.conv.3 tensor(0.1310)
features.12.conv.6 tensor(0.1930)
features.13.conv.0 tensor(0.0584)
features.13.conv.3 tensor(0.1804)
features.13.conv.6 tensor(0.0803)
features.14.conv.0 tensor(0.8066)
features.14.conv.3 tensor(0.0958)
features.14.conv.6 tensor(0.9567)
features.15.conv.0 tensor(0.9037)
features.15.conv.3 tensor(0.0931)
features.15.conv.6 tensor(0.8641)
features.16.conv.0 tensor(0.0757)
features.16.conv.3 tensor(0.1326)
features.16.conv.6 tensor(0.2121)
conv.0 tensor(0.1363)
tensor(742609.) 2188896.0
INFO - Validation [55][   40/   40]   Loss 0.289878   Top1 90.880000   Top5 99.790000   BatchTime 0.088473
INFO - ==> Top1: 90.880    Top5: 99.790    Loss: 0.290
INFO - ==> Sparsity : 0.339
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.880   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [52][Top1: 90.640   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
0.84778017
0.84819072
0.84872347
0.84945482
0.84908080
0.84896332
0.84895784
0.84868264
0.84848523
0.84862763
0.84867227
0.84861380
0.84862185
0.84872425
0.84862584
0.84842056
0.84792757
0.84754235
0.84773535
INFO - Training [56][   20/  196]   Loss 0.303421   Top1 88.828125   Top5 98.535156   BatchTime 0.437889   LR 0.000337
0.84763682
0.84749192
0.84734154
0.84716100
0.84721059
0.84743619
0.84759623
0.84752882
0.84792346
0.84767824
0.84745646
0.84748900
0.84742552
0.84746218
0.84745526
0.84754199
0.84743488
0.84744531
0.84739697
0.84734011
INFO - Training [56][   40/  196]   Loss 0.308444   Top1 88.935547   Top5 98.642578   BatchTime 0.418060   LR 0.000333
0.84740740
0.84751177
0.84750515
0.84725368
0.84655797
0.84639788
0.84619576
0.84619695
0.84651041
0.84639543
0.84629452
0.84634972
0.84727061
0.84768856
0.84752721
0.84748042
0.84731215
0.84730285
0.84744960
0.84712654
0.84726161
INFO - Training [56][   60/  196]   Loss 0.308660   Top1 89.095052   Top5 98.763021   BatchTime 0.401048   LR 0.000328
0.84738857
0.84715307
0.84706283
0.84699607
0.84699363
0.84685779
0.84685725
0.84678328
0.84656602
0.84653413
0.84682471
0.84663612
0.84666020
0.84674829
0.84675246
0.84707475
0.84715664
0.84728748
0.84713364
0.84718138
0.84719110
0.84709877
INFO - Training [56][   80/  196]   Loss 0.304544   Top1 89.238281   Top5 98.862305   BatchTime 0.393537   LR 0.000324
0.84702098
0.84696084
0.84685361
0.84696358
0.84700674
0.84701961
0.84700203
0.84682482
0.84675395
0.84672153
0.84623754
0.84562641
0.84540957
0.84526849
0.84524584
0.84541643
INFO - Training [56][  100/  196]   Loss 0.295144   Top1 89.503906   Top5 98.945312   BatchTime 0.389235   LR 0.000319
0.84572494
0.84596276
0.84590614
0.84591001
0.84613717
0.84615403
0.84624004
0.84617954
0.84603781
0.84601265
0.84630924
0.84622824
0.84625024
0.84628534
0.84652513
0.84660566
0.84647232
0.84655559
0.84654838
0.84644955
0.84646797
INFO - Training [56][  120/  196]   Loss 0.289813   Top1 89.707031   Top5 99.010417   BatchTime 0.389069   LR 0.000315
0.84668380
0.84832215
0.84844106
0.84873265
0.84842676
0.84834355
0.84853989
0.84863007
0.84829909
0.84796065
0.84793937
0.84788764
0.84787613
0.84792078
0.84798962
0.84800655
0.84796482
0.84835774
0.84841317
0.84834200
0.84816515
INFO - Training [56][  140/  196]   Loss 0.288429   Top1 89.771205   Top5 99.062500   BatchTime 0.387747   LR 0.000311
0.84813911
0.84810406
0.84815884
0.84827095
0.84832382
0.84825665
0.84818918
0.84801668
0.84804589
0.84803039
0.84802938
0.84800351
0.84805614
0.84811312
0.84806657
0.84823781
0.84822577
0.84827566
0.84855664
INFO - Training [56][  160/  196]   Loss 0.291492   Top1 89.716797   Top5 99.057617   BatchTime 0.378267   LR 0.000306
0.84837365
0.84809023
0.84785259
0.84704959
0.84659278
0.84656763
0.84651887
0.84655011
0.84655887
0.84659499
0.84803241
0.84815526
0.84799439
0.84770739
0.84781218
0.84810168
0.84809083
0.84804773
0.84807765
INFO - Training [56][  180/  196]   Loss 0.292587   Top1 89.702691   Top5 99.045139   BatchTime 0.373342   LR 0.000302
0.84820861
0.84811074
0.84797513
0.84773535
0.84757674
0.84758478
0.84774637
0.84787053
0.84791332
0.84796321
0.84799045
0.84792364
0.84796339
0.84834504
0.84821177
0.84820187
0.84820843
0.84836149
********************pre-trained*****************
INFO - ==> Top1: 89.704    Top5: 99.056    Loss: 0.292
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [56][   20/   40]   Loss 0.337411   Top1 89.863281   Top5 99.492188   BatchTime 0.119953
INFO - Validation [56][   40/   40]   Loss 0.327991   Top1 89.830000   Top5 99.660000   BatchTime 0.085282
INFO - ==> Top1: 89.830    Top5: 99.660    Loss: 0.328
INFO - ==> Sparsity : 0.340
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.880   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [52][Top1: 90.640   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4340)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0378)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0666)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0164)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.0742)
features.5.conv.0 tensor(0.0368)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.0892)
features.6.conv.0 tensor(0.0264)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0334)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.0855)
features.8.conv.0 tensor(0.0683)
features.8.conv.3 tensor(0.1134)
features.8.conv.6 tensor(0.0978)
features.9.conv.0 tensor(0.0660)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.0839)
features.10.conv.0 tensor(0.0354)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0667)
features.11.conv.0 tensor(0.1111)
features.11.conv.3 tensor(0.1547)
features.11.conv.6 tensor(0.1598)
features.12.conv.0 tensor(0.1257)
features.12.conv.3 tensor(0.1294)
features.12.conv.6 tensor(0.1916)
features.13.conv.0 tensor(0.0615)
features.13.conv.3 tensor(0.1786)
features.13.conv.6 tensor(0.0799)
features.14.conv.0 tensor(0.8128)
features.14.conv.3 tensor(0.0970)
features.14.conv.6 tensor(0.9617)
features.15.conv.0 tensor(0.9014)
features.15.conv.3 tensor(0.0920)
features.15.conv.6 tensor(0.8736)
features.16.conv.0 tensor(0.0745)
features.16.conv.3 tensor(0.1311)
features.16.conv.6 tensor(0.2114)
conv.0 tensor(0.1330)
tensor(743688.) 2188896.0
0.84857839
0.84833330
0.84817719
0.84834099
0.84825820
0.84827155
0.84830052
0.84857291
0.84861118
0.84862834
0.84844881
0.84843421
0.84827709
0.84829545
0.84828633
0.84841561
0.84856504
0.84835875
0.84843391
0.84844011
0.84852731
INFO - Training [57][   20/  196]   Loss 0.303781   Top1 89.746094   Top5 98.535156   BatchTime 0.458420   LR 0.000294
0.84868217
0.84856820
0.84837860
0.84838712
0.84857494
0.84870899
0.84857434
0.84854263
0.84875679
0.84886652
0.84884846
0.84894413
0.84925240
0.84934175
0.84947938
0.84937537
0.84899741
0.84898537
0.84898865
0.84901935
INFO - Training [57][   40/  196]   Loss 0.303349   Top1 89.580078   Top5 98.564453   BatchTime 0.425572   LR 0.000290
0.84908831
0.84901518
0.84872651
0.84858114
0.84866190
0.84797913
0.84788007
0.84818035
0.84817231
0.84847528
0.84861708
0.84876305
0.84874648
0.84895968
0.84922451
0.84928244
0.84915465
0.84913296
0.84911585
0.84911418
0.84920716
INFO - Training [57][   60/  196]   Loss 0.303106   Top1 89.485677   Top5 98.697917   BatchTime 0.409840   LR 0.000286
0.84915030
0.84902495
0.84907991
0.84912378
0.84915167
0.84916985
0.84890670
0.84884810
0.84916681
0.84888637
0.84878117
0.84883267
0.84878373
0.84857380
0.84865457
0.84867936
0.84862036
INFO - Training [57][   80/  196]   Loss 0.298587   Top1 89.555664   Top5 98.872070   BatchTime 0.398456   LR 0.000282
0.84853530
0.84871966
0.84879047
0.84865201
0.84830040
0.84822881
0.84843403
0.84827518
0.84835315
0.84837520
0.84792608
0.84782511
0.84774125
0.84772366
0.84741968
0.84740007
0.84771973
0.84830397
0.84846061
0.84852844
0.84857881
INFO - Training [57][  100/  196]   Loss 0.293517   Top1 89.792969   Top5 98.929688   BatchTime 0.393285   LR 0.000277
0.84852713
0.84829754
0.84810811
0.84808689
0.84818226
0.84810483
0.84830523
0.84826267
0.84825695
0.84841514
0.84871906
0.84890491
0.84886235
0.84898305
0.84866834
0.84854698
0.84848881
0.84845835
0.84855956
0.84826773
0.84812105
0.84831172
INFO - Training [57][  120/  196]   Loss 0.290942   Top1 89.912109   Top5 98.981120   BatchTime 0.389123   LR 0.000273
0.84830296
0.84824997
0.84876978
0.84863091
0.84857655
0.84835780
0.84834576
0.84820825
0.84805685
0.84797615
0.84801584
0.84799320
0.84794265
0.84773630
0.84744555
0.84756851
0.84793973
0.84840292
INFO - Training [57][  140/  196]   Loss 0.292188   Top1 89.944196   Top5 99.042969   BatchTime 0.382515   LR 0.000269
0.84832090
0.84835762
0.84858078
0.84863776
0.84848899
0.84880853
0.84889019
0.84849823
0.84862888
0.84864002
0.84847879
0.84852117
0.84874141
0.84873945
0.84864646
0.84873474
0.84882230
0.84872520
0.84866512
INFO - Training [57][  160/  196]   Loss 0.295374   Top1 89.768066   Top5 99.028320   BatchTime 0.375320   LR 0.000265
0.84843200
0.84849805
0.84849155
0.84844565
0.84839612
0.84835196
0.84854496
0.84873587
0.84880412
0.84894609
0.84886926
0.84888893
0.84882522
0.84859353
0.84835845
0.84814823
0.84826869
0.84835941
0.84847075
0.84884620
0.84874153
INFO - Training [57][  180/  196]   Loss 0.296609   Top1 89.717882   Top5 98.999566   BatchTime 0.374023   LR 0.000261
0.84860224
0.84857839
0.84845924
0.84785110
0.84753948
0.84733987
0.84760374
0.84752262
0.84726650
0.84689480
0.84660554
0.84676886
0.84702438
0.84771305
0.84830624
0.84826291
********************pre-trained*****************
INFO - ==> Top1: 89.722    Top5: 98.994    Loss: 0.296
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [57][   20/   40]   Loss 0.300196   Top1 90.859375   Top5 99.648438   BatchTime 0.131438
features.0.conv.0 tensor(0.4479)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0382)
features.2.conv.0 tensor(0.0139)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0674)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0319)
features.4.conv.0 tensor(0.0179)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0745)
features.5.conv.0 tensor(0.0373)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0884)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0520)
features.7.conv.0 tensor(0.0312)
features.7.conv.3 tensor(0.1045)
features.7.conv.6 tensor(0.0845)
features.8.conv.0 tensor(0.0657)
features.8.conv.3 tensor(0.1160)
features.8.conv.6 tensor(0.0979)
features.9.conv.0 tensor(0.0656)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.0839)
features.10.conv.0 tensor(0.0390)
features.10.conv.3 tensor(0.1027)
features.10.conv.6 tensor(0.0668)
features.11.conv.0 tensor(0.1083)
features.11.conv.3 tensor(0.1561)
features.11.conv.6 tensor(0.1618)
features.12.conv.0 tensor(0.1318)
features.12.conv.3 tensor(0.1292)
features.12.conv.6 tensor(0.1908)
features.13.conv.0 tensor(0.0555)
features.13.conv.3 tensor(0.1782)
features.13.conv.6 tensor(0.0809)
features.14.conv.0 tensor(0.8046)
features.14.conv.3 tensor(0.0955)
features.14.conv.6 tensor(0.9598)
features.15.conv.0 tensor(0.9015)
features.15.conv.3 tensor(0.0936)
features.15.conv.6 tensor(0.8665)
features.16.conv.0 tensor(0.0735)
features.16.conv.3 tensor(0.1286)
features.16.conv.6 tensor(0.2107)
conv.0 tensor(0.1326)
tensor(740490.) 2188896.0
INFO - Validation [57][   40/   40]   Loss 0.283270   Top1 91.100000   Top5 99.720000   BatchTime 0.095509
INFO - ==> Top1: 91.100    Top5: 99.720    Loss: 0.283
INFO - ==> Sparsity : 0.338
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [57][Top1: 91.100   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [55][Top1: 90.880   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
0.84836096
0.84828210
0.84821975
0.84782928
0.84758151
0.84735137
0.84728092
0.84735996
0.84718812
0.84633178
0.84538639
0.84525949
0.84502715
0.84512836
0.84678215
0.84636742
0.84626681
0.84601909
0.84604132
0.84592783
0.84578347
0.84588593
INFO - Training [58][   20/  196]   Loss 0.306962   Top1 89.257812   Top5 98.593750   BatchTime 0.481514   LR 0.000254
0.84582555
0.84584916
0.84631878
0.84721547
0.84701604
0.84733284
0.84755075
0.84748852
0.84746116
0.84751385
0.84738278
0.84721810
0.84700811
0.84702057
0.84704953
0.84685272
0.84694970
0.84698349
0.84694183
0.84733492
INFO - Training [58][   40/  196]   Loss 0.307546   Top1 89.257812   Top5 98.789062   BatchTime 0.435125   LR 0.000250
0.84710300
0.84694201
0.84686440
0.84676230
0.84679705
0.84676033
0.84674442
0.84694618
0.84713942
0.84715748
0.84709662
0.84742767
0.84776014
0.84772438
0.84741771
0.84774745
INFO - Training [58][   60/  196]   Loss 0.302946   Top1 89.309896   Top5 98.886719   BatchTime 0.415318   LR 0.000246
0.84749174
0.84742343
0.84727091
0.84723258
0.84731251
0.84730196
0.84733284
0.84690118
0.84703583
0.84682107
0.84678483
0.84683585
0.84691727
0.84704787
0.84731996
0.84709597
0.84741414
0.84754461
0.84754694
0.84764189
0.84751731
0.84766245
INFO - Training [58][   80/  196]   Loss 0.305697   Top1 89.194336   Top5 98.974609   BatchTime 0.404335   LR 0.000242
0.84749806
0.84742475
0.84724104
0.84716517
0.84718513
0.84708589
0.84731007
0.84747845
0.84743440
0.84729135
0.84728205
0.84710050
0.84721166
0.84732473
0.84727907
0.84742582
0.84748036
0.84742886
0.84732151
0.84703308
0.84716159
0.84725177
INFO - Training [58][  100/  196]   Loss 0.296426   Top1 89.613281   Top5 99.031250   BatchTime 0.395935   LR 0.000238
0.84716105
0.84717828
0.84719181
0.84717774
0.84711051
0.84677660
0.84694213
0.84658921
0.84654570
0.84618646
0.84591019
0.84562045
0.84539020
0.84550977
0.84520072
INFO - Training [58][  120/  196]   Loss 0.291810   Top1 89.804688   Top5 99.091797   BatchTime 0.392573   LR 0.000234
0.84503633
0.84490114
0.84478843
0.84494168
0.84507996
0.84496748
0.84491539
0.84499639
0.84526974
0.84526378
0.84504837
0.84537721
0.84500217
0.84493434
0.84627521
0.84623569
0.84640312
0.84641939
0.84621453
0.84603208
INFO - Training [58][  140/  196]   Loss 0.291281   Top1 89.829799   Top5 99.135045   BatchTime 0.379522   LR 0.000230
0.84614408
0.84619892
0.84625822
0.84674442
0.84731889
0.84773809
0.84926355
0.84915018
0.84904188
0.84946793
0.84908253
0.84864312
0.84838802
0.84829050
0.84826046
0.84824502
0.84814674
0.84818649
0.84792709
0.84824902
0.84840816
0.84821570
0.84794050
INFO - Training [58][  160/  196]   Loss 0.294957   Top1 89.709473   Top5 99.096680   BatchTime 0.376887   LR 0.000226
0.84763587
0.84738207
0.84730023
0.84744531
0.84754932
0.84780866
0.84781784
0.84789270
0.84786379
0.84778035
0.84769380
0.84788144
0.84809852
0.84813708
0.84839272
0.84867859
0.84925461
0.84897828
0.84876418
0.84861273
0.84866589
INFO - Training [58][  180/  196]   Loss 0.293777   Top1 89.772135   Top5 99.036458   BatchTime 0.376371   LR 0.000222
0.84860468
0.84873044
0.84878713
0.84883457
0.84902573
0.84920257
0.84913129
0.84939826
0.84892672
0.84833187
0.84765261
0.84731519
0.84712130
0.84700012
0.84687155
********************pre-trained*****************
INFO - ==> Top1: 89.882    Top5: 99.038    Loss: 0.291
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [58][   20/   40]   Loss 0.296627   Top1 90.644531   Top5 99.667969   BatchTime 0.132231
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0386)
features.2.conv.0 tensor(0.0156)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0657)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0330)
features.4.conv.0 tensor(0.0176)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0757)
features.5.conv.0 tensor(0.0353)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0876)
features.6.conv.0 tensor(0.0233)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0515)
features.7.conv.0 tensor(0.0301)
features.7.conv.3 tensor(0.1024)
features.7.conv.6 tensor(0.0844)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1131)
features.8.conv.6 tensor(0.1053)
features.9.conv.0 tensor(0.0647)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.0840)
features.10.conv.0 tensor(0.0404)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0672)
features.11.conv.0 tensor(0.1047)
features.11.conv.3 tensor(0.1576)
features.11.conv.6 tensor(0.1584)
features.12.conv.0 tensor(0.1322)
features.12.conv.3 tensor(0.1281)
features.12.conv.6 tensor(0.1911)
features.13.conv.0 tensor(0.0542)
features.13.conv.3 tensor(0.1748)
features.13.conv.6 tensor(0.0797)
features.14.conv.0 tensor(0.8145)
features.14.conv.3 tensor(0.0953)
features.14.conv.6 tensor(0.9620)
features.15.conv.0 tensor(0.9061)
features.15.conv.3 tensor(0.0939)
features.15.conv.6 tensor(0.8820)
features.16.conv.0 tensor(0.0742)
features.16.conv.3 tensor(0.1299)
features.16.conv.6 tensor(0.2083)
conv.0 tensor(0.1314)
tensor(743902.) 2188896.0
INFO - Validation [58][   40/   40]   Loss 0.284831   Top1 90.930000   Top5 99.760000   BatchTime 0.093126
INFO - ==> Top1: 90.930    Top5: 99.760    Loss: 0.285
INFO - ==> Sparsity : 0.340
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [57][Top1: 91.100   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 90.930   Top5: 99.760]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
0.84653306
0.84639221
0.84657437
0.84638268
0.84653592
0.84680933
0.84712726
0.84689116
0.84710169
0.84672731
0.84655690
0.84644049
0.84740841
0.84746885
0.84744900
0.84729546
0.84723437
0.84730655
INFO - Training [59][   20/  196]   Loss 0.298065   Top1 89.394531   Top5 98.261719   BatchTime 0.441452   LR 0.000215
0.84753835
0.84762520
0.84771246
0.84715378
0.84714746
0.84719682
0.84725052
0.84698898
0.84710962
0.84705859
0.84705442
0.84700733
0.84702390
0.84701711
0.84696907
0.84622753
0.84579235
0.84621012
0.84694886
0.84684348
0.84618020
INFO - Training [59][   40/  196]   Loss 0.308350   Top1 89.091797   Top5 98.486328   BatchTime 0.416231   LR 0.000212
0.84592187
0.84566116
0.84545505
0.84570557
0.84701920
0.84691459
0.84686333
0.84685898
0.84678304
0.84669918
0.84676468
0.84696490
0.84690195
0.84667104
0.84670740
0.84672815
0.84696579
0.84663635
0.84637487
0.84644562
0.84764719
INFO - Training [59][   60/  196]   Loss 0.300171   Top1 89.433594   Top5 98.645833   BatchTime 0.404323   LR 0.000208
0.84777677
0.84689295
0.84651834
0.84625143
0.84579086
0.84560162
0.84544432
0.84531635
0.84519523
0.84505683
0.84497112
0.84528476
0.84510839
0.84531170
0.84528309
0.84526652
0.84558058
0.84519756
0.84534639
0.84531271
0.84535986
INFO - Training [59][   80/  196]   Loss 0.300294   Top1 89.389648   Top5 98.842773   BatchTime 0.396829   LR 0.000204
0.84525537
0.84515989
0.84506994
0.84504086
0.84487212
0.84474468
0.84446424
0.84437525
0.84416234
0.84363419
0.84356350
0.84341848
0.84314954
0.84289294
0.84257537
0.84232110
0.84208655
0.84213668
0.84236538
0.84270471
0.84257877
INFO - Training [59][  100/  196]   Loss 0.294113   Top1 89.621094   Top5 98.925781   BatchTime 0.392915   LR 0.000201
0.84411103
0.84427685
0.84405470
0.84388250
0.84377110
0.84380406
0.84373909
0.84375715
0.84386986
0.84397846
0.84422201
0.84437466
0.84423816
0.84381127
0.84482640
0.84487194
0.84448963
0.84420991
0.84402680
INFO - Training [59][  120/  196]   Loss 0.288631   Top1 89.847005   Top5 98.987630   BatchTime 0.381098   LR 0.000197
0.84383738
0.84350634
0.84351933
0.84319085
0.84290302
0.84267521
0.84256363
0.84280062
0.84316152
0.84409702
0.84428078
0.84421402
0.84404528
0.84381706
0.84381098
0.84382468
0.84395951
0.84397191
INFO - Training [59][  140/  196]   Loss 0.287480   Top1 89.899554   Top5 99.054129   BatchTime 0.376467   LR 0.000193
0.84393376
0.84391326
0.84418970
0.84433609
0.84435558
0.84449750
0.84473503
0.84450364
0.84438604
0.84424102
0.84422243
0.84424984
0.84418863
0.84444386
0.84477496
0.84499836
0.84489626
0.84486336
0.84478444
0.84476888
0.84489715
INFO - Training [59][  160/  196]   Loss 0.290766   Top1 89.743652   Top5 99.050293   BatchTime 0.375713   LR 0.000190
0.84484971
0.84465867
0.84474504
0.84458601
0.84454370
0.84492171
0.84523660
0.84589440
0.84646726
0.84691393
0.84706700
0.84698564
0.84700465
0.84683394
0.84670085
0.84658700
0.84647560
0.84643668
0.84652811
0.84651893
0.84655243
0.84674239
0.84721261
INFO - Training [59][  180/  196]   Loss 0.292641   Top1 89.657118   Top5 99.008247   BatchTime 0.373677   LR 0.000186
0.84721714
0.84710169
0.84693754
0.84683037
0.84683412
0.84679091
0.84672862
0.84666741
0.84670085
0.84665221
0.84648275
0.84636474
0.84634656
********************pre-trained*****************
INFO - ==> Top1: 89.742    Top5: 99.006    Loss: 0.291
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [59][   20/   40]   Loss 0.319255   Top1 90.390625   Top5 99.492188   BatchTime 0.132600
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0360)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0639)
features.3.conv.0 tensor(0.0159)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0323)
features.4.conv.0 tensor(0.0171)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0773)
features.5.conv.0 tensor(0.0345)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0898)
features.6.conv.0 tensor(0.0252)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0517)
features.7.conv.0 tensor(0.0314)
features.7.conv.3 tensor(0.1033)
features.7.conv.6 tensor(0.0849)
features.8.conv.0 tensor(0.0635)
features.8.conv.3 tensor(0.1143)
features.8.conv.6 tensor(0.0991)
features.9.conv.0 tensor(0.0642)
features.9.conv.3 tensor(0.1606)
features.9.conv.6 tensor(0.1200)
features.10.conv.0 tensor(0.0397)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0670)
features.11.conv.0 tensor(0.1047)
features.11.conv.3 tensor(0.1564)
features.11.conv.6 tensor(0.1589)
features.12.conv.0 tensor(0.1377)
features.12.conv.3 tensor(0.1285)
features.12.conv.6 tensor(0.1901)
features.13.conv.0 tensor(0.0539)
features.13.conv.3 tensor(0.1792)
features.13.conv.6 tensor(0.0805)
features.14.conv.0 tensor(0.8006)
features.14.conv.3 tensor(0.0935)
features.14.conv.6 tensor(0.9607)
features.15.conv.0 tensor(0.9154)
features.15.conv.3 tensor(0.0927)
features.15.conv.6 tensor(0.8799)
features.16.conv.0 tensor(0.0739)
features.16.conv.3 tensor(0.1300)
features.16.conv.6 tensor(0.2057)
conv.0 tensor(0.1318)
tensor(743014.) 2188896.0
INFO - Validation [59][   40/   40]   Loss 0.303865   Top1 90.310000   Top5 99.680000   BatchTime 0.094060
INFO - ==> Top1: 90.310    Top5: 99.680    Loss: 0.304
INFO - ==> Sparsity : 0.339
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [57][Top1: 91.100   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 90.930   Top5: 99.760]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
0.84610319
0.84562510
0.84581250
0.84590393
0.84586757
0.84581220
0.84629345
0.84677291
0.84675854
0.84704608
0.84675723
0.84672898
0.84648228
0.84641755
0.84644240
0.84628320
0.84554517
0.84535599
0.84525257
0.84531933
INFO - Training [60][   20/  196]   Loss 0.307851   Top1 89.023438   Top5 98.847656   BatchTime 0.439395   LR 0.000180
0.84549409
0.84565872
0.84575981
0.84579206
0.84561205
0.84561455
0.84547865
0.84542811
0.84530407
0.84530514
0.84532887
0.84527665
0.84539551
0.84535998
0.84538835
0.84527838
0.84535384
0.84539729
0.84533602
0.84535533
0.84523344
INFO - Training [60][   40/  196]   Loss 0.304946   Top1 89.140625   Top5 98.876953   BatchTime 0.411202   LR 0.000176
0.84506285
0.84494942
0.84479862
0.84466958
0.84446424
0.84459120
0.84451747
0.84438938
0.84431696
0.84422809
0.84415090
0.84415257
0.84435600
0.84439760
0.84427851
0.84390122
0.84380966
0.84347302
0.84350765
0.84353125
0.84343565
INFO - Training [60][   60/  196]   Loss 0.299042   Top1 89.420573   Top5 98.997396   BatchTime 0.397809   LR 0.000173
0.84328073
0.84312946
0.84283930
0.84265739
0.84260118
0.84256512
0.84262913
0.84254712
0.84253204
0.84241641
0.84242880
0.84239918
0.84231043
0.84232402
0.84229988
0.84219068
INFO - Training [60][   80/  196]   Loss 0.290243   Top1 89.814453   Top5 99.101562   BatchTime 0.392886   LR 0.000169
0.84239918
0.84238857
0.84237123
0.84233874
0.84231758
0.84194630
0.84150910
0.84175622
0.84193933
0.84186321
0.84205723
0.84150869
0.84160268
0.84134102
0.84079081
0.84035987
0.83988672
0.83993441
0.84003961
0.84168738
0.84166843
0.84157687
INFO - Training [60][  100/  196]   Loss 0.285041   Top1 90.035156   Top5 99.109375   BatchTime 0.386301   LR 0.000166
0.84166563
0.84167528
0.84114659
0.84089750
0.84077084
0.84073687
0.84087628
0.84093356
0.84209347
0.84232998
0.84239638
0.84232819
0.84218758
0.84205967
0.84204870
0.84199411
0.84210551
0.84196854
0.84198582
0.84190279
INFO - Training [60][  120/  196]   Loss 0.281166   Top1 90.172526   Top5 99.134115   BatchTime 0.373056   LR 0.000162
0.84196246
0.84195131
0.84196258
0.84192580
0.84181267
0.84171557
0.84160954
0.84167784
0.84174269
0.84182793
0.84207934
0.84222299
0.84229696
0.84234911
0.84237623
0.84243667
0.84233528
0.84241933
0.84234577
0.84233856
0.84221834
0.84211498
INFO - Training [60][  140/  196]   Loss 0.280651   Top1 90.206473   Top5 99.162946   BatchTime 0.371951   LR 0.000159
0.84208733
0.84200567
0.84198523
0.84193534
0.84210306
0.84181887
0.84180105
0.84177065
0.84176081
0.84170681
0.84165978
0.84170997
0.84174585
0.84175253
0.84174353
0.84164327
INFO - Training [60][  160/  196]   Loss 0.285432   Top1 90.012207   Top5 99.145508   BatchTime 0.370909   LR 0.000156
0.84163815
0.84167176
0.84166563
0.84156060
0.84155035
0.84163451
0.84155351
0.84165972
0.84148157
0.84096628
0.84042776
0.84053653
0.84073001
0.84017235
0.83980542
0.83958280
0.83963305
0.83956468
0.83952701
0.83940762
0.83896863
INFO - Training [60][  180/  196]   Loss 0.285585   Top1 90.002170   Top5 99.101562   BatchTime 0.372973   LR 0.000152
0.83881450
0.83887917
0.83948696
0.84197986
0.84190226
0.84181517
0.84176999
0.84164363
0.84157044
0.84148479
0.84133762
0.84139413
0.84138161
0.84129739
0.84131485
0.84120196
0.84115255
INFO - ==> Top1: 90.028    Top5: 99.086    Loss: 0.285
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [60][   20/   40]   Loss 0.298141   Top1 90.976562   Top5 99.746094   BatchTime 0.131906
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1621)
features.1.conv.0
INFO - Validation [60][   40/   40]   Loss 0.281546   Top1 91.230000   Top5 99.800000   BatchTime 0.094489
INFO - ==> Top1: 91.230    Top5: 99.800    Loss: 0.282
INFO - ==> Sparsity : 0.341
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
features.1.conv.0 tensor(0.0111)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0651)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0312)
features.4.conv.0 tensor(0.0184)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0771)
features.5.conv.0 tensor(0.0368)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.0894)
features.6.conv.0 tensor(0.0249)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0509)
features.7.conv.0 tensor(0.0307)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.1313)
features.8.conv.0 tensor(0.0618)
features.8.conv.3 tensor(0.1131)
features.8.conv.6 tensor(0.1339)
features.9.conv.0 tensor(0.0655)
features.9.conv.3 tensor(0.1626)
features.9.conv.6 tensor(0.0959)
features.10.conv.0 tensor(0.0387)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0668)
features.11.conv.0 tensor(0.1038)
features.11.conv.3 tensor(0.1541)
features.11.conv.6 tensor(0.1651)
features.12.conv.0 tensor(0.1327)
features.12.conv.3 tensor(0.1291)
features.12.conv.6 tensor(0.1934)
features.13.conv.0 tensor(0.0566)
features.13.conv.3 tensor(0.1788)
features.13.conv.6 tensor(0.0801)
features.14.conv.0 tensor(0.8097)
features.14.conv.3 tensor(0.0932)
features.14.conv.6 tensor(0.9624)
features.15.conv.0 tensor(0.9116)
features.15.conv.3 tensor(0.0928)
features.15.conv.6 tensor(0.8878)
features.16.conv.0 tensor(0.0758)
features.16.conv.3 tensor(0.1308)
features.16.conv.6 tensor(0.2035)
conv.0 tensor(0.1322)
tensor(746794.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
0.84116346
0.84094220
0.84080291
0.84078211
0.84084159
0.84079790
0.84079307
0.84074104
0.84069937
0.84058291
0.84054363
0.84051770
0.84040821
0.84028995
0.84017938
0.84008640
0.83992434
0.83984500
0.83975095
INFO - Training [61][   20/  196]   Loss 0.308706   Top1 88.671875   Top5 98.730469   BatchTime 0.451596   LR 0.000147
0.83971882
0.83979422
0.83994931
0.84010351
0.83998555
0.83999610
0.83991772
0.84010923
0.83995402
0.83983338
0.83975971
0.83964819
0.83952677
0.83956033
0.84005433
0.84127384
0.84108019
0.84080011
0.84047359
0.84037995
0.84036940
INFO - Training [61][   40/  196]   Loss 0.300474   Top1 89.179688   Top5 98.906250   BatchTime 0.413620   LR 0.000143
0.84052223
0.84055215
0.84079999
0.84081727
0.84091204
0.84093511
0.84099352
0.84098279
0.84070957
0.84081036
0.84093368
0.84054822
0.84032923
0.84029692
0.84011793
0.83988100
0.83969533
0.83965033
0.83971769
0.84029669
0.84049237
0.84053290
INFO - Training [61][   60/  196]   Loss 0.290806   Top1 89.674479   Top5 98.964844   BatchTime 0.399048   LR 0.000140
0.84043449
0.84056056
0.84052390
0.84048170
0.84039354
0.84031826
0.84023070
0.84017694
0.84024078
0.84034324
0.84040469
0.84027910
0.84015530
0.84063339
0.84067088
0.84066248
0.84070027
0.84063536
0.84056211
INFO - Training [61][   80/  196]   Loss 0.288402   Top1 89.916992   Top5 99.062500   BatchTime 0.379762   LR 0.000137
0.84056395
0.84046704
0.84053719
0.84045488
0.84039545
0.84040099
0.84045643
0.84050685
0.84066558
0.84039706
0.84031028
0.84020829
0.84007645
0.83998632
0.83992833
0.83993775
0.84002775
0.84014982
INFO - Training [61][  100/  196]   Loss 0.279651   Top1 90.230469   Top5 99.093750   BatchTime 0.369360   LR 0.000134
0.84009248
0.84011942
0.84008294
0.84004909
0.83999836
0.83987516
0.83995146
0.84047014
0.84051639
0.84030467
0.84043556
0.84028977
0.84020084
0.84034032
0.84042972
0.84047538
0.84036434
0.84012657
0.84007937
0.84001786
0.84013265
0.84011257
INFO - Training [61][  120/  196]   Loss 0.275625   Top1 90.358073   Top5 99.173177   BatchTime 0.368963   LR 0.000131
0.84011239
0.83977580
0.83932382
0.83861786
0.83830380
0.83819276
0.83806825
0.83794606
0.83772683
0.83770275
0.83787787
0.83779943
0.83773768
0.83766872
0.83751369
0.83746320
0.83746135
0.83756435
0.83799124
0.83932245
0.83939099
INFO - Training [61][  140/  196]   Loss 0.276002   Top1 90.351562   Top5 99.202009   BatchTime 0.371229   LR 0.000128
0.83932966
0.83924979
0.83916843
0.83907330
0.83896637
0.83894390
0.83902520
0.83896112
0.83908200
0.84079295
0.84071171
0.84051716
0.84044594
0.84034663
0.84044707
0.84054416
INFO - Training [61][  160/  196]   Loss 0.279466   Top1 90.231934   Top5 99.177246   BatchTime 0.371719   LR 0.000125
0.84065002
0.84069532
0.84082848
0.84081835
0.84069759
0.84066302
0.84058088
0.84055203
0.84050244
0.84049481
0.84054559
0.84056431
0.84053355
0.84062725
0.84070557
0.84051895
0.84040171
0.84056252
0.84052622
0.84051561
0.84039760
INFO - Training [61][  180/  196]   Loss 0.279262   Top1 90.275608   Top5 99.129774   BatchTime 0.372294   LR 0.000122
0.84045255
0.84058672
0.84076768
0.84065574
0.84044200
0.84026563
0.84016913
0.84004331
0.84003115
0.84015864
0.84023756
0.84031206
0.84046513
0.84057927
0.84036905
0.84042180
0.84041840
INFO - ==> Top1: 90.278    Top5: 99.122    Loss: 0.279
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [61][   20/   40]   Loss 0.289970   Top1 91.074219   Top5 99.726562   BatchTime 0.133649
INFO - Validation [61][   40/   40]   Loss 0.276649   Top1 91.330000   Top5 99.760000   BatchTime 0.094538
INFO - ==> Top1: 91.330    Top5: 99.760    Loss: 0.277
INFO - ==> Sparsity : 0.343
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0631)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0187)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0768)
features.5.conv.0 tensor(0.0374)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.0908)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0310)
features.7.conv.3 tensor(0.1033)
features.7.conv.6 tensor(0.0940)
features.8.conv.0 tensor(0.0615)
features.8.conv.3 tensor(0.1105)
features.8.conv.6 tensor(0.1255)
features.9.conv.0 tensor(0.0642)
features.9.conv.3 tensor(0.1623)
features.9.conv.6 tensor(0.0921)
features.10.conv.0 tensor(0.0382)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0670)
features.11.conv.0 tensor(0.1036)
features.11.conv.3 tensor(0.1537)
features.11.conv.6 tensor(0.1598)
features.12.conv.0 tensor(0.1310)
features.12.conv.3 tensor(0.1292)
features.12.conv.6 tensor(0.1900)
features.13.conv.0 tensor(0.0595)
features.13.conv.3 tensor(0.1800)
features.13.conv.6 tensor(0.0798)
features.14.conv.0 tensor(0.8107)
features.14.conv.3 tensor(0.0943)
features.14.conv.6 tensor(0.9637)
features.15.conv.0 tensor(0.9110)
features.15.conv.3 tensor(0.0914)
features.15.conv.6 tensor(0.9255)
features.16.conv.0 tensor(0.0759)
features.16.conv.3 tensor(0.1306)
features.16.conv.6 tensor(0.2026)
conv.0 tensor(0.1303)
tensor(750078.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
0.84035784
0.84029311
0.84026033
0.84019619
0.84007317
0.84014386
0.84104210
0.84123230
0.84124160
0.84115893
0.84103811
0.84098756
0.84100783
0.84119362
0.84110820
0.84088194
0.84060889
0.84051120
0.84048551
INFO - Training [62][   20/  196]   Loss 0.276831   Top1 90.410156   Top5 98.457031   BatchTime 0.442551   LR 0.000117
0.84050268
0.84075260
0.84052682
0.84041774
0.84032905
0.84036106
0.84035033
0.84025598
0.84037930
0.84049582
0.84026158
0.84047043
0.84020317
0.84005749
0.83929992
0.83876669
0.83874059
0.83887547
0.83893782
0.83873868
0.83864689
0.83861923
0.83865380
0.83862853
INFO - Training [62][   40/  196]   Loss 0.281813   Top1 90.205078   Top5 98.759766   BatchTime 0.387069   LR 0.000114
0.83861893
0.83851498
0.83851027
0.83855903
0.83855504
0.84031904
0.84029925
0.84033448
0.84028912
0.84027594
0.84023833
0.84038299
0.84049708
0.84049058
0.84057081
0.84047771
0.84045053
INFO - Training [62][   60/  196]   Loss 0.278868   Top1 90.227865   Top5 98.841146   BatchTime 0.370887   LR 0.000111
0.84047210
0.84039730
0.84036726
0.84047925
0.84056216
0.84065652
0.84073257
0.84051466
0.84053677
0.84039181
0.84028995
0.84026021
0.84028566
0.84028852
0.84023452
0.84016138
0.84027725
0.84031665
0.84024858
0.84036297
0.84026724
INFO - Training [62][   80/  196]   Loss 0.278314   Top1 90.292969   Top5 98.979492   BatchTime 0.373242   LR 0.000108
0.84005380
0.83995503
0.83990645
0.83980519
0.83995855
0.83999038
0.83977985
0.83982462
0.83972192
0.83952421
0.83955276
0.83946967
0.83950984
0.83936393
0.83893830
0.83891571
0.83888322
0.83869272
0.83849508
0.83843756
0.83848649
0.83845067
INFO - Training [62][  100/  196]   Loss 0.274765   Top1 90.503906   Top5 99.003906   BatchTime 0.374166   LR 0.000105
0.83836204
0.83824921
0.83812720
0.83812732
0.83820683
0.83854920
0.83957112
0.83931577
0.83908784
0.83896351
0.83887750
0.83872807
0.83860791
0.83854610
0.83847946
INFO - Training [62][  120/  196]   Loss 0.269685   Top1 90.722656   Top5 99.049479   BatchTime 0.374250   LR 0.000102
0.83846486
0.83848888
0.83854437
0.83865178
0.83866388
0.83862412
0.83877510
0.83878714
0.83882344
0.83891284
0.83909553
0.83910251
0.83892691
0.83908403
0.83919972
0.83904207
0.83896387
0.83921051
0.83910191
0.83880717
0.83874828
INFO - Training [62][  140/  196]   Loss 0.272497   Top1 90.633371   Top5 99.093192   BatchTime 0.378278   LR 0.000100
0.83871174
0.83865201
0.83856553
0.83845526
0.83839089
0.83837044
0.83823472
0.83813471
0.83827955
0.83840328
0.83844340
0.83842492
0.83830422
0.83818775
0.83787370
0.83749276
0.83716476
0.83685559
0.83676517
0.83660036
0.83655155
INFO - Training [62][  160/  196]   Loss 0.275360   Top1 90.522461   Top5 99.082031   BatchTime 0.377352   LR 0.000097
0.83641368
0.83626717
0.83602780
0.83575398
0.83571881
0.83566463
0.83567667
0.83553755
0.83533072
0.83509177
0.83500385
0.83505827
0.83509624
0.83521903
0.83529270
0.83738160
0.83738959
0.83739358
0.83729011
0.83734375
0.83728695
0.83711910
INFO - Training [62][  180/  196]   Loss 0.275855   Top1 90.447049   Top5 99.049479   BatchTime 0.376726   LR 0.000094
0.83706290
0.83703649
0.83719575
0.83759713
0.83730847
0.83727354
0.83735806
0.83751732
0.83745360
0.83750820
0.83744228
0.83753073
0.83736634
0.83725882
********************pre-trained*****************
INFO - ==> Top1: 90.474    Top5: 99.030    Loss: 0.275
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [62][   20/   40]   Loss 0.313336   Top1 90.683594   Top5 99.648438   BatchTime 0.135718
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0365)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0639)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0199)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0794)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.0905)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0523)
features.7.conv.0 tensor(0.0311)
features.7.conv.3 tensor(0.1027)
features.7.conv.6 tensor(0.0930)
features.8.conv.0 tensor(0.0616)
features.8.conv.3 tensor(0.1100)
features.8.conv.6 tensor(0.1541)
features.9.conv.0 tensor(0.0635)
features.9.conv.3 tensor(0.1609)
features.9.conv.6 tensor(0.0929)
features.10.conv.0 tensor(0.0381)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0668)
features.11.conv.0 tensor(0.1047)
features.11.conv.3 tensor(0.1539)
features.11.conv.6 tensor(0.1620)
features.12.conv.0 tensor(0.1307)
features.12.conv.3 tensor(0.1287)
features.12.conv.6 tensor(0.1899)
features.13.conv.0
INFO - Validation [62][   40/   40]   Loss 0.295057   Top1 91.000000   Top5 99.680000   BatchTime 0.096451
INFO - ==> Top1: 91.000    Top5: 99.680    Loss: 0.295
INFO - ==> Sparsity : 0.345
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
features.13.conv.0 tensor(0.0586)
features.13.conv.3 tensor(0.1775)
features.13.conv.6 tensor(0.0802)
features.14.conv.0 tensor(0.8249)
features.14.conv.3 tensor(0.0941)
features.14.conv.6 tensor(0.9638)
features.15.conv.0 tensor(0.9129)
features.15.conv.3 tensor(0.0911)
features.15.conv.6 tensor(0.9308)
features.16.conv.0 tensor(0.0753)
features.16.conv.3 tensor(0.1291)
features.16.conv.6 tensor(0.2026)
conv.0 tensor(0.1315)
tensor(754602.) 2188896.0
0.83732134
0.83730990
0.83738923
0.83750778
0.83746159
0.83730376
0.83738756
0.83741635
0.83719152
0.83692372
0.83702409
0.83714229
0.83730114
0.83727771
0.83722878
0.83704716
0.83722049
0.83704561
0.83680207
INFO - Training [63][   20/  196]   Loss 0.300603   Top1 89.257812   Top5 98.632812   BatchTime 0.403263   LR 0.000090
0.83686125
0.83687282
0.83689350
0.83679891
0.83672118
0.83680028
0.83682132
0.83691102
0.83677894
0.83656549
0.83648354
0.83655000
0.83654708
0.83659798
0.83646667
0.83646512
0.83659440
0.83631748
0.83613986
INFO - Training [63][   40/  196]   Loss 0.300364   Top1 89.335938   Top5 98.681641   BatchTime 0.363429   LR 0.000087
0.83604360
0.83601761
0.83590192
0.83581519
0.83570468
0.83578128
0.83570772
0.83584082
0.83548218
0.83550936
0.83559799
0.83559412
0.83542490
0.83531874
0.83542764
0.83550072
0.83553934
0.83549351
0.83548075
0.83535588
0.83525425
0.83531767
INFO - Training [63][   60/  196]   Loss 0.293987   Top1 89.602865   Top5 98.795573   BatchTime 0.366112   LR 0.000085
0.83536810
0.83529478
0.83533126
0.83529407
0.83547986
0.83541727
0.83528095
0.83527088
0.83527595
0.83516723
0.83531678
0.83533913
0.83522606
0.83523446
0.83511680
0.83505392
0.83512580
0.83523333
0.83510399
0.83487236
0.83487177
0.83436012
INFO - Training [63][   80/  196]   Loss 0.288285   Top1 89.853516   Top5 98.930664   BatchTime 0.365880   LR 0.000082
0.83396918
0.83467311
0.83535254
0.83542311
0.83538479
0.83532196
0.83526617
0.83526742
0.83525914
0.83524245
0.83524495
0.83523864
0.83521408
0.83520466
0.83518338
0.83517009
INFO - Training [63][  100/  196]   Loss 0.282521   Top1 90.058594   Top5 98.984375   BatchTime 0.365895   LR 0.000080
0.83515376
0.83511770
0.83508706
0.83497912
0.83483416
0.83479410
0.83471805
0.83465666
0.83476293
0.83487624
0.83498365
0.83498245
0.83495033
0.83506668
0.83524549
0.83566338
0.83575702
0.83574486
0.83573049
0.83577710
0.83579636
INFO - Training [63][  120/  196]   Loss 0.277940   Top1 90.214844   Top5 99.049479   BatchTime 0.369338   LR 0.000077
0.83578503
0.83575028
0.83570135
0.83569956
0.83578378
0.83586389
0.83604372
0.83632725
0.83784968
0.83831865
0.83819455
0.83819705
0.83816063
0.83820814
0.83829993
0.83841300
0.83826268
0.83838284
0.83804536
0.83794326
0.83787328
INFO - Training [63][  140/  196]   Loss 0.273184   Top1 90.438058   Top5 99.109933   BatchTime 0.370902   LR 0.000075
0.83785170
0.83784968
0.83786952
0.83793586
0.83802956
0.83792216
0.83793926
0.83819705
0.83814716
0.83823115
0.83799058
0.83796442
0.83805609
0.83788377
0.83777958
0.83777100
0.83782947
0.83792406
0.83788353
0.83790475
0.83793318
0.83820510
INFO - Training [63][  160/  196]   Loss 0.274697   Top1 90.351562   Top5 99.106445   BatchTime 0.370050   LR 0.000072
0.83827287
0.83932680
0.83997816
0.84006536
0.83991152
0.83982676
0.83977073
0.83989346
0.83997321
0.83984083
0.83969331
0.83959067
0.83955467
0.83955908
0.83950269
0.83951741
0.83956486
INFO - Training [63][  180/  196]   Loss 0.275896   Top1 90.321181   Top5 99.082031   BatchTime 0.369138   LR 0.000070
0.83952421
0.83944690
0.83929884
0.83921540
0.83920342
0.83934844
0.83943349
0.83909565
0.83918828
0.83907735
0.83906960
0.83908278
0.83908623
0.83917272
0.83906609
INFO - ==> Top1: 90.320    Top5: 99.076    Loss: 0.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83921814
0.83919197
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [63][   20/   40]   Loss 0.430152   Top1 87.089844   Top5 99.257812   BatchTime 0.133147
INFO - Validation [63][   40/   40]   Loss 0.428400   Top1 87.000000   Top5 99.370000   BatchTime 0.094489
INFO - ==> Top1: 87.000    Top5: 99.370    Loss: 0.428
INFO - ==> Sparsity : 0.346
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0360)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0639)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0189)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0788)
features.5.conv.0 tensor(0.0391)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.0897)
features.6.conv.0 tensor(0.0244)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0521)
features.7.conv.0 tensor(0.0308)
features.7.conv.3 tensor(0.1042)
features.7.conv.6 tensor(0.0920)
features.8.conv.0 tensor(0.0615)
features.8.conv.3 tensor(0.1108)
features.8.conv.6 tensor(0.1296)
features.9.conv.0 tensor(0.0641)
features.9.conv.3 tensor(0.1623)
features.9.conv.6 tensor(0.0916)
features.10.conv.0 tensor(0.0382)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0674)
features.11.conv.0 tensor(0.1027)
features.11.conv.3 tensor(0.1549)
features.11.conv.6 tensor(0.1606)
features.12.conv.0 tensor(0.1325)
features.12.conv.3 tensor(0.1277)
features.12.conv.6 tensor(0.1903)
features.13.conv.0 tensor(0.0581)
features.13.conv.3 tensor(0.1792)
features.13.conv.6 tensor(0.0798)
features.14.conv.0 tensor(0.8361)
features.14.conv.3 tensor(0.0943)
features.14.conv.6 tensor(0.9642)
features.15.conv.0 tensor(0.9150)
features.15.conv.3 tensor(0.0906)
features.15.conv.6 tensor(0.9446)
features.16.conv.0 tensor(0.0758)
features.16.conv.3 tensor(0.1286)
features.16.conv.6 tensor(0.2020)
conv.0 tensor(0.1309)
tensor(757682.) 2188896.0
0.83910906
0.83897942
0.83900017
0.83892459
0.83886212
0.83898818
0.83902723
0.83898944
0.83874714
0.83866930
0.83867019
0.83872795
0.83867580
0.83871549
0.83862162
0.83845294
0.83842593
0.83845162
INFO - Training [64][   20/  196]   Loss 0.284390   Top1 89.902344   Top5 98.574219   BatchTime 0.371712   LR 0.000066
0.83827513
0.83826500
0.83818436
0.83800179
0.83805937
0.83820099
0.83802086
0.83785313
0.83789200
0.83785319
0.83771175
0.83760184
0.83756000
0.83757210
0.83750683
0.83735931
0.83726031
0.83721799
0.83718765
0.83712852
0.83708811
0.83704334
0.83702022
INFO - Training [64][   40/  196]   Loss 0.293832   Top1 89.736328   Top5 98.769531   BatchTime 0.365640   LR 0.000064
0.83702099
0.83704412
0.83711982
0.83727455
0.83718681
0.83719099
0.83728546
0.83727735
0.83727872
0.83735210
0.83737791
0.83743048
0.83758724
0.83761179
0.83772033
0.83783907
INFO - Training [64][   60/  196]   Loss 0.291221   Top1 89.791667   Top5 98.776042   BatchTime 0.368465   LR 0.000062
0.83800119
0.83805335
0.83800137
0.83785659
0.83772224
0.83758384
0.83748478
0.83751816
0.83759266
0.83771235
0.83764404
0.83781064
0.83787996
0.83778602
0.83752716
0.83742291
0.83742195
0.83746642
0.83767509
0.83771181
0.83756441
0.83746499
INFO - Training [64][   80/  196]   Loss 0.286701   Top1 90.009766   Top5 98.911133   BatchTime 0.366845   LR 0.000059
0.83739048
0.83732677
0.83730626
0.83727199
0.83719468
0.83718121
0.83700877
0.83705091
0.83699071
0.83693630
0.83694041
0.83685726
0.83674914
0.83655792
0.83650649
0.83648211
0.83642578
0.83617133
0.83600193
0.83580625
0.83588839
INFO - Training [64][  100/  196]   Loss 0.280816   Top1 90.175781   Top5 98.960938   BatchTime 0.369714   LR 0.000057
0.83596641
0.83585751
0.83576101
0.83562636
0.83537233
0.83510005
0.83502394
0.83489418
0.83473724
0.83460397
0.83455694
0.83458340
0.83455610
0.83448744
0.83444124
0.83444065
0.83440334
0.83434087
0.83430266
0.83423918
0.83423167
INFO - Training [64][  120/  196]   Loss 0.273444   Top1 90.491536   Top5 99.039714   BatchTime 0.371700   LR 0.000055
0.83417982
0.83408141
0.83376122
0.83325523
0.83306271
0.83300668
0.83296621
0.83295721
0.83298421
0.83293134
0.83291739
0.83291554
0.83293664
0.83290917
0.83291870
0.83290505
INFO - Training [64][  140/  196]   Loss 0.271727   Top1 90.588728   Top5 99.101562   BatchTime 0.371456   LR 0.000053
0.83288801
0.83288717
0.83290970
0.83290112
0.83286887
0.83289129
0.83286089
0.83285522
0.83285195
0.83283138
0.83282340
0.83282793
0.83283514
0.83283585
0.83282620
0.83283663
0.83283854
0.83282799
0.83280551
0.83280915
0.83279008
0.83278418
0.83278263
INFO - Training [64][  160/  196]   Loss 0.274713   Top1 90.454102   Top5 99.106445   BatchTime 0.369337   LR 0.000051
0.83276600
0.83276445
0.83275414
0.83274537
0.83276695
0.83276939
0.83275425
0.83276898
0.83276469
0.83275574
0.83273387
0.83269936
0.83263558
0.83255816
0.83248115
0.83240581
0.83234459
0.83232373
0.83231145
0.83229572
0.83230066
INFO - Training [64][  180/  196]   Loss 0.275432   Top1 90.414497   Top5 99.105903   BatchTime 0.369350   LR 0.000049
0.83231932
0.83229536
0.83224434
0.83220536
0.83215636
0.83213174
0.83209705
0.83207488
0.83204955
0.83203453
0.83203810
0.83204573
0.83205479
0.83203924
0.83203357
********************pre-trained*****************
INFO - ==> Top1: 90.508    Top5: 99.074    Loss: 0.273
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [64][   20/   40]   Loss 0.302811   Top1 90.742188   Top5 99.667969   BatchTime 0.139372
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0179)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0654)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0186)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0793)
features.5.conv.0 tensor(0.0397)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.0908)
features.6.conv.0 tensor(0.0244)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0523)
features.7.conv.0 tensor(0.0314)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.0934)
features.8.conv.0 tensor(0.0626)
features.8.conv.3 tensor(0.1120)
features.8.conv.6 tensor(0.1322)
features.9.conv.0 tensor(0.0646)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.0923)
features.10.conv.0 tensor(0.0387)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0674)
features.11.conv.0 tensor(0.1049)
features.11.conv.3 tensor(0.1553)
features.11.conv.6 tensor(0.1604)
features.12.conv.0 tensor(0.1333)
features.12.conv.3 tensor(0.1271)
features.12.conv.6 tensor(0.1909)
features.13.conv.0 tensor(0.0586)
features.13.conv.3 tensor(0.1784)
features.13.conv.6 tensor(0.0810)
features.14.conv.0 tensor(0.8365)
features.14.conv.3 tensor(0.0928)
features.14.conv.6 tensor(0.9639)
features.15.conv.0 tensor(0.9164)
features.15.conv.3 tensor(0.0911)
features.15.conv.6 tensor(0.9442)
features.16.conv.0 tensor(0.0758)
features.16.conv.3 tensor(0.1294)
features.16.conv.6 tensor(0.2011)
conv.0 tensor(0.1308)
tensor(758129.) 2188896.0
INFO - Validation [64][   40/   40]   Loss 0.291883   Top1 90.970000   Top5 99.700000   BatchTime 0.098698
INFO - ==> Top1: 90.970    Top5: 99.700    Loss: 0.292
INFO - ==> Sparsity : 0.346
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
0.83208233
0.83218384
0.83406407
0.83486539
0.83480203
0.83483368
0.83482033
0.83479273
0.83476216
0.83474475
0.83474267
0.83472836
0.83471245
0.83470875
0.83471006
0.83469349
0.83472162
0.83473116
0.83471638
0.83469975
INFO - Training [65][   20/  196]   Loss 0.298870   Top1 89.199219   Top5 98.574219   BatchTime 0.406962   LR 0.000046
0.83470690
0.83468693
0.83468300
0.83468550
0.83469087
0.83468068
0.83467197
0.83468777
0.83469188
0.83467776
0.83469397
0.83469123
0.83467394
0.83466953
0.83465511
0.83464688
0.83466154
0.83465832
0.83464640
0.83465523
0.83465379
0.83463061
INFO - Training [65][   40/  196]   Loss 0.293214   Top1 89.599609   Top5 98.701172   BatchTime 0.389414   LR 0.000044
0.83463484
0.83464527
0.83463436
0.83463717
0.83463335
0.83462578
0.83463103
0.83463049
0.83465719
0.83464205
0.83463532
0.83464295
0.83465755
0.83465099
0.83463395
0.83464712
INFO - Training [65][   60/  196]   Loss 0.293726   Top1 89.615885   Top5 98.808594   BatchTime 0.384703   LR 0.000042
0.83462763
0.83463579
0.83463037
0.83461785
0.83461440
0.83461243
0.83459944
0.83460355
0.83461696
0.83462721
0.83464253
0.83463275
0.83464074
0.83467931
0.83468646
0.83470637
0.83473456
0.83475906
0.83480179
0.83484238
0.83491004
INFO - Training [65][   80/  196]   Loss 0.286982   Top1 89.838867   Top5 98.999023   BatchTime 0.380200   LR 0.000040
0.83494407
0.83498424
0.83506256
0.83500665
0.83489531
0.83483130
0.83486915
0.83488250
0.83482409
0.83478004
0.83474565
0.83473021
0.83474094
0.83472252
0.83469164
0.83467919
0.83466840
0.83465058
0.83466429
0.83463603
0.83462507
INFO - Training [65][  100/  196]   Loss 0.277590   Top1 90.214844   Top5 99.054688   BatchTime 0.382847   LR 0.000039
0.83461386
0.83460456
0.83458161
0.83459407
0.83458632
0.83458227
0.83459586
0.83458149
0.83457369
0.83455145
0.83455497
0.83456904
0.83455896
0.83454156
0.83452749
0.83451539
0.83450723
0.83450824
0.83451200
0.83450377
0.83450478
0.83451682
INFO - Training [65][  120/  196]   Loss 0.271830   Top1 90.406901   Top5 99.098307   BatchTime 0.379850   LR 0.000037
0.83450192
0.83449769
0.83449274
0.83449030
0.83447695
0.83447558
0.83448315
0.83446747
0.83446503
0.83446145
0.83445090
0.83446020
0.83444291
0.83443749
0.83443707
0.83443660
INFO - Training [65][  140/  196]   Loss 0.271048   Top1 90.407366   Top5 99.148996   BatchTime 0.378125   LR 0.000035
0.83442354
0.83442855
0.83441329
0.83440208
0.83439201
0.83439553
0.83439696
0.83440119
0.83440489
0.83440018
0.83441222
0.83440936
0.83439827
0.83440959
0.83442038
0.83442563
0.83442205
0.83441049
0.83439904
0.83440036
0.83441049
0.83441085
INFO - Training [65][  160/  196]   Loss 0.271534   Top1 90.397949   Top5 99.133301   BatchTime 0.377539   LR 0.000033
0.83439022
0.83439893
0.83441228
0.83440936
0.83440244
0.83440489
0.83441454
0.83440936
0.83438426
0.83438730
0.83436847
0.83437532
0.83437556
0.83436161
0.83436102
0.83436686
0.83436596
0.83436531
0.83436608
0.83438957
0.83437634
INFO - Training [65][  180/  196]   Loss 0.271804   Top1 90.366753   Top5 99.066840   BatchTime 0.376623   LR 0.000032
0.83437461
0.83435732
0.83436060
0.83436030
0.83434743
0.83435780
0.83435607
0.83433414
0.83434576
0.83434588
0.83434689
0.83433080
0.83433378
0.83433062
0.83433777
********************pre-trained*****************
INFO - ==> Top1: 90.426    Top5: 99.062    Loss: 0.270
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [65][   20/   40]   Loss 0.355743   Top1 89.394531   Top5 99.492188   BatchTime 0.135125
features.0.conv.0 tensor(0.4132)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0179)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0654)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0190)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0798)
features.5.conv.0 tensor(0.0400)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.0905)
features.6.conv.0 tensor(0.0241)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0319)
features.7.conv.3 tensor(0.1033)
features.7.conv.6 tensor(0.0924)
features.8.conv.0 tensor(0.0627)
features.8.conv.3 tensor(0.1114)
features.8.conv.6 tensor(0.1298)
features.9.conv.0 tensor(0.0654)
features.9.conv.3 tensor(0.1617)
features.9.conv.6 tensor(0.0928)
features.10.conv.0 tensor(0.0393)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0673)
features.11.conv.0 tensor(0.1049)
features.11.conv.3 tensor(0.1537)
features.11.conv.6 tensor(0.1663)
features.12.conv.0 tensor(0.1337)
features.12.conv.3 tensor(0.1279)
features.12.conv.6 tensor(0.1926)
features.13.conv.0 tensor(0.0587)
features.13.conv.3 tensor(0.1782)
features.13.conv.6 tensor(0.0815)
features.14.conv.0 tensor(0.8398)
features.14.conv.3 tensor(0.0927)
features.14.conv.6 tensor(0.9631)
features.15.conv.0 tensor(0.9173)
features.15.conv.3 tensor(0.0911)
features.15.conv.6 tensor(0.9418)
features.16.conv.0 tensor(0.0756)
features.16.conv.3 tensor(0.1289)
features.16.conv.6 tensor(0.2011)
conv.0 tensor(0.1312)
tensor(758832.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.349852   Top1 89.260000   Top5 99.560000   BatchTime 0.093991
INFO - ==> Top1: 89.260    Top5: 99.560    Loss: 0.350
INFO - ==> Sparsity : 0.347
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
0.83434981
0.83433646
0.83431900
0.83430815
0.83430851
0.83433139
0.83432782
0.83432490
0.83433110
0.83433867
0.83433652
0.83433473
0.83433497
0.83433694
0.83432186
0.83432984
0.83432001
0.83431208
0.83432579
0.83432245
INFO - Training [66][   20/  196]   Loss 0.301373   Top1 89.453125   Top5 98.671875   BatchTime 0.414711   LR 0.000029
0.83431822
0.83433086
0.83433712
0.83433646
0.83433318
0.83435196
0.83434510
0.83434486
0.83433825
0.83433461
0.83434170
0.83433545
0.83433354
0.83434445
0.83434117
0.83433610
0.83433515
0.83434039
0.83430594
0.83430260
0.83431458
0.83429772
INFO - Training [66][   40/  196]   Loss 0.292480   Top1 89.833984   Top5 98.857422   BatchTime 0.388235   LR 0.000028
0.83429754
0.83428442
0.83427644
0.83428955
0.83429158
0.83429247
0.83427632
0.83426571
0.83425915
0.83425879
0.83424771
0.83424389
0.83425164
0.83426684
0.83425957
0.83425677
INFO - Training [66][   60/  196]   Loss 0.291517   Top1 89.759115   Top5 98.906250   BatchTime 0.382239   LR 0.000026
0.83427238
0.83429104
0.83427900
0.83427274
0.83428806
0.83430231
0.83429056
0.83427894
0.83428597
0.83429253
0.83427775
0.83427948
0.83430064
0.83429438
0.83427429
0.83428568
0.83428341
0.83427680
0.83429146
0.83429486
0.83428085
INFO - Training [66][   80/  196]   Loss 0.287105   Top1 89.882812   Top5 99.047852   BatchTime 0.384035   LR 0.000025
0.83427572
0.83428985
0.83429790
0.83428460
0.83427989
0.83429593
0.83430099
0.83429146
0.83428127
0.83427465
0.83427340
0.83427322
0.83426231
0.83427012
0.83428687
0.83427817
0.83426845
0.83427447
0.83426797
0.83425832
0.83426452
0.83425719
INFO - Training [66][  100/  196]   Loss 0.277928   Top1 90.207031   Top5 99.101562   BatchTime 0.380412   LR 0.000023
0.83423769
0.83425003
0.83424860
0.83424699
0.83422756
0.83422458
0.83422607
0.83422863
0.83423293
0.83421844
0.83422011
0.83422142
0.83422929
0.83422804
0.83422887
0.83422530
INFO - Training [66][  120/  196]   Loss 0.274043   Top1 90.338542   Top5 99.134115   BatchTime 0.378263   LR 0.000022
0.83422631
0.83422887
0.83421648
0.83421975
0.83422267
0.83420944
0.83423263
0.83423358
0.83424073
0.83424288
0.83424860
0.83425754
0.83423984
0.83421832
0.83422661
0.83424097
0.83423495
0.83425075
0.83424544
0.83424103
0.83423913
0.83424252
INFO - Training [66][  140/  196]   Loss 0.272878   Top1 90.404576   Top5 99.160156   BatchTime 0.376657   LR 0.000021
0.83424515
0.83425099
0.83424109
0.83423722
0.83423841
0.83423501
0.83423865
0.83423728
0.83423996
0.83422476
0.83421665
0.83421659
0.83421671
0.83422595
0.83421528
0.83422422
0.83422631
0.83421230
0.83422148
0.83421677
0.83420205
0.83421344
INFO - Training [66][  160/  196]   Loss 0.274700   Top1 90.273438   Top5 99.160156   BatchTime 0.375195   LR 0.000019
0.83421963
0.83423001
0.83421326
0.83421773
0.83422381
0.83422577
0.83424330
0.83423942
0.83423007
0.83423102
0.83423430
0.83424085
0.83425307
0.83423573
0.83422810
0.83423001
0.83423740
INFO - Training [66][  180/  196]   Loss 0.272938   Top1 90.345052   Top5 99.123264   BatchTime 0.372591   LR 0.000018
0.83422643
0.83420521
0.83421123
0.83421057
0.83420217
0.83421284
0.83421534
0.83420092
0.83419013
0.83419102
0.83419400
0.83419073
0.83419037
0.83418137
0.83418846
INFO - ==> Top1: 90.380    Top5: 99.112    Loss: 0.272
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83418530
0.83418733
0.83418214
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [66][   20/   40]   Loss 0.596077   Top1 82.421875   Top5 98.828125   BatchTime 0.181999
features.0.conv.0 tensor(0.4167)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0657)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0190)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0796)
features.5.conv.0 tensor(0.0400)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.0924)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0322)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.0922)
features.8.conv.0 tensor(0.0630)
features.8.conv.3 tensor(0.1128)
features.8.conv.6 tensor(0.1337)
features.9.conv.0 tensor(0.0647)
features.9.conv.3 tensor(0.1606)
features.9.conv.6 tensor(0.0939)
features.10.conv.0 tensor(0.0395)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0680)
features.11.conv.0 tensor(0.1050)
features.11.conv.3 tensor(0.1530)
features.11.conv.6 tensor(0.1762)
features.12.conv.0 tensor(0.1344)
features.12.conv.3 tensor(0.1281)
features.12.conv.6 tensor(0.1924)
features.13.conv.0 tensor(0.0586)
features.13.conv.3 tensor(0.1775)
features.13.conv.6 tensor(0.0828)
features.14.conv.0 tensor(0.8430)
features.14.conv.3 tensor(0.0926)
features.14.conv.6 tensor(0.9630)
features.15.conv.0 tensor(0.9175)
features.15.conv.3 tensor(0.0911)
features.15.conv.6 tensor(0.9422)
features.16.conv.0 tensor(0.0760)
features.16.conv.3 tensor(0.1286)
features.16.conv.6 tensor(0.2008)
conv.0 tensor(0.1312)
tensor(760191.) 2188896.0
INFO - Validation [66][   40/   40]   Loss 0.588999   Top1 82.190000   Top5 98.950000   BatchTime 0.126455
INFO - ==> Top1: 82.190    Top5: 98.950    Loss: 0.589
INFO - ==> Sparsity : 0.347
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
0.83419412
0.83419532
0.83419013
0.83419502
0.83420181
0.83420068
0.83419800
0.83418268
0.83418691
0.83419025
0.83420002
0.83419818
0.83420330
0.83420879
0.83421838
0.83422261
0.83423340
0.83422977
INFO - Training [67][   20/  196]   Loss 0.295103   Top1 89.218750   Top5 98.789062   BatchTime 0.451416   LR 0.000016
0.83422464
0.83420926
0.83421624
0.83420056
0.83421081
0.83418709
0.83420235
0.83420801
0.83419573
0.83418232
0.83417547
0.83419251
0.83419818
0.83418691
0.83417779
0.83418214
0.83419150
0.83418840
0.83418369
0.83418107
0.83418816
0.83417135
INFO - Training [67][   40/  196]   Loss 0.291832   Top1 89.628906   Top5 98.876953   BatchTime 0.412561   LR 0.000015
0.83416963
0.83416957
0.83417314
0.83416802
0.83416855
0.83417231
0.83417028
0.83416206
0.83416986
0.83416975
0.83417189
0.83417267
0.83417207
0.83418107
0.83418721
0.83418059
0.83416927
0.83416826
0.83417886
0.83417648
INFO - Training [67][   60/  196]   Loss 0.284483   Top1 89.980469   Top5 98.906250   BatchTime 0.408407   LR 0.000014
0.83416975
0.83418131
0.83417660
0.83417749
0.83419061
0.83419669
0.83420038
0.83419776
0.83417428
0.83417588
0.83417094
0.83417195
0.83415902
0.83416402
0.83417678
0.83418250
0.83417970
0.83418030
0.83416957
0.83417434
0.83416981
INFO - Training [67][   80/  196]   Loss 0.282181   Top1 90.048828   Top5 99.023438   BatchTime 0.398579   LR 0.000013
0.83416629
0.83416736
0.83417308
0.83417559
0.83417398
0.83416939
0.83418012
0.83419061
0.83419013
0.83417189
0.83417737
0.83417493
0.83416730
0.83416796
0.83416593
0.83417118
0.83416164
INFO - Training [67][  100/  196]   Loss 0.276456   Top1 90.238281   Top5 99.078125   BatchTime 0.391847   LR 0.000012
0.83415681
0.83412832
0.83413887
0.83413666
0.83413810
0.83415198
0.83414090
0.83415276
0.83414829
0.83414495
0.83414102
0.83415526
0.83415210
0.83414990
0.83416051
0.83414650
0.83414626
0.83415234
0.83414871
0.83414811
0.83415908
INFO - Training [67][  120/  196]   Loss 0.269561   Top1 90.498047   Top5 99.147135   BatchTime 0.389719   LR 0.000011
0.83414751
0.83414966
0.83415711
0.83414853
0.83413875
0.83414930
0.83415091
0.83416134
0.83415788
0.83414328
0.83415323
0.83416659
0.83415681
0.83416575
0.83416730
0.83415437
0.83415633
0.83416134
0.83415574
0.83415848
0.83414727
INFO - Training [67][  140/  196]   Loss 0.267709   Top1 90.538504   Top5 99.215960   BatchTime 0.387160   LR 0.000010
0.83414561
0.83415347
0.83416200
0.83415562
0.83414841
0.83415753
0.83415502
0.83416009
0.83414227
0.83413333
0.83414268
0.83413988
0.83415622
0.83415371
0.83415502
0.83414978
0.83414227
0.83414662
0.83414179
0.83413506
0.83412427
0.83413410
INFO - Training [67][  160/  196]   Loss 0.270718   Top1 90.419922   Top5 99.177246   BatchTime 0.384718   LR 0.000009
0.83413357
0.83412790
0.83412510
0.83412081
0.83412474
0.83412498
0.83411992
0.83412755
0.83411604
0.83411545
0.83410567
0.83409268
0.83410722
0.83409739
0.83409697
0.83410531
INFO - Training [67][  180/  196]   Loss 0.270644   Top1 90.427517   Top5 99.144965   BatchTime 0.383444   LR 0.000008
0.83411252
0.83410239
0.83409113
0.83409202
0.83408988
0.83408010
0.83407986
0.83408427
0.83408999
0.83407390
0.83407605
0.83408278
0.83408231
0.83407897
0.83408111
0.83407974
0.83408582
0.83408117
INFO - ==> Top1: 90.498    Top5: 99.144    Loss: 0.269
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [67][   20/   40]   Loss 0.350536   Top1 89.765625   Top5 99.492188   BatchTime 0.138331
INFO - Validation [67][   40/   40]   Loss 0.337182   Top1 89.640000   Top5 99.620000   BatchTime 0.096082
INFO - ==> Top1: 89.640    Top5: 99.620    Loss: 0.337
INFO - ==> Sparsity : 0.347
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.1621)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0657)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0189)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0794)
features.5.conv.0 tensor(0.0400)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.0934)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0324)
features.7.conv.3 tensor(0.1039)
features.7.conv.6 tensor(0.0922)
features.8.conv.0 tensor(0.0631)
features.8.conv.3 tensor(0.1117)
features.8.conv.6 tensor(0.1327)
features.9.conv.0 tensor(0.0649)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.0939)
features.10.conv.0 tensor(0.0395)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0677)
features.11.conv.0 tensor(0.1051)
features.11.conv.3 tensor(0.1537)
features.11.conv.6 tensor(0.1732)
features.12.conv.0 tensor(0.1349)
features.12.conv.3 tensor(0.1281)
features.12.conv.6 tensor(0.1932)
features.13.conv.0 tensor(0.0589)
features.13.conv.3 tensor(0.1765)
features.13.conv.6 tensor(0.0817)
features.14.conv.0 tensor(0.8439)
features.14.conv.3 tensor(0.0926)
features.14.conv.6 tensor(0.9630)
features.15.conv.0 tensor(0.9179)
features.15.conv.3 tensor(0.0912)
features.15.conv.6 tensor(0.9423)
features.16.conv.0 tensor(0.0757)
features.16.conv.3 tensor(0.1291)
features.16.conv.6 tensor(0.2008)
conv.0 tensor(0.1311)
tensor(760156.) 2188896.0
0.83408505
0.83409256
0.83408844
0.83409965
0.83410388
0.83410317
0.83411163
0.83411658
0.83410794
0.83410084
0.83410800
0.83410555
0.83411461
0.83411366
0.83411157
0.83410364
0.83410531
0.83410126
0.83409083
0.83409035
0.83407938
INFO - Training [68][   20/  196]   Loss 0.278176   Top1 89.746094   Top5 98.710938   BatchTime 0.466732   LR 0.000007
0.83406842
0.83407491
0.83406514
0.83406383
0.83406651
0.83406568
0.83406919
0.83406574
0.83405483
0.83405882
0.83406317
0.83406645
0.83406460
0.83406240
0.83406752
0.83407056
0.83406043
0.83406836
0.83406186
0.83406597
0.83406866
INFO - Training [68][   40/  196]   Loss 0.287451   Top1 89.902344   Top5 98.818359   BatchTime 0.421481   LR 0.000006
0.83407426
0.83407295
0.83407885
0.83407837
0.83408052
0.83407259
0.83406800
0.83406204
0.83406907
0.83405721
0.83406019
0.83406907
0.83406907
0.83408350
0.83408040
0.83407557
0.83408231
INFO - Training [68][   60/  196]   Loss 0.282241   Top1 90.162760   Top5 98.854167   BatchTime 0.402820   LR 0.000006
0.83408964
0.83409303
0.83408833
0.83409238
0.83406425
0.83407390
0.83406925
0.83407909
0.83408767
0.83408719
0.83408886
0.83408397
0.83407277
0.83406693
0.83407634
0.83407640
0.83407718
0.83407748
0.83407742
0.83407515
0.83408278
INFO - Training [68][   80/  196]   Loss 0.278815   Top1 90.278320   Top5 98.964844   BatchTime 0.396287   LR 0.000005
0.83407974
0.83407062
0.83405870
0.83407044
0.83407718
0.83407795
0.83407587
0.83406991
0.83406216
0.83405948
0.83404434
0.83405071
0.83404595
0.83405119
0.83404517
0.83404714
0.83405542
0.83405584
0.83405238
0.83404315
0.83402997
INFO - Training [68][  100/  196]   Loss 0.272375   Top1 90.484375   Top5 99.000000   BatchTime 0.393399   LR 0.000004
0.83403164
0.83403689
0.83402681
0.83402032
0.83401167
0.83401698
0.83401221
0.83402842
0.83401990
0.83402026
0.83402157
0.83402270
0.83402467
0.83402598
0.83402675
0.83402294
INFO - Training [68][  120/  196]   Loss 0.267466   Top1 90.628255   Top5 99.072266   BatchTime 0.390101   LR 0.000004
0.83402193
0.83402002
0.83401543
0.83400619
0.83400995
0.83401847
0.83402908
0.83403409
0.83403748
0.83404028
0.83403927
0.83405203
0.83404464
0.83404785
0.83404279
0.83404171
0.83404464
0.83403903
0.83402830
0.83404785
0.83403760
0.83403432
INFO - Training [68][  140/  196]   Loss 0.266989   Top1 90.678013   Top5 99.107143   BatchTime 0.385970   LR 0.000003
0.83403951
0.83403963
0.83404052
0.83403701
0.83404660
0.83404577
0.83404791
0.83404922
0.83404607
0.83404773
0.83404094
0.83404398
0.83403254
0.83402765
0.83401817
0.83402073
0.83402401
0.83402801
0.83403176
0.83402860
0.83402991
0.83402812
INFO - Training [68][  160/  196]   Loss 0.270806   Top1 90.566406   Top5 99.104004   BatchTime 0.383116   LR 0.000003
0.83402634
0.83402282
0.83401966
0.83401853
0.83402288
0.83402127
0.83402210
0.83402747
0.83402252
0.83401936
0.83402014
0.83403081
0.83402568
0.83402389
0.83402365
0.83402342
0.83401501
0.83402789
0.83402973
INFO - Training [68][  180/  196]   Loss 0.271616   Top1 90.512153   Top5 99.088542   BatchTime 0.376733   LR 0.000002
0.83401871
0.83402097
0.83401155
0.83401310
0.83401197
0.83401853
0.83401847
0.83403450
0.83402687
0.83403242
0.83402365
0.83400905
0.83401829
0.83402050
0.83402604
0.83402753
********************pre-trained*****************
INFO - ==> Top1: 90.514    Top5: 99.092    Loss: 0.271
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [68][   20/   40]   Loss 0.402186   Top1 87.558594   Top5 99.433594   BatchTime 0.139539
features.0.conv.0 tensor(0.4132)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0654)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0189)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0796)
features.5.conv.0 tensor(0.0400)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.0936)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0519)
features.7.conv.0 tensor(0.0325)
features.7.conv.3 tensor(0.1036)
features.7.conv.6 tensor(0.0921)
features.8.conv.0 tensor(0.0630)
features.8.conv.3 tensor(0.1126)
features.8.conv.6 tensor(0.1338)
features.9.conv.0 tensor(0.0648)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.0940)
features.10.conv.0 tensor(0.0393)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0678)
features.11.conv.0 tensor(0.1052)
features.11.conv.3 tensor(0.1534)
features.11.conv.6 tensor(0.1757)
features.12.conv.0 tensor(0.1349)
features.12.conv.3 tensor(0.1273)
features.12.conv.6 tensor(0.1931)
features.13.conv.0 tensor(0.0587)
features.13.conv.3 tensor(0.1773)
features.13.conv.6 tensor(0.0833)
features.14.conv.0 tensor(0.8436)
features.14.conv.3 tensor(0.0925)
features.14.conv.6 tensor(0.9629)
features.15.conv.0 tensor(0.9178)
features.15.conv.3 tensor(0.0907)
features.15.conv.6 tensor(0.9418)
features.16.conv.0 tensor(0.0757)
features.16.conv.3 tensor(0.1286)
features.16.conv.6 tensor(0.2009)
conv.0 tensor(0.1311)
tensor(760312.) 2188896.0
INFO - Validation [68][   40/   40]   Loss 0.394141   Top1 87.670000   Top5 99.510000   BatchTime 0.099256
INFO - ==> Top1: 87.670    Top5: 99.510    Loss: 0.394
INFO - ==> Sparsity : 0.347
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.100   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
0.83403599
0.83403438
0.83403134
0.83403337
0.83403218
0.83403081
0.83403260
0.83403611
0.83402914
0.83404136
0.83403403
0.83403713
0.83404344
0.83403343
0.83404088
0.83404011
0.83404058
0.83404845
0.83403105
0.83403462
0.83403063
0.83402181
INFO - Training [69][   20/  196]   Loss 0.287560   Top1 89.531250   Top5 98.808594   BatchTime 0.475154   LR 0.000002
0.83402365
0.83402210
0.83402812
0.83403236
0.83402079
0.83402526
0.83402020
0.83402097
0.83401710
0.83400363
0.83400929
0.83400923
0.83401000
0.83401364
0.83401543
0.83400798
INFO - Training [69][   40/  196]   Loss 0.287938   Top1 89.677734   Top5 98.867188   BatchTime 0.423881   LR 0.000001
0.83399886
0.83401483
0.83401704
0.83400726
0.83400601
0.83400011
0.83401477
0.83401203
0.83400923
0.83400494
0.83400673
0.83401346
0.83402216
0.83402497
0.83402419
0.83402330
0.83401978
0.83402210
0.83402014
0.83402693
0.83401692
0.83400792
INFO - Training [69][   60/  196]   Loss 0.278816   Top1 90.162760   Top5 98.997396   BatchTime 0.405396   LR 0.000001
0.83401042
0.83400649
0.83401525
0.83401966
0.83401996
0.83402717
0.83402479
0.83403254
0.83403832
0.83403176
0.83402681
0.83401805
0.83402717
0.83402693
0.83401179
0.83402175
0.83402419
0.83402437
0.83403462
0.83403069
0.83403277
INFO - Training [69][   80/  196]   Loss 0.275554   Top1 90.322266   Top5 99.091797   BatchTime 0.400879   LR 0.000001
0.83402979
0.83403182
0.83403653
0.83403522
0.83403575
0.83403146
0.83402210
0.83401680
0.83402187
0.83402085
0.83401638
0.83400524
0.83400959
0.83401364
0.83400601
0.83400971
INFO - Training [69][  100/  196]   Loss 0.269761   Top1 90.558594   Top5 99.105469   BatchTime 0.393642   LR 0.000000
0.83400118
0.83401024
0.83401394
0.83401179
0.83400661
0.83400714
0.83400333
0.83401161
0.83400035
0.83399761
0.83400065
0.83399951
0.83400875
0.83400011
0.83399606
0.83399189
0.83400548
0.83401173
0.83400154
0.83399487
0.83400148
0.83399677
INFO - Training [69][  120/  196]   Loss 0.263866   Top1 90.758464   Top5 99.150391   BatchTime 0.389457   LR 0.000000
0.83400548
0.83401477
0.83401358
0.83400017
0.83400220
0.83401388
0.83401901
0.83400637
0.83400762
0.83401293
0.83401722
0.83401656
0.83401662
0.83402038
0.83402914
0.83401614
0.83402699
0.83402252
0.83402616
0.83401960
0.83402079
INFO - Training [69][  140/  196]   Loss 0.262705   Top1 90.828683   Top5 99.190848   BatchTime 0.387358   LR 0.000000
0.83402950
0.83402801
0.83402544
0.83403367
0.83403587
0.83403927
0.83405268
0.83404469
0.83404565
0.83404922
0.83405179
0.83404589
0.83404493
0.83402908
0.83402985
0.83403713
0.83402634
0.83403432
0.83404183
0.83403474
0.83403689
0.83402610
0.83402139
INFO - Training [69][  160/  196]   Loss 0.266549   Top1 90.727539   Top5 99.187012   BatchTime 0.384390   LR 0.000000
0.83401495
0.83402574
0.83404237
0.83403075
0.83403420
0.83402592
0.83403283
0.83402938
0.83402663
0.83401370
0.83400488
0.83401036
0.83401865
0.83401489
0.83401018
0.83400917
0.83399856
0.83400202
0.83400130
0.83400255
INFO - Training [69][  180/  196]   Loss 0.266322   Top1 90.724826   Top5 99.153646   BatchTime 0.375161   LR 0.000000
0.83400357
0.83400053
0.83399940
0.83399791
0.83400446
0.83400774
0.83400494
0.83399862
0.83400232
0.83399647
0.83400369
0.83399975
0.83400422
********************pre-trained*****************
INFO - ==> Top1: 90.730    Top5: 99.150    Loss: 0.266
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [69][   20/   40]   Loss 0.296965   Top1 91.328125   Top5 99.746094   BatchTime 0.219103
INFO - Validation [69][   40/   40]   Loss 0.281931   Top1 91.480000   Top5 99.760000   BatchTime 0.160823
INFO - ==> Top1: 91.480    Top5: 99.760    Loss: 0.282
INFO - ==> Sparsity : 0.347
INFO - Scoreboard best 1 ==> Epoch [69][Top1: 91.480   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 91.330   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [60][Top1: 91.230   Top5: 99.800]
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0657)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0189)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0796)
features.5.conv.0 tensor(0.0400)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.0936)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0519)
features.7.conv.0 tensor(0.0324)
features.7.conv.3 tensor(0.1039)
features.7.conv.6 tensor(0.0922)
features.8.conv.0 tensor(0.0632)
features.8.conv.3 tensor(0.1140)
features.8.conv.6 tensor(0.1338)
features.9.conv.0 tensor(0.0649)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.0940)
features.10.conv.0 tensor(0.0394)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0679)
features.11.conv.0 tensor(0.1052)
features.11.conv.3 tensor(0.1532)
features.11.conv.6 tensor(0.1753)
features.12.conv.0 tensor(0.1351)
features.12.conv.3 tensor(0.1269)
features.12.conv.6 tensor(0.1931)
features.13.conv.0 tensor(0.0587)
features.13.conv.3 tensor(0.1773)
features.13.conv.6 tensor(0.0833)
features.14.conv.0 tensor(0.8439)
features.14.conv.3 tensor(0.0926)
features.14.conv.6 tensor(0.9628)
features.15.conv.0 tensor(0.9180)
features.15.conv.3 tensor(0.0910)
features.15.conv.6 tensor(0.9422)
features.16.conv.0 tensor(0.0758)
features.16.conv.3 tensor(0.1295)
features.16.conv.6 tensor(0.2008)
conv.0 tensor(0.1312)
tensor(760499.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [   20/   40]   Loss 0.296965   Top1 91.328125   Top5 99.746094   BatchTime 0.131434
*************hard_pruning_mode*******************
INFO - Validation [   40/   40]   Loss 0.281931   Top1 91.480000   Top5 99.760000   BatchTime 0.093978
INFO - ==> Top1: 91.480    Top5: 99.760    Loss: 0.282
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [0][   20/  196]   Loss 0.509814   Top1 82.675781   Top5 97.734375   BatchTime 0.419695   LR 0.004999
INFO - Training [0][   40/  196]   Loss 0.529756   Top1 82.031250   Top5 97.841797   BatchTime 0.380801   LR 0.004995
INFO - Training [0][   60/  196]   Loss 0.522994   Top1 82.180990   Top5 97.910156   BatchTime 0.370300   LR 0.004989
INFO - Training [0][   80/  196]   Loss 0.519268   Top1 82.255859   Top5 98.041992   BatchTime 0.364097   LR 0.004980
INFO - Training [0][  100/  196]   Loss 0.512522   Top1 82.449219   Top5 98.121094   BatchTime 0.359674   LR 0.004968
INFO - Training [0][  120/  196]   Loss 0.508112   Top1 82.548828   Top5 98.203125   BatchTime 0.351741   LR 0.004954
INFO - Training [0][  140/  196]   Loss 0.511524   Top1 82.368862   Top5 98.256138   BatchTime 0.339930   LR 0.004938
INFO - Training [0][  160/  196]   Loss 0.516164   Top1 82.221680   Top5 98.229980   BatchTime 0.338828   LR 0.004919
INFO - Training [0][  180/  196]   Loss 0.518519   Top1 82.105035   Top5 98.140191   BatchTime 0.338775   LR 0.004897
INFO - ==> Top1: 82.074    Top5: 98.150    Loss: 0.519
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [0][   20/   40]   Loss 0.514612   Top1 83.066406   Top5 99.199219   BatchTime 0.215284
INFO - Validation [0][   40/   40]   Loss 0.510492   Top1 83.220000   Top5 99.290000   BatchTime 0.133293
INFO - ==> Top1: 83.220    Top5: 99.290    Loss: 0.510
INFO - ==> Sparsity : 0.369
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 83.220   Top5: 99.290]
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0961)
features.1.conv.6 tensor(0.0382)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0608)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0319)
features.4.conv.0 tensor(0.0202)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0758)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.0990)
features.6.conv.0 tensor(0.0285)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0520)
features.7.conv.0 tensor(0.0364)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.0991)
features.8.conv.0 tensor(0.0627)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.1919)
features.9.conv.0 tensor(0.0680)
features.9.conv.3 tensor(0.1652)
features.9.conv.6 tensor(0.1112)
features.10.conv.0 tensor(0.0418)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0747)
features.11.conv.0 tensor(0.1294)
features.11.conv.3 tensor(0.1755)
features.11.conv.6 tensor(0.2250)
features.12.conv.0 tensor(0.1575)
features.12.conv.3 tensor(0.1534)
features.12.conv.6 tensor(0.2453)
features.13.conv.0 tensor(0.0798)
features.13.conv.3 tensor(0.1889)
features.13.conv.6 tensor(0.1100)
features.14.conv.0 tensor(0.8748)
features.14.conv.3 tensor(0.1133)
features.14.conv.6 tensor(0.9755)
features.15.conv.0 tensor(0.9410)
features.15.conv.3 tensor(0.1117)
features.15.conv.6 tensor(0.9709)
features.16.conv.0 tensor(0.0732)
features.16.conv.3 tensor(0.1663)
features.16.conv.6 tensor(0.1826)
conv.0 tensor(0.1858)
tensor(806929.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [1][   20/  196]   Loss 0.556592   Top1 80.781250   Top5 97.070312   BatchTime 0.412023   LR 0.004853
INFO - Training [1][   40/  196]   Loss 0.544163   Top1 81.416016   Top5 97.529297   BatchTime 0.377475   LR 0.004825
INFO - Training [1][   60/  196]   Loss 0.544920   Top1 81.549479   Top5 97.675781   BatchTime 0.368542   LR 0.004794
INFO - Training [1][   80/  196]   Loss 0.545180   Top1 81.396484   Top5 97.851562   BatchTime 0.361533   LR 0.004761
INFO - Training [1][  100/  196]   Loss 0.533951   Top1 81.738281   Top5 97.980469   BatchTime 0.352492   LR 0.004725
INFO - Training [1][  120/  196]   Loss 0.527134   Top1 81.910807   Top5 98.072917   BatchTime 0.339399   LR 0.004687
INFO - Training [1][  140/  196]   Loss 0.524866   Top1 81.978237   Top5 98.152902   BatchTime 0.327444   LR 0.004647
INFO - Training [1][  160/  196]   Loss 0.526782   Top1 81.923828   Top5 98.164062   BatchTime 0.326610   LR 0.004605
INFO - Training [1][  180/  196]   Loss 0.526249   Top1 81.881510   Top5 98.151042   BatchTime 0.330089   LR 0.004560
INFO - ==> Top1: 81.956    Top5: 98.160    Loss: 0.525
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.440961   Top1 85.585938   Top5 99.511719   BatchTime 0.132870
features.0.conv.0 tensor(0.4340)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0119)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0564)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0386)
features.3.conv.6 tensor(0.0321)
features.4.conv.0 tensor(0.0229)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0796)
features.5.conv.0 tensor(0.0314)
features.5.conv.3 tensor(0.0810)
features.5.conv.6 tensor(0.1060)
features.6.conv.0 tensor(0.0278)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0538)
features.7.conv.0 tensor(0.0410)
features.7.conv.3 tensor(0.1224)
features.7.conv.6 tensor(0.0957)
features.8.conv.0 tensor(0.0752)
features.8.conv.3 tensor(0.1250)
features.8.conv.6 tensor(0.2315)
features.9.conv.0 tensor(0.0645)
features.9.conv.3 tensor(0.1672)
features.9.conv.6 tensor(0.1219)
features.10.conv.0 tensor(0.0466)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0791)
features.11.conv.0 tensor(0.1398)
features.11.conv.3 tensor(0.1885)
features.11.conv.6 tensor(0.2460)
features.12.conv.0 tensor(0.1743)
features.12.conv.3 tensor(0.1580)
features.12.conv.6 tensor(0.2469)
features.13.conv.0 tensor(0.0940)
features.13.conv.3 tensor(0.1896)
features.13.conv.6 tensor(0.1067)
features.14.conv.0 tensor(0.8877)
features.14.conv.3 tensor(0.1272)
features.14.conv.6 tensor(0.9794)
features.15.conv.0 tensor(0.9483)
features.15.conv.3 tensor(0.1269)
features.15.conv.6 tensor(0.9610)
features.16.conv.0 tensor(0.0814)
features.16.conv.3 tensor(0.1774)
features.16.conv.6 tensor(0.2060)
conv.0 tensor(0.1309)
tensor(800597.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 0.433666   Top1 85.610000   Top5 99.580000   BatchTime 0.097317
INFO - ==> Top1: 85.610    Top5: 99.580    Loss: 0.434
INFO - ==> Sparsity : 0.366
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 85.610   Top5: 99.580]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 83.220   Top5: 99.290]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [2][   20/  196]   Loss 0.507390   Top1 82.656250   Top5 97.753906   BatchTime 0.427696   LR 0.004477
INFO - Training [2][   40/  196]   Loss 0.510347   Top1 82.578125   Top5 97.910156   BatchTime 0.387063   LR 0.004426
INFO - Training [2][   60/  196]   Loss 0.504793   Top1 82.858073   Top5 97.942708   BatchTime 0.370505   LR 0.004374
INFO - Training [2][   80/  196]   Loss 0.508064   Top1 82.705078   Top5 98.061523   BatchTime 0.364380   LR 0.004320
INFO - Training [2][  100/  196]   Loss 0.504502   Top1 82.765625   Top5 98.097656   BatchTime 0.358408   LR 0.004264
INFO - Training [2][  120/  196]   Loss 0.499583   Top1 82.897135   Top5 98.206380   BatchTime 0.343277   LR 0.004206
INFO - Training [2][  140/  196]   Loss 0.499215   Top1 82.921317   Top5 98.247768   BatchTime 0.331471   LR 0.004146
INFO - Training [2][  160/  196]   Loss 0.500572   Top1 82.839355   Top5 98.273926   BatchTime 0.332085   LR 0.004085
INFO - Training [2][  180/  196]   Loss 0.503540   Top1 82.706163   Top5 98.246528   BatchTime 0.333361   LR 0.004022
INFO - ==> Top1: 82.786    Top5: 98.234    Loss: 0.502
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.407038   Top1 86.914062   Top5 99.472656   BatchTime 0.168325
features.0.conv.0 tensor(0.4097)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0343)
features.2.conv.0 tensor(0.0205)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0590)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0254)
features.4.conv.3 tensor(0.0868)
features.4.conv.6 tensor(0.0744)
features.5.conv.0 tensor(0.0439)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.1187)
features.6.conv.0 tensor(0.0270)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0532)
features.7.conv.0 tensor(0.0312)
features.7.conv.3 tensor(0.1160)
features.7.conv.6 tensor(0.0944)
features.8.conv.0 tensor(0.0779)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.2537)
features.9.conv.0 tensor(0.0780)
features.9.conv.3 tensor(0.1681)
features.9.conv.6 tensor(0.1212)
features.10.conv.0 tensor(0.0465)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0798)
features.11.conv.0 tensor(0.1227)
features.11.conv.3 tensor(0.1914)
features.11.conv.6 tensor(0.2633)
features.12.conv.0 tensor(0.1747)
features.12.conv.3 tensor(0.1688)
features.12.conv.6 tensor(0.2492)
features.13.conv.0 tensor(0.0966)
features.13.conv.3 tensor(0.1968)
features.13.conv.6 tensor(0.1114)
features.14.conv.0 tensor(0.8931)
features.14.conv.3 tensor(0.1312)
features.14.conv.6 tensor(0.9791)
features.15.conv.0 tensor(0.9500)
features.15.conv.3 tensor(0.1263)
features.15.conv.6 tensor(0.9569)
features.16.conv.0 tensor(0.0945)
features.16.conv.3 tensor(0.1853)
features.16.conv.6 tensor(0.1968)
conv.0 tensor(0.1096)
tensor(793153.) 2188896.0
INFO - Validation [2][   40/   40]   Loss 0.392993   Top1 86.970000   Top5 99.560000   BatchTime 0.109160
INFO - ==> Top1: 86.970    Top5: 99.560    Loss: 0.393
INFO - ==> Sparsity : 0.362
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 86.970   Top5: 99.560]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 85.610   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 83.220   Top5: 99.290]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [3][   20/  196]   Loss 0.493719   Top1 82.500000   Top5 98.183594   BatchTime 0.428923   LR 0.003907
INFO - Training [3][   40/  196]   Loss 0.497120   Top1 82.656250   Top5 98.173828   BatchTime 0.383330   LR 0.003840
INFO - Training [3][   60/  196]   Loss 0.494389   Top1 82.910156   Top5 98.209635   BatchTime 0.369318   LR 0.003771
INFO - Training [3][   80/  196]   Loss 0.492513   Top1 83.066406   Top5 98.320312   BatchTime 0.361420   LR 0.003701
INFO - Training [3][  100/  196]   Loss 0.487056   Top1 83.214844   Top5 98.382812   BatchTime 0.357143   LR 0.003630
INFO - Training [3][  120/  196]   Loss 0.482263   Top1 83.404948   Top5 98.401693   BatchTime 0.343764   LR 0.003558
INFO - Training [3][  140/  196]   Loss 0.477981   Top1 83.510045   Top5 98.490513   BatchTime 0.337150   LR 0.003484
INFO - Training [3][  160/  196]   Loss 0.480311   Top1 83.417969   Top5 98.464355   BatchTime 0.328407   LR 0.003410
INFO - Training [3][  180/  196]   Loss 0.477564   Top1 83.519965   Top5 98.430990   BatchTime 0.330821   LR 0.003335
INFO - ==> Top1: 83.602    Top5: 98.430    Loss: 0.475
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.404385   Top1 86.875000   Top5 99.355469   BatchTime 0.199258
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0334)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0561)
features.3.conv.0 tensor(0.0246)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0336)
features.4.conv.0 tensor(0.0205)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0703)
features.5.conv.0 tensor(0.0444)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1281)
features.6.conv.0 tensor(0.0259)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0553)
features.7.conv.0 tensor(0.0391)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.0969)
features.8.conv.0 tensor(0.0875)
features.8.conv.3 tensor(0.1233)
features.8.conv.6 tensor(0.2716)
features.9.conv.0 tensor(0.0844)
features.9.conv.3 tensor(0.1727)
features.9.conv.6 tensor(0.1193)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.0792)
features.11.conv.0 tensor(0.1308)
features.11.conv.3 tensor(0.1935)
features.11.conv.6 tensor(0.2858)
features.12.conv.0 tensor(0.1833)
features.12.conv.3 tensor(0.1682)
features.12.conv.6 tensor(0.2596)
features.13.conv.0 tensor(0.1000)
features.13.conv.3 tensor(0.2006)
features.13.conv.6 tensor(0.1203)
features.14.conv.0 tensor(0.8948)
features.14.conv.3 tensor(0.1341)
features.14.conv.6 tensor(0.9800)
features.15.conv.0 tensor(0.9511)
features.15.conv.3 tensor(0.1275)
features.15.conv.6 tensor(0.9584)
features.16.conv.0 tensor(0.0972)
features.16.conv.3 tensor(0.1846)
features.16.conv.6 tensor(0.1959)
conv.0 tensor(0.1538)
tensor(817085.) 2188896.0
INFO - Validation [3][   40/   40]   Loss 0.388466   Top1 87.210000   Top5 99.530000   BatchTime 0.124518
INFO - ==> Top1: 87.210    Top5: 99.530    Loss: 0.388
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 87.210   Top5: 99.530]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 86.970   Top5: 99.560]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 85.610   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [4][   20/  196]   Loss 0.557534   Top1 79.941406   Top5 96.953125   BatchTime 0.430761   LR 0.003200
INFO - Training [4][   40/  196]   Loss 0.509108   Top1 82.158203   Top5 97.753906   BatchTime 0.384553   LR 0.003122
INFO - Training [4][   60/  196]   Loss 0.490109   Top1 82.805990   Top5 97.988281   BatchTime 0.371283   LR 0.003044
INFO - Training [4][   80/  196]   Loss 0.478864   Top1 83.247070   Top5 98.159180   BatchTime 0.363577   LR 0.002965
INFO - Training [4][  100/  196]   Loss 0.467453   Top1 83.707031   Top5 98.234375   BatchTime 0.360816   LR 0.002886
INFO - Training [4][  120/  196]   Loss 0.464810   Top1 83.815104   Top5 98.333333   BatchTime 0.357988   LR 0.002806
INFO - Training [4][  140/  196]   Loss 0.462108   Top1 83.911830   Top5 98.409598   BatchTime 0.344610   LR 0.002726
INFO - Training [4][  160/  196]   Loss 0.459014   Top1 83.994141   Top5 98.444824   BatchTime 0.344608   LR 0.002646
INFO - Training [4][  180/  196]   Loss 0.453413   Top1 84.179688   Top5 98.444010   BatchTime 0.344123   LR 0.002566
INFO - ==> Top1: 84.264    Top5: 98.472    Loss: 0.451
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.346948   Top1 88.691406   Top5 99.648438   BatchTime 0.133691
INFO - Validation [4][   40/   40]   Loss 0.337081   Top1 88.750000   Top5 99.660000   BatchTime 0.093965
INFO - ==> Top1: 88.750    Top5: 99.660    Loss: 0.337
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.750   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 87.210   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 86.970   Top5: 99.560]
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0365)
features.2.conv.0 tensor(0.0205)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0530)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0308)
features.4.conv.0 tensor(0.0256)
features.4.conv.3 tensor(0.0926)
features.4.conv.6 tensor(0.0692)
features.5.conv.0 tensor(0.0444)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.1320)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0543)
features.7.conv.0 tensor(0.0373)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.1002)
features.8.conv.0 tensor(0.0886)
features.8.conv.3 tensor(0.1238)
features.8.conv.6 tensor(0.2834)
features.9.conv.0 tensor(0.0844)
features.9.conv.3 tensor(0.1638)
features.9.conv.6 tensor(0.1226)
features.10.conv.0 tensor(0.0492)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0822)
features.11.conv.0 tensor(0.1333)
features.11.conv.3 tensor(0.1946)
features.11.conv.6 tensor(0.2893)
features.12.conv.0 tensor(0.1771)
features.12.conv.3 tensor(0.1701)
features.12.conv.6 tensor(0.2649)
features.13.conv.0 tensor(0.0988)
features.13.conv.3 tensor(0.1968)
features.13.conv.6 tensor(0.1218)
features.14.conv.0 tensor(0.8966)
features.14.conv.3 tensor(0.1370)
features.14.conv.6 tensor(0.9806)
features.15.conv.0 tensor(0.9534)
features.15.conv.3 tensor(0.1348)
features.15.conv.6 tensor(0.9538)
features.16.conv.0 tensor(0.1114)
features.16.conv.3 tensor(0.1863)
features.16.conv.6 tensor(0.2101)
conv.0 tensor(0.1508)
tensor(823497.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [5][   20/  196]   Loss 0.434711   Top1 85.234375   Top5 98.320312   BatchTime 0.428413   LR 0.002424
INFO - Training [5][   40/  196]   Loss 0.434188   Top1 84.980469   Top5 98.378906   BatchTime 0.381040   LR 0.002343
INFO - Training [5][   60/  196]   Loss 0.423975   Top1 85.351562   Top5 98.548177   BatchTime 0.369613   LR 0.002263
INFO - Training [5][   80/  196]   Loss 0.416832   Top1 85.629883   Top5 98.681641   BatchTime 0.363163   LR 0.002183
INFO - Training [5][  100/  196]   Loss 0.411178   Top1 85.785156   Top5 98.714844   BatchTime 0.355763   LR 0.002104
INFO - Training [5][  120/  196]   Loss 0.402338   Top1 86.064453   Top5 98.785807   BatchTime 0.342999   LR 0.002024
INFO - Training [5][  140/  196]   Loss 0.401321   Top1 86.040737   Top5 98.803013   BatchTime 0.341955   LR 0.001946
INFO - Training [5][  160/  196]   Loss 0.405084   Top1 85.910645   Top5 98.764648   BatchTime 0.341520   LR 0.001868
INFO - Training [5][  180/  196]   Loss 0.405875   Top1 85.883247   Top5 98.721788   BatchTime 0.344807   LR 0.001790
INFO - ==> Top1: 85.914    Top5: 98.718    Loss: 0.405
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.328266   Top1 89.082031   Top5 99.570312   BatchTime 0.130777
INFO - Validation [5][   40/   40]   Loss 0.311732   Top1 89.700000   Top5 99.660000   BatchTime 0.092891
INFO - ==> Top1: 89.700    Top5: 99.660    Loss: 0.312
INFO - ==> Sparsity : 0.375
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 89.700   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.750   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 87.210   Top5: 99.530]
features.0.conv.0 tensor(0.4062)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0321)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0432)
features.2.conv.6 tensor(0.0538)
features.3.conv.0 tensor(0.0205)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0236)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0675)
features.5.conv.0 tensor(0.0430)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.1385)
features.6.conv.0 tensor(0.0299)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0535)
features.7.conv.0 tensor(0.0361)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.0987)
features.8.conv.0 tensor(0.0872)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.2908)
features.9.conv.0 tensor(0.0897)
features.9.conv.3 tensor(0.1609)
features.9.conv.6 tensor(0.1290)
features.10.conv.0 tensor(0.0505)
features.10.conv.3 tensor(0.0975)
features.10.conv.6 tensor(0.0812)
features.11.conv.0 tensor(0.1366)
features.11.conv.3 tensor(0.1906)
features.11.conv.6 tensor(0.2894)
features.12.conv.0 tensor(0.1842)
features.12.conv.3 tensor(0.1684)
features.12.conv.6 tensor(0.2602)
features.13.conv.0 tensor(0.0945)
features.13.conv.3 tensor(0.1927)
features.13.conv.6 tensor(0.1305)
features.14.conv.0 tensor(0.8957)
features.14.conv.3 tensor(0.1377)
features.14.conv.6 tensor(0.9797)
features.15.conv.0 tensor(0.9538)
features.15.conv.3 tensor(0.1341)
features.15.conv.6 tensor(0.9607)
features.16.conv.0 tensor(0.1071)
features.16.conv.3 tensor(0.1899)
features.16.conv.6 tensor(0.2095)
conv.0 tensor(0.1390)
tensor(819804.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [6][   20/  196]   Loss 0.395720   Top1 85.917969   Top5 98.320312   BatchTime 0.422730   LR 0.001655
INFO - Training [6][   40/  196]   Loss 0.391925   Top1 86.005859   Top5 98.457031   BatchTime 0.389613   LR 0.001580
INFO - Training [6][   60/  196]   Loss 0.387348   Top1 86.217448   Top5 98.554688   BatchTime 0.373334   LR 0.001506
INFO - Training [6][   80/  196]   Loss 0.383176   Top1 86.391602   Top5 98.662109   BatchTime 0.360907   LR 0.001432
INFO - Training [6][  100/  196]   Loss 0.380734   Top1 86.566406   Top5 98.730469   BatchTime 0.346496   LR 0.001360
INFO - Training [6][  120/  196]   Loss 0.375145   Top1 86.829427   Top5 98.779297   BatchTime 0.345880   LR 0.001289
INFO - Training [6][  140/  196]   Loss 0.373687   Top1 86.886161   Top5 98.825335   BatchTime 0.346032   LR 0.001220
INFO - Training [6][  160/  196]   Loss 0.373370   Top1 86.901855   Top5 98.828125   BatchTime 0.345575   LR 0.001151
INFO - Training [6][  180/  196]   Loss 0.371689   Top1 86.883681   Top5 98.817274   BatchTime 0.345207   LR 0.001084
********************pre-trained*****************
INFO - ==> Top1: 86.940    Top5: 98.804    Loss: 0.371
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [6][   20/   40]   Loss 0.290795   Top1 90.351562   Top5 99.667969   BatchTime 0.132358
INFO - Validation [6][   40/   40]   Loss 0.278833   Top1 90.870000   Top5 99.710000   BatchTime 0.094062
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0098)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0343)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0530)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0293)
features.4.conv.0 tensor(0.0275)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.0664)
features.5.conv.0 tensor(0.0407)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.1414)
features.6.conv.0 tensor(0.0290)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0399)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.1003)
features.8.conv.0 tensor(0.0883)
features.8.conv.3 tensor(0.1250)
features.8.conv.6 tensor(0.2928)
features.9.conv.0 tensor(0.0894)
features.9.conv.3 tensor(0.1638)
features.9.conv.6 tensor(0.1321)
features.10.conv.0 tensor(0.0486)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.0813)
features.11.conv.0 tensor(0.1407)
features.11.conv.3 tensor(0.1937)
features.11.conv.6 tensor(0.2911)
features.12.conv.0 tensor(0.1852)
features.12.conv.3 tensor(0.1709)
features.12.conv.6 tensor(0.2581)
features.13.conv.0 tensor(0.0994)
features.13.conv.3 tensor(0.1958)
features.13.conv.6 tensor(0.1342)
features.14.conv.0 tensor(0.8967)
features.14.conv.3 tensor(0.1365)
features.14.conv.6 tensor(0.9797)
features.15.conv.0 tensor(0.9540)
features.15.conv.3 tensor(0.1353)
features.15.conv.6 tensor(0.9611)
features.16.conv.0 tensor(0.1071)
features.16.conv.3 tensor(0.1887)
features.16.conv.6 tensor(0.2009)
conv.0 tensor(0.1337)
tensor(816408.) 2188896.0
INFO - ==> Top1: 90.870    Top5: 99.710    Loss: 0.279
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.870   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 89.700   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.750   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [7][   20/  196]   Loss 0.387712   Top1 86.191406   Top5 97.988281   BatchTime 0.416714   LR 0.000969
INFO - Training [7][   40/  196]   Loss 0.376897   Top1 86.748047   Top5 98.339844   BatchTime 0.382135   LR 0.000907
INFO - Training [7][   60/  196]   Loss 0.373950   Top1 86.875000   Top5 98.450521   BatchTime 0.372310   LR 0.000845
INFO - Training [7][   80/  196]   Loss 0.363762   Top1 87.182617   Top5 98.623047   BatchTime 0.348590   LR 0.000786
INFO - Training [7][  100/  196]   Loss 0.356203   Top1 87.453125   Top5 98.714844   BatchTime 0.344027   LR 0.000728
INFO - Training [7][  120/  196]   Loss 0.349064   Top1 87.757161   Top5 98.834635   BatchTime 0.343618   LR 0.000673
INFO - Training [7][  140/  196]   Loss 0.345995   Top1 87.845982   Top5 98.900670   BatchTime 0.344663   LR 0.000619
INFO - Training [7][  160/  196]   Loss 0.345638   Top1 87.863770   Top5 98.903809   BatchTime 0.346345   LR 0.000567
INFO - Training [7][  180/  196]   Loss 0.344990   Top1 87.888455   Top5 98.893229   BatchTime 0.346045   LR 0.000517
INFO - ==> Top1: 87.970    Top5: 98.888    Loss: 0.343
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.303913   Top1 90.312500   Top5 99.707031   BatchTime 0.140904
INFO - Validation [7][   40/   40]   Loss 0.282415   Top1 90.920000   Top5 99.790000   BatchTime 0.097697
INFO - ==> Top1: 90.920    Top5: 99.790    Loss: 0.282
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 90.870   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 89.700   Top5: 99.660]
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.1211)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0492)
features.3.conv.0 tensor(0.0197)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0317)
features.4.conv.0 tensor(0.0229)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.0667)
features.5.conv.0 tensor(0.0409)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1434)
features.6.conv.0 tensor(0.0273)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0524)
features.7.conv.0 tensor(0.0390)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.1015)
features.8.conv.0 tensor(0.0868)
features.8.conv.3 tensor(0.1227)
features.8.conv.6 tensor(0.2939)
features.9.conv.0 tensor(0.0902)
features.9.conv.3 tensor(0.1626)
features.9.conv.6 tensor(0.1327)
features.10.conv.0 tensor(0.0487)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0812)
features.11.conv.0 tensor(0.1388)
features.11.conv.3 tensor(0.1927)
features.11.conv.6 tensor(0.2927)
features.12.conv.0 tensor(0.1870)
features.12.conv.3 tensor(0.1696)
features.12.conv.6 tensor(0.2589)
features.13.conv.0 tensor(0.0999)
features.13.conv.3 tensor(0.1944)
features.13.conv.6 tensor(0.1359)
features.14.conv.0 tensor(0.8971)
features.14.conv.3 tensor(0.1344)
features.14.conv.6 tensor(0.9794)
features.15.conv.0 tensor(0.9540)
features.15.conv.3 tensor(0.1341)
features.15.conv.6 tensor(0.9585)
features.16.conv.0 tensor(0.1108)
features.16.conv.3 tensor(0.1898)
features.16.conv.6 tensor(0.2011)
conv.0 tensor(0.1328)
tensor(816529.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [8][   20/  196]   Loss 0.346822   Top1 87.558594   Top5 98.457031   BatchTime 0.432924   LR 0.000434
INFO - Training [8][   40/  196]   Loss 0.346510   Top1 87.695312   Top5 98.662109   BatchTime 0.391102   LR 0.000389
INFO - Training [8][   60/  196]   Loss 0.338593   Top1 88.183594   Top5 98.710938   BatchTime 0.358829   LR 0.000347
INFO - Training [8][   80/  196]   Loss 0.341942   Top1 87.983398   Top5 98.818359   BatchTime 0.351837   LR 0.000308
INFO - Training [8][  100/  196]   Loss 0.333236   Top1 88.257812   Top5 98.898438   BatchTime 0.348729   LR 0.000270
INFO - Training [8][  120/  196]   Loss 0.328718   Top1 88.430990   Top5 98.990885   BatchTime 0.346965   LR 0.000235
INFO - Training [8][  140/  196]   Loss 0.326503   Top1 88.518415   Top5 99.031808   BatchTime 0.348975   LR 0.000202
INFO - Training [8][  160/  196]   Loss 0.329184   Top1 88.452148   Top5 99.013672   BatchTime 0.348147   LR 0.000172
INFO - Training [8][  180/  196]   Loss 0.329115   Top1 88.459201   Top5 98.982205   BatchTime 0.347197   LR 0.000143
INFO - ==> Top1: 88.496    Top5: 98.968    Loss: 0.329
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.278540   Top1 91.054688   Top5 99.765625   BatchTime 0.129458
INFO - Validation [8][   40/   40]   Loss 0.265045   Top1 91.270000   Top5 99.800000   BatchTime 0.091339
features.0.conv.0 tensor(0.4062)
features.0.conv.3 tensor(0.1191)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0339)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0489)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0323)
features.4.conv.0 tensor(0.0231)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0672)
features.5.conv.0 tensor(0.0422)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1453)
features.6.conv.0 tensor(0.0265)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0531)
features.7.conv.0 tensor(0.0402)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.1021)
features.8.conv.0 tensor(0.0854)
features.8.conv.3 tensor(0.1233)
features.8.conv.6 tensor(0.2942)
features.9.conv.0 tensor(0.0889)
features.9.conv.3 tensor(0.1606)
features.9.conv.6 tensor(0.1331)
features.10.conv.0 tensor(0.0491)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0806)
features.11.conv.0 tensor(0.1384)
features.11.conv.3 tensor(0.1906)
features.11.conv.6 tensor(0.2926)
features.12.conv.0 tensor(0.1876)
features.12.conv.3 tensor(0.1690)
features.12.conv.6 tensor(0.2582)
features.13.conv.0 tensor(0.0991)
features.13.conv.3 tensor(0.1927)
features.13.conv.6 tensor(0.1368)
features.14.conv.0 tensor(0.8973)
features.14.conv.3 tensor(0.1337)
features.14.conv.6 tensor(0.9797)
features.15.conv.0 tensor(0.9542)
features.15.conv.3 tensor(0.1344)
features.15.conv.6 tensor(0.9579)
features.16.conv.0 tensor(0.1093)
features.16.conv.3 tensor(0.1882)
features.16.conv.6 tensor(0.2020)
conv.0 tensor(0.1328)
tensor(816569.) 2188896.0
INFO - ==> Top1: 91.270    Top5: 99.800    Loss: 0.265
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.870   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [9][   20/  196]   Loss 0.353595   Top1 87.500000   Top5 98.242188   BatchTime 0.448619   LR 0.000100
INFO - Training [9][   40/  196]   Loss 0.344110   Top1 87.939453   Top5 98.525391   BatchTime 0.371059   LR 0.000079
INFO - Training [9][   60/  196]   Loss 0.339212   Top1 88.125000   Top5 98.671875   BatchTime 0.343535   LR 0.000060
INFO - Training [9][   80/  196]   Loss 0.335175   Top1 88.251953   Top5 98.789062   BatchTime 0.343794   LR 0.000044
INFO - Training [9][  100/  196]   Loss 0.330390   Top1 88.390625   Top5 98.843750   BatchTime 0.343039   LR 0.000030
INFO - Training [9][  120/  196]   Loss 0.323004   Top1 88.626302   Top5 98.916016   BatchTime 0.344505   LR 0.000019
INFO - Training [9][  140/  196]   Loss 0.321536   Top1 88.713728   Top5 98.978795   BatchTime 0.344453   LR 0.000010
INFO - Training [9][  160/  196]   Loss 0.324150   Top1 88.571777   Top5 99.008789   BatchTime 0.344420   LR 0.000004
INFO - Training [9][  180/  196]   Loss 0.323873   Top1 88.574219   Top5 98.980035   BatchTime 0.344306   LR 0.000001
INFO - ==> Top1: 88.650    Top5: 98.976    Loss: 0.323
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.294323   Top1 90.781250   Top5 99.707031   BatchTime 0.135312
INFO - Validation [9][   40/   40]   Loss 0.277061   Top1 91.120000   Top5 99.790000   BatchTime 0.095681
INFO - ==> Top1: 91.120    Top5: 99.790    Loss: 0.277
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4132)
features.0.conv.3 tensor(0.1211)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0339)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0489)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0334)
features.4.conv.0 tensor(0.0233)
features.4.conv.3 tensor(0.0926)
features.4.conv.6 tensor(0.0679)
features.5.conv.0 tensor(0.0431)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1453)
features.6.conv.0 tensor(0.0264)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0402)
features.7.conv.3 tensor(0.1157)
features.7.conv.6 tensor(0.1021)
features.8.conv.0 tensor(0.0860)
features.8.conv.3 tensor(0.1227)
features.8.conv.6 tensor(0.2941)
features.9.conv.0 tensor(0.0889)
features.9.conv.3 tensor(0.1635)
features.9.conv.6 tensor(0.1332)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0807)
features.11.conv.0 tensor(0.1383)
features.11.conv.3 tensor(0.1896)
features.11.conv.6 tensor(0.2927)
features.12.conv.0 tensor(0.1880)
features.12.conv.3 tensor(0.1703)
features.12.conv.6 tensor(0.2578)
features.13.conv.0 tensor(0.0992)
features.13.conv.3 tensor(0.1937)
features.13.conv.6 tensor(0.1368)
features.14.conv.0 tensor(0.8973)
features.14.conv.3 tensor(0.1333)
features.14.conv.6 tensor(0.9791)
features.15.conv.0 tensor(0.9539)
features.15.conv.3 tensor(0.1334)
features.15.conv.6 tensor(0.9580)
features.16.conv.0 tensor(0.1093)
features.16.conv.3 tensor(0.1903)
features.16.conv.6 tensor(0.2017)
conv.0 tensor(0.1324)
tensor(816287.) 2188896.0
INFO - Training [10][   20/  196]   Loss 0.385294   Top1 85.703125   Top5 98.437500   BatchTime 0.453060   LR 0.002500
INFO - Training [10][   40/  196]   Loss 0.387518   Top1 85.937500   Top5 98.593750   BatchTime 0.374740   LR 0.002499
INFO - Training [10][   60/  196]   Loss 0.385490   Top1 86.263021   Top5 98.717448   BatchTime 0.346973   LR 0.002499
INFO - Training [10][   80/  196]   Loss 0.391386   Top1 86.230469   Top5 98.803711   BatchTime 0.346285   LR 0.002497
INFO - Training [10][  100/  196]   Loss 0.385910   Top1 86.375000   Top5 98.832031   BatchTime 0.346198   LR 0.002496
INFO - Training [10][  120/  196]   Loss 0.383973   Top1 86.455078   Top5 98.899740   BatchTime 0.348937   LR 0.002494
INFO - Training [10][  140/  196]   Loss 0.387008   Top1 86.316964   Top5 98.909040   BatchTime 0.347584   LR 0.002492
INFO - Training [10][  160/  196]   Loss 0.390530   Top1 86.203613   Top5 98.894043   BatchTime 0.346673   LR 0.002490
INFO - Training [10][  180/  196]   Loss 0.390095   Top1 86.191406   Top5 98.854167   BatchTime 0.345845   LR 0.002487
INFO - ==> Top1: 86.246    Top5: 98.842    Loss: 0.390
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.368336   Top1 87.968750   Top5 99.531250   BatchTime 0.130994
INFO - Validation [10][   40/   40]   Loss 0.353132   Top1 88.150000   Top5 99.580000   BatchTime 0.092506
INFO - ==> Top1: 88.150    Top5: 99.580    Loss: 0.353
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4167)
features.0.conv.3 tensor(0.1113)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0475)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0284)
features.4.conv.0 tensor(0.0262)
features.4.conv.3 tensor(0.0926)
features.4.conv.6 tensor(0.0645)
features.5.conv.0 tensor(0.0461)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.1546)
features.6.conv.0 tensor(0.0319)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0549)
features.7.conv.0 tensor(0.0401)
features.7.conv.3 tensor(0.1131)
features.7.conv.6 tensor(0.1117)
features.8.conv.0 tensor(0.0895)
features.8.conv.3 tensor(0.1143)
features.8.conv.6 tensor(0.3016)
features.9.conv.0 tensor(0.0794)
features.9.conv.3 tensor(0.1635)
features.9.conv.6 tensor(0.1419)
features.10.conv.0 tensor(0.0479)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.0834)
features.11.conv.0 tensor(0.1508)
features.11.conv.3 tensor(0.1898)
features.11.conv.6 tensor(0.3032)
features.12.conv.0 tensor(0.1930)
features.12.conv.3 tensor(0.1703)
features.12.conv.6 tensor(0.2553)
features.13.conv.0 tensor(0.0992)
features.13.conv.3 tensor(0.1983)
features.13.conv.6 tensor(0.1433)
features.14.conv.0 tensor(0.8976)
features.14.conv.3 tensor(0.1356)
features.14.conv.6 tensor(0.9824)
features.15.conv.0 tensor(0.9540)
features.15.conv.3 tensor(0.1370)
features.15.conv.6 tensor(0.9597)
features.16.conv.0 tensor(0.0968)
features.16.conv.3 tensor(0.1898)
features.16.conv.6 tensor(0.2105)
conv.0 tensor(0.1376)
tensor(822711.) 2188896.0
INFO - Training [11][   20/  196]   Loss 0.400455   Top1 86.250000   Top5 98.242188   BatchTime 0.450434   LR 0.002481
INFO - Training [11][   40/  196]   Loss 0.404263   Top1 86.210938   Top5 98.437500   BatchTime 0.392793   LR 0.002478
INFO - Training [11][   60/  196]   Loss 0.400269   Top1 86.250000   Top5 98.535156   BatchTime 0.349228   LR 0.002474
INFO - Training [11][   80/  196]   Loss 0.402630   Top1 86.035156   Top5 98.632812   BatchTime 0.339196   LR 0.002470
INFO - Training [11][  100/  196]   Loss 0.398813   Top1 86.175781   Top5 98.640625   BatchTime 0.339780   LR 0.002465
INFO - Training [11][  120/  196]   Loss 0.396859   Top1 86.184896   Top5 98.707682   BatchTime 0.343191   LR 0.002460
INFO - Training [11][  140/  196]   Loss 0.398132   Top1 86.074219   Top5 98.741629   BatchTime 0.343829   LR 0.002455
INFO - Training [11][  160/  196]   Loss 0.400424   Top1 86.008301   Top5 98.730469   BatchTime 0.342770   LR 0.002450
INFO - Training [11][  180/  196]   Loss 0.399958   Top1 86.004774   Top5 98.719618   BatchTime 0.343531   LR 0.002444
INFO - ==> Top1: 84.850    Top5: 98.310    Loss: 0.452
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 2.420888   Top1 10.214844   Top5 50.019531   BatchTime 0.137945
INFO - Validation [11][   40/   40]   Loss 2.420902   Top1 9.990000   Top5 50.000000   BatchTime 0.096314
INFO - ==> Top1: 9.990    Top5: 50.000    Loss: 2.421
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3403)
features.0.conv.3 tensor(0.0977)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0355)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0179)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.0662)
features.5.conv.0 tensor(0.0444)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.1740)
features.6.conv.0 tensor(0.0252)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0564)
features.7.conv.0 tensor(0.0368)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1103)
features.8.conv.0 tensor(0.0913)
features.8.conv.3 tensor(0.1230)
features.8.conv.6 tensor(0.3131)
features.9.conv.0 tensor(0.0677)
features.9.conv.3 tensor(0.1652)
features.9.conv.6 tensor(0.1427)
features.10.conv.0 tensor(0.0497)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.0833)
features.11.conv.0 tensor(0.1606)
features.11.conv.3 tensor(0.1952)
features.11.conv.6 tensor(0.3246)
features.12.conv.0 tensor(0.1863)
features.12.conv.3 tensor(0.1568)
features.12.conv.6 tensor(0.2579)
features.13.conv.0 tensor(0.0963)
features.13.conv.3 tensor(0.1865)
features.13.conv.6 tensor(0.1372)
features.14.conv.0 tensor(0.8836)
features.14.conv.3 tensor(0.1294)
features.14.conv.6 tensor(0.9764)
features.15.conv.0 tensor(0.9437)
features.15.conv.3 tensor(0.1363)
features.15.conv.6 tensor(0.9587)
features.16.conv.0 tensor(0.1011)
features.16.conv.3 tensor(0.1881)
features.16.conv.6 tensor(0.1661)
conv.0 tensor(0.1631)
tensor(815920.) 2188896.0
INFO - Training [12][   20/  196]   Loss 2.461069   Top1 11.171875   Top5 54.296875   BatchTime 0.454483   LR 0.002433
INFO - Training [12][   40/  196]   Loss 2.366173   Top1 12.128906   Top5 55.996094   BatchTime 0.399715   LR 0.002426
INFO - Training [12][   60/  196]   Loss 2.278921   Top1 15.214844   Top5 61.894531   BatchTime 0.365048   LR 0.002419
INFO - Training [12][   80/  196]   Loss 2.211597   Top1 17.890625   Top5 65.673828   BatchTime 0.347931   LR 0.002412
INFO - Training [12][  100/  196]   Loss 2.155210   Top1 19.910156   Top5 68.621094   BatchTime 0.346821   LR 0.002404
INFO - Training [12][  120/  196]   Loss 2.109240   Top1 21.761068   Top5 70.882161   BatchTime 0.350029   LR 0.002396
INFO - Training [12][  140/  196]   Loss 2.069282   Top1 23.356585   Top5 72.781808   BatchTime 0.348576   LR 0.002388
INFO - Training [12][  160/  196]   Loss 2.042153   Top1 24.418945   Top5 74.045410   BatchTime 0.347279   LR 0.002380
INFO - Training [12][  180/  196]   Loss 2.024511   Top1 24.995660   Top5 74.631076   BatchTime 0.346988   LR 0.002371
INFO - ==> Top1: 24.998    Top5: 74.342    Loss: 2.021
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [12][   20/   40]   Loss 2.391814   Top1 10.117188   Top5 49.941406   BatchTime 0.151191
INFO - Validation [12][   40/   40]   Loss 2.391322   Top1 10.000000   Top5 50.000000   BatchTime 0.102190
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.391
INFO - ==> Sparsity : 0.482
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4097)
features.0.conv.3 tensor(0.0879)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0378)
features.2.conv.0 tensor(0.0211)
features.2.conv.3 tensor(0.0023)
features.2.conv.6 tensor(0.0310)
features.3.conv.0 tensor(0.0072)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0295)
features.4.conv.0 tensor(0.0099)
features.4.conv.3 tensor(0.0926)
features.4.conv.6 tensor(0.0645)
features.5.conv.0 tensor(0.0604)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.1662)
features.6.conv.0 tensor(0.0205)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0567)
features.7.conv.0 tensor(0.0406)
features.7.conv.3 tensor(0.1209)
features.7.conv.6 tensor(0.1024)
features.8.conv.0 tensor(0.0828)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.3020)
features.9.conv.0 tensor(0.0752)
features.9.conv.3 tensor(0.1797)
features.9.conv.6 tensor(0.1500)
features.10.conv.0 tensor(0.0507)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.0848)
features.11.conv.0 tensor(0.2028)
features.11.conv.3 tensor(0.2141)
features.11.conv.6 tensor(0.3313)
features.12.conv.0 tensor(0.1534)
features.12.conv.3 tensor(0.1825)
features.12.conv.6 tensor(0.2963)
features.13.conv.0 tensor(0.0900)
features.13.conv.3 tensor(0.1983)
features.13.conv.6 tensor(0.1475)
features.14.conv.0 tensor(0.6599)
features.14.conv.3 tensor(0.1481)
features.14.conv.6 tensor(0.9632)
features.15.conv.0 tensor(0.8697)
features.15.conv.3 tensor(0.1572)
features.15.conv.6 tensor(0.8895)
features.16.conv.0 tensor(0.0500)
features.16.conv.3 tensor(0.1928)
features.16.conv.6 tensor(1.)
conv.0 tensor(0.2733)
tensor(1055212.) 2188896.0
INFO - Training [13][   20/  196]   Loss 2.078713   Top1 21.894531   Top5 65.097656   BatchTime 0.434201   LR 0.002355
INFO - Training [13][   40/  196]   Loss 2.197727   Top1 16.191406   Top5 57.070312   BatchTime 0.405023   LR 0.002345
INFO - Training [13][   60/  196]   Loss 2.234076   Top1 13.938802   Top5 54.752604   BatchTime 0.370354   LR 0.002336
INFO - Training [13][   80/  196]   Loss 2.252103   Top1 12.973633   Top5 53.408203   BatchTime 0.346777   LR 0.002325
INFO - Training [13][  100/  196]   Loss 2.262501   Top1 12.429688   Top5 52.808594   BatchTime 0.349540   LR 0.002315
INFO - Training [13][  120/  196]   Loss 2.269484   Top1 12.005208   Top5 52.382812   BatchTime 0.350173   LR 0.002304
INFO - Training [13][  140/  196]   Loss 2.274655   Top1 11.640625   Top5 52.045201   BatchTime 0.348963   LR 0.002293
INFO - Training [13][  160/  196]   Loss 2.278381   Top1 11.472168   Top5 51.765137   BatchTime 0.348006   LR 0.002282
INFO - Training [13][  180/  196]   Loss 2.281292   Top1 11.260851   Top5 51.516927   BatchTime 0.347404   LR 0.002271
INFO - ==> Top1: 11.146    Top5: 51.326    Loss: 2.283
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [13][   20/   40]   Loss 2.303663   Top1 9.882812   Top5 49.687500   BatchTime 0.136148
INFO - Validation [13][   40/   40]   Loss 2.303339   Top1 10.000000   Top5 50.000000   BatchTime 0.096652
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3207.) 2188896.0
INFO - Training [14][   20/  196]   Loss 2.304434   Top1 9.902344   Top5 51.035156   BatchTime 0.420210   LR 0.002250
INFO - Training [14][   40/  196]   Loss 2.304369   Top1 10.175781   Top5 50.556641   BatchTime 0.393796   LR 0.002238
INFO - Training [14][   60/  196]   Loss 2.304475   Top1 10.169271   Top5 50.305990   BatchTime 0.366154   LR 0.002225
INFO - Training [14][   80/  196]   Loss 2.304217   Top1 10.126953   Top5 50.239258   BatchTime 0.339932   LR 0.002213
INFO - Training [14][  100/  196]   Loss 2.304236   Top1 9.976562   Top5 50.097656   BatchTime 0.341109   LR 0.002200
INFO - Training [14][  120/  196]   Loss 2.303992   Top1 10.039062   Top5 50.201823   BatchTime 0.344415   LR 0.002186
INFO - Training [14][  140/  196]   Loss 2.304283   Top1 9.930246   Top5 49.874442   BatchTime 0.343724   LR 0.002173
INFO - Training [14][  160/  196]   Loss 2.304229   Top1 9.848633   Top5 49.936523   BatchTime 0.343089   LR 0.002159
INFO - Training [14][  180/  196]   Loss 2.304211   Top1 9.845920   Top5 49.898003   BatchTime 0.342347   LR 0.002145
********************pre-trained*****************
INFO - ==> Top1: 9.776    Top5: 49.804    Loss: 2.304
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 2.303211   Top1 9.863281   Top5 49.687500   BatchTime 0.136546
INFO - Validation [14][   40/   40]   Loss 2.303041   Top1 10.000000   Top5 50.000000   BatchTime 0.096746
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3207.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [15][   20/  196]   Loss 2.302876   Top1 10.214844   Top5 50.644531   BatchTime 0.438164   LR 0.002120
INFO - Training [15][   40/  196]   Loss 2.303743   Top1 9.863281   Top5 49.472656   BatchTime 0.401373   LR 0.002106
INFO - Training [15][   60/  196]   Loss 2.303515   Top1 9.947917   Top5 49.563802   BatchTime 0.370982   LR 0.002091
INFO - Training [15][   80/  196]   Loss 2.303546   Top1 9.750977   Top5 49.453125   BatchTime 0.346802   LR 0.002076
INFO - Training [15][  100/  196]   Loss 2.303435   Top1 9.835938   Top5 49.625000   BatchTime 0.342603   LR 0.002061
INFO - Training [15][  120/  196]   Loss 2.303406   Top1 9.837240   Top5 49.612630   BatchTime 0.343976   LR 0.002045
INFO - Training [15][  140/  196]   Loss 2.303562   Top1 9.868862   Top5 49.475446   BatchTime 0.344742   LR 0.002030
INFO - Training [15][  160/  196]   Loss 2.303554   Top1 9.936523   Top5 49.375000   BatchTime 0.344088   LR 0.002014
INFO - Training [15][  180/  196]   Loss 2.303444   Top1 10.006510   Top5 49.479167   BatchTime 0.343571   LR 0.001998
INFO - ==> Top1: 9.938    Top5: 49.492    Loss: 2.304
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 2.302798   Top1 9.882812   Top5 49.785156   BatchTime 0.133969
INFO - Validation [15][   40/   40]   Loss 2.302734   Top1 10.000000   Top5 50.000000   BatchTime 0.095086
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [16][   20/  196]   Loss 2.303283   Top1 9.687500   Top5 49.687500   BatchTime 0.445014   LR 0.001969
INFO - Training [16][   40/  196]   Loss 2.303111   Top1 10.117188   Top5 49.609375   BatchTime 0.397347   LR 0.001953
INFO - Training [16][   60/  196]   Loss 2.302932   Top1 10.078125   Top5 49.882812   BatchTime 0.378932   LR 0.001936
INFO - Training [16][   80/  196]   Loss 2.303166   Top1 9.956055   Top5 49.614258   BatchTime 0.351500   LR 0.001919
INFO - Training [16][  100/  196]   Loss 2.303080   Top1 9.960938   Top5 49.816406   BatchTime 0.344829   LR 0.001902
INFO - Training [16][  120/  196]   Loss 2.303079   Top1 9.954427   Top5 49.964193   BatchTime 0.345975   LR 0.001885
INFO - Training [16][  140/  196]   Loss 2.303124   Top1 9.955357   Top5 49.941406   BatchTime 0.346282   LR 0.001867
INFO - Training [16][  160/  196]   Loss 2.303109   Top1 9.973145   Top5 49.924316   BatchTime 0.346399   LR 0.001850
INFO - Training [16][  180/  196]   Loss 2.303149   Top1 9.956597   Top5 49.756944   BatchTime 0.346291   LR 0.001832
INFO - ==> Top1: 9.898    Top5: 49.708    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 2.302734   Top1 9.863281   Top5 49.902344   BatchTime 0.138181
INFO - Validation [16][   40/   40]   Loss 2.302675   Top1 10.000000   Top5 50.000000   BatchTime 0.095702
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [17][   20/  196]   Loss 2.302448   Top1 10.722656   Top5 51.542969   BatchTime 0.435329   LR 0.001800
INFO - Training [17][   40/  196]   Loss 2.302822   Top1 10.234375   Top5 50.458984   BatchTime 0.403014   LR 0.001782
INFO - Training [17][   60/  196]   Loss 2.302828   Top1 10.240885   Top5 50.514323   BatchTime 0.384176   LR 0.001764
INFO - Training [17][   80/  196]   Loss 2.302777   Top1 10.263672   Top5 50.375977   BatchTime 0.358291   LR 0.001746
INFO - Training [17][  100/  196]   Loss 2.302803   Top1 10.230469   Top5 50.304688   BatchTime 0.341446   LR 0.001727
INFO - Training [17][  120/  196]   Loss 2.302829   Top1 10.227865   Top5 50.253906   BatchTime 0.332712   LR 0.001708
INFO - Training [17][  140/  196]   Loss 2.302844   Top1 10.128348   Top5 50.228795   BatchTime 0.321617   LR 0.001690
INFO - Training [17][  160/  196]   Loss 2.302997   Top1 10.026855   Top5 50.058594   BatchTime 0.313070   LR 0.001671
INFO - Training [17][  180/  196]   Loss 2.303035   Top1 9.982639   Top5 50.032552   BatchTime 0.306047   LR 0.001652
INFO - ==> Top1: 9.926    Top5: 50.016    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [17][   20/   40]   Loss 2.302740   Top1 10.117188   Top5 49.628906   BatchTime 0.146605
INFO - Validation [17][   40/   40]   Loss 2.302668   Top1 10.000000   Top5 50.000000   BatchTime 0.099703
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [18][   20/  196]   Loss 2.302854   Top1 10.117188   Top5 49.785156   BatchTime 0.338441   LR 0.001618
INFO - Training [18][   40/  196]   Loss 2.302783   Top1 10.068359   Top5 49.892578   BatchTime 0.294300   LR 0.001599
INFO - Training [18][   60/  196]   Loss 2.302829   Top1 10.110677   Top5 50.208333   BatchTime 0.277747   LR 0.001579
INFO - Training [18][   80/  196]   Loss 2.302817   Top1 10.092773   Top5 50.190430   BatchTime 0.278941   LR 0.001560
INFO - Training [18][  100/  196]   Loss 2.302871   Top1 10.230469   Top5 50.093750   BatchTime 0.272109   LR 0.001540
INFO - Training [18][  120/  196]   Loss 2.303007   Top1 10.084635   Top5 49.918620   BatchTime 0.269656   LR 0.001521
INFO - Training [18][  140/  196]   Loss 2.303048   Top1 10.089286   Top5 49.740513   BatchTime 0.267766   LR 0.001501
INFO - Training [18][  160/  196]   Loss 2.302989   Top1 10.048828   Top5 49.775391   BatchTime 0.266653   LR 0.001482
INFO - Training [18][  180/  196]   Loss 2.302972   Top1 10.067274   Top5 49.730903   BatchTime 0.266053   LR 0.001462
INFO - ==> Top1: 10.122    Top5: 49.760    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 2.302666   Top1 10.019531   Top5 50.097656   BatchTime 0.146851
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Validation [18][   40/   40]   Loss 2.302639   Top1 10.000000   Top5 50.000000   BatchTime 0.100384
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [19][   20/  196]   Loss 2.302992   Top1 9.746094   Top5 49.121094   BatchTime 0.330744   LR 0.001427
INFO - Training [19][   40/  196]   Loss 2.303015   Top1 9.863281   Top5 49.218750   BatchTime 0.287440   LR 0.001407
INFO - Training [19][   60/  196]   Loss 2.303100   Top1 9.811198   Top5 49.036458   BatchTime 0.283304   LR 0.001387
INFO - Training [19][   80/  196]   Loss 2.303051   Top1 9.838867   Top5 49.340820   BatchTime 0.276828   LR 0.001367
INFO - Training [19][  100/  196]   Loss 2.302952   Top1 9.964844   Top5 49.609375   BatchTime 0.272142   LR 0.001347
INFO - Training [19][  120/  196]   Loss 2.302903   Top1 9.918620   Top5 49.749349   BatchTime 0.268827   LR 0.001327
INFO - Training [19][  140/  196]   Loss 2.302872   Top1 10.036272   Top5 49.852121   BatchTime 0.269723   LR 0.001307
INFO - Training [19][  160/  196]   Loss 2.302888   Top1 10.087891   Top5 49.772949   BatchTime 0.270111   LR 0.001287
INFO - Training [19][  180/  196]   Loss 2.302924   Top1 10.041233   Top5 49.639757   BatchTime 0.268045   LR 0.001266
INFO - ==> Top1: 10.034    Top5: 49.712    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 2.302574   Top1 9.882812   Top5 50.429688   BatchTime 0.141636
INFO - Validation [19][   40/   40]   Loss 2.302604   Top1 10.000000   Top5 50.000000   BatchTime 0.097614
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [20][   20/  196]   Loss 2.303332   Top1 9.453125   Top5 49.335938   BatchTime 0.368402   LR 0.001231
INFO - Training [20][   40/  196]   Loss 2.303247   Top1 9.599609   Top5 49.804688   BatchTime 0.311934   LR 0.001211
INFO - Training [20][   60/  196]   Loss 2.302916   Top1 9.739583   Top5 50.110677   BatchTime 0.291562   LR 0.001191
INFO - Training [20][   80/  196]   Loss 2.302867   Top1 9.716797   Top5 50.253906   BatchTime 0.281057   LR 0.001171
INFO - Training [20][  100/  196]   Loss 2.302876   Top1 9.804688   Top5 50.226562   BatchTime 0.275748   LR 0.001151
INFO - Training [20][  120/  196]   Loss 2.302809   Top1 9.951172   Top5 50.240885   BatchTime 0.269875   LR 0.001131
INFO - Training [20][  140/  196]   Loss 2.302846   Top1 9.955357   Top5 50.122768   BatchTime 0.267091   LR 0.001111
INFO - Training [20][  160/  196]   Loss 2.302865   Top1 9.980469   Top5 50.139160   BatchTime 0.265013   LR 0.001091
INFO - Training [20][  180/  196]   Loss 2.302899   Top1 9.928385   Top5 50.032552   BatchTime 0.266503   LR 0.001071
INFO - ==> Top1: 9.968    Top5: 50.120    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [20][   20/   40]   Loss 2.302891   Top1 9.882812   Top5 49.785156   BatchTime 0.131052
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Validation [20][   40/   40]   Loss 2.302808   Top1 10.000000   Top5 50.000000   BatchTime 0.097589
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [21][   20/  196]   Loss 2.303053   Top1 9.941406   Top5 50.058594   BatchTime 0.377885   LR 0.001036
INFO - Training [21][   40/  196]   Loss 2.303159   Top1 9.931641   Top5 49.804688   BatchTime 0.328151   LR 0.001016
INFO - Training [21][   60/  196]   Loss 2.303000   Top1 9.980469   Top5 50.188802   BatchTime 0.304186   LR 0.000996
INFO - Training [21][   80/  196]   Loss 2.303007   Top1 10.058594   Top5 50.253906   BatchTime 0.291735   LR 0.000976
INFO - Training [21][  100/  196]   Loss 2.302965   Top1 9.988281   Top5 50.203125   BatchTime 0.281701   LR 0.000957
INFO - Training [21][  120/  196]   Loss 2.302999   Top1 10.013021   Top5 49.993490   BatchTime 0.277981   LR 0.000937
INFO - Training [21][  140/  196]   Loss 2.302934   Top1 10.047433   Top5 50.066964   BatchTime 0.275277   LR 0.000918
INFO - Training [21][  160/  196]   Loss 2.302879   Top1 9.975586   Top5 50.158691   BatchTime 0.273680   LR 0.000899
INFO - Training [21][  180/  196]   Loss 2.302947   Top1 9.952257   Top5 49.917535   BatchTime 0.274318   LR 0.000879
********************pre-trained*****************
INFO - ==> Top1: 9.962    Top5: 49.818    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 2.302572   Top1 10.019531   Top5 50.292969   BatchTime 0.156108
INFO - Validation [21][   40/   40]   Loss 2.302593   Top1 10.000000   Top5 50.000000   BatchTime 0.105754
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [22][   20/  196]   Loss 2.302512   Top1 10.859375   Top5 49.589844   BatchTime 0.353461   LR 0.000846
INFO - Training [22][   40/  196]   Loss 2.302948   Top1 9.970703   Top5 49.658203   BatchTime 0.304744   LR 0.000827
INFO - Training [22][   60/  196]   Loss 2.302704   Top1 10.006510   Top5 50.091146   BatchTime 0.301139   LR 0.000808
INFO - Training [22][   80/  196]   Loss 2.302823   Top1 9.990234   Top5 49.965820   BatchTime 0.294100   LR 0.000789
INFO - Training [22][  100/  196]   Loss 2.302837   Top1 9.968750   Top5 49.707031   BatchTime 0.289773   LR 0.000770
INFO - Training [22][  120/  196]   Loss 2.302786   Top1 10.013021   Top5 49.785156   BatchTime 0.285818   LR 0.000752
INFO - Training [22][  140/  196]   Loss 2.302849   Top1 9.974888   Top5 49.771205   BatchTime 0.281246   LR 0.000734
INFO - Training [22][  160/  196]   Loss 2.302884   Top1 9.951172   Top5 49.772949   BatchTime 0.278285   LR 0.000715
INFO - Training [22][  180/  196]   Loss 2.302846   Top1 10.019531   Top5 49.817708   BatchTime 0.277121   LR 0.000697
INFO - ==> Top1: 10.014    Top5: 49.744    Loss: 2.303
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 2.302703   Top1 9.882812   Top5 49.785156   BatchTime 0.146611
INFO - Validation [22][   40/   40]   Loss 2.302664   Top1 10.000000   Top5 50.000000   BatchTime 0.100293
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [23][   20/  196]   Loss 2.302375   Top1 9.960938   Top5 51.679688   BatchTime 0.354651   LR 0.000666
INFO - Training [23][   40/  196]   Loss 2.302532   Top1 9.843750   Top5 51.191406   BatchTime 0.307169   LR 0.000648
INFO - Training [23][   60/  196]   Loss 2.302565   Top1 9.967448   Top5 50.598958   BatchTime 0.290554   LR 0.000630
INFO - Training [23][   80/  196]   Loss 2.302635   Top1 9.946289   Top5 50.590820   BatchTime 0.285395   LR 0.000613
INFO - Training [23][  100/  196]   Loss 2.302692   Top1 9.929688   Top5 50.355469   BatchTime 0.280107   LR 0.000596
INFO - Training [23][  120/  196]   Loss 2.302734   Top1 9.905599   Top5 50.192057   BatchTime 0.282583   LR 0.000579
INFO - Training [23][  140/  196]   Loss 2.302748   Top1 9.905134   Top5 50.184152   BatchTime 0.280945   LR 0.000562
INFO - Training [23][  160/  196]   Loss 2.302779   Top1 9.909668   Top5 50.200195   BatchTime 0.278534   LR 0.000545
INFO - Training [23][  180/  196]   Loss 2.302789   Top1 9.928385   Top5 50.060764   BatchTime 0.274713   LR 0.000529
INFO - ==> Top1: 9.932    Top5: 49.940    Loss: 2.303
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 2.302658   Top1 9.882812   Top5 49.785156   BatchTime 0.153698
INFO - Validation [23][   40/   40]   Loss 2.302618   Top1 10.000000   Top5 50.000000   BatchTime 0.104611
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [24][   20/  196]   Loss 2.303120   Top1 9.648438   Top5 50.039062   BatchTime 0.369265   LR 0.000500
INFO - Training [24][   40/  196]   Loss 2.302765   Top1 9.863281   Top5 50.468750   BatchTime 0.325775   LR 0.000484
INFO - Training [24][   60/  196]   Loss 2.302654   Top1 10.065104   Top5 50.117188   BatchTime 0.306482   LR 0.000468
INFO - Training [24][   80/  196]   Loss 2.302770   Top1 9.921875   Top5 49.731445   BatchTime 0.294803   LR 0.000453
INFO - Training [24][  100/  196]   Loss 2.302743   Top1 9.910156   Top5 49.898438   BatchTime 0.287677   LR 0.000437
INFO - Training [24][  120/  196]   Loss 2.302709   Top1 10.026042   Top5 49.996745   BatchTime 0.281013   LR 0.000422
INFO - Training [24][  140/  196]   Loss 2.302782   Top1 9.997210   Top5 49.916295   BatchTime 0.275831   LR 0.000407
INFO - Training [24][  160/  196]   Loss 2.302780   Top1 9.992676   Top5 49.912109   BatchTime 0.273928   LR 0.000392
INFO - Training [24][  180/  196]   Loss 2.302835   Top1 9.950087   Top5 49.845920   BatchTime 0.275725   LR 0.000378
INFO - ==> Top1: 9.894    Top5: 49.772    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [24][   20/   40]   Loss 2.302635   Top1 9.882812   Top5 49.785156   BatchTime 0.149722
INFO - Validation [24][   40/   40]   Loss 2.302608   Top1 10.000000   Top5 50.000000   BatchTime 0.103008
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [25][   20/  196]   Loss 2.303339   Top1 9.179688   Top5 49.531250   BatchTime 0.359870   LR 0.000353
INFO - Training [25][   40/  196]   Loss 2.303044   Top1 9.531250   Top5 49.960938   BatchTime 0.305787   LR 0.000339
INFO - Training [25][   60/  196]   Loss 2.302853   Top1 9.739583   Top5 50.201823   BatchTime 0.293492   LR 0.000325
INFO - Training [25][   80/  196]   Loss 2.302806   Top1 9.765625   Top5 50.097656   BatchTime 0.284777   LR 0.000312
INFO - Training [25][  100/  196]   Loss 2.302742   Top1 9.800781   Top5 50.234375   BatchTime 0.280071   LR 0.000299
INFO - Training [25][  120/  196]   Loss 2.302710   Top1 9.843750   Top5 50.130208   BatchTime 0.276452   LR 0.000286
INFO - Training [25][  140/  196]   Loss 2.302766   Top1 9.773996   Top5 50.083705   BatchTime 0.274657   LR 0.000273
INFO - Training [25][  160/  196]   Loss 2.302786   Top1 9.753418   Top5 50.034180   BatchTime 0.271758   LR 0.000261
INFO - Training [25][  180/  196]   Loss 2.302763   Top1 9.811198   Top5 50.034722   BatchTime 0.268780   LR 0.000248
********************pre-trained*****************
INFO - ==> Top1: 9.872    Top5: 50.068    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 2.302622   Top1 9.882812   Top5 49.960938   BatchTime 0.144951
INFO - Validation [25][   40/   40]   Loss 2.302601   Top1 10.000000   Top5 50.000000   BatchTime 0.112192
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [26][   20/  196]   Loss 2.303016   Top1 10.039062   Top5 49.570312   BatchTime 0.400800   LR 0.000228
INFO - Training [26][   40/  196]   Loss 2.302827   Top1 10.273438   Top5 50.146484   BatchTime 0.327683   LR 0.000216
INFO - Training [26][   60/  196]   Loss 2.302755   Top1 10.436198   Top5 49.941406   BatchTime 0.307821   LR 0.000205
INFO - Training [26][   80/  196]   Loss 2.302768   Top1 10.278320   Top5 50.107422   BatchTime 0.297649   LR 0.000194
INFO - Training [26][  100/  196]   Loss 2.302752   Top1 10.222656   Top5 50.324219   BatchTime 0.289458   LR 0.000183
INFO - Training [26][  120/  196]   Loss 2.302716   Top1 10.211589   Top5 50.283203   BatchTime 0.285599   LR 0.000173
INFO - Training [26][  140/  196]   Loss 2.302744   Top1 10.164621   Top5 50.312500   BatchTime 0.282850   LR 0.000163
INFO - Training [26][  160/  196]   Loss 2.302687   Top1 10.222168   Top5 50.429688   BatchTime 0.281617   LR 0.000153
INFO - Training [26][  180/  196]   Loss 2.302674   Top1 10.197483   Top5 50.392795   BatchTime 0.279415   LR 0.000144
INFO - ==> Top1: 10.212    Top5: 50.264    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 2.302601   Top1 9.882812   Top5 49.960938   BatchTime 0.145609
INFO - Validation [26][   40/   40]   Loss 2.302595   Top1 10.000000   Top5 50.000000   BatchTime 0.099254
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [27][   20/  196]   Loss 2.302735   Top1 10.117188   Top5 50.488281   BatchTime 0.358018   LR 0.000128
INFO - Training [27][   40/  196]   Loss 2.302812   Top1 10.000000   Top5 50.185547   BatchTime 0.303482   LR 0.000119
INFO - Training [27][   60/  196]   Loss 2.302632   Top1 10.182292   Top5 50.312500   BatchTime 0.304751   LR 0.000111
INFO - Training [27][   80/  196]   Loss 2.302719   Top1 10.019531   Top5 50.239258   BatchTime 0.294514   LR 0.000102
INFO - Training [27][  100/  196]   Loss 2.302689   Top1 10.042969   Top5 50.164062   BatchTime 0.287197   LR 0.000095
INFO - Training [27][  120/  196]   Loss 2.302788   Top1 10.055339   Top5 49.850260   BatchTime 0.284204   LR 0.000087
INFO - Training [27][  140/  196]   Loss 2.302762   Top1 10.044643   Top5 50.053013   BatchTime 0.281228   LR 0.000080
INFO - Training [27][  160/  196]   Loss 2.302684   Top1 10.095215   Top5 50.070801   BatchTime 0.280900   LR 0.000073
INFO - Training [27][  180/  196]   Loss 2.302695   Top1 10.121528   Top5 50.019531   BatchTime 0.277259   LR 0.000066
INFO - ==> Top1: 10.094    Top5: 50.046    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 2.302603   Top1 9.882812   Top5 49.960938   BatchTime 0.140347
INFO - Validation [27][   40/   40]   Loss 2.302594   Top1 10.000000   Top5 50.000000   BatchTime 0.097435
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [28][   20/  196]   Loss 2.303117   Top1 9.433594   Top5 48.261719   BatchTime 0.352965   LR 0.000055
INFO - Training [28][   40/  196]   Loss 2.303007   Top1 9.414062   Top5 48.789062   BatchTime 0.310644   LR 0.000050
INFO - Training [28][   60/  196]   Loss 2.302894   Top1 9.615885   Top5 49.108073   BatchTime 0.295895   LR 0.000044
INFO - Training [28][   80/  196]   Loss 2.302745   Top1 9.682617   Top5 49.819336   BatchTime 0.285225   LR 0.000039
INFO - Training [28][  100/  196]   Loss 2.302722   Top1 9.906250   Top5 49.996094   BatchTime 0.286006   LR 0.000034
INFO - Training [28][  120/  196]   Loss 2.302729   Top1 9.947917   Top5 49.833984   BatchTime 0.286374   LR 0.000030
INFO - Training [28][  140/  196]   Loss 2.302779   Top1 9.933036   Top5 49.821429   BatchTime 0.283306   LR 0.000026
INFO - Training [28][  160/  196]   Loss 2.302793   Top1 9.877930   Top5 49.748535   BatchTime 0.281038   LR 0.000022
INFO - Training [28][  180/  196]   Loss 2.302828   Top1 9.835069   Top5 49.722222   BatchTime 0.277756   LR 0.000018
INFO - ==> Top1: 9.862    Top5: 49.686    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [28][   20/   40]   Loss 2.302602   Top1 9.882812   Top5 49.960938   BatchTime 0.138511
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
INFO - Validation [28][   40/   40]   Loss 2.302593   Top1 10.000000   Top5 50.000000   BatchTime 0.096820
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [29][   20/  196]   Loss 2.302658   Top1 10.820312   Top5 49.550781   BatchTime 0.343911   LR 0.000013
INFO - Training [29][   40/  196]   Loss 2.302698   Top1 10.615234   Top5 50.097656   BatchTime 0.306328   LR 0.000010
INFO - Training [29][   60/  196]   Loss 2.302921   Top1 10.286458   Top5 49.518229   BatchTime 0.294638   LR 0.000008
INFO - Training [29][   80/  196]   Loss 2.302963   Top1 10.034180   Top5 49.418945   BatchTime 0.287886   LR 0.000005
INFO - Training [29][  100/  196]   Loss 2.302837   Top1 10.203125   Top5 49.855469   BatchTime 0.284483   LR 0.000004
INFO - Training [29][  120/  196]   Loss 2.302781   Top1 10.084635   Top5 49.967448   BatchTime 0.278652   LR 0.000002
INFO - Training [29][  140/  196]   Loss 2.302740   Top1 10.078125   Top5 50.078125   BatchTime 0.274622   LR 0.000001
INFO - Training [29][  160/  196]   Loss 2.302787   Top1 9.929199   Top5 50.024414   BatchTime 0.272489   LR 0.000001
INFO - Training [29][  180/  196]   Loss 2.302756   Top1 10.010851   Top5 50.056424   BatchTime 0.275367   LR 0.000000
INFO - ==> Top1: 9.958    Top5: 49.960    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 2.302601   Top1 9.882812   Top5 49.960938   BatchTime 0.132730
INFO - Validation [29][   40/   40]   Loss 2.302593   Top1 10.000000   Top5 50.000000   BatchTime 0.093999
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [30][   20/  196]   Loss 2.302458   Top1 9.707031   Top5 50.312500   BatchTime 0.339126   LR 0.001250
INFO - Training [30][   40/  196]   Loss 2.302692   Top1 9.775391   Top5 50.478516   BatchTime 0.295252   LR 0.001250
INFO - Training [30][   60/  196]   Loss 2.302796   Top1 9.733073   Top5 49.941406   BatchTime 0.286018   LR 0.001250
INFO - Training [30][   80/  196]   Loss 2.302752   Top1 9.833984   Top5 50.112305   BatchTime 0.281033   LR 0.001250
INFO - Training [30][  100/  196]   Loss 2.302688   Top1 9.996094   Top5 50.023438   BatchTime 0.275900   LR 0.001250
INFO - Training [30][  120/  196]   Loss 2.302751   Top1 9.977214   Top5 49.918620   BatchTime 0.272393   LR 0.001249
INFO - Training [30][  140/  196]   Loss 2.302740   Top1 10.016741   Top5 49.955357   BatchTime 0.269367   LR 0.001249
INFO - Training [30][  160/  196]   Loss 2.302837   Top1 9.914551   Top5 49.724121   BatchTime 0.267799   LR 0.001249
INFO - Training [30][  180/  196]   Loss 2.302850   Top1 9.941406   Top5 49.778646   BatchTime 0.268076   LR 0.001248
********************pre-trained*****************
INFO - ==> Top1: 9.970    Top5: 49.788    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [30][   20/   40]   Loss 2.302578   Top1 10.019531   Top5 50.273438   BatchTime 0.136672
INFO - Validation [30][   40/   40]   Loss 2.302609   Top1 10.000000   Top5 50.000000   BatchTime 0.094477
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [31][   20/  196]   Loss 2.302786   Top1 10.078125   Top5 50.390625   BatchTime 0.408849   LR 0.001248
INFO - Training [31][   40/  196]   Loss 2.302662   Top1 10.205078   Top5 50.175781   BatchTime 0.335250   LR 0.001247
INFO - Training [31][   60/  196]   Loss 2.302645   Top1 10.260417   Top5 50.117188   BatchTime 0.309231   LR 0.001247
INFO - Training [31][   80/  196]   Loss 2.302675   Top1 10.097656   Top5 50.097656   BatchTime 0.295371   LR 0.001246
INFO - Training [31][  100/  196]   Loss 2.302648   Top1 10.027344   Top5 50.183594   BatchTime 0.285312   LR 0.001246
INFO - Training [31][  120/  196]   Loss 2.302681   Top1 10.019531   Top5 50.172526   BatchTime 0.279745   LR 0.001245
INFO - Training [31][  140/  196]   Loss 2.302677   Top1 9.994420   Top5 50.256696   BatchTime 0.275070   LR 0.001244
INFO - Training [31][  160/  196]   Loss 2.302741   Top1 9.892578   Top5 50.124512   BatchTime 0.273878   LR 0.001244
INFO - Training [31][  180/  196]   Loss 2.302758   Top1 9.884983   Top5 50.091146   BatchTime 0.269883   LR 0.001243
INFO - ==> Top1: 9.928    Top5: 50.000    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [31][   20/   40]   Loss 2.302587   Top1 9.765625   Top5 50.000000   BatchTime 0.135567
INFO - Validation [31][   40/   40]   Loss 2.302628   Top1 10.000000   Top5 50.000000   BatchTime 0.094717
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [32][   20/  196]   Loss 2.303090   Top1 9.726562   Top5 50.234375   BatchTime 0.342604   LR 0.001242
INFO - Training [32][   40/  196]   Loss 2.303097   Top1 9.472656   Top5 49.492188   BatchTime 0.294613   LR 0.001241
INFO - Training [32][   60/  196]   Loss 2.302914   Top1 9.726562   Top5 49.609375   BatchTime 0.283103   LR 0.001240
INFO - Training [32][   80/  196]   Loss 2.302966   Top1 9.692383   Top5 49.355469   BatchTime 0.284748   LR 0.001239
INFO - Training [32][  100/  196]   Loss 2.302994   Top1 9.667969   Top5 49.222656   BatchTime 0.281174   LR 0.001238
INFO - Training [32][  120/  196]   Loss 2.302924   Top1 9.918620   Top5 49.462891   BatchTime 0.274755   LR 0.001237
INFO - Training [32][  140/  196]   Loss 2.302942   Top1 9.899554   Top5 49.444754   BatchTime 0.270436   LR 0.001236
INFO - Training [32][  160/  196]   Loss 2.302909   Top1 9.929199   Top5 49.506836   BatchTime 0.268045   LR 0.001235
INFO - Training [32][  180/  196]   Loss 2.302904   Top1 9.895833   Top5 49.526910   BatchTime 0.265084   LR 0.001234
INFO - ==> Top1: 9.880    Top5: 49.452    Loss: 2.303
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 2.302586   Top1 9.765625   Top5 50.292969   BatchTime 0.135850
INFO - Validation [32][   40/   40]   Loss 2.302641   Top1 10.000000   Top5 50.000000   BatchTime 0.096082
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [33][   20/  196]   Loss 2.302754   Top1 9.667969   Top5 50.156250   BatchTime 0.366782   LR 0.001232
INFO - Training [33][   40/  196]   Loss 2.302831   Top1 9.853516   Top5 49.609375   BatchTime 0.315254   LR 0.001230
INFO - Training [33][   60/  196]   Loss 2.302812   Top1 9.843750   Top5 49.752604   BatchTime 0.299366   LR 0.001229
INFO - Training [33][   80/  196]   Loss 2.302835   Top1 9.882812   Top5 49.877930   BatchTime 0.292834   LR 0.001228
INFO - Training [33][  100/  196]   Loss 2.302853   Top1 9.722656   Top5 49.730469   BatchTime 0.284800   LR 0.001226
INFO - Training [33][  120/  196]   Loss 2.302866   Top1 9.638672   Top5 49.687500   BatchTime 0.276804   LR 0.001225
INFO - Training [33][  140/  196]   Loss 2.302843   Top1 9.729353   Top5 49.762835   BatchTime 0.274207   LR 0.001224
INFO - Training [33][  160/  196]   Loss 2.302840   Top1 9.736328   Top5 49.768066   BatchTime 0.278312   LR 0.001222
INFO - Training [33][  180/  196]   Loss 2.302810   Top1 9.754774   Top5 49.856771   BatchTime 0.278270   LR 0.001221
INFO - ==> Top1: 9.752    Top5: 49.872    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 2.302570   Top1 10.019531   Top5 50.292969   BatchTime 0.154534
INFO - Validation [33][   40/   40]   Loss 2.302660   Top1 10.000000   Top5 50.000000   BatchTime 0.103963
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [34][   20/  196]   Loss 2.302575   Top1 10.253906   Top5 50.000000   BatchTime 0.351977   LR 0.001218
INFO - Training [34][   40/  196]   Loss 2.302601   Top1 10.126953   Top5 49.726562   BatchTime 0.300385   LR 0.001216
INFO - Training [34][   60/  196]   Loss 2.302598   Top1 10.273438   Top5 49.934896   BatchTime 0.287361   LR 0.001215
INFO - Training [34][   80/  196]   Loss 2.302618   Top1 10.234375   Top5 49.853516   BatchTime 0.279079   LR 0.001213
INFO - Training [34][  100/  196]   Loss 2.302721   Top1 10.082031   Top5 49.621094   BatchTime 0.274845   LR 0.001211
INFO - Training [34][  120/  196]   Loss 2.302744   Top1 9.990234   Top5 49.713542   BatchTime 0.272727   LR 0.001209
INFO - Training [34][  140/  196]   Loss 2.302750   Top1 10.002790   Top5 49.684710   BatchTime 0.269495   LR 0.001208
INFO - Training [34][  160/  196]   Loss 2.302722   Top1 10.031738   Top5 49.753418   BatchTime 0.268766   LR 0.001206
INFO - Training [34][  180/  196]   Loss 2.302757   Top1 10.019531   Top5 49.702691   BatchTime 0.267004   LR 0.001204
INFO - ==> Top1: 10.054    Top5: 49.792    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [34][   20/   40]   Loss 2.302558   Top1 10.253906   Top5 50.312500   BatchTime 0.200377
INFO - Validation [34][   40/   40]   Loss 2.302630   Top1 10.000000   Top5 50.000000   BatchTime 0.136491
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [35][   20/  196]   Loss 2.303051   Top1 9.433594   Top5 48.769531   BatchTime 0.389239   LR 0.001201
INFO - Training [35][   40/  196]   Loss 2.302860   Top1 9.550781   Top5 49.248047   BatchTime 0.321050   LR 0.001199
INFO - Training [35][   60/  196]   Loss 2.302845   Top1 9.726562   Top5 49.472656   BatchTime 0.301214   LR 0.001197
INFO - Training [35][   80/  196]   Loss 2.302847   Top1 9.736328   Top5 49.418945   BatchTime 0.288128   LR 0.001195
INFO - Training [35][  100/  196]   Loss 2.302812   Top1 9.753906   Top5 49.558594   BatchTime 0.282026   LR 0.001192
INFO - Training [35][  120/  196]   Loss 2.302773   Top1 9.882812   Top5 49.661458   BatchTime 0.278228   LR 0.001190
INFO - Training [35][  140/  196]   Loss 2.302816   Top1 9.846540   Top5 49.584263   BatchTime 0.275200   LR 0.001188
INFO - Training [35][  160/  196]   Loss 2.302862   Top1 9.826660   Top5 49.433594   BatchTime 0.272128   LR 0.001186
INFO - Training [35][  180/  196]   Loss 2.302851   Top1 9.898003   Top5 49.505208   BatchTime 0.270789   LR 0.001184
********************pre-trained*****************
INFO - ==> Top1: 9.924    Top5: 49.630    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 2.302622   Top1 10.019531   Top5 50.234375   BatchTime 0.129076
INFO - Validation [35][   40/   40]   Loss 2.302702   Top1 10.000000   Top5 50.000000   BatchTime 0.092746
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [36][   20/  196]   Loss 2.302913   Top1 9.238281   Top5 49.843750   BatchTime 0.324844   LR 0.001180
INFO - Training [36][   40/  196]   Loss 2.302706   Top1 9.853516   Top5 50.019531   BatchTime 0.294570   LR 0.001177
INFO - Training [36][   60/  196]   Loss 2.302704   Top1 9.837240   Top5 49.785156   BatchTime 0.294272   LR 0.001175
INFO - Training [36][   80/  196]   Loss 2.302676   Top1 9.995117   Top5 50.122070   BatchTime 0.284824   LR 0.001173
INFO - Training [36][  100/  196]   Loss 2.302751   Top1 10.000000   Top5 49.949219   BatchTime 0.279133   LR 0.001170
INFO - Training [36][  120/  196]   Loss 2.302749   Top1 10.022786   Top5 50.019531   BatchTime 0.275470   LR 0.001168
INFO - Training [36][  140/  196]   Loss 2.302749   Top1 10.027902   Top5 49.980469   BatchTime 0.273939   LR 0.001165
INFO - Training [36][  160/  196]   Loss 2.302763   Top1 10.009766   Top5 49.914551   BatchTime 0.272649   LR 0.001163
INFO - Training [36][  180/  196]   Loss 2.302764   Top1 9.908854   Top5 49.837240   BatchTime 0.271962   LR 0.001160
INFO - ==> Top1: 9.890    Top5: 49.804    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 2.302616   Top1 10.078125   Top5 50.234375   BatchTime 0.139287
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Validation [36][   40/   40]   Loss 2.302700   Top1 10.000000   Top5 50.000000   BatchTime 0.096631
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [37][   20/  196]   Loss 2.302792   Top1 10.000000   Top5 49.843750   BatchTime 0.365780   LR 0.001155
INFO - Training [37][   40/  196]   Loss 2.302744   Top1 9.804688   Top5 50.009766   BatchTime 0.316257   LR 0.001153
INFO - Training [37][   60/  196]   Loss 2.302818   Top1 9.746094   Top5 49.895833   BatchTime 0.294631   LR 0.001150
INFO - Training [37][   80/  196]   Loss 2.302803   Top1 9.882812   Top5 49.853516   BatchTime 0.286907   LR 0.001147
INFO - Training [37][  100/  196]   Loss 2.302805   Top1 9.792969   Top5 49.855469   BatchTime 0.290321   LR 0.001144
INFO - Training [37][  120/  196]   Loss 2.302826   Top1 9.876302   Top5 49.853516   BatchTime 0.283558   LR 0.001142
INFO - Training [37][  140/  196]   Loss 2.302856   Top1 9.790737   Top5 49.584263   BatchTime 0.278297   LR 0.001139
INFO - Training [37][  160/  196]   Loss 2.302842   Top1 9.746094   Top5 49.631348   BatchTime 0.275582   LR 0.001136
INFO - Training [37][  180/  196]   Loss 2.302803   Top1 9.809028   Top5 49.637587   BatchTime 0.273612   LR 0.001133
INFO - ==> Top1: 9.788    Top5: 49.668    Loss: 2.303
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [37][   20/   40]   Loss 2.302630   Top1 9.765625   Top5 49.707031   BatchTime 0.129976
INFO - Validation [37][   40/   40]   Loss 2.302599   Top1 10.000000   Top5 50.000000   BatchTime 0.090640
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [38][   20/  196]   Loss 2.302744   Top1 9.628906   Top5 50.312500   BatchTime 0.334225   LR 0.001128
INFO - Training [38][   40/  196]   Loss 2.302571   Top1 10.097656   Top5 50.937500   BatchTime 0.294001   LR 0.001125
INFO - Training [38][   60/  196]   Loss 2.302657   Top1 9.876302   Top5 50.397135   BatchTime 0.282663   LR 0.001122
INFO - Training [38][   80/  196]   Loss 2.302658   Top1 9.902344   Top5 50.375977   BatchTime 0.277831   LR 0.001119
INFO - Training [38][  100/  196]   Loss 2.302690   Top1 9.957031   Top5 50.503906   BatchTime 0.270660   LR 0.001116
INFO - Training [38][  120/  196]   Loss 2.302711   Top1 9.960938   Top5 50.387370   BatchTime 0.268702   LR 0.001112
INFO - Training [38][  140/  196]   Loss 2.302727   Top1 9.921875   Top5 50.371094   BatchTime 0.268606   LR 0.001109
INFO - Training [38][  160/  196]   Loss 2.302787   Top1 9.875488   Top5 50.124512   BatchTime 0.268490   LR 0.001106
INFO - Training [38][  180/  196]   Loss 2.302764   Top1 9.852431   Top5 50.177951   BatchTime 0.267982   LR 0.001103
INFO - ==> Top1: 9.864    Top5: 50.176    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [38][   20/   40]   Loss 2.302642   Top1 10.019531   Top5 50.234375   BatchTime 0.130979
INFO - Validation [38][   40/   40]   Loss 2.302701   Top1 10.000000   Top5 50.000000   BatchTime 0.091477
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [39][   20/  196]   Loss 2.302713   Top1 9.980469   Top5 49.394531   BatchTime 0.373862   LR 0.001097
INFO - Training [39][   40/  196]   Loss 2.302708   Top1 9.843750   Top5 49.736328   BatchTime 0.313628   LR 0.001094
INFO - Training [39][   60/  196]   Loss 2.302649   Top1 9.759115   Top5 49.980469   BatchTime 0.297277   LR 0.001090
INFO - Training [39][   80/  196]   Loss 2.302682   Top1 9.814453   Top5 49.736328   BatchTime 0.284120   LR 0.001087
INFO - Training [39][  100/  196]   Loss 2.302679   Top1 9.855469   Top5 49.777344   BatchTime 0.282417   LR 0.001084
INFO - Training [39][  120/  196]   Loss 2.302735   Top1 9.931641   Top5 49.700521   BatchTime 0.276838   LR 0.001080
INFO - Training [39][  140/  196]   Loss 2.302744   Top1 9.919085   Top5 49.511719   BatchTime 0.273850   LR 0.001077
INFO - Training [39][  160/  196]   Loss 2.302809   Top1 9.819336   Top5 49.545898   BatchTime 0.270109   LR 0.001073
INFO - Training [39][  180/  196]   Loss 2.302790   Top1 9.869792   Top5 49.565972   BatchTime 0.268754   LR 0.001070
INFO - ==> Top1: 9.880    Top5: 49.592    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [39][   20/   40]   Loss 2.302619   Top1 10.078125   Top5 50.234375   BatchTime 0.153141
INFO - Validation [39][   40/   40]   Loss 2.302706   Top1 10.000000   Top5 50.000000   BatchTime 0.126168
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3208.) 2188896.0
INFO - Training [40][   20/  196]   Loss 2.302689   Top1 9.824219   Top5 50.156250   BatchTime 0.378454   LR 0.001064
INFO - Training [40][   40/  196]   Loss 2.302645   Top1 9.843750   Top5 50.253906   BatchTime 0.317142   LR 0.001060
INFO - Training [40][   60/  196]   Loss 2.302756   Top1 9.863281   Top5 50.084635   BatchTime 0.303606   LR 0.001056
INFO - Training [40][   80/  196]   Loss 2.302664   Top1 9.980469   Top5 50.170898   BatchTime 0.294591   LR 0.001053
INFO - Training [40][  100/  196]   Loss 2.302662   Top1 9.996094   Top5 50.355469   BatchTime 0.288176   LR 0.001049
INFO - Training [40][  120/  196]   Loss 2.302696   Top1 9.967448   Top5 50.205078   BatchTime 0.282762   LR 0.001045
INFO - Training [40][  140/  196]   Loss 2.302695   Top1 9.946987   Top5 50.184152   BatchTime 0.277625   LR 0.001042
INFO - Training [40][  160/  196]   Loss 2.302692   Top1 9.960938   Top5 50.212402   BatchTime 0.274481   LR 0.001038
INFO - Training [40][  180/  196]   Loss 2.302722   Top1 9.947917   Top5 50.199653   BatchTime 0.270430   LR 0.001034
INFO - ==> Top1: 9.916    Top5: 50.158    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [40][   20/   40]   Loss 2.302595   Top1 10.078125   Top5 50.234375   BatchTime 0.153922
INFO - Validation [40][   40/   40]   Loss 2.302692   Top1 10.000000   Top5 50.000000   BatchTime 0.103596
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [41][   20/  196]   Loss 2.302854   Top1 9.492188   Top5 50.996094   BatchTime 0.378191   LR 0.001027
INFO - Training [41][   40/  196]   Loss 2.302995   Top1 9.326172   Top5 49.960938   BatchTime 0.322039   LR 0.001023
INFO - Training [41][   60/  196]   Loss 2.302872   Top1 9.531250   Top5 49.967448   BatchTime 0.302796   LR 0.001020
INFO - Training [41][   80/  196]   Loss 2.302863   Top1 9.624023   Top5 49.863281   BatchTime 0.289230   LR 0.001016
INFO - Training [41][  100/  196]   Loss 2.302916   Top1 9.566406   Top5 49.699219   BatchTime 0.284271   LR 0.001012
INFO - Training [41][  120/  196]   Loss 2.302898   Top1 9.567057   Top5 49.658203   BatchTime 0.284942   LR 0.001008
INFO - Training [41][  140/  196]   Loss 2.302885   Top1 9.581473   Top5 49.718192   BatchTime 0.278986   LR 0.001004
INFO - Training [41][  160/  196]   Loss 2.302856   Top1 9.560547   Top5 49.777832   BatchTime 0.277349   LR 0.001000
INFO - Training [41][  180/  196]   Loss 2.302849   Top1 9.511719   Top5 49.854601   BatchTime 0.274741   LR 0.000996
INFO - ==> Top1: 9.550    Top5: 49.852    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [41][   20/   40]   Loss 2.302613   Top1 10.078125   Top5 50.312500   BatchTime 0.143240
INFO - Validation [41][   40/   40]   Loss 2.302696   Top1 10.000000   Top5 50.000000   BatchTime 0.097567
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [42][   20/  196]   Loss 2.302840   Top1 10.175781   Top5 49.746094   BatchTime 0.345724   LR 0.000988
INFO - Training [42][   40/  196]   Loss 2.302839   Top1 9.677734   Top5 50.009766   BatchTime 0.302425   LR 0.000984
INFO - Training [42][   60/  196]   Loss 2.302922   Top1 9.583333   Top5 49.518229   BatchTime 0.291263   LR 0.000980
INFO - Training [42][   80/  196]   Loss 2.302826   Top1 9.780273   Top5 49.785156   BatchTime 0.288196   LR 0.000976
INFO - Training [42][  100/  196]   Loss 2.302898   Top1 9.609375   Top5 49.773438   BatchTime 0.281962   LR 0.000972
INFO - Training [42][  120/  196]   Loss 2.302900   Top1 9.765625   Top5 49.833984   BatchTime 0.278434   LR 0.000968
INFO - Training [42][  140/  196]   Loss 2.302904   Top1 9.773996   Top5 49.782366   BatchTime 0.274660   LR 0.000964
INFO - Training [42][  160/  196]   Loss 2.302895   Top1 9.790039   Top5 49.750977   BatchTime 0.271526   LR 0.000959
INFO - Training [42][  180/  196]   Loss 2.302905   Top1 9.793837   Top5 49.787326   BatchTime 0.268084   LR 0.000955
INFO - ==> Top1: 9.794    Top5: 49.720    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [42][   20/   40]   Loss 2.302582   Top1 10.253906   Top5 50.234375   BatchTime 0.147156
INFO - Validation [42][   40/   40]   Loss 2.302679   Top1 10.000000   Top5 50.000000   BatchTime 0.099722
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [43][   20/  196]   Loss 2.302761   Top1 10.292969   Top5 50.273438   BatchTime 0.399272   LR 0.000947
INFO - Training [43][   40/  196]   Loss 2.302854   Top1 10.175781   Top5 49.609375   BatchTime 0.324133   LR 0.000943
INFO - Training [43][   60/  196]   Loss 2.302787   Top1 10.182292   Top5 49.772135   BatchTime 0.304571   LR 0.000939
INFO - Training [43][   80/  196]   Loss 2.302787   Top1 9.975586   Top5 49.760742   BatchTime 0.294098   LR 0.000934
INFO - Training [43][  100/  196]   Loss 2.302748   Top1 9.968750   Top5 49.765625   BatchTime 0.290731   LR 0.000930
INFO - Training [43][  120/  196]   Loss 2.302765   Top1 10.052083   Top5 49.752604   BatchTime 0.284703   LR 0.000926
INFO - Training [43][  140/  196]   Loss 2.302773   Top1 10.080915   Top5 49.771205   BatchTime 0.281050   LR 0.000921
INFO - Training [43][  160/  196]   Loss 2.302782   Top1 9.968262   Top5 49.731445   BatchTime 0.278923   LR 0.000917
INFO - Training [43][  180/  196]   Loss 2.302768   Top1 10.006510   Top5 49.796007   BatchTime 0.274060   LR 0.000912
INFO - ==> Top1: 9.980    Top5: 49.740    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [43][   20/   40]   Loss 2.302607   Top1 10.078125   Top5 50.234375   BatchTime 0.151733
INFO - Validation [43][   40/   40]   Loss 2.302701   Top1 10.000000   Top5 50.000000   BatchTime 0.101966
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [44][   20/  196]   Loss 2.302710   Top1 10.000000   Top5 49.824219   BatchTime 0.342402   LR 0.000904
INFO - Training [44][   40/  196]   Loss 2.302831   Top1 9.970703   Top5 49.482422   BatchTime 0.298472   LR 0.000900
INFO - Training [44][   60/  196]   Loss 2.302662   Top1 10.058594   Top5 50.136719   BatchTime 0.280417   LR 0.000895
INFO - Training [44][   80/  196]   Loss 2.302689   Top1 10.029297   Top5 50.200195   BatchTime 0.272701   LR 0.000891
INFO - Training [44][  100/  196]   Loss 2.302617   Top1 10.167969   Top5 50.359375   BatchTime 0.268950   LR 0.000886
INFO - Training [44][  120/  196]   Loss 2.302655   Top1 10.126953   Top5 50.315755   BatchTime 0.265061   LR 0.000882
INFO - Training [44][  140/  196]   Loss 2.302721   Top1 10.106027   Top5 50.170201   BatchTime 0.262211   LR 0.000877
INFO - Training [44][  160/  196]   Loss 2.302751   Top1 10.000000   Top5 50.122070   BatchTime 0.261781   LR 0.000873
INFO - Training [44][  180/  196]   Loss 2.302760   Top1 9.997830   Top5 50.030382   BatchTime 0.262352   LR 0.000868
INFO - ==> Top1: 10.002    Top5: 50.208    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [44][   20/   40]   Loss 2.302977   Top1 9.882812   Top5 49.765625   BatchTime 0.142440
INFO - Validation [44][   40/   40]   Loss 2.302855   Top1 10.000000   Top5 50.000000   BatchTime 0.098018
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [45][   20/  196]   Loss 2.302815   Top1 10.195312   Top5 50.566406   BatchTime 0.355394   LR 0.000860
INFO - Training [45][   40/  196]   Loss 2.302767   Top1 10.185547   Top5 50.234375   BatchTime 0.304390   LR 0.000855
INFO - Training [45][   60/  196]   Loss 2.302735   Top1 9.973958   Top5 50.214844   BatchTime 0.287530   LR 0.000850
INFO - Training [45][   80/  196]   Loss 2.302701   Top1 9.960938   Top5 50.341797   BatchTime 0.279564   LR 0.000846
INFO - Training [45][  100/  196]   Loss 2.302746   Top1 9.828125   Top5 50.191406   BatchTime 0.274561   LR 0.000841
INFO - Training [45][  120/  196]   Loss 2.302727   Top1 9.833984   Top5 50.188802   BatchTime 0.270296   LR 0.000836
INFO - Training [45][  140/  196]   Loss 2.302745   Top1 9.860491   Top5 50.086496   BatchTime 0.268419   LR 0.000832
INFO - Training [45][  160/  196]   Loss 2.302787   Top1 9.843750   Top5 49.860840   BatchTime 0.267822   LR 0.000827
INFO - Training [45][  180/  196]   Loss 2.302770   Top1 9.871962   Top5 49.865451   BatchTime 0.266213   LR 0.000822
INFO - ==> Top1: 9.862    Top5: 49.842    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [45][   20/   40]   Loss 2.302584   Top1 10.078125   Top5 50.332031   BatchTime 0.146401
INFO - Validation [45][   40/   40]   Loss 2.302637   Top1 10.000000   Top5 50.000000   BatchTime 0.099706
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [46][   20/  196]   Loss 2.302599   Top1 10.664062   Top5 50.156250   BatchTime 0.359867   LR 0.000814
INFO - Training [46][   40/  196]   Loss 2.302720   Top1 9.990234   Top5 49.804688   BatchTime 0.307037   LR 0.000809
INFO - Training [46][   60/  196]   Loss 2.302618   Top1 9.973958   Top5 50.136719   BatchTime 0.294719   LR 0.000804
INFO - Training [46][   80/  196]   Loss 2.302651   Top1 9.941406   Top5 49.951172   BatchTime 0.283876   LR 0.000799
INFO - Training [46][  100/  196]   Loss 2.302681   Top1 9.949219   Top5 49.835938   BatchTime 0.277153   LR 0.000794
INFO - Training [46][  120/  196]   Loss 2.302719   Top1 9.840495   Top5 49.794922   BatchTime 0.271977   LR 0.000789
INFO - Training [46][  140/  196]   Loss 2.302697   Top1 9.866071   Top5 49.907924   BatchTime 0.268041   LR 0.000785
INFO - Training [46][  160/  196]   Loss 2.302687   Top1 9.990234   Top5 49.948730   BatchTime 0.265300   LR 0.000780
INFO - Training [46][  180/  196]   Loss 2.302679   Top1 10.043403   Top5 49.889323   BatchTime 0.263290   LR 0.000775
INFO - ==> Top1: 9.994    Top5: 49.912    Loss: 2.303
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [46][   20/   40]   Loss 2.302580   Top1 10.078125   Top5 50.234375   BatchTime 0.147651
INFO - Validation [46][   40/   40]   Loss 2.302633   Top1 10.000000   Top5 50.000000   BatchTime 0.099849
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [47][   20/  196]   Loss 2.302705   Top1 9.531250   Top5 49.902344   BatchTime 0.369128   LR 0.000766
INFO - Training [47][   40/  196]   Loss 2.302817   Top1 9.687500   Top5 49.873047   BatchTime 0.311078   LR 0.000761
INFO - Training [47][   60/  196]   Loss 2.302830   Top1 9.785156   Top5 49.453125   BatchTime 0.294367   LR 0.000756
INFO - Training [47][   80/  196]   Loss 2.302854   Top1 9.946289   Top5 49.628906   BatchTime 0.284079   LR 0.000752
INFO - Training [47][  100/  196]   Loss 2.302870   Top1 9.847656   Top5 49.609375   BatchTime 0.280734   LR 0.000747
INFO - Training [47][  120/  196]   Loss 2.302869   Top1 9.778646   Top5 49.563802   BatchTime 0.278829   LR 0.000742
INFO - Training [47][  140/  196]   Loss 2.302845   Top1 9.760045   Top5 49.592634   BatchTime 0.273963   LR 0.000737
INFO - Training [47][  160/  196]   Loss 2.302799   Top1 9.809570   Top5 49.692383   BatchTime 0.270936   LR 0.000732
INFO - Training [47][  180/  196]   Loss 2.302802   Top1 9.769965   Top5 49.654948   BatchTime 0.268965   LR 0.000727
********************pre-trained*****************
INFO - ==> Top1: 9.808    Top5: 49.704    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [47][   20/   40]   Loss 2.302619   Top1 9.882812   Top5 49.960938   BatchTime 0.153803
INFO - Validation [47][   40/   40]   Loss 2.302605   Top1 10.000000   Top5 50.000000   BatchTime 0.104731
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [48][   20/  196]   Loss 2.302872   Top1 9.863281   Top5 49.550781   BatchTime 0.351076   LR 0.000718
INFO - Training [48][   40/  196]   Loss 2.302803   Top1 9.746094   Top5 49.384766   BatchTime 0.298139   LR 0.000713
INFO - Training [48][   60/  196]   Loss 2.302872   Top1 9.628906   Top5 49.225260   BatchTime 0.282507   LR 0.000708
INFO - Training [48][   80/  196]   Loss 2.302844   Top1 9.692383   Top5 49.165039   BatchTime 0.274484   LR 0.000703
INFO - Training [48][  100/  196]   Loss 2.302837   Top1 9.851562   Top5 49.300781   BatchTime 0.270939   LR 0.000698
INFO - Training [48][  120/  196]   Loss 2.302819   Top1 9.807943   Top5 49.401042   BatchTime 0.269137   LR 0.000693
INFO - Training [48][  140/  196]   Loss 2.302797   Top1 9.935826   Top5 49.372210   BatchTime 0.267499   LR 0.000688
INFO - Training [48][  160/  196]   Loss 2.302823   Top1 9.833984   Top5 49.213867   BatchTime 0.265581   LR 0.000683
INFO - Training [48][  180/  196]   Loss 2.302815   Top1 9.780816   Top5 49.275174   BatchTime 0.265196   LR 0.000678
INFO - ==> Top1: 9.748    Top5: 49.320    Loss: 2.303
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [48][   20/   40]   Loss 2.302667   Top1 9.863281   Top5 49.765625   BatchTime 0.164664
INFO - Validation [48][   40/   40]   Loss 2.302627   Top1 10.000000   Top5 50.000000   BatchTime 0.109663
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [49][   20/  196]   Loss 2.302765   Top1 9.687500   Top5 50.390625   BatchTime 0.365248   LR 0.000669
INFO - Training [49][   40/  196]   Loss 2.302752   Top1 9.726562   Top5 50.224609   BatchTime 0.310323   LR 0.000664
INFO - Training [49][   60/  196]   Loss 2.302760   Top1 9.739583   Top5 50.000000   BatchTime 0.287434   LR 0.000659
INFO - Training [49][   80/  196]   Loss 2.302712   Top1 9.799805   Top5 50.317383   BatchTime 0.277324   LR 0.000654
INFO - Training [49][  100/  196]   Loss 2.302755   Top1 9.726562   Top5 50.109375   BatchTime 0.271387   LR 0.000649
INFO - Training [49][  120/  196]   Loss 2.302710   Top1 9.804688   Top5 50.218099   BatchTime 0.268663   LR 0.000644
INFO - Training [49][  140/  196]   Loss 2.302732   Top1 9.765625   Top5 50.066964   BatchTime 0.266622   LR 0.000639
INFO - Training [49][  160/  196]   Loss 2.302748   Top1 9.731445   Top5 49.946289   BatchTime 0.263283   LR 0.000634
INFO - Training [49][  180/  196]   Loss 2.302736   Top1 9.754774   Top5 49.932726   BatchTime 0.261461   LR 0.000629
INFO - ==> Top1: 9.772    Top5: 49.910    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [49][   20/   40]   Loss 2.302647   Top1 9.863281   Top5 49.765625   BatchTime 0.142611
INFO - Validation [49][   40/   40]   Loss 2.302615   Top1 10.000000   Top5 50.000000   BatchTime 0.097517
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [50][   20/  196]   Loss 2.302644   Top1 9.746094   Top5 50.488281   BatchTime 0.339744   LR 0.000620
INFO - Training [50][   40/  196]   Loss 2.302593   Top1 10.175781   Top5 50.253906   BatchTime 0.299981   LR 0.000615
INFO - Training [50][   60/  196]   Loss 2.302603   Top1 10.039062   Top5 50.403646   BatchTime 0.282454   LR 0.000610
INFO - Training [50][   80/  196]   Loss 2.302610   Top1 10.029297   Top5 50.371094   BatchTime 0.274081   LR 0.000605
INFO - Training [50][  100/  196]   Loss 2.302661   Top1 9.921875   Top5 50.140625   BatchTime 0.270096   LR 0.000600
INFO - Training [50][  120/  196]   Loss 2.302701   Top1 9.850260   Top5 49.973958   BatchTime 0.266904   LR 0.000595
INFO - Training [50][  140/  196]   Loss 2.302673   Top1 9.907924   Top5 50.019531   BatchTime 0.263749   LR 0.000590
INFO - Training [50][  160/  196]   Loss 2.302658   Top1 9.929199   Top5 50.046387   BatchTime 0.262597   LR 0.000585
INFO - Training [50][  180/  196]   Loss 2.302659   Top1 9.865451   Top5 50.049913   BatchTime 0.262036   LR 0.000580
INFO - ==> Top1: 9.910    Top5: 50.050    Loss: 2.303
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [50][   20/   40]   Loss 2.302820   Top1 9.882812   Top5 49.765625   BatchTime 0.142065
INFO - Validation [50][   40/   40]   Loss 2.302726   Top1 10.000000   Top5 50.000000   BatchTime 0.097287
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [51][   20/  196]   Loss 2.302590   Top1 10.195312   Top5 50.312500   BatchTime 0.362781   LR 0.000571
INFO - Training [51][   40/  196]   Loss 2.302695   Top1 10.039062   Top5 49.941406   BatchTime 0.304855   LR 0.000566
INFO - Training [51][   60/  196]   Loss 2.302619   Top1 9.986979   Top5 50.175781   BatchTime 0.288114   LR 0.000561
INFO - Training [51][   80/  196]   Loss 2.302636   Top1 9.946289   Top5 49.946289   BatchTime 0.274345   LR 0.000556
INFO - Training [51][  100/  196]   Loss 2.302600   Top1 10.066406   Top5 49.957031   BatchTime 0.268773   LR 0.000551
INFO - Training [51][  120/  196]   Loss 2.302668   Top1 9.951172   Top5 49.781901   BatchTime 0.266778   LR 0.000546
INFO - Training [51][  140/  196]   Loss 2.302701   Top1 9.891183   Top5 49.746094   BatchTime 0.264883   LR 0.000541
INFO - Training [51][  160/  196]   Loss 2.302711   Top1 9.890137   Top5 49.707031   BatchTime 0.262612   LR 0.000536
INFO - Training [51][  180/  196]   Loss 2.302693   Top1 9.915365   Top5 49.763455   BatchTime 0.259315   LR 0.000531
INFO - ==> Top1: 9.948    Top5: 49.886    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [51][   20/   40]   Loss 2.302777   Top1 9.882812   Top5 50.039062   BatchTime 0.152379
INFO - Validation [51][   40/   40]   Loss 2.302696   Top1 10.000000   Top5 50.000000   BatchTime 0.101672
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [52][   20/  196]   Loss 2.302701   Top1 9.453125   Top5 49.707031   BatchTime 0.354330   LR 0.000523
INFO - Training [52][   40/  196]   Loss 2.302684   Top1 9.482422   Top5 49.677734   BatchTime 0.302469   LR 0.000518
INFO - Training [52][   60/  196]   Loss 2.302659   Top1 9.680990   Top5 50.117188   BatchTime 0.284045   LR 0.000513
INFO - Training [52][   80/  196]   Loss 2.302651   Top1 9.682617   Top5 50.112305   BatchTime 0.274972   LR 0.000508
INFO - Training [52][  100/  196]   Loss 2.302606   Top1 9.812500   Top5 50.332031   BatchTime 0.271356   LR 0.000503
INFO - Training [52][  120/  196]   Loss 2.302614   Top1 9.804688   Top5 50.315755   BatchTime 0.267564   LR 0.000498
INFO - Training [52][  140/  196]   Loss 2.302624   Top1 9.827009   Top5 50.304129   BatchTime 0.265546   LR 0.000493
INFO - Training [52][  160/  196]   Loss 2.302639   Top1 9.741211   Top5 50.227051   BatchTime 0.263432   LR 0.000488
INFO - Training [52][  180/  196]   Loss 2.302649   Top1 9.733073   Top5 50.075955   BatchTime 0.261784   LR 0.000483
INFO - ==> Top1: 9.796    Top5: 50.100    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [52][   20/   40]   Loss 2.302590   Top1 10.019531   Top5 50.039062   BatchTime 0.146821
INFO - Validation [52][   40/   40]   Loss 2.302590   Top1 10.000000   Top5 50.000000   BatchTime 0.100479
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [53][   20/  196]   Loss 2.302626   Top1 10.351562   Top5 49.570312   BatchTime 0.395678   LR 0.000474
INFO - Training [53][   40/  196]   Loss 2.302726   Top1 10.195312   Top5 49.599609   BatchTime 0.323326   LR 0.000470
INFO - Training [53][   60/  196]   Loss 2.302721   Top1 10.084635   Top5 49.544271   BatchTime 0.294791   LR 0.000465
INFO - Training [53][   80/  196]   Loss 2.302678   Top1 10.058594   Top5 49.780273   BatchTime 0.282100   LR 0.000460
INFO - Training [53][  100/  196]   Loss 2.302658   Top1 9.988281   Top5 49.871094   BatchTime 0.276176   LR 0.000455
INFO - Training [53][  120/  196]   Loss 2.302693   Top1 9.886068   Top5 49.703776   BatchTime 0.274354   LR 0.000450
INFO - Training [53][  140/  196]   Loss 2.302707   Top1 9.799107   Top5 49.500558   BatchTime 0.270228   LR 0.000445
INFO - Training [53][  160/  196]   Loss 2.302702   Top1 9.726562   Top5 49.621582   BatchTime 0.268375   LR 0.000441
INFO - Training [53][  180/  196]   Loss 2.302728   Top1 9.674479   Top5 49.607205   BatchTime 0.266123   LR 0.000436
INFO - ==> Top1: 9.726    Top5: 49.578    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [53][   20/   40]   Loss 2.302773   Top1 9.882812   Top5 50.039062   BatchTime 0.152974
INFO - Validation [53][   40/   40]   Loss 2.302689   Top1 10.000000   Top5 50.000000   BatchTime 0.103198
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [54][   20/  196]   Loss 2.302722   Top1 9.667969   Top5 49.121094   BatchTime 0.351036   LR 0.000427
INFO - Training [54][   40/  196]   Loss 2.302819   Top1 9.609375   Top5 48.994141   BatchTime 0.298913   LR 0.000423
INFO - Training [54][   60/  196]   Loss 2.302727   Top1 9.791667   Top5 49.414062   BatchTime 0.282705   LR 0.000418
INFO - Training [54][   80/  196]   Loss 2.302707   Top1 9.902344   Top5 49.550781   BatchTime 0.277696   LR 0.000413
INFO - Training [54][  100/  196]   Loss 2.302647   Top1 9.957031   Top5 49.769531   BatchTime 0.273085   LR 0.000408
INFO - Training [54][  120/  196]   Loss 2.302646   Top1 9.892578   Top5 49.759115   BatchTime 0.269107   LR 0.000404
INFO - Training [54][  140/  196]   Loss 2.302671   Top1 9.857701   Top5 49.575893   BatchTime 0.266483   LR 0.000399
INFO - Training [54][  160/  196]   Loss 2.302671   Top1 9.895020   Top5 49.511719   BatchTime 0.264174   LR 0.000394
INFO - Training [54][  180/  196]   Loss 2.302683   Top1 9.832899   Top5 49.570312   BatchTime 0.264852   LR 0.000390
INFO - ==> Top1: 9.828    Top5: 49.638    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [54][   20/   40]   Loss 2.302739   Top1 9.882812   Top5 50.039062   BatchTime 0.148083
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [54][   40/   40]   Loss 2.302665   Top1 10.000000   Top5 50.000000   BatchTime 0.100310
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [55][   20/  196]   Loss 2.302750   Top1 9.589844   Top5 48.886719   BatchTime 0.371224   LR 0.000381
INFO - Training [55][   40/  196]   Loss 2.302711   Top1 9.824219   Top5 49.804688   BatchTime 0.311546   LR 0.000377
INFO - Training [55][   60/  196]   Loss 2.302611   Top1 10.175781   Top5 49.960938   BatchTime 0.290689   LR 0.000372
INFO - Training [55][   80/  196]   Loss 2.302673   Top1 10.126953   Top5 49.672852   BatchTime 0.283605   LR 0.000368
INFO - Training [55][  100/  196]   Loss 2.302684   Top1 10.097656   Top5 49.785156   BatchTime 0.276211   LR 0.000363
INFO - Training [55][  120/  196]   Loss 2.302671   Top1 9.986979   Top5 49.912109   BatchTime 0.273017   LR 0.000358
INFO - Training [55][  140/  196]   Loss 2.302654   Top1 9.988839   Top5 50.008371   BatchTime 0.270187   LR 0.000354
INFO - Training [55][  160/  196]   Loss 2.302664   Top1 10.009766   Top5 50.017090   BatchTime 0.268216   LR 0.000349
INFO - Training [55][  180/  196]   Loss 2.302679   Top1 10.013021   Top5 49.971788   BatchTime 0.265986   LR 0.000345
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 10.032    Top5: 49.954    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [55][   20/   40]   Loss 2.302753   Top1 9.882812   Top5 49.765625   BatchTime 0.137950
INFO - Validation [55][   40/   40]   Loss 2.302673   Top1 10.000000   Top5 50.000000   BatchTime 0.095578
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [56][   20/  196]   Loss 2.302677   Top1 10.019531   Top5 49.785156   BatchTime 0.343385   LR 0.000337
INFO - Training [56][   40/  196]   Loss 2.302781   Top1 9.960938   Top5 49.443359   BatchTime 0.302826   LR 0.000333
INFO - Training [56][   60/  196]   Loss 2.302650   Top1 10.299479   Top5 49.921875   BatchTime 0.288920   LR 0.000328
INFO - Training [56][   80/  196]   Loss 2.302641   Top1 10.253906   Top5 50.073242   BatchTime 0.282983   LR 0.000324
INFO - Training [56][  100/  196]   Loss 2.302631   Top1 10.113281   Top5 50.027344   BatchTime 0.278127   LR 0.000319
INFO - Training [56][  120/  196]   Loss 2.302647   Top1 10.104167   Top5 49.986979   BatchTime 0.273530   LR 0.000315
INFO - Training [56][  140/  196]   Loss 2.302656   Top1 10.097656   Top5 49.952567   BatchTime 0.272535   LR 0.000311
INFO - Training [56][  160/  196]   Loss 2.302659   Top1 10.134277   Top5 49.785156   BatchTime 0.270577   LR 0.000306
INFO - Training [56][  180/  196]   Loss 2.302650   Top1 10.110677   Top5 49.815538   BatchTime 0.268694   LR 0.000302
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 10.122    Top5: 49.766    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [56][   20/   40]   Loss 2.302748   Top1 9.882812   Top5 49.765625   BatchTime 0.157055
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [56][   40/   40]   Loss 2.302668   Top1 10.000000   Top5 50.000000   BatchTime 0.106425
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [57][   20/  196]   Loss 2.302835   Top1 10.156250   Top5 50.175781   BatchTime 0.365780   LR 0.000294
INFO - Training [57][   40/  196]   Loss 2.302814   Top1 9.951172   Top5 49.785156   BatchTime 0.318038   LR 0.000290
INFO - Training [57][   60/  196]   Loss 2.302717   Top1 10.058594   Top5 49.804688   BatchTime 0.299831   LR 0.000286
INFO - Training [57][   80/  196]   Loss 2.302689   Top1 9.951172   Top5 50.053711   BatchTime 0.294011   LR 0.000282
INFO - Training [57][  100/  196]   Loss 2.302728   Top1 9.812500   Top5 49.843750   BatchTime 0.288113   LR 0.000277
INFO - Training [57][  120/  196]   Loss 2.302742   Top1 9.820964   Top5 49.720052   BatchTime 0.282846   LR 0.000273
INFO - Training [57][  140/  196]   Loss 2.302734   Top1 9.882812   Top5 49.656808   BatchTime 0.280073   LR 0.000269
INFO - Training [57][  160/  196]   Loss 2.302731   Top1 9.860840   Top5 49.741211   BatchTime 0.278753   LR 0.000265
INFO - Training [57][  180/  196]   Loss 2.302752   Top1 9.839410   Top5 49.713542   BatchTime 0.276039   LR 0.000261
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 9.820    Top5: 49.672    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [57][   20/   40]   Loss 2.302749   Top1 9.882812   Top5 49.765625   BatchTime 0.146378
INFO - Validation [57][   40/   40]   Loss 2.302670   Top1 10.000000   Top5 50.000000   BatchTime 0.101146
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [58][   20/  196]   Loss 2.302671   Top1 9.863281   Top5 49.726562   BatchTime 0.315731   LR 0.000254
INFO - Training [58][   40/  196]   Loss 2.302666   Top1 9.960938   Top5 49.970703   BatchTime 0.284813   LR 0.000250
INFO - Training [58][   60/  196]   Loss 2.302638   Top1 9.895833   Top5 50.221354   BatchTime 0.272175   LR 0.000246
INFO - Training [58][   80/  196]   Loss 2.302639   Top1 10.014648   Top5 50.083008   BatchTime 0.265687   LR 0.000242
INFO - Training [58][  100/  196]   Loss 2.302621   Top1 9.953125   Top5 50.234375   BatchTime 0.263099   LR 0.000238
INFO - Training [58][  120/  196]   Loss 2.302606   Top1 10.081380   Top5 50.214844   BatchTime 0.262663   LR 0.000234
INFO - Training [58][  140/  196]   Loss 2.302628   Top1 10.005580   Top5 50.108817   BatchTime 0.260890   LR 0.000230
INFO - Training [58][  160/  196]   Loss 2.302635   Top1 10.034180   Top5 50.173340   BatchTime 0.260092   LR 0.000226
INFO - Training [58][  180/  196]   Loss 2.302652   Top1 9.926215   Top5 50.013021   BatchTime 0.259412   LR 0.000222
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 9.948    Top5: 49.974    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [58][   20/   40]   Loss 2.302741   Top1 9.882812   Top5 50.039062   BatchTime 0.134746
INFO - Validation [58][   40/   40]   Loss 2.302668   Top1 10.000000   Top5 50.000000   BatchTime 0.094966
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [59][   20/  196]   Loss 2.302804   Top1 9.472656   Top5 49.335938   BatchTime 0.361036   LR 0.000215
INFO - Training [59][   40/  196]   Loss 2.302840   Top1 9.443359   Top5 49.091797   BatchTime 0.306295   LR 0.000212
INFO - Training [59][   60/  196]   Loss 2.302827   Top1 9.479167   Top5 49.121094   BatchTime 0.287992   LR 0.000208
INFO - Training [59][   80/  196]   Loss 2.302751   Top1 9.721680   Top5 49.467773   BatchTime 0.276959   LR 0.000204
INFO - Training [59][  100/  196]   Loss 2.302727   Top1 9.746094   Top5 49.585938   BatchTime 0.278269   LR 0.000201
INFO - Training [59][  120/  196]   Loss 2.302713   Top1 9.648438   Top5 49.602865   BatchTime 0.273438   LR 0.000197
INFO - Training [59][  140/  196]   Loss 2.302700   Top1 9.709821   Top5 49.567522   BatchTime 0.269376   LR 0.000193
INFO - Training [59][  160/  196]   Loss 2.302687   Top1 9.689941   Top5 49.648438   BatchTime 0.267131   LR 0.000190
INFO - Training [59][  180/  196]   Loss 2.302690   Top1 9.674479   Top5 49.680990   BatchTime 0.264983   LR 0.000186
********************pre-trained*****************
INFO - ==> Top1: 9.700    Top5: 49.644    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [59][   20/   40]   Loss 2.302736   Top1 9.882812   Top5 50.039062   BatchTime 0.134631
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [59][   40/   40]   Loss 2.302665   Top1 10.000000   Top5 50.000000   BatchTime 0.094220
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [60][   20/  196]   Loss 2.302525   Top1 9.980469   Top5 50.468750   BatchTime 0.336118   LR 0.000180
INFO - Training [60][   40/  196]   Loss 2.302555   Top1 9.960938   Top5 50.449219   BatchTime 0.287826   LR 0.000176
INFO - Training [60][   60/  196]   Loss 2.302613   Top1 9.615885   Top5 50.097656   BatchTime 0.275068   LR 0.000173
INFO - Training [60][   80/  196]   Loss 2.302585   Top1 9.716797   Top5 50.234375   BatchTime 0.266767   LR 0.000169
INFO - Training [60][  100/  196]   Loss 2.302580   Top1 9.828125   Top5 50.312500   BatchTime 0.264227   LR 0.000166
INFO - Training [60][  120/  196]   Loss 2.302595   Top1 9.794922   Top5 50.172526   BatchTime 0.264927   LR 0.000162
INFO - Training [60][  140/  196]   Loss 2.302603   Top1 9.782366   Top5 50.086496   BatchTime 0.264786   LR 0.000159
INFO - Training [60][  160/  196]   Loss 2.302610   Top1 9.853516   Top5 50.087891   BatchTime 0.264977   LR 0.000156
INFO - Training [60][  180/  196]   Loss 2.302605   Top1 9.887153   Top5 50.004340   BatchTime 0.264139   LR 0.000152
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 9.888    Top5: 49.944    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [60][   20/   40]   Loss 2.302736   Top1 9.882812   Top5 49.765625   BatchTime 0.132153
INFO - Validation [60][   40/   40]   Loss 2.302664   Top1 10.000000   Top5 50.000000   BatchTime 0.092139
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [61][   20/  196]   Loss 2.302580   Top1 10.234375   Top5 50.097656   BatchTime 0.356294   LR 0.000147
INFO - Training [61][   40/  196]   Loss 2.302601   Top1 9.980469   Top5 49.912109   BatchTime 0.306920   LR 0.000143
INFO - Training [61][   60/  196]   Loss 2.302646   Top1 9.752604   Top5 49.648438   BatchTime 0.290106   LR 0.000140
INFO - Training [61][   80/  196]   Loss 2.302615   Top1 9.809570   Top5 49.863281   BatchTime 0.278791   LR 0.000137
INFO - Training [61][  100/  196]   Loss 2.302605   Top1 9.890625   Top5 49.832031   BatchTime 0.272905   LR 0.000134
INFO - Training [61][  120/  196]   Loss 2.302611   Top1 9.889323   Top5 49.833984   BatchTime 0.268825   LR 0.000131
INFO - Training [61][  140/  196]   Loss 2.302642   Top1 9.944196   Top5 49.804688   BatchTime 0.265774   LR 0.000128
INFO - Training [61][  160/  196]   Loss 2.302669   Top1 9.895020   Top5 49.746094   BatchTime 0.264426   LR 0.000125
INFO - Training [61][  180/  196]   Loss 2.302681   Top1 9.876302   Top5 49.730903   BatchTime 0.263424   LR 0.000122
********************pre-trained*****************
INFO - ==> Top1: 9.946    Top5: 49.734    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [61][   20/   40]   Loss 2.302736   Top1 9.882812   Top5 49.765625   BatchTime 0.129071
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [61][   40/   40]   Loss 2.302665   Top1 10.000000   Top5 50.000000   BatchTime 0.091253
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [62][   20/  196]   Loss 2.302576   Top1 9.785156   Top5 50.156250   BatchTime 0.355041   LR 0.000117
INFO - Training [62][   40/  196]   Loss 2.302593   Top1 9.941406   Top5 50.234375   BatchTime 0.294332   LR 0.000114
INFO - Training [62][   60/  196]   Loss 2.302517   Top1 10.104167   Top5 50.826823   BatchTime 0.284459   LR 0.000111
INFO - Training [62][   80/  196]   Loss 2.302580   Top1 9.921875   Top5 50.332031   BatchTime 0.279896   LR 0.000108
INFO - Training [62][  100/  196]   Loss 2.302649   Top1 9.792969   Top5 50.171875   BatchTime 0.276858   LR 0.000105
INFO - Training [62][  120/  196]   Loss 2.302592   Top1 9.817708   Top5 50.351562   BatchTime 0.272670   LR 0.000102
INFO - Training [62][  140/  196]   Loss 2.302605   Top1 9.874442   Top5 50.256696   BatchTime 0.272045   LR 0.000100
INFO - Training [62][  160/  196]   Loss 2.302622   Top1 9.802246   Top5 50.175781   BatchTime 0.271823   LR 0.000097
INFO - Training [62][  180/  196]   Loss 2.302631   Top1 9.848090   Top5 50.052083   BatchTime 0.269724   LR 0.000094
********************pre-trained*****************
INFO - ==> Top1: 9.876    Top5: 49.938    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [62][   20/   40]   Loss 2.302736   Top1 9.882812   Top5 49.765625   BatchTime 0.175622
INFO - Validation [62][   40/   40]   Loss 2.302665   Top1 10.000000   Top5 50.000000   BatchTime 0.115998
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [63][   20/  196]   Loss 2.302775   Top1 9.609375   Top5 49.023438   BatchTime 0.368111   LR 0.000090
INFO - Training [63][   40/  196]   Loss 2.302725   Top1 9.306641   Top5 49.501953   BatchTime 0.315870   LR 0.000087
INFO - Training [63][   60/  196]   Loss 2.302668   Top1 9.667969   Top5 49.856771   BatchTime 0.299131   LR 0.000085
INFO - Training [63][   80/  196]   Loss 2.302669   Top1 9.648438   Top5 49.931641   BatchTime 0.289734   LR 0.000082
INFO - Training [63][  100/  196]   Loss 2.302661   Top1 9.707031   Top5 50.058594   BatchTime 0.280555   LR 0.000080
INFO - Training [63][  120/  196]   Loss 2.302648   Top1 9.690755   Top5 50.104167   BatchTime 0.275984   LR 0.000077
INFO - Training [63][  140/  196]   Loss 2.302678   Top1 9.665179   Top5 50.053013   BatchTime 0.273037   LR 0.000075
INFO - Training [63][  160/  196]   Loss 2.302659   Top1 9.758301   Top5 50.136719   BatchTime 0.270814   LR 0.000072
INFO - Training [63][  180/  196]   Loss 2.302643   Top1 9.835069   Top5 50.199653   BatchTime 0.269644   LR 0.000070
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 9.848    Top5: 50.132    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
INFO - Validation [63][   20/   40]   Loss 2.302733   Top1 9.882812   Top5 49.765625   BatchTime 0.141748
INFO - Validation [63][   40/   40]   Loss 2.302662   Top1 10.000000   Top5 50.000000   BatchTime 0.096846
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Training [64][   20/  196]   Loss 2.302707   Top1 9.316406   Top5 50.234375   BatchTime 0.340381   LR 0.000066
INFO - Training [64][   40/  196]   Loss 2.302635   Top1 9.453125   Top5 50.244141   BatchTime 0.308472   LR 0.000064
INFO - Training [64][   60/  196]   Loss 2.302639   Top1 9.583333   Top5 50.279948   BatchTime 0.292695   LR 0.000062
INFO - Training [64][   80/  196]   Loss 2.302719   Top1 9.501953   Top5 49.936523   BatchTime 0.284272   LR 0.000059
INFO - Training [64][  100/  196]   Loss 2.302723   Top1 9.601562   Top5 50.003906   BatchTime 0.281890   LR 0.000057
INFO - Training [64][  120/  196]   Loss 2.302688   Top1 9.759115   Top5 50.048828   BatchTime 0.277125   LR 0.000055
INFO - Training [64][  140/  196]   Loss 2.302689   Top1 9.813058   Top5 50.041853   BatchTime 0.271743   LR 0.000053
INFO - Training [64][  160/  196]   Loss 2.302703   Top1 9.814453   Top5 50.004883   BatchTime 0.270647   LR 0.000051
INFO - Training [64][  180/  196]   Loss 2.302683   Top1 9.939236   Top5 49.967448   BatchTime 0.268254   LR 0.000049
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 9.930    Top5: 49.942    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [64][   20/   40]   Loss 2.302728   Top1 9.882812   Top5 49.765625   BatchTime 0.133083
INFO - Validation [64][   40/   40]   Loss 2.302658   Top1 10.000000   Top5 50.000000   BatchTime 0.094021
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [65][   20/  196]   Loss 2.302607   Top1 10.156250   Top5 49.609375   BatchTime 0.338004   LR 0.000046
INFO - Training [65][   40/  196]   Loss 2.302816   Top1 9.941406   Top5 49.531250   BatchTime 0.286844   LR 0.000044
INFO - Training [65][   60/  196]   Loss 2.302621   Top1 10.195312   Top5 49.967448   BatchTime 0.277826   LR 0.000042
INFO - Training [65][   80/  196]   Loss 2.302630   Top1 10.161133   Top5 50.166016   BatchTime 0.274954   LR 0.000040
INFO - Training [65][  100/  196]   Loss 2.302595   Top1 10.277344   Top5 50.351562   BatchTime 0.270601   LR 0.000039
INFO - Training [65][  120/  196]   Loss 2.302584   Top1 10.224609   Top5 50.283203   BatchTime 0.264741   LR 0.000037
INFO - Training [65][  140/  196]   Loss 2.302562   Top1 10.239955   Top5 50.359933   BatchTime 0.263604   LR 0.000035
INFO - Training [65][  160/  196]   Loss 2.302600   Top1 10.168457   Top5 50.244141   BatchTime 0.265616   LR 0.000033
INFO - Training [65][  180/  196]   Loss 2.302613   Top1 10.047743   Top5 50.269097   BatchTime 0.264067   LR 0.000032
********************pre-trained*****************
INFO - ==> Top1: 10.092    Top5: 50.196    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [65][   20/   40]   Loss 2.302725   Top1 9.882812   Top5 49.765625   BatchTime 0.148884
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 2.302657   Top1 10.000000   Top5 50.000000   BatchTime 0.103656
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [66][   20/  196]   Loss 2.302450   Top1 11.015625   Top5 49.843750   BatchTime 0.350079   LR 0.000029
INFO - Training [66][   40/  196]   Loss 2.302626   Top1 10.546875   Top5 49.755859   BatchTime 0.303830   LR 0.000028
INFO - Training [66][   60/  196]   Loss 2.302655   Top1 10.338542   Top5 49.674479   BatchTime 0.282110   LR 0.000026
INFO - Training [66][   80/  196]   Loss 2.302600   Top1 10.263672   Top5 49.916992   BatchTime 0.274397   LR 0.000025
INFO - Training [66][  100/  196]   Loss 2.302542   Top1 10.277344   Top5 50.179688   BatchTime 0.271369   LR 0.000023
INFO - Training [66][  120/  196]   Loss 2.302590   Top1 10.081380   Top5 50.139974   BatchTime 0.269470   LR 0.000022
INFO - Training [66][  140/  196]   Loss 2.302584   Top1 10.025112   Top5 50.156250   BatchTime 0.268489   LR 0.000021
INFO - Training [66][  160/  196]   Loss 2.302592   Top1 10.039062   Top5 50.190430   BatchTime 0.265163   LR 0.000019
INFO - Training [66][  180/  196]   Loss 2.302593   Top1 10.065104   Top5 50.110677   BatchTime 0.264235   LR 0.000018
********************pre-trained*****************
INFO - ==> Top1: 10.036    Top5: 50.156    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [66][   20/   40]   Loss 2.302725   Top1 9.882812   Top5 49.765625   BatchTime 0.157621
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [66][   40/   40]   Loss 2.302657   Top1 10.000000   Top5 50.000000   BatchTime 0.104868
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [67][   20/  196]   Loss 2.302425   Top1 10.351562   Top5 50.781250   BatchTime 0.371581   LR 0.000016
INFO - Training [67][   40/  196]   Loss 2.302551   Top1 10.107422   Top5 50.439453   BatchTime 0.318237   LR 0.000015
INFO - Training [67][   60/  196]   Loss 2.302592   Top1 10.143229   Top5 50.169271   BatchTime 0.303053   LR 0.000014
INFO - Training [67][   80/  196]   Loss 2.302600   Top1 10.151367   Top5 50.351562   BatchTime 0.296538   LR 0.000013
INFO - Training [67][  100/  196]   Loss 2.302650   Top1 10.234375   Top5 50.152344   BatchTime 0.289031   LR 0.000012
INFO - Training [67][  120/  196]   Loss 2.302719   Top1 10.058594   Top5 50.055339   BatchTime 0.284800   LR 0.000011
INFO - Training [67][  140/  196]   Loss 2.302731   Top1 10.047433   Top5 49.913504   BatchTime 0.279841   LR 0.000010
INFO - Training [67][  160/  196]   Loss 2.302778   Top1 9.980469   Top5 49.743652   BatchTime 0.274683   LR 0.000009
INFO - Training [67][  180/  196]   Loss 2.302745   Top1 9.913194   Top5 49.911024   BatchTime 0.272320   LR 0.000008
INFO - ==> Top1: 9.924    Top5: 49.918    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [67][   20/   40]   Loss 2.302724   Top1 9.882812   Top5 49.765625   BatchTime 0.147034
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [67][   40/   40]   Loss 2.302656   Top1 10.000000   Top5 50.000000   BatchTime 0.098964
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [68][   20/  196]   Loss 2.302524   Top1 9.921875   Top5 50.273438   BatchTime 0.341836   LR 0.000007
INFO - Training [68][   40/  196]   Loss 2.302692   Top1 9.960938   Top5 50.185547   BatchTime 0.300789   LR 0.000006
INFO - Training [68][   60/  196]   Loss 2.302608   Top1 10.058594   Top5 50.390625   BatchTime 0.282475   LR 0.000006
INFO - Training [68][   80/  196]   Loss 2.302715   Top1 9.838867   Top5 50.195312   BatchTime 0.273399   LR 0.000005
INFO - Training [68][  100/  196]   Loss 2.302693   Top1 10.007812   Top5 50.207031   BatchTime 0.271086   LR 0.000004
INFO - Training [68][  120/  196]   Loss 2.302692   Top1 9.996745   Top5 50.136719   BatchTime 0.269132   LR 0.000004
INFO - Training [68][  140/  196]   Loss 2.302734   Top1 10.016741   Top5 49.938616   BatchTime 0.265043   LR 0.000003
INFO - Training [68][  160/  196]   Loss 2.302723   Top1 10.095215   Top5 49.829102   BatchTime 0.265189   LR 0.000003
INFO - Training [68][  180/  196]   Loss 2.302725   Top1 10.093316   Top5 49.884983   BatchTime 0.263098   LR 0.000002
********************pre-trained*****************
INFO - ==> Top1: 10.076    Top5: 49.910    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [68][   20/   40]   Loss 2.302724   Top1 9.882812   Top5 49.765625   BatchTime 0.147221
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - Validation [68][   40/   40]   Loss 2.302656   Top1 10.000000   Top5 50.000000   BatchTime 0.099152
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [69][   20/  196]   Loss 2.302866   Top1 9.960938   Top5 50.214844   BatchTime 0.359858   LR 0.000002
INFO - Training [69][   40/  196]   Loss 2.302800   Top1 9.970703   Top5 50.029297   BatchTime 0.302864   LR 0.000001
INFO - Training [69][   60/  196]   Loss 2.302815   Top1 9.915365   Top5 49.726562   BatchTime 0.296144   LR 0.000001
INFO - Training [69][   80/  196]   Loss 2.302801   Top1 9.882812   Top5 49.873047   BatchTime 0.286139   LR 0.000001
INFO - Training [69][  100/  196]   Loss 2.302781   Top1 9.843750   Top5 49.914062   BatchTime 0.280659   LR 0.000000
INFO - Training [69][  120/  196]   Loss 2.302784   Top1 9.798177   Top5 49.980469   BatchTime 0.276599   LR 0.000000
INFO - Training [69][  140/  196]   Loss 2.302759   Top1 9.874442   Top5 50.025112   BatchTime 0.272798   LR 0.000000
INFO - Training [69][  160/  196]   Loss 2.302765   Top1 9.931641   Top5 49.897461   BatchTime 0.268677   LR 0.000000
INFO - Training [69][  180/  196]   Loss 2.302801   Top1 9.911024   Top5 49.769965   BatchTime 0.265422   LR 0.000000
********************pre-trained*****************
INFO - ==> Top1: 9.948    Top5: 49.746    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [69][   20/   40]   Loss 2.302724   Top1 9.882812   Top5 49.765625   BatchTime 0.165084
INFO - Validation [69][   40/   40]   Loss 2.302656   Top1 10.000000   Top5 50.000000   BatchTime 0.109607
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.)
features.16.conv.3 tensor(0.)
features.16.conv.6 tensor(0.)
conv.0 tensor(0.0078)
tensor(3209.) 2188896.0
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.001
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.270   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 91.120   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.920   Top5: 99.790]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-111802/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [   20/   40]   Loss 2.302724   Top1 9.882812   Top5 49.765625   BatchTime 0.144574
INFO - Validation [   40/   40]   Loss 2.302656   Top1 10.000000   Top5 50.000000   BatchTime 0.098874
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.303
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...