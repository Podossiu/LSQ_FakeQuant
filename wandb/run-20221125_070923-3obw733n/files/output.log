Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.0005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.0005
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.96300584
0.96269518
0.96095669
0.96116918
0.92139637
0.92970812
0.93482959
0.93042272
0.93134511
0.93032771
0.92781615
INFO - Training [0][   20/  196]   Loss 1.752371   Top1 38.867188   Top5 84.003906   BatchTime 0.343995   LR 0.000500
0.92464489
0.92297429
0.92184436
0.92019826
0.91879922
0.91758013
0.91604221
0.91468912
0.91329920
0.91225058
0.91103417
0.90987247
0.90916657
0.90861917
INFO - Training [0][   40/  196]   Loss 1.683268   Top1 41.367188   Top5 85.673828   BatchTime 0.310273   LR 0.000500
0.90803939
0.90743732
0.90630394
0.90525490
0.90498567
0.90430415
0.90404838
0.90368527
0.90362161
0.90306890
0.90293020
0.90243226
0.90228373
0.90206796
0.90131587
0.90073913
0.89982271
0.89988911
0.89964992
0.89971048
0.89940685
0.89937073
INFO - Training [0][   60/  196]   Loss 1.600473   Top1 44.127604   Top5 87.213542   BatchTime 0.301625   LR 0.000499
0.89920598
0.89928591
0.89940846
0.89956212
0.89934325
0.89964092
0.89985591
0.89999163
0.89996487
0.89990395
0.90021360
0.90016550
0.90023893
0.89989549
0.89971483
0.89974260
0.89962202
0.89967132
0.89943093
0.89909416
0.89912617
INFO - Training [0][   80/  196]   Loss 1.540690   Top1 46.357422   Top5 88.383789   BatchTime 0.298205   LR 0.000498
0.89921337
0.89874536
0.89855790
0.89872658
0.89857179
0.89823169
0.89806134
0.89819086
0.89807439
0.89793491
0.89789450
0.89780831
0.89722806
0.89694554
0.89685869
0.89629823
0.89584500
0.89554340
0.89517540
0.89482081
0.89448738
INFO - Training [0][  100/  196]   Loss 1.487998   Top1 48.382812   Top5 89.218750   BatchTime 0.294233   LR 0.000497
0.89376843
0.89326173
0.89270294
0.89232475
0.89191538
0.89139313
0.89166045
0.89139694
0.89081538
0.89058179
0.89057904
0.89077061
0.89071465
0.89076477
INFO - Training [0][  120/  196]   Loss 1.442147   Top1 50.139974   Top5 89.902344   BatchTime 0.294857   LR 0.000495
0.89028704
0.88998288
0.89009941
0.89000434
0.88944876
0.88953876
0.88936198
0.88922524
0.88913554
0.88889444
0.88862091
0.88871980
0.88869822
0.88818324
0.88813168
0.88787407
0.88805723
0.88780272
0.88778442
0.88763511
INFO - Training [0][  140/  196]   Loss 1.408653   Top1 51.392299   Top5 90.368304   BatchTime 0.293344   LR 0.000494
0.88807935
0.88786584
0.88803697
0.88790911
0.88762397
0.88775307
0.88772827
0.88755202
0.88759941
0.88764942
0.88785893
0.88766527
0.88727838
0.88717031
0.88734394
0.88691813
0.88720024
0.88685536
0.88648683
0.88637555
0.88625067
INFO - Training [0][  160/  196]   Loss 1.385691   Top1 52.277832   Top5 90.698242   BatchTime 0.293045   LR 0.000492
0.88604277
0.88615429
0.88610983
0.88631475
0.88607854
0.88645732
0.88599217
0.88619894
0.88605130
0.88575971
0.88562614
0.88557106
0.88549906
0.88541967
0.88545763
0.88505977
0.88479507
0.88441986
0.88417369
0.88390350
0.88406235
0.88438505
0.88441998
0.88433319
INFO - Training [0][  180/  196]   Loss 1.359058   Top1 53.127170   Top5 91.046007   BatchTime 0.287483   LR 0.000490
0.88501239
0.88489002
0.88511813
0.88542300
0.88516301
0.88549739
0.88539940
0.88541347
0.88523781
0.88519686
0.88546377
0.88516808
0.88542753
0.88544935
0.88520676
0.88499218
INFO - ==> Top1: 53.800    Top5: 91.266    Loss: 1.340
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
0.88497382
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [0][   20/   40]   Loss 0.929637   Top1 68.398438   Top5 96.835938   BatchTime 0.116271
INFO - Validation [0][   40/   40]   Loss 0.939463   Top1 67.640000   Top5 97.030000   BatchTime 0.086083
INFO - ==> Top1: 67.640    Top5: 97.030    Loss: 0.939
INFO - ==> Sparsity : 0.302
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 67.640   Top5: 97.030]
features.0.conv.0 tensor(0.2500)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.1207)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.1956)
features.3.conv.0 tensor(0.0802)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1081)
features.4.conv.0 tensor(0.1120)
features.4.conv.3 tensor(0.3166)
features.4.conv.6 tensor(0.1715)
features.5.conv.0 tensor(0.3997)
features.5.conv.3 tensor(0.4282)
features.5.conv.6 tensor(0.1025)
features.6.conv.0 tensor(0.0531)
features.6.conv.3 tensor(0.0521)
features.6.conv.6 tensor(0.0872)
features.7.conv.0 tensor(0.2094)
features.7.conv.3 tensor(0.4470)
features.7.conv.6 tensor(0.1796)
features.8.conv.0 tensor(0.4264)
features.8.conv.3 tensor(0.5333)
features.8.conv.6 tensor(0.1277)
features.9.conv.0 tensor(0.5246)
features.9.conv.3 tensor(0.5700)
features.9.conv.6 tensor(0.1353)
features.10.conv.0 tensor(0.0861)
features.10.conv.3 tensor(0.1131)
features.10.conv.6 tensor(0.1092)
features.11.conv.0 tensor(0.6178)
features.11.conv.3 tensor(0.6491)
features.11.conv.6 tensor(0.1683)
features.12.conv.0 tensor(0.6035)
features.12.conv.3 tensor(0.6912)
features.12.conv.6 tensor(0.1668)
features.13.conv.0 tensor(0.3580)
features.13.conv.3 tensor(0.4967)
features.13.conv.6 tensor(0.0875)
features.14.conv.0 tensor(0.7974)
features.14.conv.3 tensor(0.8378)
features.14.conv.6 tensor(0.0882)
features.15.conv.0 tensor(0.4686)
features.15.conv.3 tensor(0.8263)
features.15.conv.6 tensor(0.8760)
features.16.conv.0 tensor(0.4829)
features.16.conv.3 tensor(0.8120)
features.16.conv.6 tensor(0.0497)
conv.0 tensor(0.0537)
tensor(660621.) 2188896.0
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.88481170
0.88460273
0.88438225
0.88441426
0.88439947
0.88440210
0.88455021
0.88416588
0.88391864
0.88407922
0.88391989
0.88389462
0.88365948
0.88273990
0.88234252
0.88223416
0.88153642
0.88130403
0.88064075
INFO - Training [1][   20/  196]   Loss 1.141440   Top1 60.156250   Top5 94.003906   BatchTime 0.320561   LR 0.000485
0.88037986
0.87982517
0.88020283
0.88017094
0.88043010
0.88087702
0.88103545
0.88121706
0.88113832
0.88118416
0.88119215
0.88127440
0.88150436
0.88149709
0.88168490
0.88195276
0.88189745
0.88217914
0.88246965
0.88235623
0.88253397
0.88253504
INFO - Training [1][   40/  196]   Loss 1.128027   Top1 61.230469   Top5 94.052734   BatchTime 0.293319   LR 0.000482
0.88245976
0.88228989
0.88234687
0.88239235
0.88226461
0.88251364
0.88151175
0.88262284
0.88291156
0.88292331
0.88269711
0.88269287
0.88300520
0.88309824
0.88299090
INFO - Training [1][   60/  196]   Loss 1.115104   Top1 61.497396   Top5 94.160156   BatchTime 0.288762   LR 0.000479
0.88297677
0.88303602
0.88312811
0.88321579
0.88338023
0.88308930
0.88283640
0.88270986
0.88263923
0.88257670
0.88270378
0.88266057
0.88259000
0.88294816
0.88264263
0.88250226
0.88231558
0.88228661
0.88265604
0.88267493
0.88256925
0.88267642
INFO - Training [1][   80/  196]   Loss 1.109986   Top1 61.606445   Top5 94.326172   BatchTime 0.283804   LR 0.000476
0.88240200
0.88217741
0.88218868
0.88225120
0.88179600
0.88196737
0.88198698
0.88229865
0.88340241
0.88407338
0.88438475
0.88453490
0.88429439
0.88353693
0.88329941
0.88319033
0.88318896
0.88301706
0.88294446
0.88279366
0.88289028
INFO - Training [1][  100/  196]   Loss 1.095617   Top1 62.265625   Top5 94.511719   BatchTime 0.284552   LR 0.000473
0.88293743
0.88256139
0.88209581
0.88158494
0.88160545
0.88195664
0.88269031
0.88197923
0.88177258
0.88198894
0.88227689
0.88228887
0.88222247
0.88202322
0.88231331
0.88220662
0.88192618
0.88204688
0.88233322
0.88210779
0.88186646
0.88175064
INFO - Training [1][  120/  196]   Loss 1.082397   Top1 62.747396   Top5 94.648438   BatchTime 0.282071   LR 0.000469
0.88198888
0.88235182
0.88186693
0.88143933
0.88143998
0.88148814
0.88158453
0.88181251
0.88164860
0.88182098
0.88148999
0.88154465
0.88113177
0.88104582
INFO - Training [1][  140/  196]   Loss 1.072644   Top1 63.060826   Top5 94.829799   BatchTime 0.281508   LR 0.000465
0.88102823
0.88089269
0.88108164
0.88129097
0.88153350
0.88147175
0.88042271
0.87822169
0.88177431
0.88177878
0.88174695
0.88173753
0.88164663
0.88161021
0.88162017
0.88161278
0.88173258
0.88112861
0.88103014
0.88111627
0.88107848
0.88090515
0.88079667
INFO - Training [1][  160/  196]   Loss 1.064702   Top1 63.276367   Top5 94.887695   BatchTime 0.279442   LR 0.000460
0.88075143
0.88083369
0.88022667
0.87871599
0.87726700
0.87507278
0.87600267
0.87709129
0.87697047
0.87685734
0.87670755
0.87669444
0.87661058
0.87651330
0.87618691
0.87596196
0.87627947
0.87755406
0.88092726
0.88087744
0.88054031
0.88065392
0.88087541
INFO - Training [1][  180/  196]   Loss 1.054427   Top1 63.656684   Top5 94.919705   BatchTime 0.277496   LR 0.000456
0.88080949
0.88077521
0.88049060
0.88048846
0.88071185
0.88073754
0.88073432
0.88110250
0.88105202
0.88093048
0.88106310
0.88103229
0.88088733
0.87996924
0.87965900
INFO - ==> Top1: 63.790    Top5: 94.982    Loss: 1.048
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.792540   Top1 73.339844   Top5 97.988281   BatchTime 0.119092
INFO - Validation [1][   40/   40]   Loss 0.799959   Top1 72.890000   Top5 98.060000   BatchTime 0.088095
INFO - ==> Top1: 72.890    Top5: 98.060    Loss: 0.800
INFO - ==> Sparsity : 0.304
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 72.890   Top5: 98.060]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 67.640   Top5: 97.030]
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.1183)
features.2.conv.3 tensor(0.3588)
features.2.conv.6 tensor(0.1620)
features.3.conv.0 tensor(0.0732)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.1092)
features.4.conv.0 tensor(0.0846)
features.4.conv.3 tensor(0.3125)
features.4.conv.6 tensor(0.1745)
features.5.conv.0 tensor(0.2795)
features.5.conv.3 tensor(0.4288)
features.5.conv.6 tensor(0.1073)
features.6.conv.0 tensor(0.0614)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0903)
features.7.conv.0 tensor(0.1698)
features.7.conv.3 tensor(0.4482)
features.7.conv.6 tensor(0.1828)
features.8.conv.0 tensor(0.4102)
features.8.conv.3 tensor(0.5373)
features.8.conv.6 tensor(0.1362)
features.9.conv.0 tensor(0.4330)
features.9.conv.3 tensor(0.5631)
features.9.conv.6 tensor(0.1406)
features.10.conv.0 tensor(0.0798)
features.10.conv.3 tensor(0.1152)
features.10.conv.6 tensor(0.1083)
features.11.conv.0 tensor(0.6155)
features.11.conv.3 tensor(0.6445)
features.11.conv.6 tensor(0.1915)
features.12.conv.0 tensor(0.5212)
features.12.conv.3 tensor(0.6861)
features.12.conv.6 tensor(0.1624)
features.13.conv.0 tensor(0.3665)
features.13.conv.3 tensor(0.5015)
features.13.conv.6 tensor(0.0909)
features.14.conv.0 tensor(0.7839)
features.14.conv.3 tensor(0.8350)
features.14.conv.6 tensor(0.0975)
features.15.conv.0 tensor(0.4535)
features.15.conv.3 tensor(0.8255)
features.15.conv.6 tensor(0.9445)
features.16.conv.0 tensor(0.4427)
features.16.conv.3 tensor(0.8087)
features.16.conv.6 tensor(0.0636)
conv.0 tensor(0.0719)
tensor(666484.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.87935644
0.87885660
0.87768447
0.87519425
0.86821061
0.85870397
0.85754317
0.85651451
0.85599858
0.85608840
0.85634637
0.85680705
0.85632032
0.85634953
0.85661888
INFO - Training [2][   20/  196]   Loss 1.028425   Top1 64.765625   Top5 94.531250   BatchTime 0.321447   LR 0.000448
0.85691112
0.85818630
0.85951197
0.86200434
0.87047237
0.87703210
0.87925780
0.87906456
0.87829387
0.87753880
0.87823886
0.87885547
0.87919170
0.87939298
0.87957001
0.88101786
0.88248378
0.88268989
0.88304341
0.88257492
0.88283896
0.88258547
INFO - Training [2][   40/  196]   Loss 1.005743   Top1 65.576172   Top5 94.707031   BatchTime 0.306648   LR 0.000443
0.88224572
0.88220310
0.88210803
0.88224030
0.88210326
0.88227552
0.88227272
0.88235784
0.88236195
0.88193923
0.88219696
0.88181192
0.88173813
0.88163310
0.88154382
0.88177001
0.88170540
0.88174361
INFO - Training [2][   60/  196]   Loss 0.982243   Top1 66.367188   Top5 95.156250   BatchTime 0.304952   LR 0.000437
0.88207269
0.88212687
0.88207799
0.88233322
0.88185030
0.88187110
0.88154244
0.88130176
0.88143051
0.88138837
0.88122797
0.88104284
0.88125187
0.88162041
0.88172531
0.88172305
0.88164288
0.88162863
0.88156050
0.88160765
0.88157094
0.88137794
0.88138586
0.88140750
INFO - Training [2][   80/  196]   Loss 0.966572   Top1 66.889648   Top5 95.424805   BatchTime 0.291567   LR 0.000432
0.88151777
0.88165659
0.88144833
0.88140827
0.88119584
0.88126230
0.88145542
0.88174272
0.88188946
0.88171196
0.88166177
0.88141233
0.88150096
0.88145602
0.88146853
0.88158184
0.88175070
INFO - Training [2][  100/  196]   Loss 0.955737   Top1 67.222656   Top5 95.542969   BatchTime 0.283097   LR 0.000426
0.88195443
0.88199657
0.88187438
0.88167048
0.88143688
0.88140565
0.88134831
0.88153017
0.88187039
0.88211370
0.88188648
0.88195443
0.88179475
0.88211477
0.88190573
0.88183087
0.88170433
0.88174498
0.88174921
0.88174415
0.88166082
0.88184315
0.88183939
0.88179654
INFO - Training [2][  120/  196]   Loss 0.947714   Top1 67.542318   Top5 95.660807   BatchTime 0.278003   LR 0.000421
0.88159829
0.88144881
0.88136226
0.88140553
0.88154167
0.88155746
0.88157028
0.88150996
0.88166279
0.88142502
0.88128650
0.88152415
0.88110250
0.88125265
0.88108671
0.88104743
0.88111389
0.88119400
0.88120544
0.88129056
0.88129598
0.88121182
INFO - Training [2][  140/  196]   Loss 0.946582   Top1 67.564174   Top5 95.733817   BatchTime 0.275936   LR 0.000415
0.88109940
0.88107735
0.88110667
0.88147932
0.88155603
0.88162118
0.88203371
0.88177925
0.88210076
0.88194227
0.88204205
0.88211340
0.88194007
0.88182324
0.88160902
0.88157296
INFO - Training [2][  160/  196]   Loss 0.943686   Top1 67.670898   Top5 95.771484   BatchTime 0.273441   LR 0.000409
0.88162971
0.88209277
0.88203770
0.88191092
0.88193369
0.88211638
0.88170648
0.88184315
0.88179725
0.88187486
0.88185984
0.88202679
0.88218677
0.88233817
0.88234395
0.88234252
0.88203096
0.88193798
0.88182408
0.88177860
0.88181686
0.88168186
0.88150215
0.88148391
INFO - Training [2][  180/  196]   Loss 0.937598   Top1 67.884115   Top5 95.726997   BatchTime 0.270911   LR 0.000402
0.88170797
0.88181704
0.88183361
0.88153982
0.88159817
0.88163871
0.88178772
0.88185638
0.88150507
0.88126767
0.88107222
0.88115001
0.88155603
0.88141656
********************pre-trained*****************
INFO - ==> Top1: 68.038    Top5: 95.764    Loss: 0.933
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.727242   Top1 75.371094   Top5 97.851562   BatchTime 0.116179
INFO - Validation [2][   40/   40]   Loss 0.728067   Top1 75.210000   Top5 98.110000   BatchTime 0.086253
INFO - ==> Top1: 75.210    Top5: 98.110    Loss: 0.728
INFO - ==> Sparsity : 0.295
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 75.210   Top5: 98.110]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 72.890   Top5: 98.060]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 67.640   Top5: 97.030]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1172)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.0799)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.1655)
features.3.conv.0 tensor(0.0686)
features.3.conv.3 tensor(0.0941)
features.3.conv.6 tensor(0.1013)
features.4.conv.0 tensor(0.0643)
features.4.conv.3 tensor(0.3171)
features.4.conv.6 tensor(0.1750)
features.5.conv.0 tensor(0.2570)
features.5.conv.3 tensor(0.4213)
features.5.conv.6 tensor(0.1009)
features.6.conv.0 tensor(0.0509)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0890)
features.7.conv.0 tensor(0.1531)
features.7.conv.3 tensor(0.4531)
features.7.conv.6 tensor(0.1862)
features.8.conv.0 tensor(0.4012)
features.8.conv.3 tensor(0.5362)
features.8.conv.6 tensor(0.1364)
features.9.conv.0 tensor(0.4319)
features.9.conv.3 tensor(0.5611)
features.9.conv.6 tensor(0.1320)
features.10.conv.0 tensor(0.0727)
features.10.conv.3 tensor(0.1108)
features.10.conv.6 tensor(0.1062)
features.11.conv.0 tensor(0.6553)
features.11.conv.3 tensor(0.6476)
features.11.conv.6 tensor(0.1694)
features.12.conv.0 tensor(0.4976)
features.12.conv.3 tensor(0.6840)
features.12.conv.6 tensor(0.2104)
features.13.conv.0 tensor(0.2857)
features.13.conv.3 tensor(0.4888)
features.13.conv.6 tensor(0.0880)
features.14.conv.0 tensor(0.7915)
features.14.conv.3 tensor(0.8344)
features.14.conv.6 tensor(0.1217)
features.15.conv.0 tensor(0.2749)
features.15.conv.3 tensor(0.8188)
features.15.conv.6 tensor(0.9623)
features.16.conv.0 tensor(0.4193)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.0669)
conv.0 tensor(0.0834)
tensor(644654.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.88110369
0.88089997
0.88110328
0.88102150
0.88061583
0.88051087
0.88051486
0.88059491
0.88094902
0.88077492
0.88050497
0.88056195
0.88055956
0.88045365
0.88044560
0.88053757
0.88058990
0.88037914
0.88029724
INFO - Training [3][   20/  196]   Loss 0.900565   Top1 69.121094   Top5 95.800781   BatchTime 0.356194   LR 0.000391
0.88024443
0.88052726
0.88055772
0.88070476
0.88067341
0.88057745
0.88060278
0.88072515
0.88061428
0.88031727
0.88044661
0.88062006
0.88074219
0.88086385
0.88094079
0.88089025
0.88090461
0.88087308
INFO - Training [3][   40/  196]   Loss 0.891465   Top1 69.873047   Top5 95.927734   BatchTime 0.334663   LR 0.000384
0.88090116
0.88114977
0.88075030
0.88053995
0.88094038
0.88079607
0.88079107
0.88077742
0.88072956
0.88085938
0.88085717
0.88109773
0.88076985
0.88081008
0.88092542
0.88081151
0.88063180
0.88077867
0.88112074
0.88089883
0.88116890
INFO - Training [3][   60/  196]   Loss 0.883864   Top1 69.986979   Top5 96.106771   BatchTime 0.319750   LR 0.000377
0.88131469
0.88154209
0.88113320
0.88084179
0.88070917
0.88091063
0.88111359
0.88117844
0.88110608
0.88114011
0.88094401
0.88129842
0.88151443
0.88143796
0.88161045
0.88143665
0.88138682
0.88138795
0.88149679
0.88104403
0.88025624
0.88170707
0.88175058
INFO - Training [3][   80/  196]   Loss 0.877869   Top1 70.258789   Top5 96.245117   BatchTime 0.305982   LR 0.000370
0.88128734
0.88128436
0.88109928
0.88112187
0.88147026
0.88104099
0.88098145
0.88108838
0.87996358
0.87959611
0.88132930
0.88104326
0.88103443
0.88108361
0.88098419
INFO - Training [3][  100/  196]   Loss 0.864434   Top1 70.699219   Top5 96.308594   BatchTime 0.298667   LR 0.000363
0.88086319
0.88087720
0.88094145
0.88089460
0.88092673
0.88080680
0.88093418
0.88121098
0.88114768
0.88150775
0.88131970
0.88117510
0.88092661
0.88089645
0.88087898
0.88098311
0.88122165
0.88099188
0.88080490
0.88122290
0.88136917
0.88145691
0.88113594
INFO - Training [3][  120/  196]   Loss 0.857091   Top1 70.846354   Top5 96.464844   BatchTime 0.291319   LR 0.000356
0.88109809
0.88122201
0.88151276
0.88164192
0.88170338
0.88176155
0.88160682
0.88141632
0.88143182
0.88097662
0.88104248
0.88115919
0.88109738
0.88113660
0.88112730
0.88101274
INFO - Training [3][  140/  196]   Loss 0.853988   Top1 70.786830   Top5 96.529018   BatchTime 0.284830   LR 0.000348
0.88106400
0.88086534
0.88114470
0.88130558
0.88122243
0.88100487
0.88100654
0.88088429
0.88072401
0.88053495
0.88071692
0.88088548
0.88092065
0.88082188
0.88097084
0.88072830
0.88061386
0.88087475
0.88077492
0.88089496
0.88092577
0.88106370
0.88096583
0.88075501
INFO - Training [3][  160/  196]   Loss 0.850737   Top1 70.869141   Top5 96.547852   BatchTime 0.280397   LR 0.000341
0.88045406
0.88089746
0.88077432
0.88050914
0.88071150
0.88082105
0.88071078
0.88073957
0.88063800
0.87971008
0.87896162
0.87780088
0.87910640
0.87942857
0.87897909
0.87875974
INFO - Training [3][  180/  196]   Loss 0.846140   Top1 70.991753   Top5 96.493056   BatchTime 0.276683   LR 0.000333
0.87861174
0.87840706
0.87839454
0.87848717
0.87794077
0.87739718
0.87722832
0.87718284
0.87722790
0.87731630
0.87781054
0.88059902
0.88046408
0.88020486
0.88016117
0.88040477
0.88068044
INFO - ==> Top1: 71.052    Top5: 96.482    Loss: 0.845
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.88069892
0.88054287
0.88055253
0.88044232
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [3][   20/   40]   Loss 0.593810   Top1 80.117188   Top5 98.652344   BatchTime 0.117593
features.0.conv.0 tensor(0.2708)
features.0.conv.3 tensor(0.1133)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0825)
features.2.conv.0 tensor(0.0744)
features.2.conv.3 tensor(0.3588)
features.2.conv.6 tensor(0.1586)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0926)
features.3.conv.6 tensor(0.1133)
features.4.conv.0 tensor(0.0658)
features.4.conv.3 tensor(0.3177)
features.4.conv.6 tensor(0.1610)
features.5.conv.0 tensor(0.2541)
features.5.conv.3 tensor(0.4248)
features.5.conv.6 tensor(0.0999)
features.6.conv.0 tensor(0.0568)
features.6.conv.3 tensor(0.0608)
features.6.conv.6 tensor(0.0899)
features.7.conv.0 tensor(0.1519)
features.7.conv.3 tensor(0.4491)
features.7.conv.6 tensor(0.1826)
features.8.conv.0 tensor(0.4066)
features.8.conv.3 tensor(0.5350)
features.8.conv.6 tensor(0.1410)
features.9.conv.0 tensor(0.3816)
features.9.conv.3 tensor(0.5625)
features.9.conv.6 tensor(0.1362)
features.10.conv.0 tensor(0.0764)
features.10.conv.3 tensor(0.1100)
features.10.conv.6 tensor(0.1025)
features.11.conv.0 tensor(0.6608)
features.11.conv.3 tensor(0.6476)
features.11.conv.6 tensor(0.1868)
features.12.conv.0 tensor(0.4870)
features.12.conv.3 tensor(0.6771)
features.12.conv.6 tensor(0.1989)
features.13.conv.0 tensor(0.2881)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.0883)
features.14.conv.0 tensor(0.8176)
features.14.conv.3 tensor(0.8355)
features.14.conv.6 tensor(0.1409)
features.15.conv.0 tensor(0.2571)
features.15.conv.3 tensor(0.8199)
features.15.conv.6 tensor(0.9643)
features.16.conv.0 tensor(0.4092)
features.16.conv.3 tensor(0.8013)
features.16.conv.6 tensor(0.0749)
conv.0 tensor(0.0904)
tensor(652068.) 2188896.0
INFO - Validation [3][   40/   40]   Loss 0.591472   Top1 80.150000   Top5 98.860000   BatchTime 0.084858
INFO - ==> Top1: 80.150    Top5: 98.860    Loss: 0.591
INFO - ==> Sparsity : 0.298
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 80.150   Top5: 98.860]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 75.210   Top5: 98.110]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 72.890   Top5: 98.060]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.88054252
0.88049406
0.88043970
0.88051134
0.88045913
0.88035768
0.88026851
0.88023615
0.88029617
0.88026273
0.88038462
0.88020015
0.88014942
0.88049304
0.88028741
0.88024408
0.88007694
0.88012069
0.88030910
0.88038772
0.88043439
INFO - Training [4][   20/  196]   Loss 0.826087   Top1 70.683594   Top5 96.445312   BatchTime 0.342062   LR 0.000320
0.88034779
0.88021940
0.88058656
0.88052613
0.88056922
0.88065976
0.88062817
0.88065004
0.88058132
0.88059694
0.88059765
0.88057035
0.88039613
0.88046163
0.88047081
0.88069659
INFO - Training [4][   40/  196]   Loss 0.812584   Top1 72.041016   Top5 96.611328   BatchTime 0.300650   LR 0.000312
0.88046110
0.88039070
0.88016963
0.87999141
0.87987947
0.88004154
0.88019162
0.88037300
0.88038534
0.88055629
0.88026381
0.88028258
0.88020325
0.88023549
0.88070536
0.88062167
0.88002867
0.88020766
0.88004327
0.87981814
0.88006270
INFO - Training [4][   60/  196]   Loss 0.815682   Top1 71.933594   Top5 96.679688   BatchTime 0.295653   LR 0.000304
0.87970972
0.87963170
0.87986404
0.88016880
0.87970006
0.87940896
0.87978661
0.88001978
0.87970746
0.87945813
0.87972772
0.87970757
0.87983131
0.87986666
0.87957013
0.87958199
0.87944990
0.87926292
0.87906861
0.87930590
0.87954146
INFO - Training [4][   80/  196]   Loss 0.811740   Top1 72.128906   Top5 96.787109   BatchTime 0.292782   LR 0.000296
0.87962115
0.87978137
0.87961996
0.87949729
0.87934542
0.87935829
0.87900817
0.87901229
0.87911248
0.87909597
0.87914085
0.87938118
0.87918186
0.87907648
0.87922603
0.87920523
0.87939543
0.87944269
0.87939566
0.87942290
0.87916958
0.87911463
INFO - Training [4][  100/  196]   Loss 0.801043   Top1 72.648438   Top5 96.824219   BatchTime 0.288656   LR 0.000289
0.87914807
0.87936848
0.87933779
0.87924576
0.87933016
0.87881953
0.87865257
0.87852913
0.87830096
0.87825692
0.87820613
0.87832558
0.87821043
0.87784308
INFO - Training [4][  120/  196]   Loss 0.791224   Top1 73.020833   Top5 96.910807   BatchTime 0.287838   LR 0.000281
0.87786597
0.87768871
0.87748587
0.87735587
0.87656742
0.87596554
0.87564701
0.87512904
0.87450486
0.87408918
0.87327409
0.87233818
0.87201601
0.87103420
0.86971432
0.86857361
0.86748177
0.86630690
0.86528599
0.86430818
0.86350656
0.86240906
0.86169320
INFO - Training [4][  140/  196]   Loss 0.788506   Top1 73.083147   Top5 96.953125   BatchTime 0.283113   LR 0.000273
0.86108524
0.86045581
0.86004394
0.86041307
0.85997498
0.85916477
0.85884005
0.85834157
0.85845315
0.85869616
0.85866499
0.85792518
0.85774529
0.85771012
0.85793054
0.85788774
0.85755688
0.85734481
0.85708123
0.85692644
0.85683852
0.85665804
0.85644656
0.85599619
INFO - Training [4][  160/  196]   Loss 0.787354   Top1 73.125000   Top5 96.987305   BatchTime 0.279089   LR 0.000265
0.85594600
0.85592985
0.85577530
0.85549694
0.85542238
0.85550076
0.85551232
0.85539192
0.85544074
0.85562855
0.85586530
0.85556585
0.85554767
0.85557228
0.85562593
INFO - Training [4][  180/  196]   Loss 0.782468   Top1 73.242188   Top5 96.957465   BatchTime 0.279364   LR 0.000257
0.85533875
0.85509098
0.85505360
0.85500443
0.85474116
0.85449558
0.85437077
0.85396624
0.85385370
0.85377389
0.85337663
0.85322458
0.85317945
0.85335428
0.85353434
0.85367256
0.85383642
0.85373271
0.85345584
INFO - ==> Top1: 73.324    Top5: 96.944    Loss: 0.779
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.542669   Top1 82.167969   Top5 98.906250   BatchTime 0.124368
INFO - Validation [4][   40/   40]   Loss 0.531695   Top1 82.500000   Top5 99.020000   BatchTime 0.094219
INFO - ==> Top1: 82.500    Top5: 99.020    Loss: 0.532
INFO - ==> Sparsity : 0.335
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 82.500   Top5: 99.020]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 80.150   Top5: 98.860]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 75.210   Top5: 98.110]
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1133)
features.1.conv.0 tensor(0.0573)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.0706)
features.2.conv.3 tensor(0.3565)
features.2.conv.6 tensor(0.1458)
features.3.conv.0 tensor(0.0732)
features.3.conv.3 tensor(0.0826)
features.3.conv.6 tensor(0.1124)
features.4.conv.0 tensor(0.0505)
features.4.conv.3 tensor(0.3131)
features.4.conv.6 tensor(0.1675)
features.5.conv.0 tensor(0.2550)
features.5.conv.3 tensor(0.4236)
features.5.conv.6 tensor(0.0913)
features.6.conv.0 tensor(0.0547)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0935)
features.7.conv.0 tensor(0.1543)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.1907)
features.8.conv.0 tensor(0.4486)
features.8.conv.3 tensor(0.5370)
features.8.conv.6 tensor(0.1455)
features.9.conv.0 tensor(0.3737)
features.9.conv.3 tensor(0.5611)
features.9.conv.6 tensor(0.1488)
features.10.conv.0 tensor(0.0712)
features.10.conv.3 tensor(0.1108)
features.10.conv.6 tensor(0.1033)
features.11.conv.0 tensor(0.6695)
features.11.conv.3 tensor(0.6466)
features.11.conv.6 tensor(0.1907)
features.12.conv.0 tensor(0.4746)
features.12.conv.3 tensor(0.6779)
features.12.conv.6 tensor(0.1706)
features.13.conv.0 tensor(0.2883)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.0881)
features.14.conv.0 tensor(0.8353)
features.14.conv.3 tensor(0.8352)
features.14.conv.6 tensor(0.1465)
features.15.conv.0 tensor(0.7544)
features.15.conv.3 tensor(0.8196)
features.15.conv.6 tensor(0.9652)
features.16.conv.0 tensor(0.4100)
features.16.conv.3 tensor(0.7995)
features.16.conv.6 tensor(0.0789)
conv.0 tensor(0.0927)
tensor(734097.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.85221684
0.85074681
0.85320598
0.85318422
0.85336667
0.85359055
0.85363936
0.85341936
0.85338843
0.85306132
0.85255927
0.85262656
0.85254520
0.85275382
0.85307008
0.85303706
INFO - Training [5][   20/  196]   Loss 0.755694   Top1 73.710938   Top5 96.718750   BatchTime 0.353192   LR 0.000242
0.85327309
0.85337985
0.85325813
0.85335445
0.85344905
0.85346955
0.85371596
0.85348022
0.85360247
0.85348803
0.85303879
0.85284144
0.85323387
0.85344660
0.85351956
0.85364676
0.85368651
0.85345066
0.85329372
0.85320228
0.85299939
INFO - Training [5][   40/  196]   Loss 0.770367   Top1 73.134766   Top5 96.689453   BatchTime 0.319463   LR 0.000234
0.85294223
0.85260767
0.85251218
0.85258430
0.85221869
0.85137641
0.85107535
0.85058945
0.84956086
0.84740639
0.84683120
0.84801197
0.84870636
0.85069722
0.85335833
0.85343742
0.85335451
0.85337383
0.85342979
0.85349590
0.85312688
INFO - Training [5][   60/  196]   Loss 0.757273   Top1 73.671875   Top5 96.809896   BatchTime 0.306275   LR 0.000226
0.85341042
0.85313404
0.85305130
0.85326004
0.85308963
0.85279554
0.85315347
0.85310513
0.85301065
0.85287070
0.85321999
0.85296500
0.85297143
0.85247856
0.85230935
0.85231090
0.85243100
0.85258061
0.85252625
0.85406649
0.85385537
0.85426456
INFO - Training [5][   80/  196]   Loss 0.744142   Top1 74.145508   Top5 96.982422   BatchTime 0.297149   LR 0.000218
0.85433972
0.85417509
0.85410273
0.85406560
0.85409546
0.85388714
0.85408199
0.85418075
0.85408211
0.85435277
0.85435158
0.85418177
0.85425389
0.85412902
0.85403973
0.85424131
0.85402274
0.85413057
0.85423583
0.85416174
0.85404944
0.85376716
INFO - Training [5][  100/  196]   Loss 0.737746   Top1 74.414062   Top5 97.054688   BatchTime 0.292991   LR 0.000210
0.85360640
0.85354102
0.85354996
0.85366273
0.85365087
0.85373741
0.85377204
0.85391933
0.85398012
0.85362601
0.85361826
0.85366017
0.85361928
0.85367918
0.85356879
INFO - Training [5][  120/  196]   Loss 0.730794   Top1 74.583333   Top5 97.216797   BatchTime 0.289541   LR 0.000202
0.85367447
0.85355222
0.85372269
0.85366344
0.85353214
0.85363948
0.85355395
0.85356814
0.85353869
0.85354143
0.85353279
0.85345221
0.85328102
0.85333055
0.85336429
0.85328621
0.85312331
0.85327762
0.85338581
0.85326421
0.85324937
INFO - Training [5][  140/  196]   Loss 0.728510   Top1 74.743304   Top5 97.285156   BatchTime 0.289713   LR 0.000195
0.85320663
0.85314173
0.85342252
0.85370493
0.85350049
0.85350090
0.85348177
0.85361230
0.85354382
0.85352331
0.85355192
0.85369265
0.85386533
0.85355693
0.85342997
0.85346085
0.85329044
0.85326844
0.85331762
INFO - Training [5][  160/  196]   Loss 0.728100   Top1 74.804688   Top5 97.241211   BatchTime 0.291827   LR 0.000187
0.85353196
0.85341227
0.85345298
0.85330135
0.85327983
0.85328966
0.85340416
0.85332996
0.85332680
0.85361397
0.85365403
0.85368931
0.85374057
0.85383838
0.85379297
0.85372013
0.85372764
0.85350215
0.85380054
0.85412306
0.85416830
0.85423660
0.85403442
INFO - Training [5][  180/  196]   Loss 0.725410   Top1 74.919705   Top5 97.187500   BatchTime 0.287907   LR 0.000179
0.85387522
0.85353500
0.85285044
0.85237938
0.85242295
0.85226387
0.85225970
0.85209316
0.85201997
0.85227442
0.85231507
0.85210186
0.85195833
0.85167116
0.85193545
INFO - ==> Top1: 74.998    Top5: 97.190    Loss: 0.723
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.85211861
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [5][   20/   40]   Loss 0.518735   Top1 82.792969   Top5 99.179688   BatchTime 0.122015
INFO - Validation [5][   40/   40]   Loss 0.515010   Top1 82.520000   Top5 99.300000   BatchTime 0.089704
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.0856)
features.2.conv.3 tensor(0.3542)
features.2.conv.6 tensor(0.1427)
features.3.conv.0 tensor(0.0764)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1102)
features.4.conv.0 tensor(0.0470)
features.4.conv.3 tensor(0.3142)
features.4.conv.6 tensor(0.1688)
features.5.conv.0 tensor(0.2520)
features.5.conv.3 tensor(0.4253)
features.5.conv.6 tensor(0.0939)
features.6.conv.0 tensor(0.0630)
features.6.conv.3 tensor(0.0631)
features.6.conv.6 tensor(0.0906)
features.7.conv.0 tensor(0.1547)
features.7.conv.3 tensor(0.4485)
features.7.conv.6 tensor(0.1912)
features.8.conv.0 tensor(0.4257)
features.8.conv.3 tensor(0.5341)
features.8.conv.6 tensor(0.1459)
features.9.conv.0 tensor(0.3822)
features.9.conv.3 tensor(0.5642)
features.9.conv.6 tensor(0.1440)
features.10.conv.0 tensor(0.0641)
features.10.conv.3 tensor(0.1076)
features.10.conv.6 tensor(0.1037)
features.11.conv.0 tensor(0.6879)
features.11.conv.3 tensor(0.6470)
features.11.conv.6 tensor(0.1984)
features.12.conv.0 tensor(0.4521)
features.12.conv.3 tensor(0.6780)
features.12.conv.6 tensor(0.1987)
features.13.conv.0 tensor(0.2759)
features.13.conv.3 tensor(0.4880)
features.13.conv.6 tensor(0.0888)
features.14.conv.0 tensor(0.8454)
features.14.conv.3 tensor(0.8341)
features.14.conv.6 tensor(0.1397)
features.15.conv.0 tensor(0.7157)
features.15.conv.3 tensor(0.8192)
features.15.conv.6 tensor(0.9634)
features.16.conv.0 tensor(0.4063)
features.16.conv.3 tensor(0.8002)
features.16.conv.6 tensor(0.0797)
conv.0 tensor(0.0935)
tensor(729001.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.85210854
0.85225993
0.85239947
0.85199982
0.85168517
0.85163677
0.85174990
0.85170019
0.85161293
0.85164213
0.85194397
0.85226208
0.85198772
0.85181189
0.85182828
0.85138088
0.85114723
0.85098565
0.85083628
0.85101306
0.85095429
INFO - Training [6][   20/  196]   Loss 0.718314   Top1 75.488281   Top5 96.953125   BatchTime 0.340837   LR 0.000166
0.85088050
0.85085684
0.85097927
0.85107780
0.85099411
0.85119301
0.85138458
0.85160309
0.85173923
0.85164160
0.85133737
0.85124451
0.85116750
0.85130221
0.85294336
INFO - Training [6][   40/  196]   Loss 0.711974   Top1 75.556641   Top5 97.197266   BatchTime 0.306062   LR 0.000158
0.85284084
0.85269964
0.85317087
0.85359651
0.85319746
0.85306895
0.85300291
0.85305840
0.85301018
0.85317791
0.85324365
0.85334522
0.85330111
0.85306638
0.85310322
0.85313880
0.85319299
0.85351521
0.85371125
0.85337728
0.85367996
INFO - Training [6][   60/  196]   Loss 0.700052   Top1 75.989583   Top5 97.259115   BatchTime 0.299174   LR 0.000151
0.85347724
0.85339957
0.85315156
0.85313231
0.85301566
0.85293931
0.85309386
0.85295689
0.85298455
0.85307753
0.85322708
0.85339344
0.85368496
0.85334444
0.85290229
0.85325265
0.85329962
0.85314888
0.85350317
0.85335779
0.85294467
0.85260540
INFO - Training [6][   80/  196]   Loss 0.689307   Top1 76.235352   Top5 97.421875   BatchTime 0.293291   LR 0.000143
0.85252571
0.85270500
0.85247838
0.85251117
0.85264844
0.85243559
0.85241246
0.85242993
0.85233170
0.85259682
0.85295159
0.85280496
0.85266942
0.85302097
0.85330606
0.85284895
0.85279250
0.85293049
0.85282290
0.85262173
0.85283381
INFO - Training [6][  100/  196]   Loss 0.682853   Top1 76.378906   Top5 97.496094   BatchTime 0.292634   LR 0.000136
0.85268068
0.85239655
0.85255510
0.85238004
0.85210204
0.85229087
0.85231376
0.85228664
0.85238367
0.85225403
0.85203415
0.85184860
0.85191989
0.85194081
0.85205317
0.85197341
0.85199505
0.85191214
0.85213429
INFO - Training [6][  120/  196]   Loss 0.680559   Top1 76.494141   Top5 97.607422   BatchTime 0.295542   LR 0.000129
0.85193723
0.85192078
0.85198069
0.85194355
0.85180718
0.85201317
0.85209543
0.85187948
0.85163766
0.85171282
0.85162449
0.85164243
0.85166389
0.85160810
0.85135788
0.85145420
0.85154992
0.85143548
0.85144395
0.85120296
0.85111481
0.85105616
INFO - Training [6][  140/  196]   Loss 0.677802   Top1 76.570871   Top5 97.706473   BatchTime 0.292978   LR 0.000122
0.85124373
0.85123879
0.85124218
0.85126156
0.85119247
0.85157967
0.85141784
0.85143125
0.85169649
0.85167301
0.85161632
0.85153246
0.85134947
0.85142213
0.85137802
0.85126781
0.85080254
0.85009158
0.85027671
0.84980261
0.84986001
INFO - Training [6][  160/  196]   Loss 0.678239   Top1 76.557617   Top5 97.685547   BatchTime 0.292024   LR 0.000115
0.84999728
0.85006428
0.84964269
0.84836352
0.84685260
0.84382850
0.84030586
0.83543611
0.83176088
0.83139116
0.83063751
0.82955128
0.82857221
0.82784039
0.82745665
INFO - Training [6][  180/  196]   Loss 0.676490   Top1 76.612413   Top5 97.654080   BatchTime 0.289804   LR 0.000108
0.82718188
0.82700753
0.82707024
0.82725453
0.82752240
0.82749414
0.82721901
0.82702106
0.82682568
0.82680792
0.82690275
0.82694030
0.82706398
0.82718456
0.82733065
INFO - ==> Top1: 76.636    Top5: 97.654    Loss: 0.676
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82723033
0.82679623
0.82689333
0.82671034
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.472442   Top1 84.570312   Top5 99.257812   BatchTime 0.120498
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1445)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0825)
features.2.conv.0 tensor(0.0914)
features.2.conv.3 tensor(0.3588)
features.2.conv.6 tensor(0.1374)
features.3.conv.0 tensor(0.0761)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1092)
features.4.conv.0 tensor(0.0521)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1587)
features.5.conv.0 tensor(0.2541)
features.5.conv.3 tensor(0.4248)
features.5.conv.6 tensor(0.0929)
features.6.conv.0 tensor(0.0599)
features.6.conv.3 tensor(0.0590)
features.6.conv.6 tensor(0.0905)
features.7.conv.0 tensor(0.1594)
features.7.conv.3 tensor(0.4485)
features.7.conv.6 tensor(0.1906)
features.8.conv.0 tensor(0.4401)
features.8.conv.3 tensor(0.5330)
features.8.conv.6 tensor(0.1462)
features.9.conv.0 tensor(0.3802)
features.9.conv.3 tensor(0.5631)
features.9.conv.6 tensor(0.1462)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.1105)
features.10.conv.6 tensor(0.1025)
features.11.conv.0 tensor(0.7214)
features.11.conv.3 tensor(0.6489)
features.11.conv.6 tensor(0.1856)
features.12.conv.0 tensor(0.4518)
features.12.conv.3 tensor(0.6771)
features.12.conv.6 tensor(0.1891)
features.13.conv.0 tensor(0.2769)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.0881)
features.14.conv.0 tensor(0.8536)
features.14.conv.3 tensor(0.8347)
features.14.conv.6 tensor(0.1448)
features.15.conv.0 tensor(0.7254)
features.15.conv.3 tensor(0.8196)
features.15.conv.6 tensor(0.9623)
features.16.conv.0 tensor(0.8450)
features.16.conv.3 tensor(0.7999)
features.16.conv.6 tensor(0.0785)
conv.0 tensor(0.0941)
tensor(800600.) 2188896.0
INFO - Validation [6][   40/   40]   Loss 0.472245   Top1 84.370000   Top5 99.330000   BatchTime 0.087212
INFO - ==> Top1: 84.370    Top5: 99.330    Loss: 0.472
INFO - ==> Sparsity : 0.366
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 84.370   Top5: 99.330]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 82.520   Top5: 99.300]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 82.500   Top5: 99.020]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.82673568
0.82657915
0.82647389
0.82652879
0.82670295
0.82690477
0.82678020
0.82699573
0.82725781
0.82735926
0.82719600
0.82708424
0.82718027
0.82709467
0.82706702
0.82707661
0.82716507
0.82716680
0.82724583
0.82776868
INFO - Training [7][   20/  196]   Loss 0.672531   Top1 76.718750   Top5 97.265625   BatchTime 0.350733   LR 0.000097
0.82771772
0.82747537
0.82760692
0.82753563
0.82737392
0.82725191
0.82720160
0.82721198
0.82723516
0.82738912
0.82727569
0.82743984
0.82724595
0.82733172
0.82696629
0.82670677
0.82653248
0.82662576
0.82629675
0.82615137
0.82645720
0.82638937
INFO - Training [7][   40/  196]   Loss 0.667795   Top1 76.835938   Top5 97.558594   BatchTime 0.312749   LR 0.000091
0.82630301
0.82607919
0.82587487
0.82568514
0.82497543
0.82418972
0.82380778
0.82319784
0.82274473
0.82221717
0.82186228
0.82161576
0.82138252
0.82084608
INFO - Training [7][   60/  196]   Loss 0.658860   Top1 77.141927   Top5 97.610677   BatchTime 0.301509   LR 0.000085
0.82032865
0.81942624
0.81861502
0.81765866
0.81697619
0.81625706
0.81570166
0.81520677
0.81512582
0.81489027
0.81511408
0.81529593
0.81555796
0.81554753
0.81555450
0.81561905
0.81539303
0.81535709
0.81541902
0.81534791
INFO - Training [7][   80/  196]   Loss 0.655940   Top1 77.260742   Top5 97.724609   BatchTime 0.301972   LR 0.000079
0.81565660
0.81545782
0.81512475
0.81506252
0.81505632
0.81494635
0.81501132
0.81498003
0.81523860
0.81486344
0.81479293
0.81483793
0.81480843
0.81486547
0.81491399
0.81486052
0.81515920
0.81530350
0.81516331
0.81520903
0.81517112
INFO - Training [7][  100/  196]   Loss 0.647004   Top1 77.679688   Top5 97.781250   BatchTime 0.299399   LR 0.000073
0.81510311
0.81467283
0.81414187
0.81409293
0.81396961
0.81391841
0.81375808
0.81383449
0.81365269
0.81310588
0.81248528
0.81227177
0.81207556
0.81165409
0.81142229
0.81098384
0.81013083
0.80925339
0.80844700
0.80756444
0.80697221
0.80625767
0.80563068
INFO - Training [7][  120/  196]   Loss 0.639994   Top1 77.949219   Top5 97.861328   BatchTime 0.292946   LR 0.000067
0.80483371
0.80385864
0.80309010
0.80222249
0.80186957
0.80173582
0.80149990
0.80126911
0.80118197
0.80089307
0.80093098
0.80051148
0.80036104
0.80036652
0.80007565
INFO - Training [7][  140/  196]   Loss 0.638114   Top1 78.074777   Top5 97.929688   BatchTime 0.290122   LR 0.000062
0.80017573
0.79987222
0.79972708
0.80014795
0.79994547
0.79963261
0.79946125
0.79931891
0.79900116
0.79877949
0.79855204
0.79835534
0.79843295
0.79856777
0.79837090
0.79826570
0.79801768
0.79804802
0.79784107
0.79759127
0.79708707
INFO - Training [7][  160/  196]   Loss 0.639295   Top1 78.056641   Top5 97.919922   BatchTime 0.288462   LR 0.000057
0.79693896
0.79695755
0.79692024
0.79658431
0.79663414
0.79645944
0.79622966
0.79602122
0.79596788
0.79569656
0.79587626
0.79577744
0.79573756
0.79600340
0.79586977
0.79571205
0.79547399
0.79514873
0.79397160
0.79351270
0.79326349
0.79328537
0.79344571
INFO - Training [7][  180/  196]   Loss 0.638810   Top1 78.081597   Top5 97.923177   BatchTime 0.284771   LR 0.000052
0.79316115
0.79305393
0.79297793
0.79281861
0.79259276
0.79251319
0.79250580
0.79222047
0.79209286
0.79185754
0.79155815
0.79120898
0.79134750
0.79132438
0.79114431
INFO - ==> Top1: 78.074    Top5: 97.922    Loss: 0.638
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79103547
0.79086679
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 0.427193   Top1 85.312500   Top5 99.335938   BatchTime 0.125406
INFO - Validation [7][   40/   40]   Loss 0.425314   Top1 85.390000   Top5 99.450000   BatchTime 0.092849
INFO - ==> Top1: 85.390    Top5: 99.450    Loss: 0.425
INFO - ==> Sparsity : 0.406
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 84.370   Top5: 99.330]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 82.520   Top5: 99.300]
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0527)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.0914)
features.2.conv.3 tensor(0.3565)
features.2.conv.6 tensor(0.1363)
features.3.conv.0 tensor(0.0761)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1124)
features.4.conv.0 tensor(0.0514)
features.4.conv.3 tensor(0.3113)
features.4.conv.6 tensor(0.1606)
features.5.conv.0 tensor(0.2580)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.0955)
features.6.conv.0 tensor(0.0635)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0902)
features.7.conv.0 tensor(0.1598)
features.7.conv.3 tensor(0.4488)
features.7.conv.6 tensor(0.1897)
features.8.conv.0 tensor(0.4517)
features.8.conv.3 tensor(0.5359)
features.8.conv.6 tensor(0.1455)
features.9.conv.0 tensor(0.4156)
features.9.conv.3 tensor(0.5625)
features.9.conv.6 tensor(0.1510)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.1071)
features.10.conv.6 tensor(0.1032)
features.11.conv.0 tensor(0.7361)
features.11.conv.3 tensor(0.6481)
features.11.conv.6 tensor(0.1866)
features.12.conv.0 tensor(0.6217)
features.12.conv.3 tensor(0.6771)
features.12.conv.6 tensor(0.1764)
features.13.conv.0 tensor(0.2762)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.0883)
features.14.conv.0 tensor(0.8713)
features.14.conv.3 tensor(0.8345)
features.14.conv.6 tensor(0.7222)
features.15.conv.0 tensor(0.7417)
features.15.conv.3 tensor(0.8197)
features.15.conv.6 tensor(0.9641)
features.16.conv.0 tensor(0.7282)
features.16.conv.3 tensor(0.8009)
features.16.conv.6 tensor(0.0810)
conv.0 tensor(0.0944)
tensor(888662.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.79078740
0.79016393
0.78958154
0.78937298
0.78896785
0.78868020
0.78840864
0.78814620
0.78810942
0.78805023
0.78794187
0.78794438
0.78788394
0.78740436
0.78732467
0.78723997
0.78726435
0.78710133
0.78697968
0.78702092
0.78700590
INFO - Training [8][   20/  196]   Loss 0.593895   Top1 79.902344   Top5 97.285156   BatchTime 0.340600   LR 0.000043
0.78679478
0.78662997
0.78646708
0.78629953
0.78638387
0.78605896
0.78591394
0.78581870
0.78578043
0.78563315
0.78562188
0.78593528
0.78530449
0.78497082
0.78516978
0.78511292
0.78496492
0.78452933
0.78405792
INFO - Training [8][   40/  196]   Loss 0.632642   Top1 78.378906   Top5 97.421875   BatchTime 0.328073   LR 0.000039
0.78388911
0.78398222
0.78410083
0.78370774
0.78388244
0.78362411
0.78375763
0.78359580
0.78317875
0.78288269
0.78264976
0.78251541
0.78229904
0.78210312
0.78204447
0.78205973
0.78228557
0.78264743
0.78296959
0.78316486
INFO - Training [8][   60/  196]   Loss 0.627878   Top1 78.404948   Top5 97.539062   BatchTime 0.314864   LR 0.000035
0.78304344
0.78274101
0.78271419
0.78283685
0.78279406
0.78268903
0.78234839
0.78249031
0.78245991
0.78247333
0.78255618
0.78218842
0.78209788
0.78180462
INFO - Training [8][   80/  196]   Loss 0.628624   Top1 78.374023   Top5 97.646484   BatchTime 0.305906   LR 0.000031
0.78160983
0.78124058
0.78121513
0.78141999
0.78133661
0.78129071
0.78129286
0.78157502
0.78138113
0.78105187
0.78069359
0.78071940
0.78086430
0.78116548
0.78164226
0.78161758
0.78144300
0.78135484
0.78151876
0.78149587
0.78146029
0.78143185
0.78145057
0.78148329
INFO - Training [8][  100/  196]   Loss 0.621587   Top1 78.605469   Top5 97.757812   BatchTime 0.294426   LR 0.000027
0.78141999
0.78152674
0.78155136
0.78146112
0.78135747
0.78131956
0.78109473
0.78102219
0.78109425
0.78121048
0.78149307
0.78159040
0.78168142
0.78160197
0.78139710
0.78116202
INFO - Training [8][  120/  196]   Loss 0.614533   Top1 78.899740   Top5 97.877604   BatchTime 0.286547   LR 0.000023
0.78123295
0.78115118
0.78112662
0.78099710
0.78099847
0.78106141
0.78114241
0.78101879
0.78077734
0.78068733
0.78039616
0.77974981
0.77966172
0.77919841
0.77912199
0.77921396
0.77919245
0.77911121
0.77896792
0.77918202
0.77920461
0.77913809
0.77921277
0.77907562
0.77912599
INFO - Training [8][  140/  196]   Loss 0.611053   Top1 79.029018   Top5 97.946429   BatchTime 0.281259   LR 0.000020
0.77870905
0.77858889
0.77860188
0.77852005
0.77863818
0.77871591
0.77896732
0.77934670
0.77993542
0.77997082
0.77991420
0.78001261
0.77990341
0.77983308
0.77982908
0.77989334
INFO - Training [8][  160/  196]   Loss 0.613572   Top1 78.940430   Top5 97.961426   BatchTime 0.277820   LR 0.000017
0.77966374
0.77968246
0.77959871
0.77966017
0.77984864
0.78004950
0.77978450
0.77982485
0.77956498
0.77944064
0.77950704
0.77952135
0.77946395
0.77918482
0.77904612
0.77924919
0.77942359
0.77942353
0.77935308
0.77939337
0.77946460
0.77916002
0.77897590
0.77897257
INFO - Training [8][  180/  196]   Loss 0.610697   Top1 79.053819   Top5 97.964410   BatchTime 0.274372   LR 0.000014
0.77902204
0.77900970
0.77928799
0.77922678
0.77915370
0.77907646
0.77911824
0.77914655
0.77905178
0.77894765
0.77888566
0.77889073
0.77910984
0.77915841
0.77892113
0.77889633
INFO - ==> Top1: 79.116    Top5: 97.958    Loss: 0.609
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
0.77881289
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [8][   20/   40]   Loss 0.427713   Top1 85.429688   Top5 99.414062   BatchTime 0.121420
INFO - Validation [8][   40/   40]   Loss 0.421733   Top1 85.590000   Top5 99.480000   BatchTime 0.090071
INFO - ==> Top1: 85.590    Top5: 99.480    Loss: 0.422
INFO - ==> Sparsity : 0.420
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 84.370   Top5: 99.330]
features.0.conv.0 tensor(0.2674)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.0909)
features.2.conv.3 tensor(0.3565)
features.2.conv.6 tensor(0.1337)
features.3.conv.0 tensor(0.0758)
features.3.conv.3 tensor(0.0810)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0550)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1623)
features.5.conv.0 tensor(0.2598)
features.5.conv.3 tensor(0.4236)
features.5.conv.6 tensor(0.0933)
features.6.conv.0 tensor(0.0633)
features.6.conv.3 tensor(0.0602)
features.6.conv.6 tensor(0.0917)
features.7.conv.0 tensor(0.1613)
features.7.conv.3 tensor(0.4479)
features.7.conv.6 tensor(0.1907)
features.8.conv.0 tensor(0.4638)
features.8.conv.3 tensor(0.5373)
features.8.conv.6 tensor(0.1449)
features.9.conv.0 tensor(0.4808)
features.9.conv.3 tensor(0.5611)
features.9.conv.6 tensor(0.1509)
features.10.conv.0 tensor(0.0642)
features.10.conv.3 tensor(0.1082)
features.10.conv.6 tensor(0.1028)
features.11.conv.0 tensor(0.7384)
features.11.conv.3 tensor(0.6487)
features.11.conv.6 tensor(0.1835)
features.12.conv.0 tensor(0.6427)
features.12.conv.3 tensor(0.6771)
features.12.conv.6 tensor(0.1801)
features.13.conv.0 tensor(0.2818)
features.13.conv.3 tensor(0.4894)
features.13.conv.6 tensor(0.0881)
features.14.conv.0 tensor(0.8739)
features.14.conv.3 tensor(0.8345)
features.14.conv.6 tensor(0.7946)
features.15.conv.0 tensor(0.7444)
features.15.conv.3 tensor(0.8194)
features.15.conv.6 tensor(0.9636)
features.16.conv.0 tensor(0.7288)
features.16.conv.3 tensor(0.8010)
features.16.conv.6 tensor(0.1276)
conv.0 tensor(0.0941)
tensor(918365.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.77889782
0.77805477
0.77776814
0.77741283
0.77673292
0.77670097
0.77664620
0.77673721
0.77674580
0.77641666
0.77619600
0.77602363
0.77606159
0.77594668
0.77591282
0.77586436
0.77607459
0.77623141
0.77601886
0.77614272
INFO - Training [9][   20/  196]   Loss 0.595923   Top1 79.687500   Top5 97.558594   BatchTime 0.374305   LR 0.000010
0.77606696
0.77588546
0.77584600
0.77607709
0.77573740
0.77570707
0.77551973
0.77548575
0.77556181
0.77554619
0.77561104
0.77576745
0.77599138
0.77644533
INFO - Training [9][   40/  196]   Loss 0.617230   Top1 79.023438   Top5 97.421875   BatchTime 0.327930   LR 0.000008
0.77622986
0.77604508
0.77579784
0.77559674
0.77554601
0.77549988
0.77454841
0.77401894
0.77385885
0.77388096
0.77385926
0.77382362
0.77389771
0.77401251
0.77398217
0.77362144
0.77357483
0.77286333
0.77272826
0.77286404
0.77270752
0.77222210
INFO - Training [9][   60/  196]   Loss 0.606099   Top1 79.348958   Top5 97.604167   BatchTime 0.308465   LR 0.000006
0.77148807
0.77131283
0.77135903
0.77136171
0.77131921
0.77087456
0.77061194
0.77059931
0.77063596
0.77084070
0.77089697
0.77071810
0.77100712
0.77080542
0.77061087
0.77076995
0.77086484
0.77087879
0.77065998
0.77049023
0.77064431
0.77064574
INFO - Training [9][   80/  196]   Loss 0.606486   Top1 79.277344   Top5 97.812500   BatchTime 0.299125   LR 0.000004
0.77044731
0.77037162
0.77017397
0.77029723
0.77018315
0.77034289
0.77046824
0.77056831
0.77037394
0.77035242
0.77039886
0.77029753
0.77028048
0.77041638
0.77069736
0.77068919
0.77070826
0.77070212
0.77073836
0.77058142
0.77018672
0.76976353
0.76913315
INFO - Training [9][  100/  196]   Loss 0.599272   Top1 79.605469   Top5 97.898438   BatchTime 0.293034   LR 0.000003
0.76840287
0.76802033
0.76806486
0.76804215
0.76789635
0.76781517
0.76779097
0.76764244
0.76760644
0.76761657
0.76741511
0.76668495
0.76624477
0.76584309
0.76473767
INFO - Training [9][  120/  196]   Loss 0.591393   Top1 79.817708   Top5 98.020833   BatchTime 0.287920   LR 0.000002
0.76449692
0.76433134
0.76418841
0.76416731
0.76420933
0.76414597
0.76403022
0.76395279
0.76394421
0.76393384
0.76387936
0.76385075
0.76378006
0.76381999
0.76380920
0.76374656
0.76379776
0.76375443
0.76368994
0.76366270
0.76361132
0.76364374
0.76363653
INFO - Training [9][  140/  196]   Loss 0.590442   Top1 79.846540   Top5 98.041295   BatchTime 0.284296   LR 0.000001
0.76367974
0.76366180
0.76371533
0.76369774
0.76370704
0.76378644
0.76370984
0.76373458
0.76373869
0.76373708
0.76368821
0.76371819
0.76381052
0.76397741
0.76389498
0.76389843
INFO - Training [9][  160/  196]   Loss 0.596905   Top1 79.602051   Top5 98.020020   BatchTime 0.279537   LR 0.000000
0.76381898
0.76375806
0.76370174
0.76370198
0.76370394
0.76373893
0.76371384
0.76378852
0.76386684
0.76374662
0.76370543
0.76371729
0.76378304
0.76383996
0.76382381
0.76387972
0.76385891
0.76381010
0.76376301
0.76373148
0.76368535
0.76368237
0.76374233
0.76376790
INFO - Training [9][  180/  196]   Loss 0.594459   Top1 79.674479   Top5 98.018663   BatchTime 0.276571   LR 0.000000
0.76376951
0.76375633
0.76380831
0.76369274
0.76361012
0.76350796
0.76332653
0.76301926
0.76252329
0.76237226
0.76232171
0.76230454
0.76233596
0.76233536
0.76227063
0.76222992
INFO - ==> Top1: 79.766    Top5: 98.050    Loss: 0.592
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
0.76220411
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.425153   Top1 85.488281   Top5 99.433594   BatchTime 0.120136
INFO - Validation [9][   40/   40]   Loss 0.418273   Top1 85.800000   Top5 99.490000   BatchTime 0.091365
INFO - ==> Top1: 85.800    Top5: 99.490    Loss: 0.418
INFO - ==> Sparsity : 0.424
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
features.0.conv.0 tensor(0.2708)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.0917)
features.2.conv.3 tensor(0.3573)
features.2.conv.6 tensor(0.1372)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0535)
features.4.conv.3 tensor(0.3096)
features.4.conv.6 tensor(0.1626)
features.5.conv.0 tensor(0.2604)
features.5.conv.3 tensor(0.4242)
features.5.conv.6 tensor(0.0946)
features.6.conv.0 tensor(0.0641)
features.6.conv.3 tensor(0.0590)
features.6.conv.6 tensor(0.0908)
features.7.conv.0 tensor(0.1616)
features.7.conv.3 tensor(0.4476)
features.7.conv.6 tensor(0.1962)
features.8.conv.0 tensor(0.4653)
features.8.conv.3 tensor(0.5379)
features.8.conv.6 tensor(0.1595)
features.9.conv.0 tensor(0.4760)
features.9.conv.3 tensor(0.5613)
features.9.conv.6 tensor(0.1560)
features.10.conv.0 tensor(0.0639)
features.10.conv.3 tensor(0.1079)
features.10.conv.6 tensor(0.1032)
features.11.conv.0 tensor(0.7411)
features.11.conv.3 tensor(0.6487)
features.11.conv.6 tensor(0.2138)
features.12.conv.0 tensor(0.6454)
features.12.conv.3 tensor(0.6771)
features.12.conv.6 tensor(0.2138)
features.13.conv.0 tensor(0.2865)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.0967)
features.14.conv.0 tensor(0.8738)
features.14.conv.3 tensor(0.8345)
features.14.conv.6 tensor(0.7998)
features.15.conv.0 tensor(0.7459)
features.15.conv.3 tensor(0.8194)
features.15.conv.6 tensor(0.9636)
features.16.conv.0 tensor(0.7281)
features.16.conv.3 tensor(0.8012)
features.16.conv.6 tensor(0.1417)
conv.0 tensor(0.0941)
tensor(929045.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.76220244
0.76983291
0.77162492
0.77252376
0.77253878
0.77411181
0.77451795
0.77442515
0.77348799
0.77273875
0.77239472
0.77279973
0.77252591
0.77242362
0.77228057
INFO - Training [10][   20/  196]   Loss 0.663499   Top1 77.226562   Top5 97.070312   BatchTime 0.349413   LR 0.000250
0.77237248
0.77353817
0.77338505
0.77373749
0.77390254
0.77406073
0.77396363
0.77395779
0.77405792
0.77429813
0.77483356
0.77668202
0.77759188
0.77823281
0.77911210
0.77970099
0.78027976
0.78157860
0.78294665
0.78320539
0.78419793
0.78603715
INFO - Training [10][   40/  196]   Loss 0.675558   Top1 76.855469   Top5 97.265625   BatchTime 0.311277   LR 0.000250
0.78679210
0.78675318
0.78695828
0.78698206
0.78716409
0.78724349
0.78732044
0.78749120
0.78796220
0.78814894
0.78842098
0.78887200
0.78914148
0.78920782
0.78914094
0.78926456
0.78971297
0.78973746
0.78982884
0.78999150
0.79008156
0.79025197
INFO - Training [10][   60/  196]   Loss 0.672224   Top1 76.861979   Top5 97.402344   BatchTime 0.297274   LR 0.000250
0.79016978
0.79033232
0.79021168
0.79038644
0.79079688
0.79054970
0.79066193
0.79038817
0.79010201
0.79005539
0.79009455
0.79055429
0.79060137
0.79047686
0.79069465
0.79099524
0.79105735
0.79099667
0.79110914
0.79101598
0.79110879
INFO - Training [10][   80/  196]   Loss 0.678765   Top1 76.713867   Top5 97.490234   BatchTime 0.294341   LR 0.000250
0.79126787
0.79138297
0.79127461
0.79093045
0.79077119
0.79080439
0.79080760
0.79078984
0.79085511
0.79096264
0.79082149
0.79079247
0.79071760
0.79089302
0.79094958
INFO - Training [10][  100/  196]   Loss 0.675918   Top1 76.820312   Top5 97.527344   BatchTime 0.289853   LR 0.000250
0.79104543
0.79104859
0.79100406
0.79105496
0.79105526
0.79104286
0.79087806
0.79097229
0.79079795
0.79071903
0.79076481
0.79075205
0.79057580
0.79057580
0.79071182
0.79098499
0.79095232
0.79096943
0.79043722
0.79025954
0.79001015
0.79009485
INFO - Training [10][  120/  196]   Loss 0.670242   Top1 76.969401   Top5 97.656250   BatchTime 0.287815   LR 0.000249
0.79001176
0.78991264
0.78982913
0.78982407
0.78987503
0.78971905
0.78969985
0.78941113
0.78938031
0.78956729
0.78977621
0.79015619
0.79036480
0.79006082
0.79034078
0.79095966
0.79081881
0.79070103
0.79069555
0.79076630
0.79074717
0.79072487
INFO - Training [10][  140/  196]   Loss 0.669813   Top1 77.003348   Top5 97.712054   BatchTime 0.286266   LR 0.000249
0.79074913
0.79115063
0.79099166
0.79116464
0.79104143
0.79106343
0.79142255
0.79150307
0.79174209
0.79180139
0.79187053
0.79166436
0.79171771
0.79195178
0.79161048
0.79134035
0.79123545
0.79134166
0.79104650
0.79093856
0.79092872
0.79102772
INFO - Training [10][  160/  196]   Loss 0.676457   Top1 76.779785   Top5 97.624512   BatchTime 0.284004   LR 0.000249
0.79117525
0.79128975
0.79106653
0.79083085
0.79084736
0.79075056
0.79080671
0.79066765
0.79058176
0.79059422
0.79059958
0.79060358
0.79063982
0.79067683
0.79052699
INFO - Training [10][  180/  196]   Loss 0.676207   Top1 76.684028   Top5 97.608507   BatchTime 0.284152   LR 0.000249
0.79046947
0.79036564
0.79007035
0.78982836
0.78964382
0.78960502
0.78952646
0.78939587
0.78951937
0.78950256
0.78931749
0.78961045
0.78965998
0.78968537
0.78949994
0.78935200
0.78955758
0.78947979
0.78929132
0.78930676
INFO - ==> Top1: 76.716    Top5: 97.600    Loss: 0.676
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.554690   Top1 81.582031   Top5 98.867188   BatchTime 0.128604
INFO - Validation [10][   40/   40]   Loss 0.542962   Top1 81.790000   Top5 99.070000   BatchTime 0.093943
INFO - ==> Top1: 81.790    Top5: 99.070    Loss: 0.543
INFO - ==> Sparsity : 0.411
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2500)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0920)
features.2.conv.0 tensor(0.0874)
features.2.conv.3 tensor(0.3526)
features.2.conv.6 tensor(0.1412)
features.3.conv.0 tensor(0.0694)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1105)
features.4.conv.0 tensor(0.0487)
features.4.conv.3 tensor(0.3131)
features.4.conv.6 tensor(0.1538)
features.5.conv.0 tensor(0.2562)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.0985)
features.6.conv.0 tensor(0.0578)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0898)
features.7.conv.0 tensor(0.1538)
features.7.conv.3 tensor(0.4491)
features.7.conv.6 tensor(0.1944)
features.8.conv.0 tensor(0.4427)
features.8.conv.3 tensor(0.5350)
features.8.conv.6 tensor(0.1431)
features.9.conv.0 tensor(0.2974)
features.9.conv.3 tensor(0.5616)
features.9.conv.6 tensor(0.1379)
features.10.conv.0 tensor(0.0650)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0683)
features.11.conv.0 tensor(0.6908)
features.11.conv.3 tensor(0.6422)
features.11.conv.6 tensor(0.1987)
features.12.conv.0 tensor(0.5688)
features.12.conv.3 tensor(0.6713)
features.12.conv.6 tensor(0.2166)
features.13.conv.0 tensor(0.2782)
features.13.conv.3 tensor(0.4871)
features.13.conv.6 tensor(0.0868)
features.14.conv.0 tensor(0.8613)
features.14.conv.3 tensor(0.8355)
features.14.conv.6 tensor(0.8978)
features.15.conv.0 tensor(0.7293)
features.15.conv.3 tensor(0.8200)
features.15.conv.6 tensor(0.9670)
features.16.conv.0 tensor(0.6923)
features.16.conv.3 tensor(0.7985)
features.16.conv.6 tensor(0.0839)
conv.0 tensor(0.0916)
tensor(899249.) 2188896.0
0.78940672
0.78968543
0.78972399
0.79010558
0.78995234
0.78996444
0.79002684
0.78996682
0.79006630
0.79013425
0.79018861
0.79036433
0.79008198
0.79013830
0.79002452
0.78976977
INFO - Training [11][   20/  196]   Loss 0.684008   Top1 76.621094   Top5 97.148438   BatchTime 0.365492   LR 0.000248
0.78965145
0.78968674
0.78968263
0.78979051
0.78992409
0.78962505
0.78949559
0.78952265
0.78945214
0.78930098
0.78927356
0.78913003
0.78910404
0.78917933
0.78939384
0.78957695
0.78971338
0.78997004
0.78967792
0.78961885
0.78970349
0.78954113
INFO - Training [11][   40/  196]   Loss 0.685694   Top1 76.484375   Top5 97.197266   BatchTime 0.321865   LR 0.000248
0.78936112
0.78909439
0.78921831
0.78908765
0.78928936
0.78939104
0.78952777
0.78985953
0.79009438
0.79010087
0.78976148
0.78952473
0.78938431
0.78942448
0.78946084
0.78935862
0.78923798
0.78931552
0.78930026
0.78940964
0.78931344
0.78922909
INFO - Training [11][   60/  196]   Loss 0.682650   Top1 76.497396   Top5 97.402344   BatchTime 0.305705   LR 0.000247
0.78927273
0.78917527
0.78901893
0.78908497
0.78925663
0.78926069
0.78926742
0.78931165
0.78934371
0.78944111
0.78947157
0.78928435
0.78914899
0.78906018
0.78870797
INFO - Training [11][   80/  196]   Loss 0.684454   Top1 76.459961   Top5 97.524414   BatchTime 0.297152   LR 0.000247
0.78870821
0.78870821
0.78853178
0.78858238
0.78874141
0.78918034
0.78937215
0.78962380
0.79010415
0.79003149
0.78994560
0.78982759
0.78974128
0.78968793
0.78969562
0.78955662
0.78958434
0.78956175
0.78954411
0.78975004
0.78967983
INFO - Training [11][  100/  196]   Loss 0.675287   Top1 76.753906   Top5 97.644531   BatchTime 0.294095   LR 0.000247
0.78959197
0.78947705
0.78961647
0.78965086
0.78971428
0.78970486
0.78934020
0.78922558
0.78929538
0.78926820
0.78958535
0.78957164
0.78970677
0.78954786
0.78943491
0.78932458
0.78890699
0.78873253
0.78849059
0.78834969
0.78827339
0.78837878
INFO - Training [11][  120/  196]   Loss 0.667282   Top1 77.031250   Top5 97.737630   BatchTime 0.291971   LR 0.000246
0.78866446
0.78878123
0.78870326
0.78873289
0.78873938
0.78857207
0.78829104
0.78825098
0.78836620
0.78854281
0.78875071
0.78912693
0.78861803
0.78864878
0.78883678
0.78877991
0.78910393
0.78891319
0.78901154
0.78900063
0.78881758
INFO - Training [11][  140/  196]   Loss 0.671006   Top1 76.975446   Top5 97.720424   BatchTime 0.289538   LR 0.000246
0.78884727
0.78866422
0.78854591
0.78853756
0.78856581
0.78877211
0.78883159
0.78931463
0.78907639
0.78912872
0.78891420
0.78880715
0.78865886
0.78872919
0.78870165
0.78879118
0.78875679
0.78914505
0.78925306
0.78905267
0.78878897
INFO - Training [11][  160/  196]   Loss 0.673082   Top1 76.950684   Top5 97.692871   BatchTime 0.290590   LR 0.000245
0.78855586
0.78837150
0.78830010
0.78826356
0.78833020
0.78816438
0.78809768
0.78842050
0.78854918
0.78852957
0.78833365
0.78822094
0.78789788
0.78800052
INFO - Training [11][  180/  196]   Loss 0.672848   Top1 76.881510   Top5 97.636719   BatchTime 0.288964   LR 0.000244
0.78829080
0.78827912
0.78824788
0.78836697
0.78835577
0.78822762
0.78866452
0.78858274
0.78857857
0.78819937
0.78841656
0.78818405
0.78789026
0.78787214
0.78773957
0.78777862
0.78783387
0.78795528
0.78787231
0.78794134
0.78792530
0.78806514
********************pre-trained*****************
INFO - ==> Top1: 76.934    Top5: 97.628    Loss: 0.672
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [11][   20/   40]   Loss 0.516042   Top1 82.734375   Top5 99.199219   BatchTime 0.127250
INFO - Validation [11][   40/   40]   Loss 0.506410   Top1 82.690000   Top5 99.270000   BatchTime 0.093390
INFO - ==> Top1: 82.690    Top5: 99.270    Loss: 0.506
INFO - ==> Sparsity : 0.415
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1152)
features.1.conv.0 tensor(0.0449)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0803)
features.2.conv.0 tensor(0.0735)
features.2.conv.3 tensor(0.3580)
features.2.conv.6 tensor(0.1409)
features.3.conv.0 tensor(0.0816)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1063)
features.4.conv.0 tensor(0.0389)
features.4.conv.3 tensor(0.3154)
features.4.conv.6 tensor(0.1580)
features.5.conv.0 tensor(0.2536)
features.5.conv.3 tensor(0.4225)
features.5.conv.6 tensor(0.0999)
features.6.conv.0 tensor(0.0610)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0882)
features.7.conv.0 tensor(0.1562)
features.7.conv.3 tensor(0.4525)
features.7.conv.6 tensor(0.1839)
features.8.conv.0 tensor(0.4443)
features.8.conv.3 tensor(0.5350)
features.8.conv.6 tensor(0.1419)
features.9.conv.0 tensor(0.2950)
features.9.conv.3 tensor(0.5608)
features.9.conv.6 tensor(0.1464)
features.10.conv.0 tensor(0.0694)
features.10.conv.3 tensor(0.1123)
features.10.conv.6 tensor(0.0705)
features.11.conv.0 tensor(0.7119)
features.11.conv.3 tensor(0.6416)
features.11.conv.6 tensor(0.2038)
features.12.conv.0 tensor(0.5802)
features.12.conv.3 tensor(0.6717)
features.12.conv.6 tensor(0.2088)
features.13.conv.0 tensor(0.2770)
features.13.conv.3 tensor(0.4894)
features.13.conv.6 tensor(0.0890)
features.14.conv.0 tensor(0.8516)
features.14.conv.3 tensor(0.8356)
features.14.conv.6 tensor(0.9120)
features.15.conv.0 tensor(0.7426)
features.15.conv.3 tensor(0.8208)
features.15.conv.6 tensor(0.9661)
features.16.conv.0 tensor(0.6980)
features.16.conv.3 tensor(0.7981)
features.16.conv.6 tensor(0.0925)
conv.0 tensor(0.0920)
tensor(907410.) 2188896.0
0.78803343
0.78816676
0.78817010
0.78785455
0.78785926
0.78773266
0.78784215
0.78801990
0.78764379
0.78776574
0.78779435
0.78772318
0.78766507
0.78777301
0.78795189
0.78791958
0.78781164
0.78758746
0.78747654
0.78742677
INFO - Training [12][   20/  196]   Loss 0.682794   Top1 77.089844   Top5 97.187500   BatchTime 0.344548   LR 0.000243
0.78747374
0.78713810
0.78719103
0.78705478
0.78731430
0.78723133
0.78725404
0.78713638
0.78730136
0.78755337
0.78783232
0.78812599
0.78831595
0.78831804
0.78817517
INFO - Training [12][   40/  196]   Loss 0.685437   Top1 76.533203   Top5 97.167969   BatchTime 0.310362   LR 0.000243
0.78798378
0.78790790
0.78777462
0.78768843
0.78760797
0.78792357
0.78793961
0.78759062
0.78771806
0.78779221
0.78769177
0.78774577
0.78775603
0.78757173
0.78741455
0.78731257
0.78742927
0.78767669
0.78756130
0.78745377
0.78760779
0.78734708
INFO - Training [12][   60/  196]   Loss 0.675956   Top1 76.927083   Top5 97.278646   BatchTime 0.296833   LR 0.000242
0.78736460
0.78723747
0.78728598
0.78734982
0.78725445
0.78739512
0.78742927
0.78755856
0.78794265
0.78824043
0.78794676
0.78782749
0.78766239
0.78734511
0.78736168
0.78749794
0.78758758
0.78750533
0.78739864
0.78718084
0.78715605
0.78666419
0.78667229
INFO - Training [12][   80/  196]   Loss 0.678321   Top1 76.831055   Top5 97.387695   BatchTime 0.288246   LR 0.000241
0.78678006
0.78656793
0.78646052
0.78638881
0.78628397
0.78650922
0.78651428
0.78689373
0.78673822
0.78667831
0.78686249
0.78689194
0.78740621
0.78712785
0.78707242
INFO - Training [12][  100/  196]   Loss 0.673175   Top1 77.015625   Top5 97.433594   BatchTime 0.283509   LR 0.000240
0.78750473
0.78742737
0.78719848
0.78744209
0.78765368
0.78736013
0.78739166
0.78746259
0.78703940
0.78705078
0.78672355
0.78662032
0.78663427
0.78616619
0.78622776
0.78642046
0.78651094
0.78640908
0.78630793
0.78618944
0.78590721
0.78595072
INFO - Training [12][  120/  196]   Loss 0.665121   Top1 77.294922   Top5 97.561849   BatchTime 0.280637   LR 0.000240
0.78576839
0.78576469
0.78565150
0.78569812
0.78574419
0.78583604
0.78597969
0.78628767
0.78618842
0.78589433
0.78574246
0.78593212
0.78592134
0.78582919
0.78587300
0.78576219
0.78577232
0.78582221
0.78588599
0.78598017
0.78596449
0.78570515
INFO - Training [12][  140/  196]   Loss 0.662553   Top1 77.391183   Top5 97.619978   BatchTime 0.279929   LR 0.000239
0.78702217
0.78685582
0.78669065
0.78688657
0.78719395
0.78718925
0.78670633
0.78679645
0.78714871
0.78718770
0.78694344
0.78680968
0.78661996
0.78630024
0.78632492
0.78658789
0.78615755
0.78625619
0.78609866
0.78617358
0.78612775
0.78601485
INFO - Training [12][  160/  196]   Loss 0.669207   Top1 77.160645   Top5 97.595215   BatchTime 0.279599   LR 0.000238
0.78627974
0.78625083
0.78652203
0.78670788
0.78665733
0.78671145
0.78695738
0.78671873
0.78679621
0.78696400
0.78713417
0.78645563
0.78641939
0.78683048
0.78665966
0.78683615
INFO - Training [12][  180/  196]   Loss 0.666487   Top1 77.217882   Top5 97.573785   BatchTime 0.277739   LR 0.000237
0.78682810
0.78692955
0.78685212
0.78718323
0.78677684
0.78678918
0.78687054
0.78655344
0.78671449
0.78652585
0.78634119
0.78609121
0.78627509
0.78625649
0.78631592
INFO - ==> Top1: 77.356    Top5: 97.584    Loss: 0.663
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.78633934
0.78628451
0.78633153
0.78634423
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.495756   Top1 83.105469   Top5 99.296875   BatchTime 0.134489
INFO - Validation [12][   40/   40]   Loss 0.489574   Top1 83.270000   Top5 99.340000   BatchTime 0.094879
INFO - ==> Top1: 83.270    Top5: 99.340    Loss: 0.490
INFO - ==> Sparsity : 0.420
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0846)
features.2.conv.0 tensor(0.0764)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.1403)
features.3.conv.0 tensor(0.0611)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1157)
features.4.conv.0 tensor(0.0521)
features.4.conv.3 tensor(0.3166)
features.4.conv.6 tensor(0.1598)
features.5.conv.0 tensor(0.2554)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1003)
features.6.conv.0 tensor(0.0522)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0851)
features.7.conv.0 tensor(0.1547)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.1953)
features.8.conv.0 tensor(0.4427)
features.8.conv.3 tensor(0.5339)
features.8.conv.6 tensor(0.1462)
features.9.conv.0 tensor(0.2944)
features.9.conv.3 tensor(0.5628)
features.9.conv.6 tensor(0.1485)
features.10.conv.0 tensor(0.0717)
features.10.conv.3 tensor(0.1059)
features.10.conv.6 tensor(0.0708)
features.11.conv.0 tensor(0.7155)
features.11.conv.3 tensor(0.6424)
features.11.conv.6 tensor(0.2026)
features.12.conv.0 tensor(0.5888)
features.12.conv.3 tensor(0.6721)
features.12.conv.6 tensor(0.1886)
features.13.conv.0 tensor(0.2716)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.0894)
features.14.conv.0 tensor(0.8672)
features.14.conv.3 tensor(0.8346)
features.14.conv.6 tensor(0.9294)
features.15.conv.0 tensor(0.7675)
features.15.conv.3 tensor(0.8206)
features.15.conv.6 tensor(0.9684)
features.16.conv.0 tensor(0.6996)
features.16.conv.3 tensor(0.7978)
features.16.conv.6 tensor(0.1001)
conv.0 tensor(0.0940)
tensor(919636.) 2188896.0
0.78673798
0.78669715
0.78620458
0.78613079
0.78600127
0.78611755
0.78598911
0.78618962
0.78621048
0.78640503
0.78614587
0.78594911
0.78582925
0.78606898
0.78611255
0.78611618
0.78654414
0.78669763
0.78667974
INFO - Training [13][   20/  196]   Loss 0.673267   Top1 77.031250   Top5 97.070312   BatchTime 0.365292   LR 0.000235
0.78638124
0.78632253
0.78592652
0.78588372
0.78585815
0.78586119
0.78591639
0.78578657
0.78607577
0.78605920
0.78600085
0.78615624
0.78601503
0.78616196
0.78617769
0.78616643
0.78596061
0.78605705
0.78629303
0.78614998
0.78590721
INFO - Training [13][   40/  196]   Loss 0.672853   Top1 76.972656   Top5 97.343750   BatchTime 0.329453   LR 0.000235
0.78597355
0.78609365
0.78607517
0.78572458
0.78561670
0.78552336
0.78551877
0.78548962
0.78561437
0.78568447
0.78569049
0.78579521
0.78581434
0.78563458
0.78537530
0.78533792
0.78541064
0.78514200
0.78507459
INFO - Training [13][   60/  196]   Loss 0.668685   Top1 77.180990   Top5 97.500000   BatchTime 0.324963   LR 0.000234
0.78503066
0.78518814
0.78539807
0.78543955
0.78552681
0.78552198
0.78550285
0.78563219
0.78551459
0.78553629
0.78552645
0.78556281
0.78573674
0.78599703
0.78576285
0.78557652
0.78575569
0.78582662
INFO - Training [13][   80/  196]   Loss 0.660310   Top1 77.329102   Top5 97.651367   BatchTime 0.324680   LR 0.000233
0.78571659
0.78585410
0.78609902
0.78628331
0.78643060
0.78617901
0.78640914
0.78603113
0.78572011
0.78575981
0.78583103
0.78614521
0.78619552
0.78635991
0.78626955
0.78615683
0.78598285
0.78590596
0.78621966
INFO - Training [13][  100/  196]   Loss 0.650411   Top1 77.621094   Top5 97.703125   BatchTime 0.325127   LR 0.000232
0.78650916
0.78642660
0.78648281
0.78624851
0.78603947
0.78637594
0.78648633
0.78668404
0.78642398
0.78668058
0.78600806
0.78590530
0.78580403
0.78547013
0.78540182
0.78523391
0.78526413
0.78570551
0.78587216
0.78604537
INFO - Training [13][  120/  196]   Loss 0.647011   Top1 77.766927   Top5 97.737630   BatchTime 0.318189   LR 0.000230
0.78636837
0.78670627
0.78639883
0.78605789
0.78604913
0.78598058
0.78583008
0.78579098
0.78578949
0.78588611
0.78597742
0.78581548
0.78601283
0.78610837
0.78600442
0.78560245
0.78551888
0.78535193
0.78518540
0.78527969
0.78500766
0.78489864
INFO - Training [13][  140/  196]   Loss 0.646308   Top1 77.790179   Top5 97.823661   BatchTime 0.312245   LR 0.000229
0.78478867
0.78498703
0.78528076
0.78527319
0.78514647
0.78476954
0.78434664
0.78460616
0.78462613
0.78485900
0.78457314
0.78428835
0.78444046
0.78442192
0.78458321
0.78448266
0.78409141
0.78401148
0.78404975
0.78421855
0.78444630
INFO - Training [13][  160/  196]   Loss 0.646310   Top1 77.768555   Top5 97.805176   BatchTime 0.308344   LR 0.000228
0.78459328
0.78438342
0.78439862
0.78426379
0.78429180
0.78427321
0.78420383
0.78432262
0.78450161
0.78437299
0.78436393
0.78437477
0.78440672
0.78446925
0.78465474
0.78477323
0.78452790
0.78431344
0.78413254
0.78406793
0.78367960
0.78357184
INFO - Training [13][  180/  196]   Loss 0.645566   Top1 77.816840   Top5 97.745226   BatchTime 0.305565   LR 0.000227
0.78345931
0.78356981
0.78358543
0.78396767
0.78404307
0.78402925
0.78417522
0.78388274
0.78365320
0.78365958
0.78344792
0.78316081
0.78308785
0.78315473
INFO - ==> Top1: 77.906    Top5: 97.750    Loss: 0.643
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.78320861
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [13][   20/   40]   Loss 0.520782   Top1 82.539062   Top5 99.140625   BatchTime 0.122889
INFO - Validation [13][   40/   40]   Loss 0.527518   Top1 82.160000   Top5 99.190000   BatchTime 0.088613
INFO - ==> Top1: 82.160    Top5: 99.190    Loss: 0.528
INFO - ==> Sparsity : 0.422
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.1348)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0781)
features.2.conv.0 tensor(0.0851)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1406)
features.3.conv.0 tensor(0.0686)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1105)
features.4.conv.0 tensor(0.0495)
features.4.conv.3 tensor(0.3194)
features.4.conv.6 tensor(0.1566)
features.5.conv.0 tensor(0.2508)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.0994)
features.6.conv.0 tensor(0.0482)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0865)
features.7.conv.0 tensor(0.1594)
features.7.conv.3 tensor(0.4444)
features.7.conv.6 tensor(0.2019)
features.8.conv.0 tensor(0.4742)
features.8.conv.3 tensor(0.5341)
features.8.conv.6 tensor(0.1481)
features.9.conv.0 tensor(0.3105)
features.9.conv.3 tensor(0.5619)
features.9.conv.6 tensor(0.1555)
features.10.conv.0 tensor(0.0703)
features.10.conv.3 tensor(0.1062)
features.10.conv.6 tensor(0.0718)
features.11.conv.0 tensor(0.7096)
features.11.conv.3 tensor(0.6424)
features.11.conv.6 tensor(0.2151)
features.12.conv.0 tensor(0.6037)
features.12.conv.3 tensor(0.6717)
features.12.conv.6 tensor(0.2169)
features.13.conv.0 tensor(0.2718)
features.13.conv.3 tensor(0.4904)
features.13.conv.6 tensor(0.0905)
features.14.conv.0 tensor(0.8633)
features.14.conv.3 tensor(0.8348)
features.14.conv.6 tensor(0.9280)
features.15.conv.0 tensor(0.7666)
features.15.conv.3 tensor(0.8214)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.6981)
features.16.conv.3 tensor(0.7977)
features.16.conv.6 tensor(0.1053)
conv.0 tensor(0.0940)
tensor(924449.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
0.78337723
0.78345233
0.78327990
0.78334695
0.78318244
0.78303874
0.78266913
0.78243065
0.78227282
0.78230864
0.78227460
0.78232914
0.78250259
0.78233147
0.78214985
INFO - Training [14][   20/  196]   Loss 0.629606   Top1 77.929688   Top5 97.421875   BatchTime 0.393611   LR 0.000225
0.78175890
0.78166473
0.78155869
0.78148806
0.78139704
0.78153640
0.78175610
0.78198677
0.78204679
0.78198725
0.78185987
0.78170329
0.78171825
0.78174919
0.78164756
0.78155512
0.78160042
0.78182310
0.78174025
0.78147328
0.78197610
0.78194809
0.78208810
0.78194857
0.78187346
0.78160185
INFO - Training [14][   40/  196]   Loss 0.654319   Top1 77.187500   Top5 97.402344   BatchTime 0.353578   LR 0.000224
0.78169590
0.78197008
0.78222173
0.78210956
0.78212273
0.78210264
0.78192407
0.78194964
0.78194410
0.78209293
0.78228426
0.78223842
0.78218526
0.78225696
0.78202885
INFO - Training [14][   60/  196]   Loss 0.650266   Top1 77.480469   Top5 97.519531   BatchTime 0.324132   LR 0.000223
0.78200626
0.78187060
0.78185600
0.78200555
0.78197527
0.78204793
0.78240299
0.78241962
0.78223586
0.78220123
0.78208888
0.78189331
0.78193986
0.78179449
0.78157413
0.78144479
0.78137732
0.78163785
0.78136605
0.78143185
0.78140122
0.78127557
0.78148037
INFO - Training [14][   80/  196]   Loss 0.643082   Top1 77.822266   Top5 97.739258   BatchTime 0.306438   LR 0.000221
0.78157288
0.78147936
0.78153676
0.78142661
0.78129452
0.78125012
0.78136867
0.78135782
0.78102982
0.78095937
0.78120857
0.78144366
0.78124452
0.78117484
0.78108835
0.78104645
INFO - Training [14][  100/  196]   Loss 0.630515   Top1 78.230469   Top5 97.808594   BatchTime 0.297199   LR 0.000220
0.78128737
0.78124607
0.78124547
0.78124940
0.78110272
0.78093535
0.78103632
0.78073245
0.78091156
0.78069401
0.78063542
0.78062886
0.78089970
0.78064591
0.78051651
0.78034729
0.78012800
0.77994257
0.77996635
0.77975029
0.77968180
0.77973139
0.77948052
INFO - Training [14][  120/  196]   Loss 0.628807   Top1 78.330078   Top5 97.845052   BatchTime 0.291409   LR 0.000219
0.77935189
0.77952993
0.77957052
0.77958065
0.77964073
0.77946675
0.77929449
0.77950311
0.77942145
0.77946216
0.77961093
0.77953333
0.77949280
0.77940202
0.77933174
0.77922589
0.77932650
0.77966130
0.77946037
0.77909547
0.77890128
0.77883869
INFO - Training [14][  140/  196]   Loss 0.628916   Top1 78.300781   Top5 97.876674   BatchTime 0.289372   LR 0.000217
0.77848279
0.77831894
0.77840286
0.77837473
0.77841008
0.77845263
0.77856559
0.77891439
0.77895612
0.77867693
0.77871346
0.77900183
0.77917463
0.77939820
INFO - Training [14][  160/  196]   Loss 0.627434   Top1 78.366699   Top5 97.866211   BatchTime 0.287052   LR 0.000216
0.77915722
0.77949148
0.77935934
0.77917808
0.77905238
0.77905440
0.77909315
0.77917624
0.77921271
0.77931881
0.77839410
0.77843702
0.77851522
0.77814096
0.77801025
0.77767217
0.77789831
0.77813077
0.77811384
0.77795941
0.77810222
0.77830511
0.77805805
0.77794212
INFO - Training [14][  180/  196]   Loss 0.624511   Top1 78.441840   Top5 97.827691   BatchTime 0.282927   LR 0.000215
0.77798343
0.77787882
0.77769953
0.77761948
0.77734154
0.77731323
0.77736986
0.77767742
0.77779824
0.77753693
0.77716273
0.77712429
0.77713561
0.77738124
0.77714741
INFO - ==> Top1: 78.590    Top5: 97.856    Loss: 0.621
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77733374
0.77728635
0.77742249
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 0.452703   Top1 84.804688   Top5 99.238281   BatchTime 0.129137
INFO - Validation [14][   40/   40]   Loss 0.445545   Top1 84.860000   Top5 99.350000   BatchTime 0.092545
INFO - ==> Top1: 84.860    Top5: 99.350    Loss: 0.446
INFO - ==> Sparsity : 0.426
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.390   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0449)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.0984)
features.2.conv.3 tensor(0.3534)
features.2.conv.6 tensor(0.1444)
features.3.conv.0 tensor(0.0718)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1128)
features.4.conv.0 tensor(0.0461)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.1606)
features.5.conv.0 tensor(0.2507)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.1034)
features.6.conv.0 tensor(0.0485)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0856)
features.7.conv.0 tensor(0.1585)
features.7.conv.3 tensor(0.4473)
features.7.conv.6 tensor(0.2043)
features.8.conv.0 tensor(0.4526)
features.8.conv.3 tensor(0.5353)
features.8.conv.6 tensor(0.1491)
features.9.conv.0 tensor(0.3042)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.1528)
features.10.conv.0 tensor(0.0735)
features.10.conv.3 tensor(0.1108)
features.10.conv.6 tensor(0.0708)
features.11.conv.0 tensor(0.7207)
features.11.conv.3 tensor(0.6433)
features.11.conv.6 tensor(0.1964)
features.12.conv.0 tensor(0.5914)
features.12.conv.3 tensor(0.6713)
features.12.conv.6 tensor(0.1979)
features.13.conv.0 tensor(0.3163)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.0901)
features.14.conv.0 tensor(0.8736)
features.14.conv.3 tensor(0.8344)
features.14.conv.6 tensor(0.9365)
features.15.conv.0 tensor(0.7740)
features.15.conv.3 tensor(0.8221)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.7040)
features.16.conv.3 tensor(0.7986)
features.16.conv.6 tensor(0.1167)
conv.0 tensor(0.0933)
tensor(932329.) 2188896.0
0.77787566
0.77761769
0.77764904
0.77790582
0.77812845
0.77808416
0.77789700
0.77808672
0.77797723
0.77787989
0.77780670
0.77768630
0.77771705
0.77755892
0.77735966
0.77750319
0.77770543
0.77783102
0.77779931
0.77774572
0.77738869
INFO - Training [15][   20/  196]   Loss 0.641064   Top1 77.753906   Top5 97.480469   BatchTime 0.369872   LR 0.000212
0.77714103
0.77727336
0.77881557
0.77869314
0.77864784
0.77861011
0.77889276
0.77879030
0.77866501
0.77868575
0.77852422
0.77828318
0.77838957
0.77838993
INFO - Training [15][   40/  196]   Loss 0.635733   Top1 77.832031   Top5 97.636719   BatchTime 0.331348   LR 0.000211
0.77819556
0.77838790
0.77857196
0.77866602
0.77848589
0.77861136
0.77829999
0.77805454
0.77839446
0.77865529
0.77861023
0.77840990
0.77872366
0.77865589
0.77861232
0.77846873
0.77842736
0.77867496
0.77891046
0.77983487
0.78015125
INFO - Training [15][   60/  196]   Loss 0.628472   Top1 78.229167   Top5 97.766927   BatchTime 0.317958   LR 0.000209
0.78015727
0.78018320
0.78033763
0.77999264
0.77980840
0.77975285
0.77987975
0.78001136
0.78020597
0.78008670
0.77993464
0.77977675
0.77952629
0.77963752
0.77979231
0.78020483
0.78006750
0.78029341
0.78030103
0.78045666
0.78024191
INFO - Training [15][   80/  196]   Loss 0.620832   Top1 78.359375   Top5 97.900391   BatchTime 0.306145   LR 0.000208
0.78027737
0.78023171
0.78023875
0.78026330
0.78042245
0.78028703
0.78010678
0.78019136
0.78022218
0.78017479
0.77993530
0.77990836
0.77981478
0.77997804
0.77971995
0.77985191
0.78006566
0.77994573
0.78005147
0.78029728
0.78017652
0.77997315
INFO - Training [15][  100/  196]   Loss 0.617286   Top1 78.441406   Top5 97.933594   BatchTime 0.301354   LR 0.000206
0.77973545
0.77955765
0.77940476
0.77916819
0.77919954
0.77964067
0.78007847
0.78003937
0.78036934
0.78043264
0.78068984
0.78036362
0.77985835
0.78000641
0.78004241
0.78011233
0.78014284
0.78005910
0.78012949
INFO - Training [15][  120/  196]   Loss 0.609919   Top1 78.619792   Top5 98.043620   BatchTime 0.304790   LR 0.000205
0.78010184
0.78021610
0.77985519
0.77966750
0.77976060
0.77967465
0.77974910
0.77973217
0.77980894
0.77980465
0.77981287
0.77955794
0.77924383
0.77893418
0.77868325
0.77853256
0.77848482
0.77833986
0.77853709
INFO - Training [15][  140/  196]   Loss 0.609372   Top1 78.671875   Top5 98.097098   BatchTime 0.305003   LR 0.000203
0.77859360
0.77875698
0.77849442
0.77818471
0.77803314
0.77804232
0.77796072
0.77784014
0.77787852
0.77813810
0.77840519
0.77842510
0.77840650
0.77826792
0.77799541
0.77791023
0.77818173
0.77826297
0.77837604
0.77860999
INFO - Training [15][  160/  196]   Loss 0.611346   Top1 78.737793   Top5 98.034668   BatchTime 0.304870   LR 0.000201
0.77894747
0.77856153
0.77869493
0.77886701
0.77890998
0.77898765
0.77895731
0.77883583
0.77889645
0.77904004
0.77899027
0.77907419
0.77903682
0.77924323
0.77902251
0.77904254
0.77891642
0.77899522
INFO - Training [15][  180/  196]   Loss 0.611160   Top1 78.758681   Top5 98.003472   BatchTime 0.307056   LR 0.000200
0.77911621
0.77907956
0.77882469
0.77891296
0.77947903
0.77955091
0.77932841
0.77959555
0.77948314
0.77958006
0.77938074
0.77934927
0.77933335
0.77943063
0.77953792
0.77987951
0.77984059
0.77948213
0.77943611
INFO - ==> Top1: 78.822    Top5: 97.974    Loss: 0.609
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77936858
0.77901626
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.415043   Top1 85.820312   Top5 99.375000   BatchTime 0.126192
INFO - Validation [15][   40/   40]   Loss 0.412805   Top1 85.870000   Top5 99.500000   BatchTime 0.092454
INFO - ==> Top1: 85.870    Top5: 99.500    Loss: 0.413
INFO - ==> Sparsity : 0.424
INFO - Scoreboard best 1 ==> Epoch [15][Top1: 85.870   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.0900)
features.2.conv.3 tensor(0.3534)
features.2.conv.6 tensor(0.1395)
features.3.conv.0 tensor(0.0761)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1148)
features.4.conv.0 tensor(0.0518)
features.4.conv.3 tensor(0.3154)
features.4.conv.6 tensor(0.1483)
features.5.conv.0 tensor(0.2508)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.1084)
features.6.conv.0 tensor(0.0511)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0879)
features.7.conv.0 tensor(0.1627)
features.7.conv.3 tensor(0.4473)
features.7.conv.6 tensor(0.2031)
features.8.conv.0 tensor(0.4737)
features.8.conv.3 tensor(0.5350)
features.8.conv.6 tensor(0.1512)
features.9.conv.0 tensor(0.3160)
features.9.conv.3 tensor(0.5564)
features.9.conv.6 tensor(0.1473)
features.10.conv.0 tensor(0.0642)
features.10.conv.3 tensor(0.1059)
features.10.conv.6 tensor(0.0725)
features.11.conv.0 tensor(0.7304)
features.11.conv.3 tensor(0.6433)
features.11.conv.6 tensor(0.2071)
features.12.conv.0 tensor(0.6062)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.1974)
features.13.conv.0 tensor(0.2643)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.0898)
features.14.conv.0 tensor(0.8639)
features.14.conv.3 tensor(0.8353)
features.14.conv.6 tensor(0.9336)
features.15.conv.0 tensor(0.7815)
features.15.conv.3 tensor(0.8211)
features.15.conv.6 tensor(0.9667)
features.16.conv.0 tensor(0.7039)
features.16.conv.3 tensor(0.7983)
features.16.conv.6 tensor(0.1094)
conv.0 tensor(0.0936)
tensor(928925.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
0.77888852
0.77879602
0.77904981
0.77909809
0.77878338
0.77877659
0.77876830
0.77899832
0.77891648
0.77908504
0.77930611
0.77911377
0.77923191
0.77898061
0.77877235
0.77889252
INFO - Training [16][   20/  196]   Loss 0.578407   Top1 79.707031   Top5 97.695312   BatchTime 0.327416   LR 0.000197
0.77903414
0.77909952
0.77909368
0.77915603
0.77886605
0.77885997
0.77869236
0.77852523
0.77829611
0.77845562
0.77865970
0.77880347
0.77880836
0.77893168
0.77886546
0.77872485
0.77837878
0.77819282
0.77836573
0.77863145
0.77863324
0.77885449
0.77872324
0.77867466
INFO - Training [16][   40/  196]   Loss 0.592060   Top1 79.199219   Top5 97.812500   BatchTime 0.289461   LR 0.000195
0.77834672
0.77820218
0.77813065
0.77837527
0.77803367
0.77809817
0.77804357
0.77805775
0.77830070
0.77857548
0.77820176
0.77801573
0.77810031
0.77752918
0.77733862
0.77712548
0.77711159
0.77717519
0.77726567
0.77741182
INFO - Training [16][   60/  196]   Loss 0.597312   Top1 79.199219   Top5 97.897135   BatchTime 0.293364   LR 0.000194
0.77751416
0.77780837
0.77815968
0.77829260
0.77830315
0.77820599
0.77826464
0.77839082
0.77842277
0.77838987
0.77852476
0.77840412
0.77859092
0.77847034
0.77834725
INFO - Training [16][   80/  196]   Loss 0.591049   Top1 79.414062   Top5 97.993164   BatchTime 0.287999   LR 0.000192
0.77821594
0.77830535
0.77851349
0.77852547
0.77840298
0.77879155
0.77863693
0.77873528
0.77849543
0.77833289
0.77834564
0.77835363
0.77843779
0.77842760
0.77833384
0.77850777
0.77850479
0.77830273
0.77798587
0.77828914
0.77820712
0.77822143
0.77825576
INFO - Training [16][  100/  196]   Loss 0.585537   Top1 79.582031   Top5 97.949219   BatchTime 0.282646   LR 0.000190
0.77837765
0.77835238
0.77842867
0.77860630
0.77813298
0.77806413
0.77826077
0.77839839
0.77825427
0.77816910
0.77824515
0.77822715
0.77793616
0.77785516
0.77806550
0.77841151
0.77833730
0.77852386
0.77841723
0.77830672
0.77838480
INFO - Training [16][  120/  196]   Loss 0.587094   Top1 79.658203   Top5 98.004557   BatchTime 0.282625   LR 0.000188
0.77852094
0.77839535
0.77854031
0.77855247
0.77850950
0.77831537
0.77830899
0.77850276
0.77858990
0.77854878
0.77856481
0.77877510
0.77875131
0.77858794
0.77831393
0.77826256
INFO - Training [16][  140/  196]   Loss 0.584618   Top1 79.773996   Top5 98.071987   BatchTime 0.280174   LR 0.000187
0.77821285
0.77813870
0.77832335
0.77829629
0.77821517
0.77795893
0.77785832
0.77782947
0.77764964
0.77752775
0.77778572
0.77772546
0.77777660
0.77753752
0.77763140
0.77753574
0.77762419
0.77753639
0.77757710
0.77735102
0.77711111
0.77725399
INFO - Training [16][  160/  196]   Loss 0.588788   Top1 79.587402   Top5 98.078613   BatchTime 0.279742   LR 0.000185
0.77726936
0.77725297
0.77740127
0.77765423
0.77772492
0.77773899
0.77796161
0.77752936
0.77727216
0.77709484
0.77704799
0.77701718
0.77715975
0.77726746
0.77713388
0.77717972
0.77695274
0.77723747
0.77702749
0.77700388
0.77736574
0.77712017
0.77704036
0.77707994
INFO - Training [16][  180/  196]   Loss 0.588078   Top1 79.611545   Top5 98.016493   BatchTime 0.286222   LR 0.000183
0.77709121
0.77765310
0.77758467
0.77754277
0.77754778
0.77751410
0.77746248
0.77738243
0.77745181
0.77756250
0.77744675
0.77758586
INFO - ==> Top1: 79.714    Top5: 98.002    Loss: 0.587
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77784061
0.77770549
0.77796870
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.437882   Top1 85.019531   Top5 99.316406   BatchTime 0.124249
INFO - Validation [16][   40/   40]   Loss 0.428321   Top1 85.250000   Top5 99.420000   BatchTime 0.089726
INFO - ==> Top1: 85.250    Top5: 99.420    Loss: 0.428
INFO - ==> Sparsity : 0.425
INFO - Scoreboard best 1 ==> Epoch [15][Top1: 85.870   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 85.590   Top5: 99.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0527)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.1053)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.1395)
features.3.conv.0 tensor(0.0700)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1122)
features.4.conv.0 tensor(0.0469)
features.4.conv.3 tensor(0.3194)
features.4.conv.6 tensor(0.1499)
features.5.conv.0 tensor(0.2516)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1151)
features.6.conv.0 tensor(0.0435)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0866)
features.7.conv.0 tensor(0.1657)
features.7.conv.3 tensor(0.4444)
features.7.conv.6 tensor(0.2029)
features.8.conv.0 tensor(0.4525)
features.8.conv.3 tensor(0.5347)
features.8.conv.6 tensor(0.1547)
features.9.conv.0 tensor(0.2865)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.1576)
features.10.conv.0 tensor(0.0654)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0735)
features.11.conv.0 tensor(0.7316)
features.11.conv.3 tensor(0.6433)
features.11.conv.6 tensor(0.2114)
features.12.conv.0 tensor(0.6041)
features.12.conv.3 tensor(0.6725)
features.12.conv.6 tensor(0.1993)
features.13.conv.0 tensor(0.2641)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.0910)
features.14.conv.0 tensor(0.8748)
features.14.conv.3 tensor(0.8351)
features.14.conv.6 tensor(0.9318)
features.15.conv.0 tensor(0.7847)
features.15.conv.3 tensor(0.8208)
features.15.conv.6 tensor(0.9673)
features.16.conv.0 tensor(0.7032)
features.16.conv.3 tensor(0.7985)
features.16.conv.6 tensor(0.1126)
conv.0 tensor(0.0933)
tensor(931235.) 2188896.0
0.77795219
0.77785069
0.77784216
0.77775490
0.77799898
0.77807647
0.77807188
0.77807730
0.77818966
0.77801740
0.77794087
0.77774054
0.77756929
0.77758187
0.77756697
0.77750915
0.77765512
0.77747381
0.77766871
0.77772290
INFO - Training [17][   20/  196]   Loss 0.610348   Top1 78.769531   Top5 97.597656   BatchTime 0.363821   LR 0.000180
0.77788466
0.77805144
0.77791607
0.77764982
0.77780735
0.77775007
0.77741855
0.77725458
0.77704686
0.77693862
0.77694547
0.77699172
0.77689618
0.77719229
0.77712321
0.77693212
0.77682495
0.77718943
0.77727216
0.77746367
0.77736068
INFO - Training [17][   40/  196]   Loss 0.597465   Top1 79.472656   Top5 97.802734   BatchTime 0.322144   LR 0.000178
0.77734399
0.77709740
0.77732474
0.77711529
0.77649635
0.77595693
0.77599871
0.77591729
0.77569848
0.77574462
0.77577120
0.77594239
0.77630568
0.77630931
0.77621931
0.77635723
INFO - Training [17][   60/  196]   Loss 0.587703   Top1 79.557292   Top5 97.910156   BatchTime 0.302288   LR 0.000176
0.77617210
0.77608734
0.77580929
0.77565932
0.77553624
0.77531976
0.77523154
0.77526140
0.77526534
0.77521718
0.77531850
0.77546328
0.77564460
0.77611017
0.77761638
0.77753341
0.77713829
0.77689981
0.77670777
0.77685225
0.77706099
0.77704698
0.77726138
INFO - Training [17][   80/  196]   Loss 0.587175   Top1 79.609375   Top5 98.056641   BatchTime 0.289129   LR 0.000175
0.77731472
0.77750748
0.77746576
0.77709311
0.77667350
0.77676624
0.77675593
0.77662367
0.77682739
0.77692074
0.77696735
0.77700198
0.77706218
0.77684432
0.77726793
0.77702308
0.77713895
INFO - Training [17][  100/  196]   Loss 0.578841   Top1 79.968750   Top5 98.113281   BatchTime 0.281486   LR 0.000173
0.77743036
0.77739036
0.77723312
0.77700764
0.77686745
0.77670461
0.77677774
0.77699363
0.77714521
0.77699113
0.77702910
0.77699256
0.77710438
0.77727610
0.77794260
0.77789223
0.77790046
0.77788079
0.77790684
0.77786267
0.77758843
0.77719086
0.77723002
INFO - Training [17][  120/  196]   Loss 0.574826   Top1 80.139974   Top5 98.154297   BatchTime 0.275982   LR 0.000171
0.77730405
0.77728987
0.77746230
0.77735656
0.77727991
0.77719790
0.77721328
0.77707070
0.77715468
0.77722603
0.77738619
0.77729982
0.77716434
0.77702671
0.77708071
0.77697039
0.77686155
INFO - Training [17][  140/  196]   Loss 0.571199   Top1 80.301339   Top5 98.217076   BatchTime 0.272200   LR 0.000169
0.77692008
0.77714777
0.77691770
0.77700907
0.77723557
0.77729392
0.77764744
0.77758354
0.77769428
0.77757871
0.77724493
0.77686721
0.77676517
0.77682835
0.77672803
0.77665001
0.77666765
0.77682179
0.77716500
0.77713704
0.77761638
0.77718645
0.77725160
0.77693111
INFO - Training [17][  160/  196]   Loss 0.577333   Top1 80.114746   Top5 98.190918   BatchTime 0.269308   LR 0.000167
0.77674693
0.77675569
0.77681088
0.77703249
0.77698737
0.77663672
0.77667773
0.77673239
0.77663332
0.77663332
0.77657890
0.77645123
0.77678955
0.77672082
0.77695620
0.77697039
INFO - Training [17][  180/  196]   Loss 0.574750   Top1 80.247396   Top5 98.157552   BatchTime 0.267326   LR 0.000165
0.77683216
0.77660906
0.77652836
0.77642238
0.77636439
0.77643752
0.77641827
0.77639562
0.77628094
0.77634633
0.77639616
0.77654755
0.77653873
0.77656281
INFO - ==> Top1: 80.282    Top5: 98.158    Loss: 0.574
0.77654195
0.77628165
0.77614075
0.77593338
0.77574694
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [17][   20/   40]   Loss 0.409302   Top1 86.035156   Top5 99.394531   BatchTime 0.188615
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1250)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.0972)
features.2.conv.3 tensor(0.3534)
features.2.conv.6 tensor(0.1406)
features.3.conv.0 tensor(0.0645)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0456)
features.4.conv.3 tensor(0.3194)
features.4.conv.6 tensor(0.1540)
features.5.conv.0 tensor(0.2503)
features.5.conv.3 tensor(0.4219)
features.5.conv.6 tensor(0.1089)
features.6.conv.0 tensor(0.0469)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0877)
features.7.conv.0 tensor(0.1654)
features.7.conv.3 tensor(0.4453)
features.7.conv.6 tensor(0.1995)
features.8.conv.0 tensor(0.4729)
features.8.conv.3 tensor(0.5353)
features.8.conv.6 tensor(0.1557)
features.9.conv.0 tensor(0.3070)
features.9.conv.3 tensor(0.5593)
features.9.conv.6 tensor(0.1547)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0729)
features.11.conv.0 tensor(0.7445)
features.11.conv.3 tensor(0.6458)
features.11.conv.6 tensor(0.2050)
features.12.conv.0 tensor(0.6077)
features.12.conv.3 tensor(0.6715)
features.12.conv.6 tensor(0.2069)
features.13.conv.0 tensor(0.2624)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.0903)
features.14.conv.0 tensor(0.8712)
features.14.conv.3 tensor(0.8347)
features.14.conv.6 tensor(0.9347)
features.15.conv.0 tensor(0.7889)
features.15.conv.3 tensor(0.8214)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.6990)
features.16.conv.3 tensor(0.7985)
features.16.conv.6 tensor(0.1117)
conv.0 tensor(0.0938)
tensor(932680.) 2188896.0
INFO - Validation [17][   40/   40]   Loss 0.392218   Top1 86.670000   Top5 99.530000   BatchTime 0.120248
INFO - ==> Top1: 86.670    Top5: 99.530    Loss: 0.392
INFO - ==> Sparsity : 0.426
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 86.670   Top5: 99.530]
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 85.870   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 85.800   Top5: 99.490]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.77563614
0.77556878
0.77559215
0.77562153
0.77579492
0.77614248
0.77651262
0.77655321
0.77647263
0.77649647
0.77652735
0.77653980
0.77639908
0.77611655
0.77618599
0.77625233
0.77635992
0.77637863
0.77660531
0.77654445
INFO - Training [18][   20/  196]   Loss 0.561695   Top1 80.566406   Top5 97.578125   BatchTime 0.336041   LR 0.000162
0.77633512
0.77645272
0.77635157
0.77636802
0.77674866
0.77693897
0.77666819
0.77676457
0.77680182
0.77703720
0.77710664
0.77724695
0.77742523
0.77748716
0.77716088
INFO - Training [18][   40/  196]   Loss 0.568311   Top1 80.361328   Top5 97.656250   BatchTime 0.298303   LR 0.000160
0.77704352
0.77714151
0.77716494
0.77680111
0.77668405
0.77671772
0.77678084
0.77683192
0.77676016
0.77700722
0.77716047
0.77691609
0.77669483
0.77691537
0.77729291
0.77727252
0.77714586
0.77728522
0.77741611
0.77705812
0.77687901
0.77668232
0.77671033
INFO - Training [18][   60/  196]   Loss 0.558493   Top1 80.625000   Top5 97.910156   BatchTime 0.284022   LR 0.000158
0.77664405
0.77675813
0.77672321
0.77635258
0.77624530
0.77634954
0.77652287
0.77647370
0.77659655
0.77690536
0.77687871
0.77637124
0.77628464
0.77618521
0.77596170
0.77593726
0.77586323
0.77585268
0.77587086
0.77608502
0.77643216
0.77652389
0.77652931
INFO - Training [18][   80/  196]   Loss 0.556188   Top1 80.747070   Top5 98.002930   BatchTime 0.277872   LR 0.000156
0.77654904
0.77669400
0.77655572
0.77646214
0.77630568
0.77609527
0.77614975
0.77602142
0.77574795
0.77574599
0.77563983
0.77564746
0.77540052
0.77531159
0.77548426
0.77537793
0.77526945
INFO - Training [18][  100/  196]   Loss 0.555676   Top1 80.738281   Top5 98.066406   BatchTime 0.272078   LR 0.000154
0.77529359
0.77510846
0.77517116
0.77529526
0.77540678
0.77535832
0.77529198
0.77525622
0.77526152
0.77519238
0.77518046
0.77485567
0.77483547
0.77479678
0.77487141
0.77492106
0.77534890
0.77507514
0.77486759
INFO - Training [18][  120/  196]   Loss 0.551869   Top1 80.924479   Top5 98.154297   BatchTime 0.279342   LR 0.000152
0.77482873
0.77511930
0.77539468
0.77547902
0.77559930
0.77549726
0.77551407
0.77543896
0.77526510
0.77540106
0.77539515
0.77558613
0.77579916
0.77558166
0.77563435
0.77610093
0.77597642
0.77585357
0.77579772
0.77564073
0.77551764
0.77581638
0.77584225
INFO - Training [18][  140/  196]   Loss 0.551587   Top1 80.895647   Top5 98.242188   BatchTime 0.276775   LR 0.000150
0.77592564
0.77579600
0.77590704
0.77575165
0.77555084
0.77548891
0.77532947
0.77531189
0.77522999
0.77508253
0.77497506
0.77498585
0.77509952
0.77516681
0.77546328
INFO - Training [18][  160/  196]   Loss 0.551857   Top1 80.900879   Top5 98.271484   BatchTime 0.275158   LR 0.000148
0.77548176
0.77553940
0.77529055
0.77509940
0.77503902
0.77487862
0.77482361
0.77494299
0.77510679
0.77530783
0.77560383
0.77537483
0.77517217
0.77515185
0.77530152
0.77527517
0.77564251
0.77595162
0.77591759
0.77579159
0.77547532
0.77543032
INFO - Training [18][  180/  196]   Loss 0.550893   Top1 80.907118   Top5 98.255208   BatchTime 0.274848   LR 0.000146
0.77535874
0.77518868
0.77521664
0.77517539
0.77531177
0.77534360
0.77541566
0.77536392
0.77531898
0.77513176
0.77511960
0.77518338
0.77519071
0.77522671
0.77556205
0.77590764
INFO - ==> Top1: 80.868    Top5: 98.244    Loss: 0.552
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77593195
0.77576536
0.77571744
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [18][   20/   40]   Loss 0.400459   Top1 86.035156   Top5 99.570312   BatchTime 0.128180
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.1082)
features.2.conv.3 tensor(0.3511)
features.2.conv.6 tensor(0.1432)
features.3.conv.0 tensor(0.0680)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1094)
features.4.conv.0 tensor(0.0467)
features.4.conv.3 tensor(0.3154)
features.4.conv.6 tensor(0.1580)
features.5.conv.0 tensor(0.2490)
features.5.conv.3 tensor(0.4219)
features.5.conv.6 tensor(0.1126)
features.6.conv.0 tensor(0.0472)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0869)
features.7.conv.0 tensor(0.1633)
features.7.conv.3 tensor(0.4473)
features.7.conv.6 tensor(0.2017)
features.8.conv.0 tensor(0.4695)
features.8.conv.3 tensor(0.5353)
features.8.conv.6 tensor(0.1564)
features.9.conv.0 tensor(0.3094)
features.9.conv.3 tensor(0.5611)
features.9.conv.6 tensor(0.1508)
features.10.conv.0 tensor(0.0649)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0734)
features.11.conv.0 tensor(0.7377)
features.11.conv.3 tensor(0.6462)
features.11.conv.6 tensor(0.1983)
features.12.conv.0 tensor(0.6128)
features.12.conv.3 tensor(0.6719)
features.12.conv.6 tensor(0.2002)
features.13.conv.0 tensor(0.2634)
features.13.conv.3 tensor(0.4882)
features.13.conv.6 tensor(0.0905)
features.14.conv.0 tensor(0.8756)
features.14.conv.3 tensor(0.8354)
features.14.conv.6 tensor(0.9436)
features.15.conv.0 tensor(0.7987)
features.15.conv.3 tensor(0.8211)
features.15.conv.6 tensor(0.9681)
features.16.conv.0 tensor(0.7052)
features.16.conv.3 tensor(0.7986)
features.16.conv.6 tensor(0.1139)
conv.0 tensor(0.0931)
tensor(936943.) 2188896.0
INFO - Validation [18][   40/   40]   Loss 0.392604   Top1 86.350000   Top5 99.600000   BatchTime 0.090828
INFO - ==> Top1: 86.350    Top5: 99.600    Loss: 0.393
INFO - ==> Sparsity : 0.428
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 86.670   Top5: 99.530]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 86.350   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 85.870   Top5: 99.500]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
0.77544397
0.77540350
0.77536649
0.77542812
0.77555132
0.77529007
0.77527314
0.77523142
0.77527392
0.77550352
0.77548724
0.77553582
0.77575415
0.77592635
0.77605093
0.77554297
0.77555996
INFO - Training [19][   20/  196]   Loss 0.570510   Top1 80.234375   Top5 97.890625   BatchTime 0.343767   LR 0.000143
0.77559179
0.77558708
0.77566272
0.77575302
0.77590799
0.77585745
0.77571273
0.77542776
0.77525300
0.77521092
0.77512574
0.77523190
0.77533990
0.77538830
0.77559060
0.77562970
0.77565759
0.77573037
0.77596092
0.77575982
0.77582663
INFO - Training [19][   40/  196]   Loss 0.563281   Top1 80.537109   Top5 97.919922   BatchTime 0.314014   LR 0.000141
0.77549243
0.77552873
0.77575457
0.77614552
0.77614939
0.77611899
0.77616382
0.77613729
0.77607930
0.77568018
0.77559584
0.77558643
0.77563840
0.77559358
0.77557600
0.77570403
0.77600491
0.77579963
0.77531928
0.77501464
INFO - Training [19][   60/  196]   Loss 0.558511   Top1 80.781250   Top5 98.059896   BatchTime 0.310024   LR 0.000139
0.77483195
0.77492362
0.77506346
0.77528369
0.77547228
0.77545381
0.77531308
0.77528626
0.77502960
0.77470165
0.77453554
0.77452427
0.77469265
0.77470130
0.77475059
0.77487612
0.77509588
0.77525091
0.77517354
0.77496630
0.77441722
INFO - Training [19][   80/  196]   Loss 0.557142   Top1 80.717773   Top5 98.125000   BatchTime 0.305902   LR 0.000137
0.77432972
0.77429962
0.77416521
0.77427781
0.77427053
0.77447140
0.77432400
0.77359915
0.77356303
0.77380109
0.77375078
0.77356786
0.77319729
0.77305740
0.77306992
0.77304363
0.77340966
0.77332097
INFO - Training [19][  100/  196]   Loss 0.549057   Top1 80.996094   Top5 98.203125   BatchTime 0.309222   LR 0.000135
0.77319181
0.77321583
0.77333033
0.77314663
0.77296406
0.77303952
0.77307457
0.77288628
0.77264822
0.77247876
0.77248436
0.77260405
0.77238649
0.77253282
0.77272022
0.77292407
0.77375609
0.77421349
0.77457809
INFO - Training [19][  120/  196]   Loss 0.541373   Top1 81.220703   Top5 98.274740   BatchTime 0.310558   LR 0.000133
0.77440143
0.77443302
0.77408814
0.77414185
0.77402931
0.77395016
0.77403742
0.77395260
0.77404821
0.77382523
0.77375674
0.77394170
0.77397889
0.77389765
0.77378476
0.77374965
0.77382976
0.77372032
0.77350461
INFO - Training [19][  140/  196]   Loss 0.537722   Top1 81.319754   Top5 98.351004   BatchTime 0.309234   LR 0.000131
0.77501768
0.77483940
0.77480698
0.77514458
0.77486485
0.77436876
0.77456266
0.77455705
0.77434736
0.77425915
0.77414358
0.77409536
0.77380288
0.77386421
0.77376628
0.77384812
0.77366292
0.77348375
0.77331710
0.77329701
0.77329475
0.77325493
0.77338749
0.77352953
INFO - Training [19][  160/  196]   Loss 0.539806   Top1 81.271973   Top5 98.356934   BatchTime 0.302354   LR 0.000129
0.77416801
0.77478296
0.77450633
0.77439725
0.77421153
0.77392662
0.77361393
0.77358222
0.77346164
0.77355152
0.77350402
0.77348560
0.77346325
0.77333248
0.77358401
0.77365452
0.77365559
0.77397525
0.77393466
0.77376825
0.77335006
0.77314073
0.77312547
INFO - Training [19][  180/  196]   Loss 0.538440   Top1 81.260851   Top5 98.320312   BatchTime 0.297893   LR 0.000127
0.77337015
0.77340072
0.77330065
0.77295834
0.77284586
0.77277714
0.77269286
0.77284771
0.77437699
0.77478689
0.77491885
0.77465332
0.77461690
0.77470553
INFO - ==> Top1: 81.268    Top5: 98.318    Loss: 0.538
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 0.404617   Top1 86.250000   Top5 99.492188   BatchTime 0.131731
INFO - Validation [19][   40/   40]   Loss 0.394381   Top1 86.460000   Top5 99.600000   BatchTime 0.093845
INFO - ==> Top1: 86.460    Top5: 99.600    Loss: 0.394
INFO - ==> Sparsity : 0.430
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 86.670   Top5: 99.530]
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 86.460   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 86.350   Top5: 99.600]
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.0839)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.1424)
features.3.conv.0 tensor(0.0605)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1133)
features.4.conv.0 tensor(0.0488)
features.4.conv.3 tensor(0.3131)
features.4.conv.6 tensor(0.1501)
features.5.conv.0 tensor(0.2493)
features.5.conv.3 tensor(0.4201)
features.5.conv.6 tensor(0.1076)
features.6.conv.0 tensor(0.0495)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0830)
features.7.conv.0 tensor(0.1644)
features.7.conv.3 tensor(0.4476)
features.7.conv.6 tensor(0.2025)
features.8.conv.0 tensor(0.4849)
features.8.conv.3 tensor(0.5347)
features.8.conv.6 tensor(0.1591)
features.9.conv.0 tensor(0.3205)
features.9.conv.3 tensor(0.5602)
features.9.conv.6 tensor(0.1514)
features.10.conv.0 tensor(0.0624)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.0756)
features.11.conv.0 tensor(0.7391)
features.11.conv.3 tensor(0.6443)
features.11.conv.6 tensor(0.2049)
features.12.conv.0 tensor(0.6295)
features.12.conv.3 tensor(0.6717)
features.12.conv.6 tensor(0.2067)
features.13.conv.0 tensor(0.2632)
features.13.conv.3 tensor(0.4871)
features.13.conv.6 tensor(0.0909)
features.14.conv.0 tensor(0.8827)
features.14.conv.3 tensor(0.8354)
features.14.conv.6 tensor(0.9377)
features.15.conv.0 tensor(0.8020)
features.15.conv.3 tensor(0.8214)
features.15.conv.6 tensor(0.9670)
features.16.conv.0 tensor(0.7110)
features.16.conv.3 tensor(0.7985)
features.16.conv.6 tensor(0.1174)
conv.0 tensor(0.0929)
tensor(941697.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.77480096
0.77504814
0.77481747
0.77457249
0.77460855
0.77448767
0.77438378
0.77406365
0.77398729
0.77400106
0.77410865
0.77429426
0.77424824
0.77437931
0.77475506
0.77472287
0.77481073
0.77479386
INFO - Training [20][   20/  196]   Loss 0.560653   Top1 80.332031   Top5 98.046875   BatchTime 0.353400   LR 0.000123
0.77487665
0.77524948
0.77551478
0.77555108
0.77562886
0.77552563
0.77557296
0.77518135
0.77482831
0.77471560
0.77475947
0.77480763
0.77454996
0.77449691
0.77450991
0.77423847
0.77414817
0.77405638
0.77407795
0.77422297
0.77438593
0.77447510
INFO - Training [20][   40/  196]   Loss 0.559227   Top1 80.302734   Top5 98.085938   BatchTime 0.314951   LR 0.000121
0.77451688
0.77449524
0.77387375
0.77352858
0.77349436
0.77325958
0.77312607
0.77310866
0.77330995
0.77334601
0.77370542
0.77375859
0.77342451
0.77335137
0.77326953
0.77305627
0.77304512
0.77318090
0.77315623
0.77288830
0.77292824
INFO - Training [20][   60/  196]   Loss 0.547824   Top1 80.937500   Top5 98.105469   BatchTime 0.302372   LR 0.000119
0.77284408
0.77280861
0.77280694
0.77251512
0.77243418
0.77238452
0.77244079
0.77232665
0.77238709
0.77265060
0.77284294
0.77333438
0.77342063
0.77344626
0.77341366
INFO - Training [20][   80/  196]   Loss 0.543473   Top1 81.245117   Top5 98.183594   BatchTime 0.293576   LR 0.000117
0.77313322
0.77302587
0.77269036
0.77241611
0.77240926
0.77251965
0.77258104
0.77263963
0.77293217
0.77326787
0.77351159
0.77419603
0.77395290
0.77365857
0.77334106
0.77326900
0.77318293
0.77305287
0.77308118
0.77310044
0.77285612
INFO - Training [20][  100/  196]   Loss 0.530922   Top1 81.609375   Top5 98.269531   BatchTime 0.293608   LR 0.000115
0.77310205
0.77319062
0.77339202
0.77348930
0.77343524
0.77340454
0.77315605
0.77316576
0.77328581
0.77350020
0.77349585
0.77330464
0.77345937
0.77331531
0.77335560
0.77328312
0.77319449
0.77306116
0.77284354
0.77266461
0.77265066
0.77270865
INFO - Training [20][  120/  196]   Loss 0.524963   Top1 81.852214   Top5 98.372396   BatchTime 0.290787   LR 0.000113
0.77300376
0.77346492
0.77322656
0.77321649
0.77335602
0.77351862
0.77360356
0.77363449
0.77352470
0.77321023
0.77294081
0.77288032
0.77285564
0.77282780
0.77267706
0.77268398
0.77284795
0.77291507
0.77276969
0.77259904
0.77254653
0.77253699
INFO - Training [20][  140/  196]   Loss 0.523200   Top1 81.939174   Top5 98.451451   BatchTime 0.286947   LR 0.000111
0.77243191
0.77251202
0.77248007
0.77245790
0.77265024
0.77271909
0.77261961
0.77235097
0.77221233
0.77229810
0.77236503
0.77230412
0.77257425
0.77257460
0.77315563
0.77339679
0.77314121
0.77301687
0.77290922
INFO - Training [20][  160/  196]   Loss 0.523494   Top1 81.867676   Top5 98.471680   BatchTime 0.290204   LR 0.000109
0.77279007
0.77268767
0.77260059
0.77259785
0.77273506
0.77301896
0.77302408
0.77301586
0.77308273
0.77360970
0.77325886
0.77291465
0.77285802
0.77295166
0.77297932
0.77296948
INFO - Training [20][  180/  196]   Loss 0.524378   Top1 81.905382   Top5 98.415799   BatchTime 0.287162   LR 0.000107
0.77294356
0.77314287
0.77280372
0.77269846
0.77271134
0.77297491
0.77281231
0.77292371
0.77273989
0.77281696
0.77283335
0.77234358
0.77226245
0.77240890
0.77237916
0.77242899
0.77279431
0.77286726
0.77287060
0.77303827
********************pre-trained*****************
INFO - ==> Top1: 81.922    Top5: 98.414    Loss: 0.524
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 0.380458   Top1 87.011719   Top5 99.589844   BatchTime 0.131126
INFO - Validation [20][   40/   40]   Loss 0.374096   Top1 87.520000   Top5 99.650000   BatchTime 0.094309
INFO - ==> Top1: 87.520    Top5: 99.650    Loss: 0.374
INFO - ==> Sparsity : 0.431
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 87.520   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 86.670   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 86.460   Top5: 99.600]
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1406)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.0813)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.1374)
features.3.conv.0 tensor(0.0689)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1126)
features.4.conv.0 tensor(0.0470)
features.4.conv.3 tensor(0.3137)
features.4.conv.6 tensor(0.1445)
features.5.conv.0 tensor(0.2487)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.1077)
features.6.conv.0 tensor(0.0552)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0841)
features.7.conv.0 tensor(0.1642)
features.7.conv.3 tensor(0.4485)
features.7.conv.6 tensor(0.2041)
features.8.conv.0 tensor(0.5156)
features.8.conv.3 tensor(0.5333)
features.8.conv.6 tensor(0.1602)
features.9.conv.0 tensor(0.3175)
features.9.conv.3 tensor(0.5590)
features.9.conv.6 tensor(0.1497)
features.10.conv.0 tensor(0.0644)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0741)
features.11.conv.0 tensor(0.7536)
features.11.conv.3 tensor(0.6439)
features.11.conv.6 tensor(0.2027)
features.12.conv.0 tensor(0.6305)
features.12.conv.3 tensor(0.6719)
features.12.conv.6 tensor(0.1956)
features.13.conv.0 tensor(0.2627)
features.13.conv.3 tensor(0.4884)
features.13.conv.6 tensor(0.0908)
features.14.conv.0 tensor(0.8781)
features.14.conv.3 tensor(0.8365)
features.14.conv.6 tensor(0.9389)
features.15.conv.0 tensor(0.8071)
features.15.conv.3 tensor(0.8220)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.7126)
features.16.conv.3 tensor(0.7985)
features.16.conv.6 tensor(0.1169)
conv.0 tensor(0.0926)
tensor(942731.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
0.77297670
0.77307969
0.77293634
0.77336478
0.77347159
0.77354723
0.77348846
0.77352774
0.77347481
0.77335423
0.77345175
0.77319783
0.77307636
0.77321398
0.77328962
INFO - Training [21][   20/  196]   Loss 0.527978   Top1 81.542969   Top5 98.105469   BatchTime 0.357210   LR 0.000104
0.77314842
0.77324092
0.77336133
0.77328581
0.77334070
0.77350295
0.77353460
0.77374798
0.77384561
0.77349430
0.77347094
0.77323073
0.77310616
0.77304757
0.77319103
0.77317250
0.77306134
0.77318931
0.77353054
0.77343202
0.77313590
0.77309078
INFO - Training [21][   40/  196]   Loss 0.528968   Top1 81.601562   Top5 98.115234   BatchTime 0.317578   LR 0.000102
0.77298063
0.77314270
0.77333820
0.77329451
0.77327794
0.77296585
0.77305770
0.77313280
0.77305883
0.77297777
0.77313089
0.77318597
0.77283478
0.77281535
0.77277160
0.77273202
0.77270776
0.77275211
0.77281266
0.77265924
0.77266759
0.77270406
0.77284127
INFO - Training [21][   60/  196]   Loss 0.522279   Top1 81.959635   Top5 98.196615   BatchTime 0.300231   LR 0.000100
0.77276832
0.77306050
0.77353185
0.77338815
0.77318698
0.77314526
0.77309394
0.77311528
0.77308649
0.77313411
0.77325034
0.77332991
0.77311939
0.77294093
0.77270114
INFO - Training [21][   80/  196]   Loss 0.518081   Top1 82.109375   Top5 98.305664   BatchTime 0.291437   LR 0.000098
0.77267122
0.77250469
0.77231961
0.77215821
0.77230680
0.77250791
0.77249020
0.77269465
0.77279329
0.77286875
0.77265906
0.77262330
0.77247864
0.77243513
0.77238095
0.77254027
0.77251881
0.77239203
0.77229387
0.77236688
0.77216595
0.77208221
0.77214032
0.77219748
INFO - Training [21][  100/  196]   Loss 0.514206   Top1 82.230469   Top5 98.339844   BatchTime 0.301739   LR 0.000096
0.77234781
0.77243006
0.77244109
0.77223700
0.77245957
0.77262002
0.77292770
0.77312487
0.77273911
0.77262270
0.77239442
0.77163136
0.77076644
0.77074611
0.77097732
0.77107775
0.77123958
0.77134180
INFO - Training [21][  120/  196]   Loss 0.512856   Top1 82.304688   Top5 98.391927   BatchTime 0.306861   LR 0.000094
0.77134979
0.77131045
0.77110600
0.77089876
0.77092016
0.77096051
0.77109933
0.77122140
0.77148080
0.77173221
0.77124625
0.77104664
0.77112186
0.77103150
0.77084196
0.77088219
0.77081853
0.77071637
0.77073872
0.77099609
0.77119839
0.77147609
0.77135056
0.77125448
INFO - Training [21][  140/  196]   Loss 0.509463   Top1 82.424665   Top5 98.451451   BatchTime 0.310164   LR 0.000092
0.77143627
0.77161843
0.77161080
0.77164888
0.77154142
0.77149898
0.77157307
0.77162862
0.77155292
0.77159476
0.77145535
0.77134341
0.77133280
0.77125597
0.77116758
0.77112371
0.77117932
0.77122587
0.77115291
0.77116036
INFO - Training [21][  160/  196]   Loss 0.513827   Top1 82.292480   Top5 98.400879   BatchTime 0.310796   LR 0.000090
0.77253497
0.77286512
0.77273685
0.77254725
0.77286935
0.77312660
0.77329564
0.77326775
0.77322632
0.77314007
0.77306843
0.77319652
0.77308679
0.77315104
0.77298659
0.77303272
0.77301198
0.77292204
0.77312177
0.77325463
0.77325869
INFO - Training [21][  180/  196]   Loss 0.513231   Top1 82.285156   Top5 98.394097   BatchTime 0.308219   LR 0.000088
0.77305603
0.77293408
0.77281630
0.77257401
0.77222204
0.77161092
0.77065396
0.77080214
0.77098149
0.77092564
0.77089149
0.77110708
0.77108288
0.77098238
INFO - ==> Top1: 82.370    Top5: 98.380    Loss: 0.512
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 0.371991   Top1 87.109375   Top5 99.531250   BatchTime 0.131670
INFO - Validation [21][   40/   40]   Loss 0.365862   Top1 87.480000   Top5 99.610000   BatchTime 0.092407
INFO - ==> Top1: 87.480    Top5: 99.610    Loss: 0.366
INFO - ==> Sparsity : 0.432
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 87.520   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 87.480   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 86.670   Top5: 99.530]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1426)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0833)
features.2.conv.0 tensor(0.0810)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1372)
features.3.conv.0 tensor(0.0602)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1120)
features.4.conv.0 tensor(0.0452)
features.4.conv.3 tensor(0.3125)
features.4.conv.6 tensor(0.1468)
features.5.conv.0 tensor(0.2523)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.1089)
features.6.conv.0 tensor(0.0526)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.1609)
features.7.conv.3 tensor(0.4497)
features.7.conv.6 tensor(0.2064)
features.8.conv.0 tensor(0.5017)
features.8.conv.3 tensor(0.5356)
features.8.conv.6 tensor(0.1612)
features.9.conv.0 tensor(0.3191)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.1458)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0760)
features.11.conv.0 tensor(0.7429)
features.11.conv.3 tensor(0.6454)
features.11.conv.6 tensor(0.2350)
features.12.conv.0 tensor(0.6258)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.2050)
features.13.conv.0 tensor(0.2641)
features.13.conv.3 tensor(0.4875)
features.13.conv.6 tensor(0.0905)
features.14.conv.0 tensor(0.8804)
features.14.conv.3 tensor(0.8363)
features.14.conv.6 tensor(0.9418)
features.15.conv.0 tensor(0.8137)
features.15.conv.3 tensor(0.8220)
features.15.conv.6 tensor(0.9675)
features.16.conv.0 tensor(0.7086)
features.16.conv.3 tensor(0.7986)
features.16.conv.6 tensor(0.1181)
conv.0 tensor(0.0924)
tensor(945445.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.77126384
0.77108675
0.77100486
0.77088511
0.77091652
0.77069753
0.77056813
0.77055269
0.77036905
0.77029091
0.77015483
0.76987845
0.76975745
0.76962662
0.76949376
0.76918852
0.76914936
0.76889831
INFO - Training [22][   20/  196]   Loss 0.505563   Top1 82.519531   Top5 97.949219   BatchTime 0.357439   LR 0.000085
0.76853651
0.76821268
0.76791626
0.76732409
0.76651412
0.76619679
0.76586950
0.76550549
0.76521200
0.76514035
0.76480967
0.76464003
0.76437443
0.76374656
0.76344258
0.76324838
0.76312333
0.76267636
0.76243752
0.76232725
0.76197726
INFO - Training [22][   40/  196]   Loss 0.514408   Top1 82.304688   Top5 98.085938   BatchTime 0.319784   LR 0.000083
0.76172715
0.76162487
0.76179278
0.76157910
0.76129723
0.76130438
0.76126784
0.76124430
0.76115566
0.76115686
0.76122147
0.76125616
0.76109058
0.76118636
0.76135707
0.76136214
0.76112992
0.76123351
0.76110142
INFO - Training [22][   60/  196]   Loss 0.510216   Top1 82.226562   Top5 98.248698   BatchTime 0.318260   LR 0.000081
0.76123345
0.76124132
0.76126450
0.76122725
0.76118708
0.76114714
0.76108116
0.76123291
0.76124436
0.76147765
0.76133728
0.76108724
0.76113623
0.76110482
0.76120323
0.76114476
0.76126170
0.76129580
0.76117224
0.76124936
0.76098210
0.76077652
INFO - Training [22][   80/  196]   Loss 0.507330   Top1 82.368164   Top5 98.305664   BatchTime 0.307589   LR 0.000079
0.76081729
0.76061201
0.76049572
0.76064622
0.76052248
0.76073784
0.76094866
0.76104850
0.76108092
0.76079488
0.76062953
0.76065946
0.76076782
0.76077259
0.76096618
0.76074690
0.76073748
0.76053208
0.76065153
0.76080233
0.76073182
0.76255423
INFO - Training [22][  100/  196]   Loss 0.500073   Top1 82.710938   Top5 98.296875   BatchTime 0.300989   LR 0.000077
0.76331782
0.76360536
0.76356900
0.76334327
0.76338780
0.76332092
0.76315027
0.76312852
0.76335961
0.76340878
0.76324385
0.76309258
0.76324517
0.76305181
INFO - Training [22][  120/  196]   Loss 0.496220   Top1 82.900391   Top5 98.362630   BatchTime 0.298062   LR 0.000075
0.76296204
0.76284283
0.76310742
0.76320761
0.76324439
0.76294899
0.76303434
0.76286733
0.76259929
0.76247352
0.76245272
0.76221019
0.76246130
0.76242334
0.76258695
0.76270831
0.76287419
0.76290447
0.76269048
0.76286185
0.76283884
0.76271147
INFO - Training [22][  140/  196]   Loss 0.496969   Top1 82.901786   Top5 98.420759   BatchTime 0.294560   LR 0.000073
0.76245910
0.76236117
0.76240069
0.76246721
0.76243192
0.76219326
0.76186466
0.76154745
0.76147091
0.76165992
0.76168120
0.76185381
0.76180249
0.76162195
0.76158440
0.76152420
0.76143813
0.76152384
0.76145792
0.76146358
0.76108605
0.76139671
0.76181817
INFO - Training [22][  160/  196]   Loss 0.499909   Top1 82.802734   Top5 98.415527   BatchTime 0.289815   LR 0.000072
0.76132751
0.76143622
0.76147544
0.76158285
0.76160800
0.76175445
0.76161224
0.76143330
0.76134539
0.76110339
0.76092809
0.76093066
0.76091129
0.76076967
0.76081592
0.76088423
INFO - Training [22][  180/  196]   Loss 0.499336   Top1 82.801649   Top5 98.407118   BatchTime 0.286214   LR 0.000070
0.76111239
0.76125169
0.76126343
0.76136577
0.76161933
0.76148361
0.76151866
0.76170945
0.76160294
0.76142502
0.76114875
0.76090890
0.76100487
0.76087838
0.76093233
INFO - ==> Top1: 82.804    Top5: 98.410    Loss: 0.499
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.76102990
0.76102227
0.76115906
0.76107264
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.367092   Top1 87.460938   Top5 99.511719   BatchTime 0.129228
INFO - Validation [22][   40/   40]   Loss 0.357021   Top1 87.790000   Top5 99.670000   BatchTime 0.093111
INFO - ==> Top1: 87.790    Top5: 99.670    Loss: 0.357
INFO - ==> Sparsity : 0.438
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 87.790   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 87.520   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 87.480   Top5: 99.610]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.0807)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.1398)
features.3.conv.0 tensor(0.0651)
features.3.conv.3 tensor(0.0880)
features.3.conv.6 tensor(0.1124)
features.4.conv.0 tensor(0.0472)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1418)
features.5.conv.0 tensor(0.2498)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.1152)
features.6.conv.0 tensor(0.0535)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0853)
features.7.conv.0 tensor(0.1597)
features.7.conv.3 tensor(0.4476)
features.7.conv.6 tensor(0.2055)
features.8.conv.0 tensor(0.5291)
features.8.conv.3 tensor(0.5333)
features.8.conv.6 tensor(0.1622)
features.9.conv.0 tensor(0.3442)
features.9.conv.3 tensor(0.5584)
features.9.conv.6 tensor(0.1469)
features.10.conv.0 tensor(0.0671)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0750)
features.11.conv.0 tensor(0.7407)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.3874)
features.12.conv.0 tensor(0.6363)
features.12.conv.3 tensor(0.6703)
features.12.conv.6 tensor(0.1903)
features.13.conv.0 tensor(0.2642)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.0901)
features.14.conv.0 tensor(0.8862)
features.14.conv.3 tensor(0.8369)
features.14.conv.6 tensor(0.9431)
features.15.conv.0 tensor(0.8244)
features.15.conv.3 tensor(0.8218)
features.15.conv.6 tensor(0.9676)
features.16.conv.0 tensor(0.7130)
features.16.conv.3 tensor(0.7979)
features.16.conv.6 tensor(0.1164)
conv.0 tensor(0.0923)
tensor(957703.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
0.76098931
0.76099622
0.76102817
0.76092368
0.76083720
0.76094246
0.76100659
0.76106828
0.76118666
0.76107043
0.76071173
0.76042402
0.76036453
0.76046258
0.76014316
0.76000363
0.75984716
0.75987870
0.76011705
0.75995207
0.75967294
INFO - Training [23][   20/  196]   Loss 0.510387   Top1 82.675781   Top5 97.949219   BatchTime 0.352669   LR 0.000067
0.75936168
0.75900608
0.75893849
0.75889450
0.75898325
0.75898606
0.75899637
0.75933915
0.75904059
0.75873762
0.75853258
0.75846606
0.75858009
0.75839126
0.75819767
0.75812316
INFO - Training [23][   40/  196]   Loss 0.510465   Top1 82.587891   Top5 98.164062   BatchTime 0.305792   LR 0.000065
0.75819451
0.75824636
0.75848031
0.75827938
0.75796884
0.75792116
0.75790143
0.75767267
0.75739300
0.75731194
0.75743842
0.75699866
0.75666577
0.75674248
0.75666863
0.75655097
0.75656664
0.75638634
0.75623578
0.75623566
0.75619328
0.75620741
0.75620776
INFO - Training [23][   60/  196]   Loss 0.503232   Top1 82.766927   Top5 98.300781   BatchTime 0.290245   LR 0.000063
0.75633204
0.75628519
0.75610524
0.75603038
0.75596148
0.75606179
0.75620145
0.75616759
0.75616080
0.75620884
0.75633639
0.75648397
0.75666028
0.75690800
0.75645357
0.75628406
0.75615168
0.75604588
0.75614578
0.75619477
0.75606066
0.75566441
INFO - Training [23][   80/  196]   Loss 0.505906   Top1 82.685547   Top5 98.427734   BatchTime 0.285904   LR 0.000061
0.75555378
0.75558686
0.75554162
0.75551641
0.75540960
0.75549960
0.75554085
0.75554442
0.75540423
0.75536275
0.75514776
0.75474060
0.75452477
0.75421065
0.75359356
0.75351590
INFO - Training [23][  100/  196]   Loss 0.496514   Top1 82.957031   Top5 98.457031   BatchTime 0.279454   LR 0.000060
0.75372314
0.75370675
0.75382298
0.75364298
0.75385356
0.75390857
0.75385624
0.75332046
0.75306660
0.75278878
0.75253731
0.75212145
0.75199217
0.75202149
0.75199014
0.75170809
0.75170821
0.75139946
0.75098485
0.75066590
0.75019294
INFO - Training [23][  120/  196]   Loss 0.488442   Top1 83.196615   Top5 98.538411   BatchTime 0.280263   LR 0.000058
0.74992764
0.74948770
0.74929470
0.74903977
0.74983597
0.74937850
0.74927658
0.74881643
0.74855334
0.74798548
0.74738652
0.74707425
0.74670750
0.74614805
0.74588376
0.74567515
0.74560928
0.74502391
0.74486881
0.74455410
INFO - Training [23][  140/  196]   Loss 0.486166   Top1 83.300781   Top5 98.590960   BatchTime 0.282687   LR 0.000056
0.74425691
0.74397206
0.74371684
0.74321061
0.74289244
0.74269056
0.74273449
0.74268937
0.74261892
0.74247277
0.74246371
0.74190700
0.74192643
0.74151093
0.74106812
0.74068826
0.74057138
0.74066889
0.74072850
0.74028474
INFO - Training [23][  160/  196]   Loss 0.489547   Top1 83.154297   Top5 98.581543   BatchTime 0.284677   LR 0.000055
0.73977190
0.73965555
0.73963827
0.73919034
0.73912519
0.73919511
0.73937833
0.73929453
0.73936319
0.73980153
0.73957837
0.73928386
0.73910320
0.73889542
0.73865569
0.73851258
0.73847604
0.73844898
0.73864836
0.73880780
0.73873967
0.73889554
INFO - Training [23][  180/  196]   Loss 0.487845   Top1 83.213976   Top5 98.550347   BatchTime 0.283465   LR 0.000053
0.73865771
0.73863679
0.73876876
0.73896456
0.73871171
0.73857707
0.73864728
0.73878747
0.73877740
0.73859507
0.73851621
0.73846662
0.73872119
0.73844689
0.73831177
INFO - ==> Top1: 83.212    Top5: 98.570    Loss: 0.489
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [23][   20/   40]   Loss 0.355904   Top1 88.144531   Top5 99.472656   BatchTime 0.146402
INFO - Validation [23][   40/   40]   Loss 0.347807   Top1 88.340000   Top5 99.560000   BatchTime 0.112648
INFO - ==> Top1: 88.340    Top5: 99.560    Loss: 0.348
INFO - ==> Sparsity : 0.489
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 88.340   Top5: 99.560]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 87.790   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 87.520   Top5: 99.650]
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0773)
features.2.conv.0 tensor(0.0793)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.1392)
features.3.conv.0 tensor(0.0639)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1109)
features.4.conv.0 tensor(0.0518)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1449)
features.5.conv.0 tensor(0.2484)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.1056)
features.6.conv.0 tensor(0.0508)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0853)
features.7.conv.0 tensor(0.1652)
features.7.conv.3 tensor(0.4491)
features.7.conv.6 tensor(0.2075)
features.8.conv.0 tensor(0.5155)
features.8.conv.3 tensor(0.5347)
features.8.conv.6 tensor(0.1623)
features.9.conv.0 tensor(0.3672)
features.9.conv.3 tensor(0.5582)
features.9.conv.6 tensor(0.1478)
features.10.conv.0 tensor(0.0688)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0734)
features.11.conv.0 tensor(0.7574)
features.11.conv.3 tensor(0.6458)
features.11.conv.6 tensor(0.5531)
features.12.conv.0 tensor(0.6477)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.2011)
features.13.conv.0 tensor(0.2798)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.0907)
features.14.conv.0 tensor(0.8891)
features.14.conv.3 tensor(0.8361)
features.14.conv.6 tensor(0.9416)
features.15.conv.0 tensor(0.8272)
features.15.conv.3 tensor(0.8222)
features.15.conv.6 tensor(0.9678)
features.16.conv.0 tensor(0.7203)
features.16.conv.3 tensor(0.7979)
features.16.conv.6 tensor(0.4340)
conv.0 tensor(0.0919)
tensor(1069479.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.73817581
0.73802173
0.73787367
0.73824668
0.73791140
0.73778957
0.73751968
0.73726088
0.73702514
0.73681766
0.73676759
0.73649800
0.73632288
0.73597115
0.73581111
0.73552912
0.73542583
INFO - Training [24][   20/  196]   Loss 0.478035   Top1 83.281250   Top5 98.359375   BatchTime 0.398072   LR 0.000050
0.73535115
0.73523265
0.73497283
0.73482662
0.73451751
0.73422474
0.73427880
0.73444033
0.73418474
0.73402441
0.73373526
0.73348403
0.73321527
0.73285592
0.73254710
0.73218042
0.73160988
0.73119485
0.73111695
0.73100877
0.73060626
INFO - Training [24][   40/  196]   Loss 0.491641   Top1 82.783203   Top5 98.232422   BatchTime 0.338798   LR 0.000048
0.73039734
0.73047727
0.73056471
0.73036432
0.73013628
0.73014998
0.73058450
0.73065019
0.73062390
0.73050392
0.72985977
0.72971660
0.72957379
0.72932863
0.72913533
0.72894233
0.72868252
0.72873169
0.72860402
0.72839814
0.72820395
INFO - Training [24][   60/  196]   Loss 0.479358   Top1 83.463542   Top5 98.378906   BatchTime 0.319082   LR 0.000047
0.72841126
0.72863030
0.72810489
0.72798920
0.72803843
0.72808671
0.72821212
0.72826052
0.72826362
0.72821319
0.72790188
0.72775978
0.72768879
0.72758394
0.72756726
0.72738218
0.72744662
0.72723818
INFO - Training [24][   80/  196]   Loss 0.473879   Top1 83.676758   Top5 98.520508   BatchTime 0.322383   LR 0.000045
0.72711718
0.72726679
0.72715485
0.72725672
0.72744334
0.72730541
0.72719932
0.72716141
0.72722256
0.72723842
0.72683507
0.72674018
0.72654086
0.72649145
0.72646189
0.72656649
0.72664475
0.72662407
0.72636259
0.72621828
0.72631657
0.72632927
INFO - Training [24][  100/  196]   Loss 0.470445   Top1 83.687500   Top5 98.578125   BatchTime 0.312427   LR 0.000044
0.72626251
0.72634643
0.72648525
0.72661078
0.72679204
0.72684568
0.72681111
0.72670394
0.72664398
0.72652549
0.72667801
0.72696948
0.72725362
0.72690672
0.72683132
0.72652036
0.72626632
0.72622240
0.72626579
0.72620815
0.72627985
INFO - Training [24][  120/  196]   Loss 0.469786   Top1 83.684896   Top5 98.671875   BatchTime 0.308543   LR 0.000042
0.72631669
0.72626591
0.72596890
0.72593153
0.72587967
0.72585005
0.72600061
0.72570682
0.72555012
0.72574753
0.72584474
0.72584343
0.72594416
0.72605199
0.72581571
0.72585744
0.72569615
0.72545928
0.72536469
0.72561812
INFO - Training [24][  140/  196]   Loss 0.472065   Top1 83.688616   Top5 98.691406   BatchTime 0.306356   LR 0.000041
0.72538632
0.72512579
0.72494644
0.72464693
0.72472060
0.72454655
0.72441304
0.72393250
0.72364372
0.72365928
0.72382700
0.72380471
0.72358555
0.72362226
0.72351074
0.72376794
0.72389561
0.72376305
0.72368622
0.72387308
0.72365946
INFO - Training [24][  160/  196]   Loss 0.476720   Top1 83.505859   Top5 98.664551   BatchTime 0.302658   LR 0.000039
0.72354281
0.72339159
0.72310686
0.72311455
0.72311527
0.72284091
0.72269607
0.72294968
0.72317511
0.72329336
0.72334784
0.72314221
0.72321332
0.72304559
0.72299713
INFO - Training [24][  180/  196]   Loss 0.475791   Top1 83.574219   Top5 98.621962   BatchTime 0.300638   LR 0.000038
0.72318244
0.72310537
0.72291702
0.72294950
0.72322130
0.72337461
0.72320515
0.72299665
0.72300923
0.72311211
0.72283107
0.72258121
0.72247750
0.72271782
0.72254056
0.72230345
0.72221810
0.72207475
0.72222859
0.72237957
********************pre-trained*****************
INFO - ==> Top1: 83.546    Top5: 98.628    Loss: 0.477
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [24][   20/   40]   Loss 0.354439   Top1 88.300781   Top5 99.511719   BatchTime 0.133073
INFO - Validation [24][   40/   40]   Loss 0.348410   Top1 88.330000   Top5 99.670000   BatchTime 0.105325
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1426)
features.1.conv.0 tensor(0.0495)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0764)
features.2.conv.0 tensor(0.0804)
features.2.conv.3 tensor(0.3511)
features.2.conv.6 tensor(0.1403)
features.3.conv.0 tensor(0.0631)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1083)
features.4.conv.0 tensor(0.0539)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1468)
features.5.conv.0 tensor(0.2604)
features.5.conv.3 tensor(0.4167)
features.5.conv.6 tensor(0.1055)
features.6.conv.0 tensor(0.0506)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0821)
features.7.conv.0 tensor(0.1649)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.2078)
features.8.conv.0 tensor(0.5053)
features.8.conv.3 tensor(0.5353)
features.8.conv.6 tensor(0.1760)
features.9.conv.0 tensor(0.3769)
features.9.conv.3 tensor(0.5602)
features.9.conv.6 tensor(0.1474)
features.10.conv.0 tensor(0.0680)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0742)
features.11.conv.0 tensor(0.7618)
features.11.conv.3 tensor(0.6453)
features.11.conv.6 tensor(0.6221)
features.12.conv.0 tensor(0.6746)
features.12.conv.3 tensor(0.6705)
features.12.conv.6 tensor(0.2029)
features.13.conv.0 tensor(0.2669)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.0905)
features.14.conv.0 tensor(0.8920)
features.14.conv.3 tensor(0.8363)
features.14.conv.6 tensor(0.9441)
features.15.conv.0 tensor(0.8329)
features.15.conv.3 tensor(0.8228)
features.15.conv.6 tensor(0.9676)
features.16.conv.0 tensor(0.7197)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.6792)
conv.0 tensor(0.0918)
tensor(1151665.) 2188896.0
INFO - ==> Top1: 88.330    Top5: 99.670    Loss: 0.348
INFO - ==> Sparsity : 0.526
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 88.340   Top5: 99.560]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 88.330   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 87.790   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.72236019
0.72214723
0.72225064
0.72248012
0.72284174
0.72251129
0.72222036
0.72213870
0.72202790
0.72196084
0.72197759
0.72183752
0.72180688
0.72187448
0.72201568
0.72196758
0.72216684
INFO - Training [25][   20/  196]   Loss 0.464403   Top1 83.515625   Top5 98.242188   BatchTime 0.376720   LR 0.000035
0.72223461
0.72228044
0.72222292
0.72214121
0.72200292
0.72177023
0.72163808
0.72160137
0.72175074
0.72190356
0.72201484
0.72194481
0.72191191
0.72193336
0.72195178
0.72210366
0.72234017
0.72217715
0.72195810
0.72204959
0.72228777
0.72230959
INFO - Training [25][   40/  196]   Loss 0.471295   Top1 83.603516   Top5 98.291016   BatchTime 0.326872   LR 0.000034
0.72221601
0.72214472
0.72200513
0.72184414
0.72184461
0.72187239
0.72200471
0.72197956
0.72192973
0.72161257
0.72158420
0.72142255
0.72117919
0.72112185
0.72098422
0.72081178
0.72113472
0.72140831
0.72128141
0.72133476
0.72133011
INFO - Training [25][   60/  196]   Loss 0.473832   Top1 83.372396   Top5 98.417969   BatchTime 0.315125   LR 0.000033
0.72122520
0.72099048
0.72070920
0.72075236
0.72086155
0.72092640
0.72084963
0.72069889
0.72074252
0.72088075
0.72080356
0.72108012
0.72106320
0.72099191
0.72059095
0.72058702
0.72051823
0.72064173
0.72078300
INFO - Training [25][   80/  196]   Loss 0.470854   Top1 83.613281   Top5 98.530273   BatchTime 0.316288   LR 0.000031
0.72068667
0.72055978
0.72049814
0.72048235
0.72043675
0.72047907
0.72036237
0.71999294
0.71989316
0.71992266
0.71999270
0.71986896
0.72001636
0.72045165
0.72065645
0.72059226
0.72061592
0.72073334
0.72054058
0.72026783
0.72033221
INFO - Training [25][  100/  196]   Loss 0.468807   Top1 83.628906   Top5 98.535156   BatchTime 0.309850   LR 0.000030
0.72025818
0.72027791
0.72049224
0.72043371
0.72028238
0.72018850
0.72021800
0.72026342
0.71988624
0.71964759
0.71978199
0.71974236
0.71956760
0.71963400
0.71970773
0.71980453
0.71959561
0.71940804
0.71933556
0.71938163
0.71942860
0.71963358
INFO - Training [25][  120/  196]   Loss 0.465263   Top1 83.723958   Top5 98.606771   BatchTime 0.304080   LR 0.000029
0.71953571
0.71918869
0.71915346
0.71966058
0.71943527
0.71910876
0.71924579
0.71914822
0.71898317
0.71906281
0.71924120
0.71921057
0.71904802
INFO - Training [25][  140/  196]   Loss 0.462022   Top1 83.931362   Top5 98.649554   BatchTime 0.300695   LR 0.000027
0.71919018
0.71946919
0.71965563
0.71937078
0.71950364
0.71943843
0.71911132
0.71892214
0.71881813
0.71885681
0.71914941
0.71954876
0.71963519
0.71992809
0.71989560
0.71939433
0.71903145
0.71886432
0.71870130
0.71888185
0.71906954
0.71916300
0.71911544
0.71890968
0.71887529
0.71889108
INFO - Training [25][  160/  196]   Loss 0.465387   Top1 83.806152   Top5 98.615723   BatchTime 0.302646   LR 0.000026
0.71911186
0.71884328
0.71855593
0.71839118
0.71839726
0.71866888
0.71870613
0.71864676
0.71846092
0.71853966
0.71885747
0.71907377
0.71899718
0.71892065
0.71839935
0.71845698
0.71837384
0.71798694
0.71744537
INFO - Training [25][  180/  196]   Loss 0.463382   Top1 83.901910   Top5 98.598090   BatchTime 0.304528   LR 0.000025
0.71749496
0.71748936
0.71752775
0.71750957
0.71760076
0.71739435
0.71737123
0.71733505
0.71733421
0.71688008
0.71678549
0.71701664
0.71718842
0.71684384
0.71686870
INFO - ==> Top1: 83.922    Top5: 98.616    Loss: 0.462
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.71695960
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [25][   20/   40]   Loss 0.344242   Top1 88.359375   Top5 99.531250   BatchTime 0.131673
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1445)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0773)
features.2.conv.0 tensor(0.0822)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.1412)
features.3.conv.0 tensor(0.0622)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1124)
features.4.conv.0 tensor(0.0555)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1479)
features.5.conv.0 tensor(0.2533)
features.5.conv.3 tensor(0.4132)
features.5.conv.6 tensor(0.1047)
features.6.conv.0 tensor(0.0535)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0830)
features.7.conv.0 tensor(0.1651)
features.7.conv.3 tensor(0.4511)
features.7.conv.6 tensor(0.2247)
features.8.conv.0 tensor(0.5165)
features.8.conv.3 tensor(0.5359)
features.8.conv.6 tensor(0.1816)
features.9.conv.0 tensor(0.3897)
features.9.conv.3 tensor(0.5584)
features.9.conv.6 tensor(0.1455)
features.10.conv.0 tensor(0.0677)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.0739)
features.11.conv.0 tensor(0.7626)
features.11.conv.3 tensor(0.6456)
features.11.conv.6 tensor(0.6241)
features.12.conv.0 tensor(0.6910)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.1997)
features.13.conv.0 tensor(0.2791)
features.13.conv.3 tensor(0.4898)
features.13.conv.6 tensor(0.0906)
features.14.conv.0 tensor(0.8953)
features.14.conv.3 tensor(0.8362)
features.14.conv.6 tensor(0.9449)
features.15.conv.0 tensor(0.8366)
features.15.conv.3 tensor(0.8228)
features.15.conv.6 tensor(0.9675)
features.16.conv.0 tensor(0.7290)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.7707)
conv.0 tensor(0.0917)
tensor(1185028.) 2188896.0
INFO - Validation [25][   40/   40]   Loss 0.335395   Top1 88.480000   Top5 99.700000   BatchTime 0.093990
INFO - ==> Top1: 88.480    Top5: 99.700    Loss: 0.335
INFO - ==> Sparsity : 0.541
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.480   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 88.340   Top5: 99.560]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 88.330   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
0.71700341
0.71664393
0.71641785
0.71636099
0.71627182
0.71597177
0.71573752
0.71563506
0.71567410
0.71564066
0.71562105
0.71596706
0.71591657
0.71563518
0.71605784
0.71628654
0.71623623
0.71621501
0.71610576
0.71557772
0.71527672
0.71525127
INFO - Training [26][   20/  196]   Loss 0.496201   Top1 82.480469   Top5 98.027344   BatchTime 0.400884   LR 0.000023
0.71500033
0.71479887
0.71485126
0.71488601
0.71499109
0.71491611
0.71492553
0.71489131
0.71480018
0.71437490
0.71414280
0.71419644
0.71417922
0.71412051
INFO - Training [26][   40/  196]   Loss 0.482855   Top1 83.066406   Top5 98.339844   BatchTime 0.339613   LR 0.000022
0.71395624
0.71385920
0.71357471
0.71343309
0.71355981
0.71351397
0.71376127
0.71369797
0.71379608
0.71411020
0.71360141
0.71332860
0.71306026
0.71268260
0.71260631
0.71240300
0.71227479
0.71224785
0.71221215
0.71231133
0.71218067
0.71186042
INFO - Training [26][   60/  196]   Loss 0.476791   Top1 83.352865   Top5 98.417969   BatchTime 0.320040   LR 0.000021
0.71178901
0.71155721
0.71140337
0.71155220
0.71141154
0.71126759
0.71108788
0.71078724
0.71057308
0.71039730
0.71004993
0.70995700
0.70967150
0.70958573
0.70918846
0.70921028
0.70925051
0.70911133
0.70903724
0.70940274
0.70938808
0.70927978
0.70928341
INFO - Training [26][   80/  196]   Loss 0.478011   Top1 83.437500   Top5 98.520508   BatchTime 0.305335   LR 0.000019
0.70879555
0.70850521
0.70823532
0.70823824
0.70817804
0.70770139
0.70708567
0.70724332
0.70722681
0.70716518
0.70675272
0.70637953
0.70608908
0.70588189
0.70575118
0.70555967
INFO - Training [26][  100/  196]   Loss 0.470375   Top1 83.703125   Top5 98.566406   BatchTime 0.292158   LR 0.000018
0.70558763
0.70562857
0.70571440
0.70574898
0.70544767
0.70536840
0.70541131
0.70512462
0.70500618
0.70485330
0.70486456
0.70479512
0.70485002
0.70465237
0.70441639
0.70440346
0.70448464
0.70456475
0.70423269
0.70416069
0.70431787
0.70400113
INFO - Training [26][  120/  196]   Loss 0.468690   Top1 83.769531   Top5 98.632812   BatchTime 0.290282   LR 0.000017
0.70356548
0.70318347
0.70266587
0.70248771
0.70232564
0.70230216
0.70243084
0.70237702
0.70229203
0.70236462
0.70223385
0.70193857
0.70191026
0.70182991
0.70191854
0.70160890
0.70147747
0.70138222
0.70095599
0.70084584
0.70078915
INFO - Training [26][  140/  196]   Loss 0.469177   Top1 83.805804   Top5 98.677455   BatchTime 0.290195   LR 0.000016
0.70073932
0.70080006
0.70100987
0.70113879
0.70098358
0.70073146
0.70083284
0.70066631
0.70047861
0.70034844
0.70046091
0.70047623
0.70049351
0.70054215
0.70048851
0.70047253
0.70026231
0.70020932
0.70013905
0.70021844
INFO - Training [26][  160/  196]   Loss 0.469987   Top1 83.779297   Top5 98.657227   BatchTime 0.290712   LR 0.000015
0.70013607
0.69994467
0.69975424
0.69961685
0.69945449
0.69943333
0.69944763
0.69940871
0.69946414
0.69946682
0.69946855
0.69945806
0.69947648
0.69950116
0.69964612
INFO - Training [26][  180/  196]   Loss 0.468397   Top1 83.838976   Top5 98.628472   BatchTime 0.287978   LR 0.000014
0.69982487
0.69960856
0.69943160
0.69937921
0.69926375
0.69901848
0.69886237
0.69872528
0.69872057
0.69890070
0.69906545
0.69906873
0.69893366
0.69877285
0.69909793
0.69899571
0.69872385
0.69853097
0.69842315
0.69844615
0.69860053
INFO - ==> Top1: 83.890    Top5: 98.640    Loss: 0.468
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.351807   Top1 88.242188   Top5 99.531250   BatchTime 0.146667
INFO - Validation [26][   40/   40]   Loss 0.340484   Top1 88.400000   Top5 99.680000   BatchTime 0.098847
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.1348)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0961)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0871)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.1487)
features.3.conv.0 tensor(0.0613)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1124)
features.4.conv.0 tensor(0.0527)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1453)
features.5.conv.0 tensor(0.2638)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.1038)
features.6.conv.0 tensor(0.0540)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0820)
features.7.conv.0 tensor(0.1648)
features.7.conv.3 tensor(0.4511)
features.7.conv.6 tensor(0.3701)
features.8.conv.0 tensor(0.5240)
features.8.conv.3 tensor(0.5367)
features.8.conv.6 tensor(0.1753)
features.9.conv.0 tensor(0.3930)
features.9.conv.3 tensor(0.5584)
features.9.conv.6 tensor(0.1612)
features.10.conv.0 tensor(0.0685)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0738)
features.11.conv.0 tensor(0.7678)
features.11.conv.3 tensor(0.6453)
features.11.conv.6 tensor(0.6310)
features.12.conv.0 tensor(0.6985)
features.12.conv.3 tensor(0.6696)
features.12.conv.6 tensor(0.2771)
features.13.conv.0 tensor(0.2856)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.1119)
features.14.conv.0 tensor(0.8967)
features.14.conv.3 tensor(0.8361)
features.14.conv.6 tensor(0.9446)
features.15.conv.0 tensor(0.8384)
features.15.conv.3 tensor(0.8226)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.7337)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.8282)
conv.0 tensor(0.0915)
tensor(1215543.) 2188896.0
INFO - ==> Top1: 88.400    Top5: 99.680    Loss: 0.340
INFO - ==> Sparsity : 0.555
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.480   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.400   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 88.340   Top5: 99.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
0.69847161
0.69827175
0.69827592
0.69824457
0.69813973
0.69814730
0.69818574
0.69833857
0.69846797
0.69846320
0.69831020
0.69815522
0.69821429
0.69834191
0.69824207
0.69836026
0.69853324
0.69872242
0.69845736
0.69815916
INFO - Training [27][   20/  196]   Loss 0.467724   Top1 83.574219   Top5 98.027344   BatchTime 0.407902   LR 0.000013
0.69792652
0.69790894
0.69786239
0.69782501
0.69795763
0.69823569
0.69806731
0.69801289
0.69819015
0.69801712
0.69780612
0.69782865
0.69793755
0.69796664
0.69772989
0.69786012
0.69808221
0.69784826
0.69784302
0.69774252
0.69774258
0.69774127
INFO - Training [27][   40/  196]   Loss 0.468904   Top1 83.525391   Top5 98.271484   BatchTime 0.346184   LR 0.000012
0.69766563
0.69746053
0.69754922
0.69751942
0.69760144
0.69747341
0.69738138
0.69743687
0.69735271
0.69715685
0.69709766
0.69724423
0.69706082
0.69702214
0.69700009
0.69698495
INFO - Training [27][   60/  196]   Loss 0.461269   Top1 83.854167   Top5 98.463542   BatchTime 0.318019   LR 0.000011
0.69710684
0.69729650
0.69709283
0.69697165
0.69703120
0.69701970
0.69691443
0.69702291
0.69708312
0.69709241
0.69699490
0.69693935
0.69682783
0.69666773
0.69667161
0.69662851
0.69651628
0.69643390
0.69625509
0.69617510
0.69621420
0.69633049
0.69636238
INFO - Training [27][   80/  196]   Loss 0.459849   Top1 83.901367   Top5 98.642578   BatchTime 0.303882   LR 0.000010
0.69646108
0.69626957
0.69632810
0.69632810
0.69618851
0.69606435
0.69603628
0.69599992
0.69599891
0.69597727
0.69601154
0.69593275
0.69573873
0.69497347
0.69420016
0.69402844
0.69387043
0.69367790
0.69347638
0.69343632
0.69329858
0.69328856
INFO - Training [27][  100/  196]   Loss 0.459651   Top1 83.941406   Top5 98.648438   BatchTime 0.297765   LR 0.000009
0.69328749
0.69321573
0.69327116
0.69310737
0.69303137
0.69304180
0.69305247
0.69295651
0.69297802
0.69304085
0.69307095
0.69311851
0.69314915
0.69304579
0.69299644
INFO - Training [27][  120/  196]   Loss 0.455259   Top1 84.033203   Top5 98.697917   BatchTime 0.292207   LR 0.000009
0.69292235
0.69289064
0.69286448
0.69282109
0.69279939
0.69275010
0.69277072
0.69285440
0.69293427
0.69278383
0.69269907
0.69270313
0.69267297
0.69258034
0.69250274
0.69241953
0.69233918
0.69229060
0.69225335
0.69225085
0.69222558
0.69221646
INFO - Training [27][  140/  196]   Loss 0.453148   Top1 84.112723   Top5 98.750000   BatchTime 0.288652   LR 0.000008
0.69228095
0.69248044
0.69236541
0.69237095
0.69236058
0.69236374
0.69242465
0.69267893
0.69263422
0.69252694
0.69240928
0.69234151
0.69230872
0.69229430
0.69228536
0.69237214
INFO - Training [27][  160/  196]   Loss 0.454206   Top1 84.023438   Top5 98.740234   BatchTime 0.285810   LR 0.000007
0.69245094
0.69228619
0.69233489
0.69240493
0.69243139
0.69242704
0.69242263
0.69245094
0.69261158
0.69261599
0.69263786
0.69258165
0.69251072
0.69250536
0.69244015
0.69234747
0.69230813
0.69232720
0.69242793
0.69257796
0.69255900
0.69249570
INFO - Training [27][  180/  196]   Loss 0.454010   Top1 83.997396   Top5 98.695747   BatchTime 0.283707   LR 0.000007
0.69245797
0.69236958
0.69232458
0.69228029
0.69232816
0.69223595
0.69219011
0.69228238
0.69239932
0.69244641
0.69228160
0.69221270
0.69217432
0.69220310
0.69217622
0.69217962
INFO - ==> Top1: 84.018    Top5: 98.700    Loss: 0.454
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.69228691
0.69231772
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 0.343707   Top1 88.437500   Top5 99.589844   BatchTime 0.134190
INFO - Validation [27][   40/   40]   Loss 0.335375   Top1 88.550000   Top5 99.670000   BatchTime 0.094365
INFO - ==> Top1: 88.550    Top5: 99.670    Loss: 0.335
INFO - ==> Sparsity : 0.559
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.480   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 88.400   Top5: 99.680]
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1348)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0794)
features.2.conv.0 tensor(0.0880)
features.2.conv.3 tensor(0.3511)
features.2.conv.6 tensor(0.1438)
features.3.conv.0 tensor(0.0616)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1159)
features.4.conv.0 tensor(0.0539)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1449)
features.5.conv.0 tensor(0.2629)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.1042)
features.6.conv.0 tensor(0.0535)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0828)
features.7.conv.0 tensor(0.1652)
features.7.conv.3 tensor(0.4502)
features.7.conv.6 tensor(0.3853)
features.8.conv.0 tensor(0.5295)
features.8.conv.3 tensor(0.5362)
features.8.conv.6 tensor(0.1811)
features.9.conv.0 tensor(0.3991)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.1751)
features.10.conv.0 tensor(0.0690)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0737)
features.11.conv.0 tensor(0.7690)
features.11.conv.3 tensor(0.6453)
features.11.conv.6 tensor(0.6340)
features.12.conv.0 tensor(0.7050)
features.12.conv.3 tensor(0.6696)
features.12.conv.6 tensor(0.2727)
features.13.conv.0 tensor(0.2921)
features.13.conv.3 tensor(0.4890)
features.13.conv.6 tensor(0.1228)
features.14.conv.0 tensor(0.8966)
features.14.conv.3 tensor(0.8360)
features.14.conv.6 tensor(0.9448)
features.15.conv.0 tensor(0.8391)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.7359)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.8443)
conv.0 tensor(0.0913)
tensor(1223709.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
0.69243073
0.69239002
0.69237876
0.69248658
0.69255513
0.69259220
0.69247866
0.69251317
0.69245416
0.69231373
0.69225031
0.69234252
0.69244522
0.69239247
0.69243062
0.69237262
0.69227219
0.69228035
0.69227558
INFO - Training [28][   20/  196]   Loss 0.466018   Top1 83.750000   Top5 98.046875   BatchTime 0.350826   LR 0.000006
0.69233525
0.69224739
0.69223589
0.69235909
0.69239765
0.69230086
0.69224423
0.69217479
0.69214523
0.69208235
0.69199330
0.69192332
0.69186437
0.69138521
0.69071704
0.69062245
0.69059998
0.69055974
0.69066232
0.69059294
0.69065928
0.69050366
INFO - Training [28][   40/  196]   Loss 0.474092   Top1 83.671875   Top5 98.261719   BatchTime 0.312216   LR 0.000005
0.69041044
0.69019228
0.68958127
0.68917072
0.68909514
0.68904364
0.68907994
0.68915325
0.68896449
0.68887520
0.68887597
0.68886364
0.68885952
0.68878525
0.68875277
INFO - Training [28][   60/  196]   Loss 0.466129   Top1 84.114583   Top5 98.391927   BatchTime 0.297781   LR 0.000004
0.68876028
0.68873668
0.68881565
0.68882000
0.68871397
0.68867660
0.68867117
0.68867451
0.68860334
0.68856102
0.68855542
0.68857247
0.68860751
0.68865329
0.68871135
0.68852860
0.68846422
0.68845278
0.68844044
0.68840343
0.68838847
0.68837434
0.68837583
INFO - Training [28][   80/  196]   Loss 0.463707   Top1 84.150391   Top5 98.520508   BatchTime 0.289079   LR 0.000004
0.68837649
0.68840945
0.68843526
0.68846965
0.68854409
0.68857598
0.68861312
0.68862915
0.68844169
0.68839449
0.68837404
0.68834633
0.68830115
0.68827182
0.68824285
0.68822408
0.68822294
0.68822765
INFO - Training [28][  100/  196]   Loss 0.458979   Top1 84.277344   Top5 98.578125   BatchTime 0.297626   LR 0.000003
0.68821919
0.68821579
0.68824631
0.68824792
0.68825018
0.68827224
0.68831307
0.68841755
0.68846345
0.68852985
0.68860883
0.68852764
0.68848109
0.68845421
0.68842465
0.68836975
0.68833637
0.68830526
0.68830043
0.68835497
INFO - Training [28][  120/  196]   Loss 0.455308   Top1 84.368490   Top5 98.619792   BatchTime 0.297413   LR 0.000003
0.68842340
0.68852681
0.68845898
0.68842012
0.68839025
0.68838656
0.68835640
0.68830973
0.68827909
0.68826193
0.68823928
0.68822181
0.68820441
0.68822402
0.68825597
0.68831980
0.68836325
0.68840557
0.68844151
0.68837941
0.68838412
0.68834740
INFO - Training [28][  140/  196]   Loss 0.453415   Top1 84.492188   Top5 98.649554   BatchTime 0.294525   LR 0.000003
0.68828797
0.68826801
0.68822747
0.68820858
0.68820751
0.68823302
0.68826711
0.68834555
0.68837816
0.68837970
0.68837404
0.68846458
0.68842256
0.68839574
0.68840456
0.68837154
0.68836838
0.68834460
0.68827248
INFO - Training [28][  160/  196]   Loss 0.456189   Top1 84.345703   Top5 98.632812   BatchTime 0.296809   LR 0.000002
0.68825197
0.68826830
0.68826473
0.68825823
0.68828064
0.68838245
0.68843520
0.68844080
0.68845099
0.68834418
0.68828863
0.68829322
0.68833798
0.68837243
0.68835717
0.68837571
0.68841231
0.68842685
0.68831128
0.68825746
0.68823695
INFO - Training [28][  180/  196]   Loss 0.455943   Top1 84.296875   Top5 98.628472   BatchTime 0.295912   LR 0.000002
0.68822896
0.68821704
0.68823117
0.68823564
0.68831176
0.68838412
0.68832046
0.68829441
0.68826640
0.68823779
0.68820596
0.68820417
0.68818283
0.68818527
INFO - ==> Top1: 84.396    Top5: 98.670    Loss: 0.452
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.68822783
0.68823004
0.68834633
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.341386   Top1 88.613281   Top5 99.550781   BatchTime 0.132183
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0903)
features.1.conv.6
INFO - Validation [28][   40/   40]   Loss 0.334203   Top1 88.550000   Top5 99.680000   BatchTime 0.091969
INFO - ==> Top1: 88.550    Top5: 99.680    Loss: 0.334
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0871)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.1435)
features.3.conv.0 tensor(0.0628)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1187)
features.4.conv.0 tensor(0.0540)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1449)
features.5.conv.0 tensor(0.2630)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.1126)
features.6.conv.0 tensor(0.0535)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0837)
features.7.conv.0 tensor(0.1657)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.3872)
features.8.conv.0 tensor(0.5319)
features.8.conv.3 tensor(0.5356)
features.8.conv.6 tensor(0.1813)
features.9.conv.0 tensor(0.3992)
features.9.conv.3 tensor(0.5576)
features.9.conv.6 tensor(0.1759)
features.10.conv.0 tensor(0.0682)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0738)
features.11.conv.0 tensor(0.7692)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.6359)
features.12.conv.0 tensor(0.7061)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.2727)
features.13.conv.0 tensor(0.2982)
features.13.conv.3 tensor(0.4890)
features.13.conv.6 tensor(0.1260)
features.14.conv.0 tensor(0.8968)
features.14.conv.3 tensor(0.8363)
features.14.conv.6 tensor(0.9449)
features.15.conv.0 tensor(0.8396)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9673)
features.16.conv.0 tensor(0.7392)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.8489)
conv.0 tensor(0.1037)
tensor(1231896.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
0.68827856
0.68830836
0.68832308
0.68821204
0.68817568
0.68816394
0.68817735
0.68816912
0.68819386
0.68826115
0.68839306
0.68830514
0.68821812
0.68820107
0.68819755
0.68816692
INFO - Training [29][   20/  196]   Loss 0.460775   Top1 84.140625   Top5 98.125000   BatchTime 0.401863   LR 0.000001
0.68814111
0.68816197
0.68820709
0.68827081
0.68825108
0.68834937
0.68829817
0.68823153
0.68811679
0.68802810
0.68788034
0.68756974
0.68698525
0.68686694
0.68685430
0.68682826
0.68681872
0.68679148
0.68677378
0.68676460
0.68673784
0.68672639
0.68672127
0.68670690
0.68670720
INFO - Training [29][   40/  196]   Loss 0.462162   Top1 83.916016   Top5 98.388672   BatchTime 0.362803   LR 0.000001
0.68670237
0.68669778
0.68669331
0.68669611
0.68668884
0.68668687
0.68668163
0.68667465
0.68666774
0.68666595
0.68666726
0.68666089
0.68665695
0.68665695
0.68665349
0.68664598
0.68664509
0.68664664
0.68664044
INFO - Training [29][   60/  196]   Loss 0.464704   Top1 83.736979   Top5 98.417969   BatchTime 0.350219   LR 0.000001
0.68664330
0.68664539
0.68664557
0.68664324
0.68664104
0.68663901
0.68664110
0.68664080
0.68663597
0.68663734
0.68663353
0.68663144
0.68663460
0.68663162
0.68663687
0.68663305
0.68663180
0.68662691
INFO - Training [29][   80/  196]   Loss 0.464996   Top1 83.779297   Top5 98.583984   BatchTime 0.345369   LR 0.000001
0.68662983
0.68662912
0.68663114
0.68662703
0.68662834
0.68662435
0.68662912
0.68663019
0.68663508
0.68662989
0.68662870
0.68662483
0.68662632
0.68662506
0.68662792
0.68662709
0.68662697
0.68662679
INFO - Training [29][  100/  196]   Loss 0.456656   Top1 84.140625   Top5 98.589844   BatchTime 0.337437   LR 0.000000
0.68662578
0.68662351
0.68662059
0.68661994
0.68661755
0.68662030
0.68661797
0.68661427
0.68661189
0.68661219
0.68661261
0.68661255
0.68661511
0.68661702
0.68662131
0.68661916
0.68661636
0.68661565
0.68662053
0.68662798
0.68661898
0.68661332
INFO - Training [29][  120/  196]   Loss 0.450353   Top1 84.433594   Top5 98.636068   BatchTime 0.327446   LR 0.000000
0.68661636
0.68661469
0.68661433
0.68661314
0.68661255
0.68660951
0.68661046
0.68661183
0.68660891
0.68660510
0.68660450
0.68660694
0.68661129
0.68660992
0.68660873
0.68660629
0.68660784
0.68661243
0.68660849
0.68660825
0.68660372
0.68660110
0.68660027
INFO - Training [29][  140/  196]   Loss 0.449542   Top1 84.458705   Top5 98.696987   BatchTime 0.318127   LR 0.000000
0.68659711
0.68659705
0.68659186
0.68659455
0.68659735
0.68659961
0.68659765
0.68659872
0.68660265
0.68660110
0.68660378
0.68660522
0.68660843
0.68660724
0.68660837
0.68661159
0.68661916
0.68661976
0.68661827
0.68661553
INFO - Training [29][  160/  196]   Loss 0.452339   Top1 84.313965   Top5 98.686523   BatchTime 0.315944   LR 0.000000
0.68662119
0.68662363
0.68662691
0.68662775
0.68662912
0.68662578
0.68662256
0.68662035
0.68662888
0.68662697
0.68662268
0.68662250
0.68662447
0.68662292
0.68661952
0.68662381
0.68662441
0.68661755
0.68661273
INFO - Training [29][  180/  196]   Loss 0.452486   Top1 84.225260   Top5 98.637153   BatchTime 0.315743   LR 0.000000
0.68661320
0.68661755
0.68662435
0.68662333
0.68662757
0.68662125
0.68661892
0.68661726
0.68662143
0.68661267
0.68661284
0.68660909
0.68661165
0.68661606
INFO - ==> Top1: 84.272    Top5: 98.620    Loss: 0.452
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.68661451
0.68660998
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 0.339799   Top1 88.632812   Top5 99.531250   BatchTime 0.154349
INFO - Validation [29][   40/   40]   Loss 0.330049   Top1 88.680000   Top5 99.680000   BatchTime 0.104358
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.1348)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0871)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.1435)
features.3.conv.0 tensor(0.0622)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1185)
features.4.conv.0 tensor(0.0540)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1468)
features.5.conv.0 tensor(0.2627)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.1126)
features.6.conv.0 tensor(0.0534)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0837)
features.7.conv.0 tensor(0.1658)
features.7.conv.3 tensor(0.4502)
features.7.conv.6 tensor(0.3877)
features.8.conv.0 tensor(0.5320)
features.8.conv.3 tensor(0.5356)
features.8.conv.6 tensor(0.1816)
features.9.conv.0 tensor(0.3990)
features.9.conv.3 tensor(0.5576)
features.9.conv.6 tensor(0.1760)
features.10.conv.0 tensor(0.0686)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0738)
features.11.conv.0 tensor(0.7693)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.6359)
features.12.conv.0 tensor(0.7062)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.2728)
features.13.conv.0 tensor(0.2987)
features.13.conv.3 tensor(0.4890)
features.13.conv.6 tensor(0.1274)
features.14.conv.0 tensor(0.8968)
features.14.conv.3 tensor(0.8363)
features.14.conv.6 tensor(0.9448)
features.15.conv.0 tensor(0.8396)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.7392)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.8496)
conv.0 tensor(0.1042)
tensor(1232517.) 2188896.0
INFO - ==> Top1: 88.680    Top5: 99.680    Loss: 0.330
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
0.68661815
0.68654025
0.68637359
0.68597317
0.68561679
0.68535793
0.68511355
0.68518794
0.68652451
0.68625706
0.68585581
0.68547642
0.68599170
0.68719429
0.68758124
0.68790764
0.68887079
0.69205314
0.69166189
0.69143015
0.69116032
INFO - Training [30][   20/  196]   Loss 0.470692   Top1 83.515625   Top5 98.242188   BatchTime 0.340393   LR 0.000125
0.69084471
0.69049096
0.69030374
0.69003177
0.68958014
0.69047809
0.69063812
0.69013840
0.68971699
0.68903804
0.68830162
0.68730938
0.68801713
0.68732566
0.68721533
0.68618250
0.68551600
0.68512160
0.68438280
0.68380111
0.68370610
0.68360436
INFO - Training [30][   40/  196]   Loss 0.495620   Top1 82.812500   Top5 98.183594   BatchTime 0.307829   LR 0.000125
0.68345684
0.68356150
0.68339574
0.68348539
0.68535262
0.68468624
0.68397611
0.68343121
0.68334496
0.68297708
0.68272543
0.68247831
0.68176925
0.68126142
INFO - Training [30][   60/  196]   Loss 0.490564   Top1 83.059896   Top5 98.313802   BatchTime 0.296828   LR 0.000125
0.68123770
0.68098205
0.68075192
0.68069738
0.68029934
0.68004155
0.67949831
0.67988151
0.67940283
0.67924511
0.67899454
0.67853695
0.67834729
0.67786545
0.67762959
0.67727429
0.67697215
0.67672640
0.67619127
0.67555356
0.67503268
0.67508751
0.67425746
INFO - Training [30][   80/  196]   Loss 0.493382   Top1 83.139648   Top5 98.442383   BatchTime 0.288274   LR 0.000125
0.67354167
0.67305160
0.67261273
0.67194402
0.67136717
0.67108762
0.67098242
0.67078108
0.67063212
0.67045999
0.67036629
0.67034292
0.67047065
0.67074543
0.67117649
0.67096943
INFO - Training [30][  100/  196]   Loss 0.492131   Top1 83.125000   Top5 98.437500   BatchTime 0.281900   LR 0.000125
0.67094398
0.67098504
0.67111355
0.67133486
0.67129707
0.67128801
0.67142743
0.67136300
0.67160821
0.67165047
0.67189115
0.67200458
0.67222863
0.67245662
0.67242622
0.67223072
0.67230719
0.67202252
0.67194974
0.67171472
0.67172521
0.67177224
0.67188382
INFO - Training [30][  120/  196]   Loss 0.488942   Top1 83.170573   Top5 98.518880   BatchTime 0.277661   LR 0.000125
0.67162341
0.67143506
0.67133296
0.67142969
0.67150944
0.67179376
0.67156142
0.67123795
0.67119402
0.67128688
0.67132103
0.67123824
0.67128307
0.67087209
0.67073315
0.67084867
0.67086369
0.67098278
0.67103398
0.67082655
0.67087024
0.67078716
0.67133951
INFO - Training [30][  140/  196]   Loss 0.486843   Top1 83.189174   Top5 98.577009   BatchTime 0.275716   LR 0.000125
0.67192823
0.67254555
0.67296815
0.67336202
0.67372090
0.67402649
0.67449528
0.67467171
0.67493272
0.67487895
0.67501241
0.67521465
0.67538881
0.67526603
INFO - Training [30][  160/  196]   Loss 0.487010   Top1 83.156738   Top5 98.559570   BatchTime 0.275753   LR 0.000125
0.67500359
0.67496431
0.67496538
0.67517257
0.67547482
0.67525756
0.67525840
0.67547280
0.67542398
0.67540705
0.67549813
0.67554539
0.67562807
0.67573625
0.67549026
0.67549479
0.67549455
0.67527646
0.67510200
0.67511094
0.67519718
0.67506933
0.67520207
0.67545629
0.67554176
0.67527944
0.67548257
INFO - Training [30][  180/  196]   Loss 0.491430   Top1 82.949219   Top5 98.506944   BatchTime 0.279730   LR 0.000125
0.67567044
0.67566526
0.67533481
0.67521626
0.67519319
0.67518425
0.67532909
0.67536563
0.67545485
0.67542154
0.67536116
0.67559904
0.67560917
INFO - ==> Top1: 82.884    Top5: 98.468    Loss: 0.493
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.380714   Top1 87.089844   Top5 99.628906   BatchTime 0.130134
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.0767)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1398)
features.3.conv.0 tensor(0.0530)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1150)
features.4.conv.0 tensor(0.0500)
features.4.conv.3 tensor(0.3090)
features.4.conv.6 tensor(0.1424)
features.5.conv.0 tensor(0.2664)
features.5.conv.3 tensor(0.4201)
features.5.conv.6 tensor(0.1349)
features.6.conv.0 tensor(0.0542)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.1576)
features.7.conv.3 tensor(0.4473)
features.7.conv.6 tensor(0.4904)
features.8.conv.0 tensor(0.5046)
features.8.conv.3 tensor(0.5370)
features.8.conv.6 tensor(0.2370)
features.9.conv.0 tensor(0.3504)
features.9.conv.3 tensor(0.5584)
features.9.conv.6 tensor(0.2705)
features.10.conv.0 tensor(0.0646)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0739)
features.11.conv.0 tensor(0.7500)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.7298)
features.12.conv.0 tensor(0.6629)
features.12.conv.3 tensor(0.6701)
features.12.conv.6 tensor(0.4528)
features.13.conv.0 tensor(0.2495)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.0997)
features.14.conv.0 tensor(0.8948)
features.14.conv.3 tensor(0.8366)
features.14.conv.6 tensor(0.9432)
features.15.conv.0 tensor(0.8351)
features.15.conv.3 tensor(0.8219)
features.15.conv.6 tensor(0.9676)
features.16.conv.0 tensor(0.7040)
features.16.conv.3 tensor(0.7975)
features.16.conv.6 tensor(0.8577)
conv.0 tensor(0.0919)
tensor(1233820.) 2188896.0
INFO - Validation [30][   40/   40]   Loss 0.370858   Top1 87.360000   Top5 99.670000   BatchTime 0.092828
INFO - ==> Top1: 87.360    Top5: 99.670    Loss: 0.371
INFO - ==> Sparsity : 0.564
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
0.67531067
0.67525810
0.67514443
0.67491984
0.67497581
0.67511046
0.67508203
0.67512566
0.67519289
0.67500520
0.67497617
0.67499042
0.67500472
0.67527950
0.67506200
0.67473006
INFO - Training [31][   20/  196]   Loss 0.520924   Top1 81.484375   Top5 97.988281   BatchTime 0.345621   LR 0.000125
0.67464823
0.67461622
0.67438245
0.67429698
0.67429829
0.67403066
0.67371660
0.67367297
0.67361683
0.67348295
0.67337853
0.67319649
0.67313230
0.67311716
0.67296052
0.67277133
0.67191619
0.67152244
0.67083251
0.67021286
0.66971475
0.66904098
0.66841298
0.66797334
INFO - Training [31][   40/  196]   Loss 0.515942   Top1 81.933594   Top5 98.164062   BatchTime 0.298937   LR 0.000125
0.66754222
0.66717833
0.66718167
0.66713297
0.66663259
0.66637266
0.66619688
0.66592270
0.66581392
0.66572058
0.66562146
0.66545290
0.66570598
0.66580284
0.66596377
0.66607904
0.66624033
0.66654128
0.66705036
0.66730350
0.66794598
0.66943604
INFO - Training [31][   60/  196]   Loss 0.509997   Top1 82.233073   Top5 98.255208   BatchTime 0.291408   LR 0.000125
0.66956866
0.66982144
0.66969961
0.66967064
0.66984838
0.66986555
0.66963208
0.66949415
0.66941184
0.66917467
0.66915184
0.66900456
0.66878873
0.66885000
0.66894794
0.66887778
0.66849583
0.66833067
0.66823196
0.66822636
INFO - Training [31][   80/  196]   Loss 0.500937   Top1 82.607422   Top5 98.369141   BatchTime 0.294197   LR 0.000125
0.66788214
0.66739804
0.66725200
0.66714513
0.66709071
0.66699469
0.66687387
0.66657227
0.66648370
0.66642231
0.66639709
0.66649550
0.66622478
0.66618717
INFO - Training [31][  100/  196]   Loss 0.495892   Top1 82.769531   Top5 98.417969   BatchTime 0.291804   LR 0.000125
0.66625887
0.66600811
0.66592515
0.66574746
0.66566503
0.66550863
0.66529202
0.66525877
0.66521418
0.66528487
0.66488618
0.66482937
0.66457826
0.66440505
0.66436416
0.66439331
0.66432530
0.66416800
0.66386145
0.66383028
0.66378564
0.66365719
INFO - Training [31][  120/  196]   Loss 0.493002   Top1 82.858073   Top5 98.470052   BatchTime 0.288930   LR 0.000125
0.66364199
0.66338414
0.66307855
0.66294312
0.66302997
0.66270840
0.66262382
0.66222072
0.66198766
0.66182315
0.66156363
0.66129959
0.66113168
0.66119969
0.66112447
0.66082960
0.66079849
0.66062903
0.66061133
0.66051155
0.66059053
0.66024745
INFO - Training [31][  140/  196]   Loss 0.491742   Top1 83.058036   Top5 98.523996   BatchTime 0.285941   LR 0.000124
0.66009688
0.65991884
0.65981507
0.65960866
0.65962082
0.65972054
0.65977460
0.65974492
0.65995848
0.65995991
0.66004544
0.66005212
0.65999895
0.66008657
0.66027409
0.66036671
0.66058034
0.66100448
0.66154569
0.66214448
0.66283482
0.66338640
INFO - Training [31][  160/  196]   Loss 0.494161   Top1 82.978516   Top5 98.483887   BatchTime 0.284870   LR 0.000124
0.66411316
0.66503221
0.66589558
0.66632515
0.66666859
0.66654003
0.66650337
0.66644579
0.66644961
0.66633564
0.66652304
0.66675806
0.66678584
0.66684794
0.66660029
INFO - Training [31][  180/  196]   Loss 0.497540   Top1 82.849392   Top5 98.428819   BatchTime 0.283298   LR 0.000124
0.66658086
0.66651547
0.66634572
0.66631830
0.66639012
0.66635042
0.66618907
0.66601753
0.66600585
0.66586190
0.66593909
0.66605210
0.66590518
0.66568577
0.66578138
0.66588795
0.66576147
0.66551322
0.66547173
********************pre-trained*****************
INFO - ==> Top1: 82.486    Top5: 98.202    Loss: 0.506
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [31][   20/   40]   Loss 0.390568   Top1 86.777344   Top5 99.511719   BatchTime 0.132855
INFO - Validation [31][   40/   40]   Loss 0.377874   Top1 87.120000   Top5 99.600000   BatchTime 0.094708
INFO - ==> Top1: 87.120    Top5: 99.600    Loss: 0.378
INFO - ==> Sparsity : 0.570
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1992)
features.1.conv.0 tensor(0.0417)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.0787)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.1383)
features.3.conv.0 tensor(0.0605)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1181)
features.4.conv.0 tensor(0.0477)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1406)
features.5.conv.0 tensor(0.2585)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1340)
features.6.conv.0 tensor(0.0516)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0840)
features.7.conv.0 tensor(0.1553)
features.7.conv.3 tensor(0.4485)
features.7.conv.6 tensor(0.5063)
features.8.conv.0 tensor(0.5006)
features.8.conv.3 tensor(0.5350)
features.8.conv.6 tensor(0.2083)
features.9.conv.0 tensor(0.3560)
features.9.conv.3 tensor(0.5602)
features.9.conv.6 tensor(0.5005)
features.10.conv.0 tensor(0.0676)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0732)
features.11.conv.0 tensor(0.7433)
features.11.conv.3 tensor(0.6454)
features.11.conv.6 tensor(0.7393)
features.12.conv.0 tensor(0.6459)
features.12.conv.3 tensor(0.6692)
features.12.conv.6 tensor(0.5942)
features.13.conv.0 tensor(0.2507)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.1000)
features.14.conv.0 tensor(0.8959)
features.14.conv.3 tensor(0.8372)
features.14.conv.6 tensor(0.9454)
features.15.conv.0 tensor(0.8385)
features.15.conv.3 tensor(0.8216)
features.15.conv.6 tensor(0.9666)
features.16.conv.0 tensor(0.7106)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8553)
conv.0 tensor(0.0905)
tensor(1246873.) 2188896.0
0.66556203
0.66545928
0.66527450
0.66539836
0.66523868
0.66516167
0.66538233
0.66527110
0.66513091
0.66507477
0.66479170
0.66467756
0.66490209
0.66515732
0.66515797
0.66491687
0.66480398
0.66461867
0.66445589
INFO - Training [32][   20/  196]   Loss 0.489421   Top1 83.222656   Top5 98.144531   BatchTime 0.369179   LR 0.000124
0.66460949
0.66478717
0.66488367
0.66490138
0.66482329
0.66503030
0.66508549
0.66455793
0.66412926
0.66387355
0.66360909
0.66352695
0.66350722
0.66336972
0.66344386
0.66336715
0.66328013
0.66324109
0.66306651
0.66309321
0.66310608
INFO - Training [32][   40/  196]   Loss 0.504093   Top1 82.490234   Top5 98.164062   BatchTime 0.331139   LR 0.000124
0.66266197
0.66259909
0.66257018
0.66274822
0.66279256
0.66268611
0.66257465
0.66271985
0.66267669
0.66262901
0.66273654
0.66270119
0.66269994
0.66283923
0.66292983
0.66283512
0.66285807
0.66301638
0.66314369
INFO - Training [32][   60/  196]   Loss 0.500155   Top1 82.604167   Top5 98.248698   BatchTime 0.323985   LR 0.000124
0.66312981
0.66311479
0.66306722
0.66300827
0.66308933
0.66270089
0.66265118
0.66276312
0.66257864
0.66271764
0.66280514
0.66294968
0.66311103
0.66317350
0.66280210
0.66260672
0.66264081
0.66262150
0.66250014
0.66258705
INFO - Training [32][   80/  196]   Loss 0.497837   Top1 82.778320   Top5 98.408203   BatchTime 0.320041   LR 0.000124
0.66275108
0.66266972
0.66286582
0.66301996
0.66285527
0.66304845
0.66307771
0.66305256
0.66320443
0.66292477
0.66285616
0.66300231
0.66324550
0.66329527
0.66313702
0.66343689
0.66328341
0.66303873
0.66298640
0.66314113
INFO - Training [32][  100/  196]   Loss 0.493782   Top1 82.941406   Top5 98.492188   BatchTime 0.314854   LR 0.000124
0.66334230
0.66349858
0.66333234
0.66321623
0.66356605
0.66313970
0.66324532
0.66290623
0.66297555
0.66279274
0.66270584
0.66283953
0.66275239
0.66280085
0.66279119
0.66296667
0.66278392
0.66277981
0.66274625
0.66267967
0.66246164
INFO - Training [32][  120/  196]   Loss 0.487052   Top1 83.264974   Top5 98.538411   BatchTime 0.309186   LR 0.000124
0.66242534
0.66255391
0.66266632
0.66277009
0.66279227
0.66272837
0.66259819
0.66249806
0.66262507
0.66261357
0.66252381
0.66243219
0.66287506
0.66239446
0.66217679
0.66192961
0.66186881
0.66176391
0.66175044
0.66180307
0.66169125
0.66175836
INFO - Training [32][  140/  196]   Loss 0.486826   Top1 83.231027   Top5 98.588170   BatchTime 0.304604   LR 0.000124
0.66194552
0.66224432
0.66239887
0.66256863
0.66264111
0.66254604
0.66247994
0.66254079
0.66226208
0.66237247
0.66244942
0.66260445
0.66238797
0.66233432
0.66268498
INFO - Training [32][  160/  196]   Loss 0.488388   Top1 83.107910   Top5 98.588867   BatchTime 0.299485   LR 0.000123
0.66275942
0.66292810
0.66300476
0.66283238
0.66286683
0.66298532
0.66293311
0.66303486
0.66290855
0.66266340
0.66255373
0.66263229
0.66261435
0.66243464
0.66229045
0.66233706
0.66244787
0.66243321
0.66210288
0.66204691
0.66192359
INFO - Training [32][  180/  196]   Loss 0.489849   Top1 83.051215   Top5 98.548177   BatchTime 0.298949   LR 0.000123
0.66190261
0.66160154
0.66135788
0.66113508
0.66110152
0.66098362
0.66080612
0.66095865
0.66092455
0.66098088
0.66100597
0.66105026
0.66116315
0.66121185
0.66117436
0.66095430
INFO - ==> Top1: 83.038    Top5: 98.530    Loss: 0.490
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.66080946
0.66087532
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.387745   Top1 87.207031   Top5 99.414062   BatchTime 0.149970
INFO - Validation [32][   40/   40]   Loss 0.381535   Top1 87.190000   Top5 99.590000   BatchTime 0.101039
INFO - ==> Top1: 87.190    Top5: 99.590    Loss: 0.382
INFO - ==> Sparsity : 0.558
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1895)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1027)
features.2.conv.3 tensor(0.3410)
features.2.conv.6 tensor(0.1415)
features.3.conv.0 tensor(0.0605)
features.3.conv.3 tensor(0.0841)
features.3.conv.6 tensor(0.1233)
features.4.conv.0 tensor(0.0516)
features.4.conv.3 tensor(0.3113)
features.4.conv.6 tensor(0.1444)
features.5.conv.0 tensor(0.2581)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.1387)
features.6.conv.0 tensor(0.0482)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0811)
features.7.conv.0 tensor(0.1615)
features.7.conv.3 tensor(0.4482)
features.7.conv.6 tensor(0.5048)
features.8.conv.0 tensor(0.5040)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.2017)
features.9.conv.0 tensor(0.3621)
features.9.conv.3 tensor(0.5538)
features.9.conv.6 tensor(0.5953)
features.10.conv.0 tensor(0.0669)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0720)
features.11.conv.0 tensor(0.7430)
features.11.conv.3 tensor(0.6456)
features.11.conv.6 tensor(0.7338)
features.12.conv.0 tensor(0.6584)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.6477)
features.13.conv.0 tensor(0.2538)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.0998)
features.14.conv.0 tensor(0.8928)
features.14.conv.3 tensor(0.8384)
features.14.conv.6 tensor(0.9451)
features.15.conv.0 tensor(0.8405)
features.15.conv.3 tensor(0.8226)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.7127)
features.16.conv.3 tensor(0.7977)
features.16.conv.6 tensor(0.7511)
conv.0 tensor(0.0917)
tensor(1221631.) 2188896.0
0.66068816
0.66108942
0.66106761
0.66103083
0.66085333
0.66086680
0.66100961
0.66083932
0.66096473
0.66063148
0.66071844
0.66092378
0.66075855
0.66072398
0.66072237
0.66066504
0.66077185
0.66076785
0.66069579
INFO - Training [33][   20/  196]   Loss 0.507903   Top1 82.441406   Top5 98.398438   BatchTime 0.376944   LR 0.000123
0.66076070
0.66061074
0.66057354
0.66057938
0.66069996
0.66085523
0.66075718
0.66105759
0.66094702
0.66079032
0.66081548
0.66050196
0.66051233
0.66059214
0.66072732
0.66048330
0.66045874
0.66042626
0.66041714
0.66065133
0.66043299
INFO - Training [33][   40/  196]   Loss 0.512096   Top1 82.246094   Top5 98.359375   BatchTime 0.332474   LR 0.000123
0.66030234
0.66043621
0.66053683
0.66042161
0.66041088
0.66042781
0.66056204
0.66048390
0.66051704
0.66056508
0.66057789
0.66065174
0.66062647
0.66051841
0.66047531
0.66047972
0.66049993
0.66063553
0.66058552
0.66071433
INFO - Training [33][   60/  196]   Loss 0.501018   Top1 82.610677   Top5 98.450521   BatchTime 0.323517   LR 0.000123
0.66069376
0.66049725
0.66023296
0.66017747
0.66014534
0.66009688
0.66014147
0.66022259
0.66023690
0.66034544
0.66033703
0.66014308
0.66004205
0.66007322
0.66002160
0.65985131
0.65987754
0.65994281
INFO - Training [33][   80/  196]   Loss 0.500843   Top1 82.670898   Top5 98.505859   BatchTime 0.326096   LR 0.000123
0.66013199
0.66159934
0.66191584
0.66212475
0.66223705
0.66232991
0.66228646
0.66235173
0.66240090
0.66220325
0.66223294
0.66210586
0.66205209
0.66204500
0.66224253
0.66242760
0.66227984
0.66246539
0.66217268
INFO - Training [33][  100/  196]   Loss 0.494676   Top1 82.886719   Top5 98.527344   BatchTime 0.321561   LR 0.000123
0.66210699
0.66196364
0.66179973
0.66165584
0.66167015
0.66182142
0.66149724
0.66150379
0.66168374
0.66176826
0.66171682
0.66188830
0.66149640
0.66151488
0.66160113
0.66151381
0.66163141
0.66167498
0.66160285
0.66149050
0.66164422
INFO - Training [33][  120/  196]   Loss 0.488135   Top1 83.082682   Top5 98.616536   BatchTime 0.316577   LR 0.000123
0.66147649
0.66141939
0.66124147
0.66111875
0.66101658
0.66096228
0.66083199
0.66063064
0.66057408
0.66065848
0.66064787
0.66067463
0.66092688
0.66103351
0.66085082
0.66091514
0.66102040
0.66100299
0.66095519
0.66072541
0.66063064
0.66054600
0.66028380
INFO - Training [33][  140/  196]   Loss 0.490486   Top1 83.041295   Top5 98.635603   BatchTime 0.308163   LR 0.000122
0.66023904
0.66007048
0.66020215
0.66036952
0.66018248
0.66019917
0.66012168
0.66009837
0.66020292
0.66034001
0.66044313
0.66065109
0.66057283
0.66074383
0.66093558
0.66095811
INFO - Training [33][  160/  196]   Loss 0.493076   Top1 82.973633   Top5 98.630371   BatchTime 0.300936   LR 0.000122
0.66105008
0.66068780
0.66074777
0.66070706
0.66061425
0.66055483
0.66053939
0.66065758
0.66087300
0.66085917
0.66087675
0.66091144
0.66058958
0.66055167
0.66055644
0.66046244
0.66050708
0.66038620
0.66044974
0.66056097
0.66061056
0.66056228
0.66041690
INFO - Training [33][  180/  196]   Loss 0.493532   Top1 82.988281   Top5 98.576389   BatchTime 0.296601   LR 0.000122
0.66026670
0.66039926
0.66002846
0.65996391
0.66023773
0.66050839
0.66072136
0.66073912
0.66065705
0.66070622
0.66053134
0.66054946
0.66013801
INFO - ==> Top1: 83.096    Top5: 98.592    Loss: 0.490
0.66020793
0.66058284
0.66055131
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 0.386516   Top1 87.011719   Top5 99.511719   BatchTime 0.132428
INFO - Validation [33][   40/   40]   Loss 0.373339   Top1 87.220000   Top5 99.610000   BatchTime 0.093094
INFO - ==> Top1: 87.220    Top5: 99.610    Loss: 0.373
INFO - ==> Sparsity : 0.565
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2708)
features.0.conv.3 tensor(0.1895)
features.1.conv.0 tensor(0.0443)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.0969)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.1412)
features.3.conv.0 tensor(0.0587)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1191)
features.4.conv.0 tensor(0.0524)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1504)
features.5.conv.0 tensor(0.2433)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.1309)
features.6.conv.0 tensor(0.0514)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0824)
features.7.conv.0 tensor(0.1591)
features.7.conv.3 tensor(0.4447)
features.7.conv.6 tensor(0.5130)
features.8.conv.0 tensor(0.4991)
features.8.conv.3 tensor(0.5365)
features.8.conv.6 tensor(0.2012)
features.9.conv.0 tensor(0.3577)
features.9.conv.3 tensor(0.5593)
features.9.conv.6 tensor(0.6153)
features.10.conv.0 tensor(0.0661)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0719)
features.11.conv.0 tensor(0.7502)
features.11.conv.3 tensor(0.6464)
features.11.conv.6 tensor(0.7486)
features.12.conv.0 tensor(0.6525)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.6619)
features.13.conv.0 tensor(0.2541)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.0996)
features.14.conv.0 tensor(0.8909)
features.14.conv.3 tensor(0.8366)
features.14.conv.6 tensor(0.9481)
features.15.conv.0 tensor(0.8430)
features.15.conv.3 tensor(0.8226)
features.15.conv.6 tensor(0.9673)
features.16.conv.0 tensor(0.7182)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.7930)
conv.0 tensor(0.0911)
tensor(1237561.) 2188896.0
0.66038072
0.66037560
0.66066551
0.66046804
0.66023624
0.66017061
0.66015548
0.65993273
0.66007590
0.66025245
0.66021985
0.66017157
0.66002530
0.66011214
0.66006798
0.66024703
0.66011870
0.65992320
INFO - Training [34][   20/  196]   Loss 0.477888   Top1 82.695312   Top5 98.046875   BatchTime 0.389544   LR 0.000122
0.66006494
0.65991122
0.66002023
0.65980822
0.65977210
0.65990543
0.65995759
0.65988880
0.65979207
0.65980339
0.65966427
0.65988624
0.65993118
0.65985590
0.65983462
0.65998888
0.65985841
0.65989417
0.65982896
0.65969056
0.65974182
INFO - Training [34][   40/  196]   Loss 0.494724   Top1 82.373047   Top5 98.222656   BatchTime 0.341206   LR 0.000122
0.65945840
0.65961337
0.65955782
0.65905517
0.65892869
0.65910894
0.65942299
0.65929186
0.65917319
0.65903240
0.65909111
0.65876079
0.65879840
0.65895754
0.65921450
0.65937954
0.65958774
0.65971899
0.65985239
0.65993214
0.65989894
0.65944743
INFO - Training [34][   60/  196]   Loss 0.491701   Top1 82.753906   Top5 98.287760   BatchTime 0.319350   LR 0.000121
0.65938723
0.65940320
0.65925962
0.65922260
0.65925342
0.65924883
0.65906572
0.65948957
0.65943539
0.65942895
0.65941292
0.65951294
0.65959066
0.65937448
0.65947396
0.65923303
0.65939778
0.65921819
0.65916991
0.65908504
0.65904391
0.65921676
INFO - Training [34][   80/  196]   Loss 0.492806   Top1 82.836914   Top5 98.398438   BatchTime 0.307859   LR 0.000121
0.65911102
0.65908301
0.65898502
0.65884125
0.65897381
0.65900612
0.65874064
0.65863049
0.65865916
0.65885603
0.65899277
0.65916646
0.65913087
0.65902537
0.65906805
0.65898973
INFO - Training [34][  100/  196]   Loss 0.489161   Top1 82.957031   Top5 98.441406   BatchTime 0.296157   LR 0.000121
0.65907121
0.65879613
0.65885293
0.65889406
0.65920132
0.65943438
0.65937293
0.65939087
0.65920264
0.65910977
0.65896642
0.65877247
0.65879679
0.65853155
0.65838659
0.65863824
0.65855223
0.65844548
0.65844202
0.65852767
0.65856946
0.65815878
0.65806109
INFO - Training [34][  120/  196]   Loss 0.485385   Top1 83.111979   Top5 98.544922   BatchTime 0.289679   LR 0.000121
0.65803194
0.65815920
0.65827596
0.65805656
0.65783846
0.65775323
0.65782315
0.65785688
0.65806156
0.65828878
0.65843999
0.65869194
0.65893465
0.65875483
INFO - Training [34][  140/  196]   Loss 0.483043   Top1 83.217076   Top5 98.596540   BatchTime 0.289057   LR 0.000121
0.65901077
0.65878093
0.65871137
0.65852344
0.65869832
0.65837359
0.65821940
0.65826261
0.65847892
0.65856737
0.65868318
0.65855098
0.65845186
0.65812004
0.65817881
0.65812463
0.65809482
0.65809083
0.65797287
0.65785944
0.65782928
0.65764898
INFO - Training [34][  160/  196]   Loss 0.484418   Top1 83.122559   Top5 98.579102   BatchTime 0.286571   LR 0.000121
0.65767562
0.65775502
0.65782714
0.65786916
0.65800530
0.65809023
0.65846026
0.65869248
0.65863770
0.65821493
0.65812856
0.65821141
0.65797836
0.65791172
0.65808260
0.65808755
0.65825230
0.65830231
0.65849829
0.65900874
0.65879339
0.65846914
0.65860134
0.65864682
INFO - Training [34][  180/  196]   Loss 0.485004   Top1 83.042535   Top5 98.565538   BatchTime 0.282360   LR 0.000120
0.65873206
0.65881145
0.65837151
0.65842563
0.65854263
0.65869039
0.65865117
0.65869266
0.65888584
0.65874481
0.65890831
0.65874201
0.65883666
0.65909421
********************pre-trained*****************
INFO - ==> Top1: 83.064    Top5: 98.540    Loss: 0.485
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.375101   Top1 87.285156   Top5 99.414062   BatchTime 0.132210
INFO - Validation [34][   40/   40]   Loss 0.364903   Top1 87.530000   Top5 99.550000   BatchTime 0.093650
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.1895)
features.1.conv.0 tensor(0.0430)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0977)
features.2.conv.0 tensor(0.0938)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.1337)
features.3.conv.0 tensor(0.0573)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1194)
features.4.conv.0 tensor(0.0439)
features.4.conv.3 tensor(0.3084)
features.4.conv.6 tensor(0.1432)
features.5.conv.0 tensor(0.2204)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.1296)
features.6.conv.0 tensor(0.0560)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0848)
features.7.conv.0 tensor(0.1576)
features.7.conv.3 tensor(0.4433)
features.7.conv.6 tensor(0.5226)
features.8.conv.0 tensor(0.5012)
features.8.conv.3 tensor(0.5359)
features.8.conv.6 tensor(0.2022)
features.9.conv.0 tensor(0.3623)
features.9.conv.3 tensor(0.5550)
features.9.conv.6 tensor(0.6189)
features.10.conv.0 tensor(0.0662)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0736)
features.11.conv.0 tensor(0.7452)
features.11.conv.3 tensor(0.6458)
features.11.conv.6 tensor(0.7616)
features.12.conv.0 tensor(0.6573)
features.12.conv.3 tensor(0.6688)
features.12.conv.6 tensor(0.6990)
features.13.conv.0 tensor(0.2546)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.0999)
features.14.conv.0 tensor(0.8928)
features.14.conv.3 tensor(0.8378)
features.14.conv.6 tensor(0.9471)
features.15.conv.0 tensor(0.8451)
features.15.conv.3 tensor(0.8218)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.7158)
features.16.conv.3 tensor(0.7972)
features.16.conv.6 tensor(0.8020)
conv.0 tensor(0.0910)
tensor(1243633.) 2188896.0
INFO - ==> Top1: 87.530    Top5: 99.550    Loss: 0.365
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
0.65936953
0.65894610
0.65908635
0.65904653
0.65915722
0.65914816
0.65927428
0.65931088
0.65935087
0.65915412
0.65912753
0.65925825
0.65927744
0.65913266
0.65912968
0.65926802
0.65945488
0.65935081
0.65923077
0.65949452
0.65921336
0.65895325
INFO - Training [35][   20/  196]   Loss 0.485392   Top1 83.046875   Top5 97.734375   BatchTime 0.394722   LR 0.000120
0.65900093
0.65887409
0.65878016
0.65863180
0.65854388
0.65838510
0.65837735
0.65840966
0.65849358
0.65862411
0.65905893
0.65950888
0.65926641
0.65899056
0.65903807
0.65895617
0.65884537
0.65860009
INFO - Training [35][   40/  196]   Loss 0.496421   Top1 82.626953   Top5 98.056641   BatchTime 0.365125   LR 0.000120
0.65867841
0.65866691
0.65877622
0.65907341
0.65863472
0.65846080
0.65858978
0.65873581
0.65874988
0.65879112
0.65890050
0.65871739
0.65856409
0.65851516
0.65893018
0.65872341
0.65842837
0.65843040
INFO - Training [35][   60/  196]   Loss 0.493551   Top1 82.851562   Top5 98.190104   BatchTime 0.354428   LR 0.000120
0.65827036
0.65835798
0.65849137
0.65851647
0.65837067
0.65859300
0.65826190
0.65815967
0.65812749
0.65814590
0.65812922
0.65794665
0.65815979
0.65843052
0.65846562
0.65814447
0.65822577
0.65821195
0.65818316
0.65813124
0.65821207
0.65837967
0.65849358
0.65844077
INFO - Training [35][   80/  196]   Loss 0.490407   Top1 82.993164   Top5 98.305664   BatchTime 0.350381   LR 0.000119
0.65818036
0.65823448
0.65828359
0.65823227
0.65827543
0.65816128
0.65812480
0.65814322
0.65814573
0.65805358
0.65781176
0.65800458
0.65828580
0.65819085
0.65816528
0.65814161
0.65820026
INFO - Training [35][  100/  196]   Loss 0.483695   Top1 83.277344   Top5 98.406250   BatchTime 0.329488   LR 0.000119
0.65831703
0.65847176
0.65820777
0.65807313
0.65843302
0.65842265
0.65794897
0.65781498
0.65794015
0.65780294
0.65765971
0.65764803
0.65773034
0.65758759
0.65756035
0.65781426
0.65782547
0.65788895
0.65797651
0.65808731
0.65773970
0.65789944
0.65806729
0.65791053
INFO - Training [35][  120/  196]   Loss 0.476545   Top1 83.479818   Top5 98.509115   BatchTime 0.315807   LR 0.000119
0.65771765
0.65757614
0.65752727
0.65744138
0.65756845
0.65747082
0.65748781
0.65768486
0.65783173
0.65778297
0.65775734
0.65769821
0.65746868
0.65738881
0.65748358
0.65754580
0.65786946
0.65740585
0.65724933
INFO - Training [35][  140/  196]   Loss 0.476878   Top1 83.468192   Top5 98.523996   BatchTime 0.313551   LR 0.000119
0.65726787
0.65725940
0.65728396
0.65729594
0.65732497
0.65728307
0.65746224
0.65769154
0.65769184
0.65790874
0.65806627
0.65797263
0.65788156
0.65785998
0.65790355
INFO - Training [35][  160/  196]   Loss 0.479515   Top1 83.371582   Top5 98.496094   BatchTime 0.309485   LR 0.000119
0.65756935
0.65765637
0.65781075
0.65776420
0.65781355
0.65783626
0.65794712
0.65809572
0.65818685
0.65805662
0.65792900
0.65806025
0.65809482
0.65803361
0.65829021
0.65782177
0.65750939
0.65700275
0.65709543
0.65724361
0.65727234
0.65738922
INFO - Training [35][  180/  196]   Loss 0.478236   Top1 83.424479   Top5 98.513455   BatchTime 0.306162   LR 0.000118
0.65736395
0.65747941
0.65747130
0.65755910
0.65751731
0.65728331
0.65689892
0.65680438
0.65675992
0.65683109
0.65679032
0.65682447
0.65700173
0.65722752
0.65740359
INFO - ==> Top1: 83.416    Top5: 98.524    Loss: 0.478
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.65723950
0.65700203
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.370444   Top1 87.441406   Top5 99.472656   BatchTime 0.130400
INFO - Validation [35][   40/   40]   Loss 0.362015   Top1 87.770000   Top5 99.610000   BatchTime 0.094062
INFO - ==> Top1: 87.770    Top5: 99.610    Loss: 0.362
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0417)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0911)
features.2.conv.0 tensor(0.1016)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1348)
features.3.conv.0 tensor(0.0622)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.1155)
features.4.conv.0 tensor(0.0452)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1463)
features.5.conv.0 tensor(0.2437)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.1266)
features.6.conv.0 tensor(0.0498)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0840)
features.7.conv.0 tensor(0.1580)
features.7.conv.3 tensor(0.4462)
features.7.conv.6 tensor(0.5220)
features.8.conv.0 tensor(0.5184)
features.8.conv.3 tensor(0.5347)
features.8.conv.6 tensor(0.2011)
features.9.conv.0 tensor(0.3671)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6201)
features.10.conv.0 tensor(0.0662)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0725)
features.11.conv.0 tensor(0.7423)
features.11.conv.3 tensor(0.6454)
features.11.conv.6 tensor(0.7625)
features.12.conv.0 tensor(0.6493)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.7010)
features.13.conv.0 tensor(0.2576)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.0999)
features.14.conv.0 tensor(0.8924)
features.14.conv.3 tensor(0.8373)
features.14.conv.6 tensor(0.9459)
features.15.conv.0 tensor(0.8486)
features.15.conv.3 tensor(0.8220)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.7129)
features.16.conv.3 tensor(0.7978)
features.16.conv.6 tensor(0.8068)
conv.0 tensor(0.0909)
tensor(1245163.) 2188896.0
0.65669411
0.65657759
0.65646917
0.65646029
0.65660268
0.65710956
0.65668732
0.65674067
0.65695584
0.65696663
0.65711379
0.65696442
0.65699106
0.65707481
0.65700084
0.65691984
INFO - Training [36][   20/  196]   Loss 0.476982   Top1 83.632812   Top5 98.007812   BatchTime 0.334275   LR 0.000118
0.65702492
0.65704066
0.65712941
0.65720963
0.65712363
0.65746951
0.65756774
0.65750271
0.65742880
0.65739596
0.65714616
0.65694070
0.65695435
0.65691251
0.65691572
0.65689802
0.65697509
0.65712827
0.65718025
0.65714896
0.65714890
0.65710717
INFO - Training [36][   40/  196]   Loss 0.483756   Top1 83.447266   Top5 98.125000   BatchTime 0.302595   LR 0.000118
0.65726352
0.65727180
0.65713412
0.65705293
0.65694463
0.65690255
0.65705675
0.65706283
0.65746468
0.65740383
0.65746510
0.65752858
0.65729392
0.65713930
0.65705067
0.65688753
0.65701818
0.65722668
0.65710884
0.65698206
0.65730387
0.65738052
0.65723252
INFO - Training [36][   60/  196]   Loss 0.483152   Top1 83.352865   Top5 98.229167   BatchTime 0.288542   LR 0.000117
0.65724295
0.65713233
0.65706772
0.65686500
0.65694553
0.65679193
0.65658534
0.65669876
0.65696961
0.65684271
0.65669781
0.65663213
0.65641898
0.65636814
0.65632588
INFO - Training [36][   80/  196]   Loss 0.474594   Top1 83.691406   Top5 98.398438   BatchTime 0.283870   LR 0.000117
0.65636635
0.65628058
0.65618002
0.65598512
0.65593010
0.65591115
0.65614522
0.65640599
0.65617692
0.65598637
0.65617067
0.65593666
0.65592903
0.65602350
0.65597814
0.65570277
0.65581536
0.65576380
0.65576422
0.65590018
0.65760630
0.65754521
INFO - Training [36][  100/  196]   Loss 0.472186   Top1 83.796875   Top5 98.464844   BatchTime 0.282665   LR 0.000117
0.65758932
0.65758932
0.65742481
0.65732741
0.65751189
0.65749949
0.65735310
0.65737259
0.65741384
0.65761155
0.65797997
0.65805316
0.65821213
0.65793175
0.65793979
0.65800893
0.65792739
0.65770793
0.65787172
0.65806538
0.65814710
0.65833640
INFO - Training [36][  120/  196]   Loss 0.465602   Top1 84.020182   Top5 98.522135   BatchTime 0.280753   LR 0.000117
0.65819120
0.65816724
0.65793073
0.65790343
0.65778315
0.65773720
0.65781337
0.65808332
0.65806907
0.65797079
0.65786541
0.65792042
0.65796065
0.65794778
0.65834290
0.65809315
0.65791357
0.65788418
0.65782815
0.65784520
0.65786320
0.65781528
0.65754473
INFO - Training [36][  140/  196]   Loss 0.466779   Top1 83.939732   Top5 98.599330   BatchTime 0.277167   LR 0.000117
0.65753073
0.65753859
0.65769595
0.65793204
0.65802860
0.65786487
0.65785140
0.65769804
0.65775347
0.65768826
0.65737921
0.65742606
0.65761334
0.65768307
INFO - Training [36][  160/  196]   Loss 0.472357   Top1 83.798828   Top5 98.603516   BatchTime 0.277903   LR 0.000116
0.65786874
0.65802228
0.65793401
0.65778881
0.65759706
0.65754688
0.65724921
0.65719891
0.65711606
0.65729618
0.65718615
0.65692478
0.65650570
0.65633291
0.65642333
0.65611416
0.65596086
0.65566522
0.65535975
0.65514660
0.65507859
0.65492797
INFO - Training [36][  180/  196]   Loss 0.474152   Top1 83.736979   Top5 98.561198   BatchTime 0.278361   LR 0.000116
0.65491569
0.65477866
0.65469509
0.65448189
0.65418261
0.65408838
0.65375662
0.65360194
0.65361339
0.65364802
0.65340668
0.65349323
0.65342957
0.65345722
0.65355897
0.65323120
0.65304852
********************pre-trained*****************
INFO - ==> Top1: 83.686    Top5: 98.542    Loss: 0.475
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [36][   20/   40]   Loss 0.361096   Top1 87.929688   Top5 99.511719   BatchTime 0.129831
INFO - Validation [36][   40/   40]   Loss 0.357876   Top1 87.880000   Top5 99.640000   BatchTime 0.093752
INFO - ==> Top1: 87.880    Top5: 99.640    Loss: 0.358
INFO - ==> Sparsity : 0.571
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1758)
features.1.conv.0 tensor(0.0410)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.1011)
features.2.conv.0 tensor(0.0940)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.1829)
features.3.conv.0 tensor(0.0582)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.1165)
features.4.conv.0 tensor(0.0366)
features.4.conv.3 tensor(0.3119)
features.4.conv.6 tensor(0.1475)
features.5.conv.0 tensor(0.2327)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.1261)
features.6.conv.0 tensor(0.0493)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.1576)
features.7.conv.3 tensor(0.4447)
features.7.conv.6 tensor(0.5185)
features.8.conv.0 tensor(0.5109)
features.8.conv.3 tensor(0.5365)
features.8.conv.6 tensor(0.2020)
features.9.conv.0 tensor(0.3685)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.6195)
features.10.conv.0 tensor(0.0663)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0742)
features.11.conv.0 tensor(0.7463)
features.11.conv.3 tensor(0.6472)
features.11.conv.6 tensor(0.7597)
features.12.conv.0 tensor(0.6474)
features.12.conv.3 tensor(0.6705)
features.12.conv.6 tensor(0.7224)
features.13.conv.0 tensor(0.2469)
features.13.conv.3 tensor(0.4894)
features.13.conv.6 tensor(0.1147)
features.14.conv.0 tensor(0.8938)
features.14.conv.3 tensor(0.8373)
features.14.conv.6 tensor(0.9489)
features.15.conv.0 tensor(0.8512)
features.15.conv.3 tensor(0.8223)
features.15.conv.6 tensor(0.9682)
features.16.conv.0 tensor(0.7158)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8096)
conv.0 tensor(0.0904)
tensor(1249218.) 2188896.0
0.65280128
0.65265012
0.65227884
0.65199739
0.65181249
0.65156603
0.65146220
0.65139157
0.65109092
0.65105373
0.65117031
0.65122271
0.65075010
0.65064102
0.65188700
0.65192872
0.65184045
0.65181166
0.65178001
INFO - Training [37][   20/  196]   Loss 0.479409   Top1 83.691406   Top5 98.183594   BatchTime 0.374875   LR 0.000116
0.65153873
0.65144497
0.65148777
0.65135676
0.65114087
0.65115857
0.65105164
0.65105444
0.65111053
0.65113032
0.65150207
0.65209270
0.65318233
0.65501171
0.65576488
0.65555990
0.65568358
0.65701866
0.65745598
0.65747249
0.65756810
INFO - Training [37][   40/  196]   Loss 0.487435   Top1 83.437500   Top5 98.222656   BatchTime 0.328930   LR 0.000115
0.65781468
0.65808535
0.65776402
0.65752906
0.65726686
0.65718168
0.65706390
0.65689421
0.65685707
0.65683454
0.65687591
0.65692431
0.65690023
0.65701318
0.65691048
0.65680963
0.65668827
0.65673476
0.65664721
0.65670222
0.65675426
0.65669364
0.65678853
INFO - Training [37][   60/  196]   Loss 0.484229   Top1 83.528646   Top5 98.404948   BatchTime 0.309123   LR 0.000115
0.65711039
0.65740401
0.65741348
0.65704030
0.65694356
0.65710765
0.65709060
0.65698677
0.65712059
0.65693855
0.65690589
0.65676779
0.65672886
0.65660620
0.65662205
INFO - Training [37][   80/  196]   Loss 0.478859   Top1 83.662109   Top5 98.496094   BatchTime 0.297152   LR 0.000115
0.65650982
0.65650749
0.65692520
0.65683037
0.65706974
0.65707326
0.65698075
0.65682727
0.65692228
0.65691429
0.65701550
0.65690005
0.65694672
0.65708202
0.65723270
0.65730900
0.65687722
0.65695190
0.65714604
0.65696907
0.65691370
0.65676409
INFO - Training [37][  100/  196]   Loss 0.473503   Top1 83.714844   Top5 98.527344   BatchTime 0.293045   LR 0.000114
0.65662444
0.65643996
0.65657812
0.65674365
0.65702021
0.65722919
0.65725797
0.65712917
0.65716839
0.65701967
0.65676206
0.65656304
0.65661788
0.65672141
0.65683275
0.65695280
0.65720499
0.65744293
0.65715241
0.65695888
0.65682930
0.65685630
INFO - Training [37][  120/  196]   Loss 0.468553   Top1 83.902995   Top5 98.554688   BatchTime 0.289256   LR 0.000114
0.65701574
0.65706700
0.65726298
0.65710944
0.65705264
0.65709937
0.65691185
0.65671313
0.65658641
0.65676934
0.65652603
0.65653127
0.65661776
0.65673500
0.65666443
0.65707225
INFO - Training [37][  140/  196]   Loss 0.465724   Top1 84.012277   Top5 98.593750   BatchTime 0.283571   LR 0.000114
0.65694243
0.65681273
0.65645689
0.65652835
0.65666932
0.65660268
0.65672195
0.65698510
0.65686989
0.65700489
0.65692484
0.65676934
0.65677142
0.65672183
0.65677029
0.65660924
0.65650427
0.65672898
0.65687162
0.65699232
0.65708375
0.65685010
0.65679181
INFO - Training [37][  160/  196]   Loss 0.468007   Top1 83.889160   Top5 98.564453   BatchTime 0.281324   LR 0.000114
0.65677667
0.65660232
0.65630639
0.65616298
0.65628052
0.65656739
0.65650260
0.65663165
0.65669274
0.65655619
0.65630060
0.65624321
0.65641987
0.65643716
0.65657175
0.65644062
0.65666926
0.65690750
0.65690571
0.65701348
INFO - Training [37][  180/  196]   Loss 0.469246   Top1 83.838976   Top5 98.537326   BatchTime 0.283792   LR 0.000113
0.65680957
0.65657568
0.65660906
0.65667039
0.65674609
0.65694982
0.65709835
0.65699720
0.65693623
0.65721089
0.65688246
0.65684122
0.65699714
0.65693849
0.65679115
INFO - ==> Top1: 83.878    Top5: 98.530    Loss: 0.468
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.372259   Top1 87.031250   Top5 99.492188   BatchTime 0.134743
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0436)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1039)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1574)
features.3.conv.0 tensor(0.0558)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1137)
features.4.conv.0 tensor(0.0418)
features.4.conv.3 tensor(0.3113)
features.4.conv.6 tensor(0.1493)
features.5.conv.0 tensor(0.2477)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1257)
features.6.conv.0 tensor(0.0467)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0823)
features.7.conv.0 tensor(0.1516)
features.7.conv.3 tensor(0.4462)
features.7.conv.6 tensor(0.5264)
features.8.conv.0 tensor(0.5164)
features.8.conv.3 tensor(0.5359)
features.8.conv.6 tensor(0.2014)
features.9.conv.0 tensor(0.3802)
features.9.conv.3 tensor(0.5561)
features.9.conv.6 tensor(0.6235)
features.10.conv.0 tensor(0.0614)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0733)
features.11.conv.0 tensor(0.7533)
features.11.conv.3 tensor(0.6453)
features.11.conv.6 tensor(0.7583)
features.12.conv.0 tensor(0.6620)
features.12.conv.3 tensor(0.6698)
features.12.conv.6 tensor(0.7248)
features.13.conv.0 tensor(0.2464)
features.13.conv.3 tensor(0.4890)
features.13.conv.6 tensor(0.0989)
features.14.conv.0 tensor(0.8962)
features.14.conv.3 tensor(0.8373)
features.14.conv.6 tensor(0.9498)
features.15.conv.0 tensor(0.8505)
features.15.conv.3 tensor(0.8233)
features.15.conv.6 tensor(0.9675)
features.16.conv.0 tensor(0.7204)
features.16.conv.3 tensor(0.7977)
features.16.conv.6 tensor(0.8183)
conv.0 tensor(0.0897)
tensor(1252793.) 2188896.0
INFO - Validation [37][   40/   40]   Loss 0.362172   Top1 87.530000   Top5 99.640000   BatchTime 0.095731
INFO - ==> Top1: 87.530    Top5: 99.640    Loss: 0.362
INFO - ==> Sparsity : 0.572
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
0.65688413
0.65670294
0.65660667
0.65657198
0.65664631
0.65676296
0.65670913
0.65665072
0.65660185
0.65658838
0.65663671
0.65658927
0.65643573
0.65631461
0.65628052
0.65606076
0.65597099
0.65591168
0.65583599
0.65592402
0.65596122
0.65580231
0.65589124
INFO - Training [38][   20/  196]   Loss 0.473519   Top1 83.906250   Top5 98.222656   BatchTime 0.367131   LR 0.000113
0.65591395
0.65586179
0.65592164
0.65587008
0.65593791
0.65605360
0.65622324
0.65626252
0.65628189
0.65615994
0.65627056
0.65619659
0.65613514
0.65598416
0.65602356
INFO - Training [38][   40/  196]   Loss 0.478907   Top1 83.603516   Top5 98.261719   BatchTime 0.315040   LR 0.000112
0.65583968
0.65563619
0.65582711
0.65603417
0.65608966
0.65611750
0.65618593
0.65611780
0.65618986
0.65611178
0.65600151
0.65592343
0.65592271
0.65590233
0.65607017
0.65618563
0.65602738
0.65573746
0.65584499
0.65576333
0.65577900
0.65581840
INFO - Training [38][   60/  196]   Loss 0.468910   Top1 83.893229   Top5 98.359375   BatchTime 0.302866   LR 0.000112
0.65600306
0.65601873
0.65606201
0.65619624
0.65617096
0.65604049
0.65617156
0.65600991
0.65587819
0.65595907
0.65602165
0.65591341
0.65609348
0.65640056
0.65628868
0.65593022
0.65583348
0.65586346
0.65585172
0.65585190
INFO - Training [38][   80/  196]   Loss 0.467292   Top1 83.989258   Top5 98.471680   BatchTime 0.304271   LR 0.000112
0.65591151
0.65599334
0.65603077
0.65595418
0.65599412
0.65628427
0.65617806
0.65631062
0.65622014
0.65624648
0.65619665
0.65604949
0.65604883
0.65613085
0.65616363
0.65610898
0.65622288
0.65618783
0.65605408
0.65640044
INFO - Training [38][  100/  196]   Loss 0.461740   Top1 84.183594   Top5 98.527344   BatchTime 0.301176   LR 0.000112
0.65643758
0.65618610
0.65611833
0.65619761
0.65653175
0.65635556
0.65600097
0.65606546
0.65633470
0.65597826
0.65589184
0.65572220
0.65600145
0.65577906
0.65570414
0.65577787
0.65568262
0.65576273
0.65590334
0.65591890
0.65565795
INFO - Training [38][  120/  196]   Loss 0.458419   Top1 84.345703   Top5 98.629557   BatchTime 0.299200   LR 0.000111
0.65551084
0.65574580
0.65561634
0.65615892
0.65606153
0.65586990
0.65565640
0.65549839
0.65531206
0.65535611
0.65524644
0.65536755
0.65514243
0.65505517
0.65534389
INFO - Training [38][  140/  196]   Loss 0.459286   Top1 84.305246   Top5 98.677455   BatchTime 0.294479   LR 0.000111
0.65551716
0.65557528
0.65566409
0.65538937
0.65574050
0.65546632
0.65535939
0.65515417
0.65494692
0.65484619
0.65465796
0.65460044
0.65457153
0.65446842
0.65452152
0.65453023
0.65464455
0.65473551
0.65473586
0.65487403
0.65510422
0.65553421
INFO - Training [38][  160/  196]   Loss 0.462988   Top1 84.145508   Top5 98.691406   BatchTime 0.292292   LR 0.000111
0.65545380
0.65556741
0.65571737
0.65523958
0.65506899
0.65512013
0.65500951
0.65469623
0.65480179
0.65487915
0.65493983
0.65515333
0.65511930
0.65498900
0.65501171
0.65498984
0.65478438
0.65470654
0.65479720
0.65491211
0.65488648
0.65506297
INFO - Training [38][  180/  196]   Loss 0.465491   Top1 84.027778   Top5 98.656684   BatchTime 0.290673   LR 0.000110
0.65546751
0.65559864
0.65552348
0.65545821
0.65518379
0.65508264
0.65508884
0.65508157
0.65517181
0.65537351
0.65541321
0.65548515
0.65531087
0.65545261
0.65511525
0.65512407
INFO - ==> Top1: 84.014    Top5: 98.632    Loss: 0.465
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [38][   20/   40]   Loss 0.380451   Top1 87.246094   Top5 99.531250   BatchTime 0.133970
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.0859)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.1574)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1152)
features.4.conv.0 tensor(0.0446)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1466)
features.5.conv.0 tensor(0.2424)
features.5.conv.3 tensor(0.4201)
features.5.conv.6 tensor(0.1265)
features.6.conv.0 tensor(0.0467)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0787)
features.7.conv.0 tensor(0.1548)
features.7.conv.3 tensor(0.4450)
features.7.conv.6 tensor(0.5252)
features.8.conv.0 tensor(0.5282)
features.8.conv.3 tensor(0.5344)
features.8.conv.6 tensor(0.2004)
features.9.conv.0 tensor(0.3772)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.6247)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.0746)
features.11.conv.0 tensor(0.7557)
features.11.conv.3 tensor(0.6474)
features.11.conv.6 tensor(0.7712)
features.12.conv.0 tensor(0.6537)
features.12.conv.3 tensor(0.6705)
features.12.conv.6 tensor(0.7361)
features.13.conv.0 tensor(0.2450)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.0997)
features.14.conv.0 tensor(0.8950)
features.14.conv.3 tensor(0.8383)
features.14.conv.6 tensor(0.9504)
features.15.conv.0 tensor(0.8538)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9682)
features.16.conv.0 tensor(0.7143)
features.16.conv.3 tensor(0.7969)
features.16.conv.6 tensor(0.8383)
conv.0 tensor(0.0897)
tensor(1259801.) 2188896.0
INFO - Validation [38][   40/   40]   Loss 0.366503   Top1 87.770000   Top5 99.610000   BatchTime 0.096689
INFO - ==> Top1: 87.770    Top5: 99.610    Loss: 0.367
INFO - ==> Sparsity : 0.576
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
0.65518528
0.65528232
0.65535504
0.65537167
0.65557557
0.65559310
0.65547585
0.65550113
0.65532809
0.65520436
0.65531206
0.65543324
0.65550900
0.65552497
0.65540284
0.65565038
0.65555686
INFO - Training [39][   20/  196]   Loss 0.471327   Top1 83.339844   Top5 98.066406   BatchTime 0.344200   LR 0.000110
0.65544337
0.65542394
0.65545237
0.65524048
0.65557021
0.65551466
0.65555167
0.65573269
0.65563250
0.65560460
0.65538520
0.65530270
0.65562248
0.65553600
0.65563887
0.65575862
0.65574265
0.65543246
0.65543479
0.65537512
0.65532207
0.65530014
INFO - Training [39][   40/  196]   Loss 0.481741   Top1 83.251953   Top5 98.212891   BatchTime 0.306592   LR 0.000109
0.65543383
0.65553039
0.65535289
0.65527308
0.65553963
0.65575457
0.65559053
0.65577263
0.65587336
0.65573728
0.65562683
0.65554029
0.65556663
0.65518999
0.65530479
0.65511709
0.65513700
0.65509272
0.65509152
0.65496522
0.65431213
0.65413278
0.65404278
0.65429294
INFO - Training [39][   60/  196]   Loss 0.476542   Top1 83.404948   Top5 98.268229   BatchTime 0.288222   LR 0.000109
0.65457964
0.65437549
0.65424985
0.65421510
0.65422243
0.65411830
0.65398520
0.65403211
0.65400153
0.65403759
0.65398878
0.65410995
0.65409148
0.65399379
0.65396327
0.65389848
0.65393692
0.65399641
0.65419823
0.65388483
INFO - Training [39][   80/  196]   Loss 0.477146   Top1 83.457031   Top5 98.378906   BatchTime 0.290380   LR 0.000109
0.65374500
0.65364432
0.65356481
0.65371644
0.65377450
0.65375543
0.65354294
0.65358847
0.65363461
0.65368348
0.65345788
0.65344822
0.65336519
0.65328074
0.65306938
0.65288460
0.65307581
INFO - Training [39][  100/  196]   Loss 0.468336   Top1 83.777344   Top5 98.449219   BatchTime 0.300366   LR 0.000108
0.65315825
0.65321660
0.65316796
0.65338796
0.65346730
0.65344292
0.65342814
0.65312535
0.65293306
0.65284824
0.65279353
0.65285897
0.65285414
0.65273792
0.65267169
0.65257984
0.65255600
0.65245867
0.65246630
0.65249127
INFO - Training [39][  120/  196]   Loss 0.459045   Top1 84.095052   Top5 98.557943   BatchTime 0.302288   LR 0.000108
0.65259588
0.65280026
0.65297270
0.65301466
0.65330690
0.65369290
0.65342981
0.65336049
0.65336508
0.65342128
0.65311295
0.65330440
0.65315044
0.65302682
0.65308815
0.65323234
0.65309006
0.65310693
0.65318239
0.65304261
INFO - Training [39][  140/  196]   Loss 0.458670   Top1 84.093192   Top5 98.638393   BatchTime 0.301734   LR 0.000108
0.65286541
0.65301019
0.65326107
0.65500754
0.65531069
0.65539861
0.65532696
0.65567034
0.65546048
0.65535355
0.65525275
0.65501338
0.65488029
0.65491921
0.65498215
0.65500170
0.65521824
0.65562057
0.65561104
0.65565646
0.65563798
INFO - Training [39][  160/  196]   Loss 0.464058   Top1 83.925781   Top5 98.620605   BatchTime 0.298493   LR 0.000107
0.65550643
0.65541595
0.65545422
0.65546173
0.65552467
0.65550369
0.65548182
0.65566522
0.65575701
0.65585440
0.65559173
0.65533102
0.65530622
0.65520996
0.65509778
0.65506625
INFO - Training [39][  180/  196]   Loss 0.465145   Top1 83.901910   Top5 98.589410   BatchTime 0.293634   LR 0.000107
0.65503299
0.65523171
0.65518129
0.65510899
0.65503234
0.65500677
0.65531629
0.65536743
0.65534931
0.65535480
0.65542597
0.65545911
0.65534550
0.65505600
0.65508515
0.65514481
INFO - ==> Top1: 83.960    Top5: 98.616    Loss: 0.464
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.65539968
0.65526038
0.65518361
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [39][   20/   40]   Loss 0.359337   Top1 87.753906   Top5 99.609375   BatchTime 0.131247
features.0.conv.0 tensor(0.2674)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.0903)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.1591)
features.3.conv.0 tensor(0.0637)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1128)
features.4.conv.0 tensor(0.0444)
features.4.conv.3 tensor(0.3084)
features.4.conv.6 tensor(0.1460)
features.5.conv.0 tensor(0.2466)
features.5.conv.3 tensor(0.4242)
features.5.conv.6 tensor(0.1273)
features.6.conv.0 tensor(0.0461)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0808)
features.7.conv.0 tensor(0.1582)
features.7.conv.3 tensor(0.4453)
features.7.conv.6 tensor(0.5331)
features.8.conv.0 tensor(0.5096)
features.8.conv.3 tensor(0.5365)
features.8.conv.6 tensor(0.2000)
features.9.conv.0 tensor(0.3942)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.6289)
features.10.conv.0 tensor(0.0670)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.0747)
features.11.conv.0 tensor(0.7382)
features.11.conv.3 tensor(0.6460)
features.11.conv.6 tensor(0.7710)
features.12.conv.0 tensor(0.6549)
features.12.conv.3 tensor(0.6715)
features.12.conv.6 tensor(0.7361)
features.13.conv.0 tensor(0.2449)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.1001)
features.14.conv.0 tensor(0.8976)
features.14.conv.3 tensor(0.8375)
features.14.conv.6 tensor(0.9533)
features.15.conv.0 tensor(0.8558)
features.15.conv.3 tensor(0.8228)
features.15.conv.6 tensor(0.9679)
features.16.conv.0 tensor(0.7228)
features.16.conv.3 tensor(0.7972)
features.16.conv.6 tensor(0.8375)
conv.0 tensor(0.0899)
tensor(1261692.) 2188896.0
INFO - Validation [39][   40/   40]   Loss 0.347634   Top1 87.960000   Top5 99.700000   BatchTime 0.094008
INFO - ==> Top1: 87.960    Top5: 99.700    Loss: 0.348
INFO - ==> Sparsity : 0.576
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
0.65540463
0.65545231
0.65537477
0.65508783
0.65505129
0.65498918
0.65486372
0.65475357
0.65493280
0.65537733
0.65546948
0.65553695
0.65548813
0.65544879
0.65535158
0.65521860
0.65536445
0.65533197
0.65546674
0.65536678
0.65508217
INFO - Training [40][   20/  196]   Loss 0.494812   Top1 82.929688   Top5 97.851562   BatchTime 0.375861   LR 0.000106
0.65534669
0.65546936
0.65513378
0.65507978
0.65506512
0.65524763
0.65525049
0.65516615
0.65528262
0.65518963
0.65557200
0.65521479
0.65507555
0.65515047
0.65510637
0.65509182
0.65500849
INFO - Training [40][   40/  196]   Loss 0.488978   Top1 83.105469   Top5 98.222656   BatchTime 0.359141   LR 0.000106
0.65522993
0.65509629
0.65523934
0.65532881
0.65515679
0.65516800
0.65519971
0.65510124
0.65496379
0.65494984
0.65495926
0.65511006
0.65511185
0.65522879
0.65525538
0.65548187
0.65544522
0.65530968
0.65519410
0.65526855
INFO - Training [40][   60/  196]   Loss 0.477787   Top1 83.496094   Top5 98.326823   BatchTime 0.341580   LR 0.000106
0.65507984
0.65512490
0.65511501
0.65536481
0.65529668
0.65519559
0.65504813
0.65509307
0.65498847
0.65488905
0.65498728
0.65501308
0.65490025
0.65485215
0.65483844
0.65475339
0.65512222
0.65508705
0.65512162
0.65463412
0.65456635
0.65464658
INFO - Training [40][   80/  196]   Loss 0.472345   Top1 83.710938   Top5 98.457031   BatchTime 0.324785   LR 0.000105
0.65462303
0.65473753
0.65489823
0.65497273
0.65469557
0.65461099
0.65466130
0.65467024
0.65471250
0.65479815
0.65492195
0.65493518
0.65497196
0.65495515
0.65495998
0.65536344
0.65535200
0.65518659
0.65492654
0.65504038
0.65523565
0.65511119
INFO - Training [40][  100/  196]   Loss 0.468041   Top1 83.835938   Top5 98.531250   BatchTime 0.314532   LR 0.000105
0.65517044
0.65535897
0.65552801
0.65542752
0.65523595
0.65529996
0.65521318
0.65515685
0.65518528
0.65533119
0.65560490
0.65544367
0.65551472
0.65547127
0.65538907
INFO - Training [40][  120/  196]   Loss 0.461876   Top1 84.065755   Top5 98.597005   BatchTime 0.305997   LR 0.000105
0.65534645
0.65516561
0.65514958
0.65509099
0.65534317
0.65535486
0.65541303
0.65556931
0.65586650
0.65577936
0.65572834
0.65571862
0.65592068
0.65573841
0.65573621
0.65545803
0.65556324
0.65550649
0.65520847
0.65522462
0.65531987
0.65524912
0.65544134
INFO - Training [40][  140/  196]   Loss 0.457453   Top1 84.277344   Top5 98.632812   BatchTime 0.300144   LR 0.000104
0.65555912
0.65540296
0.65557122
0.65541810
0.65534729
0.65534025
0.65526736
0.65521353
0.65509140
0.65511924
0.65508705
0.65533805
0.65568745
0.65551364
0.65554321
0.65559644
0.65533954
0.65506911
0.65496480
0.65497202
0.65496558
INFO - Training [40][  160/  196]   Loss 0.461513   Top1 84.108887   Top5 98.630371   BatchTime 0.297622   LR 0.000104
0.65474439
0.65462059
0.65452248
0.65461308
0.65479445
0.65506494
0.65486807
0.65490597
0.65505332
0.65487599
0.65467870
0.65462583
0.65462649
0.65460652
0.65454257
0.65477473
0.65523404
0.65524089
0.65503275
0.65502053
INFO - Training [40][  180/  196]   Loss 0.459648   Top1 84.160156   Top5 98.602431   BatchTime 0.297853   LR 0.000103
0.65495312
0.65500653
0.65490538
0.65477675
0.65467757
0.65501690
0.65488780
0.65485913
0.65488726
0.65458941
0.65443230
0.65439087
0.65445554
0.65438932
0.65439302
INFO - ==> Top1: 84.222    Top5: 98.608    Loss: 0.458
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.359356   Top1 88.144531   Top5 99.589844   BatchTime 0.130382
features.0.conv.0 tensor(0.2674)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0951)
features.2.conv.0 tensor(0.0871)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.1551)
features.3.conv.0 tensor(0.0637)
features.3.conv.3 tensor(0.0810)
features.3.conv.6 tensor(0.1124)
features.4.conv.0 tensor(0.0430)
features.4.conv.3 tensor(0.3131)
features.4.conv.6 tensor(0.1486)
features.5.conv.0 tensor(0.2380)
features.5.conv.3 tensor(0.4242)
features.5.conv.6 tensor(0.1326)
features.6.conv.0 tensor(0.0433)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0816)
features.7.conv.0 tensor(0.1565)
features.7.conv.3 tensor(0.4447)
features.7.conv.6 tensor(0.5349)
features.8.conv.0 tensor(0.5216)
features.8.conv.3 tensor(0.5350)
features.8.conv.6 tensor(0.1989)
features.9.conv.0 tensor(0.3961)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.6323)
features.10.conv.0 tensor(0.0645)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0745)
features.11.conv.0 tensor(0.7432)
features.11.conv.3 tensor(0.6480)
features.11.conv.6 tensor(0.7717)
features.12.conv.0 tensor(0.6479)
features.12.conv.3 tensor(0.6707)
features.12.conv.6 tensor(0.7460)
features.13.conv.0 tensor(0.2461)
features.13.conv.3 tensor(0.4917)
features.13.conv.6 tensor(0.0999)
features.14.conv.0 tensor(0.8996)
features.14.conv.3 tensor(0.8377)
features.14.conv.6 tensor(0.9506)
features.15.conv.0 tensor(0.8540)
features.15.conv.3 tensor(0.8222)
features.15.conv.6 tensor(0.9679)
features.16.conv.0 tensor(0.7225)
features.16.conv.3 tensor(0.7971)
features.16.conv.6 tensor(0.8385)
conv.0 tensor(0.0902)
tensor(1262503.) 2188896.0
INFO - Validation [40][   40/   40]   Loss 0.350301   Top1 88.180000   Top5 99.670000   BatchTime 0.094054
INFO - ==> Top1: 88.180    Top5: 99.670    Loss: 0.350
INFO - ==> Sparsity : 0.577
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
0.65438163
0.65428257
0.65427661
0.65440357
0.65449601
0.65451771
0.65446228
0.65429604
0.65422082
0.65410346
0.65405488
0.65400112
0.65406412
0.65415746
0.65423608
0.65412086
0.65400624
0.65429014
0.65458590
0.65458733
0.65465569
INFO - Training [41][   20/  196]   Loss 0.475610   Top1 83.183594   Top5 98.300781   BatchTime 0.400092   LR 0.000103
0.65452331
0.65451622
0.65465885
0.65458333
0.65432733
0.65407103
0.65389854
0.65386218
0.65385741
0.65397829
0.65432864
0.65435547
0.65464044
0.65413845
0.65392435
0.65399480
0.65384316
0.65375674
0.65384436
0.65403724
0.65412456
INFO - Training [41][   40/  196]   Loss 0.466226   Top1 83.486328   Top5 98.359375   BatchTime 0.344934   LR 0.000102
0.65401274
0.65397757
0.65410227
0.65395886
0.65387732
0.65393275
0.65374291
0.65372866
0.65406471
0.65404099
0.65413958
0.65437764
0.65423608
0.65426952
0.65428859
0.65420502
0.65419394
0.65441203
0.65419835
0.65461403
0.65445101
INFO - Training [41][   60/  196]   Loss 0.462737   Top1 83.691406   Top5 98.457031   BatchTime 0.323744   LR 0.000102
0.65467304
0.65451646
0.65432346
0.65400273
0.65388960
0.65388119
0.65401632
0.65432811
0.65470356
0.65452266
0.65462834
0.65465873
0.65465701
0.65449029
0.65471154
0.65461177
INFO - Training [41][   80/  196]   Loss 0.463990   Top1 83.681641   Top5 98.598633   BatchTime 0.308097   LR 0.000102
0.65451843
0.65439105
0.65434062
0.65436411
0.65423828
0.65436280
0.65437478
0.65429264
0.65433228
0.65459180
0.65457815
0.65474266
0.65446842
0.65442097
0.65442431
0.65423113
0.65406704
0.65421146
0.65417534
0.65403408
0.65394378
0.65377367
INFO - Training [41][  100/  196]   Loss 0.456825   Top1 84.000000   Top5 98.625000   BatchTime 0.300844   LR 0.000101
0.65372705
0.65380043
0.65402007
0.65416324
0.65410972
0.65437353
0.65430170
0.65432888
0.65440428
0.65416682
0.65380889
0.65372539
0.65368342
0.65375286
0.65379041
0.65400457
0.65441006
0.65456384
0.65452498
0.65412831
0.65388590
INFO - Training [41][  120/  196]   Loss 0.447540   Top1 84.381510   Top5 98.684896   BatchTime 0.296360   LR 0.000101
0.65370202
0.65352213
0.65356076
0.65344542
0.65343416
0.65366006
0.65395886
0.65413308
0.65403289
0.65395695
0.65398562
0.65397531
0.65377343
0.65361005
0.65349513
0.65354919
0.65368640
0.65387887
0.65410930
INFO - Training [41][  140/  196]   Loss 0.443997   Top1 84.573103   Top5 98.750000   BatchTime 0.300222   LR 0.000100
0.65415955
0.65422803
0.65386891
0.65375739
0.65366346
0.65382057
0.65383339
0.65358061
0.65364599
0.65377414
0.65383095
0.65375519
0.65380061
0.65403974
0.65394473
0.65415305
INFO - Training [41][  160/  196]   Loss 0.447084   Top1 84.445801   Top5 98.720703   BatchTime 0.295317   LR 0.000100
0.65428585
0.65441459
0.65445995
0.65430409
0.65407753
0.65397662
0.65374231
0.65381747
0.65381497
0.65364468
0.65350080
0.65349793
0.65352231
0.65363503
0.65354896
0.65340728
0.65337247
0.65329415
0.65322495
0.65328813
0.65361822
INFO - Training [41][  180/  196]   Loss 0.448153   Top1 84.405382   Top5 98.702257   BatchTime 0.293860   LR 0.000100
0.65370327
0.65370417
0.65373218
0.65369993
0.65388489
0.65382856
0.65390766
0.65391415
0.65377808
0.65393496
0.65400392
0.65410477
0.65439242
0.65405434
0.65414619
0.65401316
INFO - ==> Top1: 84.398    Top5: 98.700    Loss: 0.448
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.65402311
0.65420985
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [41][   20/   40]   Loss 0.358541   Top1 88.007812   Top5 99.492188   BatchTime 0.132972
INFO - Validation [41][   40/   40]   Loss 0.349955   Top1 88.190000   Top5 99.620000   BatchTime 0.093041
INFO - ==> Top1: 88.190    Top5: 99.620    Loss: 0.350
INFO - ==> Sparsity : 0.578
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0955)
features.2.conv.0 tensor(0.0761)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1565)
features.3.conv.0 tensor(0.0602)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1105)
features.4.conv.0 tensor(0.0425)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1473)
features.5.conv.0 tensor(0.2448)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.1305)
features.6.conv.0 tensor(0.0422)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0830)
features.7.conv.0 tensor(0.1643)
features.7.conv.3 tensor(0.4433)
features.7.conv.6 tensor(0.5347)
features.8.conv.0 tensor(0.5140)
features.8.conv.3 tensor(0.5327)
features.8.conv.6 tensor(0.2013)
features.9.conv.0 tensor(0.3769)
features.9.conv.3 tensor(0.5593)
features.9.conv.6 tensor(0.6309)
features.10.conv.0 tensor(0.0633)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0745)
features.11.conv.0 tensor(0.7446)
features.11.conv.3 tensor(0.6470)
features.11.conv.6 tensor(0.7740)
features.12.conv.0 tensor(0.6638)
features.12.conv.3 tensor(0.6709)
features.12.conv.6 tensor(0.7521)
features.13.conv.0 tensor(0.2457)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.0993)
features.14.conv.0 tensor(0.9005)
features.14.conv.3 tensor(0.8375)
features.14.conv.6 tensor(0.9513)
features.15.conv.0 tensor(0.8545)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7247)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8416)
conv.0 tensor(0.0896)
tensor(1264681.) 2188896.0
0.65433216
0.65412498
0.65438777
0.65413976
0.65405518
0.65401745
0.65405643
0.65395570
0.65400720
0.65393394
0.65408021
0.65426433
0.65407228
0.65402526
0.65391630
0.65365463
0.65345496
0.65344745
0.65342897
0.65350473
INFO - Training [42][   20/  196]   Loss 0.467935   Top1 83.671875   Top5 98.046875   BatchTime 0.376773   LR 0.000099
0.65355963
0.65385807
0.65399605
0.65403223
0.65403807
0.65398657
0.65415943
0.65426499
0.65440720
0.65404218
0.65386534
0.65375811
0.65381140
0.65407866
0.65369457
0.65376085
0.65363359
0.65321815
0.65282136
0.65265387
0.65265244
INFO - Training [42][   40/  196]   Loss 0.458585   Top1 84.199219   Top5 98.310547   BatchTime 0.381677   LR 0.000098
0.65272164
0.65272087
0.65291172
0.65269953
0.65265697
0.65267056
0.65290159
0.65307182
0.65307635
0.65321052
0.65304112
0.65299773
0.65293115
0.65261000
0.65258825
0.65266252
0.65269345
INFO - Training [42][   60/  196]   Loss 0.459160   Top1 84.010417   Top5 98.417969   BatchTime 0.374780   LR 0.000098
0.65275675
0.65252733
0.65260005
0.65256488
0.65254074
0.65255761
0.65244597
0.65244275
0.65211540
0.65211242
0.65226078
0.65220642
0.65215403
0.65198302
0.65193903
0.65212363
0.65241867
0.65237969
0.65249234
0.65264302
0.65250373
0.65235239
INFO - Training [42][   80/  196]   Loss 0.456452   Top1 84.233398   Top5 98.554688   BatchTime 0.369488   LR 0.000098
0.65229934
0.65204048
0.65202546
0.65211624
0.65239871
0.65220165
0.65218520
0.65232837
0.65225488
0.65239096
0.65368527
0.65381944
0.65381199
0.65383440
0.65377516
0.65361911
0.65368134
0.65381902
0.65391642
0.65385717
0.65373266
0.65378708
INFO - Training [42][  100/  196]   Loss 0.448778   Top1 84.539062   Top5 98.640625   BatchTime 0.369436   LR 0.000097
0.65383130
0.65389520
0.65383577
0.65387636
0.65340489
0.65327924
0.65325803
0.65285265
0.65247387
0.65228558
0.65194690
0.65172970
0.65189606
0.65206897
0.65175426
0.65124923
INFO - Training [42][  120/  196]   Loss 0.443252   Top1 84.765625   Top5 98.717448   BatchTime 0.371001   LR 0.000097
0.65110421
0.65105760
0.65117162
0.65107208
0.65099102
0.65103632
0.65116751
0.65101397
0.65123999
0.65107560
0.65102619
0.65111595
0.65107179
0.65107054
0.65113425
0.65095395
0.65047258
0.65041393
0.65056938
0.65061885
INFO - Training [42][  140/  196]   Loss 0.442750   Top1 84.835379   Top5 98.763951   BatchTime 0.373795   LR 0.000096
0.65036923
0.65049845
0.65071142
0.65070730
0.65035576
0.65016931
0.65008003
0.65003371
0.65009016
0.65156847
0.65172207
0.65152919
0.65123671
0.65115446
0.65116113
0.65099537
0.65065879
0.65068954
0.65064585
0.65045220
0.64984876
0.64978474
0.64965308
INFO - Training [42][  160/  196]   Loss 0.445654   Top1 84.772949   Top5 98.750000   BatchTime 0.371445   LR 0.000096
0.64929205
0.64906335
0.64892358
0.64905173
0.64869958
0.64838737
0.64838672
0.64855283
0.64857239
0.64839923
0.64844596
0.64843398
0.64818370
0.64813393
0.64794785
0.64802945
0.64779067
0.64767659
0.64757097
0.64759994
0.64751476
INFO - Training [42][  180/  196]   Loss 0.445775   Top1 84.730903   Top5 98.743490   BatchTime 0.371658   LR 0.000096
0.64758998
0.64728081
0.64718556
0.64706075
0.64691901
0.64694202
0.64680141
0.64660698
0.64623833
0.64618301
0.64567590
0.64498794
INFO - ==> Top1: 84.712    Top5: 98.734    Loss: 0.446
0.64457673
0.64424473
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [42][   20/   40]   Loss 0.357429   Top1 87.988281   Top5 99.550781   BatchTime 0.133440
INFO - Validation [42][   40/   40]   Loss 0.340691   Top1 88.510000   Top5 99.690000   BatchTime 0.093673
INFO - ==> Top1: 88.510    Top5: 99.690    Loss: 0.341
INFO - ==> Sparsity : 0.580
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.550   Top5: 99.670]
features.0.conv.0 tensor(0.2708)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0436)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0951)
features.2.conv.0 tensor(0.0874)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.1560)
features.3.conv.0 tensor(0.0556)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1141)
features.4.conv.0 tensor(0.0444)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1491)
features.5.conv.0 tensor(0.2440)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.1270)
features.6.conv.0 tensor(0.0470)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0815)
features.7.conv.0 tensor(0.1632)
features.7.conv.3 tensor(0.4444)
features.7.conv.6 tensor(0.5357)
features.8.conv.0 tensor(0.5227)
features.8.conv.3 tensor(0.5341)
features.8.conv.6 tensor(0.3551)
features.9.conv.0 tensor(0.3833)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.6371)
features.10.conv.0 tensor(0.0631)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0761)
features.11.conv.0 tensor(0.7542)
features.11.conv.3 tensor(0.6456)
features.11.conv.6 tensor(0.7754)
features.12.conv.0 tensor(0.6605)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.7497)
features.13.conv.0 tensor(0.2449)
features.13.conv.3 tensor(0.4921)
features.13.conv.6 tensor(0.1003)
features.14.conv.0 tensor(0.8990)
features.14.conv.3 tensor(0.8374)
features.14.conv.6 tensor(0.9493)
features.15.conv.0 tensor(0.8568)
features.15.conv.3 tensor(0.8225)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7172)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8430)
conv.0 tensor(0.0897)
tensor(1268591.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
0.64382571
0.64349228
0.64329684
0.64303946
0.64293247
0.64281940
0.64265215
0.64264846
0.64225376
0.64197052
0.64195108
0.64204597
0.64217532
0.64224660
0.64185059
0.64170724
0.64159095
0.64161491
0.64148104
0.64150572
0.64122808
0.64133298
INFO - Training [43][   20/  196]   Loss 0.456173   Top1 83.984375   Top5 98.359375   BatchTime 0.429185   LR 0.000095
0.64120215
0.64106297
0.64113259
0.64129442
0.64137828
0.64112860
0.64097959
0.64094114
0.64087498
0.64078289
0.64059079
0.64049608
0.64049023
0.64076751
0.64085591
0.64097542
0.64066941
0.64067829
INFO - Training [43][   40/  196]   Loss 0.453061   Top1 83.964844   Top5 98.515625   BatchTime 0.377410   LR 0.000094
0.64061087
0.64047807
0.64045423
0.64040667
0.64040679
0.64063364
0.64076447
0.64069444
0.64082128
0.64076662
0.64083713
0.64083046
0.64089227
0.64067847
0.64056963
0.64054644
0.64049631
0.64042795
0.64046800
INFO - Training [43][   60/  196]   Loss 0.449837   Top1 84.153646   Top5 98.509115   BatchTime 0.360491   LR 0.000094
0.64044589
0.64053321
0.64087242
0.64068520
0.64079356
0.64097869
0.64109522
0.64118534
0.64085138
0.64074922
0.64057320
0.64054948
0.64066035
0.64055359
0.64061207
0.64048868
0.64055747
0.64110196
0.64089739
0.64082515
0.64090765
0.64058352
INFO - Training [43][   80/  196]   Loss 0.450219   Top1 84.155273   Top5 98.608398   BatchTime 0.360703   LR 0.000093
0.64038736
0.64041865
0.64044744
0.64069664
0.64077824
0.64061683
0.64065766
0.64089787
0.64054799
0.64055967
0.64042431
0.64036179
0.64060742
0.64066744
0.64090413
0.64087492
0.64043391
INFO - Training [43][  100/  196]   Loss 0.442770   Top1 84.425781   Top5 98.628906   BatchTime 0.359182   LR 0.000093
0.64020967
0.64014369
0.64010900
0.63992280
0.63989311
0.63992226
0.63995761
0.64020115
0.64037526
0.64023650
0.64008498
0.64033026
0.64057761
0.64053792
0.64045471
0.64077836
0.64066178
0.64077377
0.64066029
0.64055085
INFO - Training [43][  120/  196]   Loss 0.436673   Top1 84.749349   Top5 98.727214   BatchTime 0.365370   LR 0.000093
0.64048624
0.64051586
0.64071041
0.64091432
0.64101344
0.64101565
0.64089799
0.64079344
0.64060616
0.64047277
0.64035887
0.64034128
0.64044416
0.64038926
0.64056820
0.64035070
0.64058405
0.64064431
0.64042777
0.64030343
INFO - Training [43][  140/  196]   Loss 0.438864   Top1 84.715402   Top5 98.769531   BatchTime 0.371556   LR 0.000092
0.64019638
0.64014435
0.64026868
0.64033163
0.64031804
0.64028651
0.64050901
0.64047670
0.64040041
0.64039665
0.64057213
0.64071918
0.64068055
0.64056307
0.64070547
0.64017111
0.64002764
0.64032602
0.64030337
0.64036232
0.64025348
0.64019990
0.64006042
0.63991708
0.63987696
INFO - Training [43][  160/  196]   Loss 0.444165   Top1 84.533691   Top5 98.742676   BatchTime 0.375707   LR 0.000092
0.63997823
0.63995993
0.63973838
0.64011562
0.64025658
0.64029926
0.64030838
0.64037877
0.64026392
0.63998348
0.63987488
0.63986892
0.63959819
0.63962656
0.63972288
0.63983738
INFO - Training [43][  180/  196]   Loss 0.445756   Top1 84.507378   Top5 98.710938   BatchTime 0.374215   LR 0.000091
0.63982815
0.63996100
0.63972610
0.63949573
0.63935578
0.63935673
0.63931865
0.63943714
0.63949060
0.63955307
0.63952720
0.63944876
0.63949662
0.63936526
0.63950479
INFO - ==> Top1: 84.616    Top5: 98.706    Loss: 0.443
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.63954419
0.63960767
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [43][   20/   40]   Loss 0.346244   Top1 88.359375   Top5 99.667969   BatchTime 0.149868
INFO - Validation [43][   40/   40]   Loss 0.333104   Top1 88.770000   Top5 99.710000   BatchTime 0.105912
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.0816)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.1589)
features.3.conv.0 tensor(0.0570)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1148)
features.4.conv.0 tensor(0.0470)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1504)
features.5.conv.0 tensor(0.2477)
features.5.conv.3 tensor(0.4213)
features.5.conv.6 tensor(0.1305)
features.6.conv.0 tensor(0.0482)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0807)
features.7.conv.0 tensor(0.1556)
features.7.conv.3 tensor(0.4427)
features.7.conv.6 tensor(0.5411)
features.8.conv.0 tensor(0.5183)
features.8.conv.3 tensor(0.5344)
features.8.conv.6 tensor(0.4101)
features.9.conv.0 tensor(0.3931)
features.9.conv.3 tensor(0.5576)
features.9.conv.6 tensor(0.6384)
features.10.conv.0 tensor(0.0648)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0750)
features.11.conv.0 tensor(0.7541)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.7814)
features.12.conv.0 tensor(0.6663)
features.12.conv.3 tensor(0.6719)
features.12.conv.6 tensor(0.7628)
features.13.conv.0 tensor(0.2451)
features.13.conv.3 tensor(0.4907)
features.13.conv.6 tensor(0.0998)
features.14.conv.0 tensor(0.8992)
features.14.conv.3 tensor(0.8360)
features.14.conv.6 tensor(0.9507)
features.15.conv.0 tensor(0.8584)
features.15.conv.3 tensor(0.8215)
features.15.conv.6 tensor(0.9678)
features.16.conv.0 tensor(0.7209)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.8459)
conv.0 tensor(0.0892)
tensor(1273142.) 2188896.0
INFO - ==> Top1: 88.770    Top5: 99.710    Loss: 0.333
INFO - ==> Sparsity : 0.582
INFO - Scoreboard best 1 ==> Epoch [43][Top1: 88.770   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.550   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
0.63977009
0.63983542
0.63995135
0.63982213
0.63977277
0.63965565
0.63978976
0.63952792
0.63941038
0.63957363
0.63961083
0.63962156
0.63967335
0.63990146
0.63985491
0.63980573
0.63973111
0.63979870
0.63972616
0.63962895
0.63982826
0.63990146
INFO - Training [44][   20/  196]   Loss 0.446875   Top1 84.570312   Top5 98.261719   BatchTime 0.381193   LR 0.000090
0.63994664
0.64005154
0.63995200
0.64012223
0.63995516
0.64013392
0.64024454
0.64005440
0.63994884
0.63993442
0.63998246
0.64005524
0.64031827
0.64040947
0.64034688
0.64000916
0.63985771
0.63989741
0.63995779
0.63995129
INFO - Training [44][   40/  196]   Loss 0.451148   Top1 84.150391   Top5 98.447266   BatchTime 0.328181   LR 0.000090
0.63962162
0.64005357
0.64017922
0.63992512
0.63977689
0.63964576
0.63963372
0.63961005
0.63948160
0.63926822
0.63935393
0.63968766
0.63983423
0.64000893
0.63984734
0.63959932
INFO - Training [44][   60/  196]   Loss 0.447665   Top1 84.303385   Top5 98.496094   BatchTime 0.343818   LR 0.000090
0.63942343
0.63920957
0.63926673
0.63921177
0.63930798
0.63929695
0.63932776
0.63937342
0.63936937
0.63942134
0.63928145
0.63895774
0.63918942
0.63932949
0.63935226
0.63930959
0.63935471
0.63965738
0.63960522
0.63950783
0.63965064
INFO - Training [44][   80/  196]   Loss 0.441882   Top1 84.482422   Top5 98.637695   BatchTime 0.356622   LR 0.000089
0.63969958
0.63973022
0.64002192
0.63999838
0.63973564
0.63949639
0.63964474
0.63958180
0.63957936
0.63959396
0.63967246
0.63953656
0.63946366
0.63929743
0.63926977
0.63914293
0.63900542
0.63889033
0.63897830
0.63917738
0.63929260
0.63926095
INFO - Training [44][  100/  196]   Loss 0.438949   Top1 84.503906   Top5 98.679688   BatchTime 0.357107   LR 0.000089
0.63911575
0.63908243
0.63914222
0.63929814
0.63938975
0.63923109
0.63941324
0.63956362
0.63977033
0.63979965
0.63967681
0.63954097
0.63959491
0.63937491
0.63944817
0.63918602
0.63928908
INFO - Training [44][  120/  196]   Loss 0.435777   Top1 84.707031   Top5 98.727214   BatchTime 0.357549   LR 0.000088
0.63930064
0.63953626
0.63933539
0.63925171
0.63943875
0.63954872
0.63935190
0.63904136
0.63909024
0.63930124
0.63939595
0.63918823
0.63905883
0.63871288
0.63811785
0.63818604
0.63803709
0.63805681
0.63809431
0.63824040
0.63850582
0.63841593
INFO - Training [44][  140/  196]   Loss 0.433692   Top1 84.785156   Top5 98.791853   BatchTime 0.358025   LR 0.000088
0.63854164
0.63838500
0.63809448
0.63818318
0.63810825
0.63825601
0.63823140
0.63821310
0.63828725
0.63837940
0.63839024
0.63838673
0.63824475
0.63833416
0.63838208
0.63815933
0.63801515
0.63791019
0.63809371
0.63803315
0.63795525
INFO - Training [44][  160/  196]   Loss 0.436854   Top1 84.697266   Top5 98.754883   BatchTime 0.360136   LR 0.000087
0.63802767
0.63802177
0.63823980
0.63784134
0.63767964
0.63791209
0.63777572
0.63756377
0.63748056
0.63745153
0.63760835
0.63750327
0.63741368
0.63763493
0.63760972
0.63769007
0.63742369
0.63741678
0.63740778
0.63752162
0.63749516
0.63749844
INFO - Training [44][  180/  196]   Loss 0.437560   Top1 84.639757   Top5 98.697917   BatchTime 0.361172   LR 0.000087
0.63760781
0.63746619
0.63756728
0.63774133
0.63742423
0.63737768
0.63729352
0.63724381
0.63721734
0.63715321
0.63749343
0.63758898
0.63865727
********************pre-trained*****************
INFO - ==> Top1: 84.712    Top5: 98.716    Loss: 0.436
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [44][   20/   40]   Loss 0.347452   Top1 88.437500   Top5 99.550781   BatchTime 0.159925
INFO - Validation [44][   40/   40]   Loss 0.337947   Top1 88.720000   Top5 99.730000   BatchTime 0.106938
INFO - ==> Top1: 88.720    Top5: 99.730    Loss: 0.338
INFO - ==> Sparsity : 0.583
INFO - Scoreboard best 1 ==> Epoch [43][Top1: 88.770   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 88.720   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 88.680   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0885)
features.2.conv.0 tensor(0.0761)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.1591)
features.3.conv.0 tensor(0.0509)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1105)
features.4.conv.0 tensor(0.0529)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1489)
features.5.conv.0 tensor(0.2484)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.1320)
features.6.conv.0 tensor(0.0397)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0824)
features.7.conv.0 tensor(0.1630)
features.7.conv.3 tensor(0.4436)
features.7.conv.6 tensor(0.5449)
features.8.conv.0 tensor(0.5255)
features.8.conv.3 tensor(0.5365)
features.8.conv.6 tensor(0.4394)
features.9.conv.0 tensor(0.3908)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.6403)
features.10.conv.0 tensor(0.0668)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0769)
features.11.conv.0 tensor(0.7534)
features.11.conv.3 tensor(0.6456)
features.11.conv.6 tensor(0.7765)
features.12.conv.0 tensor(0.6485)
features.12.conv.3 tensor(0.6721)
features.12.conv.6 tensor(0.7696)
features.13.conv.0 tensor(0.2448)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.1001)
features.14.conv.0 tensor(0.8994)
features.14.conv.3 tensor(0.8358)
features.14.conv.6 tensor(0.9511)
features.15.conv.0 tensor(0.8589)
features.15.conv.3 tensor(0.8222)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7313)
features.16.conv.3 tensor(0.7979)
features.16.conv.6 tensor(0.8465)
conv.0 tensor(0.0894)
tensor(1275491.) 2188896.0
0.63887328
0.63870889
0.63864785
0.63864034
0.63868135
0.63879037
0.63885576
0.63866997
0.63848823
0.63831073
0.63820302
0.63822657
0.63819158
0.63818216
0.63817710
0.63859230
0.63863522
0.63838464
0.63828236
0.63839924
0.63848358
0.63830400
INFO - Training [45][   20/  196]   Loss 0.442299   Top1 84.140625   Top5 98.066406   BatchTime 0.408398   LR 0.000086
0.63830996
0.63835686
0.63848299
0.63836938
0.63843876
0.63835537
0.63847578
0.63862139
0.63851333
0.63852918
0.63846803
0.63823754
0.63849229
0.63829446
INFO - Training [45][   40/  196]   Loss 0.452107   Top1 83.984375   Top5 98.310547   BatchTime 0.348435   LR 0.000086
0.63825685
0.63842922
0.63819271
0.63836747
0.63843185
0.63857371
0.63857549
0.63832414
0.63818556
0.63806200
0.63788909
0.63785166
0.63784248
0.63781315
0.63782871
0.63769364
0.63784313
0.63800782
0.63794190
0.63793087
0.63790148
0.63777095
0.63766599
0.63762248
INFO - Training [45][   60/  196]   Loss 0.444517   Top1 84.322917   Top5 98.444010   BatchTime 0.344629   LR 0.000085
0.63741183
0.63742596
0.63754362
0.63778687
0.63765031
0.63778013
0.63800222
0.63791722
0.63750988
0.63729489
0.63728118
0.63717234
0.63729054
0.63726509
0.63729799
0.63735658
0.63745910
INFO - Training [45][   80/  196]   Loss 0.438580   Top1 84.638672   Top5 98.574219   BatchTime 0.346051   LR 0.000085
0.63765520
0.63751751
0.63736266
0.63709730
0.63719314
0.63725919
0.63727838
0.63719571
0.63704902
0.63707834
0.63721514
0.63738936
0.63735527
0.63757712
0.63759106
0.63762701
0.63763201
0.63753945
0.63744044
0.63741434
0.63722384
0.63712913
0.63717347
0.63734525
INFO - Training [45][  100/  196]   Loss 0.432487   Top1 84.910156   Top5 98.621094   BatchTime 0.345145   LR 0.000084
0.63713753
0.63702828
0.63682604
0.63691950
0.63701296
0.63706297
0.63687426
0.63660657
0.63658309
0.63655031
0.63660991
0.63662159
0.63665527
0.63694894
0.63693863
0.63686049
INFO - Training [45][  120/  196]   Loss 0.426909   Top1 85.113932   Top5 98.678385   BatchTime 0.348636   LR 0.000084
0.63664275
0.63662463
0.63666183
0.63656479
0.63656849
0.63674039
0.63685364
0.63668317
0.63653404
0.63657022
0.63683271
0.63681591
0.63668555
0.63660806
0.63654119
0.63669026
0.63675255
0.63647556
0.63660228
0.63667399
0.63625431
INFO - Training [45][  140/  196]   Loss 0.427075   Top1 85.080915   Top5 98.713728   BatchTime 0.351972   LR 0.000083
0.63578415
0.63538021
0.63529259
0.63504130
0.63468832
0.63472861
0.63479930
0.63459194
0.63441515
0.63425976
0.63413292
0.63401705
0.63391978
0.63382494
0.63375103
0.63368702
0.63375133
0.63361359
0.63372320
0.63379556
0.63371068
0.63362378
0.63359410
INFO - Training [45][  160/  196]   Loss 0.428395   Top1 85.046387   Top5 98.698730   BatchTime 0.353452   LR 0.000083
0.63350332
0.63334554
0.63335025
0.63336235
0.63337183
0.63349396
0.63414812
0.63434190
0.63452655
0.63467556
0.63472772
0.63480723
0.63478255
0.63458878
0.63453239
0.63456678
INFO - Training [45][  180/  196]   Loss 0.426271   Top1 85.117188   Top5 98.708767   BatchTime 0.354170   LR 0.000082
0.63467968
0.63462949
0.63490081
0.63526845
0.63559747
0.63607621
0.63661802
0.63674986
0.63645726
0.63640463
0.63634866
0.63634175
0.63621479
0.63615263
0.63627738
0.63604075
0.63605845
INFO - ==> Top1: 85.092    Top5: 98.708    Loss: 0.426
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.63622004
0.63637388
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.342279   Top1 88.378906   Top5 99.570312   BatchTime 0.197743
INFO - Validation [45][   40/   40]   Loss 0.330393   Top1 88.820000   Top5 99.700000   BatchTime 0.146993
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.0897)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1557)
features.3.conv.0 tensor(0.0570)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1113)
features.4.conv.0 tensor(0.0553)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1509)
features.5.conv.0 tensor(0.2590)
features.5.conv.3 tensor(0.4219)
features.5.conv.6 tensor(0.1278)
features.6.conv.0 tensor(0.0438)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0817)
features.7.conv.0 tensor(0.1611)
features.7.conv.3 tensor(0.4430)
features.7.conv.6 tensor(0.5541)
features.8.conv.0 tensor(0.5261)
features.8.conv.3 tensor(0.5333)
features.8.conv.6 tensor(0.4707)
features.9.conv.0 tensor(0.4041)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6456)
features.10.conv.0 tensor(0.0606)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0780)
features.11.conv.0 tensor(0.7523)
features.11.conv.3 tensor(0.6441)
features.11.conv.6 tensor(0.7859)
features.12.conv.0 tensor(0.6493)
features.12.conv.3 tensor(0.6709)
features.12.conv.6 tensor(0.7733)
features.13.conv.0 tensor(0.2444)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.1005)
features.14.conv.0 tensor(0.8995)
features.14.conv.3 tensor(0.8356)
features.14.conv.6 tensor(0.9507)
features.15.conv.0 tensor(0.8632)
features.15.conv.3 tensor(0.8221)
features.15.conv.6 tensor(0.9681)
features.16.conv.0 tensor(0.7288)
features.16.conv.3 tensor(0.7975)
features.16.conv.6 tensor(0.8518)
conv.0 tensor(0.0894)
tensor(1279514.) 2188896.0
INFO - ==> Top1: 88.820    Top5: 99.700    Loss: 0.330
INFO - ==> Sparsity : 0.585
INFO - Scoreboard best 1 ==> Epoch [45][Top1: 88.820   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [43][Top1: 88.770   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 88.720   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
0.63624102
0.63613188
0.63587123
0.63563466
0.63548076
0.63526064
0.63542068
0.63555628
0.63556767
0.63533729
0.63506931
0.63495302
0.63509125
0.63494438
0.63493967
0.63508910
INFO - Training [46][   20/  196]   Loss 0.448980   Top1 84.707031   Top5 98.183594   BatchTime 0.411431   LR 0.000081
0.63472366
0.63445133
0.63443619
0.63441521
0.63417757
0.63416207
0.63395005
0.63386399
0.63396180
0.63410687
0.63398874
0.63384032
0.63364863
0.63370049
0.63367707
0.63348949
0.63347495
0.63355798
0.63351452
0.63335609
0.63355917
INFO - Training [46][   40/  196]   Loss 0.443565   Top1 84.707031   Top5 98.349609   BatchTime 0.359624   LR 0.000081
0.63365334
0.63399273
0.63450652
0.63467300
0.63461185
0.63444221
0.63440973
0.63442153
0.63449484
0.63462424
0.63484627
0.63453639
0.63448268
0.63444954
0.63447964
0.63440251
0.63421679
0.63410693
0.63414240
0.63418561
0.63450980
INFO - Training [46][   60/  196]   Loss 0.440080   Top1 84.902344   Top5 98.444010   BatchTime 0.361093   LR 0.000080
0.63447165
0.63441330
0.63475817
0.63466871
0.63451368
0.63441050
0.63441062
0.63448870
0.63468248
0.63455075
0.63467002
0.63459182
0.63440078
0.63437992
0.63421476
0.63431317
0.63436311
0.63435286
0.63420534
0.63428247
0.63458389
0.63441592
INFO - Training [46][   80/  196]   Loss 0.433962   Top1 85.068359   Top5 98.627930   BatchTime 0.361543   LR 0.000080
0.63405681
0.63376099
0.63375157
0.63369292
0.63344157
0.63328785
0.63335532
0.63348353
0.63349879
0.63330126
0.63369167
0.63362652
0.63360775
0.63352805
0.63329560
0.63326204
0.63312006
INFO - Training [46][  100/  196]   Loss 0.431261   Top1 85.207031   Top5 98.585938   BatchTime 0.361931   LR 0.000079
0.63311040
0.63323170
0.63344240
0.63372475
0.63411146
0.63479596
0.63646692
0.63626546
0.63622743
0.63604414
0.63600004
0.63589448
0.63607138
0.63595849
0.63584489
0.63571215
0.63603932
0.63613003
0.63597244
0.63593513
0.63603526
0.63631898
INFO - Training [46][  120/  196]   Loss 0.426353   Top1 85.270182   Top5 98.723958   BatchTime 0.362801   LR 0.000079
0.63617831
0.63610280
0.63635027
0.63636792
0.63659960
0.63650972
0.63637811
0.63608181
0.63603646
0.63604951
0.63600922
0.63588136
0.63622713
0.63616425
0.63622826
0.63625091
0.63621533
0.63604200
0.63581330
0.63594818
0.63594586
0.63579011
INFO - Training [46][  140/  196]   Loss 0.426053   Top1 85.284598   Top5 98.775112   BatchTime 0.362241   LR 0.000078
0.63574398
0.63578552
0.63607329
0.63608205
0.63632280
0.63623810
0.63603258
0.63615936
0.63623101
0.63605267
0.63598615
0.63608605
0.63599652
0.63595891
0.63589180
0.63564909
INFO - Training [46][  160/  196]   Loss 0.428266   Top1 85.234375   Top5 98.793945   BatchTime 0.362803   LR 0.000078
0.63569784
0.63586509
0.63591790
0.63574243
0.63570738
0.63579804
0.63589400
0.63579357
0.63562876
0.63544053
0.63551420
0.63550371
0.63533264
0.63541770
0.63569641
0.63567400
0.63547391
0.63566083
0.63569337
0.63543701
0.63528407
0.63533646
INFO - Training [46][  180/  196]   Loss 0.427196   Top1 85.206163   Top5 98.763021   BatchTime 0.364343   LR 0.000077
0.63528126
0.63507909
0.63500071
0.63509774
0.63536298
0.63557410
0.63552958
0.63548189
0.63528037
0.63519633
0.63519818
0.63514137
0.63510591
0.63508350
0.63530570
0.63556045
INFO - ==> Top1: 85.200    Top5: 98.760    Loss: 0.427
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.63575399
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [46][   20/   40]   Loss 0.336323   Top1 88.691406   Top5 99.707031   BatchTime 0.175657
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1621)
features.1.conv.0 tensor(0.0378)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0890)
features.2.conv.0 tensor(0.0932)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1568)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0826)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0539)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1483)
features.5.conv.0 tensor(0.2534)
features.5.conv.3 tensor(0.4225)
features.5.conv.6 tensor(0.1266)
features.6.conv.0 tensor(0.0409)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0837)
features.7.conv.0 tensor(0.1607)
features.7.conv.3 tensor(0.4416)
features.7.conv.6 tensor(0.5522)
features.8.conv.0 tensor(0.5243)
features.8.conv.3 tensor(0.5339)
features.8.conv.6 tensor(0.4816)
features.9.conv.0 tensor(0.4451)
features.9.conv.3 tensor(0.5564)
features.9.conv.6 tensor(0.6476)
features.10.conv.0 tensor(0.0616)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0793)
features.11.conv.0 tensor(0.7480)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.7800)
features.12.conv.0 tensor(0.6512)
features.12.conv.3 tensor(0.6703)
features.12.conv.6 tensor(0.7764)
features.13.conv.0 tensor(0.2442)
features.13.conv.3 tensor(0.4907)
features.13.conv.6 tensor(0.1055)
features.14.conv.0 tensor(0.9019)
features.14.conv.3 tensor(0.8365)
features.14.conv.6 tensor(0.9531)
features.15.conv.0 tensor(0.8642)
features.15.conv.3 tensor(0.8227)
features.15.conv.6 tensor(0.9681)
features.16.conv.0 tensor(0.7197)
features.16.conv.3 tensor(0.7977)
features.16.conv.6 tensor(0.8628)
conv.0 tensor(0.0890)
tensor(1283648.) 2188896.0
INFO - Validation [46][   40/   40]   Loss 0.320558   Top1 88.940000   Top5 99.810000   BatchTime 0.126992
INFO - ==> Top1: 88.940    Top5: 99.810    Loss: 0.321
INFO - ==> Sparsity : 0.586
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 88.940   Top5: 99.810]
INFO - Scoreboard best 2 ==> Epoch [45][Top1: 88.820   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 88.770   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
0.63558346
0.63568199
0.63572270
0.63544858
0.63568932
0.63543373
0.63537890
0.63527524
0.63532239
0.63528454
0.63553947
0.63531089
0.63544512
0.63560158
0.63566768
0.63562912
0.63544601
0.63522488
0.63527220
INFO - Training [47][   20/  196]   Loss 0.428352   Top1 85.195312   Top5 98.417969   BatchTime 0.413044   LR 0.000077
0.63523805
0.63517714
0.63534212
0.63521981
0.63524562
0.63529086
0.63505715
0.63511157
0.63514000
0.63510084
0.63502175
0.63502431
0.63520265
0.63496512
0.63509721
0.63517892
0.63487297
0.63481015
0.63474381
0.63454753
0.63435960
0.63418043
0.63416278
INFO - Training [47][   40/  196]   Loss 0.441483   Top1 84.541016   Top5 98.564453   BatchTime 0.381489   LR 0.000076
0.63404810
0.63398194
0.63436323
0.63431472
0.63412029
0.63411403
0.63392490
0.63374579
0.63360173
0.63371682
0.63366055
0.63364363
0.63349152
0.63342273
0.63345754
0.63345718
INFO - Training [47][   60/  196]   Loss 0.440813   Top1 84.674479   Top5 98.587240   BatchTime 0.378462   LR 0.000076
0.63349319
0.63346124
0.63382190
0.63370562
0.63367921
0.63371396
0.63384902
0.63381535
0.63353121
0.63357264
0.63359207
0.63366127
0.63353175
0.63369095
0.63366598
0.63357806
0.63346213
0.63350779
0.63310683
0.63304800
INFO - Training [47][   80/  196]   Loss 0.436992   Top1 84.833984   Top5 98.706055   BatchTime 0.383722   LR 0.000075
0.63315231
0.63309735
0.63303792
0.63333863
0.63331270
0.63341594
0.63336581
0.63351208
0.63337666
0.63439256
0.63448274
0.63440955
0.63447243
0.63462621
0.63454729
0.63451892
0.63459611
0.63483644
0.63453716
0.63445801
0.63461357
0.63472414
INFO - Training [47][  100/  196]   Loss 0.429287   Top1 85.082031   Top5 98.785156   BatchTime 0.379439   LR 0.000075
0.63476336
0.63477093
0.63476998
0.63464218
0.63462430
0.63461792
0.63454694
0.63447291
0.63455963
0.63462234
0.63473064
0.63501114
0.63504869
0.63497639
0.63488030
0.63475084
0.63452280
0.63435817
0.63422823
0.63430882
0.63450420
INFO - Training [47][  120/  196]   Loss 0.426012   Top1 85.169271   Top5 98.841146   BatchTime 0.378088   LR 0.000074
0.63457835
0.63473463
0.63467431
0.63470066
0.63462043
0.63445348
0.63436639
0.63429248
0.63437438
0.63412338
0.63406754
0.63416874
0.63432235
0.63418454
0.63436347
0.63456905
INFO - Training [47][  140/  196]   Loss 0.423777   Top1 85.231585   Top5 98.895089   BatchTime 0.378254   LR 0.000074
0.63424963
0.63435006
0.63415313
0.63420653
0.63436550
0.63445181
0.63431960
0.63415182
0.63407856
0.63417250
0.63438630
0.63439107
0.63434845
0.63456976
0.63443428
0.63443774
0.63420218
0.63414496
0.63407350
0.63403600
0.63402957
0.63428140
INFO - Training [47][  160/  196]   Loss 0.423073   Top1 85.234375   Top5 98.864746   BatchTime 0.376303   LR 0.000073
0.63440460
0.63442332
0.63441521
0.63461709
0.63482624
0.63454753
0.63436478
0.63438505
0.63437909
0.63439596
0.63430589
0.63420945
0.63437647
0.63436973
0.63440108
0.63456142
0.63479042
0.63499248
0.63499135
0.63471460
0.63454282
0.63441145
INFO - Training [47][  180/  196]   Loss 0.420861   Top1 85.319010   Top5 98.841146   BatchTime 0.375815   LR 0.000073
0.63440108
0.63455534
0.63437152
0.63425875
0.63409477
0.63414788
0.63439548
0.63448578
0.63452822
0.63455379
0.63479042
0.63464546
0.63447624
0.63422650
0.63406426
INFO - ==> Top1: 85.380    Top5: 98.832    Loss: 0.419
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [47][   20/   40]   Loss 0.330865   Top1 89.082031   Top5 99.492188   BatchTime 0.134758
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0410)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0907)
features.2.conv.0 tensor(0.1036)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.1557)
features.3.conv.0 tensor(0.0532)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0464)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.1502)
features.5.conv.0 tensor(0.2796)
features.5.conv.3 tensor(0.4242)
features.5.conv.6 tensor(0.1263)
features.6.conv.0 tensor(0.0418)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0824)
features.7.conv.0 tensor(0.1620)
features.7.conv.3 tensor(0.4427)
features.7.conv.6 tensor(0.5531)
features.8.conv.0 tensor(0.5372)
features.8.conv.3 tensor(0.5295)
features.8.conv.6 tensor(0.4796)
features.9.conv.0 tensor(0.4254)
features.9.conv.3 tensor(0.5602)
features.9.conv.6 tensor(0.6431)
features.10.conv.0 tensor(0.0636)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0806)
features.11.conv.0 tensor(0.7478)
features.11.conv.3 tensor(0.6447)
features.11.conv.6 tensor(0.7788)
features.12.conv.0 tensor(0.6590)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.7767)
features.13.conv.0 tensor(0.2447)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.1045)
features.14.conv.0 tensor(0.9029)
features.14.conv.3 tensor(0.8359)
features.14.conv.6 tensor(0.9517)
features.15.conv.0 tensor(0.8692)
features.15.conv.3 tensor(0.8227)
features.15.conv.6 tensor(0.9685)
features.16.conv.0 tensor(0.7363)
features.16.conv.3 tensor(0.7979)
features.16.conv.6 tensor(0.8639)
conv.0 tensor(0.0885)
tensor(1287403.) 2188896.0
INFO - Validation [47][   40/   40]   Loss 0.321579   Top1 89.220000   Top5 99.660000   BatchTime 0.094634
INFO - ==> Top1: 89.220    Top5: 99.660    Loss: 0.322
INFO - ==> Sparsity : 0.588
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.220   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 88.940   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 88.820   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
0.63424355
0.63427120
0.63436711
0.63441980
0.63432246
0.63437700
0.63421154
0.63430339
0.63430160
0.63417274
0.63442302
0.63455218
0.63466376
0.63456577
0.63444906
0.63425213
0.63404924
0.63420039
0.63412875
0.63409847
0.63417393
0.63416231
INFO - Training [48][   20/  196]   Loss 0.433151   Top1 84.921875   Top5 98.437500   BatchTime 0.438381   LR 0.000072
0.63434607
0.63439721
0.63428110
0.63437688
0.63434947
0.63473648
0.63449782
0.63437337
0.63451535
0.63460827
0.63457680
0.63453865
0.63487083
0.63477552
0.63457948
0.63450813
INFO - Training [48][   40/  196]   Loss 0.442147   Top1 84.716797   Top5 98.427734   BatchTime 0.401306   LR 0.000071
0.63440907
0.63428408
0.63436353
0.63465005
0.63448006
0.63446701
0.63469136
0.63456357
0.63443118
0.63450962
0.63448101
0.63423741
0.63424450
0.63428074
0.63451952
0.63447595
0.63449419
0.63424402
0.63432163
0.63429648
0.63443565
0.63436574
INFO - Training [48][   60/  196]   Loss 0.432452   Top1 85.039062   Top5 98.535156   BatchTime 0.392623   LR 0.000071
0.63423336
0.63427854
0.63425064
0.63427991
0.63448435
0.63424975
0.63423461
0.63449520
0.63421166
0.63426298
0.63421327
0.63431424
0.63448560
0.63460094
0.63438880
0.63437712
0.63430876
0.63416886
0.63403940
0.63403195
0.63405651
INFO - Training [48][   80/  196]   Loss 0.433211   Top1 85.039062   Top5 98.618164   BatchTime 0.389576   LR 0.000070
0.63434130
0.63455379
0.63440371
0.63411695
0.63398147
0.63406038
0.63411152
0.63389373
0.63391399
0.63408345
0.63408226
0.63394958
0.63412768
0.63386178
0.63375294
0.63371933
0.63365281
0.63368356
0.63352573
0.63361216
INFO - Training [48][  100/  196]   Loss 0.426178   Top1 85.167969   Top5 98.734375   BatchTime 0.391459   LR 0.000070
0.63394886
0.63394523
0.63413006
0.63401580
0.63388753
0.63392884
0.63390094
0.63406193
0.63399756
0.63387781
0.63379580
0.63367760
0.63370466
0.63367474
0.63394868
0.63397598
INFO - Training [48][  120/  196]   Loss 0.417278   Top1 85.465495   Top5 98.769531   BatchTime 0.386691   LR 0.000069
0.63415432
0.63407975
0.63388604
0.63415706
0.63409108
0.63396484
0.63377953
0.63378924
0.63375622
0.63379866
0.63400817
0.63391739
0.63402486
0.63411725
0.63415378
0.63393098
0.63382262
0.63367015
0.63362890
0.63381320
0.63384926
0.63397640
INFO - Training [48][  140/  196]   Loss 0.417149   Top1 85.502232   Top5 98.814174   BatchTime 0.383233   LR 0.000069
0.63378865
0.63382500
0.63388878
0.63400167
0.63397229
0.63395309
0.63380414
0.63357711
0.63374966
0.63383418
0.63396341
0.63367426
0.63393039
0.63370270
0.63360709
0.63369185
0.63387209
0.63389587
0.63405049
0.63400245
0.63396758
0.63405150
INFO - Training [48][  160/  196]   Loss 0.419307   Top1 85.397949   Top5 98.789062   BatchTime 0.380979   LR 0.000068
0.63421482
0.63413799
0.63389832
0.63412249
0.63414049
0.63414103
0.63425887
0.63421416
0.63465917
0.63446236
0.63453257
0.63460863
0.63462406
0.63456792
0.63409919
0.63407773
INFO - Training [48][  180/  196]   Loss 0.419505   Top1 85.340712   Top5 98.786892   BatchTime 0.380771   LR 0.000068
0.63406068
0.63404423
0.63403255
0.63412148
0.63418806
0.63445318
0.63471347
0.63471377
0.63443577
0.63415080
0.63393247
0.63380295
0.63374668
0.63388824
0.63389981
0.63375843
0.63387144
0.63425279
0.63418567
********************pre-trained*****************
INFO - ==> Top1: 85.316    Top5: 98.780    Loss: 0.420
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [48][   20/   40]   Loss 0.346965   Top1 88.125000   Top5 99.609375   BatchTime 0.141690
INFO - Validation [48][   40/   40]   Loss 0.336830   Top1 88.420000   Top5 99.720000   BatchTime 0.098343
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1016)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.1528)
features.3.conv.0 tensor(0.0547)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1118)
features.4.conv.0 tensor(0.0488)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1458)
features.5.conv.0 tensor(0.2674)
features.5.conv.3 tensor(0.4225)
features.5.conv.6 tensor(0.1281)
features.6.conv.0 tensor(0.0431)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.1593)
features.7.conv.3 tensor(0.4427)
features.7.conv.6 tensor(0.5554)
features.8.conv.0 tensor(0.5321)
features.8.conv.3 tensor(0.5324)
features.8.conv.6 tensor(0.4926)
features.9.conv.0 tensor(0.4257)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.6473)
features.10.conv.0 tensor(0.0638)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0798)
features.11.conv.0 tensor(0.7478)
features.11.conv.3 tensor(0.6431)
features.11.conv.6 tensor(0.7836)
features.12.conv.0 tensor(0.6449)
features.12.conv.3 tensor(0.6703)
features.12.conv.6 tensor(0.7799)
features.13.conv.0 tensor(0.2452)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.1048)
features.14.conv.0 tensor(0.9045)
features.14.conv.3 tensor(0.8363)
features.14.conv.6 tensor(0.9520)
features.15.conv.0 tensor(0.8702)
features.15.conv.3 tensor(0.8228)
features.15.conv.6 tensor(0.9681)
features.16.conv.0 tensor(0.7332)
features.16.conv.3 tensor(0.7972)
features.16.conv.6 tensor(0.8658)
conv.0 tensor(0.0882)
tensor(1287613.) 2188896.0
INFO - ==> Top1: 88.420    Top5: 99.720    Loss: 0.337
INFO - ==> Sparsity : 0.588
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.220   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 88.940   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 88.820   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
0.63433158
0.63452297
0.63463831
0.63452876
0.63438934
0.63416839
0.63416857
0.63424736
0.63417900
0.63413644
0.63407505
0.63407844
0.63408715
0.63409030
0.63382697
0.63394392
0.63372546
0.63371617
0.63372368
0.63369268
0.63371283
INFO - Training [49][   20/  196]   Loss 0.439213   Top1 84.472656   Top5 98.437500   BatchTime 0.411947   LR 0.000067
0.63357747
0.63356251
0.63359654
0.63367540
0.63384169
0.63405943
0.63419783
0.63413477
0.63394356
0.63363940
0.63357425
0.63347358
0.63334340
0.63346660
0.63342631
0.63344532
0.63338184
INFO - Training [49][   40/  196]   Loss 0.437061   Top1 84.472656   Top5 98.447266   BatchTime 0.381875   LR 0.000066
0.63373011
0.63366854
0.63368100
0.63357162
0.63349867
0.63347292
0.63331717
0.63318819
0.63294089
0.63289905
0.63292557
0.63318878
0.63320643
0.63322067
0.63320714
0.63301331
0.63296050
0.63302147
0.63339210
0.63361412
0.63351488
0.63366789
INFO - Training [49][   60/  196]   Loss 0.429953   Top1 84.850260   Top5 98.541667   BatchTime 0.377390   LR 0.000066
0.63344765
0.63329971
0.63330340
0.63323820
0.63322043
0.63324600
0.63325679
0.63337070
0.63354391
0.63346273
0.63328475
0.63337612
0.63332272
0.63318199
0.63289541
0.63289249
0.63280636
0.63289350
0.63305861
0.63314867
0.63306034
INFO - Training [49][   80/  196]   Loss 0.430564   Top1 84.926758   Top5 98.691406   BatchTime 0.379690   LR 0.000065
0.63269287
0.63263232
0.63265532
0.63263041
0.63263828
0.63298261
0.63324469
0.63319635
0.63298833
0.63287264
0.63269705
0.63248974
0.63240099
0.63219160
0.63201880
0.63209611
0.63207650
0.63219923
0.63231277
0.63219768
0.63222772
INFO - Training [49][  100/  196]   Loss 0.424040   Top1 85.089844   Top5 98.742188   BatchTime 0.379407   LR 0.000065
0.63159955
0.63125217
0.63122094
0.63097358
0.63099575
0.63131279
0.63123649
0.63118464
0.63085490
0.63085163
0.63071495
0.63057238
0.63034678
0.63006669
0.62996417
0.63004321
INFO - Training [49][  120/  196]   Loss 0.417826   Top1 85.397135   Top5 98.808594   BatchTime 0.377565   LR 0.000064
0.63000882
0.63009501
0.63026106
0.63021243
0.63035733
0.63068604
0.63023353
0.62992847
0.62987834
0.62993973
0.63002074
0.63003719
0.62962425
0.62978780
0.62949395
0.62940598
0.62927121
0.62933397
0.62955499
0.62950671
INFO - Training [49][  140/  196]   Loss 0.416461   Top1 85.521763   Top5 98.836496   BatchTime 0.380118   LR 0.000064
0.62973994
0.62945515
0.62927675
0.62931120
0.62938488
0.62956923
0.62933820
0.62933272
0.62931377
0.62915421
0.62914693
0.62895077
0.62891823
0.62849182
0.62847370
0.62847531
0.62861788
0.62866235
0.62872827
0.62859064
0.62864262
0.62882453
INFO - Training [49][  160/  196]   Loss 0.416201   Top1 85.476074   Top5 98.850098   BatchTime 0.378890   LR 0.000063
0.62859637
0.62864757
0.62891400
0.62991679
0.62993205
0.62992197
0.63000762
0.63005561
0.63005394
0.63009113
0.63068104
0.63075924
0.63065249
0.63022012
0.63000739
0.62993509
0.62982875
INFO - Training [49][  180/  196]   Loss 0.415693   Top1 85.492622   Top5 98.843316   BatchTime 0.377075   LR 0.000063
0.62986958
0.62955099
0.62956750
0.62968642
0.62972778
0.62999052
0.62989140
0.63012606
0.62989885
0.62961441
0.62953597
0.62926650
0.62918562
0.62926424
0.62922847
0.62905365
INFO - ==> Top1: 85.512    Top5: 98.814    Loss: 0.416
0.62871116
0.62875170
0.62880874
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [49][   20/   40]   Loss 0.338036   Top1 88.496094   Top5 99.531250   BatchTime 0.142561
INFO - Validation [49][   40/   40]   Loss 0.328536   Top1 88.780000   Top5 99.710000   BatchTime 0.097753
INFO - ==> Top1: 88.780    Top5: 99.710    Loss: 0.329
INFO - ==> Sparsity : 0.590
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.220   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 88.940   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 88.820   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.1738)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1056)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1562)
features.3.conv.0 tensor(0.0561)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1118)
features.4.conv.0 tensor(0.0496)
features.4.conv.3 tensor(0.3096)
features.4.conv.6 tensor(0.1460)
features.5.conv.0 tensor(0.2707)
features.5.conv.3 tensor(0.4201)
features.5.conv.6 tensor(0.1258)
features.6.conv.0 tensor(0.0433)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0837)
features.7.conv.0 tensor(0.1673)
features.7.conv.3 tensor(0.4430)
features.7.conv.6 tensor(0.5603)
features.8.conv.0 tensor(0.5398)
features.8.conv.3 tensor(0.5324)
features.8.conv.6 tensor(0.5767)
features.9.conv.0 tensor(0.4352)
features.9.conv.3 tensor(0.5541)
features.9.conv.6 tensor(0.6520)
features.10.conv.0 tensor(0.0665)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.0807)
features.11.conv.0 tensor(0.7524)
features.11.conv.3 tensor(0.6445)
features.11.conv.6 tensor(0.7865)
features.12.conv.0 tensor(0.6435)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.7870)
features.13.conv.0 tensor(0.2446)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.1104)
features.14.conv.0 tensor(0.9042)
features.14.conv.3 tensor(0.8368)
features.14.conv.6 tensor(0.9532)
features.15.conv.0 tensor(0.8703)
features.15.conv.3 tensor(0.8225)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.7367)
features.16.conv.3 tensor(0.7977)
features.16.conv.6 tensor(0.8659)
conv.0 tensor(0.0879)
tensor(1292482.) 2188896.0
0.62854540
0.62872118
0.62918943
0.62918687
0.62930906
0.62924153
0.62893254
0.62881786
0.62876552
0.62861723
0.62853980
0.62847286
0.62845016
0.62841511
0.62840098
0.62859547
INFO - Training [50][   20/  196]   Loss 0.402825   Top1 86.035156   Top5 98.300781   BatchTime 0.406012   LR 0.000062
0.62857121
0.62844390
0.62829715
0.62822443
0.62830871
0.62865561
0.62851101
0.62855297
0.62850314
0.62854785
0.62885273
0.62884653
0.62874490
0.62868863
0.62842995
0.62815690
0.62799931
0.62785125
0.62799412
0.62762749
0.62740260
0.62738842
0.62724644
0.62704974
0.62688977
INFO - Training [50][   40/  196]   Loss 0.414535   Top1 85.664062   Top5 98.505859   BatchTime 0.364908   LR 0.000062
0.62689078
0.62698954
0.62679791
0.62651062
0.62654740
0.62661642
0.62652576
0.62634742
0.62608832
0.62592179
0.62588716
0.62598026
0.62595308
0.62605053
0.62591225
0.62579888
0.62555015
0.62548125
INFO - Training [50][   60/  196]   Loss 0.412157   Top1 85.690104   Top5 98.509115   BatchTime 0.358498   LR 0.000061
0.62532383
0.62505358
0.62505543
0.62513000
0.62518078
0.62533313
0.62569922
0.62578380
0.62561125
0.62533367
0.62514710
0.62501168
0.62487185
0.62488455
0.62494135
0.62521338
0.62501633
0.62513119
0.62524492
0.62504685
0.62501782
INFO - Training [50][   80/  196]   Loss 0.412766   Top1 85.751953   Top5 98.681641   BatchTime 0.360608   LR 0.000061
0.62487793
0.62504292
0.62510365
0.62516022
0.62482208
0.62496084
0.62495464
0.62484986
0.62474871
0.62459099
0.62472570
0.62479460
0.62480032
0.62462056
0.62481552
0.62494779
0.62523746
0.62518930
0.62503010
0.62492901
0.62489295
0.62487578
INFO - Training [50][  100/  196]   Loss 0.408252   Top1 85.925781   Top5 98.726562   BatchTime 0.363757   LR 0.000060
0.62493825
0.62498003
0.62470204
0.62467074
0.62462008
0.62483519
0.62468249
0.62468988
0.62471813
0.62460959
0.62449896
0.62453467
0.62486875
0.62623709
0.62713838
INFO - Training [50][  120/  196]   Loss 0.404417   Top1 86.009115   Top5 98.805339   BatchTime 0.366443   LR 0.000060
0.62773734
0.62839139
0.62994868
0.62985408
0.62982708
0.62980956
0.62992299
0.62984031
0.62967747
0.62958539
0.62942505
0.62942684
0.62948900
0.62980753
0.62987775
0.62971807
0.62950391
0.62945396
0.62923932
0.62934595
0.62920803
INFO - Training [50][  140/  196]   Loss 0.402940   Top1 86.060268   Top5 98.839286   BatchTime 0.368600   LR 0.000059
0.62931305
0.62935817
0.62962586
0.62955004
0.62928605
0.62927675
0.62924522
0.62928486
0.62945634
0.62941778
0.62936068
0.62952620
0.62987208
0.62987536
0.62992197
0.62988579
0.62976551
0.62956405
0.62943739
0.62925607
0.62925655
0.62920505
0.62959617
INFO - Training [50][  160/  196]   Loss 0.407801   Top1 85.925293   Top5 98.833008   BatchTime 0.367390   LR 0.000059
0.62959450
0.62961715
0.62975621
0.62971014
0.62979865
0.62969983
0.62963015
0.62950718
0.62921983
0.62909639
0.62904841
0.62907761
0.62906718
0.62931138
0.62957406
0.62956041
INFO - Training [50][  180/  196]   Loss 0.408157   Top1 85.881076   Top5 98.815104   BatchTime 0.366656   LR 0.000058
0.62953949
0.62972367
0.62987703
0.62982416
0.62962753
0.62949359
0.62953341
0.62966734
0.62956530
0.62946135
0.62935567
0.62940747
0.62933385
0.62934923
0.62960130
0.62959075
0.62942582
0.62946343
0.62942511
********************pre-trained*****************
INFO - ==> Top1: 85.868    Top5: 98.798    Loss: 0.408
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [50][   20/   40]   Loss 0.328113   Top1 89.355469   Top5 99.667969   BatchTime 0.150547
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1030)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1574)
features.3.conv.0 tensor(0.0570)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1111)
features.4.conv.0 tensor(0.0467)
features.4.conv.3 tensor(0.3056)
features.4.conv.6 tensor(0.1468)
features.5.conv.0 tensor(0.2876)
features.5.conv.3 tensor(0.4201)
features.5.conv.6 tensor(0.1273)
features.6.conv.0 tensor(0.0438)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0819)
features.7.conv.0 tensor(0.1616)
features.7.conv.3 tensor(0.4442)
features.7.conv.6 tensor(0.5646)
features.8.conv.0 tensor(0.5429)
features.8.conv.3 tensor(0.5315)
features.8.conv.6 tensor(0.5904)
features.9.conv.0 tensor(0.4381)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6512)
features.10.conv.0 tensor(0.0658)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.0800)
features.11.conv.0 tensor(0.7542)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.7882)
features.12.conv.0 tensor(0.6519)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.7845)
features.13.conv.0 tensor(0.2440)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.1093)
features.14.conv.0 tensor(0.9047)
features.14.conv.3 tensor(0.8370)
features.14.conv.6 tensor(0.9533)
features.15.conv.0 tensor(0.8702)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.7391)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8689)
conv.0 tensor(0.0877)
tensor(1294557.) 2188896.0
INFO - Validation [50][   40/   40]   Loss 0.317359   Top1 89.400000   Top5 99.730000   BatchTime 0.103967
INFO - ==> Top1: 89.400    Top5: 99.730    Loss: 0.317
INFO - ==> Sparsity : 0.591
INFO - Scoreboard best 1 ==> Epoch [50][Top1: 89.400   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.220   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 88.940   Top5: 99.810]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
0.62951583
0.62957966
0.62952477
0.62948644
0.62933415
0.62947208
0.62940180
0.62956464
0.62957060
0.62936020
0.62925673
0.62905276
0.62898850
0.62912536
0.62939173
0.62948787
INFO - Training [51][   20/  196]   Loss 0.409504   Top1 85.585938   Top5 98.183594   BatchTime 0.371775   LR 0.000057
0.62925607
0.62941265
0.62962669
0.62961912
0.62930322
0.62942272
0.62924832
0.62923467
0.62917858
0.62916052
0.62911379
0.62905502
0.62922668
0.62912285
0.62919974
0.62902033
0.62897986
0.62931073
0.62941426
0.62927490
0.62925726
INFO - Training [51][   40/  196]   Loss 0.417418   Top1 85.712891   Top5 98.398438   BatchTime 0.329547   LR 0.000057
0.62937510
0.62960875
0.62923878
0.62906688
0.62914914
0.62917429
0.62925082
0.62910652
0.62891477
0.62884718
0.62893480
0.62888092
0.62889773
0.62901086
0.62946218
0.62966055
0.62962568
0.62929457
0.62889147
0.62877709
0.62875628
0.62892580
INFO - Training [51][   60/  196]   Loss 0.413938   Top1 85.735677   Top5 98.515625   BatchTime 0.343175   LR 0.000056
0.62913936
0.62920761
0.62944561
0.62924439
0.62958944
0.62894517
0.62869859
0.62885576
0.62868357
0.62870359
0.62881243
0.62894112
0.62915283
0.62916821
0.62921292
0.62910163
0.62915546
0.62929344
INFO - Training [51][   80/  196]   Loss 0.415481   Top1 85.683594   Top5 98.623047   BatchTime 0.344299   LR 0.000056
0.62930346
0.62924469
0.62900388
0.62900591
0.62906986
0.62890953
0.62882173
0.62866169
0.62865484
0.62863594
0.62866199
0.62867063
0.62861389
0.62876421
0.62906492
0.62903827
0.62883842
0.62864774
0.62861782
0.62863100
0.62854534
0.62862647
INFO - Training [51][  100/  196]   Loss 0.409516   Top1 85.929688   Top5 98.687500   BatchTime 0.346445   LR 0.000055
0.62874597
0.62890804
0.62864596
0.62866718
0.62870497
0.62871730
0.62855011
0.62880796
0.62885910
0.62849492
0.62823051
0.62840801
0.62834156
0.62807477
0.62804955
0.62801957
0.62811726
0.62793607
0.62781227
0.62772179
0.62776077
0.62780517
INFO - Training [51][  120/  196]   Loss 0.405122   Top1 86.035156   Top5 98.782552   BatchTime 0.350737   LR 0.000055
0.62784070
0.62766546
0.62757927
0.62747246
0.62760603
0.62803733
0.62806308
0.62821472
0.62800968
0.62790936
0.62821490
0.62831730
0.62810785
0.62832958
0.62816912
0.62820315
INFO - Training [51][  140/  196]   Loss 0.405432   Top1 85.990513   Top5 98.825335   BatchTime 0.354027   LR 0.000054
0.62813503
0.62805086
0.62807328
0.62795383
0.62794250
0.62788999
0.62786239
0.62797153
0.62798649
0.62812066
0.62808466
0.62807900
0.62824631
0.62837529
0.62844712
0.62837827
0.62818795
0.62816995
0.62811661
0.62827998
0.62833101
INFO - Training [51][  160/  196]   Loss 0.408290   Top1 85.881348   Top5 98.813477   BatchTime 0.357408   LR 0.000054
0.62825441
0.62817639
0.62825197
0.62822109
0.62808639
0.62821513
0.62840807
0.62844211
0.62842762
0.62841743
0.62847894
0.62845498
0.62849998
0.62841034
0.62863630
0.62867147
0.62847114
0.62780017
0.62719160
0.62704790
0.62699783
INFO - Training [51][  180/  196]   Loss 0.407850   Top1 85.917969   Top5 98.791233   BatchTime 0.360205   LR 0.000053
0.62693542
0.62689680
0.62709963
0.62708700
0.62737578
0.62781668
0.62754768
0.62755817
0.62749970
0.62713265
0.62686634
0.62678313
0.62670213
0.62669426
0.62654406
0.62671483
INFO - ==> Top1: 86.004    Top5: 98.796    Loss: 0.406
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.62693155
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [51][   20/   40]   Loss 0.324949   Top1 88.925781   Top5 99.687500   BatchTime 0.136223
INFO - Validation [51][   40/   40]   Loss 0.315338   Top1 89.320000   Top5 99.790000   BatchTime 0.095116
INFO - ==> Top1: 89.320    Top5: 99.790    Loss: 0.315
INFO - ==> Sparsity : 0.592
INFO - Scoreboard best 1 ==> Epoch [50][Top1: 89.400   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [51][Top1: 89.320   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [47][Top1: 89.220   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0404)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0885)
features.2.conv.0 tensor(0.0995)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.1568)
features.3.conv.0 tensor(0.0527)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1115)
features.4.conv.0 tensor(0.0472)
features.4.conv.3 tensor(0.3084)
features.4.conv.6 tensor(0.1462)
features.5.conv.0 tensor(0.2816)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1276)
features.6.conv.0 tensor(0.0441)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0808)
features.7.conv.0 tensor(0.1650)
features.7.conv.3 tensor(0.4424)
features.7.conv.6 tensor(0.5704)
features.8.conv.0 tensor(0.5389)
features.8.conv.3 tensor(0.5353)
features.8.conv.6 tensor(0.5962)
features.9.conv.0 tensor(0.4447)
features.9.conv.3 tensor(0.5556)
features.9.conv.6 tensor(0.6491)
features.10.conv.0 tensor(0.0652)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0807)
features.11.conv.0 tensor(0.7516)
features.11.conv.3 tensor(0.6445)
features.11.conv.6 tensor(0.7874)
features.12.conv.0 tensor(0.6695)
features.12.conv.3 tensor(0.6701)
features.12.conv.6 tensor(0.7861)
features.13.conv.0 tensor(0.2477)
features.13.conv.3 tensor(0.4919)
features.13.conv.6 tensor(0.1082)
features.14.conv.0 tensor(0.9058)
features.14.conv.3 tensor(0.8373)
features.14.conv.6 tensor(0.9526)
features.15.conv.0 tensor(0.8705)
features.15.conv.3 tensor(0.8237)
features.15.conv.6 tensor(0.9673)
features.16.conv.0 tensor(0.7322)
features.16.conv.3 tensor(0.7972)
features.16.conv.6 tensor(0.8708)
conv.0 tensor(0.0874)
tensor(1295370.) 2188896.0
0.62682533
0.62692618
0.62698221
0.62695605
0.62704581
0.62721407
0.62754095
0.62741035
0.62759471
0.62721533
0.62716687
0.62712073
0.62711376
0.62690175
0.62685508
0.62704366
0.62736297
INFO - Training [52][   20/  196]   Loss 0.414506   Top1 85.390625   Top5 98.261719   BatchTime 0.425584   LR 0.000052
0.62733269
0.62724566
0.62719208
0.62691653
0.62666458
0.62660384
0.62658668
0.62654543
0.62674791
0.62695587
0.62671524
0.62683529
0.62718469
0.62725216
0.62671715
0.62683344
0.62684733
0.62669015
0.62669820
0.62651747
0.62649256
0.62656409
0.62644118
0.62631017
0.62636161
INFO - Training [52][   40/  196]   Loss 0.415476   Top1 85.507812   Top5 98.525391   BatchTime 0.372868   LR 0.000052
0.62639731
0.62645078
0.62641805
0.62622857
0.62620860
0.62631965
0.62615550
0.62608182
0.62620151
0.62621254
0.62612951
0.62610346
0.62595516
0.62592340
0.62594926
0.62596810
0.62584627
0.62588501
0.62605351
INFO - Training [52][   60/  196]   Loss 0.415087   Top1 85.520833   Top5 98.561198   BatchTime 0.348995   LR 0.000051
0.62601805
0.62606001
0.62616175
0.62613744
0.62609386
0.62603581
0.62607253
0.62610680
0.62602270
0.62595040
0.62584955
0.62597507
0.62597364
0.62614065
0.62663370
0.62676978
0.62672198
INFO - Training [52][   80/  196]   Loss 0.409500   Top1 85.717773   Top5 98.681641   BatchTime 0.352695   LR 0.000051
0.62644213
0.62625027
0.62625057
0.62622046
0.62611938
0.62606889
0.62625581
0.62605792
0.62624574
0.62620807
0.62617350
0.62602389
0.62600011
0.62610012
0.62591439
0.62576056
0.62575948
0.62588072
0.62602943
0.62603581
0.62602490
0.62594736
INFO - Training [52][  100/  196]   Loss 0.405364   Top1 85.945312   Top5 98.746094   BatchTime 0.355542   LR 0.000050
0.62590009
0.62604499
0.62596345
0.62574965
0.62562662
0.62575370
0.62577474
0.62576002
0.62592983
0.62595713
0.62612832
0.62625861
0.62630951
0.62618560
0.62617624
0.62625384
0.62611592
0.62581915
0.62582302
0.62581718
0.62575895
INFO - Training [52][  120/  196]   Loss 0.400052   Top1 86.165365   Top5 98.815104   BatchTime 0.359726   LR 0.000050
0.62588882
0.62601620
0.62621123
0.62630945
0.62622964
0.62633920
0.62644374
0.62774348
0.62775189
0.62767863
0.62802529
0.62823081
0.62807649
0.62797695
0.62806219
0.62784004
0.62760216
0.62771630
0.62792665
0.62798226
INFO - Training [52][  140/  196]   Loss 0.400348   Top1 86.146763   Top5 98.861607   BatchTime 0.364087   LR 0.000049
0.62801653
0.62812310
0.62816685
0.62803966
0.62809223
0.62795520
0.62796605
0.62798339
0.62803960
0.62801278
0.62801147
0.62807703
0.62820607
0.62831628
0.62814385
0.62790418
0.62804389
INFO - Training [52][  160/  196]   Loss 0.402401   Top1 86.044922   Top5 98.823242   BatchTime 0.364202   LR 0.000049
0.62819505
0.62810695
0.62794930
0.62803239
0.62791568
0.62774211
0.62799668
0.62820435
0.62821937
0.62835127
0.62824941
0.62828982
0.62826741
0.62847573
0.62822235
0.62802798
0.62767977
0.62768519
0.62750047
0.62746286
0.62743789
INFO - Training [52][  180/  196]   Loss 0.401978   Top1 86.067708   Top5 98.806424   BatchTime 0.365189   LR 0.000048
0.62736946
0.62742347
0.62748963
0.62762678
0.62754434
0.62746787
0.62754095
0.62737244
0.62727064
0.62734616
0.62722331
0.62722147
0.62727547
0.62750024
0.62743986
0.62745321
INFO - ==> Top1: 86.050    Top5: 98.810    Loss: 0.402
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.62752849
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [52][   20/   40]   Loss 0.328105   Top1 89.140625   Top5 99.550781   BatchTime 0.178952
INFO - Validation [52][   40/   40]   Loss 0.320142   Top1 89.140000   Top5 99.690000   BatchTime 0.151854
INFO - ==> Top1: 89.140    Top5: 99.690    Loss: 0.320
INFO - ==> Sparsity : 0.593
INFO - Scoreboard best 1 ==> Epoch [50][Top1: 89.400   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [51][Top1: 89.320   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [47][Top1: 89.220   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1895)
features.1.conv.0 tensor(0.0423)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1019)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1548)
features.3.conv.0 tensor(0.0480)
features.3.conv.3 tensor(0.0810)
features.3.conv.6 tensor(0.1118)
features.4.conv.0 tensor(0.0519)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1475)
features.5.conv.0 tensor(0.2803)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.1271)
features.6.conv.0 tensor(0.0443)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0817)
features.7.conv.0 tensor(0.1638)
features.7.conv.3 tensor(0.4439)
features.7.conv.6 tensor(0.5692)
features.8.conv.0 tensor(0.5412)
features.8.conv.3 tensor(0.5347)
features.8.conv.6 tensor(0.5974)
features.9.conv.0 tensor(0.4476)
features.9.conv.3 tensor(0.5550)
features.9.conv.6 tensor(0.6517)
features.10.conv.0 tensor(0.0655)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0819)
features.11.conv.0 tensor(0.7523)
features.11.conv.3 tensor(0.6447)
features.11.conv.6 tensor(0.7865)
features.12.conv.0 tensor(0.6657)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.7912)
features.13.conv.0 tensor(0.2457)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.1077)
features.14.conv.0 tensor(0.9069)
features.14.conv.3 tensor(0.8377)
features.14.conv.6 tensor(0.9530)
features.15.conv.0 tensor(0.8734)
features.15.conv.3 tensor(0.8234)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.7328)
features.16.conv.3 tensor(0.7972)
features.16.conv.6 tensor(0.8755)
conv.0 tensor(0.0877)
tensor(1297891.) 2188896.0
0.62760472
0.62754810
0.62782556
0.62764949
0.62745464
0.62765223
0.62743741
0.62752533
0.62751704
0.62769514
0.62781996
0.62785584
0.62771845
0.62750870
0.62739086
0.62736338
0.62742406
0.62752277
0.62753177
INFO - Training [53][   20/  196]   Loss 0.406528   Top1 86.093750   Top5 98.535156   BatchTime 0.459955   LR 0.000047
0.62740356
0.62757242
0.62795615
0.62816381
0.62822014
0.62793159
0.62758088
0.62768656
0.62763017
0.62756985
0.62755775
0.62779492
0.62810200
0.62808222
0.62804341
0.62837172
0.62823665
0.62818956
0.62773955
0.62771499
INFO - Training [53][   40/  196]   Loss 0.395987   Top1 86.318359   Top5 98.652344   BatchTime 0.386746   LR 0.000047
0.62757409
0.62756056
0.62751383
0.62740421
0.62770915
0.62814069
0.62829721
0.62848473
0.62860960
0.62845433
0.62825179
0.62830824
0.62800127
0.62796330
0.62793559
0.62793416
0.62799579
0.62832105
INFO - Training [53][   60/  196]   Loss 0.393660   Top1 86.445312   Top5 98.697917   BatchTime 0.367996   LR 0.000046
0.62828505
0.62812942
0.62800831
0.62782341
0.62752938
0.62742418
0.62736428
0.62752837
0.62750983
0.62760812
0.62761927
0.62768024
0.62768507
0.62759131
0.62764603
0.62785554
0.62785375
0.62808216
0.62794214
0.62765074
0.62767923
INFO - Training [53][   80/  196]   Loss 0.398610   Top1 86.157227   Top5 98.813477   BatchTime 0.372338   LR 0.000046
0.62786442
0.62782913
0.62764776
0.62761307
0.62765568
0.62782186
0.62759346
0.62753642
0.62806052
0.62810016
0.62803483
0.62782151
0.62744057
0.62745219
0.62750947
0.62753272
0.62757355
0.62758243
0.62780911
0.62821478
0.62784344
INFO - Training [53][  100/  196]   Loss 0.395380   Top1 86.343750   Top5 98.871094   BatchTime 0.375468   LR 0.000046
0.62810338
0.62802327
0.62797982
0.62763751
0.62786108
0.62795383
0.62817746
0.62823153
0.62816960
0.62817341
0.62814659
0.62802052
0.62799770
0.62794518
0.62815744
0.62816608
0.62794548
0.62802494
0.62803680
0.62762743
0.62757200
INFO - Training [53][  120/  196]   Loss 0.391268   Top1 86.552734   Top5 98.935547   BatchTime 0.376600   LR 0.000045
0.62748182
0.62741446
0.62736142
0.62729329
0.62738597
0.62741899
0.62735623
0.62761837
0.62781215
0.62792605
0.62790769
0.62773156
0.62749761
0.62734330
0.62735230
0.62731105
0.62734604
0.62731630
0.62731630
0.62718350
0.62721807
0.62729478
INFO - Training [53][  140/  196]   Loss 0.390132   Top1 86.562500   Top5 98.987165   BatchTime 0.374764   LR 0.000045
0.62750959
0.62759012
0.62767196
0.62777817
0.62776309
0.62792677
0.62797636
0.62765855
0.62788278
0.62763959
0.62760150
0.62755173
0.62745911
0.62753040
0.62723452
0.62715191
INFO - Training [53][  160/  196]   Loss 0.394483   Top1 86.506348   Top5 98.935547   BatchTime 0.373746   LR 0.000044
0.62713075
0.62713915
0.62727952
0.62727189
0.62735558
0.62732732
0.62736076
0.62718177
0.62729651
0.62766951
0.62765658
0.62758839
0.62744284
0.62725323
0.62751824
0.62758082
0.62757653
0.62736160
0.62747592
0.62733054
0.62714732
INFO - Training [53][  180/  196]   Loss 0.394072   Top1 86.475694   Top5 98.897569   BatchTime 0.373868   LR 0.000044
0.62719524
0.62736243
0.62721390
0.62732452
0.62748492
0.62740946
0.62722230
0.62735969
0.62722051
0.62715417
0.62719345
0.62728947
0.62747687
0.62756753
0.62746871
0.62735182
INFO - ==> Top1: 86.472    Top5: 98.898    Loss: 0.393
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.62740862
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [53][   20/   40]   Loss 0.321859   Top1 89.238281   Top5 99.550781   BatchTime 0.139959
INFO - Validation [53][   40/   40]   Loss 0.312942   Top1 89.430000   Top5 99.680000   BatchTime 0.097012
INFO - ==> Top1: 89.430    Top5: 99.680    Loss: 0.313
INFO - ==> Sparsity : 0.593
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 89.430   Top5: 99.680]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 89.400   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [51][Top1: 89.320   Top5: 99.790]
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0890)
features.2.conv.0 tensor(0.1091)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.1562)
features.3.conv.0 tensor(0.0532)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1150)
features.4.conv.0 tensor(0.0539)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1484)
features.5.conv.0 tensor(0.2801)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1266)
features.6.conv.0 tensor(0.0422)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0809)
features.7.conv.0 tensor(0.1665)
features.7.conv.3 tensor(0.4421)
features.7.conv.6 tensor(0.5723)
features.8.conv.0 tensor(0.5449)
features.8.conv.3 tensor(0.5339)
features.8.conv.6 tensor(0.5932)
features.9.conv.0 tensor(0.4501)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6529)
features.10.conv.0 tensor(0.0640)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0828)
features.11.conv.0 tensor(0.7602)
features.11.conv.3 tensor(0.6454)
features.11.conv.6 tensor(0.7873)
features.12.conv.0 tensor(0.6601)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.7908)
features.13.conv.0 tensor(0.2455)
features.13.conv.3 tensor(0.4898)
features.13.conv.6 tensor(0.1079)
features.14.conv.0 tensor(0.9076)
features.14.conv.3 tensor(0.8367)
features.14.conv.6 tensor(0.9540)
features.15.conv.0 tensor(0.8751)
features.15.conv.3 tensor(0.8233)
features.15.conv.6 tensor(0.9679)
features.16.conv.0 tensor(0.7334)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8766)
conv.0 tensor(0.0871)
tensor(1299079.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
0.62721580
0.62714803
0.62732416
0.62745339
0.62756723
0.62752086
0.62728620
0.62732530
0.62733215
0.62735474
0.62742192
0.62735015
0.62678999
0.62697428
0.62714750
0.62708730
0.62691957
0.62700975
INFO - Training [54][   20/  196]   Loss 0.395045   Top1 86.132812   Top5 98.574219   BatchTime 0.439580   LR 0.000043
0.62717509
0.62745488
0.62740535
0.62754893
0.62781578
0.62760919
0.62721753
0.62702864
0.62677568
0.62682009
0.62694407
0.62681228
0.62656504
0.62662923
0.62674075
0.62670797
0.62631428
0.62571907
0.62553161
0.62579399
0.62596232
0.62588298
INFO - Training [54][   40/  196]   Loss 0.403789   Top1 85.859375   Top5 98.623047   BatchTime 0.402757   LR 0.000042
0.62578857
0.62607861
0.62603980
0.62572443
0.62549740
0.62518561
0.62509036
0.62501943
0.62499857
0.62503392
0.62516564
0.62510341
0.62505186
0.62493432
0.62479109
0.62459600
0.62431759
0.62422770
0.62432122
0.62405652
0.62389934
INFO - Training [54][   60/  196]   Loss 0.399748   Top1 86.113281   Top5 98.736979   BatchTime 0.397857   LR 0.000042
0.62384123
0.62375528
0.62372601
0.62375629
0.62363732
0.62363333
0.62351906
0.62338954
0.62331390
0.62346739
0.62338692
0.62323624
0.62303835
0.62297934
0.62248868
0.62268847
0.62261921
0.62228829
0.62203079
0.62164867
0.62150037
INFO - Training [54][   80/  196]   Loss 0.399533   Top1 86.059570   Top5 98.857422   BatchTime 0.393063   LR 0.000041
0.62156969
0.62162417
0.62136215
0.62139577
0.62112451
0.62120438
0.62101799
0.62081337
0.62055677
0.62023723
0.62010705
0.62003303
0.62012058
0.62006754
0.62026954
0.62015402
INFO - Training [54][  100/  196]   Loss 0.393705   Top1 86.300781   Top5 98.875000   BatchTime 0.387949   LR 0.000041
0.61987358
0.61963665
0.61930913
0.61907387
0.61905742
0.61877513
0.61865002
0.61845738
0.61869246
0.61850542
0.61830813
0.61822373
0.61806077
0.61755824
0.61752570
0.61750740
0.61739910
0.61730725
0.61706942
0.61691499
0.61684138
INFO - Training [54][  120/  196]   Loss 0.387958   Top1 86.546224   Top5 98.919271   BatchTime 0.387517   LR 0.000040
0.61678696
0.61668915
0.61668789
0.61656600
0.61648226
0.61675322
0.61662424
0.61639833
0.61630106
0.61632216
0.61631799
0.61626780
0.61613590
0.61592525
0.61573285
0.61571515
0.61581671
0.61570930
0.61543477
0.61549550
0.61546707
0.61538249
INFO - Training [54][  140/  196]   Loss 0.386374   Top1 86.635045   Top5 98.939732   BatchTime 0.383492   LR 0.000040
0.61546874
0.61549795
0.61553890
0.61566430
0.61552602
0.61553538
0.61562496
0.61555696
0.61564320
0.61545718
0.61528271
0.61562127
0.61541569
0.61537611
0.61541188
0.61525369
INFO - Training [54][  160/  196]   Loss 0.390293   Top1 86.474609   Top5 98.913574   BatchTime 0.382097   LR 0.000039
0.61497211
0.61474591
0.61479568
0.61462873
0.61447418
0.61442780
0.61452729
0.61461115
0.61477047
0.61467838
0.61482066
0.61495906
0.61495298
0.61492866
0.61489016
0.61461121
0.61451870
0.61458874
0.61467415
0.61458451
0.61478347
INFO - Training [54][  180/  196]   Loss 0.391797   Top1 86.386719   Top5 98.886719   BatchTime 0.382874   LR 0.000039
0.61483711
0.61474758
0.61486536
0.61471289
0.61458856
0.61456847
0.61462730
0.61468303
0.61462915
0.61467987
0.61469638
0.61486214
0.61507785
0.61514640
0.61530483
INFO - ==> Top1: 86.410    Top5: 98.888    Loss: 0.390
0.61543393
0.61534041
0.61530250
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [54][   20/   40]   Loss 0.326622   Top1 89.472656   Top5 99.472656   BatchTime 0.157095
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0894)
features.2.conv.0 tensor(0.0929)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.1577)
features.3.conv.0 tensor(0.0550)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1139)
features.4.conv.0 tensor(0.0483)
features.4.conv.3 tensor(0.3084)
features.4.conv.6 tensor(0.1494)
features.5.conv.0 tensor(0.2832)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.1326)
features.6.conv.0 tensor(0.0400)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0804)
features.7.conv.0 tensor(0.1665)
features.7.conv.3 tensor(0.4424)
features.7.conv.6 tensor(0.5799)
features.8.conv.0 tensor(0.5470)
features.8.conv.3 tensor(0.5341)
features.8.conv.6 tensor(0.5972)
features.9.conv.0 tensor(0.4515)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.6567)
features.10.conv.0 tensor(0.0654)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0833)
features.11.conv.0 tensor(0.7607)
features.11.conv.3 tensor(0.6462)
features.11.conv.6 tensor(0.7868)
features.12.conv.0 tensor(0.6636)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.7929)
features.13.conv.0 tensor(0.2618)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.2033)
features.14.conv.0 tensor(0.9079)
features.14.conv.3 tensor(0.8370)
features.14.conv.6 tensor(0.9534)
features.15.conv.0 tensor(0.8768)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9681)
features.16.conv.0 tensor(0.7369)
features.16.conv.3 tensor(0.7972)
features.16.conv.6 tensor(0.8781)
conv.0 tensor(0.0870)
tensor(1310778.) 2188896.0
INFO - Validation [54][   40/   40]   Loss 0.318511   Top1 89.510000   Top5 99.670000   BatchTime 0.107690
INFO - ==> Top1: 89.510    Top5: 99.670    Loss: 0.319
INFO - ==> Sparsity : 0.599
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 89.510   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 89.430   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 89.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
0.61510503
0.61505276
0.61511570
0.61510330
0.61501467
0.61502522
0.61511308
0.61495942
0.61490160
0.61510301
0.61512959
0.61504179
0.61505795
0.61514491
0.61526239
0.61509770
0.61487591
0.61486232
0.61493641
0.61490649
INFO - Training [55][   20/  196]   Loss 0.419957   Top1 85.527344   Top5 98.398438   BatchTime 0.461966   LR 0.000038
0.61494380
0.61476046
0.61481357
0.61458409
0.61443251
0.61430705
0.61436117
0.61447096
0.61442643
0.61450851
0.61448532
0.61456031
0.61468351
0.61490887
0.61480540
0.61458194
0.61451757
0.61451966
0.61459148
0.61441827
0.61460006
INFO - Training [55][   40/  196]   Loss 0.416263   Top1 85.546875   Top5 98.369141   BatchTime 0.416056   LR 0.000038
0.61476374
0.61462873
0.61454231
0.61442339
0.61458689
0.61453503
0.61455065
0.61477101
0.61489439
0.61485076
0.61492896
0.61472237
0.61465752
0.61474967
0.61467850
0.61493647
INFO - Training [55][   60/  196]   Loss 0.410301   Top1 85.761719   Top5 98.509115   BatchTime 0.402880   LR 0.000037
0.61497813
0.61489600
0.61493641
0.61498755
0.61511987
0.61500806
0.61482233
0.61467767
0.61464769
0.61453068
0.61453807
0.61445338
0.61439282
0.61459935
0.61458570
0.61456096
0.61462677
0.61467993
0.61471927
0.61444825
0.61440617
0.61432356
0.61432505
INFO - Training [55][   80/  196]   Loss 0.404001   Top1 85.903320   Top5 98.652344   BatchTime 0.390823   LR 0.000037
0.61430413
0.61430717
0.61422235
0.61432439
0.61408091
0.61398417
0.61398041
0.61428493
0.61411852
0.61396545
0.61406910
0.61404866
0.61410147
0.61414045
0.61387581
0.61390102
0.61416507
0.61435342
INFO - Training [55][  100/  196]   Loss 0.393151   Top1 86.289062   Top5 98.726562   BatchTime 0.380535   LR 0.000036
0.61418396
0.61415839
0.61396199
0.61394393
0.61396575
0.61397099
0.61391854
0.61375463
0.61365181
0.61379570
0.61386287
0.61388457
0.61376143
0.61372393
0.61350393
0.61342412
0.61349797
0.61347115
0.61328137
0.61325508
0.61334330
0.61326891
0.61314255
INFO - Training [55][  120/  196]   Loss 0.389307   Top1 86.429036   Top5 98.802083   BatchTime 0.372938   LR 0.000036
0.61325544
0.61358386
0.61362737
0.61368877
0.61362123
0.61335540
0.61322773
0.61328167
0.61335373
0.61337668
0.61338097
0.61328787
0.61321813
0.61290598
0.61291301
0.61261243
0.61250430
0.61242956
0.61237764
0.61240476
0.61249024
INFO - Training [55][  140/  196]   Loss 0.387479   Top1 86.517857   Top5 98.883929   BatchTime 0.373711   LR 0.000035
0.61270487
0.61265552
0.61261803
0.61260629
0.61264074
0.61256295
0.61254591
0.61267525
0.61270881
0.61251438
0.61220795
0.61218184
0.61214137
0.61207837
0.61200726
0.61210108
0.61186922
INFO - Training [55][  160/  196]   Loss 0.386540   Top1 86.530762   Top5 98.884277   BatchTime 0.372205   LR 0.000035
0.61212730
0.61177564
0.61162090
0.61147815
0.61141640
0.61147767
0.61142617
0.61135900
0.61142135
0.61120468
0.61117309
0.61116946
0.61114609
0.61106008
0.61113513
0.61108428
0.61119628
0.61099964
0.61100739
0.61107975
0.61120981
0.61111748
0.61101449
INFO - Training [55][  180/  196]   Loss 0.387370   Top1 86.495226   Top5 98.841146   BatchTime 0.370271   LR 0.000034
0.61090237
0.61072713
0.61092848
0.61078852
0.61087722
0.61066073
0.61067635
0.61080348
0.61072028
0.61054188
0.61049587
INFO - ==> Top1: 86.580    Top5: 98.848    Loss: 0.386
0.61041296
0.61041909
0.61039025
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [55][   20/   40]   Loss 0.321680   Top1 89.394531   Top5 99.531250   BatchTime 0.183191
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0911)
features.2.conv.0 tensor(0.0865)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.1658)
features.3.conv.0 tensor(0.0547)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1141)
features.4.conv.0 tensor(0.0535)
features.4.conv.3 tensor(0.3113)
features.4.conv.6 tensor(0.1489)
features.5.conv.0 tensor(0.2915)
features.5.conv.3 tensor(0.4167)
features.5.conv.6 tensor(0.1610)
features.6.conv.0 tensor(0.0423)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0821)
features.7.conv.0 tensor(0.1663)
features.7.conv.3 tensor(0.4421)
features.7.conv.6 tensor(0.5842)
features.8.conv.0 tensor(0.5461)
features.8.conv.3 tensor(0.5330)
features.8.conv.6 tensor(0.5958)
features.9.conv.0 tensor(0.4509)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6612)
features.10.conv.0 tensor(0.0658)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0844)
features.11.conv.0 tensor(0.7597)
features.11.conv.3 tensor(0.6470)
features.11.conv.6 tensor(0.7897)
features.12.conv.0 tensor(0.6717)
features.12.conv.3 tensor(0.6688)
features.12.conv.6 tensor(0.7974)
features.13.conv.0 tensor(0.2550)
features.13.conv.3 tensor(0.4898)
features.13.conv.6 tensor(0.2339)
features.14.conv.0 tensor(0.9083)
features.14.conv.3 tensor(0.8365)
features.14.conv.6 tensor(0.9526)
features.15.conv.0 tensor(0.8765)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9676)
features.16.conv.0 tensor(0.7360)
features.16.conv.3 tensor(0.7973)
features.16.conv.6 tensor(0.8793)
conv.0 tensor(0.0870)
tensor(1314542.) 2188896.0
INFO - Validation [55][   40/   40]   Loss 0.308801   Top1 89.620000   Top5 99.720000   BatchTime 0.127635
INFO - ==> Top1: 89.620    Top5: 99.720    Loss: 0.309
INFO - ==> Sparsity : 0.601
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 89.620   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 89.510   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 89.430   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
0.61044502
0.61045277
0.61053139
0.61056739
0.61074030
0.61064798
0.61050707
0.61028403
0.61026812
0.61012173
0.61007601
0.61003304
0.60994858
0.60997725
0.61008835
0.61003095
0.61000562
0.60997021
0.60995537
0.61004382
0.60991007
INFO - Training [56][   20/  196]   Loss 0.389446   Top1 86.679688   Top5 98.339844   BatchTime 0.478423   LR 0.000034
0.60980195
0.60983443
0.61014414
0.61006230
0.60995066
0.61002660
0.60992038
0.60990447
0.60987931
0.60983562
0.61011225
0.61002010
0.61003989
0.60999972
0.60979778
0.60965919
0.60959297
0.60964912
0.60968155
0.60961151
0.60959274
INFO - Training [56][   40/  196]   Loss 0.399564   Top1 86.171875   Top5 98.505859   BatchTime 0.427438   LR 0.000033
0.60966843
0.60991138
0.61006069
0.60989779
0.61001396
0.60976571
0.60975498
0.60976273
0.61001259
0.60982865
0.60972542
0.60976928
0.60979915
0.60951501
0.60944349
0.60948586
0.60933179
INFO - Training [56][   60/  196]   Loss 0.398082   Top1 86.087240   Top5 98.645833   BatchTime 0.403620   LR 0.000033
0.60931724
0.60938698
0.60938370
0.60941923
0.60944492
0.60940373
0.60972100
0.60978276
0.60994756
0.61001980
0.60977799
0.60949230
0.60964948
0.60946333
0.60930079
0.60916710
0.60911280
0.60913312
0.60953403
0.60953373
0.60933262
INFO - Training [56][   80/  196]   Loss 0.398191   Top1 86.147461   Top5 98.759766   BatchTime 0.398991   LR 0.000032
0.60937905
0.60938781
0.60946280
0.60949355
0.60929412
0.60920471
0.60933548
0.60946071
0.60941392
0.60939562
0.60920322
0.60941571
0.60937130
0.60916054
0.60899413
0.60886651
0.60881031
0.60879493
0.60889232
0.60893118
0.60901082
INFO - Training [56][  100/  196]   Loss 0.389575   Top1 86.433594   Top5 98.785156   BatchTime 0.397608   LR 0.000032
0.60929632
0.60939354
0.60931516
0.60918009
0.60888237
0.60873616
0.60871381
0.60870111
0.60894901
0.60906410
0.60912091
0.60917270
0.60917920
0.60911667
0.60922456
0.60916764
0.60899115
INFO - Training [56][  120/  196]   Loss 0.385024   Top1 86.611328   Top5 98.854167   BatchTime 0.389587   LR 0.000031
0.60882574
0.60881245
0.60880417
0.60872704
0.60876262
0.60875487
0.60865438
0.60860074
0.60867256
0.60906333
0.60880154
0.60877216
0.60890061
0.60888541
0.60877943
0.60875094
0.60891825
0.60887992
0.60900217
0.60898560
0.60876429
0.60891867
INFO - Training [56][  140/  196]   Loss 0.386546   Top1 86.515067   Top5 98.906250   BatchTime 0.385013   LR 0.000031
0.60864627
0.60886675
0.60898060
0.60895807
0.60878813
0.60867935
0.60880142
0.60874075
0.60877377
0.60849851
0.60846585
0.60839790
0.60843772
0.60844857
0.60848075
0.60850221
0.60835069
0.60825652
0.60815233
0.60804373
0.60804230
INFO - Training [56][  160/  196]   Loss 0.388759   Top1 86.503906   Top5 98.872070   BatchTime 0.383407   LR 0.000031
0.60813969
0.60820001
0.60837299
0.60826212
0.60829824
0.60809505
0.60803586
0.60806453
0.60801607
0.60806185
0.60816562
0.60864705
0.60838038
0.60831147
0.60828823
0.60816979
0.60805011
0.60798895
INFO - Training [56][  180/  196]   Loss 0.388277   Top1 86.519097   Top5 98.860677   BatchTime 0.379983   LR 0.000030
0.60792136
0.60781783
0.60782993
0.60781020
0.60779369
0.60782069
0.60776699
0.60787719
0.60774857
0.60763985
0.60766202
0.60764861
0.60755986
0.60753655
0.60738605
0.60730803
0.60729545
INFO - ==> Top1: 86.624    Top5: 98.862    Loss: 0.386
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [56][   20/   40]   Loss 0.318400   Top1 89.472656   Top5 99.531250   BatchTime 0.142654
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0929)
features.2.conv.0 tensor(0.0845)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1655)
features.3.conv.0 tensor(0.0556)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0566)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1484)
features.5.conv.0 tensor(0.2832)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.1540)
features.6.conv.0 tensor(0.0407)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0808)
features.7.conv.0 tensor(0.1673)
features.7.conv.3 tensor(0.4416)
features.7.conv.6 tensor(0.5899)
features.8.conv.0 tensor(0.5539)
features.8.conv.3 tensor(0.5321)
features.8.conv.6 tensor(0.5967)
features.9.conv.0 tensor(0.4590)
features.9.conv.3 tensor(0.5561)
features.9.conv.6 tensor(0.6617)
features.10.conv.0 tensor(0.0670)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0848)
features.11.conv.0 tensor(0.7620)
features.11.conv.3 tensor(0.6472)
features.11.conv.6 tensor(0.7930)
features.12.conv.0 tensor(0.6760)
features.12.conv.3 tensor(0.6672)
features.12.conv.6 tensor(0.7936)
features.13.conv.0 tensor(0.2635)
features.13.conv.3 tensor(0.4884)
features.13.conv.6 tensor(0.2344)
features.14.conv.0 tensor(0.9088)
features.14.conv.3 tensor(0.8370)
features.14.conv.6 tensor(0.9528)
features.15.conv.0 tensor(0.8780)
features.15.conv.3 tensor(0.8228)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7420)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8796)
conv.0 tensor(0.0869)
tensor(1317285.) 2188896.0
INFO - Validation [56][   40/   40]   Loss 0.309678   Top1 89.610000   Top5 99.650000   BatchTime 0.100219
INFO - ==> Top1: 89.610    Top5: 99.650    Loss: 0.310
INFO - ==> Sparsity : 0.602
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 89.620   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [56][Top1: 89.610   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 89.510   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
0.60725909
0.60725719
0.60731441
0.60745007
0.60733539
0.60741830
0.60744041
0.60744393
0.60747486
0.60739797
0.60752600
0.60794234
0.60782456
0.60790831
0.60800970
0.60796219
0.60785460
0.60777539
0.60780764
0.60761958
0.60763985
0.60767746
INFO - Training [57][   20/  196]   Loss 0.392142   Top1 86.269531   Top5 98.281250   BatchTime 0.459248   LR 0.000029
0.60750389
0.60734850
0.60750037
0.60767609
0.60741657
0.60744870
0.60764223
0.60789037
0.60781443
0.60792917
0.60780466
0.60756981
0.60750335
0.60742188
0.60738558
0.60739237
0.60737377
INFO - Training [57][   40/  196]   Loss 0.406577   Top1 85.566406   Top5 98.388672   BatchTime 0.413688   LR 0.000029
0.60740393
0.60727918
0.60725331
0.60748315
0.60747612
0.60744059
0.60735893
0.60730296
0.60716587
0.60729122
0.60733503
0.60750937
0.60766029
0.60772783
0.60774189
0.60788065
0.60778290
0.60753793
0.60737920
0.60721445
0.60716289
INFO - Training [57][   60/  196]   Loss 0.392032   Top1 86.204427   Top5 98.528646   BatchTime 0.401297   LR 0.000029
0.60707891
0.60712677
0.60704458
0.60700482
0.60695308
0.60693014
0.60691428
0.60702664
0.60721833
0.60747045
0.60755783
0.60744298
0.60733753
0.60729557
0.60719883
0.60718602
0.60725105
0.60738426
0.60726911
0.60730094
0.60720307
INFO - Training [57][   80/  196]   Loss 0.390866   Top1 86.157227   Top5 98.701172   BatchTime 0.394580   LR 0.000028
0.60706955
0.60709530
0.60707057
0.60697162
0.60691059
0.60696596
0.60673028
0.60668600
0.60681725
0.60700989
0.60721159
0.60728610
0.60712707
0.60690653
0.60672975
0.60664803
0.60668200
0.60668308
0.60657620
0.60659480
0.60650241
INFO - Training [57][  100/  196]   Loss 0.383401   Top1 86.531250   Top5 98.769531   BatchTime 0.391721   LR 0.000028
0.60636926
0.60635787
0.60630786
0.60617101
0.60613877
0.60614294
0.60611254
0.60614961
0.60632044
0.60641706
0.60637146
0.60629576
0.60635179
0.60643214
0.60644722
0.60671413
INFO - Training [57][  120/  196]   Loss 0.378246   Top1 86.764323   Top5 98.867188   BatchTime 0.389488   LR 0.000027
0.60691017
0.60672891
0.60664421
0.60681522
0.60683513
0.60667920
0.60676754
0.60662258
0.60672313
0.60692668
0.60685724
0.60676128
0.60662532
0.60670108
0.60652280
0.60633278
0.60631961
0.60628533
0.60621935
0.60625559
0.60649741
INFO - Training [57][  140/  196]   Loss 0.376289   Top1 86.961496   Top5 98.883929   BatchTime 0.387883   LR 0.000027
0.60646921
0.60627389
0.60618639
0.60631830
0.60622865
0.60614878
0.60604078
0.60593319
0.60588098
0.60584664
0.60586458
0.60593385
0.60607034
0.60635340
0.60639405
0.60636479
0.60630649
0.60648394
0.60649735
0.60641849
0.60613811
0.60592610
0.60579795
INFO - Training [57][  160/  196]   Loss 0.379292   Top1 86.845703   Top5 98.903809   BatchTime 0.384554   LR 0.000027
0.60568464
0.60563165
0.60568285
0.60568577
0.60574788
0.60581976
0.60642749
0.60657829
0.60652441
0.60642660
0.60627139
0.60618377
0.60602432
0.60589498
0.60595697
0.60603583
0.60611922
0.60635567
0.60634774
0.60626918
INFO - Training [57][  180/  196]   Loss 0.379629   Top1 86.870660   Top5 98.884549   BatchTime 0.375959   LR 0.000026
0.60599017
0.60584480
0.60566622
0.60553402
0.60542059
0.60547626
0.60551941
0.60551989
0.60550636
0.60558319
0.60570830
0.60589457
0.60578489
0.60566890
INFO - ==> Top1: 86.914    Top5: 98.870    Loss: 0.378
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [57][   20/   40]   Loss 0.322409   Top1 89.296875   Top5 99.589844   BatchTime 0.145462
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0907)
features.2.conv.0 tensor(0.0854)
features.2.conv.3 tensor(0.3403)
features.2.conv.6 tensor(0.1678)
features.3.conv.0 tensor(0.0579)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1157)
features.4.conv.0 tensor(0.0537)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.1486)
features.5.conv.0 tensor(0.2873)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.1522)
features.6.conv.0 tensor(0.0409)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0809)
features.7.conv.0 tensor(0.1669)
features.7.conv.3 tensor(0.4424)
features.7.conv.6 tensor(0.5913)
features.8.conv.0 tensor(0.5678)
features.8.conv.3 tensor(0.5327)
features.8.conv.6 tensor(0.5981)
features.9.conv.0 tensor(0.4567)
features.9.conv.3 tensor(0.5538)
features.9.conv.6 tensor(0.6635)
features.10.conv.0 tensor(0.0660)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0854)
features.11.conv.0 tensor(0.7647)
features.11.conv.3 tensor(0.6476)
features.11.conv.6 tensor(0.7945)
features.12.conv.0 tensor(0.6760)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.7956)
features.13.conv.0 tensor(0.2613)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.2634)
features.14.conv.0 tensor(0.9087)
features.14.conv.3 tensor(0.8367)
features.14.conv.6 tensor(0.9523)
features.15.conv.0 tensor(0.8779)
features.15.conv.3 tensor(0.8228)
features.15.conv.6 tensor(0.9682)
features.16.conv.0 tensor(0.7446)
features.16.conv.3 tensor(0.7980)
features.16.conv.6 tensor(0.8809)
conv.0 tensor(0.0872)
tensor(1321379.) 2188896.0
INFO - Validation [57][   40/   40]   Loss 0.307836   Top1 89.680000   Top5 99.710000   BatchTime 0.099750
INFO - ==> Top1: 89.680    Top5: 99.710    Loss: 0.308
INFO - ==> Sparsity : 0.604
INFO - Scoreboard best 1 ==> Epoch [57][Top1: 89.680   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 89.620   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 89.610   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
0.60550165
0.60543847
0.60549235
0.60533690
0.60537636
0.60539079
0.60535330
0.60528713
0.60533398
0.60538721
0.60559720
0.60557419
0.60548836
0.60568553
0.60538799
0.60532451
0.60542363
0.60542530
0.60525411
INFO - Training [58][   20/  196]   Loss 0.390924   Top1 86.503906   Top5 98.281250   BatchTime 0.478912   LR 0.000025
0.60531420
0.60529023
0.60534769
0.60536194
0.60527211
0.60536599
0.60524434
0.60466993
0.60464805
0.60458422
0.60445315
0.60429305
0.60427624
0.60420924
0.60416085
0.60422200
0.60416466
0.60425782
0.60424191
0.60436177
0.60446626
0.60440809
INFO - Training [58][   40/  196]   Loss 0.392016   Top1 86.406250   Top5 98.603516   BatchTime 0.428646   LR 0.000025
0.60414416
0.60403293
0.60372698
0.60297638
0.60267192
0.60269058
0.60259807
0.60251540
0.60277689
0.60277420
0.60299593
0.60310793
0.60308278
0.60296273
0.60278720
0.60292095
0.60311115
INFO - Training [58][   60/  196]   Loss 0.385059   Top1 86.621094   Top5 98.736979   BatchTime 0.404416   LR 0.000025
0.60292691
0.60261512
0.60243213
0.60243070
0.60246325
0.60248148
0.60246569
0.60258162
0.60267568
0.60264373
0.60278487
0.60289931
0.60288888
0.60280293
0.60273659
0.60269421
0.60253191
0.60264605
0.60277951
0.60267204
0.60271430
INFO - Training [58][   80/  196]   Loss 0.382855   Top1 86.733398   Top5 98.886719   BatchTime 0.397174   LR 0.000024
0.60262895
0.60248178
0.60248995
0.60397214
0.60376775
0.60361397
0.60357851
0.60343337
0.60350543
0.60358495
0.60370815
0.60371023
0.60359877
0.60368627
0.60389495
0.60386175
0.60357511
0.60367858
0.60362953
0.60362524
0.60359889
0.60362381
INFO - Training [58][  100/  196]   Loss 0.380491   Top1 86.808594   Top5 98.882812   BatchTime 0.390224   LR 0.000024
0.60361767
0.60355544
0.60350496
0.60359550
0.60367233
0.60382849
0.60415864
0.60409439
0.60394597
0.60392582
0.60389519
0.60401624
0.60401529
0.60392106
0.60373276
0.60369343
0.60381216
0.60373986
0.60385358
0.60401088
0.60385245
0.60397559
INFO - Training [58][  120/  196]   Loss 0.373761   Top1 87.063802   Top5 98.938802   BatchTime 0.387153   LR 0.000023
0.60414976
0.60396862
0.60387641
0.60389692
0.60402572
0.60413194
0.60402286
0.60398936
0.60399234
0.60392624
0.60384262
0.60382622
0.60381222
0.60376900
0.60386640
0.60367113
INFO - Training [58][  140/  196]   Loss 0.370908   Top1 87.173549   Top5 98.964844   BatchTime 0.383910   LR 0.000023
0.60379481
0.60384119
0.60379475
0.60373205
0.60368800
0.60359007
0.60354823
0.60367441
0.60398799
0.60388732
0.60385853
0.60393381
0.60403806
0.60381240
0.60369539
0.60379386
0.60375810
0.60364568
0.60367417
0.60369474
0.60381067
INFO - Training [58][  160/  196]   Loss 0.375285   Top1 86.977539   Top5 98.942871   BatchTime 0.383364   LR 0.000023
0.60378569
0.60377270
0.60377896
0.60364741
0.60353577
0.60352153
0.60341263
0.60337293
0.60352570
0.60341650
0.60332292
0.60364759
0.60381275
0.60383880
0.60381818
0.60372406
0.60377842
0.60360295
0.60343081
0.60336429
INFO - Training [58][  180/  196]   Loss 0.376365   Top1 86.909722   Top5 98.927951   BatchTime 0.374947   LR 0.000022
0.60336101
0.60338759
0.60344863
0.60341889
0.60310084
0.60304487
0.60314232
0.60341454
0.60359067
0.60355490
0.60343742
0.60347492
0.60341758
0.60349578
0.60352826
0.60343719
********************pre-trained*****************
INFO - ==> Top1: 86.858    Top5: 98.918    Loss: 0.377
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [58][   20/   40]   Loss 0.316971   Top1 89.550781   Top5 99.609375   BatchTime 0.142119
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0951)
features.2.conv.0 tensor(0.0845)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1745)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1135)
features.4.conv.0 tensor(0.0583)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1481)
features.5.conv.0 tensor(0.2832)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.1548)
features.6.conv.0 tensor(0.0402)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0809)
features.7.conv.0 tensor(0.1674)
features.7.conv.3 tensor(0.4427)
features.7.conv.6 tensor(0.5913)
features.8.conv.0 tensor(0.5650)
features.8.conv.3 tensor(0.5339)
features.8.conv.6 tensor(0.6006)
features.9.conv.0 tensor(0.4584)
features.9.conv.3 tensor(0.5538)
features.9.conv.6 tensor(0.6640)
features.10.conv.0 tensor(0.0649)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.0856)
features.11.conv.0 tensor(0.7694)
features.11.conv.3 tensor(0.6460)
features.11.conv.6 tensor(0.7967)
features.12.conv.0 tensor(0.6746)
features.12.conv.3 tensor(0.6688)
features.12.conv.6 tensor(0.7960)
features.13.conv.0 tensor(0.2654)
features.13.conv.3 tensor(0.4886)
features.13.conv.6 tensor(0.2692)
features.14.conv.0 tensor(0.9089)
features.14.conv.3 tensor(0.8368)
features.14.conv.6 tensor(0.9523)
features.15.conv.0 tensor(0.8787)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9681)
features.16.conv.0 tensor(0.7353)
features.16.conv.3 tensor(0.7979)
features.16.conv.6 tensor(0.8826)
conv.0 tensor(0.1327)
tensor(1340438.) 2188896.0
INFO - Validation [58][   40/   40]   Loss 0.305082   Top1 89.700000   Top5 99.690000   BatchTime 0.099479
INFO - ==> Top1: 89.700    Top5: 99.690    Loss: 0.305
INFO - ==> Sparsity : 0.612
INFO - Scoreboard best 1 ==> Epoch [58][Top1: 89.700   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [57][Top1: 89.680   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [55][Top1: 89.620   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
0.60329360
0.60336310
0.60349208
0.60351813
0.60355210
0.60340703
0.60349059
0.60337466
0.60323769
0.60325742
0.60319203
0.60308492
0.60295904
0.60290372
0.60291570
0.60295635
0.60302711
0.60299158
0.60293728
INFO - Training [59][   20/  196]   Loss 0.383912   Top1 86.601562   Top5 98.437500   BatchTime 0.447142   LR 0.000022
0.60285145
0.60287315
0.60297340
0.60316366
0.60306704
0.60314590
0.60315508
0.60326564
0.60310292
0.60304117
0.60298204
0.60309607
0.60323238
0.60309714
0.60288364
0.60328430
0.60340959
0.60323447
0.60329819
0.60313338
0.60294598
INFO - Training [59][   40/  196]   Loss 0.386273   Top1 86.630859   Top5 98.632812   BatchTime 0.413769   LR 0.000021
0.60273933
0.60291189
0.60312325
0.60322726
0.60318911
0.60317171
0.60310292
0.60293823
0.60287565
0.60276943
0.60269475
0.60261309
0.60261619
0.60264367
0.60267639
0.60277200
0.60291505
0.60280925
0.60279095
0.60275531
0.60278577
INFO - Training [59][   60/  196]   Loss 0.382045   Top1 86.783854   Top5 98.743490   BatchTime 0.402352   LR 0.000021
0.60274845
0.60259634
0.60258538
0.60263544
0.60293877
0.60265893
0.60254985
0.60259628
0.60266548
0.60262895
0.60262829
0.60269523
0.60258585
0.60272521
0.60274333
0.60266906
0.60286814
0.60273325
0.60288489
0.60273463
0.60275751
0.60270739
INFO - Training [59][   80/  196]   Loss 0.381648   Top1 86.767578   Top5 98.833008   BatchTime 0.395672   LR 0.000020
0.60281485
0.60282463
0.60262400
0.60257274
0.60257071
0.60240746
0.60233676
0.60257483
0.60272241
0.60273135
0.60282326
0.60273480
0.60269159
0.60265696
0.60280108
0.60268044
0.60268742
INFO - Training [59][  100/  196]   Loss 0.373703   Top1 86.972656   Top5 98.906250   BatchTime 0.387691   LR 0.000020
0.60278153
0.60274273
0.60266203
0.60260278
0.60252571
0.60242325
0.60229808
0.60209972
0.60110098
0.60115498
0.60118777
0.60109907
0.60111231
0.60108918
0.60104245
0.60105443
0.60099959
0.60110992
0.60140586
0.60137403
INFO - Training [59][  120/  196]   Loss 0.368517   Top1 87.154948   Top5 98.948568   BatchTime 0.387969   LR 0.000020
0.60109574
0.60098660
0.60089296
0.60076243
0.60069281
0.60063803
0.60123760
0.60165793
0.60169268
0.60168087
0.60164219
0.60167539
0.60166979
0.60160702
0.60160023
0.60159683
0.60149437
0.60152054
0.60159576
0.60158885
0.60156262
0.60153806
0.60148656
INFO - Training [59][  140/  196]   Loss 0.367745   Top1 87.184710   Top5 99.001116   BatchTime 0.384606   LR 0.000019
0.60148865
0.60147262
0.60157710
0.60173774
0.60183626
0.60184860
0.60187757
0.60165989
0.60155243
0.60142809
0.60139161
0.60139322
0.60176259
0.60194552
0.60167843
0.60165340
0.60164857
0.60172272
0.60190684
0.60182494
INFO - Training [59][  160/  196]   Loss 0.371382   Top1 87.053223   Top5 98.981934   BatchTime 0.374224   LR 0.000019
0.60186553
0.60170567
0.60176545
0.60197335
0.60188389
0.60196376
0.60187733
0.60203737
0.60204530
0.60201573
0.60194945
0.60184300
0.60173988
0.60159349
0.60158885
0.60170758
0.60184407
0.60198849
0.60197234
INFO - Training [59][  180/  196]   Loss 0.372785   Top1 87.042101   Top5 98.964844   BatchTime 0.366698   LR 0.000019
0.60187054
0.60182297
0.60191309
0.60161257
0.60160881
0.60180551
0.60177606
0.60179371
0.60178435
0.60187769
0.60192478
0.60170949
0.60166395
0.60174322
********************pre-trained*****************
INFO - ==> Top1: 87.102    Top5: 98.956    Loss: 0.371
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [59][   20/   40]   Loss 0.308304   Top1 89.902344   Top5 99.648438   BatchTime 0.153897
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0898)
features.2.conv.0 tensor(0.0880)
features.2.conv.3 tensor(0.3410)
features.2.conv.6 tensor(0.1756)
features.3.conv.0 tensor(0.0579)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1126)
features.4.conv.0 tensor(0.0601)
features.4.conv.3 tensor(0.3090)
features.4.conv.6 tensor(0.1466)
features.5.conv.0 tensor(0.2874)
features.5.conv.3 tensor(0.4167)
features.5.conv.6 tensor(0.1584)
features.6.conv.0 tensor(0.0397)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0797)
features.7.conv.0 tensor(0.1673)
features.7.conv.3 tensor(0.4439)
features.7.conv.6 tensor(0.5961)
features.8.conv.0 tensor(0.5689)
features.8.conv.3 tensor(0.5333)
features.8.conv.6 tensor(0.6005)
features.9.conv.0 tensor(0.4598)
features.9.conv.3 tensor(0.5544)
features.9.conv.6 tensor(0.6638)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0866)
features.11.conv.0 tensor(0.7714)
features.11.conv.3 tensor(0.6456)
features.11.conv.6 tensor(0.7978)
features.12.conv.0 tensor(0.6761)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.7971)
features.13.conv.0 tensor(0.2628)
features.13.conv.3 tensor(0.4886)
features.13.conv.6 tensor(0.2895)
features.14.conv.0 tensor(0.9086)
features.14.conv.3 tensor(0.8368)
features.14.conv.6 tensor(0.9523)
features.15.conv.0 tensor(0.8791)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9682)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.7979)
features.16.conv.6 tensor(0.8834)
conv.0 tensor(0.1245)
tensor(1340933.) 2188896.0
INFO - Validation [59][   40/   40]   Loss 0.301256   Top1 89.820000   Top5 99.740000   BatchTime 0.103504
INFO - ==> Top1: 89.820    Top5: 99.740    Loss: 0.301
INFO - ==> Sparsity : 0.613
INFO - Scoreboard best 1 ==> Epoch [59][Top1: 89.820   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [58][Top1: 89.700   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [57][Top1: 89.680   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
0.60177547
0.60174578
0.60177547
0.60185272
0.60180867
0.60170102
0.60172457
0.60165673
0.60165119
0.60163420
0.60156173
0.60160398
0.60163814
0.60177648
0.60179788
0.60177869
0.60173553
0.60160750
0.60160303
0.60172719
INFO - Training [60][   20/  196]   Loss 0.379250   Top1 86.601562   Top5 98.300781   BatchTime 0.459617   LR 0.000018
0.60161877
0.60128784
0.60122156
0.60124284
0.60132343
0.60121554
0.60127622
0.60128570
0.60123533
0.60126454
0.60124838
0.60111988
0.60103518
0.60120785
0.60137951
0.60145909
0.60130715
0.60135800
0.60161936
0.60165620
0.60151225
0.60156727
INFO - Training [60][   40/  196]   Loss 0.373928   Top1 86.816406   Top5 98.496094   BatchTime 0.409708   LR 0.000018
0.60130554
0.60141879
0.60147458
0.60144752
0.60156405
0.60139108
0.60132128
0.60138893
0.60143739
0.60141116
0.60143077
0.60141283
0.60159022
0.60164899
0.60159069
0.60144371
INFO - Training [60][   60/  196]   Loss 0.374864   Top1 86.757812   Top5 98.600260   BatchTime 0.394298   LR 0.000017
0.60151798
0.60138625
0.60123760
0.60139632
0.60118610
0.60129100
0.60134387
0.60139358
0.60143709
0.60139710
0.60147470
0.60150617
0.60134465
0.60137147
0.60127032
0.60133880
0.60151058
0.60148317
0.60147297
0.60120779
0.60113746
INFO - Training [60][   80/  196]   Loss 0.376983   Top1 86.743164   Top5 98.745117   BatchTime 0.394352   LR 0.000017
0.60107481
0.60104465
0.60108346
0.60111254
0.60124433
0.60148734
0.60154969
0.60150695
0.60136670
0.60139251
0.60131609
0.60121971
0.60110068
0.60091829
0.60083991
0.60101521
0.60106456
0.60096025
0.60090953
0.60093087
0.60089642
0.60088086
INFO - Training [60][  100/  196]   Loss 0.370409   Top1 87.031250   Top5 98.832031   BatchTime 0.387709   LR 0.000017
0.60083151
0.60085917
0.60082883
0.60081863
0.60072273
0.60062462
0.60067570
0.60073459
0.60083574
0.60075235
0.60062277
0.60058576
0.60065418
0.60064429
0.60059178
0.60051244
0.60059482
INFO - Training [60][  120/  196]   Loss 0.366554   Top1 87.229818   Top5 98.857422   BatchTime 0.380785   LR 0.000016
0.60072201
0.60079288
0.60066658
0.60073054
0.60073519
0.60105377
0.60099733
0.60090560
0.60078746
0.60066777
0.60071665
0.60066861
0.60057980
0.60048610
0.60039103
0.60033607
0.60033572
0.60024530
0.60018098
0.60022694
0.60031003
0.60030687
0.60027570
INFO - Training [60][  140/  196]   Loss 0.364925   Top1 87.282366   Top5 98.936942   BatchTime 0.377745   LR 0.000016
0.60019368
0.60023904
0.60037637
0.60039473
0.60046589
0.60031217
0.60054082
0.60048193
0.60041904
0.60028583
0.60025686
0.60052276
0.60066521
0.60083950
0.60082906
0.60089391
0.60053515
0.60049242
0.60027385
INFO - Training [60][  160/  196]   Loss 0.366784   Top1 87.241211   Top5 98.898926   BatchTime 0.369915   LR 0.000016
0.60012126
0.60017592
0.60026497
0.60027033
0.60041273
0.60048664
0.60060745
0.60046339
0.60050571
0.60060823
0.60087609
0.60073888
0.60065144
0.60046279
0.60050833
0.60040247
0.60043335
0.60030687
INFO - Training [60][  180/  196]   Loss 0.369465   Top1 87.141927   Top5 98.882378   BatchTime 0.366963   LR 0.000015
0.60044569
0.60032326
0.60029358
0.60035467
0.60033721
0.60032338
0.60048771
0.60032564
0.60014457
0.59990817
0.59983307
0.59972930
0.59970909
0.59976238
0.59986609
0.59992826
0.60002095
0.59986407
********************pre-trained*****************
INFO - ==> Top1: 87.240    Top5: 98.894    Loss: 0.368
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [60][   20/   40]   Loss 0.310591   Top1 89.902344   Top5 99.609375   BatchTime 0.143589
INFO - Validation [60][   40/   40]   Loss 0.299850   Top1 90.100000   Top5 99.720000   BatchTime 0.098272
INFO - ==> Top1: 90.100    Top5: 99.720    Loss: 0.300
INFO - ==> Sparsity : 0.616
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.100   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 89.820   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 89.700   Top5: 99.690]
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.0842)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1733)
features.3.conv.0 tensor(0.0547)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1122)
features.4.conv.0 tensor(0.0623)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.1468)
features.5.conv.0 tensor(0.2853)
features.5.conv.3 tensor(0.4172)
features.5.conv.6 tensor(0.1782)
features.6.conv.0 tensor(0.0392)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0792)
features.7.conv.0 tensor(0.1681)
features.7.conv.3 tensor(0.4450)
features.7.conv.6 tensor(0.5971)
features.8.conv.0 tensor(0.5691)
features.8.conv.3 tensor(0.5327)
features.8.conv.6 tensor(0.6004)
features.9.conv.0 tensor(0.4598)
features.9.conv.3 tensor(0.5541)
features.9.conv.6 tensor(0.6667)
features.10.conv.0 tensor(0.0652)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0869)
features.11.conv.0 tensor(0.7741)
features.11.conv.3 tensor(0.6456)
features.11.conv.6 tensor(0.7998)
features.12.conv.0 tensor(0.6807)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.7960)
features.13.conv.0 tensor(0.2697)
features.13.conv.3 tensor(0.4882)
features.13.conv.6 tensor(0.3194)
features.14.conv.0 tensor(0.9096)
features.14.conv.3 tensor(0.8369)
features.14.conv.6 tensor(0.9524)
features.15.conv.0 tensor(0.8795)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9681)
features.16.conv.0 tensor(0.7447)
features.16.conv.3 tensor(0.7977)
features.16.conv.6 tensor(0.8845)
conv.0 tensor(0.1314)
tensor(1348352.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
0.60007465
0.60018921
0.60031939
0.60016674
0.60007113
0.59994453
0.59982991
0.59976453
0.59974873
0.59990680
0.59985369
0.59983778
0.59980232
0.59955966
0.59943748
0.59933311
0.59933704
0.59933442
0.59946519
0.59939140
INFO - Training [61][   20/  196]   Loss 0.378427   Top1 86.953125   Top5 98.300781   BatchTime 0.477829   LR 0.000015
0.59943920
0.59950602
0.59940493
0.59943360
0.59944832
0.59943336
0.59939831
0.59927189
0.59927166
0.59940773
0.59928280
0.59922361
0.59912336
0.59922653
0.59921592
0.59907591
0.59907699
0.59912354
INFO - Training [61][   40/  196]   Loss 0.383423   Top1 86.650391   Top5 98.505859   BatchTime 0.409836   LR 0.000014
0.59896189
0.59889156
0.59878778
0.59872556
0.59871048
0.59863847
0.59859812
0.59876555
0.59872067
0.59860992
0.59876353
0.59850806
0.59840292
0.59831566
0.59823483
0.59820700
0.59806591
0.59796739
0.59796131
0.59789038
0.59772468
0.59794194
INFO - Training [61][   60/  196]   Loss 0.380073   Top1 86.725260   Top5 98.645833   BatchTime 0.392765   LR 0.000014
0.59778076
0.59727883
0.59680915
0.59665811
0.59660172
0.59659600
0.59649140
0.59646302
0.59639901
0.59647065
0.59613615
0.59622711
0.59624213
0.59624535
0.59610289
0.59609973
0.59605092
0.59608060
0.59603834
0.59603745
0.59598041
0.59569985
0.59586328
INFO - Training [61][   80/  196]   Loss 0.377005   Top1 86.870117   Top5 98.745117   BatchTime 0.385127   LR 0.000014
0.59574378
0.59561473
0.59553289
0.59540898
0.59556657
0.59535462
0.59519023
0.59514952
0.59526986
0.59520364
0.59521115
0.59517515
0.59515125
0.59504545
0.59482479
0.59439558
INFO - Training [61][  100/  196]   Loss 0.371083   Top1 87.054688   Top5 98.820312   BatchTime 0.380967   LR 0.000013
0.59405583
0.59379345
0.59372324
0.59364891
0.59364176
0.59359109
0.59367657
0.59359789
0.59345037
0.59331548
0.59332442
0.59322846
0.59306514
0.59301502
0.59308368
0.59319371
0.59305495
0.59290081
0.59292030
0.59304971
INFO - Training [61][  120/  196]   Loss 0.364414   Top1 87.317708   Top5 98.916016   BatchTime 0.382524   LR 0.000013
0.59295762
0.59287769
0.59283096
0.59276330
0.59272575
0.59266812
0.59261709
0.59256083
0.59254634
0.59272891
0.59268850
0.59283793
0.59246314
0.59235203
0.59232706
0.59229845
0.59223413
0.59216893
0.59210187
0.59207308
INFO - Training [61][  140/  196]   Loss 0.361853   Top1 87.424665   Top5 98.973214   BatchTime 0.373290   LR 0.000013
0.59203774
0.59205055
0.59204304
0.59200627
0.59204060
0.59207004
0.59219742
0.59222502
0.59222806
0.59197897
0.59188855
0.59180629
0.59174848
0.59169072
0.59163481
0.59158671
0.59154338
0.59151995
0.59148633
0.59144902
INFO - Training [61][  160/  196]   Loss 0.366089   Top1 87.253418   Top5 98.979492   BatchTime 0.363367   LR 0.000012
0.59142751
0.59140164
0.59136665
0.59133339
0.59131217
0.59129769
0.59126174
0.59122616
0.59120256
0.59119469
0.59118420
0.59116781
0.59114528
0.59117407
0.59118551
0.59116548
0.59118682
0.59112597
0.59131682
INFO - Training [61][  180/  196]   Loss 0.365875   Top1 87.280816   Top5 98.967014   BatchTime 0.357614   LR 0.000012
0.59160143
0.59144491
0.59118587
0.59117073
0.59114164
0.59110779
0.59103698
0.59099680
0.59094030
0.59087896
0.59084225
0.59088790
0.59090680
0.59091371
0.59088492
0.59084374
0.59092760
0.59097201
********************pre-trained*****************
INFO - ==> Top1: 87.336    Top5: 98.966    Loss: 0.365
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [61][   20/   40]   Loss 0.306530   Top1 90.292969   Top5 99.570312   BatchTime 0.151949
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0443)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0898)
features.2.conv.0 tensor(0.0842)
features.2.conv.3 tensor(0.3410)
features.2.conv.6 tensor(0.1753)
features.3.conv.0 tensor(0.0553)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1204)
features.4.conv.0 tensor(0.0614)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1484)
features.5.conv.0 tensor(0.2852)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.1903)
features.6.conv.0 tensor(0.0387)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0801)
features.7.conv.0 tensor(0.1678)
features.7.conv.3 tensor(0.4447)
features.7.conv.6 tensor(0.5987)
features.8.conv.0 tensor(0.5725)
features.8.conv.3 tensor(0.5327)
features.8.conv.6 tensor(0.6010)
features.9.conv.0 tensor(0.4607)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6674)
features.10.conv.0 tensor(0.0647)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0877)
features.11.conv.0 tensor(0.7763)
features.11.conv.3 tensor(0.6462)
features.11.conv.6 tensor(0.8014)
features.12.conv.0 tensor(0.6823)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.7968)
features.13.conv.0 tensor(0.2721)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.4282)
features.14.conv.0 tensor(0.9096)
features.14.conv.3 tensor(0.8368)
features.14.conv.6 tensor(0.9523)
features.15.conv.0 tensor(0.8797)
features.15.conv.3 tensor(0.8231)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7505)
features.16.conv.3 tensor(0.7977)
features.16.conv.6 tensor(0.8865)
conv.0 tensor(0.1327)
tensor(1361210.) 2188896.0
INFO - Validation [61][   40/   40]   Loss 0.297894   Top1 90.300000   Top5 99.690000   BatchTime 0.104001
INFO - ==> Top1: 90.300    Top5: 99.690    Loss: 0.298
INFO - ==> Sparsity : 0.622
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 90.100   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [59][Top1: 89.820   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
0.59115380
0.59128934
0.59124607
0.59116697
0.59115124
0.59113830
0.59096301
0.59087270
0.59097445
0.59120697
0.59119982
0.59121621
0.59101754
0.59107465
0.59103614
0.59093612
0.59084278
0.59080505
0.59078646
0.59075886
INFO - Training [62][   20/  196]   Loss 0.386617   Top1 86.113281   Top5 98.554688   BatchTime 0.456692   LR 0.000012
0.59092331
0.59098059
0.59080261
0.59077394
0.59081984
0.59074861
0.59067434
0.59078431
0.59105849
0.59087068
0.59086525
0.59102201
0.59079194
0.59057271
0.59054172
0.59055877
0.59052020
0.59047848
0.59050304
0.59057593
0.59057653
INFO - Training [62][   40/  196]   Loss 0.392172   Top1 86.250000   Top5 98.535156   BatchTime 0.426867   LR 0.000011
0.59061098
0.59077120
0.59083009
0.59067160
0.59056622
0.59048289
0.59043813
0.59040314
0.59036177
0.59032136
0.59030938
0.59030592
0.59029138
0.59026557
0.59022272
0.59019250
0.59016001
0.59017175
0.59019923
0.59021991
0.59020495
INFO - Training [62][   60/  196]   Loss 0.392680   Top1 86.406250   Top5 98.652344   BatchTime 0.409544   LR 0.000011
0.59021688
0.59019512
0.59023374
0.59062493
0.59060329
0.59063214
0.59063697
0.59049201
0.59034121
0.59030330
0.59045559
0.59051377
0.59039241
0.59033149
0.59036070
0.59054375
INFO - Training [62][   80/  196]   Loss 0.384869   Top1 86.665039   Top5 98.793945   BatchTime 0.397722   LR 0.000011
0.59063888
0.59045166
0.59025586
0.59027421
0.59040719
0.59045392
0.59047991
0.59041160
0.59054941
0.59042829
0.59030253
0.59040791
0.59040695
0.59038299
0.59039670
0.59050804
0.59016830
0.59027159
0.59030372
0.59031385
0.59034139
0.59031576
0.59023958
INFO - Training [62][  100/  196]   Loss 0.375890   Top1 87.007812   Top5 98.843750   BatchTime 0.389937   LR 0.000011
0.59044552
0.59030497
0.59028631
0.59036219
0.59036756
0.59046805
0.59047019
0.59040517
0.59034961
0.59038198
0.59030610
0.59025198
0.59027374
0.59036499
0.59044552
0.59033328
0.59027845
0.59018648
0.59019679
0.59019512
0.59017229
0.59009057
INFO - Training [62][  120/  196]   Loss 0.370121   Top1 87.220052   Top5 98.932292   BatchTime 0.385388   LR 0.000010
0.59005696
0.58989620
0.58986169
0.58992970
0.58991534
0.58993071
0.58991706
0.58991677
0.58995301
0.59024841
0.59030622
0.59002048
0.59007436
0.59014302
0.59019357
0.59020668
0.59016383
0.59004045
INFO - Training [62][  140/  196]   Loss 0.368451   Top1 87.246094   Top5 98.950893   BatchTime 0.378145   LR 0.000010
0.59013873
0.59015197
0.59009755
0.59001940
0.58989573
0.58986050
0.58981025
0.58972627
0.58963430
0.58959776
0.58942032
0.58969414
0.58983856
0.58957106
0.58931410
0.58891296
0.58836466
0.58823544
0.58813024
0.58802640
INFO - Training [62][  160/  196]   Loss 0.369646   Top1 87.202148   Top5 98.942871   BatchTime 0.368749   LR 0.000010
0.58795780
0.58796155
0.58797139
0.58794385
0.58793449
0.58791089
0.58789480
0.58787638
0.58786267
0.58784878
0.58783305
0.58782333
0.58781761
0.58781129
0.58780700
0.58780164
0.58779907
0.58779514
0.58779281
0.58778447
INFO - Training [62][  180/  196]   Loss 0.369683   Top1 87.180990   Top5 98.923611   BatchTime 0.360954   LR 0.000009
0.58777475
0.58776945
0.58776534
0.58775544
0.58774841
0.58773309
0.58771944
0.58770561
0.58769417
0.58768576
0.58767164
0.58766884
0.58765692
0.58764726
0.58763558
INFO - ==> Top1: 87.168    Top5: 98.942    Loss: 0.370
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [62][   20/   40]   Loss 0.306163   Top1 89.960938   Top5 99.628906   BatchTime 0.142642
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0924)
features.2.conv.0 tensor(0.0842)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1782)
features.3.conv.0 tensor(0.0553)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1168)
features.4.conv.0 tensor(0.0599)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.1506)
features.5.conv.0 tensor(0.2879)
features.5.conv.3 tensor(0.4167)
features.5.conv.6 tensor(0.1999)
features.6.conv.0 tensor(0.0407)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.1676)
features.7.conv.3 tensor(0.4442)
features.7.conv.6 tensor(0.5993)
features.8.conv.0 tensor(0.5714)
features.8.conv.3 tensor(0.5324)
features.8.conv.6 tensor(0.6024)
features.9.conv.0 tensor(0.4587)
features.9.conv.3 tensor(0.5544)
features.9.conv.6 tensor(0.6681)
features.10.conv.0 tensor(0.0642)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0881)
features.11.conv.0 tensor(0.7781)
features.11.conv.3 tensor(0.6462)
features.11.conv.6 tensor(0.8016)
features.12.conv.0 tensor(0.6834)
features.12.conv.3 tensor(0.6671)
features.12.conv.6 tensor(0.7968)
features.13.conv.0 tensor(0.2690)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.4558)
features.14.conv.0 tensor(0.9094)
features.14.conv.3 tensor(0.8366)
features.14.conv.6 tensor(0.9524)
features.15.conv.0 tensor(0.8801)
features.15.conv.3 tensor(0.8229)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7495)
features.16.conv.3 tensor(0.7978)
features.16.conv.6 tensor(0.8877)
conv.0 tensor(0.1337)
tensor(1364470.) 2188896.0
INFO - Validation [62][   40/   40]   Loss 0.296091   Top1 90.220000   Top5 99.740000   BatchTime 0.099077
INFO - ==> Top1: 90.220    Top5: 99.740    Loss: 0.296
INFO - ==> Sparsity : 0.623
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.220   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [60][Top1: 90.100   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
0.58762234
0.58760744
0.58760262
0.58758980
0.58758694
0.58758342
0.58757800
0.58757550
0.58757216
0.58756900
0.58756268
0.58755606
0.58755046
0.58754551
0.58753598
0.58753049
0.58752292
0.58751869
0.58751166
0.58750129
INFO - Training [63][   20/  196]   Loss 0.383415   Top1 86.699219   Top5 98.671875   BatchTime 0.486015   LR 0.000009
0.58749390
0.58748627
0.58748114
0.58747756
0.58746785
0.58745182
0.58744621
0.58743793
0.58742911
0.58742416
0.58741808
0.58741051
0.58739525
0.58738291
0.58737195
0.58735913
0.58734959
0.58734208
0.58733088
0.58732134
0.58730644
0.58729219
0.58727264
INFO - Training [63][   40/  196]   Loss 0.384125   Top1 86.630859   Top5 98.828125   BatchTime 0.421436   LR 0.000009
0.58725220
0.58723825
0.58722174
0.58720356
0.58717954
0.58717030
0.58715934
0.58714437
0.58712399
0.58710617
0.58708155
0.58706939
0.58705848
0.58703810
0.58701855
0.58699000
INFO - Training [63][   60/  196]   Loss 0.375598   Top1 86.835938   Top5 98.854167   BatchTime 0.404100   LR 0.000008
0.58695549
0.58691508
0.58686113
0.58680028
0.58674443
0.58670276
0.58663446
0.58655411
0.58642018
0.58621371
0.58597720
0.58575451
0.58564222
0.58561230
0.58560693
0.58560419
0.58559954
0.58560097
0.58559799
0.58558881
0.58557463
INFO - Training [63][   80/  196]   Loss 0.375626   Top1 86.826172   Top5 98.881836   BatchTime 0.396774   LR 0.000008
0.58555216
0.58553022
0.58551812
0.58549529
0.58547646
0.58545482
0.58543533
0.58542371
0.58540994
0.58539748
0.58538330
0.58537102
0.58534610
0.58532101
0.58530772
0.58528924
0.58527362
0.58525604
0.58524847
0.58523166
0.58522177
INFO - Training [63][  100/  196]   Loss 0.365876   Top1 87.257812   Top5 98.906250   BatchTime 0.393852   LR 0.000008
0.58520496
0.58518559
0.58516014
0.58512837
0.58510268
0.58507991
0.58504307
0.58499867
0.58494717
0.58489382
0.58484924
0.58481354
0.58478379
0.58475232
0.58472818
0.58473170
0.58473253
0.58472264
0.58471757
0.58471352
0.58470494
INFO - Training [63][  120/  196]   Loss 0.362090   Top1 87.412109   Top5 98.935547   BatchTime 0.393146   LR 0.000008
0.58468473
0.58465999
0.58463556
0.58461231
0.58459002
0.58456200
0.58453292
0.58450282
0.58447224
0.58445346
0.58443260
0.58441812
0.58439475
0.58437389
0.58435476
0.58433580
INFO - Training [63][  140/  196]   Loss 0.361485   Top1 87.413504   Top5 99.003906   BatchTime 0.390114   LR 0.000007
0.58431870
0.58431208
0.58429313
0.58429170
0.58428776
0.58428305
0.58427584
0.58426148
0.58425581
0.58425194
0.58423930
0.58423561
0.58423090
0.58422089
0.58421451
0.58420408
0.58419359
0.58418393
0.58418149
0.58417839
INFO - Training [63][  160/  196]   Loss 0.362810   Top1 87.365723   Top5 98.977051   BatchTime 0.379169   LR 0.000007
0.58417284
0.58416212
0.58415496
0.58414894
0.58414268
0.58413440
0.58412629
0.58411980
0.58410859
0.58409274
0.58407795
0.58406788
0.58405644
0.58404112
0.58402860
0.58401829
0.58399844
0.58398473
0.58396626
0.58393717
0.58391482
0.58389175
INFO - Training [63][  180/  196]   Loss 0.362929   Top1 87.315538   Top5 98.956163   BatchTime 0.366933   LR 0.000007
0.58386379
0.58382958
0.58379769
0.58377379
0.58373767
0.58370817
0.58367294
0.58364558
0.58363235
0.58363146
0.58362478
0.58361137
0.58360368
0.58360380
0.58360213
INFO - ==> Top1: 87.348    Top5: 98.950    Loss: 0.362
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.58360434
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [63][   20/   40]   Loss 0.308181   Top1 89.921875   Top5 99.648438   BatchTime 0.145526
INFO - Validation [63][   40/   40]   Loss 0.298435   Top1 90.190000   Top5 99.750000   BatchTime 0.100275
INFO - ==> Top1: 90.190    Top5: 99.750    Loss: 0.298
INFO - ==> Sparsity : 0.625
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.220   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [63][Top1: 90.190   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1973)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0933)
features.2.conv.0 tensor(0.0845)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.1800)
features.3.conv.0 tensor(0.0558)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1196)
features.4.conv.0 tensor(0.0599)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1494)
features.5.conv.0 tensor(0.2887)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.2116)
features.6.conv.0 tensor(0.0386)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0793)
features.7.conv.0 tensor(0.1721)
features.7.conv.3 tensor(0.4436)
features.7.conv.6 tensor(0.5988)
features.8.conv.0 tensor(0.5738)
features.8.conv.3 tensor(0.5330)
features.8.conv.6 tensor(0.6029)
features.9.conv.0 tensor(0.4607)
features.9.conv.3 tensor(0.5544)
features.9.conv.6 tensor(0.6681)
features.10.conv.0 tensor(0.0645)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0881)
features.11.conv.0 tensor(0.7776)
features.11.conv.3 tensor(0.6458)
features.11.conv.6 tensor(0.8022)
features.12.conv.0 tensor(0.6862)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.7977)
features.13.conv.0 tensor(0.2752)
features.13.conv.3 tensor(0.4888)
features.13.conv.6 tensor(0.4722)
features.14.conv.0 tensor(0.9099)
features.14.conv.3 tensor(0.8367)
features.14.conv.6 tensor(0.9525)
features.15.conv.0 tensor(0.8804)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7522)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8890)
conv.0 tensor(0.1355)
tensor(1368553.) 2188896.0
0.58361119
0.58360291
0.58359593
0.58358550
0.58357579
0.58356971
0.58356243
0.58355016
0.58354050
0.58353668
0.58352923
0.58352596
0.58351654
0.58351296
0.58351111
0.58351082
0.58350402
0.58349568
0.58347803
0.58346695
INFO - Training [64][   20/  196]   Loss 0.392933   Top1 86.054688   Top5 98.359375   BatchTime 0.459884   LR 0.000007
0.58346534
0.58345848
0.58345515
0.58344561
0.58344513
0.58344513
0.58344364
0.58344382
0.58343947
0.58344072
0.58343202
0.58342296
0.58341879
0.58340973
0.58340985
0.58340323
0.58339691
0.58338290
0.58337605
0.58337003
0.58336908
INFO - Training [64][   40/  196]   Loss 0.392961   Top1 86.240234   Top5 98.632812   BatchTime 0.428388   LR 0.000006
0.58336318
0.58335555
0.58335376
0.58334690
0.58334529
0.58334082
0.58333397
0.58332837
0.58332759
0.58332086
0.58331633
0.58331090
0.58330506
0.58330864
0.58330190
0.58329529
0.58328873
0.58328271
0.58327603
0.58327264
INFO - Training [64][   60/  196]   Loss 0.388290   Top1 86.627604   Top5 98.652344   BatchTime 0.414091   LR 0.000006
0.58327436
0.58326834
0.58325696
0.58325505
0.58325845
0.58324879
0.58324486
0.58323640
0.58323950
0.58323860
0.58323467
0.58323026
0.58322930
0.58322763
0.58322859
0.58322293
0.58322108
0.58321047
0.58321249
0.58320719
0.58320743
INFO - Training [64][   80/  196]   Loss 0.376540   Top1 87.089844   Top5 98.764648   BatchTime 0.406970   LR 0.000006
0.58320540
0.58320290
0.58319950
0.58319736
0.58319139
0.58318925
0.58318734
0.58318406
0.58318365
0.58317620
0.58316666
0.58315504
0.58314693
0.58314186
0.58313280
0.58312643
0.58312225
INFO - Training [64][  100/  196]   Loss 0.370703   Top1 87.281250   Top5 98.843750   BatchTime 0.398538   LR 0.000006
0.58311790
0.58311450
0.58311212
0.58310848
0.58310443
0.58310443
0.58309907
0.58309782
0.58309513
0.58309621
0.58309329
0.58308583
0.58308268
0.58308017
0.58307457
0.58307087
0.58306903
0.58306962
0.58306414
0.58306527
0.58305705
INFO - Training [64][  120/  196]   Loss 0.363092   Top1 87.516276   Top5 98.916016   BatchTime 0.394969   LR 0.000006
0.58305603
0.58304840
0.58304352
0.58304477
0.58303767
0.58303952
0.58303505
0.58303201
0.58303112
0.58302510
0.58302021
0.58302164
0.58302081
0.58302099
0.58302224
0.58302498
0.58302051
0.58301336
0.58300841
0.58300829
0.58300793
INFO - Training [64][  140/  196]   Loss 0.359806   Top1 87.664621   Top5 98.956473   BatchTime 0.394640   LR 0.000005
0.58300567
0.58299792
0.58299011
0.58298731
0.58298451
0.58298308
0.58297586
0.58296686
0.58296037
0.58295417
0.58295149
0.58294177
0.58293581
0.58293205
0.58293319
0.58293366
0.58292478
0.58292049
0.58291847
0.58291966
0.58291203
0.58291012
INFO - Training [64][  160/  196]   Loss 0.362609   Top1 87.531738   Top5 98.928223   BatchTime 0.391355   LR 0.000005
0.58290786
0.58290988
0.58290613
0.58289611
0.58289057
0.58288616
0.58288503
0.58288431
0.58287758
0.58287287
0.58286560
0.58285642
0.58285439
0.58285040
0.58284038
0.58283067
0.58282608
0.58281732
0.58281326
0.58280647
INFO - Training [64][  180/  196]   Loss 0.362777   Top1 87.495660   Top5 98.921441   BatchTime 0.382019   LR 0.000005
0.58280158
0.58279341
0.58278781
0.58278006
0.58276492
0.58275223
0.58274436
0.58273369
0.58271927
0.58270711
0.58269292
0.58267748
0.58266020
********************pre-trained*****************
INFO - ==> Top1: 87.506    Top5: 98.922    Loss: 0.362
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [64][   20/   40]   Loss 0.313992   Top1 89.414062   Top5 99.531250   BatchTime 0.164622
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.1973)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0938)
features.2.conv.0 tensor(0.0880)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.1803)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1181)
features.4.conv.0 tensor(0.0597)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.1532)
features.5.conv.0 tensor(0.2905)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.2158)
features.6.conv.0 tensor(0.0391)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0793)
features.7.conv.0 tensor(0.1783)
features.7.conv.3 tensor(0.4436)
features.7.conv.6 tensor(0.5992)
features.8.conv.0 tensor(0.5742)
features.8.conv.3 tensor(0.5321)
features.8.conv.6 tensor(0.6040)
features.9.conv.0 tensor(0.4651)
features.9.conv.3 tensor(0.5541)
features.9.conv.6 tensor(0.6684)
features.10.conv.0 tensor(0.0645)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0880)
features.11.conv.0 tensor(0.7778)
features.11.conv.3 tensor(0.6458)
features.11.conv.6 tensor(0.8034)
features.12.conv.0 tensor(0.6874)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.7982)
features.13.conv.0 tensor(0.2800)
features.13.conv.3 tensor(0.4898)
features.13.conv.6 tensor(0.4770)
features.14.conv.0 tensor(0.9099)
features.14.conv.3 tensor(0.8365)
features.14.conv.6 tensor(0.9525)
features.15.conv.0 tensor(0.8806)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7538)
features.16.conv.3 tensor(0.7976)
features.16.conv.6 tensor(0.8907)
conv.0 tensor(0.1362)
tensor(1370899.) 2188896.0
INFO - Validation [64][   40/   40]   Loss 0.305029   Top1 89.700000   Top5 99.700000   BatchTime 0.109677
INFO - ==> Top1: 89.700    Top5: 99.700    Loss: 0.305
INFO - ==> Sparsity : 0.626
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.220   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [63][Top1: 90.190   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
0.58263409
0.58261329
0.58259881
0.58257920
0.58256400
0.58254635
0.58252716
0.58251047
0.58249331
0.58247417
0.58245862
0.58244604
0.58243191
0.58241391
0.58240277
0.58239740
0.58239216
0.58238447
INFO - Training [65][   20/  196]   Loss 0.373099   Top1 86.835938   Top5 98.710938   BatchTime 0.511662   LR 0.000005
0.58238220
0.58238369
0.58237928
0.58238333
0.58238816
0.58238524
0.58238214
0.58238000
0.58237898
0.58238029
0.58237422
0.58237231
0.58237064
0.58237255
0.58237261
0.58236730
0.58235937
0.58235258
0.58235246
0.58235127
0.58234984
0.58234292
INFO - Training [65][   40/  196]   Loss 0.379405   Top1 86.601562   Top5 98.701172   BatchTime 0.443710   LR 0.000004
0.58233899
0.58234167
0.58233696
0.58232945
0.58232307
0.58232015
0.58231467
0.58230573
0.58230203
0.58229619
0.58229774
0.58229744
0.58229363
0.58229518
0.58230114
0.58230191
0.58229899
0.58229774
0.58230019
0.58229816
0.58229804
INFO - Training [65][   60/  196]   Loss 0.375233   Top1 86.738281   Top5 98.750000   BatchTime 0.419224   LR 0.000004
0.58229727
0.58229226
0.58229131
0.58228636
0.58228260
0.58228052
0.58227789
0.58227324
0.58227426
0.58227313
0.58227223
0.58226752
0.58226520
0.58226520
0.58226746
0.58226305
0.58225924
INFO - Training [65][   80/  196]   Loss 0.370989   Top1 87.016602   Top5 98.867188   BatchTime 0.404261   LR 0.000004
0.58225858
0.58225131
0.58225071
0.58224702
0.58224577
0.58223963
0.58223212
0.58222848
0.58222055
0.58221698
0.58221585
0.58221275
0.58221596
0.58221048
0.58221078
0.58220726
0.58220214
0.58220392
0.58220232
0.58220088
0.58219075
INFO - Training [65][  100/  196]   Loss 0.365293   Top1 87.230469   Top5 98.902344   BatchTime 0.399022   LR 0.000004
0.58218563
0.58217812
0.58217508
0.58217138
0.58217061
0.58217037
0.58217186
0.58216769
0.58216333
0.58215630
0.58215654
0.58215106
0.58215052
0.58214515
0.58214617
0.58214337
0.58214176
0.58213842
0.58213896
0.58213520
0.58213723
0.58213192
INFO - Training [65][  120/  196]   Loss 0.359420   Top1 87.473958   Top5 98.964844   BatchTime 0.395650   LR 0.000004
0.58213341
0.58213437
0.58213460
0.58213204
0.58212864
0.58213216
0.58212847
0.58212548
0.58212090
0.58211583
0.58211601
0.58211267
0.58211124
0.58210534
0.58210665
0.58210522
0.58210641
0.58209968
0.58210111
0.58209455
0.58209050
INFO - Training [65][  140/  196]   Loss 0.358926   Top1 87.516741   Top5 98.998326   BatchTime 0.392084   LR 0.000004
0.58209103
0.58209270
0.58209127
0.58208728
0.58208489
0.58207959
0.58207381
0.58207095
0.58206511
0.58206040
0.58205467
0.58205140
0.58204663
0.58204013
0.58203882
0.58203495
INFO - Training [65][  160/  196]   Loss 0.360915   Top1 87.480469   Top5 98.994141   BatchTime 0.389874   LR 0.000003
0.58203304
0.58202857
0.58202595
0.58202821
0.58202708
0.58202517
0.58202738
0.58202654
0.58202189
0.58201820
0.58202618
0.58202457
0.58201474
0.58201486
0.58201486
0.58201438
0.58200884
0.58200407
0.58200204
0.58199668
0.58199137
0.58199066
0.58199108
0.58198625
INFO - Training [65][  180/  196]   Loss 0.359948   Top1 87.523872   Top5 98.958333   BatchTime 0.383090   LR 0.000003
0.58198237
0.58198017
0.58197677
0.58197474
0.58197367
0.58197254
0.58197141
0.58196777
0.58196777
0.58196598
0.58196688
0.58196127
0.58196080
INFO - ==> Top1: 87.540    Top5: 98.976    Loss: 0.360
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.58195716
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [65][   20/   40]   Loss 0.304027   Top1 89.902344   Top5 99.628906   BatchTime 0.176601
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.1973)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0938)
features.2.conv.0 tensor(0.0871)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1846)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1185)
features.4.conv.0 tensor(0.0602)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.1540)
features.5.conv.0 tensor(0.2892)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.2183)
features.6.conv.0 tensor(0.0392)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0789)
features.7.conv.0 tensor(0.1805)
features.7.conv.3 tensor(0.4439)
features.7.conv.6 tensor(0.5998)
features.8.conv.0 tensor(0.5758)
features.8.conv.3 tensor(0.5324)
features.8.conv.6 tensor(0.6041)
features.9.conv.0 tensor(0.4669)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6687)
features.10.conv.0 tensor(0.0647)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0881)
features.11.conv.0 tensor(0.7782)
features.11.conv.3 tensor(0.6460)
features.11.conv.6 tensor(0.8039)
features.12.conv.0 tensor(0.6874)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.7981)
features.13.conv.0 tensor(0.2801)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.4794)
features.14.conv.0 tensor(0.9099)
features.14.conv.3 tensor(0.8365)
features.14.conv.6 tensor(0.9525)
features.15.conv.0 tensor(0.8807)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7546)
features.16.conv.3 tensor(0.7973)
features.16.conv.6 tensor(0.8914)
conv.0 tensor(0.1361)
tensor(1371671.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.296207   Top1 90.100000   Top5 99.750000   BatchTime 0.123553
INFO - ==> Top1: 90.100    Top5: 99.750    Loss: 0.296
INFO - ==> Sparsity : 0.627
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.220   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [63][Top1: 90.190   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
0.58195001
0.58194953
0.58194888
0.58194619
0.58194667
0.58194399
0.58194661
0.58194226
0.58193886
0.58193946
0.58193219
0.58193201
0.58193356
0.58192593
0.58192652
0.58191848
0.58191961
0.58191872
0.58191919
0.58191758
0.58191198
0.58191210
INFO - Training [66][   20/  196]   Loss 0.375417   Top1 86.503906   Top5 98.652344   BatchTime 0.468717   LR 0.000003
0.58190781
0.58190441
0.58190536
0.58189785
0.58189541
0.58189219
0.58188754
0.58188462
0.58188480
0.58188182
0.58187932
0.58187282
0.58186507
0.58186817
0.58186990
0.58186764
0.58186507
INFO - Training [66][   40/  196]   Loss 0.373711   Top1 86.552734   Top5 98.691406   BatchTime 0.411795   LR 0.000003
0.58185917
0.58185709
0.58185387
0.58185440
0.58185005
0.58184713
0.58184212
0.58184046
0.58183861
0.58183908
0.58183467
0.58183199
0.58183646
0.58183736
0.58183563
0.58183628
0.58183444
0.58183163
0.58182579
INFO - Training [66][   60/  196]   Loss 0.373994   Top1 86.575521   Top5 98.828125   BatchTime 0.415824   LR 0.000003
0.58182400
0.58182245
0.58181691
0.58181340
0.58181149
0.58180583
0.58180827
0.58180773
0.58180666
0.58180493
0.58180559
0.58179927
0.58179456
0.58179229
0.58179098
0.58178955
0.58178765
0.58178240
0.58178115
0.58177859
0.58178198
0.58177912
0.58177501
0.58177048
INFO - Training [66][   80/  196]   Loss 0.373612   Top1 86.679688   Top5 98.916016   BatchTime 0.420355   LR 0.000002
0.58177036
0.58177292
0.58177054
0.58177048
0.58177161
0.58176690
0.58176649
0.58176225
0.58176762
0.58176571
0.58176398
0.58176553
0.58176488
0.58175814
0.58175766
0.58175999
0.58176154
0.58175647
0.58175427
0.58175665
0.58175915
INFO - Training [66][  100/  196]   Loss 0.365020   Top1 86.992188   Top5 98.929688   BatchTime 0.411077   LR 0.000002
0.58175915
0.58175862
0.58176017
0.58175665
0.58175403
0.58174849
0.58175164
0.58174950
0.58174741
0.58174771
0.58174336
0.58174008
0.58173549
0.58173269
0.58173102
0.58172804
INFO - Training [66][  120/  196]   Loss 0.361277   Top1 87.194010   Top5 98.971354   BatchTime 0.404247   LR 0.000002
0.58172911
0.58172923
0.58172810
0.58172852
0.58172876
0.58172685
0.58172762
0.58172518
0.58172721
0.58171964
0.58172023
0.58171707
0.58171749
0.58171898
0.58171588
0.58171517
0.58171308
0.58171296
0.58171004
0.58170611
0.58170670
INFO - Training [66][  140/  196]   Loss 0.359021   Top1 87.354911   Top5 98.992746   BatchTime 0.403469   LR 0.000002
0.58170450
0.58170456
0.58170384
0.58170658
0.58170480
0.58170319
0.58170182
0.58169949
0.58169597
0.58169454
0.58169186
0.58169609
0.58169442
0.58169395
0.58169538
0.58169210
0.58169037
0.58169049
0.58169073
0.58169007
0.58169013
INFO - Training [66][  160/  196]   Loss 0.363168   Top1 87.199707   Top5 98.962402   BatchTime 0.400185   LR 0.000002
0.58169425
0.58169615
0.58169699
0.58169651
0.58169556
0.58168972
0.58168972
0.58168548
0.58168185
0.58167970
0.58167940
0.58167845
0.58167595
0.58167100
0.58166301
0.58165914
0.58165401
0.58165586
0.58165056
0.58164620
0.58164310
INFO - Training [66][  180/  196]   Loss 0.363601   Top1 87.246094   Top5 98.943142   BatchTime 0.398390   LR 0.000002
0.58163887
0.58163148
0.58163118
0.58163261
0.58162689
0.58162612
0.58162028
0.58161932
0.58161479
0.58161366
0.58161128
0.58161122
0.58161384
0.58161265
********************pre-trained*****************
INFO - ==> Top1: 87.402    Top5: 98.946    Loss: 0.361
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [66][   20/   40]   Loss 0.308858   Top1 89.765625   Top5 99.628906   BatchTime 0.183093
INFO - Validation [66][   40/   40]   Loss 0.300733   Top1 89.940000   Top5 99.770000   BatchTime 0.119627
INFO - ==> Top1: 89.940    Top5: 99.770    Loss: 0.301
INFO - ==> Sparsity : 0.627
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.220   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [63][Top1: 90.190   Top5: 99.750]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1953)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0933)
features.2.conv.0 tensor(0.0874)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1866)
features.3.conv.0 tensor(0.0564)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1181)
features.4.conv.0 tensor(0.0604)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1561)
features.5.conv.0 tensor(0.2904)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.2204)
features.6.conv.0 tensor(0.0394)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0792)
features.7.conv.0 tensor(0.1822)
features.7.conv.3 tensor(0.4436)
features.7.conv.6 tensor(0.5997)
features.8.conv.0 tensor(0.5761)
features.8.conv.3 tensor(0.5318)
features.8.conv.6 tensor(0.6036)
features.9.conv.0 tensor(0.4663)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6690)
features.10.conv.0 tensor(0.0644)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0880)
features.11.conv.0 tensor(0.7784)
features.11.conv.3 tensor(0.6462)
features.11.conv.6 tensor(0.8043)
features.12.conv.0 tensor(0.6877)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.7985)
features.13.conv.0 tensor(0.2811)
features.13.conv.3 tensor(0.4894)
features.13.conv.6 tensor(0.4814)
features.14.conv.0 tensor(0.9099)
features.14.conv.3 tensor(0.8366)
features.14.conv.6 tensor(0.9525)
features.15.conv.0 tensor(0.8808)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7549)
features.16.conv.3 tensor(0.7973)
features.16.conv.6 tensor(0.8917)
conv.0 tensor(0.1363)
tensor(1372293.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
0.58161718
0.58161521
0.58161271
0.58160710
0.58160424
0.58160526
0.58160156
0.58159977
0.58160055
0.58160192
0.58159661
0.58159763
0.58159423
0.58159781
0.58159626
0.58159000
0.58158785
0.58158576
0.58158642
0.58158618
0.58157921
INFO - Training [67][   20/  196]   Loss 0.374896   Top1 87.207031   Top5 98.574219   BatchTime 0.446219   LR 0.000002
0.58157337
0.58157432
0.58157068
0.58156550
0.58156830
0.58156306
0.58156192
0.58155888
0.58155352
0.58155078
0.58154982
0.58154798
0.58154464
0.58153778
0.58153808
0.58153427
0.58153588
0.58153284
0.58152920
0.58152896
INFO - Training [67][   40/  196]   Loss 0.367973   Top1 87.177734   Top5 98.710938   BatchTime 0.416722   LR 0.000002
0.58153278
0.58153033
0.58153069
0.58152741
0.58152837
0.58152741
0.58152223
0.58152485
0.58152544
0.58152443
0.58152592
0.58152628
0.58152616
0.58152646
0.58152497
0.58152729
0.58152860
0.58152848
0.58153111
0.58152926
0.58152670
0.58152235
INFO - Training [67][   60/  196]   Loss 0.366925   Top1 87.220052   Top5 98.860677   BatchTime 0.400419   LR 0.000001
0.58151871
0.58151448
0.58151489
0.58151221
0.58151460
0.58151400
0.58151323
0.58151299
0.58151191
0.58151394
0.58150876
0.58150941
0.58150578
0.58150464
0.58150077
0.58150053
0.58150315
0.58150107
0.58150107
0.58149922
INFO - Training [67][   80/  196]   Loss 0.365666   Top1 87.387695   Top5 98.925781   BatchTime 0.401228   LR 0.000001
0.58149600
0.58149874
0.58149379
0.58148849
0.58148873
0.58148700
0.58148682
0.58148378
0.58148640
0.58148438
0.58148181
0.58147866
0.58147955
0.58148056
0.58147210
0.58147252
INFO - Training [67][  100/  196]   Loss 0.361233   Top1 87.468750   Top5 98.960938   BatchTime 0.396965   LR 0.000001
0.58147258
0.58147353
0.58146703
0.58146596
0.58146793
0.58146894
0.58146852
0.58146358
0.58145928
0.58145761
0.58145809
0.58145767
0.58145499
0.58145124
0.58145267
0.58145076
0.58145058
0.58144772
0.58145005
0.58144718
INFO - Training [67][  120/  196]   Loss 0.358195   Top1 87.639974   Top5 99.033203   BatchTime 0.397345   LR 0.000001
0.58144975
0.58144909
0.58144706
0.58144850
0.58144861
0.58144975
0.58144492
0.58144963
0.58144963
0.58144861
0.58144796
0.58144611
0.58144134
0.58144236
0.58144367
0.58144110
0.58144033
0.58143741
0.58144075
0.58144206
0.58144009
INFO - Training [67][  140/  196]   Loss 0.358116   Top1 87.622768   Top5 99.029018   BatchTime 0.395703   LR 0.000001
0.58143783
0.58143711
0.58143830
0.58144146
0.58144063
0.58143997
0.58143592
0.58143795
0.58143806
0.58143675
0.58143651
0.58143824
0.58143741
0.58144027
0.58144152
0.58144355
0.58144528
0.58144605
0.58144575
0.58144224
0.58144295
INFO - Training [67][  160/  196]   Loss 0.358216   Top1 87.622070   Top5 99.020996   BatchTime 0.393476   LR 0.000001
0.58143806
0.58144516
0.58144444
0.58144766
0.58144897
0.58145261
0.58145165
0.58145118
0.58144963
0.58144790
0.58144492
0.58144808
0.58144957
0.58144772
0.58144861
0.58144581
0.58143920
0.58143908
0.58143687
0.58143860
0.58143592
INFO - Training [67][  180/  196]   Loss 0.357695   Top1 87.560764   Top5 99.003906   BatchTime 0.391583   LR 0.000001
0.58143252
0.58143348
0.58143264
0.58143234
0.58143240
0.58143204
0.58143169
0.58143234
0.58143181
0.58142734
0.58142626
0.58142889
0.58143097
0.58143091
********************pre-trained*****************
INFO - ==> Top1: 87.552    Top5: 99.006    Loss: 0.357
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [67][   20/   40]   Loss 0.311944   Top1 89.921875   Top5 99.609375   BatchTime 0.151410
INFO - Validation [67][   40/   40]   Loss 0.302384   Top1 90.000000   Top5 99.710000   BatchTime 0.137714
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1973)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0924)
features.2.conv.0 tensor(0.0880)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1872)
features.3.conv.0 tensor(0.0564)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1178)
features.4.conv.0 tensor(0.0605)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1561)
features.5.conv.0 tensor(0.2910)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.2212)
features.6.conv.0 tensor(0.0399)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0791)
features.7.conv.0 tensor(0.1829)
features.7.conv.3 tensor(0.4439)
features.7.conv.6 tensor(0.6000)
features.8.conv.0 tensor(0.5760)
features.8.conv.3 tensor(0.5318)
features.8.conv.6 tensor(0.6035)
features.9.conv.0 tensor(0.4669)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6693)
features.10.conv.0 tensor(0.0642)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0882)
features.11.conv.0 tensor(0.7786)
features.11.conv.3 tensor(0.6462)
features.11.conv.6 tensor(0.8045)
features.12.conv.0 tensor(0.6884)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.7984)
features.13.conv.0 tensor(0.2814)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.4819)
features.14.conv.0 tensor(0.9099)
features.14.conv.3 tensor(0.8366)
features.14.conv.6 tensor(0.9525)
features.15.conv.0 tensor(0.8808)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7554)
features.16.conv.3 tensor(0.7973)
features.16.conv.6 tensor(0.8919)
conv.0 tensor(0.1366)
tensor(1372735.) 2188896.0
INFO - ==> Top1: 90.000    Top5: 99.710    Loss: 0.302
INFO - ==> Sparsity : 0.627
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.220   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [63][Top1: 90.190   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
0.58143860
0.58143502
0.58143675
0.58144069
0.58143461
0.58143443
0.58143753
0.58143479
0.58143991
0.58144069
0.58144438
0.58144480
0.58144003
0.58144236
0.58144134
0.58143842
0.58143502
0.58143365
0.58143222
INFO - Training [68][   20/  196]   Loss 0.378567   Top1 86.347656   Top5 98.554688   BatchTime 0.467316   LR 0.000001
0.58142984
0.58142853
0.58142638
0.58142346
0.58142507
0.58142668
0.58142459
0.58142775
0.58142900
0.58142495
0.58142501
0.58142155
0.58142030
0.58142161
0.58142036
0.58141816
0.58141863
0.58141917
0.58142138
0.58142078
0.58141845
INFO - Training [68][   40/  196]   Loss 0.382699   Top1 86.337891   Top5 98.671875   BatchTime 0.418994   LR 0.000001
0.58141661
0.58141732
0.58141339
0.58141577
0.58141387
0.58141458
0.58140934
0.58141375
0.58141023
0.58141375
0.58141482
0.58141530
0.58141363
0.58141237
0.58141047
0.58140898
0.58141476
0.58141494
0.58141464
0.58141327
0.58141059
INFO - Training [68][   60/  196]   Loss 0.378083   Top1 86.751302   Top5 98.795573   BatchTime 0.407084   LR 0.000001
0.58141059
0.58141685
0.58141565
0.58141267
0.58141285
0.58141553
0.58141834
0.58141470
0.58141375
0.58141190
0.58141428
0.58141351
0.58141297
0.58140922
0.58140957
0.58140844
0.58141124
0.58140802
0.58140701
0.58140713
0.58140641
INFO - Training [68][   80/  196]   Loss 0.372523   Top1 87.055664   Top5 98.867188   BatchTime 0.401422   LR 0.000000
0.58140779
0.58140612
0.58140206
0.58139861
0.58139688
0.58139825
0.58139485
0.58139342
0.58139521
0.58139342
0.58139324
0.58139026
0.58138883
0.58138859
0.58138955
0.58138782
0.58138657
0.58138520
0.58138317
0.58137852
0.58138329
INFO - Training [68][  100/  196]   Loss 0.363423   Top1 87.351562   Top5 98.933594   BatchTime 0.397030   LR 0.000000
0.58138108
0.58137667
0.58137918
0.58138090
0.58137959
0.58138287
0.58138901
0.58138609
0.58138710
0.58138710
0.58138740
0.58138591
0.58138692
0.58138478
0.58138466
0.58138645
0.58138710
0.58138251
0.58138269
INFO - Training [68][  120/  196]   Loss 0.354812   Top1 87.701823   Top5 99.013672   BatchTime 0.400066   LR 0.000000
0.58137923
0.58137578
0.58137590
0.58137512
0.58137560
0.58137405
0.58137530
0.58137596
0.58137876
0.58137661
0.58137387
0.58137643
0.58137625
0.58137506
0.58137578
0.58137518
0.58137941
0.58137500
0.58137232
0.58137316
0.58137220
INFO - Training [68][  140/  196]   Loss 0.351768   Top1 87.753906   Top5 99.079241   BatchTime 0.398485   LR 0.000000
0.58137691
0.58137560
0.58137679
0.58137727
0.58137929
0.58137858
0.58137637
0.58137792
0.58137780
0.58137685
0.58137733
0.58137941
0.58137894
0.58137894
0.58137757
0.58137757
INFO - Training [68][  160/  196]   Loss 0.358451   Top1 87.534180   Top5 99.045410   BatchTime 0.397494   LR 0.000000
0.58137971
0.58137679
0.58138049
0.58137971
0.58138061
0.58138150
0.58138537
0.58139133
0.58138883
0.58139139
0.58138943
0.58138883
0.58138645
0.58138704
0.58138543
0.58138591
0.58138531
0.58138508
0.58138478
0.58138525
0.58138049
INFO - Training [68][  180/  196]   Loss 0.361653   Top1 87.415365   Top5 98.999566   BatchTime 0.395615   LR 0.000000
0.58138049
0.58138061
0.58138901
0.58138335
0.58138138
0.58138222
0.58137733
0.58137751
0.58137524
0.58137280
0.58137059
0.58137167
0.58136988
0.58137679
0.58138126
INFO - ==> Top1: 87.484    Top5: 98.996    Loss: 0.360
0.58138233
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [68][   20/   40]   Loss 0.307889   Top1 89.804688   Top5 99.628906   BatchTime 0.245033
INFO - Validation [68][   40/   40]   Loss 0.298554   Top1 90.150000   Top5 99.750000   BatchTime 0.156853
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1953)
features.1.conv.0 tensor(0.0449)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0924)
features.2.conv.0 tensor(0.0865)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1881)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1178)
features.4.conv.0 tensor(0.0605)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1562)
features.5.conv.0 tensor(0.2909)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.2215)
features.6.conv.0 tensor(0.0400)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0791)
features.7.conv.0 tensor(0.1834)
features.7.conv.3 tensor(0.4436)
features.7.conv.6 tensor(0.6000)
features.8.conv.0 tensor(0.5761)
features.8.conv.3 tensor(0.5318)
features.8.conv.6 tensor(0.6036)
features.9.conv.0 tensor(0.4671)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6694)
features.10.conv.0 tensor(0.0642)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0884)
features.11.conv.0 tensor(0.7787)
features.11.conv.3 tensor(0.6462)
features.11.conv.6 tensor(0.8046)
features.12.conv.0 tensor(0.6886)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.7985)
features.13.conv.0 tensor(0.2815)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.4824)
features.14.conv.0 tensor(0.9099)
features.14.conv.3 tensor(0.8366)
features.14.conv.6 tensor(0.9525)
features.15.conv.0 tensor(0.8808)
features.15.conv.3 tensor(0.8230)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.7555)
features.16.conv.3 tensor(0.7973)
features.16.conv.6 tensor(0.8920)
conv.0 tensor(0.1367)
tensor(1372940.) 2188896.0
INFO - ==> Top1: 90.150    Top5: 99.750    Loss: 0.299
INFO - ==> Sparsity : 0.627
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 90.300   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.220   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [63][Top1: 90.190   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-070922/_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
0.58138013
0.58137727
0.58137506
0.58137655
0.58137751
0.58137602
0.58138102
0.58138543
0.58138520
0.58138520
0.58138710
0.58138216
0.58138204
0.58138543
0.58138716
0.58138973
0.58139163
0.58139038
0.58139282
0.58139360
0.58139467
0.58139092
INFO - Training [69][   20/  196]   Loss 0.368313   Top1 87.324219   Top5 98.652344   BatchTime 0.478007   LR 0.000000
0.58138752
0.58138508
0.58138537
0.58138669
0.58138484
0.58138311
0.58138472
0.58138490
0.58138400
0.58138478
0.58138412
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fddefb15c10>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1466, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1430, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    optimizer, lr_scheduler, args.epochs, monitors, args, init_qparams = False, hard_pruning = True)
  File "main_slsq.py", line 77, in main
    logger.info(('Optimizer: %s' % optimizer).replace('\n', '\n' + ' ' * 11))
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 186, in train_one_epoch_slsq
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt