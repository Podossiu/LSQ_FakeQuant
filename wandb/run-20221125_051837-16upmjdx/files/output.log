Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.0005
               maximize: False
               weight_decay: 0.0005
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.0005
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
tensor(1.3929, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.2812, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.3346, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.1696, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.2479, device='cuda:0', grad_fn=<AddBackward0>)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
tensor(1.0232, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.0863, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.1042, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.00000000
tensor(1.2059, device='cuda:0', grad_fn=<AddBackward0>)
0.96274340
tensor(1.6089, device='cuda:0', grad_fn=<AddBackward0>)
0.96293885
tensor(1.4970, device='cuda:0', grad_fn=<AddBackward0>)
0.96094739
tensor(1.4719, device='cuda:0', grad_fn=<AddBackward0>)
0.94074595
tensor(1.4981, device='cuda:0', grad_fn=<AddBackward0>)
0.90003330
tensor(1.2971, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][   20/  196]   Loss 1.290304   Top1 57.480469   Top5 93.242188   BatchTime 0.296435   LR 0.000500
0.90020865
tensor(1.4316, device='cuda:0', grad_fn=<AddBackward0>)
0.89968693
tensor(1.2761, device='cuda:0', grad_fn=<AddBackward0>)
0.89587921
tensor(1.3410, device='cuda:0', grad_fn=<AddBackward0>)
0.88924956
tensor(1.2831, device='cuda:0', grad_fn=<AddBackward0>)
0.88626480
tensor(1.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.88408333
tensor(1.1879, device='cuda:0', grad_fn=<AddBackward0>)
0.88276094
tensor(1.2235, device='cuda:0', grad_fn=<AddBackward0>)
0.88158733
tensor(1.2685, device='cuda:0', grad_fn=<AddBackward0>)
0.88042051
tensor(1.3207, device='cuda:0', grad_fn=<AddBackward0>)
0.87879181
tensor(1.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.87763208
tensor(1.2803, device='cuda:0', grad_fn=<AddBackward0>)
0.87623638
tensor(1.1842, device='cuda:0', grad_fn=<AddBackward0>)
0.87547851
tensor(1.1418, device='cuda:0', grad_fn=<AddBackward0>)
0.87445801
tensor(1.1400, device='cuda:0', grad_fn=<AddBackward0>)
0.87404019
tensor(1.1907, device='cuda:0', grad_fn=<AddBackward0>)
0.87338620
tensor(1.2985, device='cuda:0', grad_fn=<AddBackward0>)
0.87304789
tensor(1.1340, device='cuda:0', grad_fn=<AddBackward0>)
0.87288636
tensor(1.3145, device='cuda:0', grad_fn=<AddBackward0>)
0.87270784
tensor(1.0468, device='cuda:0', grad_fn=<AddBackward0>)
0.87263095
tensor(1.0628, device='cuda:0', grad_fn=<AddBackward0>)
0.87284988
tensor(1.2115, device='cuda:0', grad_fn=<AddBackward0>)
0.87278879
tensor(1.1440, device='cuda:0', grad_fn=<AddBackward0>)
0.87272930
tensor(1.1552, device='cuda:0', grad_fn=<AddBackward0>)
0.87289244
tensor(1.1178, device='cuda:0', grad_fn=<AddBackward0>)
0.87275231
tensor(1.0937, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][   40/  196]   Loss 1.237710   Top1 58.554688   Top5 94.101562   BatchTime 0.272888   LR 0.000500
0.87292534
tensor(1.1847, device='cuda:0', grad_fn=<AddBackward0>)
0.87301999
tensor(0.9862, device='cuda:0', grad_fn=<AddBackward0>)
0.87317771
tensor(1.0339, device='cuda:0', grad_fn=<AddBackward0>)
0.87332159
tensor(0.9604, device='cuda:0', grad_fn=<AddBackward0>)
0.87345827
tensor(1.0686, device='cuda:0', grad_fn=<AddBackward0>)
0.87356669
tensor(1.1500, device='cuda:0', grad_fn=<AddBackward0>)
0.87388915
tensor(1.0242, device='cuda:0', grad_fn=<AddBackward0>)
0.87379891
tensor(0.9272, device='cuda:0', grad_fn=<AddBackward0>)
0.87404931
tensor(1.0118, device='cuda:0', grad_fn=<AddBackward0>)
0.87405640
tensor(1.0249, device='cuda:0', grad_fn=<AddBackward0>)
0.87413460
tensor(1.1393, device='cuda:0', grad_fn=<AddBackward0>)
0.87422389
tensor(1.0029, device='cuda:0', grad_fn=<AddBackward0>)
0.87424999
tensor(0.9634, device='cuda:0', grad_fn=<AddBackward0>)
0.87450355
tensor(1.0124, device='cuda:0', grad_fn=<AddBackward0>)
0.87440860
tensor(0.9697, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][   60/  196]   Loss 1.164539   Top1 60.950521   Top5 94.817708   BatchTime 0.266547   LR 0.000499
0.87418073
tensor(0.9433, device='cuda:0', grad_fn=<AddBackward0>)
0.87418646
tensor(0.8744, device='cuda:0', grad_fn=<AddBackward0>)
0.87412822
tensor(1.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.87381989
tensor(0.8870, device='cuda:0', grad_fn=<AddBackward0>)
0.87409186
tensor(0.9272, device='cuda:0', grad_fn=<AddBackward0>)
0.87399429
tensor(0.9715, device='cuda:0', grad_fn=<AddBackward0>)
0.87421560
tensor(0.9599, device='cuda:0', grad_fn=<AddBackward0>)
0.87418604
tensor(0.8740, device='cuda:0', grad_fn=<AddBackward0>)
0.87400585
tensor(1.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.87391102
tensor(0.9003, device='cuda:0', grad_fn=<AddBackward0>)
0.87387788
tensor(0.8871, device='cuda:0', grad_fn=<AddBackward0>)
0.87403446
tensor(0.9169, device='cuda:0', grad_fn=<AddBackward0>)
0.87391812
tensor(0.9685, device='cuda:0', grad_fn=<AddBackward0>)
0.87392658
tensor(1.0112, device='cuda:0', grad_fn=<AddBackward0>)
0.87386471
tensor(0.8412, device='cuda:0', grad_fn=<AddBackward0>)
0.87405115
tensor(0.9706, device='cuda:0', grad_fn=<AddBackward0>)
0.87382901
tensor(0.9653, device='cuda:0', grad_fn=<AddBackward0>)
0.87367362
tensor(0.9104, device='cuda:0', grad_fn=<AddBackward0>)
0.87353998
tensor(0.9684, device='cuda:0', grad_fn=<AddBackward0>)
0.87351269
tensor(0.9452, device='cuda:0', grad_fn=<AddBackward0>)
0.87352234
tensor(1.0117, device='cuda:0', grad_fn=<AddBackward0>)
0.87342489
tensor(1.0631, device='cuda:0', grad_fn=<AddBackward0>)
0.87329489
tensor(0.9245, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][   80/  196]   Loss 1.111891   Top1 62.788086   Top5 95.200195   BatchTime 0.265594   LR 0.000498
0.87354624
tensor(0.9777, device='cuda:0', grad_fn=<AddBackward0>)
0.87371910
tensor(0.8707, device='cuda:0', grad_fn=<AddBackward0>)
0.87395060
tensor(1.0387, device='cuda:0', grad_fn=<AddBackward0>)
0.87389094
tensor(0.9603, device='cuda:0', grad_fn=<AddBackward0>)
0.87388396
tensor(0.9476, device='cuda:0', grad_fn=<AddBackward0>)
0.87370414
tensor(0.9087, device='cuda:0', grad_fn=<AddBackward0>)
0.87359589
tensor(0.9736, device='cuda:0', grad_fn=<AddBackward0>)
0.87341446
tensor(0.9324, device='cuda:0', grad_fn=<AddBackward0>)
0.87337542
tensor(0.8206, device='cuda:0', grad_fn=<AddBackward0>)
0.87351155
tensor(0.9071, device='cuda:0', grad_fn=<AddBackward0>)
0.87368667
tensor(0.8177, device='cuda:0', grad_fn=<AddBackward0>)
0.87347317
tensor(0.9571, device='cuda:0', grad_fn=<AddBackward0>)
0.87328577
tensor(0.8723, device='cuda:0', grad_fn=<AddBackward0>)
0.87311661
tensor(0.8816, device='cuda:0', grad_fn=<AddBackward0>)
0.87297785
tensor(0.9007, device='cuda:0', grad_fn=<AddBackward0>)
0.87297231
tensor(0.9592, device='cuda:0', grad_fn=<AddBackward0>)
0.87304884
tensor(0.9616, device='cuda:0', grad_fn=<AddBackward0>)
0.87286782
tensor(0.9805, device='cuda:0', grad_fn=<AddBackward0>)
0.87266564
tensor(0.9196, device='cuda:0', grad_fn=<AddBackward0>)
0.87225449
tensor(0.9191, device='cuda:0', grad_fn=<AddBackward0>)
0.87242019
tensor(0.7459, device='cuda:0', grad_fn=<AddBackward0>)
0.87248689
tensor(0.9878, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][  100/  196]   Loss 1.072263   Top1 64.167969   Top5 95.566406   BatchTime 0.267399   LR 0.000497
0.87240374
tensor(1.0159, device='cuda:0', grad_fn=<AddBackward0>)
0.87209755
tensor(1.0262, device='cuda:0', grad_fn=<AddBackward0>)
0.87180763
tensor(1.0406, device='cuda:0', grad_fn=<AddBackward0>)
0.87196785
tensor(0.7859, device='cuda:0', grad_fn=<AddBackward0>)
0.87206632
tensor(0.9055, device='cuda:0', grad_fn=<AddBackward0>)
0.87176234
tensor(0.8674, device='cuda:0', grad_fn=<AddBackward0>)
0.87170476
tensor(0.8917, device='cuda:0', grad_fn=<AddBackward0>)
0.87210178
tensor(0.8933, device='cuda:0', grad_fn=<AddBackward0>)
0.87208605
tensor(1.0077, device='cuda:0', grad_fn=<AddBackward0>)
0.87218529
tensor(0.8989, device='cuda:0', grad_fn=<AddBackward0>)
0.87219393
tensor(0.8871, device='cuda:0', grad_fn=<AddBackward0>)
0.87199950
tensor(0.8682, device='cuda:0', grad_fn=<AddBackward0>)
0.87199384
tensor(0.9350, device='cuda:0', grad_fn=<AddBackward0>)
0.87225533
tensor(0.9642, device='cuda:0', grad_fn=<AddBackward0>)
0.87185884
tensor(0.8707, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][  120/  196]   Loss 1.047326   Top1 65.068359   Top5 95.768229   BatchTime 0.272214   LR 0.000495
0.87161481
tensor(0.9292, device='cuda:0', grad_fn=<AddBackward0>)
0.87178880
tensor(0.8741, device='cuda:0', grad_fn=<AddBackward0>)
0.87156236
tensor(0.8967, device='cuda:0', grad_fn=<AddBackward0>)
0.87155008
tensor(0.9065, device='cuda:0', grad_fn=<AddBackward0>)
0.87124991
tensor(0.9654, device='cuda:0', grad_fn=<AddBackward0>)
0.87102157
tensor(0.9345, device='cuda:0', grad_fn=<AddBackward0>)
0.87097180
tensor(0.9577, device='cuda:0', grad_fn=<AddBackward0>)
0.87021637
tensor(0.8013, device='cuda:0', grad_fn=<AddBackward0>)
0.86995393
tensor(0.9739, device='cuda:0', grad_fn=<AddBackward0>)
0.87044883
tensor(0.8382, device='cuda:0', grad_fn=<AddBackward0>)
0.87013966
tensor(0.8946, device='cuda:0', grad_fn=<AddBackward0>)
0.86994404
tensor(0.8251, device='cuda:0', grad_fn=<AddBackward0>)
0.87012780
tensor(0.9623, device='cuda:0', grad_fn=<AddBackward0>)
0.87015164
tensor(0.8591, device='cuda:0', grad_fn=<AddBackward0>)
0.86993980
tensor(0.9512, device='cuda:0', grad_fn=<AddBackward0>)
0.87017190
tensor(0.8583, device='cuda:0', grad_fn=<AddBackward0>)
0.87033206
tensor(0.8424, device='cuda:0', grad_fn=<AddBackward0>)
0.86999655
tensor(0.9224, device='cuda:0', grad_fn=<AddBackward0>)
0.86990196
tensor(0.8935, device='cuda:0', grad_fn=<AddBackward0>)
0.86991298
tensor(0.9234, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][  140/  196]   Loss 1.025066   Top1 65.828683   Top5 95.934710   BatchTime 0.275378   LR 0.000494
0.86980677
tensor(0.9399, device='cuda:0', grad_fn=<AddBackward0>)
0.86934161
tensor(0.7066, device='cuda:0', grad_fn=<AddBackward0>)
0.86923552
tensor(0.8604, device='cuda:0', grad_fn=<AddBackward0>)
0.86905444
tensor(0.9199, device='cuda:0', grad_fn=<AddBackward0>)
0.86894172
tensor(0.7368, device='cuda:0', grad_fn=<AddBackward0>)
0.86921173
tensor(0.8930, device='cuda:0', grad_fn=<AddBackward0>)
0.86943120
tensor(0.9015, device='cuda:0', grad_fn=<AddBackward0>)
0.86941528
tensor(0.7714, device='cuda:0', grad_fn=<AddBackward0>)
0.86922318
tensor(0.9488, device='cuda:0', grad_fn=<AddBackward0>)
0.86977243
tensor(0.8073, device='cuda:0', grad_fn=<AddBackward0>)
0.87021369
tensor(0.7483, device='cuda:0', grad_fn=<AddBackward0>)
0.86984307
tensor(0.7377, device='cuda:0', grad_fn=<AddBackward0>)
0.87006354
tensor(0.8678, device='cuda:0', grad_fn=<AddBackward0>)
0.87020695
tensor(0.8300, device='cuda:0', grad_fn=<AddBackward0>)
0.87022740
tensor(0.7800, device='cuda:0', grad_fn=<AddBackward0>)
0.87003922
tensor(0.9078, device='cuda:0', grad_fn=<AddBackward0>)
0.87009412
tensor(0.8626, device='cuda:0', grad_fn=<AddBackward0>)
0.87015241
tensor(0.7958, device='cuda:0', grad_fn=<AddBackward0>)
0.87031329
tensor(0.6967, device='cuda:0', grad_fn=<AddBackward0>)
0.87044954
tensor(0.8022, device='cuda:0', grad_fn=<AddBackward0>)
0.87041551
tensor(0.8349, device='cuda:0', grad_fn=<AddBackward0>)
0.87048507
tensor(0.7484, device='cuda:0', grad_fn=<AddBackward0>)
0.87074584
tensor(0.9632, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][  160/  196]   Loss 0.999382   Top1 66.647949   Top5 96.147461   BatchTime 0.283633   LR 0.000492
0.87072164
tensor(0.7575, device='cuda:0', grad_fn=<AddBackward0>)
0.87060404
tensor(0.7589, device='cuda:0', grad_fn=<AddBackward0>)
0.87077391
tensor(0.8870, device='cuda:0', grad_fn=<AddBackward0>)
0.87062377
tensor(0.7575, device='cuda:0', grad_fn=<AddBackward0>)
0.86904210
tensor(0.7842, device='cuda:0', grad_fn=<AddBackward0>)
0.86706591
tensor(0.9589, device='cuda:0', grad_fn=<AddBackward0>)
0.86876065
tensor(0.8875, device='cuda:0', grad_fn=<AddBackward0>)
0.87053025
tensor(0.7767, device='cuda:0', grad_fn=<AddBackward0>)
0.87072098
tensor(0.7131, device='cuda:0', grad_fn=<AddBackward0>)
0.87075794
tensor(0.7118, device='cuda:0', grad_fn=<AddBackward0>)
0.87095034
tensor(0.7820, device='cuda:0', grad_fn=<AddBackward0>)
0.87049395
tensor(0.8092, device='cuda:0', grad_fn=<AddBackward0>)
0.87020707
tensor(0.7729, device='cuda:0', grad_fn=<AddBackward0>)
0.87034404
tensor(0.8009, device='cuda:0', grad_fn=<AddBackward0>)
0.87028462
tensor(0.8441, device='cuda:0', grad_fn=<AddBackward0>)
0.87010956
tensor(0.7800, device='cuda:0', grad_fn=<AddBackward0>)
0.87008440
tensor(0.8928, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [0][  180/  196]   Loss 0.977951   Top1 67.345920   Top5 96.347656   BatchTime 0.291362   LR 0.000490
0.86963093
tensor(0.8076, device='cuda:0', grad_fn=<AddBackward0>)
0.86956203
tensor(0.7694, device='cuda:0', grad_fn=<AddBackward0>)
0.86998224
tensor(0.8276, device='cuda:0', grad_fn=<AddBackward0>)
0.87017894
tensor(0.8082, device='cuda:0', grad_fn=<AddBackward0>)
0.87022549
tensor(0.8105, device='cuda:0', grad_fn=<AddBackward0>)
0.87030023
tensor(0.7259, device='cuda:0', grad_fn=<AddBackward0>)
0.87034887
tensor(0.8536, device='cuda:0', grad_fn=<AddBackward0>)
0.87046880
tensor(0.7451, device='cuda:0', grad_fn=<AddBackward0>)
0.87066525
tensor(0.8249, device='cuda:0', grad_fn=<AddBackward0>)
0.87065887
tensor(0.8015, device='cuda:0', grad_fn=<AddBackward0>)
0.87051761
tensor(0.9064, device='cuda:0', grad_fn=<AddBackward0>)
0.87046599
tensor(0.8226, device='cuda:0', grad_fn=<AddBackward0>)
0.87043339
tensor(0.8795, device='cuda:0', grad_fn=<AddBackward0>)
0.87058407
tensor(0.7940, device='cuda:0', grad_fn=<AddBackward0>)
0.87032342
tensor(0.6851, device='cuda:0', grad_fn=<AddBackward0>)
0.87019414
tensor(0.8226, device='cuda:0', grad_fn=<AddBackward0>)
0.87034088
tensor(0.8143, device='cuda:0', grad_fn=<AddBackward0>)
0.87054336
tensor(0.7704, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 67.792    Top5: 96.486    Loss: 0.964
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87038678
tensor(0.7117, device='cuda:0', grad_fn=<AddBackward0>)
0.87020034
tensor(0.6894, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.827062   Top1 72.382812   Top5 97.558594   BatchTime 0.101313
INFO - Validation [0][   40/   40]   Loss 0.820364   Top1 72.380000   Top5 97.710000   BatchTime 0.077610
INFO - ==> Top1: 72.380    Top5: 97.710    Loss: 0.820
INFO - ==> Sparsity : 0.267
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 72.380   Top5: 97.710]
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0573)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0781)
features.2.conv.0 tensor(0.1594)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.1878)
features.3.conv.0 tensor(0.0683)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1024)
features.4.conv.0 tensor(0.1257)
features.4.conv.3 tensor(0.3148)
features.4.conv.6 tensor(0.2005)
features.5.conv.0 tensor(0.3957)
features.5.conv.3 tensor(0.4306)
features.5.conv.6 tensor(0.1138)
features.6.conv.0 tensor(0.0589)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0859)
features.7.conv.0 tensor(0.2469)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.1956)
features.8.conv.0 tensor(0.4370)
features.8.conv.3 tensor(0.5341)
features.8.conv.6 tensor(0.1344)
features.9.conv.0 tensor(0.4712)
features.9.conv.3 tensor(0.5755)
features.9.conv.6 tensor(0.1318)
features.10.conv.0 tensor(0.0871)
features.10.conv.3 tensor(0.1178)
features.10.conv.6 tensor(0.1038)
features.11.conv.0 tensor(0.5716)
features.11.conv.3 tensor(0.6453)
features.11.conv.6 tensor(0.1694)
features.12.conv.0 tensor(0.5954)
features.12.conv.3 tensor(0.6763)
features.12.conv.6 tensor(0.1614)
features.13.conv.0 tensor(0.3734)
features.13.conv.3 tensor(0.4946)
features.13.conv.6 tensor(0.0935)
features.14.conv.0 tensor(0.7710)
features.14.conv.3 tensor(0.8277)
features.14.conv.6 tensor(0.1041)
features.15.conv.0 tensor(0.7330)
features.15.conv.3 tensor(0.8323)
features.15.conv.6 tensor(0.1238)
features.16.conv.0 tensor(0.4675)
features.16.conv.3 tensor(0.8072)
features.16.conv.6 tensor(0.0578)
conv.0 tensor(0.0563)
tensor(583892.) 2188896.0
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.87033308
tensor(0.7124, device='cuda:0', grad_fn=<AddBackward0>)
0.87023300
tensor(0.6695, device='cuda:0', grad_fn=<AddBackward0>)
0.87033975
tensor(0.6875, device='cuda:0', grad_fn=<AddBackward0>)
0.86954457
tensor(0.7962, device='cuda:0', grad_fn=<AddBackward0>)
0.86931652
tensor(0.9651, device='cuda:0', grad_fn=<AddBackward0>)
0.86939567
tensor(0.8791, device='cuda:0', grad_fn=<AddBackward0>)
0.86954206
tensor(0.8063, device='cuda:0', grad_fn=<AddBackward0>)
0.86945397
tensor(0.7608, device='cuda:0', grad_fn=<AddBackward0>)
0.86940092
tensor(0.6552, device='cuda:0', grad_fn=<AddBackward0>)
0.86917317
tensor(0.8586, device='cuda:0', grad_fn=<AddBackward0>)
0.86875767
tensor(0.7343, device='cuda:0', grad_fn=<AddBackward0>)
0.86897957
tensor(0.8957, device='cuda:0', grad_fn=<AddBackward0>)
0.86946791
tensor(0.7038, device='cuda:0', grad_fn=<AddBackward0>)
0.86959684
tensor(0.6988, device='cuda:0', grad_fn=<AddBackward0>)
0.86928046
tensor(0.7867, device='cuda:0', grad_fn=<AddBackward0>)
0.86901027
tensor(0.7016, device='cuda:0', grad_fn=<AddBackward0>)
0.86892766
tensor(0.8255, device='cuda:0', grad_fn=<AddBackward0>)
0.86917996
tensor(0.8070, device='cuda:0', grad_fn=<AddBackward0>)
0.86941391
tensor(0.7216, device='cuda:0', grad_fn=<AddBackward0>)
0.86929256
tensor(0.7577, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][   20/  196]   Loss 0.771163   Top1 73.750000   Top5 98.281250   BatchTime 0.296156   LR 0.000485
0.86928385
tensor(0.7839, device='cuda:0', grad_fn=<AddBackward0>)
0.86929059
tensor(0.7175, device='cuda:0', grad_fn=<AddBackward0>)
0.86937976
tensor(0.7376, device='cuda:0', grad_fn=<AddBackward0>)
0.86948013
tensor(0.7010, device='cuda:0', grad_fn=<AddBackward0>)
0.86941230
tensor(0.7958, device='cuda:0', grad_fn=<AddBackward0>)
0.86934656
tensor(0.7985, device='cuda:0', grad_fn=<AddBackward0>)
0.86911875
tensor(0.8133, device='cuda:0', grad_fn=<AddBackward0>)
0.86940598
tensor(0.7314, device='cuda:0', grad_fn=<AddBackward0>)
0.86948955
tensor(0.6814, device='cuda:0', grad_fn=<AddBackward0>)
0.86929232
tensor(0.7472, device='cuda:0', grad_fn=<AddBackward0>)
0.86950511
tensor(0.8072, device='cuda:0', grad_fn=<AddBackward0>)
0.86981630
tensor(0.7992, device='cuda:0', grad_fn=<AddBackward0>)
0.86982065
tensor(0.7476, device='cuda:0', grad_fn=<AddBackward0>)
0.86997336
tensor(0.6801, device='cuda:0', grad_fn=<AddBackward0>)
0.86990827
tensor(0.7737, device='cuda:0', grad_fn=<AddBackward0>)
0.86985779
tensor(0.7768, device='cuda:0', grad_fn=<AddBackward0>)
0.86954218
tensor(0.6821, device='cuda:0', grad_fn=<AddBackward0>)
0.86959958
tensor(0.7733, device='cuda:0', grad_fn=<AddBackward0>)
0.86933589
tensor(0.7474, device='cuda:0', grad_fn=<AddBackward0>)
0.86989021
tensor(0.7092, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][   40/  196]   Loss 0.760686   Top1 74.433594   Top5 98.203125   BatchTime 0.299726   LR 0.000482
0.86978918
tensor(0.6775, device='cuda:0', grad_fn=<AddBackward0>)
0.86986303
tensor(0.6948, device='cuda:0', grad_fn=<AddBackward0>)
0.86997116
tensor(0.6657, device='cuda:0', grad_fn=<AddBackward0>)
0.86976075
tensor(0.6603, device='cuda:0', grad_fn=<AddBackward0>)
0.86992145
tensor(0.7730, device='cuda:0', grad_fn=<AddBackward0>)
0.86994112
tensor(0.7821, device='cuda:0', grad_fn=<AddBackward0>)
0.86982805
tensor(0.7169, device='cuda:0', grad_fn=<AddBackward0>)
0.86997730
tensor(0.7646, device='cuda:0', grad_fn=<AddBackward0>)
0.86975527
tensor(0.7691, device='cuda:0', grad_fn=<AddBackward0>)
0.87012929
tensor(0.7683, device='cuda:0', grad_fn=<AddBackward0>)
0.87007481
tensor(0.8313, device='cuda:0', grad_fn=<AddBackward0>)
0.86993098
tensor(0.6588, device='cuda:0', grad_fn=<AddBackward0>)
0.86962473
tensor(0.8181, device='cuda:0', grad_fn=<AddBackward0>)
0.86966050
tensor(0.7935, device='cuda:0', grad_fn=<AddBackward0>)
0.86968762
tensor(0.8841, device='cuda:0', grad_fn=<AddBackward0>)
0.86935222
tensor(0.6621, device='cuda:0', grad_fn=<AddBackward0>)
0.86921680
tensor(0.5991, device='cuda:0', grad_fn=<AddBackward0>)
0.86907482
tensor(0.7753, device='cuda:0', grad_fn=<AddBackward0>)
0.86913425
tensor(0.7086, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][   60/  196]   Loss 0.751333   Top1 74.804688   Top5 98.216146   BatchTime 0.300650   LR 0.000479
0.86908674
tensor(0.6494, device='cuda:0', grad_fn=<AddBackward0>)
0.86886632
tensor(0.5516, device='cuda:0', grad_fn=<AddBackward0>)
0.86885351
tensor(0.7955, device='cuda:0', grad_fn=<AddBackward0>)
0.86901891
tensor(0.7237, device='cuda:0', grad_fn=<AddBackward0>)
0.86837542
tensor(0.6651, device='cuda:0', grad_fn=<AddBackward0>)
0.86848944
tensor(0.7076, device='cuda:0', grad_fn=<AddBackward0>)
0.86847442
tensor(0.7433, device='cuda:0', grad_fn=<AddBackward0>)
0.86819595
tensor(0.7417, device='cuda:0', grad_fn=<AddBackward0>)
0.86814612
tensor(0.7205, device='cuda:0', grad_fn=<AddBackward0>)
0.86804771
tensor(0.6409, device='cuda:0', grad_fn=<AddBackward0>)
0.86799699
tensor(0.6665, device='cuda:0', grad_fn=<AddBackward0>)
0.86812717
tensor(0.6959, device='cuda:0', grad_fn=<AddBackward0>)
0.86821771
tensor(0.7803, device='cuda:0', grad_fn=<AddBackward0>)
0.86821109
tensor(0.8171, device='cuda:0', grad_fn=<AddBackward0>)
0.86806720
tensor(0.8033, device='cuda:0', grad_fn=<AddBackward0>)
0.86786687
tensor(0.6669, device='cuda:0', grad_fn=<AddBackward0>)
0.86800003
tensor(0.6730, device='cuda:0', grad_fn=<AddBackward0>)
0.86814880
tensor(0.6554, device='cuda:0', grad_fn=<AddBackward0>)
0.86809224
tensor(0.7014, device='cuda:0', grad_fn=<AddBackward0>)
0.86831284
tensor(0.8415, device='cuda:0', grad_fn=<AddBackward0>)
0.86796194
tensor(0.6035, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][   80/  196]   Loss 0.740936   Top1 75.053711   Top5 98.354492   BatchTime 0.299106   LR 0.000476
0.86778212
tensor(0.6119, device='cuda:0', grad_fn=<AddBackward0>)
0.86805218
tensor(0.6528, device='cuda:0', grad_fn=<AddBackward0>)
0.86814916
tensor(0.6633, device='cuda:0', grad_fn=<AddBackward0>)
0.86804342
tensor(0.7996, device='cuda:0', grad_fn=<AddBackward0>)
0.86791474
tensor(0.7693, device='cuda:0', grad_fn=<AddBackward0>)
0.86764818
tensor(0.6570, device='cuda:0', grad_fn=<AddBackward0>)
0.86768115
tensor(0.6579, device='cuda:0', grad_fn=<AddBackward0>)
0.86795914
tensor(0.6462, device='cuda:0', grad_fn=<AddBackward0>)
0.86770004
tensor(0.7457, device='cuda:0', grad_fn=<AddBackward0>)
0.86784220
tensor(0.5959, device='cuda:0', grad_fn=<AddBackward0>)
0.86766768
tensor(0.7668, device='cuda:0', grad_fn=<AddBackward0>)
0.86769903
tensor(0.7270, device='cuda:0', grad_fn=<AddBackward0>)
0.86787122
tensor(0.7213, device='cuda:0', grad_fn=<AddBackward0>)
0.86774635
tensor(0.6203, device='cuda:0', grad_fn=<AddBackward0>)
0.86778069
tensor(0.6964, device='cuda:0', grad_fn=<AddBackward0>)
0.86763155
tensor(0.5513, device='cuda:0', grad_fn=<AddBackward0>)
0.86748540
tensor(0.7428, device='cuda:0', grad_fn=<AddBackward0>)
0.86766028
tensor(0.7294, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][  100/  196]   Loss 0.729468   Top1 75.441406   Top5 98.351562   BatchTime 0.304340   LR 0.000473
0.86781245
tensor(0.6201, device='cuda:0', grad_fn=<AddBackward0>)
0.86790836
tensor(0.6970, device='cuda:0', grad_fn=<AddBackward0>)
0.86754185
tensor(0.7608, device='cuda:0', grad_fn=<AddBackward0>)
0.86751074
tensor(0.6622, device='cuda:0', grad_fn=<AddBackward0>)
0.86762261
tensor(0.7981, device='cuda:0', grad_fn=<AddBackward0>)
0.86769772
tensor(0.6365, device='cuda:0', grad_fn=<AddBackward0>)
0.86787593
tensor(0.7498, device='cuda:0', grad_fn=<AddBackward0>)
0.86797571
tensor(0.6526, device='cuda:0', grad_fn=<AddBackward0>)
0.86811560
tensor(0.6306, device='cuda:0', grad_fn=<AddBackward0>)
0.86815172
tensor(0.7166, device='cuda:0', grad_fn=<AddBackward0>)
0.86809886
tensor(0.5835, device='cuda:0', grad_fn=<AddBackward0>)
0.86808777
tensor(0.6951, device='cuda:0', grad_fn=<AddBackward0>)
0.86808580
tensor(0.7231, device='cuda:0', grad_fn=<AddBackward0>)
0.86824179
tensor(0.7483, device='cuda:0', grad_fn=<AddBackward0>)
0.86822575
tensor(0.7587, device='cuda:0', grad_fn=<AddBackward0>)
0.86816269
tensor(0.6323, device='cuda:0', grad_fn=<AddBackward0>)
0.86874807
tensor(0.6912, device='cuda:0', grad_fn=<AddBackward0>)
0.86860394
tensor(0.6950, device='cuda:0', grad_fn=<AddBackward0>)
0.86827284
tensor(0.7491, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][  120/  196]   Loss 0.723520   Top1 75.553385   Top5 98.375651   BatchTime 0.306505   LR 0.000469
0.86866581
tensor(0.5728, device='cuda:0', grad_fn=<AddBackward0>)
0.86885250
tensor(0.6899, device='cuda:0', grad_fn=<AddBackward0>)
0.86862200
tensor(0.7293, device='cuda:0', grad_fn=<AddBackward0>)
0.86870807
tensor(0.6710, device='cuda:0', grad_fn=<AddBackward0>)
0.86846286
tensor(0.6232, device='cuda:0', grad_fn=<AddBackward0>)
0.86836416
tensor(0.7359, device='cuda:0', grad_fn=<AddBackward0>)
0.86837018
tensor(0.6718, device='cuda:0', grad_fn=<AddBackward0>)
0.86860985
tensor(0.6515, device='cuda:0', grad_fn=<AddBackward0>)
0.86857450
tensor(0.6408, device='cuda:0', grad_fn=<AddBackward0>)
0.86884004
tensor(0.7094, device='cuda:0', grad_fn=<AddBackward0>)
0.86909652
tensor(0.7780, device='cuda:0', grad_fn=<AddBackward0>)
0.86894184
tensor(0.7259, device='cuda:0', grad_fn=<AddBackward0>)
0.86914939
tensor(0.6931, device='cuda:0', grad_fn=<AddBackward0>)
0.86905617
tensor(0.5870, device='cuda:0', grad_fn=<AddBackward0>)
0.86904091
tensor(0.6643, device='cuda:0', grad_fn=<AddBackward0>)
0.86905986
tensor(0.6118, device='cuda:0', grad_fn=<AddBackward0>)
0.86885047
tensor(0.7996, device='cuda:0', grad_fn=<AddBackward0>)
0.86867684
tensor(0.7273, device='cuda:0', grad_fn=<AddBackward0>)
0.86873925
tensor(0.7145, device='cuda:0', grad_fn=<AddBackward0>)
0.86862707
tensor(0.6817, device='cuda:0', grad_fn=<AddBackward0>)
0.86853385
tensor(0.6900, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][  140/  196]   Loss 0.718275   Top1 75.731027   Top5 98.381696   BatchTime 0.304039   LR 0.000465
0.86844891
tensor(0.7368, device='cuda:0', grad_fn=<AddBackward0>)
0.86858505
tensor(0.6225, device='cuda:0', grad_fn=<AddBackward0>)
0.86861366
tensor(0.6197, device='cuda:0', grad_fn=<AddBackward0>)
0.86897522
tensor(0.7421, device='cuda:0', grad_fn=<AddBackward0>)
0.86835343
tensor(0.6444, device='cuda:0', grad_fn=<AddBackward0>)
0.86874807
tensor(0.8108, device='cuda:0', grad_fn=<AddBackward0>)
0.86867797
tensor(0.5944, device='cuda:0', grad_fn=<AddBackward0>)
0.86833811
tensor(0.6912, device='cuda:0', grad_fn=<AddBackward0>)
0.86828601
tensor(0.8081, device='cuda:0', grad_fn=<AddBackward0>)
0.86846834
tensor(0.5338, device='cuda:0', grad_fn=<AddBackward0>)
0.86861014
tensor(0.7012, device='cuda:0', grad_fn=<AddBackward0>)
0.86819202
tensor(0.7582, device='cuda:0', grad_fn=<AddBackward0>)
0.86864549
tensor(0.5772, device='cuda:0', grad_fn=<AddBackward0>)
0.86859447
tensor(0.6266, device='cuda:0', grad_fn=<AddBackward0>)
0.86831552
tensor(0.5890, device='cuda:0', grad_fn=<AddBackward0>)
0.86838531
tensor(0.6348, device='cuda:0', grad_fn=<AddBackward0>)
0.86807978
tensor(0.5667, device='cuda:0', grad_fn=<AddBackward0>)
0.86810690
tensor(0.6765, device='cuda:0', grad_fn=<AddBackward0>)
0.86771065
tensor(0.6549, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][  160/  196]   Loss 0.711008   Top1 75.964355   Top5 98.400879   BatchTime 0.306679   LR 0.000460
0.86775666
tensor(0.6756, device='cuda:0', grad_fn=<AddBackward0>)
0.86794752
tensor(0.6393, device='cuda:0', grad_fn=<AddBackward0>)
0.86773133
tensor(0.6584, device='cuda:0', grad_fn=<AddBackward0>)
0.86765355
tensor(0.7234, device='cuda:0', grad_fn=<AddBackward0>)
0.86711037
tensor(0.7491, device='cuda:0', grad_fn=<AddBackward0>)
0.86630499
tensor(0.7122, device='cuda:0', grad_fn=<AddBackward0>)
0.86578745
tensor(0.7260, device='cuda:0', grad_fn=<AddBackward0>)
0.86508209
tensor(0.7677, device='cuda:0', grad_fn=<AddBackward0>)
0.86481309
tensor(0.7185, device='cuda:0', grad_fn=<AddBackward0>)
0.86439031
tensor(0.6433, device='cuda:0', grad_fn=<AddBackward0>)
0.86422426
tensor(0.6431, device='cuda:0', grad_fn=<AddBackward0>)
0.86410880
tensor(0.7115, device='cuda:0', grad_fn=<AddBackward0>)
0.86357594
tensor(0.7240, device='cuda:0', grad_fn=<AddBackward0>)
0.86290473
tensor(0.7191, device='cuda:0', grad_fn=<AddBackward0>)
0.86259162
tensor(0.6162, device='cuda:0', grad_fn=<AddBackward0>)
0.86223692
tensor(0.6779, device='cuda:0', grad_fn=<AddBackward0>)
0.86223501
tensor(0.7459, device='cuda:0', grad_fn=<AddBackward0>)
0.86252421
tensor(0.7343, device='cuda:0', grad_fn=<AddBackward0>)
0.86294860
tensor(0.6984, device='cuda:0', grad_fn=<AddBackward0>)
0.86357254
tensor(0.7122, device='cuda:0', grad_fn=<AddBackward0>)
0.86401832
tensor(0.6055, device='cuda:0', grad_fn=<AddBackward0>)
0.86397523
tensor(0.6127, device='cuda:0', grad_fn=<AddBackward0>)
0.86428589
tensor(0.6155, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [1][  180/  196]   Loss 0.708987   Top1 76.022135   Top5 98.407118   BatchTime 0.310071   LR 0.000456
0.86407810
tensor(0.6124, device='cuda:0', grad_fn=<AddBackward0>)
0.86420846
tensor(0.7532, device='cuda:0', grad_fn=<AddBackward0>)
0.86452031
tensor(0.6899, device='cuda:0', grad_fn=<AddBackward0>)
0.86499900
tensor(0.5334, device='cuda:0', grad_fn=<AddBackward0>)
0.86544353
tensor(0.6720, device='cuda:0', grad_fn=<AddBackward0>)
0.86560863
tensor(0.5806, device='cuda:0', grad_fn=<AddBackward0>)
0.86539513
tensor(0.7217, device='cuda:0', grad_fn=<AddBackward0>)
0.86531854
tensor(0.6248, device='cuda:0', grad_fn=<AddBackward0>)
0.86476564
tensor(0.6788, device='cuda:0', grad_fn=<AddBackward0>)
0.86447722
tensor(0.6364, device='cuda:0', grad_fn=<AddBackward0>)
0.86379778
tensor(0.6337, device='cuda:0', grad_fn=<AddBackward0>)
0.86221743
tensor(0.7437, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 76.120    Top5: 98.424    Loss: 0.705
0.86275136
tensor(0.6410, device='cuda:0', grad_fn=<AddBackward0>)
0.86081624
tensor(0.7208, device='cuda:0', grad_fn=<AddBackward0>)
0.86029571
tensor(0.6551, device='cuda:0', grad_fn=<AddBackward0>)
0.85767913
tensor(0.7058, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [1][   20/   40]   Loss 0.768516   Top1 75.019531   Top5 97.851562   BatchTime 0.105180
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0664)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1360)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1823)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0903)
features.3.conv.6 tensor(0.1055)
features.4.conv.0 tensor(0.0934)
features.4.conv.3 tensor(0.3096)
features.4.conv.6 tensor(0.2043)
features.5.conv.0 tensor(0.2619)
features.5.conv.3 tensor(0.4352)
features.5.conv.6 tensor(0.1164)
features.6.conv.0 tensor(0.0470)
features.6.conv.3 tensor(0.0584)
features.6.conv.6 tensor(0.0861)
features.7.conv.0 tensor(0.1641)
features.7.conv.3 tensor(0.4549)
features.7.conv.6 tensor(0.1864)
features.8.conv.0 tensor(0.4065)
features.8.conv.3 tensor(0.5365)
features.8.conv.6 tensor(0.1412)
features.9.conv.0 tensor(0.4646)
features.9.conv.3 tensor(0.5706)
features.9.conv.6 tensor(0.1406)
features.10.conv.0 tensor(0.0734)
features.10.conv.3 tensor(0.1068)
features.10.conv.6 tensor(0.1038)
features.11.conv.0 tensor(0.6180)
features.11.conv.3 tensor(0.6528)
features.11.conv.6 tensor(0.1794)
features.12.conv.0 tensor(0.6427)
features.12.conv.3 tensor(0.6715)
features.12.conv.6 tensor(0.1455)
features.13.conv.0 tensor(0.3803)
features.13.conv.3 tensor(0.5006)
features.13.conv.6 tensor(0.0970)
features.14.conv.0 tensor(0.7791)
features.14.conv.3 tensor(0.8262)
features.14.conv.6 tensor(0.1395)
features.15.conv.0 tensor(0.7474)
features.15.conv.3 tensor(0.8331)
features.15.conv.6 tensor(0.1717)
features.16.conv.0 tensor(0.6666)
features.16.conv.3 tensor(0.8093)
features.16.conv.6 tensor(0.0790)
conv.0 tensor(0.0779)
tensor(647432.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 0.753583   Top1 74.640000   Top5 98.130000   BatchTime 0.075332
INFO - ==> Top1: 74.640    Top5: 98.130    Loss: 0.754
INFO - ==> Sparsity : 0.296
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 74.640   Top5: 98.130]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 72.380   Top5: 97.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.85504484
tensor(0.4815, device='cuda:0', grad_fn=<AddBackward0>)
0.85030967
tensor(0.6977, device='cuda:0', grad_fn=<AddBackward0>)
0.84588462
tensor(0.6503, device='cuda:0', grad_fn=<AddBackward0>)
0.84546429
tensor(0.6170, device='cuda:0', grad_fn=<AddBackward0>)
0.84628415
tensor(0.6511, device='cuda:0', grad_fn=<AddBackward0>)
0.84929663
tensor(0.6118, device='cuda:0', grad_fn=<AddBackward0>)
0.85558045
tensor(0.5650, device='cuda:0', grad_fn=<AddBackward0>)
0.86561036
tensor(0.6448, device='cuda:0', grad_fn=<AddBackward0>)
0.86756057
tensor(0.6224, device='cuda:0', grad_fn=<AddBackward0>)
0.86743915
tensor(0.5713, device='cuda:0', grad_fn=<AddBackward0>)
0.86772907
tensor(0.6910, device='cuda:0', grad_fn=<AddBackward0>)
0.86754316
tensor(0.7248, device='cuda:0', grad_fn=<AddBackward0>)
0.86757034
tensor(0.6420, device='cuda:0', grad_fn=<AddBackward0>)
0.86764771
tensor(0.6087, device='cuda:0', grad_fn=<AddBackward0>)
0.86791283
tensor(0.6914, device='cuda:0', grad_fn=<AddBackward0>)
0.86758739
tensor(0.7249, device='cuda:0', grad_fn=<AddBackward0>)
0.86748874
tensor(0.5228, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][   20/  196]   Loss 0.629220   Top1 78.652344   Top5 98.847656   BatchTime 0.378079   LR 0.000448
0.86778122
tensor(0.5769, device='cuda:0', grad_fn=<AddBackward0>)
0.86756480
tensor(0.6052, device='cuda:0', grad_fn=<AddBackward0>)
0.86762071
tensor(0.6835, device='cuda:0', grad_fn=<AddBackward0>)
0.86754227
tensor(0.6813, device='cuda:0', grad_fn=<AddBackward0>)
0.86758828
tensor(0.6715, device='cuda:0', grad_fn=<AddBackward0>)
0.86762059
tensor(0.6573, device='cuda:0', grad_fn=<AddBackward0>)
0.86739719
tensor(0.6085, device='cuda:0', grad_fn=<AddBackward0>)
0.86694956
tensor(0.5828, device='cuda:0', grad_fn=<AddBackward0>)
0.86679411
tensor(0.6608, device='cuda:0', grad_fn=<AddBackward0>)
0.86692768
tensor(0.6089, device='cuda:0', grad_fn=<AddBackward0>)
0.86670965
tensor(0.6290, device='cuda:0', grad_fn=<AddBackward0>)
0.86650926
tensor(0.5876, device='cuda:0', grad_fn=<AddBackward0>)
0.86615252
tensor(0.5796, device='cuda:0', grad_fn=<AddBackward0>)
0.86621392
tensor(0.6668, device='cuda:0', grad_fn=<AddBackward0>)
0.86639321
tensor(0.6542, device='cuda:0', grad_fn=<AddBackward0>)
0.86633426
tensor(0.7094, device='cuda:0', grad_fn=<AddBackward0>)
0.86621499
tensor(0.6577, device='cuda:0', grad_fn=<AddBackward0>)
0.86663294
tensor(0.6934, device='cuda:0', grad_fn=<AddBackward0>)
0.86635274
tensor(0.6018, device='cuda:0', grad_fn=<AddBackward0>)
0.86658198
tensor(0.6717, device='cuda:0', grad_fn=<AddBackward0>)
0.86665416
tensor(0.6364, device='cuda:0', grad_fn=<AddBackward0>)
0.86646348
tensor(0.6843, device='cuda:0', grad_fn=<AddBackward0>)
0.86637723
tensor(0.6513, device='cuda:0', grad_fn=<AddBackward0>)
0.86615771
tensor(0.5898, device='cuda:0', grad_fn=<AddBackward0>)
0.86584169
tensor(0.5730, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][   40/  196]   Loss 0.636963   Top1 78.720703   Top5 98.583984   BatchTime 0.354982   LR 0.000443
0.86584514
tensor(0.5426, device='cuda:0', grad_fn=<AddBackward0>)
0.86578494
tensor(0.6612, device='cuda:0', grad_fn=<AddBackward0>)
0.86603796
tensor(0.5666, device='cuda:0', grad_fn=<AddBackward0>)
0.86597008
tensor(0.5639, device='cuda:0', grad_fn=<AddBackward0>)
0.86565101
tensor(0.5732, device='cuda:0', grad_fn=<AddBackward0>)
0.86518973
tensor(0.6075, device='cuda:0', grad_fn=<AddBackward0>)
0.86540288
tensor(0.6682, device='cuda:0', grad_fn=<AddBackward0>)
0.86506647
tensor(0.5842, device='cuda:0', grad_fn=<AddBackward0>)
0.86507642
tensor(0.6184, device='cuda:0', grad_fn=<AddBackward0>)
0.86480445
tensor(0.6874, device='cuda:0', grad_fn=<AddBackward0>)
0.86506373
tensor(0.7442, device='cuda:0', grad_fn=<AddBackward0>)
0.86543083
tensor(0.5361, device='cuda:0', grad_fn=<AddBackward0>)
0.86535376
tensor(0.5335, device='cuda:0', grad_fn=<AddBackward0>)
0.86569649
tensor(0.6394, device='cuda:0', grad_fn=<AddBackward0>)
0.86577839
tensor(0.6651, device='cuda:0', grad_fn=<AddBackward0>)
0.86534262
tensor(0.5738, device='cuda:0', grad_fn=<AddBackward0>)
0.86557424
tensor(0.6320, device='cuda:0', grad_fn=<AddBackward0>)
0.86548603
tensor(0.4513, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][   60/  196]   Loss 0.624834   Top1 78.958333   Top5 98.769531   BatchTime 0.346382   LR 0.000437
0.86512643
tensor(0.7329, device='cuda:0', grad_fn=<AddBackward0>)
0.86531937
tensor(0.5910, device='cuda:0', grad_fn=<AddBackward0>)
0.86498260
tensor(0.7110, device='cuda:0', grad_fn=<AddBackward0>)
0.86470425
tensor(0.6197, device='cuda:0', grad_fn=<AddBackward0>)
0.86461413
tensor(0.6734, device='cuda:0', grad_fn=<AddBackward0>)
0.86476684
tensor(0.5348, device='cuda:0', grad_fn=<AddBackward0>)
0.86323917
tensor(0.6962, device='cuda:0', grad_fn=<AddBackward0>)
0.86240792
tensor(0.7058, device='cuda:0', grad_fn=<AddBackward0>)
0.86176497
tensor(0.6701, device='cuda:0', grad_fn=<AddBackward0>)
0.86230874
tensor(0.5579, device='cuda:0', grad_fn=<AddBackward0>)
0.86180151
tensor(0.6116, device='cuda:0', grad_fn=<AddBackward0>)
0.86194003
tensor(0.4868, device='cuda:0', grad_fn=<AddBackward0>)
0.86198604
tensor(0.6229, device='cuda:0', grad_fn=<AddBackward0>)
0.86147702
tensor(0.6123, device='cuda:0', grad_fn=<AddBackward0>)
0.86109048
tensor(0.6845, device='cuda:0', grad_fn=<AddBackward0>)
0.86145329
tensor(0.5397, device='cuda:0', grad_fn=<AddBackward0>)
0.86109650
tensor(0.5821, device='cuda:0', grad_fn=<AddBackward0>)
0.86104047
tensor(0.6858, device='cuda:0', grad_fn=<AddBackward0>)
0.86066777
tensor(0.5978, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][   80/  196]   Loss 0.623406   Top1 79.091797   Top5 98.764648   BatchTime 0.339778   LR 0.000432
0.86029011
tensor(0.4660, device='cuda:0', grad_fn=<AddBackward0>)
0.85984832
tensor(0.7516, device='cuda:0', grad_fn=<AddBackward0>)
0.85990965
tensor(0.5724, device='cuda:0', grad_fn=<AddBackward0>)
0.85898519
tensor(0.5875, device='cuda:0', grad_fn=<AddBackward0>)
0.85736811
tensor(0.6893, device='cuda:0', grad_fn=<AddBackward0>)
0.85719663
tensor(0.6546, device='cuda:0', grad_fn=<AddBackward0>)
0.85656571
tensor(0.5424, device='cuda:0', grad_fn=<AddBackward0>)
0.85664916
tensor(0.5104, device='cuda:0', grad_fn=<AddBackward0>)
0.85667557
tensor(0.5957, device='cuda:0', grad_fn=<AddBackward0>)
0.85589701
tensor(0.6749, device='cuda:0', grad_fn=<AddBackward0>)
0.85503298
tensor(0.5564, device='cuda:0', grad_fn=<AddBackward0>)
0.85409260
tensor(0.6846, device='cuda:0', grad_fn=<AddBackward0>)
0.85294515
tensor(0.6568, device='cuda:0', grad_fn=<AddBackward0>)
0.85139680
tensor(0.6946, device='cuda:0', grad_fn=<AddBackward0>)
0.85014224
tensor(0.6855, device='cuda:0', grad_fn=<AddBackward0>)
0.84796381
tensor(0.5986, device='cuda:0', grad_fn=<AddBackward0>)
0.84620720
tensor(0.5942, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][  100/  196]   Loss 0.622691   Top1 79.078125   Top5 98.769531   BatchTime 0.340787   LR 0.000426
0.84550691
tensor(0.5256, device='cuda:0', grad_fn=<AddBackward0>)
0.84473622
tensor(0.6567, device='cuda:0', grad_fn=<AddBackward0>)
0.84405607
tensor(0.6069, device='cuda:0', grad_fn=<AddBackward0>)
0.84348744
tensor(0.5579, device='cuda:0', grad_fn=<AddBackward0>)
0.84358668
tensor(0.5801, device='cuda:0', grad_fn=<AddBackward0>)
0.84381527
tensor(0.5606, device='cuda:0', grad_fn=<AddBackward0>)
0.84388179
tensor(0.6559, device='cuda:0', grad_fn=<AddBackward0>)
0.84396863
tensor(0.6430, device='cuda:0', grad_fn=<AddBackward0>)
0.84398794
tensor(0.6507, device='cuda:0', grad_fn=<AddBackward0>)
0.84375519
tensor(0.5434, device='cuda:0', grad_fn=<AddBackward0>)
0.84372354
tensor(0.4475, device='cuda:0', grad_fn=<AddBackward0>)
0.84391230
tensor(0.6239, device='cuda:0', grad_fn=<AddBackward0>)
0.84418839
tensor(0.6070, device='cuda:0', grad_fn=<AddBackward0>)
0.84548146
tensor(0.6036, device='cuda:0', grad_fn=<AddBackward0>)
0.84514827
tensor(0.5731, device='cuda:0', grad_fn=<AddBackward0>)
0.84505332
tensor(0.6261, device='cuda:0', grad_fn=<AddBackward0>)
0.84501129
tensor(0.6066, device='cuda:0', grad_fn=<AddBackward0>)
0.84465873
tensor(0.5784, device='cuda:0', grad_fn=<AddBackward0>)
0.84435576
tensor(0.7172, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][  120/  196]   Loss 0.620212   Top1 79.166667   Top5 98.805339   BatchTime 0.333446   LR 0.000421
0.84426403
tensor(0.6085, device='cuda:0', grad_fn=<AddBackward0>)
0.84394324
tensor(0.6851, device='cuda:0', grad_fn=<AddBackward0>)
0.84378570
tensor(0.6523, device='cuda:0', grad_fn=<AddBackward0>)
0.84350395
tensor(0.5784, device='cuda:0', grad_fn=<AddBackward0>)
0.84274340
tensor(0.6150, device='cuda:0', grad_fn=<AddBackward0>)
0.84175223
tensor(0.5440, device='cuda:0', grad_fn=<AddBackward0>)
0.84167844
tensor(0.5878, device='cuda:0', grad_fn=<AddBackward0>)
0.84091240
tensor(0.6192, device='cuda:0', grad_fn=<AddBackward0>)
0.83954811
tensor(0.7642, device='cuda:0', grad_fn=<AddBackward0>)
0.83759660
tensor(0.5896, device='cuda:0', grad_fn=<AddBackward0>)
0.83689332
tensor(0.5664, device='cuda:0', grad_fn=<AddBackward0>)
0.83612496
tensor(0.5113, device='cuda:0', grad_fn=<AddBackward0>)
0.83659089
tensor(0.6921, device='cuda:0', grad_fn=<AddBackward0>)
0.83632994
tensor(0.5743, device='cuda:0', grad_fn=<AddBackward0>)
0.83617800
tensor(0.6223, device='cuda:0', grad_fn=<AddBackward0>)
0.83591801
tensor(0.6070, device='cuda:0', grad_fn=<AddBackward0>)
0.83600086
tensor(0.6032, device='cuda:0', grad_fn=<AddBackward0>)
0.83603847
tensor(0.8933, device='cuda:0', grad_fn=<AddBackward0>)
0.83602017
tensor(0.6634, device='cuda:0', grad_fn=<AddBackward0>)
0.83613652
tensor(0.5274, device='cuda:0', grad_fn=<AddBackward0>)
0.83598304
tensor(0.5280, device='cuda:0', grad_fn=<AddBackward0>)
0.83606929
tensor(0.5994, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][  140/  196]   Loss 0.619980   Top1 79.126674   Top5 98.816964   BatchTime 0.325975   LR 0.000415
0.83768803
tensor(0.6243, device='cuda:0', grad_fn=<AddBackward0>)
0.84023488
tensor(0.6533, device='cuda:0', grad_fn=<AddBackward0>)
0.84006041
tensor(0.6013, device='cuda:0', grad_fn=<AddBackward0>)
0.83999884
tensor(0.4690, device='cuda:0', grad_fn=<AddBackward0>)
0.83988172
tensor(0.5801, device='cuda:0', grad_fn=<AddBackward0>)
0.83973122
tensor(0.5124, device='cuda:0', grad_fn=<AddBackward0>)
0.83939230
tensor(0.6529, device='cuda:0', grad_fn=<AddBackward0>)
0.83949524
tensor(0.6598, device='cuda:0', grad_fn=<AddBackward0>)
0.83949113
tensor(0.7585, device='cuda:0', grad_fn=<AddBackward0>)
0.83927029
tensor(0.6125, device='cuda:0', grad_fn=<AddBackward0>)
0.83934301
tensor(0.5786, device='cuda:0', grad_fn=<AddBackward0>)
0.83937490
tensor(0.6556, device='cuda:0', grad_fn=<AddBackward0>)
0.83928055
tensor(0.6424, device='cuda:0', grad_fn=<AddBackward0>)
0.83935946
tensor(0.6127, device='cuda:0', grad_fn=<AddBackward0>)
0.83918440
tensor(0.5061, device='cuda:0', grad_fn=<AddBackward0>)
0.83900386
tensor(0.6347, device='cuda:0', grad_fn=<AddBackward0>)
0.83899355
tensor(0.6462, device='cuda:0', grad_fn=<AddBackward0>)
0.83894116
tensor(0.5285, device='cuda:0', grad_fn=<AddBackward0>)
0.83916754
tensor(0.6606, device='cuda:0', grad_fn=<AddBackward0>)
0.83887947
tensor(0.5468, device='cuda:0', grad_fn=<AddBackward0>)
0.83854330
tensor(0.6532, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][  160/  196]   Loss 0.617265   Top1 79.228516   Top5 98.808594   BatchTime 0.321731   LR 0.000409
0.83866388
tensor(0.5202, device='cuda:0', grad_fn=<AddBackward0>)
0.83868247
tensor(0.5345, device='cuda:0', grad_fn=<AddBackward0>)
0.83862644
tensor(0.5286, device='cuda:0', grad_fn=<AddBackward0>)
0.83823317
tensor(0.5286, device='cuda:0', grad_fn=<AddBackward0>)
0.83796442
tensor(0.5546, device='cuda:0', grad_fn=<AddBackward0>)
0.83747357
tensor(0.5524, device='cuda:0', grad_fn=<AddBackward0>)
0.83717495
tensor(0.6401, device='cuda:0', grad_fn=<AddBackward0>)
0.83691227
tensor(0.6042, device='cuda:0', grad_fn=<AddBackward0>)
0.83709985
tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)
0.83674628
tensor(0.5542, device='cuda:0', grad_fn=<AddBackward0>)
0.83681041
tensor(0.5762, device='cuda:0', grad_fn=<AddBackward0>)
0.83719581
tensor(0.6002, device='cuda:0', grad_fn=<AddBackward0>)
0.83678979
tensor(0.6207, device='cuda:0', grad_fn=<AddBackward0>)
0.83651310
tensor(0.5100, device='cuda:0', grad_fn=<AddBackward0>)
0.83662415
tensor(0.5071, device='cuda:0', grad_fn=<AddBackward0>)
0.83659601
tensor(0.5062, device='cuda:0', grad_fn=<AddBackward0>)
0.83666760
tensor(0.5100, device='cuda:0', grad_fn=<AddBackward0>)
0.83729881
tensor(0.5533, device='cuda:0', grad_fn=<AddBackward0>)
0.83665234
tensor(0.5503, device='cuda:0', grad_fn=<AddBackward0>)
0.83666509
tensor(0.6100, device='cuda:0', grad_fn=<AddBackward0>)
0.83672142
tensor(0.5936, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [2][  180/  196]   Loss 0.610838   Top1 79.405382   Top5 98.823785   BatchTime 0.318087   LR 0.000402
0.83680713
tensor(0.5812, device='cuda:0', grad_fn=<AddBackward0>)
0.83681041
tensor(0.5270, device='cuda:0', grad_fn=<AddBackward0>)
0.83658254
tensor(0.5769, device='cuda:0', grad_fn=<AddBackward0>)
0.83674026
tensor(0.5597, device='cuda:0', grad_fn=<AddBackward0>)
0.83640820
tensor(0.6298, device='cuda:0', grad_fn=<AddBackward0>)
0.83664960
tensor(0.5956, device='cuda:0', grad_fn=<AddBackward0>)
0.83674061
tensor(0.5739, device='cuda:0', grad_fn=<AddBackward0>)
0.83680600
tensor(0.3989, device='cuda:0', grad_fn=<AddBackward0>)
0.83668536
tensor(0.4406, device='cuda:0', grad_fn=<AddBackward0>)
0.83671796
tensor(0.5982, device='cuda:0', grad_fn=<AddBackward0>)
0.83663654
tensor(0.5322, device='cuda:0', grad_fn=<AddBackward0>)
0.83665413
tensor(0.6076, device='cuda:0', grad_fn=<AddBackward0>)
0.83642972
tensor(0.5868, device='cuda:0', grad_fn=<AddBackward0>)
0.83633614
tensor(0.5875, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 79.500    Top5: 98.830    Loss: 0.607
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83627194
tensor(0.5785, device='cuda:0', grad_fn=<AddBackward0>)
0.83612758
tensor(0.6593, device='cuda:0', grad_fn=<AddBackward0>)
0.83625144
tensor(0.7263, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.661916   Top1 77.578125   Top5 98.789062   BatchTime 0.107391
features.0.conv.0 tensor(0.2778)
features.0.conv.3
INFO - Validation [2][   40/   40]   Loss 0.662213   Top1 77.490000   Top5 98.750000   BatchTime 0.079458
INFO - ==> Top1: 77.490    Top5: 98.750    Loss: 0.662
INFO - ==> Sparsity : 0.328
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 77.490   Top5: 98.750]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 74.640   Top5: 98.130]
features.0.conv.3 tensor(0.1152)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0816)
features.2.conv.0 tensor(0.0998)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.1913)
features.3.conv.0 tensor(0.0619)
features.3.conv.3 tensor(0.0880)
features.3.conv.6 tensor(0.1079)
features.4.conv.0 tensor(0.0723)
features.4.conv.3 tensor(0.3160)
features.4.conv.6 tensor(0.1911)
features.5.conv.0 tensor(0.2435)
features.5.conv.3 tensor(0.4248)
features.5.conv.6 tensor(0.1066)
features.6.conv.0 tensor(0.0579)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0869)
features.7.conv.0 tensor(0.1866)
features.7.conv.3 tensor(0.4592)
features.7.conv.6 tensor(0.1942)
features.8.conv.0 tensor(0.4309)
features.8.conv.3 tensor(0.5359)
features.8.conv.6 tensor(0.1450)
features.9.conv.0 tensor(0.3468)
features.9.conv.3 tensor(0.5683)
features.9.conv.6 tensor(0.1393)
features.10.conv.0 tensor(0.0793)
features.10.conv.3 tensor(0.1097)
features.10.conv.6 tensor(0.1043)
features.11.conv.0 tensor(0.6347)
features.11.conv.3 tensor(0.6481)
features.11.conv.6 tensor(0.1799)
features.12.conv.0 tensor(0.6532)
features.12.conv.3 tensor(0.6738)
features.12.conv.6 tensor(0.2080)
features.13.conv.0 tensor(0.3001)
features.13.conv.3 tensor(0.4931)
features.13.conv.6 tensor(0.0948)
features.14.conv.0 tensor(0.8023)
features.14.conv.3 tensor(0.8244)
features.14.conv.6 tensor(0.1540)
features.15.conv.0 tensor(0.7664)
features.15.conv.3 tensor(0.8353)
features.15.conv.6 tensor(0.8664)
features.16.conv.0 tensor(0.3705)
features.16.conv.3 tensor(0.8052)
features.16.conv.6 tensor(0.0757)
conv.0 tensor(0.0881)
tensor(718978.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.83647329
tensor(0.4901, device='cuda:0', grad_fn=<AddBackward0>)
0.83661979
tensor(0.5034, device='cuda:0', grad_fn=<AddBackward0>)
0.83667284
tensor(0.5635, device='cuda:0', grad_fn=<AddBackward0>)
0.83676273
tensor(0.4394, device='cuda:0', grad_fn=<AddBackward0>)
0.83648133
tensor(0.5606, device='cuda:0', grad_fn=<AddBackward0>)
0.83666801
tensor(0.6131, device='cuda:0', grad_fn=<AddBackward0>)
0.83646446
tensor(0.5393, device='cuda:0', grad_fn=<AddBackward0>)
0.83622336
tensor(0.4851, device='cuda:0', grad_fn=<AddBackward0>)
0.83627462
tensor(0.6013, device='cuda:0', grad_fn=<AddBackward0>)
0.83631176
tensor(0.5830, device='cuda:0', grad_fn=<AddBackward0>)
0.83654130
tensor(0.5929, device='cuda:0', grad_fn=<AddBackward0>)
0.83645397
tensor(0.5228, device='cuda:0', grad_fn=<AddBackward0>)
0.83639026
tensor(0.5247, device='cuda:0', grad_fn=<AddBackward0>)
0.83638203
tensor(0.5361, device='cuda:0', grad_fn=<AddBackward0>)
0.83639079
tensor(0.5688, device='cuda:0', grad_fn=<AddBackward0>)
0.83639771
tensor(0.5539, device='cuda:0', grad_fn=<AddBackward0>)
0.83601969
tensor(0.6168, device='cuda:0', grad_fn=<AddBackward0>)
0.83599812
tensor(0.4603, device='cuda:0', grad_fn=<AddBackward0>)
0.83654362
tensor(0.5445, device='cuda:0', grad_fn=<AddBackward0>)
0.83648986
tensor(0.5077, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][   20/  196]   Loss 0.540364   Top1 81.796875   Top5 99.023438   BatchTime 0.367166   LR 0.000391
0.83639550
tensor(0.5899, device='cuda:0', grad_fn=<AddBackward0>)
0.83678335
tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)
0.83657056
tensor(0.6008, device='cuda:0', grad_fn=<AddBackward0>)
0.83631122
tensor(0.5368, device='cuda:0', grad_fn=<AddBackward0>)
0.83622277
tensor(0.5137, device='cuda:0', grad_fn=<AddBackward0>)
0.83603895
tensor(0.6139, device='cuda:0', grad_fn=<AddBackward0>)
0.83615923
tensor(0.4487, device='cuda:0', grad_fn=<AddBackward0>)
0.83614278
tensor(0.5818, device='cuda:0', grad_fn=<AddBackward0>)
0.83607483
tensor(0.5105, device='cuda:0', grad_fn=<AddBackward0>)
0.83601290
tensor(0.6574, device='cuda:0', grad_fn=<AddBackward0>)
0.83589953
tensor(0.6166, device='cuda:0', grad_fn=<AddBackward0>)
0.83589470
tensor(0.4847, device='cuda:0', grad_fn=<AddBackward0>)
0.83622599
tensor(0.4152, device='cuda:0', grad_fn=<AddBackward0>)
0.83620292
tensor(0.5809, device='cuda:0', grad_fn=<AddBackward0>)
0.83601254
tensor(0.5519, device='cuda:0', grad_fn=<AddBackward0>)
0.83593345
tensor(0.5178, device='cuda:0', grad_fn=<AddBackward0>)
0.83596045
tensor(0.5683, device='cuda:0', grad_fn=<AddBackward0>)
0.83602208
tensor(0.5433, device='cuda:0', grad_fn=<AddBackward0>)
0.83567560
tensor(0.6187, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][   40/  196]   Loss 0.543351   Top1 81.542969   Top5 99.091797   BatchTime 0.346626   LR 0.000384
0.83611751
tensor(0.4690, device='cuda:0', grad_fn=<AddBackward0>)
0.83594865
tensor(0.4899, device='cuda:0', grad_fn=<AddBackward0>)
0.83564985
tensor(0.5199, device='cuda:0', grad_fn=<AddBackward0>)
0.83561760
tensor(0.5321, device='cuda:0', grad_fn=<AddBackward0>)
0.83548373
tensor(0.6134, device='cuda:0', grad_fn=<AddBackward0>)
0.83537036
tensor(0.5238, device='cuda:0', grad_fn=<AddBackward0>)
0.83504432
tensor(0.4668, device='cuda:0', grad_fn=<AddBackward0>)
0.83486545
tensor(0.4650, device='cuda:0', grad_fn=<AddBackward0>)
0.83484358
tensor(0.6070, device='cuda:0', grad_fn=<AddBackward0>)
0.83533728
tensor(0.5165, device='cuda:0', grad_fn=<AddBackward0>)
0.83564669
tensor(0.4976, device='cuda:0', grad_fn=<AddBackward0>)
0.83546168
tensor(0.4481, device='cuda:0', grad_fn=<AddBackward0>)
0.83554888
tensor(0.6707, device='cuda:0', grad_fn=<AddBackward0>)
0.83566624
tensor(0.5037, device='cuda:0', grad_fn=<AddBackward0>)
0.83542418
tensor(0.4775, device='cuda:0', grad_fn=<AddBackward0>)
0.83560467
tensor(0.5310, device='cuda:0', grad_fn=<AddBackward0>)
0.83552319
tensor(0.5706, device='cuda:0', grad_fn=<AddBackward0>)
0.83551472
tensor(0.4829, device='cuda:0', grad_fn=<AddBackward0>)
0.83526766
tensor(0.6636, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][   60/  196]   Loss 0.540198   Top1 81.653646   Top5 99.127604   BatchTime 0.336250   LR 0.000377
0.83536047
tensor(0.5932, device='cuda:0', grad_fn=<AddBackward0>)
0.83541352
tensor(0.5045, device='cuda:0', grad_fn=<AddBackward0>)
0.83550435
tensor(0.4895, device='cuda:0', grad_fn=<AddBackward0>)
0.83571398
tensor(0.4884, device='cuda:0', grad_fn=<AddBackward0>)
0.83576012
tensor(0.4673, device='cuda:0', grad_fn=<AddBackward0>)
0.83554125
tensor(0.5175, device='cuda:0', grad_fn=<AddBackward0>)
0.83551484
tensor(0.6176, device='cuda:0', grad_fn=<AddBackward0>)
0.83562905
tensor(0.5254, device='cuda:0', grad_fn=<AddBackward0>)
0.83567005
tensor(0.5116, device='cuda:0', grad_fn=<AddBackward0>)
0.83554929
tensor(0.6545, device='cuda:0', grad_fn=<AddBackward0>)
0.83542520
tensor(0.5444, device='cuda:0', grad_fn=<AddBackward0>)
0.83518773
tensor(0.5587, device='cuda:0', grad_fn=<AddBackward0>)
0.83535272
tensor(0.5241, device='cuda:0', grad_fn=<AddBackward0>)
0.83556992
tensor(0.5492, device='cuda:0', grad_fn=<AddBackward0>)
0.83546215
tensor(0.5978, device='cuda:0', grad_fn=<AddBackward0>)
0.83498341
tensor(0.5821, device='cuda:0', grad_fn=<AddBackward0>)
0.83505404
tensor(0.4807, device='cuda:0', grad_fn=<AddBackward0>)
0.83528328
tensor(0.5111, device='cuda:0', grad_fn=<AddBackward0>)
0.83546740
tensor(0.5616, device='cuda:0', grad_fn=<AddBackward0>)
0.83547378
tensor(0.5471, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][   80/  196]   Loss 0.539271   Top1 81.503906   Top5 99.179688   BatchTime 0.329774   LR 0.000370
0.83586252
tensor(0.5091, device='cuda:0', grad_fn=<AddBackward0>)
0.83602464
tensor(0.4922, device='cuda:0', grad_fn=<AddBackward0>)
0.83544993
tensor(0.5439, device='cuda:0', grad_fn=<AddBackward0>)
0.83524698
tensor(0.5283, device='cuda:0', grad_fn=<AddBackward0>)
0.83536053
tensor(0.5870, device='cuda:0', grad_fn=<AddBackward0>)
0.83516711
tensor(0.5196, device='cuda:0', grad_fn=<AddBackward0>)
0.83503300
tensor(0.5055, device='cuda:0', grad_fn=<AddBackward0>)
0.83512700
tensor(0.6277, device='cuda:0', grad_fn=<AddBackward0>)
0.83487821
tensor(0.5818, device='cuda:0', grad_fn=<AddBackward0>)
0.83543009
tensor(0.5330, device='cuda:0', grad_fn=<AddBackward0>)
0.83508444
tensor(0.5658, device='cuda:0', grad_fn=<AddBackward0>)
0.83502823
tensor(0.5226, device='cuda:0', grad_fn=<AddBackward0>)
0.83485961
tensor(0.5597, device='cuda:0', grad_fn=<AddBackward0>)
0.83488518
tensor(0.5690, device='cuda:0', grad_fn=<AddBackward0>)
0.83485085
tensor(0.4872, device='cuda:0', grad_fn=<AddBackward0>)
0.83481854
tensor(0.4813, device='cuda:0', grad_fn=<AddBackward0>)
0.83504826
tensor(0.5904, device='cuda:0', grad_fn=<AddBackward0>)
0.83519500
tensor(0.6055, device='cuda:0', grad_fn=<AddBackward0>)
0.83550704
tensor(0.4659, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][  100/  196]   Loss 0.537962   Top1 81.558594   Top5 99.152344   BatchTime 0.327132   LR 0.000363
0.83519578
tensor(0.4387, device='cuda:0', grad_fn=<AddBackward0>)
0.83541912
tensor(0.5273, device='cuda:0', grad_fn=<AddBackward0>)
0.83543271
tensor(0.4143, device='cuda:0', grad_fn=<AddBackward0>)
0.83512604
tensor(0.5320, device='cuda:0', grad_fn=<AddBackward0>)
0.83490390
tensor(0.5556, device='cuda:0', grad_fn=<AddBackward0>)
0.83490318
tensor(0.4412, device='cuda:0', grad_fn=<AddBackward0>)
0.83462429
tensor(0.5723, device='cuda:0', grad_fn=<AddBackward0>)
0.83483464
tensor(0.4600, device='cuda:0', grad_fn=<AddBackward0>)
0.83457607
tensor(0.3983, device='cuda:0', grad_fn=<AddBackward0>)
0.83511162
tensor(0.3581, device='cuda:0', grad_fn=<AddBackward0>)
0.83495933
tensor(0.5101, device='cuda:0', grad_fn=<AddBackward0>)
0.83498877
tensor(0.5423, device='cuda:0', grad_fn=<AddBackward0>)
0.83511859
tensor(0.5078, device='cuda:0', grad_fn=<AddBackward0>)
0.83506417
tensor(0.5039, device='cuda:0', grad_fn=<AddBackward0>)
0.83502173
tensor(0.5644, device='cuda:0', grad_fn=<AddBackward0>)
0.83528799
tensor(0.5565, device='cuda:0', grad_fn=<AddBackward0>)
0.83527327
tensor(0.4911, device='cuda:0', grad_fn=<AddBackward0>)
0.83523595
tensor(0.4805, device='cuda:0', grad_fn=<AddBackward0>)
0.83528775
tensor(0.5007, device='cuda:0', grad_fn=<AddBackward0>)
0.83525974
tensor(0.5127, device='cuda:0', grad_fn=<AddBackward0>)
0.83524078
tensor(0.5873, device='cuda:0', grad_fn=<AddBackward0>)
0.83513767
tensor(0.5428, device='cuda:0', grad_fn=<AddBackward0>)
0.83516169
tensor(0.5338, device='cuda:0', grad_fn=<AddBackward0>)
0.83533847
tensor(0.5045, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][  120/  196]   Loss 0.532897   Top1 81.800130   Top5 99.169922   BatchTime 0.327834   LR 0.000356
0.83510667
tensor(0.6047, device='cuda:0', grad_fn=<AddBackward0>)
0.83506310
tensor(0.5375, device='cuda:0', grad_fn=<AddBackward0>)
0.83525330
tensor(0.5603, device='cuda:0', grad_fn=<AddBackward0>)
0.83528453
tensor(0.5266, device='cuda:0', grad_fn=<AddBackward0>)
0.83524561
tensor(0.4739, device='cuda:0', grad_fn=<AddBackward0>)
0.83521283
tensor(0.5658, device='cuda:0', grad_fn=<AddBackward0>)
0.83466536
tensor(0.6248, device='cuda:0', grad_fn=<AddBackward0>)
0.83436310
tensor(0.5440, device='cuda:0', grad_fn=<AddBackward0>)
0.83451694
tensor(0.5506, device='cuda:0', grad_fn=<AddBackward0>)
0.83464772
tensor(0.5970, device='cuda:0', grad_fn=<AddBackward0>)
0.83473259
tensor(0.4657, device='cuda:0', grad_fn=<AddBackward0>)
0.83474529
tensor(0.6294, device='cuda:0', grad_fn=<AddBackward0>)
0.83488756
tensor(0.5708, device='cuda:0', grad_fn=<AddBackward0>)
0.83503544
tensor(0.5783, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][  140/  196]   Loss 0.536602   Top1 81.819196   Top5 99.146205   BatchTime 0.322432   LR 0.000348
0.83528572
tensor(0.5691, device='cuda:0', grad_fn=<AddBackward0>)
0.83518577
tensor(0.5817, device='cuda:0', grad_fn=<AddBackward0>)
0.83502853
tensor(0.6219, device='cuda:0', grad_fn=<AddBackward0>)
0.83492965
tensor(0.5488, device='cuda:0', grad_fn=<AddBackward0>)
0.83489919
tensor(0.5214, device='cuda:0', grad_fn=<AddBackward0>)
0.83475065
tensor(0.4388, device='cuda:0', grad_fn=<AddBackward0>)
0.83452666
tensor(0.6520, device='cuda:0', grad_fn=<AddBackward0>)
0.83461648
tensor(0.5173, device='cuda:0', grad_fn=<AddBackward0>)
0.83501208
tensor(0.4411, device='cuda:0', grad_fn=<AddBackward0>)
0.83531737
tensor(0.6059, device='cuda:0', grad_fn=<AddBackward0>)
0.83523917
tensor(0.5605, device='cuda:0', grad_fn=<AddBackward0>)
0.83502132
tensor(0.6112, device='cuda:0', grad_fn=<AddBackward0>)
0.83491474
tensor(0.4977, device='cuda:0', grad_fn=<AddBackward0>)
0.83489138
tensor(0.5768, device='cuda:0', grad_fn=<AddBackward0>)
0.83479142
tensor(0.3902, device='cuda:0', grad_fn=<AddBackward0>)
0.83489084
tensor(0.5455, device='cuda:0', grad_fn=<AddBackward0>)
0.83500171
tensor(0.6124, device='cuda:0', grad_fn=<AddBackward0>)
0.83529216
tensor(0.5449, device='cuda:0', grad_fn=<AddBackward0>)
0.83519530
tensor(0.5012, device='cuda:0', grad_fn=<AddBackward0>)
0.83557963
tensor(0.4839, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][  160/  196]   Loss 0.535565   Top1 81.892090   Top5 99.155273   BatchTime 0.320229   LR 0.000341
0.83563793
tensor(0.5061, device='cuda:0', grad_fn=<AddBackward0>)
0.83544803
tensor(0.5518, device='cuda:0', grad_fn=<AddBackward0>)
0.83562028
tensor(0.4587, device='cuda:0', grad_fn=<AddBackward0>)
0.83506930
tensor(0.4403, device='cuda:0', grad_fn=<AddBackward0>)
0.83504653
tensor(0.6298, device='cuda:0', grad_fn=<AddBackward0>)
0.83520824
tensor(0.6048, device='cuda:0', grad_fn=<AddBackward0>)
0.83505160
tensor(0.3968, device='cuda:0', grad_fn=<AddBackward0>)
0.83504903
tensor(0.5529, device='cuda:0', grad_fn=<AddBackward0>)
0.83501989
tensor(0.4680, device='cuda:0', grad_fn=<AddBackward0>)
0.83500999
tensor(0.4943, device='cuda:0', grad_fn=<AddBackward0>)
0.83510673
tensor(0.4737, device='cuda:0', grad_fn=<AddBackward0>)
0.83516335
tensor(0.4626, device='cuda:0', grad_fn=<AddBackward0>)
0.83534759
tensor(0.5851, device='cuda:0', grad_fn=<AddBackward0>)
0.83535391
tensor(0.4682, device='cuda:0', grad_fn=<AddBackward0>)
0.83510190
tensor(0.4450, device='cuda:0', grad_fn=<AddBackward0>)
0.83506405
tensor(0.5105, device='cuda:0', grad_fn=<AddBackward0>)
0.83510172
tensor(0.5242, device='cuda:0', grad_fn=<AddBackward0>)
0.83528870
tensor(0.4864, device='cuda:0', grad_fn=<AddBackward0>)
0.83537060
tensor(0.4397, device='cuda:0', grad_fn=<AddBackward0>)
0.83501130
tensor(0.5696, device='cuda:0', grad_fn=<AddBackward0>)
0.83508587
tensor(0.4908, device='cuda:0', grad_fn=<AddBackward0>)
0.83496225
tensor(0.4986, device='cuda:0', grad_fn=<AddBackward0>)
0.83495814
tensor(0.5177, device='cuda:0', grad_fn=<AddBackward0>)
0.83523327
tensor(0.4683, device='cuda:0', grad_fn=<AddBackward0>)
0.83525759
tensor(0.5667, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [3][  180/  196]   Loss 0.531748   Top1 82.011719   Top5 99.184028   BatchTime 0.320569   LR 0.000333
0.83499759
tensor(0.4949, device='cuda:0', grad_fn=<AddBackward0>)
0.83485740
tensor(0.5656, device='cuda:0', grad_fn=<AddBackward0>)
0.83503628
tensor(0.4636, device='cuda:0', grad_fn=<AddBackward0>)
0.83522439
tensor(0.4914, device='cuda:0', grad_fn=<AddBackward0>)
0.83509302
tensor(0.5151, device='cuda:0', grad_fn=<AddBackward0>)
0.83502835
tensor(0.5725, device='cuda:0', grad_fn=<AddBackward0>)
0.83501852
tensor(0.4993, device='cuda:0', grad_fn=<AddBackward0>)
0.83492863
tensor(0.4536, device='cuda:0', grad_fn=<AddBackward0>)
0.83494687
tensor(0.5409, device='cuda:0', grad_fn=<AddBackward0>)
0.83519971
tensor(0.4601, device='cuda:0', grad_fn=<AddBackward0>)
0.83506697
tensor(0.5416, device='cuda:0', grad_fn=<AddBackward0>)
0.83506811
tensor(0.6162, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 82.032    Top5: 99.176    Loss: 0.531
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83500707
tensor(0.5985, device='cuda:0', grad_fn=<AddBackward0>)
0.83504969
tensor(0.5466, device='cuda:0', grad_fn=<AddBackward0>)
0.83488458
tensor(0.5391, device='cuda:0', grad_fn=<AddBackward0>)
0.83453041
tensor(0.4757, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [3][   20/   40]   Loss 0.569279   Top1 81.015625   Top5 98.730469   BatchTime 0.105656
INFO - Validation [3][   40/   40]   Loss 0.561701   Top1 81.120000   Top5 98.960000   BatchTime 0.078201
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.1133)
features.1.conv.0 tensor(0.0625)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0803)
features.2.conv.0 tensor(0.1042)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1849)
features.3.conv.0 tensor(0.0637)
features.3.conv.3 tensor(0.0887)
features.3.conv.6 tensor(0.1061)
features.4.conv.0 tensor(0.0610)
features.4.conv.3 tensor(0.3148)
features.4.conv.6 tensor(0.1842)
features.5.conv.0 tensor(0.2446)
features.5.conv.3 tensor(0.4253)
features.5.conv.6 tensor(0.1112)
features.6.conv.0 tensor(0.0449)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0841)
features.7.conv.0 tensor(0.1834)
features.7.conv.3 tensor(0.4598)
features.7.conv.6 tensor(0.2026)
features.8.conv.0 tensor(0.4320)
features.8.conv.3 tensor(0.5370)
features.8.conv.6 tensor(0.1432)
features.9.conv.0 tensor(0.3451)
features.9.conv.3 tensor(0.5671)
features.9.conv.6 tensor(0.1341)
features.10.conv.0 tensor(0.0720)
features.10.conv.3 tensor(0.1071)
features.10.conv.6 tensor(0.1041)
features.11.conv.0 tensor(0.6749)
features.11.conv.3 tensor(0.6495)
features.11.conv.6 tensor(0.1882)
features.12.conv.0 tensor(0.6554)
features.12.conv.3 tensor(0.6736)
features.12.conv.6 tensor(0.1994)
features.13.conv.0 tensor(0.3088)
features.13.conv.3 tensor(0.4969)
features.13.conv.6 tensor(0.0953)
features.14.conv.0 tensor(0.8293)
features.14.conv.3 tensor(0.8258)
features.14.conv.6 tensor(0.1753)
features.15.conv.0 tensor(0.7752)
features.15.conv.3 tensor(0.8346)
features.15.conv.6 tensor(0.8914)
features.16.conv.0 tensor(0.3675)
features.16.conv.3 tensor(0.8038)
features.16.conv.6 tensor(0.0800)
conv.0 tensor(0.0917)
tensor(736424.) 2188896.0
INFO - ==> Top1: 81.120    Top5: 98.960    Loss: 0.562
INFO - ==> Sparsity : 0.336
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 81.120   Top5: 98.960]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 77.490   Top5: 98.750]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 74.640   Top5: 98.130]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.83419108
tensor(0.4912, device='cuda:0', grad_fn=<AddBackward0>)
0.83421952
tensor(0.5110, device='cuda:0', grad_fn=<AddBackward0>)
0.83414978
tensor(0.4192, device='cuda:0', grad_fn=<AddBackward0>)
0.83431065
tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)
0.83397597
tensor(0.4989, device='cuda:0', grad_fn=<AddBackward0>)
0.83305383
tensor(0.5893, device='cuda:0', grad_fn=<AddBackward0>)
0.83264685
tensor(0.4513, device='cuda:0', grad_fn=<AddBackward0>)
0.83192354
tensor(0.5653, device='cuda:0', grad_fn=<AddBackward0>)
0.83078527
tensor(0.4997, device='cuda:0', grad_fn=<AddBackward0>)
0.82654637
tensor(0.5278, device='cuda:0', grad_fn=<AddBackward0>)
0.82264590
tensor(0.5745, device='cuda:0', grad_fn=<AddBackward0>)
0.82000172
tensor(0.4498, device='cuda:0', grad_fn=<AddBackward0>)
0.81767899
tensor(0.4747, device='cuda:0', grad_fn=<AddBackward0>)
0.81515557
tensor(0.4678, device='cuda:0', grad_fn=<AddBackward0>)
0.81237942
tensor(0.4816, device='cuda:0', grad_fn=<AddBackward0>)
0.81095970
tensor(0.5151, device='cuda:0', grad_fn=<AddBackward0>)
0.80975175
tensor(0.5356, device='cuda:0', grad_fn=<AddBackward0>)
0.80863327
tensor(0.5072, device='cuda:0', grad_fn=<AddBackward0>)
0.80787724
tensor(0.4784, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][   20/  196]   Loss 0.503022   Top1 83.691406   Top5 99.179688   BatchTime 0.376341   LR 0.000320
0.80754447
tensor(0.5153, device='cuda:0', grad_fn=<AddBackward0>)
0.80729556
tensor(0.3831, device='cuda:0', grad_fn=<AddBackward0>)
0.80701703
tensor(0.5442, device='cuda:0', grad_fn=<AddBackward0>)
0.80652738
tensor(0.5066, device='cuda:0', grad_fn=<AddBackward0>)
0.80640590
tensor(0.5948, device='cuda:0', grad_fn=<AddBackward0>)
0.80655199
tensor(0.4906, device='cuda:0', grad_fn=<AddBackward0>)
0.80669504
tensor(0.4794, device='cuda:0', grad_fn=<AddBackward0>)
0.80649614
tensor(0.5737, device='cuda:0', grad_fn=<AddBackward0>)
0.80627209
tensor(0.5363, device='cuda:0', grad_fn=<AddBackward0>)
0.80625683
tensor(0.5460, device='cuda:0', grad_fn=<AddBackward0>)
0.80633855
tensor(0.4244, device='cuda:0', grad_fn=<AddBackward0>)
0.80615145
tensor(0.4990, device='cuda:0', grad_fn=<AddBackward0>)
0.80599022
tensor(0.5925, device='cuda:0', grad_fn=<AddBackward0>)
0.80592561
tensor(0.5305, device='cuda:0', grad_fn=<AddBackward0>)
0.80603117
tensor(0.4658, device='cuda:0', grad_fn=<AddBackward0>)
0.80598640
tensor(0.5378, device='cuda:0', grad_fn=<AddBackward0>)
0.80586612
tensor(0.5273, device='cuda:0', grad_fn=<AddBackward0>)
0.80603725
tensor(0.4358, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][   40/  196]   Loss 0.499016   Top1 83.417969   Top5 99.228516   BatchTime 0.361777   LR 0.000312
0.80619448
tensor(0.4712, device='cuda:0', grad_fn=<AddBackward0>)
0.80628461
tensor(0.3990, device='cuda:0', grad_fn=<AddBackward0>)
0.80624115
tensor(0.3623, device='cuda:0', grad_fn=<AddBackward0>)
0.80632389
tensor(0.4842, device='cuda:0', grad_fn=<AddBackward0>)
0.80652887
tensor(0.4968, device='cuda:0', grad_fn=<AddBackward0>)
0.80658966
tensor(0.4249, device='cuda:0', grad_fn=<AddBackward0>)
0.80671769
tensor(0.5304, device='cuda:0', grad_fn=<AddBackward0>)
0.80624545
tensor(0.4940, device='cuda:0', grad_fn=<AddBackward0>)
0.80617660
tensor(0.4798, device='cuda:0', grad_fn=<AddBackward0>)
0.80622196
tensor(0.6506, device='cuda:0', grad_fn=<AddBackward0>)
0.80604452
tensor(0.5192, device='cuda:0', grad_fn=<AddBackward0>)
0.80596185
tensor(0.4322, device='cuda:0', grad_fn=<AddBackward0>)
0.80623943
tensor(0.4800, device='cuda:0', grad_fn=<AddBackward0>)
0.80636579
tensor(0.5350, device='cuda:0', grad_fn=<AddBackward0>)
0.80676490
tensor(0.4223, device='cuda:0', grad_fn=<AddBackward0>)
0.80666715
tensor(0.4892, device='cuda:0', grad_fn=<AddBackward0>)
0.80657744
tensor(0.4708, device='cuda:0', grad_fn=<AddBackward0>)
0.80702472
tensor(0.4943, device='cuda:0', grad_fn=<AddBackward0>)
0.80696565
tensor(0.5322, device='cuda:0', grad_fn=<AddBackward0>)
0.80675340
tensor(0.4187, device='cuda:0', grad_fn=<AddBackward0>)
0.80652916
tensor(0.5167, device='cuda:0', grad_fn=<AddBackward0>)
0.80657679
tensor(0.4913, device='cuda:0', grad_fn=<AddBackward0>)
0.80656916
tensor(0.3838, device='cuda:0', grad_fn=<AddBackward0>)
0.80681276
tensor(0.4231, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][   60/  196]   Loss 0.495116   Top1 83.404948   Top5 99.244792   BatchTime 0.351934   LR 0.000304
0.80677652
tensor(0.5214, device='cuda:0', grad_fn=<AddBackward0>)
0.80653876
tensor(0.5383, device='cuda:0', grad_fn=<AddBackward0>)
0.80638319
tensor(0.5135, device='cuda:0', grad_fn=<AddBackward0>)
0.80528808
tensor(0.5154, device='cuda:0', grad_fn=<AddBackward0>)
0.80509287
tensor(0.4647, device='cuda:0', grad_fn=<AddBackward0>)
0.80510497
tensor(0.5584, device='cuda:0', grad_fn=<AddBackward0>)
0.80512261
tensor(0.5416, device='cuda:0', grad_fn=<AddBackward0>)
0.80494392
tensor(0.4116, device='cuda:0', grad_fn=<AddBackward0>)
0.80485970
tensor(0.4796, device='cuda:0', grad_fn=<AddBackward0>)
0.80514055
tensor(0.4714, device='cuda:0', grad_fn=<AddBackward0>)
0.80524784
tensor(0.4407, device='cuda:0', grad_fn=<AddBackward0>)
0.80542868
tensor(0.4701, device='cuda:0', grad_fn=<AddBackward0>)
0.80526310
tensor(0.4751, device='cuda:0', grad_fn=<AddBackward0>)
0.80499649
tensor(0.4104, device='cuda:0', grad_fn=<AddBackward0>)
0.80404246
tensor(0.3880, device='cuda:0', grad_fn=<AddBackward0>)
0.80291420
tensor(0.3975, device='cuda:0', grad_fn=<AddBackward0>)
0.80166441
tensor(0.5946, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][   80/  196]   Loss 0.491364   Top1 83.374023   Top5 99.267578   BatchTime 0.347601   LR 0.000296
0.80083680
tensor(0.5261, device='cuda:0', grad_fn=<AddBackward0>)
0.79928720
tensor(0.4607, device='cuda:0', grad_fn=<AddBackward0>)
0.79775673
tensor(0.4111, device='cuda:0', grad_fn=<AddBackward0>)
0.79590505
tensor(0.4640, device='cuda:0', grad_fn=<AddBackward0>)
0.79366976
tensor(0.4511, device='cuda:0', grad_fn=<AddBackward0>)
0.79203135
tensor(0.4614, device='cuda:0', grad_fn=<AddBackward0>)
0.78958303
tensor(0.3846, device='cuda:0', grad_fn=<AddBackward0>)
0.78792202
tensor(0.5371, device='cuda:0', grad_fn=<AddBackward0>)
0.78667861
tensor(0.4673, device='cuda:0', grad_fn=<AddBackward0>)
0.78620875
tensor(0.5341, device='cuda:0', grad_fn=<AddBackward0>)
0.78660339
tensor(0.4310, device='cuda:0', grad_fn=<AddBackward0>)
0.78712720
tensor(0.4711, device='cuda:0', grad_fn=<AddBackward0>)
0.78730971
tensor(0.4542, device='cuda:0', grad_fn=<AddBackward0>)
0.78749710
tensor(0.4795, device='cuda:0', grad_fn=<AddBackward0>)
0.78761309
tensor(0.4757, device='cuda:0', grad_fn=<AddBackward0>)
0.78743988
tensor(0.4509, device='cuda:0', grad_fn=<AddBackward0>)
0.78712487
tensor(0.4916, device='cuda:0', grad_fn=<AddBackward0>)
0.78668481
tensor(0.4359, device='cuda:0', grad_fn=<AddBackward0>)
0.78621155
tensor(0.5319, device='cuda:0', grad_fn=<AddBackward0>)
0.78605145
tensor(0.4432, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][  100/  196]   Loss 0.487710   Top1 83.476562   Top5 99.257812   BatchTime 0.342129   LR 0.000289
0.78583771
tensor(0.5063, device='cuda:0', grad_fn=<AddBackward0>)
0.78601193
tensor(0.5799, device='cuda:0', grad_fn=<AddBackward0>)
0.78600174
tensor(0.5110, device='cuda:0', grad_fn=<AddBackward0>)
0.78596419
tensor(0.4374, device='cuda:0', grad_fn=<AddBackward0>)
0.78597897
tensor(0.4344, device='cuda:0', grad_fn=<AddBackward0>)
0.78604901
tensor(0.4542, device='cuda:0', grad_fn=<AddBackward0>)
0.78580713
tensor(0.4082, device='cuda:0', grad_fn=<AddBackward0>)
0.78594798
tensor(0.4762, device='cuda:0', grad_fn=<AddBackward0>)
0.78581977
tensor(0.4011, device='cuda:0', grad_fn=<AddBackward0>)
0.78545916
tensor(0.5095, device='cuda:0', grad_fn=<AddBackward0>)
0.78561044
tensor(0.4983, device='cuda:0', grad_fn=<AddBackward0>)
0.78591686
tensor(0.5119, device='cuda:0', grad_fn=<AddBackward0>)
0.78593892
tensor(0.4648, device='cuda:0', grad_fn=<AddBackward0>)
0.78607291
tensor(0.5451, device='cuda:0', grad_fn=<AddBackward0>)
0.78622431
tensor(0.4111, device='cuda:0', grad_fn=<AddBackward0>)
0.78643405
tensor(0.4422, device='cuda:0', grad_fn=<AddBackward0>)
0.78607523
tensor(0.3950, device='cuda:0', grad_fn=<AddBackward0>)
0.78581440
tensor(0.4476, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][  120/  196]   Loss 0.482870   Top1 83.681641   Top5 99.267578   BatchTime 0.339741   LR 0.000281
0.78572911
tensor(0.4067, device='cuda:0', grad_fn=<AddBackward0>)
0.78564185
tensor(0.4212, device='cuda:0', grad_fn=<AddBackward0>)
0.78573185
tensor(0.4376, device='cuda:0', grad_fn=<AddBackward0>)
0.78560787
tensor(0.5598, device='cuda:0', grad_fn=<AddBackward0>)
0.78590953
tensor(0.4471, device='cuda:0', grad_fn=<AddBackward0>)
0.78580755
tensor(0.3974, device='cuda:0', grad_fn=<AddBackward0>)
0.78583288
tensor(0.5142, device='cuda:0', grad_fn=<AddBackward0>)
0.78566718
tensor(0.5625, device='cuda:0', grad_fn=<AddBackward0>)
0.78564316
tensor(0.3769, device='cuda:0', grad_fn=<AddBackward0>)
0.78559238
tensor(0.4557, device='cuda:0', grad_fn=<AddBackward0>)
0.78544939
tensor(0.4630, device='cuda:0', grad_fn=<AddBackward0>)
0.78518283
tensor(0.4431, device='cuda:0', grad_fn=<AddBackward0>)
0.78492594
tensor(0.4337, device='cuda:0', grad_fn=<AddBackward0>)
0.78488255
tensor(0.5610, device='cuda:0', grad_fn=<AddBackward0>)
0.78484422
tensor(0.5196, device='cuda:0', grad_fn=<AddBackward0>)
0.78469557
tensor(0.5097, device='cuda:0', grad_fn=<AddBackward0>)
0.78469139
tensor(0.3477, device='cuda:0', grad_fn=<AddBackward0>)
0.78508657
tensor(0.5604, device='cuda:0', grad_fn=<AddBackward0>)
0.78531158
tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)
0.78502727
tensor(0.4607, device='cuda:0', grad_fn=<AddBackward0>)
0.78498429
tensor(0.4262, device='cuda:0', grad_fn=<AddBackward0>)
0.78487474
tensor(0.4689, device='cuda:0', grad_fn=<AddBackward0>)
0.78457695
tensor(0.4621, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][  140/  196]   Loss 0.481045   Top1 83.747210   Top5 99.274554   BatchTime 0.339471   LR 0.000273
0.78455812
tensor(0.5845, device='cuda:0', grad_fn=<AddBackward0>)
0.78472352
tensor(0.3484, device='cuda:0', grad_fn=<AddBackward0>)
0.78468394
tensor(0.4146, device='cuda:0', grad_fn=<AddBackward0>)
0.78464609
tensor(0.4612, device='cuda:0', grad_fn=<AddBackward0>)
0.78466159
tensor(0.5104, device='cuda:0', grad_fn=<AddBackward0>)
0.78479564
tensor(0.4713, device='cuda:0', grad_fn=<AddBackward0>)
0.78468663
tensor(0.4050, device='cuda:0', grad_fn=<AddBackward0>)
0.78475183
tensor(0.4430, device='cuda:0', grad_fn=<AddBackward0>)
0.78419739
tensor(0.4054, device='cuda:0', grad_fn=<AddBackward0>)
0.78380173
tensor(0.3461, device='cuda:0', grad_fn=<AddBackward0>)
0.78333914
tensor(0.3995, device='cuda:0', grad_fn=<AddBackward0>)
0.78269804
tensor(0.4215, device='cuda:0', grad_fn=<AddBackward0>)
0.78256196
tensor(0.5273, device='cuda:0', grad_fn=<AddBackward0>)
0.78252125
tensor(0.4014, device='cuda:0', grad_fn=<AddBackward0>)
0.78262681
tensor(0.4908, device='cuda:0', grad_fn=<AddBackward0>)
0.78278255
tensor(0.4462, device='cuda:0', grad_fn=<AddBackward0>)
0.78281581
tensor(0.4858, device='cuda:0', grad_fn=<AddBackward0>)
0.78294683
tensor(0.5331, device='cuda:0', grad_fn=<AddBackward0>)
0.78302205
tensor(0.4544, device='cuda:0', grad_fn=<AddBackward0>)
0.78315794
tensor(0.4053, device='cuda:0', grad_fn=<AddBackward0>)
0.78282583
tensor(0.3641, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][  160/  196]   Loss 0.475508   Top1 83.886719   Top5 99.318848   BatchTime 0.334128   LR 0.000265
0.78271240
tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)
0.78265625
tensor(0.4904, device='cuda:0', grad_fn=<AddBackward0>)
0.78244716
tensor(0.3766, device='cuda:0', grad_fn=<AddBackward0>)
0.78247190
tensor(0.4597, device='cuda:0', grad_fn=<AddBackward0>)
0.78257263
tensor(0.4485, device='cuda:0', grad_fn=<AddBackward0>)
0.78231430
tensor(0.4315, device='cuda:0', grad_fn=<AddBackward0>)
0.78189433
tensor(0.4713, device='cuda:0', grad_fn=<AddBackward0>)
0.78179300
tensor(0.4595, device='cuda:0', grad_fn=<AddBackward0>)
0.78172016
tensor(0.4828, device='cuda:0', grad_fn=<AddBackward0>)
0.78149509
tensor(0.4744, device='cuda:0', grad_fn=<AddBackward0>)
0.78135341
tensor(0.4618, device='cuda:0', grad_fn=<AddBackward0>)
0.78121382
tensor(0.4612, device='cuda:0', grad_fn=<AddBackward0>)
0.78136969
tensor(0.4576, device='cuda:0', grad_fn=<AddBackward0>)
0.78127074
tensor(0.4440, device='cuda:0', grad_fn=<AddBackward0>)
0.78140229
tensor(0.5059, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [4][  180/  196]   Loss 0.474113   Top1 83.973524   Top5 99.327257   BatchTime 0.325672   LR 0.000257
0.78098541
tensor(0.5261, device='cuda:0', grad_fn=<AddBackward0>)
0.78065920
tensor(0.4719, device='cuda:0', grad_fn=<AddBackward0>)
0.77986753
tensor(0.5415, device='cuda:0', grad_fn=<AddBackward0>)
0.77959138
tensor(0.4549, device='cuda:0', grad_fn=<AddBackward0>)
0.77908617
tensor(0.4131, device='cuda:0', grad_fn=<AddBackward0>)
0.77874595
tensor(0.6188, device='cuda:0', grad_fn=<AddBackward0>)
0.77850688
tensor(0.4419, device='cuda:0', grad_fn=<AddBackward0>)
0.77822709
tensor(0.5230, device='cuda:0', grad_fn=<AddBackward0>)
0.77757871
tensor(0.4051, device='cuda:0', grad_fn=<AddBackward0>)
0.77707869
tensor(0.4616, device='cuda:0', grad_fn=<AddBackward0>)
0.77679968
tensor(0.4389, device='cuda:0', grad_fn=<AddBackward0>)
0.77674150
tensor(0.4391, device='cuda:0', grad_fn=<AddBackward0>)
0.77659059
tensor(0.5136, device='cuda:0', grad_fn=<AddBackward0>)
0.77609891
tensor(0.4347, device='cuda:0', grad_fn=<AddBackward0>)
0.77626801
tensor(0.4888, device='cuda:0', grad_fn=<AddBackward0>)
0.77647573
tensor(0.4528, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 83.984    Top5: 99.332    Loss: 0.474
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77666885
tensor(0.5340, device='cuda:0', grad_fn=<AddBackward0>)
0.77666765
tensor(0.4653, device='cuda:0', grad_fn=<AddBackward0>)
0.77605015
tensor(0.3981, device='cuda:0', grad_fn=<AddBackward0>)
0.77633780
tensor(0.4418, device='cuda:0', grad_fn=<AddBackward0>)
0.77681476
tensor(0.5185, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.511308   Top1 82.871094   Top5 99.238281   BatchTime 0.113980
INFO - Validation [4][   40/   40]   Loss 0.503951   Top1 82.880000   Top5 99.360000   BatchTime 0.085109
INFO - ==> Top1: 82.880    Top5: 99.360    Loss: 0.504
INFO - ==> Sparsity : 0.413
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 82.880   Top5: 99.360]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 81.120   Top5: 98.960]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 77.490   Top5: 98.750]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0825)
features.2.conv.0 tensor(0.1059)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1895)
features.3.conv.0 tensor(0.0628)
features.3.conv.3 tensor(0.0957)
features.3.conv.6 tensor(0.1087)
features.4.conv.0 tensor(0.0632)
features.4.conv.3 tensor(0.3125)
features.4.conv.6 tensor(0.1875)
features.5.conv.0 tensor(0.2459)
features.5.conv.3 tensor(0.4259)
features.5.conv.6 tensor(0.1068)
features.6.conv.0 tensor(0.0524)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0853)
features.7.conv.0 tensor(0.1673)
features.7.conv.3 tensor(0.4627)
features.7.conv.6 tensor(0.1914)
features.8.conv.0 tensor(0.4685)
features.8.conv.3 tensor(0.5373)
features.8.conv.6 tensor(0.1499)
features.9.conv.0 tensor(0.3660)
features.9.conv.3 tensor(0.5631)
features.9.conv.6 tensor(0.1436)
features.10.conv.0 tensor(0.0743)
features.10.conv.3 tensor(0.1059)
features.10.conv.6 tensor(0.1019)
features.11.conv.0 tensor(0.6838)
features.11.conv.3 tensor(0.6493)
features.11.conv.6 tensor(0.1740)
features.12.conv.0 tensor(0.6866)
features.12.conv.3 tensor(0.6748)
features.12.conv.6 tensor(0.2002)
features.13.conv.0 tensor(0.3907)
features.13.conv.3 tensor(0.4996)
features.13.conv.6 tensor(0.0952)
features.14.conv.0 tensor(0.8440)
features.14.conv.3 tensor(0.8248)
features.14.conv.6 tensor(0.8833)
features.15.conv.0 tensor(0.7880)
features.15.conv.3 tensor(0.8348)
features.15.conv.6 tensor(0.9028)
features.16.conv.0 tensor(0.6645)
features.16.conv.3 tensor(0.8036)
features.16.conv.6 tensor(0.0806)
conv.0 tensor(0.0939)
tensor(905042.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.77710605
tensor(0.4687, device='cuda:0', grad_fn=<AddBackward0>)
0.77736408
tensor(0.4183, device='cuda:0', grad_fn=<AddBackward0>)
0.77801597
tensor(0.4560, device='cuda:0', grad_fn=<AddBackward0>)
0.77838898
tensor(0.4939, device='cuda:0', grad_fn=<AddBackward0>)
0.77882463
tensor(0.2938, device='cuda:0', grad_fn=<AddBackward0>)
0.78018558
tensor(0.4300, device='cuda:0', grad_fn=<AddBackward0>)
0.78060561
tensor(0.4258, device='cuda:0', grad_fn=<AddBackward0>)
0.78088880
tensor(0.4190, device='cuda:0', grad_fn=<AddBackward0>)
0.78103125
tensor(0.4281, device='cuda:0', grad_fn=<AddBackward0>)
0.78095841
tensor(0.5017, device='cuda:0', grad_fn=<AddBackward0>)
0.78110391
tensor(0.4073, device='cuda:0', grad_fn=<AddBackward0>)
0.78156835
tensor(0.4630, device='cuda:0', grad_fn=<AddBackward0>)
0.78262633
tensor(0.4046, device='cuda:0', grad_fn=<AddBackward0>)
0.78261054
tensor(0.4979, device='cuda:0', grad_fn=<AddBackward0>)
0.78250080
tensor(0.4367, device='cuda:0', grad_fn=<AddBackward0>)
0.78226709
tensor(0.4142, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][   20/  196]   Loss 0.428443   Top1 85.664062   Top5 99.609375   BatchTime 0.378767   LR 0.000242
0.78213221
tensor(0.4153, device='cuda:0', grad_fn=<AddBackward0>)
0.78242719
tensor(0.3194, device='cuda:0', grad_fn=<AddBackward0>)
0.78233951
tensor(0.3976, device='cuda:0', grad_fn=<AddBackward0>)
0.78227752
tensor(0.4778, device='cuda:0', grad_fn=<AddBackward0>)
0.78226411
tensor(0.3275, device='cuda:0', grad_fn=<AddBackward0>)
0.78255588
tensor(0.4339, device='cuda:0', grad_fn=<AddBackward0>)
0.78262937
tensor(0.3259, device='cuda:0', grad_fn=<AddBackward0>)
0.78235674
tensor(0.3989, device='cuda:0', grad_fn=<AddBackward0>)
0.78208858
tensor(0.4279, device='cuda:0', grad_fn=<AddBackward0>)
0.78184611
tensor(0.3586, device='cuda:0', grad_fn=<AddBackward0>)
0.78178746
tensor(0.3625, device='cuda:0', grad_fn=<AddBackward0>)
0.78167951
tensor(0.4782, device='cuda:0', grad_fn=<AddBackward0>)
0.78165585
tensor(0.4179, device='cuda:0', grad_fn=<AddBackward0>)
0.78153282
tensor(0.3638, device='cuda:0', grad_fn=<AddBackward0>)
0.78131104
tensor(0.4355, device='cuda:0', grad_fn=<AddBackward0>)
0.78142065
tensor(0.5271, device='cuda:0', grad_fn=<AddBackward0>)
0.78187728
tensor(0.4675, device='cuda:0', grad_fn=<AddBackward0>)
0.78169948
tensor(0.5240, device='cuda:0', grad_fn=<AddBackward0>)
0.78125018
tensor(0.3879, device='cuda:0', grad_fn=<AddBackward0>)
0.78095031
tensor(0.4410, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][   40/  196]   Loss 0.424728   Top1 85.664062   Top5 99.560547   BatchTime 0.349868   LR 0.000234
0.78039765
tensor(0.3924, device='cuda:0', grad_fn=<AddBackward0>)
0.78043479
tensor(0.3822, device='cuda:0', grad_fn=<AddBackward0>)
0.78025615
tensor(0.4913, device='cuda:0', grad_fn=<AddBackward0>)
0.78028190
tensor(0.4761, device='cuda:0', grad_fn=<AddBackward0>)
0.78071231
tensor(0.4027, device='cuda:0', grad_fn=<AddBackward0>)
0.78103489
tensor(0.5247, device='cuda:0', grad_fn=<AddBackward0>)
0.78133327
tensor(0.4012, device='cuda:0', grad_fn=<AddBackward0>)
0.78155041
tensor(0.3718, device='cuda:0', grad_fn=<AddBackward0>)
0.78183252
tensor(0.3915, device='cuda:0', grad_fn=<AddBackward0>)
0.78130823
tensor(0.4465, device='cuda:0', grad_fn=<AddBackward0>)
0.78134596
tensor(0.4600, device='cuda:0', grad_fn=<AddBackward0>)
0.78123587
tensor(0.4580, device='cuda:0', grad_fn=<AddBackward0>)
0.78117031
tensor(0.4596, device='cuda:0', grad_fn=<AddBackward0>)
0.78140187
tensor(0.4596, device='cuda:0', grad_fn=<AddBackward0>)
0.78173035
tensor(0.4067, device='cuda:0', grad_fn=<AddBackward0>)
0.78150356
tensor(0.4377, device='cuda:0', grad_fn=<AddBackward0>)
0.78183925
tensor(0.4057, device='cuda:0', grad_fn=<AddBackward0>)
0.78163028
tensor(0.4178, device='cuda:0', grad_fn=<AddBackward0>)
0.78114587
tensor(0.5013, device='cuda:0', grad_fn=<AddBackward0>)
0.80205286
tensor(0.4129, device='cuda:0', grad_fn=<AddBackward0>)
0.80207568
tensor(0.3629, device='cuda:0', grad_fn=<AddBackward0>)
0.80212653
tensor(0.4561, device='cuda:0', grad_fn=<AddBackward0>)
0.80204505
tensor(0.4514, device='cuda:0', grad_fn=<AddBackward0>)
0.80194563
tensor(0.5144, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][   60/  196]   Loss 0.428860   Top1 85.423177   Top5 99.537760   BatchTime 0.343144   LR 0.000226
0.80185217
tensor(0.3621, device='cuda:0', grad_fn=<AddBackward0>)
0.80176538
tensor(0.4406, device='cuda:0', grad_fn=<AddBackward0>)
0.80156970
tensor(0.3941, device='cuda:0', grad_fn=<AddBackward0>)
0.80183232
tensor(0.4020, device='cuda:0', grad_fn=<AddBackward0>)
0.80210185
tensor(0.3953, device='cuda:0', grad_fn=<AddBackward0>)
0.80219084
tensor(0.4778, device='cuda:0', grad_fn=<AddBackward0>)
0.80223387
tensor(0.5187, device='cuda:0', grad_fn=<AddBackward0>)
0.80228913
tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)
0.80225271
tensor(0.3785, device='cuda:0', grad_fn=<AddBackward0>)
0.80225593
tensor(0.4432, device='cuda:0', grad_fn=<AddBackward0>)
0.80221379
tensor(0.3461, device='cuda:0', grad_fn=<AddBackward0>)
0.80209452
tensor(0.3999, device='cuda:0', grad_fn=<AddBackward0>)
0.80233526
tensor(0.3626, device='cuda:0', grad_fn=<AddBackward0>)
0.80222797
tensor(0.4677, device='cuda:0', grad_fn=<AddBackward0>)
0.80232722
tensor(0.4775, device='cuda:0', grad_fn=<AddBackward0>)
0.80244213
tensor(0.4338, device='cuda:0', grad_fn=<AddBackward0>)
0.80232877
tensor(0.3970, device='cuda:0', grad_fn=<AddBackward0>)
0.80239809
tensor(0.4893, device='cuda:0', grad_fn=<AddBackward0>)
0.80237526
tensor(0.4758, device='cuda:0', grad_fn=<AddBackward0>)
0.80247945
tensor(0.3903, device='cuda:0', grad_fn=<AddBackward0>)
0.80229092
tensor(0.5340, device='cuda:0', grad_fn=<AddBackward0>)
0.80217004
tensor(0.4669, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][   80/  196]   Loss 0.427823   Top1 85.541992   Top5 99.458008   BatchTime 0.326349   LR 0.000218
0.80217469
tensor(0.4194, device='cuda:0', grad_fn=<AddBackward0>)
0.80179584
tensor(0.4624, device='cuda:0', grad_fn=<AddBackward0>)
0.80190265
tensor(0.4544, device='cuda:0', grad_fn=<AddBackward0>)
0.80202574
tensor(0.4888, device='cuda:0', grad_fn=<AddBackward0>)
0.80192453
tensor(0.3569, device='cuda:0', grad_fn=<AddBackward0>)
0.80205435
tensor(0.3819, device='cuda:0', grad_fn=<AddBackward0>)
0.80179191
tensor(0.4414, device='cuda:0', grad_fn=<AddBackward0>)
0.80172372
tensor(0.4214, device='cuda:0', grad_fn=<AddBackward0>)
0.80161476
tensor(0.5541, device='cuda:0', grad_fn=<AddBackward0>)
0.80125618
tensor(0.4485, device='cuda:0', grad_fn=<AddBackward0>)
0.80129087
tensor(0.4991, device='cuda:0', grad_fn=<AddBackward0>)
0.80125695
tensor(0.3708, device='cuda:0', grad_fn=<AddBackward0>)
0.80156744
tensor(0.4114, device='cuda:0', grad_fn=<AddBackward0>)
0.80158257
tensor(0.4154, device='cuda:0', grad_fn=<AddBackward0>)
0.80176473
tensor(0.5142, device='cuda:0', grad_fn=<AddBackward0>)
0.80197906
tensor(0.4731, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][  100/  196]   Loss 0.432940   Top1 85.417969   Top5 99.445312   BatchTime 0.310672   LR 0.000210
0.80189168
tensor(0.5701, device='cuda:0', grad_fn=<AddBackward0>)
0.80172539
tensor(0.3838, device='cuda:0', grad_fn=<AddBackward0>)
0.80166841
tensor(0.5088, device='cuda:0', grad_fn=<AddBackward0>)
0.80158013
tensor(0.3638, device='cuda:0', grad_fn=<AddBackward0>)
0.80148566
tensor(0.3609, device='cuda:0', grad_fn=<AddBackward0>)
0.80164564
tensor(0.3299, device='cuda:0', grad_fn=<AddBackward0>)
0.80150139
tensor(0.5290, device='cuda:0', grad_fn=<AddBackward0>)
0.80150652
tensor(0.4701, device='cuda:0', grad_fn=<AddBackward0>)
0.80158478
tensor(0.4207, device='cuda:0', grad_fn=<AddBackward0>)
0.80195296
tensor(0.4178, device='cuda:0', grad_fn=<AddBackward0>)
0.80192834
tensor(0.4267, device='cuda:0', grad_fn=<AddBackward0>)
0.80168533
tensor(0.3577, device='cuda:0', grad_fn=<AddBackward0>)
0.80165935
tensor(0.4881, device='cuda:0', grad_fn=<AddBackward0>)
0.80163199
tensor(0.5037, device='cuda:0', grad_fn=<AddBackward0>)
0.80180043
tensor(0.4412, device='cuda:0', grad_fn=<AddBackward0>)
0.80178857
tensor(0.4465, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][  120/  196]   Loss 0.432933   Top1 85.367839   Top5 99.472656   BatchTime 0.300732   LR 0.000202
0.80170482
tensor(0.4365, device='cuda:0', grad_fn=<AddBackward0>)
0.80184352
tensor(0.4581, device='cuda:0', grad_fn=<AddBackward0>)
0.80165815
tensor(0.3831, device='cuda:0', grad_fn=<AddBackward0>)
0.80178761
tensor(0.3733, device='cuda:0', grad_fn=<AddBackward0>)
0.80180264
tensor(0.4435, device='cuda:0', grad_fn=<AddBackward0>)
0.80184668
tensor(0.4983, device='cuda:0', grad_fn=<AddBackward0>)
0.80199569
tensor(0.3628, device='cuda:0', grad_fn=<AddBackward0>)
0.80194563
tensor(0.4896, device='cuda:0', grad_fn=<AddBackward0>)
0.80185878
tensor(0.3963, device='cuda:0', grad_fn=<AddBackward0>)
0.80197477
tensor(0.4683, device='cuda:0', grad_fn=<AddBackward0>)
0.80208474
tensor(0.4211, device='cuda:0', grad_fn=<AddBackward0>)
0.80182028
tensor(0.4059, device='cuda:0', grad_fn=<AddBackward0>)
0.80178773
tensor(0.4670, device='cuda:0', grad_fn=<AddBackward0>)
0.80177116
tensor(0.3769, device='cuda:0', grad_fn=<AddBackward0>)
0.80178410
tensor(0.3227, device='cuda:0', grad_fn=<AddBackward0>)
0.80180031
tensor(0.3696, device='cuda:0', grad_fn=<AddBackward0>)
0.80183268
tensor(0.4020, device='cuda:0', grad_fn=<AddBackward0>)
0.80202365
tensor(0.4812, device='cuda:0', grad_fn=<AddBackward0>)
0.80158854
tensor(0.4701, device='cuda:0', grad_fn=<AddBackward0>)
0.80158567
tensor(0.3431, device='cuda:0', grad_fn=<AddBackward0>)
0.80159897
tensor(0.3837, device='cuda:0', grad_fn=<AddBackward0>)
0.80159175
tensor(0.4582, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][  140/  196]   Loss 0.431139   Top1 85.376674   Top5 99.444754   BatchTime 0.298682   LR 0.000195
0.80158532
tensor(0.5560, device='cuda:0', grad_fn=<AddBackward0>)
0.80150038
tensor(0.3728, device='cuda:0', grad_fn=<AddBackward0>)
0.80140239
tensor(0.4561, device='cuda:0', grad_fn=<AddBackward0>)
0.80136621
tensor(0.4041, device='cuda:0', grad_fn=<AddBackward0>)
0.80138874
tensor(0.4831, device='cuda:0', grad_fn=<AddBackward0>)
0.80131406
tensor(0.3939, device='cuda:0', grad_fn=<AddBackward0>)
0.80133110
tensor(0.3473, device='cuda:0', grad_fn=<AddBackward0>)
0.80143028
tensor(0.3808, device='cuda:0', grad_fn=<AddBackward0>)
0.80120933
tensor(0.3579, device='cuda:0', grad_fn=<AddBackward0>)
0.80101287
tensor(0.4283, device='cuda:0', grad_fn=<AddBackward0>)
0.80092192
tensor(0.3464, device='cuda:0', grad_fn=<AddBackward0>)
0.80100697
tensor(0.3491, device='cuda:0', grad_fn=<AddBackward0>)
0.80090874
tensor(0.3748, device='cuda:0', grad_fn=<AddBackward0>)
0.80089873
tensor(0.3792, device='cuda:0', grad_fn=<AddBackward0>)
0.80094540
tensor(0.4423, device='cuda:0', grad_fn=<AddBackward0>)
0.80105704
tensor(0.4785, device='cuda:0', grad_fn=<AddBackward0>)
0.80063587
tensor(0.3762, device='cuda:0', grad_fn=<AddBackward0>)
0.80080664
tensor(0.4034, device='cuda:0', grad_fn=<AddBackward0>)
0.80084401
tensor(0.4506, device='cuda:0', grad_fn=<AddBackward0>)
0.80028802
tensor(0.5990, device='cuda:0', grad_fn=<AddBackward0>)
0.80043960
tensor(0.3596, device='cuda:0', grad_fn=<AddBackward0>)
0.80047506
tensor(0.3467, device='cuda:0', grad_fn=<AddBackward0>)
0.80034953
tensor(0.4192, device='cuda:0', grad_fn=<AddBackward0>)
0.80031735
tensor(0.4987, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][  160/  196]   Loss 0.428591   Top1 85.493164   Top5 99.453125   BatchTime 0.303668   LR 0.000187
0.80034643
tensor(0.4118, device='cuda:0', grad_fn=<AddBackward0>)
0.80039066
tensor(0.4870, device='cuda:0', grad_fn=<AddBackward0>)
0.80044544
tensor(0.3882, device='cuda:0', grad_fn=<AddBackward0>)
0.80034345
tensor(0.3815, device='cuda:0', grad_fn=<AddBackward0>)
0.80038702
tensor(0.4071, device='cuda:0', grad_fn=<AddBackward0>)
0.80070925
tensor(0.3339, device='cuda:0', grad_fn=<AddBackward0>)
0.80063456
tensor(0.3888, device='cuda:0', grad_fn=<AddBackward0>)
0.80067503
tensor(0.4909, device='cuda:0', grad_fn=<AddBackward0>)
0.80060387
tensor(0.4997, device='cuda:0', grad_fn=<AddBackward0>)
0.80043751
tensor(0.3798, device='cuda:0', grad_fn=<AddBackward0>)
0.80050093
tensor(0.4066, device='cuda:0', grad_fn=<AddBackward0>)
0.80049425
tensor(0.3349, device='cuda:0', grad_fn=<AddBackward0>)
0.80045134
tensor(0.3937, device='cuda:0', grad_fn=<AddBackward0>)
0.80030793
tensor(0.4564, device='cuda:0', grad_fn=<AddBackward0>)
0.80022514
tensor(0.3449, device='cuda:0', grad_fn=<AddBackward0>)
0.80015236
tensor(0.4796, device='cuda:0', grad_fn=<AddBackward0>)
0.80022198
tensor(0.4059, device='cuda:0', grad_fn=<AddBackward0>)
0.80009276
tensor(0.3598, device='cuda:0', grad_fn=<AddBackward0>)
0.80038810
tensor(0.5280, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [5][  180/  196]   Loss 0.427281   Top1 85.466580   Top5 99.472656   BatchTime 0.304960   LR 0.000179
0.80030507
tensor(0.4576, device='cuda:0', grad_fn=<AddBackward0>)
0.80025023
tensor(0.4030, device='cuda:0', grad_fn=<AddBackward0>)
0.80013520
tensor(0.3459, device='cuda:0', grad_fn=<AddBackward0>)
0.80019087
tensor(0.4240, device='cuda:0', grad_fn=<AddBackward0>)
0.80014217
tensor(0.4276, device='cuda:0', grad_fn=<AddBackward0>)
0.80012536
tensor(0.3725, device='cuda:0', grad_fn=<AddBackward0>)
0.79990906
tensor(0.4015, device='cuda:0', grad_fn=<AddBackward0>)
0.79995114
tensor(0.3365, device='cuda:0', grad_fn=<AddBackward0>)
0.79990715
tensor(0.4168, device='cuda:0', grad_fn=<AddBackward0>)
0.80024642
tensor(0.3949, device='cuda:0', grad_fn=<AddBackward0>)
0.80017412
tensor(0.4434, device='cuda:0', grad_fn=<AddBackward0>)
0.80014765
tensor(0.4785, device='cuda:0', grad_fn=<AddBackward0>)
0.80024719
tensor(0.3217, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 85.554    Top5: 99.476    Loss: 0.425
0.80023384
tensor(0.4052, device='cuda:0', grad_fn=<AddBackward0>)
0.80009109
tensor(0.4507, device='cuda:0', grad_fn=<AddBackward0>)
0.79996544
tensor(0.4220, device='cuda:0', grad_fn=<AddBackward0>)
0.80012196
tensor(0.3460, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.470346   Top1 84.101562   Top5 99.433594   BatchTime 0.113428
INFO - Validation [5][   40/   40]   Loss 0.459344   Top1 84.570000   Top5 99.450000   BatchTime 0.083438
INFO - ==> Top1: 84.570    Top5: 99.450    Loss: 0.459
INFO - ==> Sparsity : 0.392
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 84.570   Top5: 99.450]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 82.880   Top5: 99.360]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 81.120   Top5: 98.960]
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1191)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0894)
features.2.conv.0 tensor(0.1030)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.1907)
features.3.conv.0 tensor(0.0651)
features.3.conv.3 tensor(0.0880)
features.3.conv.6 tensor(0.1089)
features.4.conv.0 tensor(0.0524)
features.4.conv.3 tensor(0.3183)
features.4.conv.6 tensor(0.1865)
features.5.conv.0 tensor(0.3062)
features.5.conv.3 tensor(0.4259)
features.5.conv.6 tensor(0.1048)
features.6.conv.0 tensor(0.0483)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0861)
features.7.conv.0 tensor(0.1646)
features.7.conv.3 tensor(0.4601)
features.7.conv.6 tensor(0.1937)
features.8.conv.0 tensor(0.4971)
features.8.conv.3 tensor(0.5379)
features.8.conv.6 tensor(0.1510)
features.9.conv.0 tensor(0.3376)
features.9.conv.3 tensor(0.5642)
features.9.conv.6 tensor(0.1401)
features.10.conv.0 tensor(0.0736)
features.10.conv.3 tensor(0.1047)
features.10.conv.6 tensor(0.1044)
features.11.conv.0 tensor(0.6990)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.1894)
features.12.conv.0 tensor(0.6799)
features.12.conv.3 tensor(0.6744)
features.12.conv.6 tensor(0.2032)
features.13.conv.0 tensor(0.1973)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.0937)
features.14.conv.0 tensor(0.8446)
features.14.conv.3 tensor(0.8250)
features.14.conv.6 tensor(0.9044)
features.15.conv.0 tensor(0.7992)
features.15.conv.3 tensor(0.8348)
features.15.conv.6 tensor(0.9165)
features.16.conv.0 tensor(0.3565)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.0854)
conv.0 tensor(0.0944)
tensor(857497.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.80018079
tensor(0.4077, device='cuda:0', grad_fn=<AddBackward0>)
0.80019748
tensor(0.3874, device='cuda:0', grad_fn=<AddBackward0>)
0.80022848
tensor(0.4434, device='cuda:0', grad_fn=<AddBackward0>)
0.80030686
tensor(0.3857, device='cuda:0', grad_fn=<AddBackward0>)
0.80059022
tensor(0.3911, device='cuda:0', grad_fn=<AddBackward0>)
0.80025995
tensor(0.4043, device='cuda:0', grad_fn=<AddBackward0>)
0.80036467
tensor(0.3469, device='cuda:0', grad_fn=<AddBackward0>)
0.79999071
tensor(0.3287, device='cuda:0', grad_fn=<AddBackward0>)
0.79972076
tensor(0.3192, device='cuda:0', grad_fn=<AddBackward0>)
0.79976708
tensor(0.4213, device='cuda:0', grad_fn=<AddBackward0>)
0.79952085
tensor(0.3213, device='cuda:0', grad_fn=<AddBackward0>)
0.79958880
tensor(0.3833, device='cuda:0', grad_fn=<AddBackward0>)
0.79954046
tensor(0.3506, device='cuda:0', grad_fn=<AddBackward0>)
0.79997665
tensor(0.4151, device='cuda:0', grad_fn=<AddBackward0>)
0.79967511
tensor(0.3237, device='cuda:0', grad_fn=<AddBackward0>)
0.79962128
tensor(0.4236, device='cuda:0', grad_fn=<AddBackward0>)
0.79942495
tensor(0.3962, device='cuda:0', grad_fn=<AddBackward0>)
0.79930478
tensor(0.3512, device='cuda:0', grad_fn=<AddBackward0>)
0.79919040
tensor(0.4080, device='cuda:0', grad_fn=<AddBackward0>)
0.79905725
tensor(0.4028, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][   20/  196]   Loss 0.380571   Top1 87.324219   Top5 99.609375   BatchTime 0.351391   LR 0.000166
0.79878187
tensor(0.3655, device='cuda:0', grad_fn=<AddBackward0>)
0.79850334
tensor(0.3812, device='cuda:0', grad_fn=<AddBackward0>)
0.79847962
tensor(0.3389, device='cuda:0', grad_fn=<AddBackward0>)
0.79834932
tensor(0.3864, device='cuda:0', grad_fn=<AddBackward0>)
0.79834265
tensor(0.3412, device='cuda:0', grad_fn=<AddBackward0>)
0.79865962
tensor(0.3387, device='cuda:0', grad_fn=<AddBackward0>)
0.79876781
tensor(0.3885, device='cuda:0', grad_fn=<AddBackward0>)
0.79920655
tensor(0.3901, device='cuda:0', grad_fn=<AddBackward0>)
0.79914719
tensor(0.3667, device='cuda:0', grad_fn=<AddBackward0>)
0.79895008
tensor(0.4537, device='cuda:0', grad_fn=<AddBackward0>)
0.79880625
tensor(0.3768, device='cuda:0', grad_fn=<AddBackward0>)
0.79890263
tensor(0.4158, device='cuda:0', grad_fn=<AddBackward0>)
0.79887223
tensor(0.3881, device='cuda:0', grad_fn=<AddBackward0>)
0.79873121
tensor(0.3646, device='cuda:0', grad_fn=<AddBackward0>)
0.79865682
tensor(0.3343, device='cuda:0', grad_fn=<AddBackward0>)
0.79873872
tensor(0.3748, device='cuda:0', grad_fn=<AddBackward0>)
0.79867613
tensor(0.3735, device='cuda:0', grad_fn=<AddBackward0>)
0.79894882
tensor(0.4238, device='cuda:0', grad_fn=<AddBackward0>)
0.79891080
tensor(0.3456, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][   40/  196]   Loss 0.380244   Top1 87.158203   Top5 99.638672   BatchTime 0.330329   LR 0.000158
0.79879403
tensor(0.4501, device='cuda:0', grad_fn=<AddBackward0>)
0.79863012
tensor(0.4707, device='cuda:0', grad_fn=<AddBackward0>)
0.79866713
tensor(0.4179, device='cuda:0', grad_fn=<AddBackward0>)
0.79863107
tensor(0.3216, device='cuda:0', grad_fn=<AddBackward0>)
0.79874384
tensor(0.4446, device='cuda:0', grad_fn=<AddBackward0>)
0.79884291
tensor(0.4135, device='cuda:0', grad_fn=<AddBackward0>)
0.79884338
tensor(0.3164, device='cuda:0', grad_fn=<AddBackward0>)
0.79876864
tensor(0.4192, device='cuda:0', grad_fn=<AddBackward0>)
0.79898274
tensor(0.3841, device='cuda:0', grad_fn=<AddBackward0>)
0.79938757
tensor(0.4376, device='cuda:0', grad_fn=<AddBackward0>)
0.79963195
tensor(0.3140, device='cuda:0', grad_fn=<AddBackward0>)
0.79919261
tensor(0.3666, device='cuda:0', grad_fn=<AddBackward0>)
0.79943478
tensor(0.4187, device='cuda:0', grad_fn=<AddBackward0>)
0.79945272
tensor(0.4002, device='cuda:0', grad_fn=<AddBackward0>)
0.79937905
tensor(0.3678, device='cuda:0', grad_fn=<AddBackward0>)
0.79927951
tensor(0.3663, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][   60/  196]   Loss 0.382261   Top1 87.011719   Top5 99.576823   BatchTime 0.307179   LR 0.000151
0.79934078
tensor(0.4258, device='cuda:0', grad_fn=<AddBackward0>)
0.79910433
tensor(0.3481, device='cuda:0', grad_fn=<AddBackward0>)
0.79935414
tensor(0.4097, device='cuda:0', grad_fn=<AddBackward0>)
0.79931718
tensor(0.3392, device='cuda:0', grad_fn=<AddBackward0>)
0.79894489
tensor(0.3438, device='cuda:0', grad_fn=<AddBackward0>)
0.79873461
tensor(0.3454, device='cuda:0', grad_fn=<AddBackward0>)
0.79877317
tensor(0.4682, device='cuda:0', grad_fn=<AddBackward0>)
0.79858923
tensor(0.3619, device='cuda:0', grad_fn=<AddBackward0>)
0.79846764
tensor(0.4419, device='cuda:0', grad_fn=<AddBackward0>)
0.79839867
tensor(0.4225, device='cuda:0', grad_fn=<AddBackward0>)
0.79843384
tensor(0.4234, device='cuda:0', grad_fn=<AddBackward0>)
0.79841077
tensor(0.3563, device='cuda:0', grad_fn=<AddBackward0>)
0.79820621
tensor(0.4148, device='cuda:0', grad_fn=<AddBackward0>)
0.79806697
tensor(0.4027, device='cuda:0', grad_fn=<AddBackward0>)
0.79805481
tensor(0.3449, device='cuda:0', grad_fn=<AddBackward0>)
0.79834706
tensor(0.4373, device='cuda:0', grad_fn=<AddBackward0>)
0.79799032
tensor(0.3285, device='cuda:0', grad_fn=<AddBackward0>)
0.79787141
tensor(0.3533, device='cuda:0', grad_fn=<AddBackward0>)
0.79765034
tensor(0.3275, device='cuda:0', grad_fn=<AddBackward0>)
0.79754215
tensor(0.3757, device='cuda:0', grad_fn=<AddBackward0>)
0.79760617
tensor(0.4105, device='cuda:0', grad_fn=<AddBackward0>)
0.79756111
tensor(0.3700, device='cuda:0', grad_fn=<AddBackward0>)
0.79734820
tensor(0.3906, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][   80/  196]   Loss 0.382395   Top1 87.001953   Top5 99.555664   BatchTime 0.293594   LR 0.000143
0.79723638
tensor(0.3290, device='cuda:0', grad_fn=<AddBackward0>)
0.79718375
tensor(0.3515, device='cuda:0', grad_fn=<AddBackward0>)
0.79717743
tensor(0.4493, device='cuda:0', grad_fn=<AddBackward0>)
0.79705352
tensor(0.3896, device='cuda:0', grad_fn=<AddBackward0>)
0.79700065
tensor(0.4699, device='cuda:0', grad_fn=<AddBackward0>)
0.79716659
tensor(0.3700, device='cuda:0', grad_fn=<AddBackward0>)
0.79733849
tensor(0.3088, device='cuda:0', grad_fn=<AddBackward0>)
0.79717815
tensor(0.3162, device='cuda:0', grad_fn=<AddBackward0>)
0.79716927
tensor(0.4710, device='cuda:0', grad_fn=<AddBackward0>)
0.79717404
tensor(0.4406, device='cuda:0', grad_fn=<AddBackward0>)
0.79735214
tensor(0.3314, device='cuda:0', grad_fn=<AddBackward0>)
0.79719657
tensor(0.3519, device='cuda:0', grad_fn=<AddBackward0>)
0.79750311
tensor(0.3567, device='cuda:0', grad_fn=<AddBackward0>)
0.79785174
tensor(0.3851, device='cuda:0', grad_fn=<AddBackward0>)
0.79767382
tensor(0.3068, device='cuda:0', grad_fn=<AddBackward0>)
0.79749769
tensor(0.3663, device='cuda:0', grad_fn=<AddBackward0>)
0.79741442
tensor(0.2958, device='cuda:0', grad_fn=<AddBackward0>)
0.79756153
tensor(0.3951, device='cuda:0', grad_fn=<AddBackward0>)
0.79725987
tensor(0.3137, device='cuda:0', grad_fn=<AddBackward0>)
0.79674023
tensor(0.4003, device='cuda:0', grad_fn=<AddBackward0>)
0.79686505
tensor(0.6056, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][  100/  196]   Loss 0.382050   Top1 87.003906   Top5 99.535156   BatchTime 0.294122   LR 0.000136
0.79684031
tensor(0.2891, device='cuda:0', grad_fn=<AddBackward0>)
0.79696184
tensor(0.3021, device='cuda:0', grad_fn=<AddBackward0>)
0.79684925
tensor(0.3934, device='cuda:0', grad_fn=<AddBackward0>)
0.79690468
tensor(0.4166, device='cuda:0', grad_fn=<AddBackward0>)
0.79697341
tensor(0.3491, device='cuda:0', grad_fn=<AddBackward0>)
0.79714835
tensor(0.4110, device='cuda:0', grad_fn=<AddBackward0>)
0.79713446
tensor(0.3360, device='cuda:0', grad_fn=<AddBackward0>)
0.79711533
tensor(0.5404, device='cuda:0', grad_fn=<AddBackward0>)
0.79707569
tensor(0.4334, device='cuda:0', grad_fn=<AddBackward0>)
0.79696059
tensor(0.4468, device='cuda:0', grad_fn=<AddBackward0>)
0.79701614
tensor(0.3696, device='cuda:0', grad_fn=<AddBackward0>)
0.79690731
tensor(0.3125, device='cuda:0', grad_fn=<AddBackward0>)
0.79712266
tensor(0.2991, device='cuda:0', grad_fn=<AddBackward0>)
0.79705924
tensor(0.3916, device='cuda:0', grad_fn=<AddBackward0>)
0.79694837
tensor(0.3786, device='cuda:0', grad_fn=<AddBackward0>)
0.79678774
tensor(0.4391, device='cuda:0', grad_fn=<AddBackward0>)
0.79679614
tensor(0.3207, device='cuda:0', grad_fn=<AddBackward0>)
0.79673040
tensor(0.2777, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][  120/  196]   Loss 0.380924   Top1 87.018229   Top5 99.557292   BatchTime 0.299992   LR 0.000129
0.79675037
tensor(0.3019, device='cuda:0', grad_fn=<AddBackward0>)
0.79684883
tensor(0.4335, device='cuda:0', grad_fn=<AddBackward0>)
0.79700464
tensor(0.3526, device='cuda:0', grad_fn=<AddBackward0>)
0.79717702
tensor(0.3137, device='cuda:0', grad_fn=<AddBackward0>)
0.79703748
tensor(0.2776, device='cuda:0', grad_fn=<AddBackward0>)
0.79667240
tensor(0.4039, device='cuda:0', grad_fn=<AddBackward0>)
0.79639769
tensor(0.4031, device='cuda:0', grad_fn=<AddBackward0>)
0.79629475
tensor(0.3334, device='cuda:0', grad_fn=<AddBackward0>)
0.79625511
tensor(0.3626, device='cuda:0', grad_fn=<AddBackward0>)
0.79605389
tensor(0.4407, device='cuda:0', grad_fn=<AddBackward0>)
0.79574406
tensor(0.3643, device='cuda:0', grad_fn=<AddBackward0>)
0.79575229
tensor(0.3436, device='cuda:0', grad_fn=<AddBackward0>)
0.79572153
tensor(0.3964, device='cuda:0', grad_fn=<AddBackward0>)
0.79589969
tensor(0.5021, device='cuda:0', grad_fn=<AddBackward0>)
0.79609787
tensor(0.4168, device='cuda:0', grad_fn=<AddBackward0>)
0.79648668
tensor(0.4002, device='cuda:0', grad_fn=<AddBackward0>)
0.79588372
tensor(0.3232, device='cuda:0', grad_fn=<AddBackward0>)
0.79613322
tensor(0.4240, device='cuda:0', grad_fn=<AddBackward0>)
0.79603940
tensor(0.2906, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][  140/  196]   Loss 0.380102   Top1 87.025670   Top5 99.570312   BatchTime 0.303499   LR 0.000122
0.79579735
tensor(0.3251, device='cuda:0', grad_fn=<AddBackward0>)
0.79552597
tensor(0.3947, device='cuda:0', grad_fn=<AddBackward0>)
0.79545975
tensor(0.3778, device='cuda:0', grad_fn=<AddBackward0>)
0.79534584
tensor(0.4097, device='cuda:0', grad_fn=<AddBackward0>)
0.79537380
tensor(0.3764, device='cuda:0', grad_fn=<AddBackward0>)
0.79542089
tensor(0.3864, device='cuda:0', grad_fn=<AddBackward0>)
0.79545790
tensor(0.3526, device='cuda:0', grad_fn=<AddBackward0>)
0.79546624
tensor(0.2921, device='cuda:0', grad_fn=<AddBackward0>)
0.79546791
tensor(0.3337, device='cuda:0', grad_fn=<AddBackward0>)
0.79561502
tensor(0.4282, device='cuda:0', grad_fn=<AddBackward0>)
0.79563588
tensor(0.4769, device='cuda:0', grad_fn=<AddBackward0>)
0.79553688
tensor(0.3697, device='cuda:0', grad_fn=<AddBackward0>)
0.79581857
tensor(0.4120, device='cuda:0', grad_fn=<AddBackward0>)
0.79570603
tensor(0.3337, device='cuda:0', grad_fn=<AddBackward0>)
0.79584759
tensor(0.3649, device='cuda:0', grad_fn=<AddBackward0>)
0.79576683
tensor(0.3830, device='cuda:0', grad_fn=<AddBackward0>)
0.79547387
tensor(0.3640, device='cuda:0', grad_fn=<AddBackward0>)
0.79532200
tensor(0.3572, device='cuda:0', grad_fn=<AddBackward0>)
0.79524624
tensor(0.3622, device='cuda:0', grad_fn=<AddBackward0>)
0.79499143
tensor(0.2920, device='cuda:0', grad_fn=<AddBackward0>)
0.79512519
tensor(0.3244, device='cuda:0', grad_fn=<AddBackward0>)
0.79514050
tensor(0.3856, device='cuda:0', grad_fn=<AddBackward0>)
0.79508996
tensor(0.2954, device='cuda:0', grad_fn=<AddBackward0>)
0.79510629
tensor(0.2863, device='cuda:0', grad_fn=<AddBackward0>)
0.79494488
tensor(0.4511, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][  160/  196]   Loss 0.377444   Top1 87.155762   Top5 99.570312   BatchTime 0.305519   LR 0.000115
0.79508758
tensor(0.4507, device='cuda:0', grad_fn=<AddBackward0>)
0.79497594
tensor(0.3786, device='cuda:0', grad_fn=<AddBackward0>)
0.79481572
tensor(0.3176, device='cuda:0', grad_fn=<AddBackward0>)
0.79416668
tensor(0.4277, device='cuda:0', grad_fn=<AddBackward0>)
0.79359972
tensor(0.3061, device='cuda:0', grad_fn=<AddBackward0>)
0.79343283
tensor(0.4575, device='cuda:0', grad_fn=<AddBackward0>)
0.79348248
tensor(0.3046, device='cuda:0', grad_fn=<AddBackward0>)
0.79358917
tensor(0.3622, device='cuda:0', grad_fn=<AddBackward0>)
0.79378247
tensor(0.4303, device='cuda:0', grad_fn=<AddBackward0>)
0.79359865
tensor(0.4448, device='cuda:0', grad_fn=<AddBackward0>)
0.79400378
tensor(0.4058, device='cuda:0', grad_fn=<AddBackward0>)
0.79397166
tensor(0.2822, device='cuda:0', grad_fn=<AddBackward0>)
0.79364377
tensor(0.3396, device='cuda:0', grad_fn=<AddBackward0>)
0.79370379
tensor(0.3666, device='cuda:0', grad_fn=<AddBackward0>)
0.79370624
tensor(0.3316, device='cuda:0', grad_fn=<AddBackward0>)
0.79342914
tensor(0.4139, device='cuda:0', grad_fn=<AddBackward0>)
0.79353446
tensor(0.3689, device='cuda:0', grad_fn=<AddBackward0>)
0.79334587
tensor(0.3521, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [6][  180/  196]   Loss 0.377378   Top1 87.172309   Top5 99.583333   BatchTime 0.308439   LR 0.000108
0.79364735
tensor(0.3453, device='cuda:0', grad_fn=<AddBackward0>)
0.79353142
tensor(0.4291, device='cuda:0', grad_fn=<AddBackward0>)
0.79331887
tensor(0.2667, device='cuda:0', grad_fn=<AddBackward0>)
0.79322296
tensor(0.3111, device='cuda:0', grad_fn=<AddBackward0>)
0.79367447
tensor(0.3242, device='cuda:0', grad_fn=<AddBackward0>)
0.79367316
tensor(0.3163, device='cuda:0', grad_fn=<AddBackward0>)
0.79353881
tensor(0.3708, device='cuda:0', grad_fn=<AddBackward0>)
0.79332995
tensor(0.3975, device='cuda:0', grad_fn=<AddBackward0>)
0.79317820
tensor(0.3620, device='cuda:0', grad_fn=<AddBackward0>)
0.79302895
tensor(0.3277, device='cuda:0', grad_fn=<AddBackward0>)
0.79294097
tensor(0.3942, device='cuda:0', grad_fn=<AddBackward0>)
0.79273689
tensor(0.3002, device='cuda:0', grad_fn=<AddBackward0>)
0.79274297
tensor(0.3352, device='cuda:0', grad_fn=<AddBackward0>)
0.79261994
tensor(0.3380, device='cuda:0', grad_fn=<AddBackward0>)
0.79260498
tensor(0.3063, device='cuda:0', grad_fn=<AddBackward0>)
0.79269308
tensor(0.3855, device='cuda:0', grad_fn=<AddBackward0>)
0.79277408
tensor(0.4650, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 87.318    Top5: 99.582    Loss: 0.375
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.434124   Top1 85.761719   Top5 99.257812   BatchTime 0.113260
INFO - Validation [6][   40/   40]   Loss 0.433225   Top1 85.430000   Top5 99.370000   BatchTime 0.084487
INFO - ==> Top1: 85.430    Top5: 99.370    Loss: 0.433
INFO - ==> Sparsity : 0.398
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 85.430   Top5: 99.370]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 84.570   Top5: 99.450]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 82.880   Top5: 99.360]
features.0.conv.0 tensor(0.2708)
features.0.conv.3 tensor(0.1191)
features.1.conv.0 tensor(0.0671)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1073)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1956)
features.3.conv.0 tensor(0.0747)
features.3.conv.3 tensor(0.0864)
features.3.conv.6 tensor(0.1074)
features.4.conv.0 tensor(0.0636)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1841)
features.5.conv.0 tensor(0.2749)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.1017)
features.6.conv.0 tensor(0.0492)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0875)
features.7.conv.0 tensor(0.1742)
features.7.conv.3 tensor(0.4560)
features.7.conv.6 tensor(0.1906)
features.8.conv.0 tensor(0.5369)
features.8.conv.3 tensor(0.5367)
features.8.conv.6 tensor(0.1524)
features.9.conv.0 tensor(0.3649)
features.9.conv.3 tensor(0.5668)
features.9.conv.6 tensor(0.1368)
features.10.conv.0 tensor(0.0698)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.1019)
features.11.conv.0 tensor(0.7026)
features.11.conv.3 tensor(0.6495)
features.11.conv.6 tensor(0.1902)
features.12.conv.0 tensor(0.7403)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.2036)
features.13.conv.0 tensor(0.1974)
features.13.conv.3 tensor(0.4923)
features.13.conv.6 tensor(0.0940)
features.14.conv.0 tensor(0.8551)
features.14.conv.3 tensor(0.8250)
features.14.conv.6 tensor(0.9266)
features.15.conv.0 tensor(0.8137)
features.15.conv.3 tensor(0.8353)
features.15.conv.6 tensor(0.9277)
features.16.conv.0 tensor(0.3494)
features.16.conv.3 tensor(0.8060)
features.16.conv.6 tensor(0.0872)
conv.0 tensor(0.0942)
tensor(870977.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.79251206
tensor(0.3343, device='cuda:0', grad_fn=<AddBackward0>)
0.79249424
tensor(0.3732, device='cuda:0', grad_fn=<AddBackward0>)
0.79273707
tensor(0.3961, device='cuda:0', grad_fn=<AddBackward0>)
0.79270500
tensor(0.3935, device='cuda:0', grad_fn=<AddBackward0>)
0.79259545
tensor(0.3498, device='cuda:0', grad_fn=<AddBackward0>)
0.79245907
tensor(0.4814, device='cuda:0', grad_fn=<AddBackward0>)
0.79230773
tensor(0.3393, device='cuda:0', grad_fn=<AddBackward0>)
0.79231328
tensor(0.3523, device='cuda:0', grad_fn=<AddBackward0>)
0.79228067
tensor(0.4520, device='cuda:0', grad_fn=<AddBackward0>)
0.79231256
tensor(0.3301, device='cuda:0', grad_fn=<AddBackward0>)
0.79204214
tensor(0.2747, device='cuda:0', grad_fn=<AddBackward0>)
0.79210734
tensor(0.4046, device='cuda:0', grad_fn=<AddBackward0>)
0.79200536
tensor(0.4042, device='cuda:0', grad_fn=<AddBackward0>)
0.79189038
tensor(0.3327, device='cuda:0', grad_fn=<AddBackward0>)
0.79179758
tensor(0.3682, device='cuda:0', grad_fn=<AddBackward0>)
0.79176062
tensor(0.3095, device='cuda:0', grad_fn=<AddBackward0>)
0.79181629
tensor(0.3449, device='cuda:0', grad_fn=<AddBackward0>)
0.79178095
tensor(0.3742, device='cuda:0', grad_fn=<AddBackward0>)
0.79209518
tensor(0.3775, device='cuda:0', grad_fn=<AddBackward0>)
0.79210359
tensor(0.2780, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][   20/  196]   Loss 0.363530   Top1 87.363281   Top5 99.628906   BatchTime 0.378724   LR 0.000097
0.79209995
tensor(0.3544, device='cuda:0', grad_fn=<AddBackward0>)
0.79219943
tensor(0.3277, device='cuda:0', grad_fn=<AddBackward0>)
0.79222065
tensor(0.3752, device='cuda:0', grad_fn=<AddBackward0>)
0.79233778
tensor(0.3245, device='cuda:0', grad_fn=<AddBackward0>)
0.79204482
tensor(0.3694, device='cuda:0', grad_fn=<AddBackward0>)
0.79174274
tensor(0.3719, device='cuda:0', grad_fn=<AddBackward0>)
0.79153299
tensor(0.3212, device='cuda:0', grad_fn=<AddBackward0>)
0.79161060
tensor(0.3153, device='cuda:0', grad_fn=<AddBackward0>)
0.79177779
tensor(0.4048, device='cuda:0', grad_fn=<AddBackward0>)
0.79146725
tensor(0.4117, device='cuda:0', grad_fn=<AddBackward0>)
0.79112297
tensor(0.3221, device='cuda:0', grad_fn=<AddBackward0>)
0.79101562
tensor(0.3579, device='cuda:0', grad_fn=<AddBackward0>)
0.79093623
tensor(0.3377, device='cuda:0', grad_fn=<AddBackward0>)
0.79083818
tensor(0.3218, device='cuda:0', grad_fn=<AddBackward0>)
0.79057771
tensor(0.3871, device='cuda:0', grad_fn=<AddBackward0>)
0.79057938
tensor(0.2477, device='cuda:0', grad_fn=<AddBackward0>)
0.79067266
tensor(0.2451, device='cuda:0', grad_fn=<AddBackward0>)
0.79071748
tensor(0.3539, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][   40/  196]   Loss 0.351811   Top1 87.763672   Top5 99.619141   BatchTime 0.356841   LR 0.000091
0.79076070
tensor(0.3389, device='cuda:0', grad_fn=<AddBackward0>)
0.79081249
tensor(0.3133, device='cuda:0', grad_fn=<AddBackward0>)
0.79075444
tensor(0.4331, device='cuda:0', grad_fn=<AddBackward0>)
0.79104322
tensor(0.4100, device='cuda:0', grad_fn=<AddBackward0>)
0.79070622
tensor(0.3047, device='cuda:0', grad_fn=<AddBackward0>)
0.79067588
tensor(0.3793, device='cuda:0', grad_fn=<AddBackward0>)
0.79060531
tensor(0.2441, device='cuda:0', grad_fn=<AddBackward0>)
0.79061860
tensor(0.2711, device='cuda:0', grad_fn=<AddBackward0>)
0.79063416
tensor(0.2360, device='cuda:0', grad_fn=<AddBackward0>)
0.79076695
tensor(0.3193, device='cuda:0', grad_fn=<AddBackward0>)
0.79102451
tensor(0.3951, device='cuda:0', grad_fn=<AddBackward0>)
0.79106480
tensor(0.3106, device='cuda:0', grad_fn=<AddBackward0>)
0.79119718
tensor(0.3561, device='cuda:0', grad_fn=<AddBackward0>)
0.79120010
tensor(0.3518, device='cuda:0', grad_fn=<AddBackward0>)
0.79119396
tensor(0.3571, device='cuda:0', grad_fn=<AddBackward0>)
0.79112548
tensor(0.3392, device='cuda:0', grad_fn=<AddBackward0>)
0.79124975
tensor(0.2875, device='cuda:0', grad_fn=<AddBackward0>)
0.79102111
tensor(0.3730, device='cuda:0', grad_fn=<AddBackward0>)
0.79098827
tensor(0.3094, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][   60/  196]   Loss 0.346437   Top1 88.007812   Top5 99.628906   BatchTime 0.343085   LR 0.000085
0.79117858
tensor(0.3451, device='cuda:0', grad_fn=<AddBackward0>)
0.79100657
tensor(0.3569, device='cuda:0', grad_fn=<AddBackward0>)
0.79126549
tensor(0.3343, device='cuda:0', grad_fn=<AddBackward0>)
0.79148465
tensor(0.3043, device='cuda:0', grad_fn=<AddBackward0>)
0.79170853
tensor(0.3191, device='cuda:0', grad_fn=<AddBackward0>)
0.79175407
tensor(0.3376, device='cuda:0', grad_fn=<AddBackward0>)
0.79156440
tensor(0.3856, device='cuda:0', grad_fn=<AddBackward0>)
0.79139274
tensor(0.3521, device='cuda:0', grad_fn=<AddBackward0>)
0.79123282
tensor(0.3633, device='cuda:0', grad_fn=<AddBackward0>)
0.79112375
tensor(0.3151, device='cuda:0', grad_fn=<AddBackward0>)
0.79108137
tensor(0.3324, device='cuda:0', grad_fn=<AddBackward0>)
0.79097277
tensor(0.4140, device='cuda:0', grad_fn=<AddBackward0>)
0.79105383
tensor(0.3111, device='cuda:0', grad_fn=<AddBackward0>)
0.79119617
tensor(0.4178, device='cuda:0', grad_fn=<AddBackward0>)
0.79118133
tensor(0.4028, device='cuda:0', grad_fn=<AddBackward0>)
0.79119068
tensor(0.3187, device='cuda:0', grad_fn=<AddBackward0>)
0.79108292
tensor(0.2648, device='cuda:0', grad_fn=<AddBackward0>)
0.79124337
tensor(0.3149, device='cuda:0', grad_fn=<AddBackward0>)
0.79118186
tensor(0.3176, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][   80/  196]   Loss 0.343735   Top1 88.325195   Top5 99.633789   BatchTime 0.338407   LR 0.000079
0.79121619
tensor(0.3009, device='cuda:0', grad_fn=<AddBackward0>)
0.79122090
tensor(0.3055, device='cuda:0', grad_fn=<AddBackward0>)
0.79113346
tensor(0.3284, device='cuda:0', grad_fn=<AddBackward0>)
0.79106790
tensor(0.3065, device='cuda:0', grad_fn=<AddBackward0>)
0.79115742
tensor(0.4044, device='cuda:0', grad_fn=<AddBackward0>)
0.79096097
tensor(0.3334, device='cuda:0', grad_fn=<AddBackward0>)
0.79073381
tensor(0.2682, device='cuda:0', grad_fn=<AddBackward0>)
0.79074210
tensor(0.3802, device='cuda:0', grad_fn=<AddBackward0>)
0.79077506
tensor(0.3462, device='cuda:0', grad_fn=<AddBackward0>)
0.79070950
tensor(0.2921, device='cuda:0', grad_fn=<AddBackward0>)
0.79081523
tensor(0.2989, device='cuda:0', grad_fn=<AddBackward0>)
0.79083657
tensor(0.3666, device='cuda:0', grad_fn=<AddBackward0>)
0.79071712
tensor(0.3132, device='cuda:0', grad_fn=<AddBackward0>)
0.79067677
tensor(0.2962, device='cuda:0', grad_fn=<AddBackward0>)
0.79069757
tensor(0.2731, device='cuda:0', grad_fn=<AddBackward0>)
0.79076290
tensor(0.3452, device='cuda:0', grad_fn=<AddBackward0>)
0.79067153
tensor(0.2748, device='cuda:0', grad_fn=<AddBackward0>)
0.79073745
tensor(0.2987, device='cuda:0', grad_fn=<AddBackward0>)
0.79077643
tensor(0.3285, device='cuda:0', grad_fn=<AddBackward0>)
0.79084384
tensor(0.3798, device='cuda:0', grad_fn=<AddBackward0>)
0.79075754
tensor(0.3168, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][  100/  196]   Loss 0.339744   Top1 88.390625   Top5 99.648438   BatchTime 0.329766   LR 0.000073
0.79059476
tensor(0.3664, device='cuda:0', grad_fn=<AddBackward0>)
0.79066008
tensor(0.2924, device='cuda:0', grad_fn=<AddBackward0>)
0.79045475
tensor(0.3004, device='cuda:0', grad_fn=<AddBackward0>)
0.79060316
tensor(0.3632, device='cuda:0', grad_fn=<AddBackward0>)
0.79041821
tensor(0.4002, device='cuda:0', grad_fn=<AddBackward0>)
0.79053462
tensor(0.3979, device='cuda:0', grad_fn=<AddBackward0>)
0.79071707
tensor(0.4188, device='cuda:0', grad_fn=<AddBackward0>)
0.79043782
tensor(0.3406, device='cuda:0', grad_fn=<AddBackward0>)
0.79036099
tensor(0.3424, device='cuda:0', grad_fn=<AddBackward0>)
0.79033935
tensor(0.3095, device='cuda:0', grad_fn=<AddBackward0>)
0.79013693
tensor(0.3425, device='cuda:0', grad_fn=<AddBackward0>)
0.79049790
tensor(0.3941, device='cuda:0', grad_fn=<AddBackward0>)
0.79057527
tensor(0.2881, device='cuda:0', grad_fn=<AddBackward0>)
0.79070312
tensor(0.2957, device='cuda:0', grad_fn=<AddBackward0>)
0.79101044
tensor(0.2747, device='cuda:0', grad_fn=<AddBackward0>)
0.79089868
tensor(0.2823, device='cuda:0', grad_fn=<AddBackward0>)
0.79089153
tensor(0.2956, device='cuda:0', grad_fn=<AddBackward0>)
0.79069799
tensor(0.3300, device='cuda:0', grad_fn=<AddBackward0>)
0.79068369
tensor(0.3319, device='cuda:0', grad_fn=<AddBackward0>)
0.79055768
tensor(0.2675, device='cuda:0', grad_fn=<AddBackward0>)
0.79063576
tensor(0.3513, device='cuda:0', grad_fn=<AddBackward0>)
0.79037410
tensor(0.3502, device='cuda:0', grad_fn=<AddBackward0>)
0.79039669
tensor(0.3131, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][  120/  196]   Loss 0.338867   Top1 88.457031   Top5 99.641927   BatchTime 0.331171   LR 0.000067
0.79054666
tensor(0.3386, device='cuda:0', grad_fn=<AddBackward0>)
0.79061896
tensor(0.3738, device='cuda:0', grad_fn=<AddBackward0>)
0.79057771
tensor(0.3256, device='cuda:0', grad_fn=<AddBackward0>)
0.79063129
tensor(0.3150, device='cuda:0', grad_fn=<AddBackward0>)
0.79057521
tensor(0.3832, device='cuda:0', grad_fn=<AddBackward0>)
0.79049289
tensor(0.4158, device='cuda:0', grad_fn=<AddBackward0>)
0.79034239
tensor(0.2705, device='cuda:0', grad_fn=<AddBackward0>)
0.79028958
tensor(0.2759, device='cuda:0', grad_fn=<AddBackward0>)
0.79010063
tensor(0.3251, device='cuda:0', grad_fn=<AddBackward0>)
0.79008585
tensor(0.3334, device='cuda:0', grad_fn=<AddBackward0>)
0.78995097
tensor(0.3058, device='cuda:0', grad_fn=<AddBackward0>)
0.79008585
tensor(0.3212, device='cuda:0', grad_fn=<AddBackward0>)
0.78992754
tensor(0.3522, device='cuda:0', grad_fn=<AddBackward0>)
0.78961790
tensor(0.3974, device='cuda:0', grad_fn=<AddBackward0>)
0.78960317
tensor(0.3128, device='cuda:0', grad_fn=<AddBackward0>)
0.78966749
tensor(0.3134, device='cuda:0', grad_fn=<AddBackward0>)
0.78909928
tensor(0.3140, device='cuda:0', grad_fn=<AddBackward0>)
0.78902650
tensor(0.3135, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][  140/  196]   Loss 0.338086   Top1 88.440290   Top5 99.631696   BatchTime 0.332709   LR 0.000062
0.78883851
tensor(0.3765, device='cuda:0', grad_fn=<AddBackward0>)
0.78880084
tensor(0.3043, device='cuda:0', grad_fn=<AddBackward0>)
0.78872502
tensor(0.3613, device='cuda:0', grad_fn=<AddBackward0>)
0.78849536
tensor(0.3766, device='cuda:0', grad_fn=<AddBackward0>)
0.78854245
tensor(0.3170, device='cuda:0', grad_fn=<AddBackward0>)
0.78847563
tensor(0.3436, device='cuda:0', grad_fn=<AddBackward0>)
0.78839666
tensor(0.3769, device='cuda:0', grad_fn=<AddBackward0>)
0.78841501
tensor(0.2854, device='cuda:0', grad_fn=<AddBackward0>)
0.78821409
tensor(0.4244, device='cuda:0', grad_fn=<AddBackward0>)
0.78818887
tensor(0.3165, device='cuda:0', grad_fn=<AddBackward0>)
0.78822982
tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)
0.78825271
tensor(0.3978, device='cuda:0', grad_fn=<AddBackward0>)
0.78833866
tensor(0.3308, device='cuda:0', grad_fn=<AddBackward0>)
0.78791296
tensor(0.2503, device='cuda:0', grad_fn=<AddBackward0>)
0.78783572
tensor(0.3506, device='cuda:0', grad_fn=<AddBackward0>)
0.78823417
tensor(0.3856, device='cuda:0', grad_fn=<AddBackward0>)
0.78815329
tensor(0.2347, device='cuda:0', grad_fn=<AddBackward0>)
0.78802818
tensor(0.2959, device='cuda:0', grad_fn=<AddBackward0>)
0.78799629
tensor(0.3768, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][  160/  196]   Loss 0.337231   Top1 88.540039   Top5 99.645996   BatchTime 0.329244   LR 0.000057
0.78805184
tensor(0.3101, device='cuda:0', grad_fn=<AddBackward0>)
0.78801584
tensor(0.3597, device='cuda:0', grad_fn=<AddBackward0>)
0.78781891
tensor(0.2559, device='cuda:0', grad_fn=<AddBackward0>)
0.78782880
tensor(0.2550, device='cuda:0', grad_fn=<AddBackward0>)
0.78770369
tensor(0.2971, device='cuda:0', grad_fn=<AddBackward0>)
0.78760618
tensor(0.2925, device='cuda:0', grad_fn=<AddBackward0>)
0.78756028
tensor(0.3718, device='cuda:0', grad_fn=<AddBackward0>)
0.78746355
tensor(0.2978, device='cuda:0', grad_fn=<AddBackward0>)
0.78726834
tensor(0.2806, device='cuda:0', grad_fn=<AddBackward0>)
0.78724921
tensor(0.2904, device='cuda:0', grad_fn=<AddBackward0>)
0.78729695
tensor(0.3367, device='cuda:0', grad_fn=<AddBackward0>)
0.78670424
tensor(0.3631, device='cuda:0', grad_fn=<AddBackward0>)
0.78619343
tensor(0.3187, device='cuda:0', grad_fn=<AddBackward0>)
0.78601754
tensor(0.2966, device='cuda:0', grad_fn=<AddBackward0>)
0.78550828
tensor(0.3680, device='cuda:0', grad_fn=<AddBackward0>)
0.78504837
tensor(0.3604, device='cuda:0', grad_fn=<AddBackward0>)
0.78481925
tensor(0.3779, device='cuda:0', grad_fn=<AddBackward0>)
0.78464365
tensor(0.3441, device='cuda:0', grad_fn=<AddBackward0>)
0.78313118
tensor(0.2595, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [7][  180/  196]   Loss 0.336680   Top1 88.587240   Top5 99.650608   BatchTime 0.327677   LR 0.000052
0.78280216
tensor(0.3079, device='cuda:0', grad_fn=<AddBackward0>)
0.78216118
tensor(0.3837, device='cuda:0', grad_fn=<AddBackward0>)
0.78142965
tensor(0.4183, device='cuda:0', grad_fn=<AddBackward0>)
0.78085160
tensor(0.4255, device='cuda:0', grad_fn=<AddBackward0>)
0.78043342
tensor(0.2419, device='cuda:0', grad_fn=<AddBackward0>)
0.77992409
tensor(0.3672, device='cuda:0', grad_fn=<AddBackward0>)
0.77952677
tensor(0.2849, device='cuda:0', grad_fn=<AddBackward0>)
0.77922755
tensor(0.3306, device='cuda:0', grad_fn=<AddBackward0>)
0.77912456
tensor(0.2827, device='cuda:0', grad_fn=<AddBackward0>)
0.77893621
tensor(0.2818, device='cuda:0', grad_fn=<AddBackward0>)
0.77909160
tensor(0.2882, device='cuda:0', grad_fn=<AddBackward0>)
0.77871037
tensor(0.4029, device='cuda:0', grad_fn=<AddBackward0>)
0.77814305
tensor(0.2655, device='cuda:0', grad_fn=<AddBackward0>)
0.77750772
tensor(0.3324, device='cuda:0', grad_fn=<AddBackward0>)
0.77713060
tensor(0.3213, device='cuda:0', grad_fn=<AddBackward0>)
0.77681822
tensor(0.3331, device='cuda:0', grad_fn=<AddBackward0>)
0.77614850
tensor(0.2982, device='cuda:0', grad_fn=<AddBackward0>)
0.77560943
tensor(0.3523, device='cuda:0', grad_fn=<AddBackward0>)
0.77526295
tensor(0.3478, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 88.620    Top5: 99.660    Loss: 0.335
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77492052
tensor(0.2980, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 0.393673   Top1 86.464844   Top5 99.433594   BatchTime 0.130325
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.1250)
features.1.conv.0 tensor(0.0684)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1146)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.1999)
features.3.conv.0 tensor(0.0770)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1083)
features.4.conv.0 tensor(0.0610)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1852)
features.5.conv.0 tensor(0.2752)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.0991)
features.6.conv.0 tensor(0.0498)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0866)
features.7.conv.0 tensor(0.1780)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.1941)
features.8.conv.0 tensor(0.5234)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.1519)
features.9.conv.0 tensor(0.4351)
features.9.conv.3 tensor(0.5668)
features.9.conv.6 tensor(0.1369)
features.10.conv.0 tensor(0.0676)
features.10.conv.3 tensor(0.1088)
features.10.conv.6 tensor(0.1014)
features.11.conv.0 tensor(0.7143)
features.11.conv.3 tensor(0.6493)
features.11.conv.6 tensor(0.1892)
features.12.conv.0 tensor(0.7070)
features.12.conv.3 tensor(0.6738)
features.12.conv.6 tensor(0.1991)
features.13.conv.0 tensor(0.1977)
features.13.conv.3 tensor(0.4917)
features.13.conv.6 tensor(0.0921)
features.14.conv.0 tensor(0.8598)
features.14.conv.3 tensor(0.8249)
features.14.conv.6 tensor(0.9306)
features.15.conv.0 tensor(0.8190)
features.15.conv.3 tensor(0.8353)
features.15.conv.6 tensor(0.9316)
features.16.conv.0 tensor(0.4614)
features.16.conv.3 tensor(0.8066)
features.16.conv.6 tensor(0.0862)
conv.0 tensor(0.0939)
tensor(890339.) 2188896.0
INFO - Validation [7][   40/   40]   Loss 0.388468   Top1 86.830000   Top5 99.520000   BatchTime 0.091392
INFO - ==> Top1: 86.830    Top5: 99.520    Loss: 0.388
INFO - ==> Sparsity : 0.407
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 85.430   Top5: 99.370]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 84.570   Top5: 99.450]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.77471209
tensor(0.3703, device='cuda:0', grad_fn=<AddBackward0>)
0.77438158
tensor(0.3971, device='cuda:0', grad_fn=<AddBackward0>)
0.77392465
tensor(0.3490, device='cuda:0', grad_fn=<AddBackward0>)
0.77363938
tensor(0.3162, device='cuda:0', grad_fn=<AddBackward0>)
0.77310336
tensor(0.3074, device='cuda:0', grad_fn=<AddBackward0>)
0.77286333
tensor(0.2452, device='cuda:0', grad_fn=<AddBackward0>)
0.77260095
tensor(0.2461, device='cuda:0', grad_fn=<AddBackward0>)
0.77234256
tensor(0.2675, device='cuda:0', grad_fn=<AddBackward0>)
0.77213925
tensor(0.2473, device='cuda:0', grad_fn=<AddBackward0>)
0.77170330
tensor(0.2745, device='cuda:0', grad_fn=<AddBackward0>)
0.77144498
tensor(0.3222, device='cuda:0', grad_fn=<AddBackward0>)
0.77117008
tensor(0.3373, device='cuda:0', grad_fn=<AddBackward0>)
0.77103245
tensor(0.2579, device='cuda:0', grad_fn=<AddBackward0>)
0.77061439
tensor(0.4125, device='cuda:0', grad_fn=<AddBackward0>)
0.77042323
tensor(0.2629, device='cuda:0', grad_fn=<AddBackward0>)
0.77015716
tensor(0.2314, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][   20/  196]   Loss 0.302736   Top1 89.746094   Top5 99.726562   BatchTime 0.315609   LR 0.000043
0.77024728
tensor(0.2826, device='cuda:0', grad_fn=<AddBackward0>)
0.77016920
tensor(0.3440, device='cuda:0', grad_fn=<AddBackward0>)
0.76996148
tensor(0.3240, device='cuda:0', grad_fn=<AddBackward0>)
0.76971853
tensor(0.2594, device='cuda:0', grad_fn=<AddBackward0>)
0.76923901
tensor(0.3136, device='cuda:0', grad_fn=<AddBackward0>)
0.76870334
tensor(0.3751, device='cuda:0', grad_fn=<AddBackward0>)
0.76820844
tensor(0.4169, device='cuda:0', grad_fn=<AddBackward0>)
0.76800525
tensor(0.3130, device='cuda:0', grad_fn=<AddBackward0>)
0.76786774
tensor(0.3267, device='cuda:0', grad_fn=<AddBackward0>)
0.76754910
tensor(0.2475, device='cuda:0', grad_fn=<AddBackward0>)
0.76729208
tensor(0.2887, device='cuda:0', grad_fn=<AddBackward0>)
0.76708633
tensor(0.3283, device='cuda:0', grad_fn=<AddBackward0>)
0.76675916
tensor(0.3449, device='cuda:0', grad_fn=<AddBackward0>)
0.76676154
tensor(0.2956, device='cuda:0', grad_fn=<AddBackward0>)
0.76673234
tensor(0.2974, device='cuda:0', grad_fn=<AddBackward0>)
0.76678091
tensor(0.2865, device='cuda:0', grad_fn=<AddBackward0>)
0.76684624
tensor(0.2962, device='cuda:0', grad_fn=<AddBackward0>)
0.76645476
tensor(0.2835, device='cuda:0', grad_fn=<AddBackward0>)
0.76620877
tensor(0.3239, device='cuda:0', grad_fn=<AddBackward0>)
0.76609737
tensor(0.2334, device='cuda:0', grad_fn=<AddBackward0>)
0.76597238
tensor(0.2826, device='cuda:0', grad_fn=<AddBackward0>)
0.76599449
tensor(0.3119, device='cuda:0', grad_fn=<AddBackward0>)
0.76570553
tensor(0.2954, device='cuda:0', grad_fn=<AddBackward0>)
0.76568896
tensor(0.3199, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][   40/  196]   Loss 0.305890   Top1 89.677734   Top5 99.736328   BatchTime 0.325401   LR 0.000039
0.76589406
tensor(0.3329, device='cuda:0', grad_fn=<AddBackward0>)
0.76596707
tensor(0.3301, device='cuda:0', grad_fn=<AddBackward0>)
0.76595432
tensor(0.3955, device='cuda:0', grad_fn=<AddBackward0>)
0.76564205
tensor(0.2721, device='cuda:0', grad_fn=<AddBackward0>)
0.76566291
tensor(0.2233, device='cuda:0', grad_fn=<AddBackward0>)
0.76557308
tensor(0.3165, device='cuda:0', grad_fn=<AddBackward0>)
0.76512337
tensor(0.2356, device='cuda:0', grad_fn=<AddBackward0>)
0.76514012
tensor(0.3406, device='cuda:0', grad_fn=<AddBackward0>)
0.76528305
tensor(0.3245, device='cuda:0', grad_fn=<AddBackward0>)
0.76535732
tensor(0.2723, device='cuda:0', grad_fn=<AddBackward0>)
0.76541293
tensor(0.3036, device='cuda:0', grad_fn=<AddBackward0>)
0.76526248
tensor(0.2838, device='cuda:0', grad_fn=<AddBackward0>)
0.76498312
tensor(0.3264, device='cuda:0', grad_fn=<AddBackward0>)
0.76517135
tensor(0.4124, device='cuda:0', grad_fn=<AddBackward0>)
0.76512498
tensor(0.3822, device='cuda:0', grad_fn=<AddBackward0>)
0.76481521
tensor(0.3123, device='cuda:0', grad_fn=<AddBackward0>)
0.76452041
tensor(0.2798, device='cuda:0', grad_fn=<AddBackward0>)
0.76449955
tensor(0.3469, device='cuda:0', grad_fn=<AddBackward0>)
0.76432747
tensor(0.3094, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][   60/  196]   Loss 0.308100   Top1 89.511719   Top5 99.752604   BatchTime 0.324944   LR 0.000035
0.76411682
tensor(0.2502, device='cuda:0', grad_fn=<AddBackward0>)
0.76393992
tensor(0.2755, device='cuda:0', grad_fn=<AddBackward0>)
0.76379991
tensor(0.3945, device='cuda:0', grad_fn=<AddBackward0>)
0.76404393
tensor(0.2760, device='cuda:0', grad_fn=<AddBackward0>)
0.76376003
tensor(0.3940, device='cuda:0', grad_fn=<AddBackward0>)
0.76357722
tensor(0.3188, device='cuda:0', grad_fn=<AddBackward0>)
0.76359040
tensor(0.2474, device='cuda:0', grad_fn=<AddBackward0>)
0.76362175
tensor(0.3895, device='cuda:0', grad_fn=<AddBackward0>)
0.76320076
tensor(0.3227, device='cuda:0', grad_fn=<AddBackward0>)
0.76312798
tensor(0.3247, device='cuda:0', grad_fn=<AddBackward0>)
0.76301968
tensor(0.3454, device='cuda:0', grad_fn=<AddBackward0>)
0.76284373
tensor(0.3177, device='cuda:0', grad_fn=<AddBackward0>)
0.76232284
tensor(0.2838, device='cuda:0', grad_fn=<AddBackward0>)
0.76199186
tensor(0.3851, device='cuda:0', grad_fn=<AddBackward0>)
0.76171809
tensor(0.3194, device='cuda:0', grad_fn=<AddBackward0>)
0.76167780
tensor(0.3128, device='cuda:0', grad_fn=<AddBackward0>)
0.76180130
tensor(0.3132, device='cuda:0', grad_fn=<AddBackward0>)
0.76192671
tensor(0.2645, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][   80/  196]   Loss 0.311748   Top1 89.433594   Top5 99.741211   BatchTime 0.327137   LR 0.000031
0.76156908
tensor(0.2885, device='cuda:0', grad_fn=<AddBackward0>)
0.76139468
tensor(0.3318, device='cuda:0', grad_fn=<AddBackward0>)
0.76125562
tensor(0.3483, device='cuda:0', grad_fn=<AddBackward0>)
0.76156932
tensor(0.2948, device='cuda:0', grad_fn=<AddBackward0>)
0.76136577
tensor(0.3560, device='cuda:0', grad_fn=<AddBackward0>)
0.76133132
tensor(0.4017, device='cuda:0', grad_fn=<AddBackward0>)
0.76147169
tensor(0.3638, device='cuda:0', grad_fn=<AddBackward0>)
0.76129407
tensor(0.1847, device='cuda:0', grad_fn=<AddBackward0>)
0.76118165
tensor(0.3785, device='cuda:0', grad_fn=<AddBackward0>)
0.76142764
tensor(0.3231, device='cuda:0', grad_fn=<AddBackward0>)
0.76166713
tensor(0.4484, device='cuda:0', grad_fn=<AddBackward0>)
0.76144993
tensor(0.3353, device='cuda:0', grad_fn=<AddBackward0>)
0.76142448
tensor(0.3076, device='cuda:0', grad_fn=<AddBackward0>)
0.76123911
tensor(0.3012, device='cuda:0', grad_fn=<AddBackward0>)
0.76106083
tensor(0.3012, device='cuda:0', grad_fn=<AddBackward0>)
0.76078618
tensor(0.3027, device='cuda:0', grad_fn=<AddBackward0>)
0.76066798
tensor(0.3372, device='cuda:0', grad_fn=<AddBackward0>)
0.76071519
tensor(0.2664, device='cuda:0', grad_fn=<AddBackward0>)
0.76059467
tensor(0.2786, device='cuda:0', grad_fn=<AddBackward0>)
0.76071686
tensor(0.2708, device='cuda:0', grad_fn=<AddBackward0>)
0.76053429
tensor(0.3133, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][  100/  196]   Loss 0.312042   Top1 89.484375   Top5 99.757812   BatchTime 0.316303   LR 0.000027
0.76063436
tensor(0.2382, device='cuda:0', grad_fn=<AddBackward0>)
0.76071113
tensor(0.2609, device='cuda:0', grad_fn=<AddBackward0>)
0.76067138
tensor(0.2777, device='cuda:0', grad_fn=<AddBackward0>)
0.76070142
tensor(0.2983, device='cuda:0', grad_fn=<AddBackward0>)
0.76049715
tensor(0.2829, device='cuda:0', grad_fn=<AddBackward0>)
0.76024055
tensor(0.2898, device='cuda:0', grad_fn=<AddBackward0>)
0.76029921
tensor(0.3301, device='cuda:0', grad_fn=<AddBackward0>)
0.76061261
tensor(0.3209, device='cuda:0', grad_fn=<AddBackward0>)
0.76060039
tensor(0.2906, device='cuda:0', grad_fn=<AddBackward0>)
0.76052600
tensor(0.2263, device='cuda:0', grad_fn=<AddBackward0>)
0.76059496
tensor(0.3751, device='cuda:0', grad_fn=<AddBackward0>)
0.76062471
tensor(0.3613, device='cuda:0', grad_fn=<AddBackward0>)
0.76034015
tensor(0.2455, device='cuda:0', grad_fn=<AddBackward0>)
0.76013333
tensor(0.2712, device='cuda:0', grad_fn=<AddBackward0>)
0.76012528
tensor(0.2927, device='cuda:0', grad_fn=<AddBackward0>)
0.76007426
tensor(0.2370, device='cuda:0', grad_fn=<AddBackward0>)
0.76016563
tensor(0.2786, device='cuda:0', grad_fn=<AddBackward0>)
0.76016694
tensor(0.2932, device='cuda:0', grad_fn=<AddBackward0>)
0.76041257
tensor(0.2565, device='cuda:0', grad_fn=<AddBackward0>)
0.76071668
tensor(0.3358, device='cuda:0', grad_fn=<AddBackward0>)
0.76027036
tensor(0.2602, device='cuda:0', grad_fn=<AddBackward0>)
0.75933963
tensor(0.3055, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][  120/  196]   Loss 0.308613   Top1 89.645182   Top5 99.749349   BatchTime 0.310631   LR 0.000023
0.75909477
tensor(0.3391, device='cuda:0', grad_fn=<AddBackward0>)
0.75920790
tensor(0.2596, device='cuda:0', grad_fn=<AddBackward0>)
0.75916737
tensor(0.3717, device='cuda:0', grad_fn=<AddBackward0>)
0.75915051
tensor(0.2975, device='cuda:0', grad_fn=<AddBackward0>)
0.75920051
tensor(0.2837, device='cuda:0', grad_fn=<AddBackward0>)
0.75925869
tensor(0.3402, device='cuda:0', grad_fn=<AddBackward0>)
0.75920093
tensor(0.2922, device='cuda:0', grad_fn=<AddBackward0>)
0.75928164
tensor(0.3542, device='cuda:0', grad_fn=<AddBackward0>)
0.75925028
tensor(0.3228, device='cuda:0', grad_fn=<AddBackward0>)
0.75918007
tensor(0.3526, device='cuda:0', grad_fn=<AddBackward0>)
0.75925332
tensor(0.2700, device='cuda:0', grad_fn=<AddBackward0>)
0.75901282
tensor(0.2970, device='cuda:0', grad_fn=<AddBackward0>)
0.75910854
tensor(0.4000, device='cuda:0', grad_fn=<AddBackward0>)
0.75944102
tensor(0.2771, device='cuda:0', grad_fn=<AddBackward0>)
0.75955629
tensor(0.2611, device='cuda:0', grad_fn=<AddBackward0>)
0.75957364
tensor(0.3636, device='cuda:0', grad_fn=<AddBackward0>)
0.75936323
tensor(0.3427, device='cuda:0', grad_fn=<AddBackward0>)
0.75938696
tensor(0.3276, device='cuda:0', grad_fn=<AddBackward0>)
0.75928926
tensor(0.2787, device='cuda:0', grad_fn=<AddBackward0>)
0.75897890
tensor(0.2999, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][  140/  196]   Loss 0.309749   Top1 89.620536   Top5 99.737723   BatchTime 0.308185   LR 0.000020
0.75862712
tensor(0.2993, device='cuda:0', grad_fn=<AddBackward0>)
0.75847179
tensor(0.2852, device='cuda:0', grad_fn=<AddBackward0>)
0.75851399
tensor(0.3335, device='cuda:0', grad_fn=<AddBackward0>)
0.75849384
tensor(0.3165, device='cuda:0', grad_fn=<AddBackward0>)
0.75841534
tensor(0.3624, device='cuda:0', grad_fn=<AddBackward0>)
0.75859290
tensor(0.2720, device='cuda:0', grad_fn=<AddBackward0>)
0.75909621
tensor(0.2424, device='cuda:0', grad_fn=<AddBackward0>)
0.75933343
tensor(0.2596, device='cuda:0', grad_fn=<AddBackward0>)
0.75936925
tensor(0.3568, device='cuda:0', grad_fn=<AddBackward0>)
0.75928193
tensor(0.2636, device='cuda:0', grad_fn=<AddBackward0>)
0.75894219
tensor(0.3649, device='cuda:0', grad_fn=<AddBackward0>)
0.75875169
tensor(0.3592, device='cuda:0', grad_fn=<AddBackward0>)
0.75868285
tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)
0.75856179
tensor(0.3478, device='cuda:0', grad_fn=<AddBackward0>)
0.75846595
tensor(0.4085, device='cuda:0', grad_fn=<AddBackward0>)
0.75847352
tensor(0.2927, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [8][  160/  196]   Loss 0.310262   Top1 89.609375   Top5 99.724121   BatchTime 0.300968   LR 0.000017
0.75855887
tensor(0.3630, device='cuda:0', grad_fn=<AddBackward0>)
0.75871825
tensor(0.2902, device='cuda:0', grad_fn=<AddBackward0>)
0.75880736
tensor(0.3544, device='cuda:0', grad_fn=<AddBackward0>)
0.75886267
tensor(0.2969, device='cuda:0', grad_fn=<AddBackward0>)
0.75890619
tensor(0.3465, device='cuda:0', grad_fn=<AddBackward0>)
0.75882101
tensor(0.3721, device='cuda:0', grad_fn=<AddBackward0>)
0.75867409
tensor(0.2484, device='cuda:0', grad_fn=<AddBackward0>)
0.75871754
tensor(0.3036, device='cuda:0', grad_fn=<AddBackward0>)
0.75856441
tensor(0.2071, device='cuda:0', grad_fn=<AddBackward0>)
0.75776333
tensor(0.3585, device='cuda:0', grad_fn=<AddBackward0>)
0.75672233
tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)
0.75655133
tensor(0.3361, device='cuda:0', grad_fn=<AddBackward0>)
0.75663185
tensor(0.2736, device='cuda:0', grad_fn=<AddBackward0>)
0.75665641
tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)
0.75670356
tensor(0.2834, device='cuda:0', grad_fn=<AddBackward0>)
0.75669044
tensor(0.2975, device='cuda:0', grad_fn=<AddBackward0>)
0.75677067
tensor(0.3126, device='cuda:0', grad_fn=<AddBackward0>)
0.75664544
tensor(0.2671, device='cuda:0', grad_fn=<AddBackward0>)
0.75649357
tensor(0.3167, device='cuda:0', grad_fn=<AddBackward0>)
0.75642407
tensor(0.3288, device='cuda:0', grad_fn=<AddBackward0>)
0.75643957
tensor(0.3358, device='cuda:0', grad_fn=<AddBackward0>)
0.75663155
tensor(0.3332, device='cuda:0', grad_fn=<AddBackward0>)
0.75666982
tensor(0.3836, device='cuda:0', grad_fn=<AddBackward0>)
0.75662637
INFO - Training [8][  180/  196]   Loss 0.310173   Top1 89.583333   Top5 99.717882   BatchTime 0.296565   LR 0.000014
tensor(0.3555, device='cuda:0', grad_fn=<AddBackward0>)
0.75646538
tensor(0.3212, device='cuda:0', grad_fn=<AddBackward0>)
0.75652903
tensor(0.3710, device='cuda:0', grad_fn=<AddBackward0>)
0.75645679
tensor(0.2991, device='cuda:0', grad_fn=<AddBackward0>)
0.75630116
tensor(0.2889, device='cuda:0', grad_fn=<AddBackward0>)
0.75624496
tensor(0.2940, device='cuda:0', grad_fn=<AddBackward0>)
0.75612640
tensor(0.3434, device='cuda:0', grad_fn=<AddBackward0>)
0.75600022
tensor(0.2949, device='cuda:0', grad_fn=<AddBackward0>)
0.75596672
tensor(0.2531, device='cuda:0', grad_fn=<AddBackward0>)
0.75590169
tensor(0.2487, device='cuda:0', grad_fn=<AddBackward0>)
0.75585228
tensor(0.2795, device='cuda:0', grad_fn=<AddBackward0>)
0.75598294
tensor(0.4189, device='cuda:0', grad_fn=<AddBackward0>)
0.75603038
tensor(0.3029, device='cuda:0', grad_fn=<AddBackward0>)
0.75641471
tensor(0.2890, device='cuda:0', grad_fn=<AddBackward0>)
0.75662088
tensor(0.3322, device='cuda:0', grad_fn=<AddBackward0>)
0.75651181
tensor(0.2273, device='cuda:0', grad_fn=<AddBackward0>)
0.75625670
tensor(0.3598, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 89.572    Top5: 99.712    Loss: 0.310
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.388256   Top1 86.816406   Top5 99.394531   BatchTime 0.112533
INFO - Validation [8][   40/   40]   Loss 0.382897   Top1 87.090000   Top5 99.500000   BatchTime 0.084538
INFO - ==> Top1: 87.090    Top5: 99.500    Loss: 0.383
INFO - ==> Sparsity : 0.431
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 85.430   Top5: 99.370]
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0684)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1128)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.1953)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1100)
features.4.conv.0 tensor(0.0640)
features.4.conv.3 tensor(0.3113)
features.4.conv.6 tensor(0.1839)
features.5.conv.0 tensor(0.2856)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.0964)
features.6.conv.0 tensor(0.0514)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0873)
features.7.conv.0 tensor(0.1816)
features.7.conv.3 tensor(0.4546)
features.7.conv.6 tensor(0.1927)
features.8.conv.0 tensor(0.5382)
features.8.conv.3 tensor(0.5376)
features.8.conv.6 tensor(0.1522)
features.9.conv.0 tensor(0.4360)
features.9.conv.3 tensor(0.5645)
features.9.conv.6 tensor(0.1375)
features.10.conv.0 tensor(0.0687)
features.10.conv.3 tensor(0.1082)
features.10.conv.6 tensor(0.1014)
features.11.conv.0 tensor(0.7269)
features.11.conv.3 tensor(0.6497)
features.11.conv.6 tensor(0.1921)
features.12.conv.0 tensor(0.7209)
features.12.conv.3 tensor(0.6734)
features.12.conv.6 tensor(0.2464)
features.13.conv.0 tensor(0.1995)
features.13.conv.3 tensor(0.4931)
features.13.conv.6 tensor(0.1030)
features.14.conv.0 tensor(0.8622)
features.14.conv.3 tensor(0.8248)
features.14.conv.6 tensor(0.9314)
features.15.conv.0 tensor(0.8243)
features.15.conv.3 tensor(0.8353)
features.15.conv.6 tensor(0.9329)
features.16.conv.0 tensor(0.6151)
features.16.conv.3 tensor(0.8061)
features.16.conv.6 tensor(0.1573)
conv.0 tensor(0.0935)
tensor(943030.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.75617296
tensor(0.2930, device='cuda:0', grad_fn=<AddBackward0>)
0.75620538
tensor(0.3242, device='cuda:0', grad_fn=<AddBackward0>)
0.75628591
tensor(0.3128, device='cuda:0', grad_fn=<AddBackward0>)
0.75616425
tensor(0.2933, device='cuda:0', grad_fn=<AddBackward0>)
0.75615484
tensor(0.2927, device='cuda:0', grad_fn=<AddBackward0>)
0.75610620
tensor(0.2169, device='cuda:0', grad_fn=<AddBackward0>)
0.75603962
tensor(0.2591, device='cuda:0', grad_fn=<AddBackward0>)
0.75591683
tensor(0.2732, device='cuda:0', grad_fn=<AddBackward0>)
0.75586236
tensor(0.3160, device='cuda:0', grad_fn=<AddBackward0>)
0.75582314
tensor(0.3208, device='cuda:0', grad_fn=<AddBackward0>)
0.75571972
tensor(0.3039, device='cuda:0', grad_fn=<AddBackward0>)
0.75558054
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
0.75574535
tensor(0.2899, device='cuda:0', grad_fn=<AddBackward0>)
0.75594974
tensor(0.2976, device='cuda:0', grad_fn=<AddBackward0>)
0.75578117
tensor(0.3015, device='cuda:0', grad_fn=<AddBackward0>)
0.75602019
tensor(0.2898, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][   20/  196]   Loss 0.290384   Top1 90.136719   Top5 99.746094   BatchTime 0.351646   LR 0.000010
0.75618130
tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)
0.75600868
tensor(0.2904, device='cuda:0', grad_fn=<AddBackward0>)
0.75570202
tensor(0.3723, device='cuda:0', grad_fn=<AddBackward0>)
0.75556368
tensor(0.2294, device='cuda:0', grad_fn=<AddBackward0>)
0.75570816
tensor(0.2955, device='cuda:0', grad_fn=<AddBackward0>)
0.75555593
tensor(0.3780, device='cuda:0', grad_fn=<AddBackward0>)
0.75558496
tensor(0.3188, device='cuda:0', grad_fn=<AddBackward0>)
0.75565094
tensor(0.2873, device='cuda:0', grad_fn=<AddBackward0>)
0.75572008
tensor(0.3782, device='cuda:0', grad_fn=<AddBackward0>)
0.75564820
tensor(0.3080, device='cuda:0', grad_fn=<AddBackward0>)
0.75559765
tensor(0.2666, device='cuda:0', grad_fn=<AddBackward0>)
0.75548011
tensor(0.2981, device='cuda:0', grad_fn=<AddBackward0>)
0.75543034
tensor(0.3604, device='cuda:0', grad_fn=<AddBackward0>)
0.75503200
tensor(0.2956, device='cuda:0', grad_fn=<AddBackward0>)
0.75453877
tensor(0.3478, device='cuda:0', grad_fn=<AddBackward0>)
0.75393701
tensor(0.3524, device='cuda:0', grad_fn=<AddBackward0>)
0.75295901
tensor(0.2576, device='cuda:0', grad_fn=<AddBackward0>)
0.75208455
tensor(0.2447, device='cuda:0', grad_fn=<AddBackward0>)
0.75160253
tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)
0.75102770
tensor(0.2964, device='cuda:0', grad_fn=<AddBackward0>)
0.75084013
tensor(0.2713, device='cuda:0', grad_fn=<AddBackward0>)
0.75083327
tensor(0.3363, device='cuda:0', grad_fn=<AddBackward0>)
0.75083303
tensor(0.2878, device='cuda:0', grad_fn=<AddBackward0>)
0.75064343
tensor(0.2634, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][   40/  196]   Loss 0.296397   Top1 90.322266   Top5 99.726562   BatchTime 0.343687   LR 0.000008
0.75061691
tensor(0.2834, device='cuda:0', grad_fn=<AddBackward0>)
0.75050211
tensor(0.3218, device='cuda:0', grad_fn=<AddBackward0>)
0.75048840
tensor(0.2678, device='cuda:0', grad_fn=<AddBackward0>)
0.75028652
tensor(0.2486, device='cuda:0', grad_fn=<AddBackward0>)
0.75031805
tensor(0.3113, device='cuda:0', grad_fn=<AddBackward0>)
0.75033355
tensor(0.3327, device='cuda:0', grad_fn=<AddBackward0>)
0.75032765
tensor(0.2795, device='cuda:0', grad_fn=<AddBackward0>)
0.75022709
tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)
0.75018245
tensor(0.3818, device='cuda:0', grad_fn=<AddBackward0>)
0.75026941
tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)
0.75018805
tensor(0.3051, device='cuda:0', grad_fn=<AddBackward0>)
0.75020719
tensor(0.2607, device='cuda:0', grad_fn=<AddBackward0>)
0.75028217
tensor(0.2726, device='cuda:0', grad_fn=<AddBackward0>)
0.75031424
tensor(0.3274, device='cuda:0', grad_fn=<AddBackward0>)
0.75039148
tensor(0.2793, device='cuda:0', grad_fn=<AddBackward0>)
0.75041956
tensor(0.2670, device='cuda:0', grad_fn=<AddBackward0>)
0.75045455
tensor(0.2499, device='cuda:0', grad_fn=<AddBackward0>)
0.75042701
tensor(0.3044, device='cuda:0', grad_fn=<AddBackward0>)
0.75023973
tensor(0.3374, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][   60/  196]   Loss 0.295308   Top1 90.286458   Top5 99.733073   BatchTime 0.335280   LR 0.000006
0.75015473
tensor(0.3285, device='cuda:0', grad_fn=<AddBackward0>)
0.75019157
tensor(0.3385, device='cuda:0', grad_fn=<AddBackward0>)
0.75015920
tensor(0.3069, device='cuda:0', grad_fn=<AddBackward0>)
0.75017804
tensor(0.3207, device='cuda:0', grad_fn=<AddBackward0>)
0.75018388
tensor(0.2306, device='cuda:0', grad_fn=<AddBackward0>)
0.75005215
tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)
0.74993998
tensor(0.3151, device='cuda:0', grad_fn=<AddBackward0>)
0.75001234
tensor(0.2419, device='cuda:0', grad_fn=<AddBackward0>)
0.75001085
tensor(0.2376, device='cuda:0', grad_fn=<AddBackward0>)
0.74995017
tensor(0.3216, device='cuda:0', grad_fn=<AddBackward0>)
0.74992120
tensor(0.2357, device='cuda:0', grad_fn=<AddBackward0>)
0.74989128
tensor(0.3274, device='cuda:0', grad_fn=<AddBackward0>)
0.74993670
tensor(0.3222, device='cuda:0', grad_fn=<AddBackward0>)
0.74988800
tensor(0.2883, device='cuda:0', grad_fn=<AddBackward0>)
0.74991482
tensor(0.3455, device='cuda:0', grad_fn=<AddBackward0>)
0.74987769
tensor(0.2899, device='cuda:0', grad_fn=<AddBackward0>)
0.74967450
tensor(0.2969, device='cuda:0', grad_fn=<AddBackward0>)
0.74967438
tensor(0.2696, device='cuda:0', grad_fn=<AddBackward0>)
0.74965960
tensor(0.3239, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][   80/  196]   Loss 0.293508   Top1 90.307617   Top5 99.736328   BatchTime 0.328823   LR 0.000004
0.74962795
tensor(0.2696, device='cuda:0', grad_fn=<AddBackward0>)
0.74975914
tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)
0.74973273
tensor(0.2917, device='cuda:0', grad_fn=<AddBackward0>)
0.74991393
tensor(0.3290, device='cuda:0', grad_fn=<AddBackward0>)
0.74964768
tensor(0.2182, device='cuda:0', grad_fn=<AddBackward0>)
0.74962986
tensor(0.2595, device='cuda:0', grad_fn=<AddBackward0>)
0.74955952
tensor(0.2826, device='cuda:0', grad_fn=<AddBackward0>)
0.74952501
tensor(0.3470, device='cuda:0', grad_fn=<AddBackward0>)
0.74948257
tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)
0.74937731
tensor(0.2187, device='cuda:0', grad_fn=<AddBackward0>)
0.74933434
tensor(0.3246, device='cuda:0', grad_fn=<AddBackward0>)
0.74923652
tensor(0.2702, device='cuda:0', grad_fn=<AddBackward0>)
0.74866396
tensor(0.3215, device='cuda:0', grad_fn=<AddBackward0>)
0.74851149
tensor(0.3644, device='cuda:0', grad_fn=<AddBackward0>)
0.74863547
tensor(0.2856, device='cuda:0', grad_fn=<AddBackward0>)
0.74836975
tensor(0.3361, device='cuda:0', grad_fn=<AddBackward0>)
0.74830818
tensor(0.3394, device='cuda:0', grad_fn=<AddBackward0>)
0.74821520
tensor(0.3430, device='cuda:0', grad_fn=<AddBackward0>)
0.74814034
tensor(0.3637, device='cuda:0', grad_fn=<AddBackward0>)
0.74813056
tensor(0.2508, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][  100/  196]   Loss 0.294788   Top1 90.214844   Top5 99.753906   BatchTime 0.323627   LR 0.000003
0.74809885
tensor(0.3054, device='cuda:0', grad_fn=<AddBackward0>)
0.74813884
tensor(0.3299, device='cuda:0', grad_fn=<AddBackward0>)
0.74829781
tensor(0.3545, device='cuda:0', grad_fn=<AddBackward0>)
0.74830037
tensor(0.2900, device='cuda:0', grad_fn=<AddBackward0>)
0.74820757
tensor(0.2731, device='cuda:0', grad_fn=<AddBackward0>)
0.74829429
tensor(0.3156, device='cuda:0', grad_fn=<AddBackward0>)
0.74824053
tensor(0.2785, device='cuda:0', grad_fn=<AddBackward0>)
0.74802041
tensor(0.2645, device='cuda:0', grad_fn=<AddBackward0>)
0.74793297
tensor(0.3390, device='cuda:0', grad_fn=<AddBackward0>)
0.74759305
tensor(0.2571, device='cuda:0', grad_fn=<AddBackward0>)
0.74712205
tensor(0.2748, device='cuda:0', grad_fn=<AddBackward0>)
0.74684376
tensor(0.3145, device='cuda:0', grad_fn=<AddBackward0>)
0.74673355
tensor(0.3557, device='cuda:0', grad_fn=<AddBackward0>)
0.74671209
tensor(0.2811, device='cuda:0', grad_fn=<AddBackward0>)
0.74677509
tensor(0.2568, device='cuda:0', grad_fn=<AddBackward0>)
0.74662745
tensor(0.3019, device='cuda:0', grad_fn=<AddBackward0>)
0.74663746
tensor(0.2891, device='cuda:0', grad_fn=<AddBackward0>)
0.74678242
tensor(0.2647, device='cuda:0', grad_fn=<AddBackward0>)
0.74668670
tensor(0.2942, device='cuda:0', grad_fn=<AddBackward0>)
0.74666530
tensor(0.3223, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][  120/  196]   Loss 0.295566   Top1 90.175781   Top5 99.749349   BatchTime 0.321174   LR 0.000002
0.74669290
tensor(0.3513, device='cuda:0', grad_fn=<AddBackward0>)
0.74661177
tensor(0.3103, device='cuda:0', grad_fn=<AddBackward0>)
0.74660128
tensor(0.3292, device='cuda:0', grad_fn=<AddBackward0>)
0.74662054
tensor(0.2468, device='cuda:0', grad_fn=<AddBackward0>)
0.74649274
tensor(0.2796, device='cuda:0', grad_fn=<AddBackward0>)
0.74654013
tensor(0.2933, device='cuda:0', grad_fn=<AddBackward0>)
0.74651134
tensor(0.2921, device='cuda:0', grad_fn=<AddBackward0>)
0.74652404
tensor(0.2775, device='cuda:0', grad_fn=<AddBackward0>)
0.74642861
tensor(0.3972, device='cuda:0', grad_fn=<AddBackward0>)
0.74638784
tensor(0.2957, device='cuda:0', grad_fn=<AddBackward0>)
0.74626642
tensor(0.2984, device='cuda:0', grad_fn=<AddBackward0>)
0.74622601
tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)
0.74625283
tensor(0.2574, device='cuda:0', grad_fn=<AddBackward0>)
0.74625564
tensor(0.3433, device='cuda:0', grad_fn=<AddBackward0>)
0.74623805
tensor(0.3332, device='cuda:0', grad_fn=<AddBackward0>)
0.74638838
tensor(0.2684, device='cuda:0', grad_fn=<AddBackward0>)
0.74661750
tensor(0.2499, device='cuda:0', grad_fn=<AddBackward0>)
0.74651104
tensor(0.2869, device='cuda:0', grad_fn=<AddBackward0>)
0.74639398
tensor(0.2845, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][  140/  196]   Loss 0.295839   Top1 90.136719   Top5 99.768415   BatchTime 0.319438   LR 0.000001
0.74641842
tensor(0.3901, device='cuda:0', grad_fn=<AddBackward0>)
0.74637932
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.74639386
tensor(0.2629, device='cuda:0', grad_fn=<AddBackward0>)
0.74633861
tensor(0.2761, device='cuda:0', grad_fn=<AddBackward0>)
0.74632388
tensor(0.2964, device='cuda:0', grad_fn=<AddBackward0>)
0.74643523
tensor(0.2958, device='cuda:0', grad_fn=<AddBackward0>)
0.74653929
tensor(0.2823, device='cuda:0', grad_fn=<AddBackward0>)
0.74648011
tensor(0.2564, device='cuda:0', grad_fn=<AddBackward0>)
0.74653494
tensor(0.3413, device='cuda:0', grad_fn=<AddBackward0>)
0.74645090
tensor(0.3672, device='cuda:0', grad_fn=<AddBackward0>)
0.74626833
tensor(0.2956, device='cuda:0', grad_fn=<AddBackward0>)
0.74617583
tensor(0.3253, device='cuda:0', grad_fn=<AddBackward0>)
0.74608868
tensor(0.2880, device='cuda:0', grad_fn=<AddBackward0>)
0.74604243
tensor(0.2061, device='cuda:0', grad_fn=<AddBackward0>)
0.74602115
tensor(0.2683, device='cuda:0', grad_fn=<AddBackward0>)
0.74603337
tensor(0.3083, device='cuda:0', grad_fn=<AddBackward0>)
0.74610192
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
0.74620372
tensor(0.4084, device='cuda:0', grad_fn=<AddBackward0>)
0.74628121
tensor(0.2634, device='cuda:0', grad_fn=<AddBackward0>)
0.74650866
tensor(0.2878, device='cuda:0', grad_fn=<AddBackward0>)
0.74644625
tensor(0.2744, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][  160/  196]   Loss 0.295801   Top1 90.144043   Top5 99.772949   BatchTime 0.316385   LR 0.000000
0.74642998
tensor(0.2971, device='cuda:0', grad_fn=<AddBackward0>)
0.74646342
tensor(0.3169, device='cuda:0', grad_fn=<AddBackward0>)
0.74641621
tensor(0.3607, device='cuda:0', grad_fn=<AddBackward0>)
0.74634010
tensor(0.2507, device='cuda:0', grad_fn=<AddBackward0>)
0.74630171
tensor(0.2740, device='cuda:0', grad_fn=<AddBackward0>)
0.74630737
tensor(0.3556, device='cuda:0', grad_fn=<AddBackward0>)
0.74631333
tensor(0.2705, device='cuda:0', grad_fn=<AddBackward0>)
0.74626940
tensor(0.2747, device='cuda:0', grad_fn=<AddBackward0>)
0.74620897
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
0.74619579
tensor(0.2378, device='cuda:0', grad_fn=<AddBackward0>)
0.74620968
tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)
0.74620974
tensor(0.3568, device='cuda:0', grad_fn=<AddBackward0>)
0.74620783
tensor(0.3582, device='cuda:0', grad_fn=<AddBackward0>)
0.74630147
tensor(0.3786, device='cuda:0', grad_fn=<AddBackward0>)
0.74624926
tensor(0.3258, device='cuda:0', grad_fn=<AddBackward0>)
0.74625546
tensor(0.2977, device='cuda:0', grad_fn=<AddBackward0>)
0.74627036
tensor(0.2412, device='cuda:0', grad_fn=<AddBackward0>)
0.74623871
tensor(0.2426, device='cuda:0', grad_fn=<AddBackward0>)
0.74611294
tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [9][  180/  196]   Loss 0.294786   Top1 90.149740   Top5 99.782986   BatchTime 0.315956   LR 0.000000
0.74612159
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
0.74615872
tensor(0.2375, device='cuda:0', grad_fn=<AddBackward0>)
0.74615026
tensor(0.2935, device='cuda:0', grad_fn=<AddBackward0>)
0.74629694
tensor(0.2844, device='cuda:0', grad_fn=<AddBackward0>)
0.74626553
tensor(0.3472, device='cuda:0', grad_fn=<AddBackward0>)
0.74625605
tensor(0.2222, device='cuda:0', grad_fn=<AddBackward0>)
0.74627250
tensor(0.3382, device='cuda:0', grad_fn=<AddBackward0>)
0.74629730
tensor(0.3202, device='cuda:0', grad_fn=<AddBackward0>)
0.74626970
tensor(0.2954, device='cuda:0', grad_fn=<AddBackward0>)
0.74613363
tensor(0.3501, device='cuda:0', grad_fn=<AddBackward0>)
0.74602062
tensor(0.2448, device='cuda:0', grad_fn=<AddBackward0>)
0.74567765
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
0.74528277
tensor(0.2944, device='cuda:0', grad_fn=<AddBackward0>)
0.74475861
tensor(0.3747, device='cuda:0', grad_fn=<AddBackward0>)
0.74443531
tensor(0.2921, device='cuda:0', grad_fn=<AddBackward0>)
0.74401629
tensor(0.3397, device='cuda:0', grad_fn=<AddBackward0>)
0.74359113
tensor(0.3378, device='cuda:0', grad_fn=<AddBackward0>)
0.74342614
tensor(0.2331, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 90.132    Top5: 99.780    Loss: 0.295
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.74338835
tensor(0.3197, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [9][   20/   40]   Loss 0.387453   Top1 86.875000   Top5 99.375000   BatchTime 0.112134
INFO - Validation [9][   40/   40]   Loss 0.381298   Top1 87.220000   Top5 99.510000   BatchTime 0.082464
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0684)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.1134)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.2008)
features.3.conv.0 tensor(0.0744)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1109)
features.4.conv.0 tensor(0.0649)
features.4.conv.3 tensor(0.3096)
features.4.conv.6 tensor(0.1838)
features.5.conv.0 tensor(0.2852)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.1016)
features.6.conv.0 tensor(0.0505)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0873)
features.7.conv.0 tensor(0.1821)
features.7.conv.3 tensor(0.4546)
features.7.conv.6 tensor(0.1930)
features.8.conv.0 tensor(0.5414)
features.8.conv.3 tensor(0.5373)
features.8.conv.6 tensor(0.1514)
features.9.conv.0 tensor(0.4364)
features.9.conv.3 tensor(0.5645)
features.9.conv.6 tensor(0.1418)
features.10.conv.0 tensor(0.0693)
features.10.conv.3 tensor(0.1076)
features.10.conv.6 tensor(0.1032)
features.11.conv.0 tensor(0.7299)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.2031)
features.12.conv.0 tensor(0.7223)
features.12.conv.3 tensor(0.6734)
features.12.conv.6 tensor(0.2485)
features.13.conv.0 tensor(0.2002)
features.13.conv.3 tensor(0.4936)
features.13.conv.6 tensor(0.1054)
features.14.conv.0 tensor(0.8626)
features.14.conv.3 tensor(0.8248)
features.14.conv.6 tensor(0.9318)
features.15.conv.0 tensor(0.8251)
features.15.conv.3 tensor(0.8352)
features.15.conv.6 tensor(0.9332)
features.16.conv.0 tensor(0.6206)
features.16.conv.3 tensor(0.8060)
features.16.conv.6 tensor(0.1803)
conv.0 tensor(0.0934)
tensor(952745.) 2188896.0
INFO - ==> Top1: 87.220    Top5: 99.510    Loss: 0.381
INFO - ==> Sparsity : 0.435
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.74339157
tensor(0.2928, device='cuda:0', grad_fn=<AddBackward0>)
0.75258583
tensor(0.3399, device='cuda:0', grad_fn=<AddBackward0>)
0.75245976
tensor(0.3136, device='cuda:0', grad_fn=<AddBackward0>)
0.75246578
tensor(0.3524, device='cuda:0', grad_fn=<AddBackward0>)
0.75242078
tensor(0.2804, device='cuda:0', grad_fn=<AddBackward0>)
0.75224549
tensor(0.3625, device='cuda:0', grad_fn=<AddBackward0>)
0.75213224
tensor(0.3238, device='cuda:0', grad_fn=<AddBackward0>)
0.75116104
tensor(0.4394, device='cuda:0', grad_fn=<AddBackward0>)
0.74978602
tensor(0.2928, device='cuda:0', grad_fn=<AddBackward0>)
0.74998504
tensor(0.4094, device='cuda:0', grad_fn=<AddBackward0>)
0.75104511
tensor(0.3533, device='cuda:0', grad_fn=<AddBackward0>)
0.75061965
tensor(0.2985, device='cuda:0', grad_fn=<AddBackward0>)
0.75009638
tensor(0.3979, device='cuda:0', grad_fn=<AddBackward0>)
0.75035214
tensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)
0.75040489
tensor(0.3737, device='cuda:0', grad_fn=<AddBackward0>)
0.75074631
tensor(0.3487, device='cuda:0', grad_fn=<AddBackward0>)
0.75162655
tensor(0.3202, device='cuda:0', grad_fn=<AddBackward0>)
0.75279242
tensor(0.3783, device='cuda:0', grad_fn=<AddBackward0>)
0.75380278
tensor(0.3638, device='cuda:0', grad_fn=<AddBackward0>)
0.75532687
tensor(0.3928, device='cuda:0', grad_fn=<AddBackward0>)
0.75493491
tensor(0.3132, device='cuda:0', grad_fn=<AddBackward0>)
0.75459683
tensor(0.3489, device='cuda:0', grad_fn=<AddBackward0>)
0.75430948
tensor(0.3651, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][   20/  196]   Loss 0.352573   Top1 88.007812   Top5 99.746094   BatchTime 0.333070   LR 0.000250
0.75412184
tensor(0.3366, device='cuda:0', grad_fn=<AddBackward0>)
0.75405598
tensor(0.3187, device='cuda:0', grad_fn=<AddBackward0>)
0.75414532
tensor(0.4043, device='cuda:0', grad_fn=<AddBackward0>)
0.75431758
tensor(0.4140, device='cuda:0', grad_fn=<AddBackward0>)
0.75458890
tensor(0.4774, device='cuda:0', grad_fn=<AddBackward0>)
0.75516391
tensor(0.4430, device='cuda:0', grad_fn=<AddBackward0>)
0.75537479
tensor(0.3699, device='cuda:0', grad_fn=<AddBackward0>)
0.75560933
tensor(0.4016, device='cuda:0', grad_fn=<AddBackward0>)
0.75523859
tensor(0.4125, device='cuda:0', grad_fn=<AddBackward0>)
0.75516671
tensor(0.4205, device='cuda:0', grad_fn=<AddBackward0>)
0.75487238
tensor(0.3857, device='cuda:0', grad_fn=<AddBackward0>)
0.75444937
tensor(0.3462, device='cuda:0', grad_fn=<AddBackward0>)
0.75423789
tensor(0.4193, device='cuda:0', grad_fn=<AddBackward0>)
0.75384748
tensor(0.3615, device='cuda:0', grad_fn=<AddBackward0>)
0.75336212
tensor(0.3835, device='cuda:0', grad_fn=<AddBackward0>)
0.75332624
tensor(0.3303, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][   40/  196]   Loss 0.366870   Top1 87.470703   Top5 99.638672   BatchTime 0.296041   LR 0.000250
0.75295776
tensor(0.3713, device='cuda:0', grad_fn=<AddBackward0>)
0.75286889
tensor(0.4228, device='cuda:0', grad_fn=<AddBackward0>)
0.75266325
tensor(0.3547, device='cuda:0', grad_fn=<AddBackward0>)
0.75221962
tensor(0.3856, device='cuda:0', grad_fn=<AddBackward0>)
0.75207585
tensor(0.3442, device='cuda:0', grad_fn=<AddBackward0>)
0.75203007
tensor(0.3887, device='cuda:0', grad_fn=<AddBackward0>)
0.75201011
tensor(0.4542, device='cuda:0', grad_fn=<AddBackward0>)
0.75125301
tensor(0.3471, device='cuda:0', grad_fn=<AddBackward0>)
0.75065881
tensor(0.3510, device='cuda:0', grad_fn=<AddBackward0>)
0.75057465
tensor(0.4172, device='cuda:0', grad_fn=<AddBackward0>)
0.75071627
tensor(0.3456, device='cuda:0', grad_fn=<AddBackward0>)
0.75039631
tensor(0.3792, device='cuda:0', grad_fn=<AddBackward0>)
0.75047410
tensor(0.3806, device='cuda:0', grad_fn=<AddBackward0>)
0.75072658
tensor(0.3728, device='cuda:0', grad_fn=<AddBackward0>)
0.75071943
tensor(0.4333, device='cuda:0', grad_fn=<AddBackward0>)
0.75068426
tensor(0.3705, device='cuda:0', grad_fn=<AddBackward0>)
0.75076365
tensor(0.4424, device='cuda:0', grad_fn=<AddBackward0>)
0.75109953
tensor(0.3813, device='cuda:0', grad_fn=<AddBackward0>)
0.75112545
tensor(0.4672, device='cuda:0', grad_fn=<AddBackward0>)
0.75104463
tensor(0.5081, device='cuda:0', grad_fn=<AddBackward0>)
0.75092369
tensor(0.3993, device='cuda:0', grad_fn=<AddBackward0>)
0.75122923
tensor(0.3168, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][   60/  196]   Loss 0.377007   Top1 87.122396   Top5 99.576823   BatchTime 0.285227   LR 0.000250
0.75090635
tensor(0.3028, device='cuda:0', grad_fn=<AddBackward0>)
0.75103283
tensor(0.3938, device='cuda:0', grad_fn=<AddBackward0>)
0.75101835
tensor(0.3962, device='cuda:0', grad_fn=<AddBackward0>)
0.75086194
tensor(0.3623, device='cuda:0', grad_fn=<AddBackward0>)
0.75073099
tensor(0.3721, device='cuda:0', grad_fn=<AddBackward0>)
0.75058907
tensor(0.3469, device='cuda:0', grad_fn=<AddBackward0>)
0.75056368
tensor(0.4499, device='cuda:0', grad_fn=<AddBackward0>)
0.75095874
tensor(0.4690, device='cuda:0', grad_fn=<AddBackward0>)
0.75062162
tensor(0.3948, device='cuda:0', grad_fn=<AddBackward0>)
0.75083834
tensor(0.3466, device='cuda:0', grad_fn=<AddBackward0>)
0.75080109
tensor(0.4272, device='cuda:0', grad_fn=<AddBackward0>)
0.75072014
tensor(0.3369, device='cuda:0', grad_fn=<AddBackward0>)
0.75067556
tensor(0.4139, device='cuda:0', grad_fn=<AddBackward0>)
0.75047028
tensor(0.4156, device='cuda:0', grad_fn=<AddBackward0>)
0.75046080
tensor(0.5284, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][   80/  196]   Loss 0.381823   Top1 86.997070   Top5 99.570312   BatchTime 0.280967   LR 0.000250
0.75032246
tensor(0.3660, device='cuda:0', grad_fn=<AddBackward0>)
0.75018936
tensor(0.3796, device='cuda:0', grad_fn=<AddBackward0>)
0.75016099
tensor(0.4750, device='cuda:0', grad_fn=<AddBackward0>)
0.74999100
tensor(0.4316, device='cuda:0', grad_fn=<AddBackward0>)
0.75015599
tensor(0.4486, device='cuda:0', grad_fn=<AddBackward0>)
0.75023443
tensor(0.3414, device='cuda:0', grad_fn=<AddBackward0>)
0.75008529
tensor(0.3709, device='cuda:0', grad_fn=<AddBackward0>)
0.75032109
tensor(0.4503, device='cuda:0', grad_fn=<AddBackward0>)
0.75060397
tensor(0.4354, device='cuda:0', grad_fn=<AddBackward0>)
0.75000674
tensor(0.3904, device='cuda:0', grad_fn=<AddBackward0>)
0.75011581
tensor(0.5040, device='cuda:0', grad_fn=<AddBackward0>)
0.74991035
tensor(0.3312, device='cuda:0', grad_fn=<AddBackward0>)
0.75001806
tensor(0.4664, device='cuda:0', grad_fn=<AddBackward0>)
0.75010324
tensor(0.4489, device='cuda:0', grad_fn=<AddBackward0>)
0.75024480
tensor(0.4311, device='cuda:0', grad_fn=<AddBackward0>)
0.75047642
tensor(0.4360, device='cuda:0', grad_fn=<AddBackward0>)
0.75041455
tensor(0.4383, device='cuda:0', grad_fn=<AddBackward0>)
0.75060898
tensor(0.3546, device='cuda:0', grad_fn=<AddBackward0>)
0.75085247
tensor(0.4564, device='cuda:0', grad_fn=<AddBackward0>)
0.75122666
tensor(0.3943, device='cuda:0', grad_fn=<AddBackward0>)
0.75068879
tensor(0.4011, device='cuda:0', grad_fn=<AddBackward0>)
0.75078797
tensor(0.4347, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][  100/  196]   Loss 0.389770   Top1 86.765625   Top5 99.562500   BatchTime 0.282704   LR 0.000250
0.75090545
tensor(0.4639, device='cuda:0', grad_fn=<AddBackward0>)
0.75115305
tensor(0.4334, device='cuda:0', grad_fn=<AddBackward0>)
0.75091285
tensor(0.2735, device='cuda:0', grad_fn=<AddBackward0>)
0.75097722
tensor(0.4180, device='cuda:0', grad_fn=<AddBackward0>)
0.75118965
tensor(0.3320, device='cuda:0', grad_fn=<AddBackward0>)
0.75152797
tensor(0.4418, device='cuda:0', grad_fn=<AddBackward0>)
0.75128525
tensor(0.4304, device='cuda:0', grad_fn=<AddBackward0>)
0.75126916
tensor(0.5360, device='cuda:0', grad_fn=<AddBackward0>)
0.75142366
tensor(0.4204, device='cuda:0', grad_fn=<AddBackward0>)
0.75129396
tensor(0.3798, device='cuda:0', grad_fn=<AddBackward0>)
0.75143760
tensor(0.4034, device='cuda:0', grad_fn=<AddBackward0>)
0.75153071
tensor(0.3127, device='cuda:0', grad_fn=<AddBackward0>)
0.75166142
tensor(0.3168, device='cuda:0', grad_fn=<AddBackward0>)
0.75188452
tensor(0.4312, device='cuda:0', grad_fn=<AddBackward0>)
0.75184852
tensor(0.4501, device='cuda:0', grad_fn=<AddBackward0>)
0.75193065
tensor(0.4467, device='cuda:0', grad_fn=<AddBackward0>)
0.75184536
tensor(0.4269, device='cuda:0', grad_fn=<AddBackward0>)
0.75239009
tensor(0.2921, device='cuda:0', grad_fn=<AddBackward0>)
0.75186110
tensor(0.3678, device='cuda:0', grad_fn=<AddBackward0>)
0.75160843
tensor(0.3841, device='cuda:0', grad_fn=<AddBackward0>)
0.75164068
tensor(0.4251, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][  120/  196]   Loss 0.391064   Top1 86.708984   Top5 99.557292   BatchTime 0.280995   LR 0.000249
0.75138366
tensor(0.4620, device='cuda:0', grad_fn=<AddBackward0>)
0.75155431
tensor(0.3954, device='cuda:0', grad_fn=<AddBackward0>)
0.75170213
tensor(0.4131, device='cuda:0', grad_fn=<AddBackward0>)
0.75178808
tensor(0.3338, device='cuda:0', grad_fn=<AddBackward0>)
0.75187415
tensor(0.4886, device='cuda:0', grad_fn=<AddBackward0>)
0.75201553
tensor(0.3968, device='cuda:0', grad_fn=<AddBackward0>)
0.75216413
tensor(0.3308, device='cuda:0', grad_fn=<AddBackward0>)
0.75200129
tensor(0.4230, device='cuda:0', grad_fn=<AddBackward0>)
0.75193799
tensor(0.3516, device='cuda:0', grad_fn=<AddBackward0>)
0.75203824
tensor(0.3524, device='cuda:0', grad_fn=<AddBackward0>)
0.75216395
tensor(0.3025, device='cuda:0', grad_fn=<AddBackward0>)
0.75208944
tensor(0.4111, device='cuda:0', grad_fn=<AddBackward0>)
0.75233883
tensor(0.4417, device='cuda:0', grad_fn=<AddBackward0>)
0.75264043
tensor(0.3214, device='cuda:0', grad_fn=<AddBackward0>)
0.75239676
tensor(0.4070, device='cuda:0', grad_fn=<AddBackward0>)
0.75261295
tensor(0.4442, device='cuda:0', grad_fn=<AddBackward0>)
0.75273043
tensor(0.4498, device='cuda:0', grad_fn=<AddBackward0>)
0.75279552
tensor(0.4103, device='cuda:0', grad_fn=<AddBackward0>)
0.75266027
tensor(0.2570, device='cuda:0', grad_fn=<AddBackward0>)
0.75251162
tensor(0.4863, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][  140/  196]   Loss 0.390928   Top1 86.777344   Top5 99.550781   BatchTime 0.285530   LR 0.000249
0.75259906
tensor(0.3855, device='cuda:0', grad_fn=<AddBackward0>)
0.75280011
tensor(0.4663, device='cuda:0', grad_fn=<AddBackward0>)
0.75255358
tensor(0.4877, device='cuda:0', grad_fn=<AddBackward0>)
0.75242412
tensor(0.3676, device='cuda:0', grad_fn=<AddBackward0>)
0.75243896
tensor(0.3714, device='cuda:0', grad_fn=<AddBackward0>)
0.75234646
tensor(0.3737, device='cuda:0', grad_fn=<AddBackward0>)
0.75267845
tensor(0.4334, device='cuda:0', grad_fn=<AddBackward0>)
0.75244749
tensor(0.4331, device='cuda:0', grad_fn=<AddBackward0>)
0.75268459
tensor(0.4303, device='cuda:0', grad_fn=<AddBackward0>)
0.75312221
tensor(0.3751, device='cuda:0', grad_fn=<AddBackward0>)
0.75353634
tensor(0.3546, device='cuda:0', grad_fn=<AddBackward0>)
0.75366068
tensor(0.2844, device='cuda:0', grad_fn=<AddBackward0>)
0.75417745
tensor(0.4614, device='cuda:0', grad_fn=<AddBackward0>)
0.75459260
tensor(0.4836, device='cuda:0', grad_fn=<AddBackward0>)
0.75465900
tensor(0.4231, device='cuda:0', grad_fn=<AddBackward0>)
0.75486344
tensor(0.3626, device='cuda:0', grad_fn=<AddBackward0>)
0.75477892
tensor(0.4181, device='cuda:0', grad_fn=<AddBackward0>)
0.75466472
tensor(0.3613, device='cuda:0', grad_fn=<AddBackward0>)
0.75429350
tensor(0.4071, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][  160/  196]   Loss 0.392332   Top1 86.696777   Top5 99.565430   BatchTime 0.289240   LR 0.000249
0.75414324
tensor(0.4558, device='cuda:0', grad_fn=<AddBackward0>)
0.75460756
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.75426149
tensor(0.3665, device='cuda:0', grad_fn=<AddBackward0>)
0.75398296
tensor(0.4097, device='cuda:0', grad_fn=<AddBackward0>)
0.75387979
tensor(0.4396, device='cuda:0', grad_fn=<AddBackward0>)
0.75370044
tensor(0.4215, device='cuda:0', grad_fn=<AddBackward0>)
0.75386351
tensor(0.3442, device='cuda:0', grad_fn=<AddBackward0>)
0.75365579
tensor(0.4805, device='cuda:0', grad_fn=<AddBackward0>)
0.75387144
tensor(0.3219, device='cuda:0', grad_fn=<AddBackward0>)
0.75362432
tensor(0.3917, device='cuda:0', grad_fn=<AddBackward0>)
0.75351757
tensor(0.4187, device='cuda:0', grad_fn=<AddBackward0>)
0.75378376
tensor(0.3505, device='cuda:0', grad_fn=<AddBackward0>)
0.75375003
tensor(0.3555, device='cuda:0', grad_fn=<AddBackward0>)
0.75365835
tensor(0.3870, device='cuda:0', grad_fn=<AddBackward0>)
0.75386620
tensor(0.3909, device='cuda:0', grad_fn=<AddBackward0>)
0.75377935
tensor(0.4492, device='cuda:0', grad_fn=<AddBackward0>)
0.75358582
tensor(0.2881, device='cuda:0', grad_fn=<AddBackward0>)
0.75326550
tensor(0.4383, device='cuda:0', grad_fn=<AddBackward0>)
0.75304097
tensor(0.3821, device='cuda:0', grad_fn=<AddBackward0>)
0.75271159
tensor(0.3783, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [10][  180/  196]   Loss 0.392175   Top1 86.701389   Top5 99.576823   BatchTime 0.289814   LR 0.000249
0.75289679
tensor(0.3969, device='cuda:0', grad_fn=<AddBackward0>)
0.75286376
tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)
0.75291592
tensor(0.3752, device='cuda:0', grad_fn=<AddBackward0>)
0.75309944
tensor(0.4549, device='cuda:0', grad_fn=<AddBackward0>)
0.75351036
tensor(0.3675, device='cuda:0', grad_fn=<AddBackward0>)
0.75320852
tensor(0.4823, device='cuda:0', grad_fn=<AddBackward0>)
0.75299084
tensor(0.4318, device='cuda:0', grad_fn=<AddBackward0>)
0.75272471
tensor(0.4766, device='cuda:0', grad_fn=<AddBackward0>)
0.75244194
tensor(0.3415, device='cuda:0', grad_fn=<AddBackward0>)
0.75246239
tensor(0.4489, device='cuda:0', grad_fn=<AddBackward0>)
0.75239408
tensor(0.3786, device='cuda:0', grad_fn=<AddBackward0>)
0.75254607
tensor(0.4292, device='cuda:0', grad_fn=<AddBackward0>)
0.75264865
tensor(0.3931, device='cuda:0', grad_fn=<AddBackward0>)
0.75279862
tensor(0.4113, device='cuda:0', grad_fn=<AddBackward0>)
0.75261945
tensor(0.4098, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 86.672    Top5: 99.572    Loss: 0.394
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.75280321
tensor(0.4176, device='cuda:0', grad_fn=<AddBackward0>)
0.75247246
tensor(0.3471, device='cuda:0', grad_fn=<AddBackward0>)
0.75238228
tensor(0.4374, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.488809   Top1 83.105469   Top5 99.199219   BatchTime 0.116383
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0938)
features.2.conv.0 tensor(0.0992)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.4100)
features.3.conv.0 tensor(0.0819)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.1083)
features.4.conv.0 tensor(0.0646)
features.4.conv.3 tensor(0.3125)
features.4.conv.6 tensor(0.1838)
features.5.conv.0 tensor(0.2666)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1043)
features.6.conv.0 tensor(0.0490)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0881)
features.7.conv.0 tensor(0.1643)
features.7.conv.3 tensor(0.4601)
features.7.conv.6 tensor(0.1894)
features.8.conv.0 tensor(0.4910)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.1506)
features.9.conv.0 tensor(0.3903)
features.9.conv.3 tensor(0.5660)
features.9.conv.6 tensor(0.1293)
features.10.conv.0 tensor(0.0658)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.1023)
features.11.conv.0 tensor(0.6708)
features.11.conv.3 tensor(0.6472)
features.11.conv.6 tensor(0.1999)
features.12.conv.0 tensor(0.6983)
features.12.conv.3 tensor(0.6744)
features.12.conv.6 tensor(0.1997)
features.13.conv.0 tensor(0.2215)
features.13.conv.3 tensor(0.4917)
features.13.conv.6 tensor(0.0896)
features.14.conv.0 tensor(0.8657)
features.14.conv.3 tensor(0.8265)
features.14.conv.6 tensor(0.9314)
features.15.conv.0 tensor(0.8185)
features.15.conv.3 tensor(0.8340)
features.15.conv.6 tensor(0.9421)
features.16.conv.0 tensor(0.6172)
features.16.conv.3 tensor(0.8056)
features.16.conv.6 tensor(0.1701)
conv.0 tensor(0.0819)
tensor(934632.) 2188896.0
INFO - Validation [10][   40/   40]   Loss 0.494158   Top1 83.360000   Top5 99.260000   BatchTime 0.084304
INFO - ==> Top1: 83.360    Top5: 99.260    Loss: 0.494
INFO - ==> Sparsity : 0.427
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
0.75226539
tensor(0.3859, device='cuda:0', grad_fn=<AddBackward0>)
0.75224942
tensor(0.3164, device='cuda:0', grad_fn=<AddBackward0>)
0.75218743
tensor(0.2950, device='cuda:0', grad_fn=<AddBackward0>)
0.75207263
tensor(0.3225, device='cuda:0', grad_fn=<AddBackward0>)
0.75192732
tensor(0.3904, device='cuda:0', grad_fn=<AddBackward0>)
0.75190276
tensor(0.2989, device='cuda:0', grad_fn=<AddBackward0>)
0.75191373
tensor(0.4714, device='cuda:0', grad_fn=<AddBackward0>)
0.75179338
tensor(0.3442, device='cuda:0', grad_fn=<AddBackward0>)
0.75168204
tensor(0.3985, device='cuda:0', grad_fn=<AddBackward0>)
0.75140023
tensor(0.4458, device='cuda:0', grad_fn=<AddBackward0>)
0.75173086
tensor(0.4163, device='cuda:0', grad_fn=<AddBackward0>)
0.75159001
tensor(0.3469, device='cuda:0', grad_fn=<AddBackward0>)
0.75176722
tensor(0.4312, device='cuda:0', grad_fn=<AddBackward0>)
0.75195640
tensor(0.4799, device='cuda:0', grad_fn=<AddBackward0>)
0.75198966
tensor(0.3031, device='cuda:0', grad_fn=<AddBackward0>)
0.75200337
tensor(0.4381, device='cuda:0', grad_fn=<AddBackward0>)
0.75232631
tensor(0.4513, device='cuda:0', grad_fn=<AddBackward0>)
0.75253880
tensor(0.2782, device='cuda:0', grad_fn=<AddBackward0>)
0.75310075
tensor(0.3677, device='cuda:0', grad_fn=<AddBackward0>)
0.75423992
tensor(0.3404, device='cuda:0', grad_fn=<AddBackward0>)
0.75506157
tensor(0.3671, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][   20/  196]   Loss 0.376111   Top1 86.855469   Top5 99.472656   BatchTime 0.399645   LR 0.000248
0.75502127
tensor(0.3762, device='cuda:0', grad_fn=<AddBackward0>)
0.75522935
tensor(0.4570, device='cuda:0', grad_fn=<AddBackward0>)
0.75497204
tensor(0.3983, device='cuda:0', grad_fn=<AddBackward0>)
0.75498658
tensor(0.4691, device='cuda:0', grad_fn=<AddBackward0>)
0.75494611
tensor(0.4528, device='cuda:0', grad_fn=<AddBackward0>)
0.75487411
tensor(0.3792, device='cuda:0', grad_fn=<AddBackward0>)
0.75495064
tensor(0.3674, device='cuda:0', grad_fn=<AddBackward0>)
0.75482064
tensor(0.4453, device='cuda:0', grad_fn=<AddBackward0>)
0.75480855
tensor(0.3922, device='cuda:0', grad_fn=<AddBackward0>)
0.75465161
tensor(0.4020, device='cuda:0', grad_fn=<AddBackward0>)
0.75456280
tensor(0.4475, device='cuda:0', grad_fn=<AddBackward0>)
0.75477821
tensor(0.4540, device='cuda:0', grad_fn=<AddBackward0>)
0.75459117
tensor(0.2663, device='cuda:0', grad_fn=<AddBackward0>)
0.75458270
tensor(0.4225, device='cuda:0', grad_fn=<AddBackward0>)
0.75469679
tensor(0.4659, device='cuda:0', grad_fn=<AddBackward0>)
0.75486737
tensor(0.3744, device='cuda:0', grad_fn=<AddBackward0>)
0.75473619
tensor(0.3629, device='cuda:0', grad_fn=<AddBackward0>)
0.75487775
tensor(0.4303, device='cuda:0', grad_fn=<AddBackward0>)
0.75486720
tensor(0.3844, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][   40/  196]   Loss 0.390929   Top1 86.396484   Top5 99.570312   BatchTime 0.355987   LR 0.000248
0.75481701
tensor(0.3504, device='cuda:0', grad_fn=<AddBackward0>)
0.75455713
tensor(0.3733, device='cuda:0', grad_fn=<AddBackward0>)
0.75462979
tensor(0.4349, device='cuda:0', grad_fn=<AddBackward0>)
0.75473392
tensor(0.3308, device='cuda:0', grad_fn=<AddBackward0>)
0.75486892
tensor(0.3735, device='cuda:0', grad_fn=<AddBackward0>)
0.75495577
tensor(0.3376, device='cuda:0', grad_fn=<AddBackward0>)
0.75500828
tensor(0.3361, device='cuda:0', grad_fn=<AddBackward0>)
0.75486189
tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)
0.75488096
tensor(0.3939, device='cuda:0', grad_fn=<AddBackward0>)
0.75471264
tensor(0.3902, device='cuda:0', grad_fn=<AddBackward0>)
0.75501359
tensor(0.3763, device='cuda:0', grad_fn=<AddBackward0>)
0.75454372
tensor(0.4020, device='cuda:0', grad_fn=<AddBackward0>)
0.75449485
tensor(0.3297, device='cuda:0', grad_fn=<AddBackward0>)
0.75432140
tensor(0.3743, device='cuda:0', grad_fn=<AddBackward0>)
0.75453782
tensor(0.3392, device='cuda:0', grad_fn=<AddBackward0>)
0.75436324
tensor(0.4551, device='cuda:0', grad_fn=<AddBackward0>)
0.75447065
tensor(0.4306, device='cuda:0', grad_fn=<AddBackward0>)
0.75436634
tensor(0.3361, device='cuda:0', grad_fn=<AddBackward0>)
0.75433356
tensor(0.4605, device='cuda:0', grad_fn=<AddBackward0>)
0.75454473
tensor(0.3822, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][   60/  196]   Loss 0.387527   Top1 86.529948   Top5 99.550781   BatchTime 0.338045   LR 0.000247
0.75452191
tensor(0.3098, device='cuda:0', grad_fn=<AddBackward0>)
0.75424337
tensor(0.3530, device='cuda:0', grad_fn=<AddBackward0>)
0.75428301
tensor(0.3296, device='cuda:0', grad_fn=<AddBackward0>)
0.75444502
tensor(0.3064, device='cuda:0', grad_fn=<AddBackward0>)
0.75434685
tensor(0.2910, device='cuda:0', grad_fn=<AddBackward0>)
0.75428146
tensor(0.3759, device='cuda:0', grad_fn=<AddBackward0>)
0.75443667
tensor(0.4656, device='cuda:0', grad_fn=<AddBackward0>)
0.75443035
tensor(0.3777, device='cuda:0', grad_fn=<AddBackward0>)
0.75439286
tensor(0.4009, device='cuda:0', grad_fn=<AddBackward0>)
0.75448805
tensor(0.4256, device='cuda:0', grad_fn=<AddBackward0>)
0.75441533
tensor(0.4576, device='cuda:0', grad_fn=<AddBackward0>)
0.75447631
tensor(0.3912, device='cuda:0', grad_fn=<AddBackward0>)
0.75462610
tensor(0.3003, device='cuda:0', grad_fn=<AddBackward0>)
0.75460994
tensor(0.3968, device='cuda:0', grad_fn=<AddBackward0>)
0.75447679
tensor(0.4510, device='cuda:0', grad_fn=<AddBackward0>)
0.75461775
tensor(0.2906, device='cuda:0', grad_fn=<AddBackward0>)
0.75444859
tensor(0.4290, device='cuda:0', grad_fn=<AddBackward0>)
0.75452381
tensor(0.4099, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][   80/  196]   Loss 0.385654   Top1 86.694336   Top5 99.570312   BatchTime 0.338143   LR 0.000247
0.75478476
tensor(0.4201, device='cuda:0', grad_fn=<AddBackward0>)
0.75447488
tensor(0.4188, device='cuda:0', grad_fn=<AddBackward0>)
0.75433838
tensor(0.4238, device='cuda:0', grad_fn=<AddBackward0>)
0.75458920
tensor(0.4457, device='cuda:0', grad_fn=<AddBackward0>)
0.75412631
tensor(0.3433, device='cuda:0', grad_fn=<AddBackward0>)
0.75405794
tensor(0.3468, device='cuda:0', grad_fn=<AddBackward0>)
0.75423646
tensor(0.2946, device='cuda:0', grad_fn=<AddBackward0>)
0.75438017
tensor(0.4953, device='cuda:0', grad_fn=<AddBackward0>)
0.75443590
tensor(0.4683, device='cuda:0', grad_fn=<AddBackward0>)
0.75444001
tensor(0.4083, device='cuda:0', grad_fn=<AddBackward0>)
0.75409544
tensor(0.4771, device='cuda:0', grad_fn=<AddBackward0>)
0.75402755
tensor(0.4404, device='cuda:0', grad_fn=<AddBackward0>)
0.75407594
tensor(0.3065, device='cuda:0', grad_fn=<AddBackward0>)
0.75410753
tensor(0.3909, device='cuda:0', grad_fn=<AddBackward0>)
0.75400150
tensor(0.3580, device='cuda:0', grad_fn=<AddBackward0>)
0.75396001
tensor(0.2575, device='cuda:0', grad_fn=<AddBackward0>)
0.75417268
tensor(0.4580, device='cuda:0', grad_fn=<AddBackward0>)
0.75392240
tensor(0.2818, device='cuda:0', grad_fn=<AddBackward0>)
0.75414670
tensor(0.3714, device='cuda:0', grad_fn=<AddBackward0>)
0.75421649
tensor(0.3414, device='cuda:0', grad_fn=<AddBackward0>)
0.75382471
tensor(0.5607, device='cuda:0', grad_fn=<AddBackward0>)
0.75392401
tensor(0.3618, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][  100/  196]   Loss 0.386841   Top1 86.699219   Top5 99.562500   BatchTime 0.322120   LR 0.000247
0.75407881
tensor(0.3553, device='cuda:0', grad_fn=<AddBackward0>)
0.75412494
tensor(0.5356, device='cuda:0', grad_fn=<AddBackward0>)
0.75405842
tensor(0.4101, device='cuda:0', grad_fn=<AddBackward0>)
0.75391418
tensor(0.4101, device='cuda:0', grad_fn=<AddBackward0>)
0.75395215
tensor(0.3892, device='cuda:0', grad_fn=<AddBackward0>)
0.75383061
tensor(0.4279, device='cuda:0', grad_fn=<AddBackward0>)
0.75380105
tensor(0.3948, device='cuda:0', grad_fn=<AddBackward0>)
0.75380403
tensor(0.3153, device='cuda:0', grad_fn=<AddBackward0>)
0.75401056
tensor(0.4292, device='cuda:0', grad_fn=<AddBackward0>)
0.75394505
tensor(0.3445, device='cuda:0', grad_fn=<AddBackward0>)
0.75392944
tensor(0.3496, device='cuda:0', grad_fn=<AddBackward0>)
0.75386685
tensor(0.4715, device='cuda:0', grad_fn=<AddBackward0>)
0.75377518
tensor(0.4021, device='cuda:0', grad_fn=<AddBackward0>)
0.75373966
tensor(0.3860, device='cuda:0', grad_fn=<AddBackward0>)
0.75381875
tensor(0.4354, device='cuda:0', grad_fn=<AddBackward0>)
0.75358063
tensor(0.3965, device='cuda:0', grad_fn=<AddBackward0>)
0.75350189
tensor(0.4045, device='cuda:0', grad_fn=<AddBackward0>)
0.75378537
tensor(0.5196, device='cuda:0', grad_fn=<AddBackward0>)
0.75372350
tensor(0.3691, device='cuda:0', grad_fn=<AddBackward0>)
0.75384539
tensor(0.4459, device='cuda:0', grad_fn=<AddBackward0>)
0.75399828
tensor(0.4149, device='cuda:0', grad_fn=<AddBackward0>)
0.75394070
tensor(0.4633, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][  120/  196]   Loss 0.390636   Top1 86.614583   Top5 99.563802   BatchTime 0.315891   LR 0.000246
0.75407904
tensor(0.4010, device='cuda:0', grad_fn=<AddBackward0>)
0.75402445
tensor(0.3873, device='cuda:0', grad_fn=<AddBackward0>)
0.75375098
tensor(0.3463, device='cuda:0', grad_fn=<AddBackward0>)
0.75362879
tensor(0.3721, device='cuda:0', grad_fn=<AddBackward0>)
0.75336581
tensor(0.4180, device='cuda:0', grad_fn=<AddBackward0>)
0.75351983
tensor(0.3761, device='cuda:0', grad_fn=<AddBackward0>)
0.75340271
tensor(0.4374, device='cuda:0', grad_fn=<AddBackward0>)
0.75360048
tensor(0.2990, device='cuda:0', grad_fn=<AddBackward0>)
0.75395596
tensor(0.4165, device='cuda:0', grad_fn=<AddBackward0>)
0.75359118
tensor(0.3273, device='cuda:0', grad_fn=<AddBackward0>)
0.75370091
tensor(0.3976, device='cuda:0', grad_fn=<AddBackward0>)
0.75388193
tensor(0.4638, device='cuda:0', grad_fn=<AddBackward0>)
0.75415164
tensor(0.2700, device='cuda:0', grad_fn=<AddBackward0>)
0.75392526
tensor(0.4643, device='cuda:0', grad_fn=<AddBackward0>)
0.75378925
tensor(0.3414, device='cuda:0', grad_fn=<AddBackward0>)
0.75365424
tensor(0.4789, device='cuda:0', grad_fn=<AddBackward0>)
0.75418270
tensor(0.4747, device='cuda:0', grad_fn=<AddBackward0>)
0.75381428
tensor(0.4186, device='cuda:0', grad_fn=<AddBackward0>)
0.75389928
tensor(0.3187, device='cuda:0', grad_fn=<AddBackward0>)
0.75410485
tensor(0.4476, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][  140/  196]   Loss 0.391750   Top1 86.618304   Top5 99.559152   BatchTime 0.312054   LR 0.000246
0.75388068
tensor(0.3680, device='cuda:0', grad_fn=<AddBackward0>)
0.75386351
tensor(0.3969, device='cuda:0', grad_fn=<AddBackward0>)
0.75384021
tensor(0.3687, device='cuda:0', grad_fn=<AddBackward0>)
0.75349993
tensor(0.4400, device='cuda:0', grad_fn=<AddBackward0>)
0.75355428
tensor(0.4494, device='cuda:0', grad_fn=<AddBackward0>)
0.75377280
tensor(0.4576, device='cuda:0', grad_fn=<AddBackward0>)
0.75375086
tensor(0.3929, device='cuda:0', grad_fn=<AddBackward0>)
0.75375253
tensor(0.3271, device='cuda:0', grad_fn=<AddBackward0>)
0.75390029
tensor(0.3639, device='cuda:0', grad_fn=<AddBackward0>)
0.75399286
tensor(0.3890, device='cuda:0', grad_fn=<AddBackward0>)
0.75401956
tensor(0.3969, device='cuda:0', grad_fn=<AddBackward0>)
0.75414729
tensor(0.3755, device='cuda:0', grad_fn=<AddBackward0>)
0.75414258
tensor(0.3922, device='cuda:0', grad_fn=<AddBackward0>)
0.75409716
tensor(0.3378, device='cuda:0', grad_fn=<AddBackward0>)
0.75420225
tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)
0.75447088
tensor(0.3830, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][  160/  196]   Loss 0.392011   Top1 86.589355   Top5 99.558105   BatchTime 0.305776   LR 0.000245
0.75459045
tensor(0.4039, device='cuda:0', grad_fn=<AddBackward0>)
0.75449139
tensor(0.4600, device='cuda:0', grad_fn=<AddBackward0>)
0.75450802
tensor(0.3888, device='cuda:0', grad_fn=<AddBackward0>)
0.75436080
tensor(0.4087, device='cuda:0', grad_fn=<AddBackward0>)
0.75425464
tensor(0.4361, device='cuda:0', grad_fn=<AddBackward0>)
0.75413740
tensor(0.4082, device='cuda:0', grad_fn=<AddBackward0>)
0.75422591
tensor(0.4425, device='cuda:0', grad_fn=<AddBackward0>)
0.75425875
tensor(0.4249, device='cuda:0', grad_fn=<AddBackward0>)
0.75431585
tensor(0.4380, device='cuda:0', grad_fn=<AddBackward0>)
0.75431108
tensor(0.3220, device='cuda:0', grad_fn=<AddBackward0>)
0.75463945
tensor(0.3773, device='cuda:0', grad_fn=<AddBackward0>)
0.75455993
tensor(0.3513, device='cuda:0', grad_fn=<AddBackward0>)
0.75480103
tensor(0.4812, device='cuda:0', grad_fn=<AddBackward0>)
0.75497472
tensor(0.3527, device='cuda:0', grad_fn=<AddBackward0>)
0.75510639
tensor(0.4131, device='cuda:0', grad_fn=<AddBackward0>)
0.75512516
tensor(0.3675, device='cuda:0', grad_fn=<AddBackward0>)
0.75506824
tensor(0.3882, device='cuda:0', grad_fn=<AddBackward0>)
0.75511742
tensor(0.4803, device='cuda:0', grad_fn=<AddBackward0>)
0.75492209
tensor(0.4246, device='cuda:0', grad_fn=<AddBackward0>)
0.75479984
tensor(0.3897, device='cuda:0', grad_fn=<AddBackward0>)
0.75455600
tensor(0.3442, device='cuda:0', grad_fn=<AddBackward0>)
0.75480974
tensor(0.3645, device='cuda:0', grad_fn=<AddBackward0>)
0.75489694
tensor(0.4503, device='cuda:0', grad_fn=<AddBackward0>)
0.75475121
tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [11][  180/  196]   Loss 0.392920   Top1 86.592882   Top5 99.563802   BatchTime 0.299706   LR 0.000244
0.75448990
tensor(0.3905, device='cuda:0', grad_fn=<AddBackward0>)
0.75472158
tensor(0.4236, device='cuda:0', grad_fn=<AddBackward0>)
0.75484401
tensor(0.3981, device='cuda:0', grad_fn=<AddBackward0>)
0.75461793
tensor(0.4422, device='cuda:0', grad_fn=<AddBackward0>)
0.75449777
tensor(0.4427, device='cuda:0', grad_fn=<AddBackward0>)
0.75433439
tensor(0.3836, device='cuda:0', grad_fn=<AddBackward0>)
0.75429755
tensor(0.3612, device='cuda:0', grad_fn=<AddBackward0>)
0.75454354
tensor(0.4846, device='cuda:0', grad_fn=<AddBackward0>)
0.75472808
tensor(0.3602, device='cuda:0', grad_fn=<AddBackward0>)
0.75499916
tensor(0.3276, device='cuda:0', grad_fn=<AddBackward0>)
0.75464016
tensor(0.3432, device='cuda:0', grad_fn=<AddBackward0>)
0.75471246
tensor(0.3713, device='cuda:0', grad_fn=<AddBackward0>)
0.75462556
tensor(0.4080, device='cuda:0', grad_fn=<AddBackward0>)
0.75458896
tensor(0.5077, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 86.586    Top5: 99.560    Loss: 0.394
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.502621   Top1 83.222656   Top5 99.082031   BatchTime 0.109605
INFO - Validation [11][   40/   40]   Loss 0.500533   Top1 83.240000   Top5 99.250000   BatchTime 0.081827
INFO - ==> Top1: 83.240    Top5: 99.250    Loss: 0.501
INFO - ==> Sparsity : 0.420
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.0943)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.4346)
features.3.conv.0 tensor(0.0709)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1076)
features.4.conv.0 tensor(0.0597)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1810)
features.5.conv.0 tensor(0.2713)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.1064)
features.6.conv.0 tensor(0.0529)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0839)
features.7.conv.0 tensor(0.1718)
features.7.conv.3 tensor(0.4583)
features.7.conv.6 tensor(0.1866)
features.8.conv.0 tensor(0.5166)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.1567)
features.9.conv.0 tensor(0.3900)
features.9.conv.3 tensor(0.5639)
features.9.conv.6 tensor(0.1348)
features.10.conv.0 tensor(0.0684)
features.10.conv.3 tensor(0.1079)
features.10.conv.6 tensor(0.1005)
features.11.conv.0 tensor(0.6869)
features.11.conv.3 tensor(0.6454)
features.11.conv.6 tensor(0.1870)
features.12.conv.0 tensor(0.7003)
features.12.conv.3 tensor(0.6748)
features.12.conv.6 tensor(0.2241)
features.13.conv.0 tensor(0.2173)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.0885)
features.14.conv.0 tensor(0.8541)
features.14.conv.3 tensor(0.8273)
features.14.conv.6 tensor(0.9339)
features.15.conv.0 tensor(0.8192)
features.15.conv.3 tensor(0.8343)
features.15.conv.6 tensor(0.9438)
features.16.conv.0 tensor(0.6164)
features.16.conv.3 tensor(0.8065)
features.16.conv.6 tensor(0.1011)
conv.0 tensor(0.0940)
tensor(919552.) 2188896.0
0.75432914
tensor(0.4549, device='cuda:0', grad_fn=<AddBackward0>)
0.75421995
tensor(0.4037, device='cuda:0', grad_fn=<AddBackward0>)
0.75410414
tensor(0.3596, device='cuda:0', grad_fn=<AddBackward0>)
0.75445926
tensor(0.3287, device='cuda:0', grad_fn=<AddBackward0>)
0.75440192
tensor(0.3755, device='cuda:0', grad_fn=<AddBackward0>)
0.75456834
tensor(0.3760, device='cuda:0', grad_fn=<AddBackward0>)
0.75480765
tensor(0.3591, device='cuda:0', grad_fn=<AddBackward0>)
0.75485486
tensor(0.4705, device='cuda:0', grad_fn=<AddBackward0>)
0.75471294
tensor(0.4007, device='cuda:0', grad_fn=<AddBackward0>)
0.75448716
tensor(0.3797, device='cuda:0', grad_fn=<AddBackward0>)
0.75436354
tensor(0.3629, device='cuda:0', grad_fn=<AddBackward0>)
0.75444460
tensor(0.3572, device='cuda:0', grad_fn=<AddBackward0>)
0.75440663
tensor(0.3739, device='cuda:0', grad_fn=<AddBackward0>)
0.75423270
tensor(0.4250, device='cuda:0', grad_fn=<AddBackward0>)
0.75424320
tensor(0.3838, device='cuda:0', grad_fn=<AddBackward0>)
0.75429189
tensor(0.3516, device='cuda:0', grad_fn=<AddBackward0>)
0.75435239
tensor(0.3364, device='cuda:0', grad_fn=<AddBackward0>)
0.75442392
tensor(0.4369, device='cuda:0', grad_fn=<AddBackward0>)
0.75463539
tensor(0.3578, device='cuda:0', grad_fn=<AddBackward0>)
0.75486612
tensor(0.3646, device='cuda:0', grad_fn=<AddBackward0>)
0.75451404
tensor(0.4814, device='cuda:0', grad_fn=<AddBackward0>)
0.75437289
tensor(0.3293, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][   20/  196]   Loss 0.382927   Top1 86.972656   Top5 99.472656   BatchTime 0.341446   LR 0.000243
0.75446248
tensor(0.2592, device='cuda:0', grad_fn=<AddBackward0>)
0.75461584
tensor(0.3539, device='cuda:0', grad_fn=<AddBackward0>)
0.75463080
tensor(0.3411, device='cuda:0', grad_fn=<AddBackward0>)
0.75453287
tensor(0.3015, device='cuda:0', grad_fn=<AddBackward0>)
0.75461143
tensor(0.5225, device='cuda:0', grad_fn=<AddBackward0>)
0.75457150
tensor(0.3123, device='cuda:0', grad_fn=<AddBackward0>)
0.75460774
tensor(0.3221, device='cuda:0', grad_fn=<AddBackward0>)
0.75449920
tensor(0.4009, device='cuda:0', grad_fn=<AddBackward0>)
0.75457257
tensor(0.3107, device='cuda:0', grad_fn=<AddBackward0>)
0.75447530
tensor(0.3632, device='cuda:0', grad_fn=<AddBackward0>)
0.75464302
tensor(0.4700, device='cuda:0', grad_fn=<AddBackward0>)
0.75488549
tensor(0.3506, device='cuda:0', grad_fn=<AddBackward0>)
0.75473481
tensor(0.3273, device='cuda:0', grad_fn=<AddBackward0>)
0.75492758
tensor(0.3597, device='cuda:0', grad_fn=<AddBackward0>)
0.75484681
tensor(0.4257, device='cuda:0', grad_fn=<AddBackward0>)
0.75442421
tensor(0.2859, device='cuda:0', grad_fn=<AddBackward0>)
0.75432819
tensor(0.3750, device='cuda:0', grad_fn=<AddBackward0>)
0.75426102
tensor(0.3676, device='cuda:0', grad_fn=<AddBackward0>)
0.75420564
tensor(0.4571, device='cuda:0', grad_fn=<AddBackward0>)
0.75423819
tensor(0.4105, device='cuda:0', grad_fn=<AddBackward0>)
0.75425416
tensor(0.3861, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][   40/  196]   Loss 0.372962   Top1 87.314453   Top5 99.531250   BatchTime 0.314975   LR 0.000243
0.75418305
tensor(0.3842, device='cuda:0', grad_fn=<AddBackward0>)
0.75421947
tensor(0.4108, device='cuda:0', grad_fn=<AddBackward0>)
0.75400126
tensor(0.3300, device='cuda:0', grad_fn=<AddBackward0>)
0.75380671
tensor(0.3403, device='cuda:0', grad_fn=<AddBackward0>)
0.75367552
tensor(0.5378, device='cuda:0', grad_fn=<AddBackward0>)
0.75383717
tensor(0.3907, device='cuda:0', grad_fn=<AddBackward0>)
0.75406915
tensor(0.3816, device='cuda:0', grad_fn=<AddBackward0>)
0.75424296
tensor(0.3303, device='cuda:0', grad_fn=<AddBackward0>)
0.75432700
tensor(0.3889, device='cuda:0', grad_fn=<AddBackward0>)
0.75435764
tensor(0.4584, device='cuda:0', grad_fn=<AddBackward0>)
0.75426793
tensor(0.3471, device='cuda:0', grad_fn=<AddBackward0>)
0.75399590
tensor(0.4551, device='cuda:0', grad_fn=<AddBackward0>)
0.75354022
tensor(0.4346, device='cuda:0', grad_fn=<AddBackward0>)
0.75339276
tensor(0.4099, device='cuda:0', grad_fn=<AddBackward0>)
0.75335288
tensor(0.4310, device='cuda:0', grad_fn=<AddBackward0>)
0.75340039
tensor(0.3060, device='cuda:0', grad_fn=<AddBackward0>)
0.75351006
tensor(0.4427, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][   60/  196]   Loss 0.382527   Top1 86.992188   Top5 99.505208   BatchTime 0.322161   LR 0.000242
0.75371677
tensor(0.4037, device='cuda:0', grad_fn=<AddBackward0>)
0.75375813
tensor(0.3012, device='cuda:0', grad_fn=<AddBackward0>)
0.75381237
tensor(0.3230, device='cuda:0', grad_fn=<AddBackward0>)
0.75395644
tensor(0.3553, device='cuda:0', grad_fn=<AddBackward0>)
0.75390685
tensor(0.4066, device='cuda:0', grad_fn=<AddBackward0>)
0.75373256
tensor(0.3961, device='cuda:0', grad_fn=<AddBackward0>)
0.75365531
tensor(0.3551, device='cuda:0', grad_fn=<AddBackward0>)
0.75368458
tensor(0.3468, device='cuda:0', grad_fn=<AddBackward0>)
0.75389183
tensor(0.2652, device='cuda:0', grad_fn=<AddBackward0>)
0.75396574
tensor(0.3730, device='cuda:0', grad_fn=<AddBackward0>)
0.75402057
tensor(0.3636, device='cuda:0', grad_fn=<AddBackward0>)
0.75400478
tensor(0.3025, device='cuda:0', grad_fn=<AddBackward0>)
0.75383943
tensor(0.4140, device='cuda:0', grad_fn=<AddBackward0>)
0.75378394
tensor(0.4271, device='cuda:0', grad_fn=<AddBackward0>)
0.75389683
tensor(0.3781, device='cuda:0', grad_fn=<AddBackward0>)
0.75395226
tensor(0.3252, device='cuda:0', grad_fn=<AddBackward0>)
0.75388569
tensor(0.4096, device='cuda:0', grad_fn=<AddBackward0>)
0.75386226
tensor(0.3390, device='cuda:0', grad_fn=<AddBackward0>)
0.75380826
tensor(0.4060, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][   80/  196]   Loss 0.377567   Top1 87.207031   Top5 99.506836   BatchTime 0.324490   LR 0.000241
0.75362992
tensor(0.3627, device='cuda:0', grad_fn=<AddBackward0>)
0.75364035
tensor(0.4874, device='cuda:0', grad_fn=<AddBackward0>)
0.75342482
tensor(0.3786, device='cuda:0', grad_fn=<AddBackward0>)
0.75333643
tensor(0.3116, device='cuda:0', grad_fn=<AddBackward0>)
0.75340945
tensor(0.4538, device='cuda:0', grad_fn=<AddBackward0>)
0.75320333
tensor(0.3972, device='cuda:0', grad_fn=<AddBackward0>)
0.75322753
tensor(0.2780, device='cuda:0', grad_fn=<AddBackward0>)
0.75331855
tensor(0.4228, device='cuda:0', grad_fn=<AddBackward0>)
0.75338709
tensor(0.3285, device='cuda:0', grad_fn=<AddBackward0>)
0.75332254
tensor(0.3699, device='cuda:0', grad_fn=<AddBackward0>)
0.75348312
tensor(0.3175, device='cuda:0', grad_fn=<AddBackward0>)
0.75347912
tensor(0.4314, device='cuda:0', grad_fn=<AddBackward0>)
0.75342506
tensor(0.3358, device='cuda:0', grad_fn=<AddBackward0>)
0.75347942
tensor(0.3176, device='cuda:0', grad_fn=<AddBackward0>)
0.75375134
tensor(0.3914, device='cuda:0', grad_fn=<AddBackward0>)
0.75366032
tensor(0.2980, device='cuda:0', grad_fn=<AddBackward0>)
0.75342697
tensor(0.3447, device='cuda:0', grad_fn=<AddBackward0>)
0.75345260
tensor(0.3477, device='cuda:0', grad_fn=<AddBackward0>)
0.75335538
tensor(0.3256, device='cuda:0', grad_fn=<AddBackward0>)
0.75343275
tensor(0.4224, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][  100/  196]   Loss 0.375276   Top1 87.210938   Top5 99.542969   BatchTime 0.319060   LR 0.000240
0.75334519
tensor(0.3626, device='cuda:0', grad_fn=<AddBackward0>)
0.75334674
tensor(0.3458, device='cuda:0', grad_fn=<AddBackward0>)
0.75328785
tensor(0.4810, device='cuda:0', grad_fn=<AddBackward0>)
0.75329870
tensor(0.3768, device='cuda:0', grad_fn=<AddBackward0>)
0.75331354
tensor(0.3656, device='cuda:0', grad_fn=<AddBackward0>)
0.75335526
tensor(0.3613, device='cuda:0', grad_fn=<AddBackward0>)
0.75311893
tensor(0.4299, device='cuda:0', grad_fn=<AddBackward0>)
0.75332588
tensor(0.3457, device='cuda:0', grad_fn=<AddBackward0>)
0.75346726
tensor(0.3126, device='cuda:0', grad_fn=<AddBackward0>)
0.75342417
tensor(0.3679, device='cuda:0', grad_fn=<AddBackward0>)
0.75320750
tensor(0.3353, device='cuda:0', grad_fn=<AddBackward0>)
0.75332028
tensor(0.3884, device='cuda:0', grad_fn=<AddBackward0>)
0.75335377
tensor(0.3509, device='cuda:0', grad_fn=<AddBackward0>)
0.75337523
tensor(0.3309, device='cuda:0', grad_fn=<AddBackward0>)
0.75350559
tensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)
0.75328964
tensor(0.3957, device='cuda:0', grad_fn=<AddBackward0>)
0.75320786
tensor(0.3642, device='cuda:0', grad_fn=<AddBackward0>)
0.75320017
tensor(0.3713, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][  120/  196]   Loss 0.374677   Top1 87.278646   Top5 99.544271   BatchTime 0.320109   LR 0.000240
0.75353312
tensor(0.4307, device='cuda:0', grad_fn=<AddBackward0>)
0.75339991
tensor(0.3261, device='cuda:0', grad_fn=<AddBackward0>)
0.75342292
tensor(0.3361, device='cuda:0', grad_fn=<AddBackward0>)
0.75333780
tensor(0.3120, device='cuda:0', grad_fn=<AddBackward0>)
0.75303978
tensor(0.4328, device='cuda:0', grad_fn=<AddBackward0>)
0.75282127
tensor(0.3889, device='cuda:0', grad_fn=<AddBackward0>)
0.75272816
tensor(0.3977, device='cuda:0', grad_fn=<AddBackward0>)
0.75278187
tensor(0.3429, device='cuda:0', grad_fn=<AddBackward0>)
0.75277925
tensor(0.4282, device='cuda:0', grad_fn=<AddBackward0>)
0.75261855
tensor(0.4127, device='cuda:0', grad_fn=<AddBackward0>)
0.75261855
tensor(0.4501, device='cuda:0', grad_fn=<AddBackward0>)
0.75277644
tensor(0.3365, device='cuda:0', grad_fn=<AddBackward0>)
0.75273526
tensor(0.3294, device='cuda:0', grad_fn=<AddBackward0>)
0.75296015
tensor(0.3689, device='cuda:0', grad_fn=<AddBackward0>)
0.75313932
tensor(0.3399, device='cuda:0', grad_fn=<AddBackward0>)
0.75303388
tensor(0.2672, device='cuda:0', grad_fn=<AddBackward0>)
0.75307709
tensor(0.4501, device='cuda:0', grad_fn=<AddBackward0>)
0.75354886
tensor(0.4388, device='cuda:0', grad_fn=<AddBackward0>)
0.75349289
tensor(0.3741, device='cuda:0', grad_fn=<AddBackward0>)
0.75353801
tensor(0.4096, device='cuda:0', grad_fn=<AddBackward0>)
0.75335068
tensor(0.4512, device='cuda:0', grad_fn=<AddBackward0>)
0.75331074
tensor(0.4203, device='cuda:0', grad_fn=<AddBackward0>)
0.75347072
tensor(0.3775, device='cuda:0', grad_fn=<AddBackward0>)
0.75342721
tensor(0.3706, device='cuda:0', grad_fn=<AddBackward0>)
0.75327408
tensor(0.3676, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][  140/  196]   Loss 0.376357   Top1 87.226562   Top5 99.547991   BatchTime 0.320385   LR 0.000239
0.75317276
tensor(0.3839, device='cuda:0', grad_fn=<AddBackward0>)
0.75326604
tensor(0.3135, device='cuda:0', grad_fn=<AddBackward0>)
0.75306982
tensor(0.3912, device='cuda:0', grad_fn=<AddBackward0>)
0.75298488
tensor(0.4855, device='cuda:0', grad_fn=<AddBackward0>)
0.75316006
tensor(0.3728, device='cuda:0', grad_fn=<AddBackward0>)
0.75332630
tensor(0.4349, device='cuda:0', grad_fn=<AddBackward0>)
0.75370115
tensor(0.3989, device='cuda:0', grad_fn=<AddBackward0>)
0.75352848
tensor(0.3277, device='cuda:0', grad_fn=<AddBackward0>)
0.75333685
tensor(0.3409, device='cuda:0', grad_fn=<AddBackward0>)
0.75332564
tensor(0.3289, device='cuda:0', grad_fn=<AddBackward0>)
0.75308126
tensor(0.3513, device='cuda:0', grad_fn=<AddBackward0>)
0.75288808
tensor(0.4558, device='cuda:0', grad_fn=<AddBackward0>)
0.75273097
tensor(0.3507, device='cuda:0', grad_fn=<AddBackward0>)
0.75264776
tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)
0.75267065
tensor(0.3362, device='cuda:0', grad_fn=<AddBackward0>)
0.75276047
tensor(0.3043, device='cuda:0', grad_fn=<AddBackward0>)
0.75283867
tensor(0.3300, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][  160/  196]   Loss 0.375043   Top1 87.329102   Top5 99.572754   BatchTime 0.324255   LR 0.000238
0.75268978
tensor(0.2645, device='cuda:0', grad_fn=<AddBackward0>)
0.75269991
tensor(0.3943, device='cuda:0', grad_fn=<AddBackward0>)
0.75292373
tensor(0.2711, device='cuda:0', grad_fn=<AddBackward0>)
0.75277901
tensor(0.3523, device='cuda:0', grad_fn=<AddBackward0>)
0.75260514
tensor(0.3270, device='cuda:0', grad_fn=<AddBackward0>)
0.75243044
tensor(0.3143, device='cuda:0', grad_fn=<AddBackward0>)
0.75230455
tensor(0.4453, device='cuda:0', grad_fn=<AddBackward0>)
0.75192833
tensor(0.2646, device='cuda:0', grad_fn=<AddBackward0>)
0.75176919
tensor(0.3645, device='cuda:0', grad_fn=<AddBackward0>)
0.75163633
tensor(0.4010, device='cuda:0', grad_fn=<AddBackward0>)
0.75142968
tensor(0.3545, device='cuda:0', grad_fn=<AddBackward0>)
0.75124079
tensor(0.3523, device='cuda:0', grad_fn=<AddBackward0>)
0.75130397
tensor(0.3967, device='cuda:0', grad_fn=<AddBackward0>)
0.75124079
tensor(0.3553, device='cuda:0', grad_fn=<AddBackward0>)
0.75146914
tensor(0.4787, device='cuda:0', grad_fn=<AddBackward0>)
0.75178450
tensor(0.4133, device='cuda:0', grad_fn=<AddBackward0>)
0.75164753
tensor(0.4389, device='cuda:0', grad_fn=<AddBackward0>)
0.75147563
tensor(0.2349, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [12][  180/  196]   Loss 0.373417   Top1 87.439236   Top5 99.583333   BatchTime 0.325035   LR 0.000237
0.75106484
tensor(0.3932, device='cuda:0', grad_fn=<AddBackward0>)
0.75097120
tensor(0.2863, device='cuda:0', grad_fn=<AddBackward0>)
0.75073421
tensor(0.3694, device='cuda:0', grad_fn=<AddBackward0>)
0.75081402
tensor(0.2591, device='cuda:0', grad_fn=<AddBackward0>)
0.75098151
tensor(0.3557, device='cuda:0', grad_fn=<AddBackward0>)
0.75109458
tensor(0.4287, device='cuda:0', grad_fn=<AddBackward0>)
0.75105244
tensor(0.4101, device='cuda:0', grad_fn=<AddBackward0>)
0.75093853
tensor(0.3837, device='cuda:0', grad_fn=<AddBackward0>)
0.75111884
tensor(0.3434, device='cuda:0', grad_fn=<AddBackward0>)
0.75091326
tensor(0.3476, device='cuda:0', grad_fn=<AddBackward0>)
0.75079739
tensor(0.4431, device='cuda:0', grad_fn=<AddBackward0>)
0.75072354
tensor(0.5201, device='cuda:0', grad_fn=<AddBackward0>)
0.75085378
tensor(0.3754, device='cuda:0', grad_fn=<AddBackward0>)
0.75059903
tensor(0.3481, device='cuda:0', grad_fn=<AddBackward0>)
0.75051904
tensor(0.2890, device='cuda:0', grad_fn=<AddBackward0>)
0.75055915
tensor(0.3225, device='cuda:0', grad_fn=<AddBackward0>)
0.75081325
tensor(0.3785, device='cuda:0', grad_fn=<AddBackward0>)
0.75160658
tensor(0.3856, device='cuda:0', grad_fn=<AddBackward0>)
0.75098950
tensor(0.3830, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 87.392    Top5: 99.584    Loss: 0.373
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [12][   20/   40]   Loss 0.473083   Top1 84.140625   Top5 99.296875   BatchTime 0.137250
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0521)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0894)
features.2.conv.0 tensor(0.0906)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.5029)
features.3.conv.0 tensor(0.0758)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1059)
features.4.conv.0 tensor(0.0584)
features.4.conv.3 tensor(0.3096)
features.4.conv.6 tensor(0.1813)
features.5.conv.0 tensor(0.2651)
features.5.conv.3 tensor(0.4219)
features.5.conv.6 tensor(0.1053)
features.6.conv.0 tensor(0.0506)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0878)
features.7.conv.0 tensor(0.1691)
features.7.conv.3 tensor(0.4615)
features.7.conv.6 tensor(0.1877)
features.8.conv.0 tensor(0.5179)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.1556)
features.9.conv.0 tensor(0.4176)
features.9.conv.3 tensor(0.5645)
features.9.conv.6 tensor(0.1438)
features.10.conv.0 tensor(0.0688)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0980)
features.11.conv.0 tensor(0.6932)
features.11.conv.3 tensor(0.6456)
features.11.conv.6 tensor(0.1904)
features.12.conv.0 tensor(0.6995)
features.12.conv.3 tensor(0.6746)
features.12.conv.6 tensor(0.2014)
features.13.conv.0 tensor(0.2262)
features.13.conv.3 tensor(0.4948)
features.13.conv.6 tensor(0.0891)
features.14.conv.0 tensor(0.8687)
features.14.conv.3 tensor(0.8270)
features.14.conv.6 tensor(0.9426)
features.15.conv.0 tensor(0.8277)
features.15.conv.3 tensor(0.8344)
features.15.conv.6 tensor(0.9478)
features.16.conv.0 tensor(0.6120)
features.16.conv.3 tensor(0.8064)
features.16.conv.6 tensor(0.1082)
conv.0 tensor(0.0950)
tensor(927762.) 2188896.0
INFO - Validation [12][   40/   40]   Loss 0.466446   Top1 84.200000   Top5 99.280000   BatchTime 0.097754
INFO - ==> Top1: 84.200    Top5: 99.280    Loss: 0.466
INFO - ==> Sparsity : 0.424
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
0.75107110
tensor(0.3413, device='cuda:0', grad_fn=<AddBackward0>)
0.75131381
tensor(0.3603, device='cuda:0', grad_fn=<AddBackward0>)
0.75106579
tensor(0.3838, device='cuda:0', grad_fn=<AddBackward0>)
0.75114000
tensor(0.3760, device='cuda:0', grad_fn=<AddBackward0>)
0.75134885
tensor(0.3415, device='cuda:0', grad_fn=<AddBackward0>)
0.75100273
tensor(0.3684, device='cuda:0', grad_fn=<AddBackward0>)
0.75063801
tensor(0.3634, device='cuda:0', grad_fn=<AddBackward0>)
0.75093991
tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)
0.75092357
tensor(0.3068, device='cuda:0', grad_fn=<AddBackward0>)
0.75113273
tensor(0.3504, device='cuda:0', grad_fn=<AddBackward0>)
0.75145799
tensor(0.3230, device='cuda:0', grad_fn=<AddBackward0>)
0.75178176
tensor(0.2731, device='cuda:0', grad_fn=<AddBackward0>)
0.75174958
tensor(0.3831, device='cuda:0', grad_fn=<AddBackward0>)
0.75152832
tensor(0.3230, device='cuda:0', grad_fn=<AddBackward0>)
0.75125057
tensor(0.3040, device='cuda:0', grad_fn=<AddBackward0>)
0.75086123
tensor(0.4140, device='cuda:0', grad_fn=<AddBackward0>)
0.75091398
tensor(0.3826, device='cuda:0', grad_fn=<AddBackward0>)
0.75086337
tensor(0.3277, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][   20/  196]   Loss 0.351432   Top1 88.300781   Top5 99.570312   BatchTime 0.351820   LR 0.000235
0.75094229
tensor(0.3029, device='cuda:0', grad_fn=<AddBackward0>)
0.75098544
tensor(0.4638, device='cuda:0', grad_fn=<AddBackward0>)
0.75100762
tensor(0.3172, device='cuda:0', grad_fn=<AddBackward0>)
0.75095028
tensor(0.3136, device='cuda:0', grad_fn=<AddBackward0>)
0.75100541
tensor(0.3890, device='cuda:0', grad_fn=<AddBackward0>)
0.75137359
tensor(0.3266, device='cuda:0', grad_fn=<AddBackward0>)
0.75093627
tensor(0.2976, device='cuda:0', grad_fn=<AddBackward0>)
0.75070041
tensor(0.3171, device='cuda:0', grad_fn=<AddBackward0>)
0.75084376
tensor(0.3022, device='cuda:0', grad_fn=<AddBackward0>)
0.75075114
tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)
0.75076979
tensor(0.2649, device='cuda:0', grad_fn=<AddBackward0>)
0.75058722
tensor(0.3568, device='cuda:0', grad_fn=<AddBackward0>)
0.75077224
tensor(0.4714, device='cuda:0', grad_fn=<AddBackward0>)
0.75089252
tensor(0.3633, device='cuda:0', grad_fn=<AddBackward0>)
0.75049579
tensor(0.4216, device='cuda:0', grad_fn=<AddBackward0>)
0.75088769
tensor(0.3118, device='cuda:0', grad_fn=<AddBackward0>)
0.75078839
tensor(0.4034, device='cuda:0', grad_fn=<AddBackward0>)
0.75081623
tensor(0.3831, device='cuda:0', grad_fn=<AddBackward0>)
0.75280786
tensor(0.2866, device='cuda:0', grad_fn=<AddBackward0>)
0.75337595
tensor(0.3543, device='cuda:0', grad_fn=<AddBackward0>)
0.75326121
tensor(0.3536, device='cuda:0', grad_fn=<AddBackward0>)
0.75322348
tensor(0.2467, device='cuda:0', grad_fn=<AddBackward0>)
0.75298923
tensor(0.4013, device='cuda:0', grad_fn=<AddBackward0>)
0.75304109
tensor(0.3790, device='cuda:0', grad_fn=<AddBackward0>)
0.75304282
tensor(0.4149, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][   40/  196]   Loss 0.344481   Top1 88.437500   Top5 99.599609   BatchTime 0.334867   LR 0.000235
0.75294483
tensor(0.3900, device='cuda:0', grad_fn=<AddBackward0>)
0.75291568
tensor(0.4554, device='cuda:0', grad_fn=<AddBackward0>)
0.75241226
tensor(0.3577, device='cuda:0', grad_fn=<AddBackward0>)
0.75248104
tensor(0.2944, device='cuda:0', grad_fn=<AddBackward0>)
0.75263774
tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>)
0.75253981
tensor(0.3868, device='cuda:0', grad_fn=<AddBackward0>)
0.75260311
tensor(0.3702, device='cuda:0', grad_fn=<AddBackward0>)
0.75257969
tensor(0.3293, device='cuda:0', grad_fn=<AddBackward0>)
0.75303370
tensor(0.3953, device='cuda:0', grad_fn=<AddBackward0>)
0.75295085
tensor(0.3305, device='cuda:0', grad_fn=<AddBackward0>)
0.75279778
tensor(0.3887, device='cuda:0', grad_fn=<AddBackward0>)
0.75224531
tensor(0.3633, device='cuda:0', grad_fn=<AddBackward0>)
0.75190431
tensor(0.3512, device='cuda:0', grad_fn=<AddBackward0>)
0.75182480
tensor(0.3189, device='cuda:0', grad_fn=<AddBackward0>)
0.75178260
tensor(0.3083, device='cuda:0', grad_fn=<AddBackward0>)
0.75178009
tensor(0.3414, device='cuda:0', grad_fn=<AddBackward0>)
0.75180721
tensor(0.4109, device='cuda:0', grad_fn=<AddBackward0>)
0.75201404
tensor(0.3801, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][   60/  196]   Loss 0.349846   Top1 88.365885   Top5 99.589844   BatchTime 0.332628   LR 0.000234
0.75208759
tensor(0.2369, device='cuda:0', grad_fn=<AddBackward0>)
0.75184059
tensor(0.3861, device='cuda:0', grad_fn=<AddBackward0>)
0.75176901
tensor(0.3738, device='cuda:0', grad_fn=<AddBackward0>)
0.75166321
tensor(0.3618, device='cuda:0', grad_fn=<AddBackward0>)
0.75147831
tensor(0.3894, device='cuda:0', grad_fn=<AddBackward0>)
0.75141525
tensor(0.4518, device='cuda:0', grad_fn=<AddBackward0>)
0.75146824
tensor(0.4005, device='cuda:0', grad_fn=<AddBackward0>)
0.75247598
tensor(0.3426, device='cuda:0', grad_fn=<AddBackward0>)
0.75258356
tensor(0.3360, device='cuda:0', grad_fn=<AddBackward0>)
0.75250220
tensor(0.4001, device='cuda:0', grad_fn=<AddBackward0>)
0.75247288
tensor(0.2896, device='cuda:0', grad_fn=<AddBackward0>)
0.75216174
tensor(0.2867, device='cuda:0', grad_fn=<AddBackward0>)
0.75228554
tensor(0.3032, device='cuda:0', grad_fn=<AddBackward0>)
0.75212401
tensor(0.3462, device='cuda:0', grad_fn=<AddBackward0>)
0.75226617
tensor(0.3482, device='cuda:0', grad_fn=<AddBackward0>)
0.75246608
tensor(0.3605, device='cuda:0', grad_fn=<AddBackward0>)
0.75241774
tensor(0.2821, device='cuda:0', grad_fn=<AddBackward0>)
0.75242907
tensor(0.3650, device='cuda:0', grad_fn=<AddBackward0>)
0.75248092
tensor(0.4523, device='cuda:0', grad_fn=<AddBackward0>)
0.75231797
tensor(0.4318, device='cuda:0', grad_fn=<AddBackward0>)
0.75200975
tensor(0.3989, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][   80/  196]   Loss 0.351045   Top1 88.222656   Top5 99.580078   BatchTime 0.320254   LR 0.000233
0.75201946
tensor(0.2970, device='cuda:0', grad_fn=<AddBackward0>)
0.75210559
tensor(0.3834, device='cuda:0', grad_fn=<AddBackward0>)
0.75224626
tensor(0.3661, device='cuda:0', grad_fn=<AddBackward0>)
0.75226867
tensor(0.3342, device='cuda:0', grad_fn=<AddBackward0>)
0.75224024
tensor(0.3754, device='cuda:0', grad_fn=<AddBackward0>)
0.75208420
tensor(0.3950, device='cuda:0', grad_fn=<AddBackward0>)
0.75232196
tensor(0.3089, device='cuda:0', grad_fn=<AddBackward0>)
0.75238609
tensor(0.4463, device='cuda:0', grad_fn=<AddBackward0>)
0.75230920
tensor(0.3823, device='cuda:0', grad_fn=<AddBackward0>)
0.75208235
tensor(0.3335, device='cuda:0', grad_fn=<AddBackward0>)
0.75243950
tensor(0.3570, device='cuda:0', grad_fn=<AddBackward0>)
0.75239497
tensor(0.4219, device='cuda:0', grad_fn=<AddBackward0>)
0.75224030
tensor(0.3682, device='cuda:0', grad_fn=<AddBackward0>)
0.75222951
tensor(0.3604, device='cuda:0', grad_fn=<AddBackward0>)
0.75227290
tensor(0.3313, device='cuda:0', grad_fn=<AddBackward0>)
0.75219029
tensor(0.3273, device='cuda:0', grad_fn=<AddBackward0>)
0.75220144
tensor(0.3280, device='cuda:0', grad_fn=<AddBackward0>)
0.75211459
tensor(0.4239, device='cuda:0', grad_fn=<AddBackward0>)
0.75216824
tensor(0.4131, device='cuda:0', grad_fn=<AddBackward0>)
0.75184971
tensor(0.4564, device='cuda:0', grad_fn=<AddBackward0>)
0.75182199
tensor(0.4645, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][  100/  196]   Loss 0.354542   Top1 88.058594   Top5 99.546875   BatchTime 0.314970   LR 0.000232
0.75185287
tensor(0.3235, device='cuda:0', grad_fn=<AddBackward0>)
0.75207508
tensor(0.5614, device='cuda:0', grad_fn=<AddBackward0>)
0.75218499
tensor(0.3973, device='cuda:0', grad_fn=<AddBackward0>)
0.75238997
tensor(0.3315, device='cuda:0', grad_fn=<AddBackward0>)
0.75226080
tensor(0.3449, device='cuda:0', grad_fn=<AddBackward0>)
0.75222218
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.75217837
tensor(0.2780, device='cuda:0', grad_fn=<AddBackward0>)
0.75233614
tensor(0.3310, device='cuda:0', grad_fn=<AddBackward0>)
0.75242960
tensor(0.3659, device='cuda:0', grad_fn=<AddBackward0>)
0.75226510
tensor(0.3235, device='cuda:0', grad_fn=<AddBackward0>)
0.75238526
tensor(0.3049, device='cuda:0', grad_fn=<AddBackward0>)
0.75225103
tensor(0.2925, device='cuda:0', grad_fn=<AddBackward0>)
0.75228512
tensor(0.4498, device='cuda:0', grad_fn=<AddBackward0>)
0.75237113
tensor(0.3720, device='cuda:0', grad_fn=<AddBackward0>)
0.75239396
tensor(0.4744, device='cuda:0', grad_fn=<AddBackward0>)
0.75205475
tensor(0.3012, device='cuda:0', grad_fn=<AddBackward0>)
0.75214756
tensor(0.4126, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][  120/  196]   Loss 0.357875   Top1 87.867839   Top5 99.573568   BatchTime 0.319561   LR 0.000230
0.75197929
tensor(0.2648, device='cuda:0', grad_fn=<AddBackward0>)
0.75178802
tensor(0.3342, device='cuda:0', grad_fn=<AddBackward0>)
0.75155449
tensor(0.3910, device='cuda:0', grad_fn=<AddBackward0>)
0.75163305
tensor(0.3256, device='cuda:0', grad_fn=<AddBackward0>)
0.75174677
tensor(0.3894, device='cuda:0', grad_fn=<AddBackward0>)
0.75204009
tensor(0.3702, device='cuda:0', grad_fn=<AddBackward0>)
0.75253350
tensor(0.4130, device='cuda:0', grad_fn=<AddBackward0>)
0.75230992
tensor(0.4196, device='cuda:0', grad_fn=<AddBackward0>)
0.75214016
tensor(0.3987, device='cuda:0', grad_fn=<AddBackward0>)
0.75218254
tensor(0.3827, device='cuda:0', grad_fn=<AddBackward0>)
0.75214916
tensor(0.3713, device='cuda:0', grad_fn=<AddBackward0>)
0.75196880
tensor(0.3693, device='cuda:0', grad_fn=<AddBackward0>)
0.75190514
tensor(0.3571, device='cuda:0', grad_fn=<AddBackward0>)
0.75195277
tensor(0.3333, device='cuda:0', grad_fn=<AddBackward0>)
0.75207919
tensor(0.3188, device='cuda:0', grad_fn=<AddBackward0>)
0.75229824
tensor(0.3436, device='cuda:0', grad_fn=<AddBackward0>)
0.75215685
tensor(0.2855, device='cuda:0', grad_fn=<AddBackward0>)
0.75222725
tensor(0.3221, device='cuda:0', grad_fn=<AddBackward0>)
0.75193113
tensor(0.3389, device='cuda:0', grad_fn=<AddBackward0>)
0.75176483
tensor(0.3855, device='cuda:0', grad_fn=<AddBackward0>)
0.75186473
tensor(0.4236, device='cuda:0', grad_fn=<AddBackward0>)
0.75181812
tensor(0.4095, device='cuda:0', grad_fn=<AddBackward0>)
0.75198543
tensor(0.3678, device='cuda:0', grad_fn=<AddBackward0>)
0.75178236
tensor(0.3047, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][  140/  196]   Loss 0.357567   Top1 87.823661   Top5 99.598214   BatchTime 0.322983   LR 0.000229
0.75175083
tensor(0.3215, device='cuda:0', grad_fn=<AddBackward0>)
0.75196940
tensor(0.2898, device='cuda:0', grad_fn=<AddBackward0>)
0.75181776
tensor(0.3454, device='cuda:0', grad_fn=<AddBackward0>)
0.75152129
tensor(0.3793, device='cuda:0', grad_fn=<AddBackward0>)
0.75174195
tensor(0.3071, device='cuda:0', grad_fn=<AddBackward0>)
0.75200319
tensor(0.3843, device='cuda:0', grad_fn=<AddBackward0>)
0.75206161
tensor(0.3651, device='cuda:0', grad_fn=<AddBackward0>)
0.75193495
tensor(0.2951, device='cuda:0', grad_fn=<AddBackward0>)
0.75176358
tensor(0.3444, device='cuda:0', grad_fn=<AddBackward0>)
0.75182509
tensor(0.3337, device='cuda:0', grad_fn=<AddBackward0>)
0.75175959
tensor(0.3648, device='cuda:0', grad_fn=<AddBackward0>)
0.75168443
tensor(0.2613, device='cuda:0', grad_fn=<AddBackward0>)
0.75168937
tensor(0.3574, device='cuda:0', grad_fn=<AddBackward0>)
0.75183254
INFO - Training [13][  160/  196]   Loss 0.356071   Top1 87.885742   Top5 99.604492   BatchTime 0.318888   LR 0.000228
tensor(0.3032, device='cuda:0', grad_fn=<AddBackward0>)
0.75181246
tensor(0.3577, device='cuda:0', grad_fn=<AddBackward0>)
0.75198519
tensor(0.3963, device='cuda:0', grad_fn=<AddBackward0>)
0.75194752
tensor(0.3422, device='cuda:0', grad_fn=<AddBackward0>)
0.75163758
tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)
0.75154066
tensor(0.3535, device='cuda:0', grad_fn=<AddBackward0>)
0.75167990
tensor(0.4029, device='cuda:0', grad_fn=<AddBackward0>)
0.75180870
tensor(0.3674, device='cuda:0', grad_fn=<AddBackward0>)
0.75173432
tensor(0.3484, device='cuda:0', grad_fn=<AddBackward0>)
0.75170612
tensor(0.4210, device='cuda:0', grad_fn=<AddBackward0>)
0.75142187
tensor(0.3499, device='cuda:0', grad_fn=<AddBackward0>)
0.75145274
tensor(0.3338, device='cuda:0', grad_fn=<AddBackward0>)
0.75149357
tensor(0.3043, device='cuda:0', grad_fn=<AddBackward0>)
0.75153166
tensor(0.3720, device='cuda:0', grad_fn=<AddBackward0>)
0.75111455
tensor(0.3730, device='cuda:0', grad_fn=<AddBackward0>)
0.75109160
tensor(0.3671, device='cuda:0', grad_fn=<AddBackward0>)
0.75103939
tensor(0.3148, device='cuda:0', grad_fn=<AddBackward0>)
0.75105619
tensor(0.3020, device='cuda:0', grad_fn=<AddBackward0>)
0.75098622
tensor(0.3832, device='cuda:0', grad_fn=<AddBackward0>)
0.75068998
tensor(0.3201, device='cuda:0', grad_fn=<AddBackward0>)
0.75070518
tensor(0.3605, device='cuda:0', grad_fn=<AddBackward0>)
0.75091147
tensor(0.2947, device='cuda:0', grad_fn=<AddBackward0>)
0.75082070
tensor(0.3401, device='cuda:0', grad_fn=<AddBackward0>)
0.75085181
tensor(0.3549, device='cuda:0', grad_fn=<AddBackward0>)
0.75083297
tensor(0.3829, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [13][  180/  196]   Loss 0.355343   Top1 87.890625   Top5 99.602865   BatchTime 0.320131   LR 0.000227
0.75089771
tensor(0.3858, device='cuda:0', grad_fn=<AddBackward0>)
0.75108755
tensor(0.4565, device='cuda:0', grad_fn=<AddBackward0>)
0.75146461
tensor(0.3794, device='cuda:0', grad_fn=<AddBackward0>)
0.75153840
tensor(0.3993, device='cuda:0', grad_fn=<AddBackward0>)
0.75153399
tensor(0.4163, device='cuda:0', grad_fn=<AddBackward0>)
0.75170857
tensor(0.3407, device='cuda:0', grad_fn=<AddBackward0>)
0.75178593
tensor(0.3842, device='cuda:0', grad_fn=<AddBackward0>)
0.75154179
tensor(0.2810, device='cuda:0', grad_fn=<AddBackward0>)
0.75146401
tensor(0.3541, device='cuda:0', grad_fn=<AddBackward0>)
0.75147438
tensor(0.4911, device='cuda:0', grad_fn=<AddBackward0>)
0.75149471
tensor(0.3393, device='cuda:0', grad_fn=<AddBackward0>)
0.75168008
tensor(0.3848, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 87.812    Top5: 99.596    Loss: 0.357
0.75149292
tensor(0.3145, device='cuda:0', grad_fn=<AddBackward0>)
0.75158548
tensor(0.5303, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [13][   20/   40]   Loss 0.434088   Top1 85.371094   Top5 99.296875   BatchTime 0.113050
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1250)
features.1.conv.0 tensor(0.0566)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0933)
features.2.conv.0 tensor(0.1175)
features.2.conv.3 tensor(0.3534)
features.2.conv.6 tensor(0.5049)
features.3.conv.0 tensor(0.0920)
features.3.conv.3 tensor(0.0910)
features.3.conv.6 tensor(0.1053)
features.4.conv.0 tensor(0.0549)
features.4.conv.3 tensor(0.3154)
features.4.conv.6 tensor(0.1833)
features.5.conv.0 tensor(0.2822)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.1074)
features.6.conv.0 tensor(0.0532)
features.6.conv.3 tensor(0.0596)
features.6.conv.6 tensor(0.0879)
features.7.conv.0 tensor(0.1641)
features.7.conv.3 tensor(0.4612)
features.7.conv.6 tensor(0.1899)
features.8.conv.0 tensor(0.5385)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.1564)
features.9.conv.0 tensor(0.4477)
features.9.conv.3 tensor(0.5613)
features.9.conv.6 tensor(0.1316)
features.10.conv.0 tensor(0.0649)
features.10.conv.3 tensor(0.1059)
features.10.conv.6 tensor(0.1006)
features.11.conv.0 tensor(0.7037)
features.11.conv.3 tensor(0.6454)
features.11.conv.6 tensor(0.1930)
features.12.conv.0 tensor(0.7073)
features.12.conv.3 tensor(0.6748)
features.12.conv.6 tensor(0.2380)
features.13.conv.0 tensor(0.2107)
features.13.conv.3 tensor(0.4905)
features.13.conv.6 tensor(0.0882)
features.14.conv.0 tensor(0.8693)
features.14.conv.3 tensor(0.8265)
features.14.conv.6 tensor(0.9456)
features.15.conv.0 tensor(0.8300)
features.15.conv.3 tensor(0.8350)
features.15.conv.6 tensor(0.9488)
features.16.conv.0 tensor(0.6250)
features.16.conv.3 tensor(0.8072)
features.16.conv.6 tensor(0.1126)
conv.0 tensor(0.0945)
tensor(935405.) 2188896.0
INFO - Validation [13][   40/   40]   Loss 0.429785   Top1 85.460000   Top5 99.390000   BatchTime 0.083879
INFO - ==> Top1: 85.460    Top5: 99.390    Loss: 0.430
INFO - ==> Sparsity : 0.427
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
0.75173813
tensor(0.3711, device='cuda:0', grad_fn=<AddBackward0>)
0.75161374
tensor(0.3239, device='cuda:0', grad_fn=<AddBackward0>)
0.75156361
tensor(0.2429, device='cuda:0', grad_fn=<AddBackward0>)
0.75129825
tensor(0.3208, device='cuda:0', grad_fn=<AddBackward0>)
0.75132287
tensor(0.2743, device='cuda:0', grad_fn=<AddBackward0>)
0.75125754
tensor(0.3316, device='cuda:0', grad_fn=<AddBackward0>)
0.75140971
tensor(0.4555, device='cuda:0', grad_fn=<AddBackward0>)
0.75147450
tensor(0.3177, device='cuda:0', grad_fn=<AddBackward0>)
0.75145870
tensor(0.3548, device='cuda:0', grad_fn=<AddBackward0>)
0.75136149
tensor(0.3372, device='cuda:0', grad_fn=<AddBackward0>)
0.75133020
tensor(0.4559, device='cuda:0', grad_fn=<AddBackward0>)
0.75137329
tensor(0.3303, device='cuda:0', grad_fn=<AddBackward0>)
0.75141871
tensor(0.3150, device='cuda:0', grad_fn=<AddBackward0>)
0.75116539
tensor(0.3268, device='cuda:0', grad_fn=<AddBackward0>)
0.75099272
tensor(0.3288, device='cuda:0', grad_fn=<AddBackward0>)
0.75102830
tensor(0.2830, device='cuda:0', grad_fn=<AddBackward0>)
0.75124151
tensor(0.3682, device='cuda:0', grad_fn=<AddBackward0>)
0.75142658
tensor(0.4037, device='cuda:0', grad_fn=<AddBackward0>)
0.75158423
tensor(0.3328, device='cuda:0', grad_fn=<AddBackward0>)
0.75137866
tensor(0.3285, device='cuda:0', grad_fn=<AddBackward0>)
0.75137204
tensor(0.3241, device='cuda:0', grad_fn=<AddBackward0>)
0.75133061
tensor(0.3254, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][   20/  196]   Loss 0.340139   Top1 88.125000   Top5 99.785156   BatchTime 0.377728   LR 0.000225
0.75124764
tensor(0.4039, device='cuda:0', grad_fn=<AddBackward0>)
0.75100732
tensor(0.3195, device='cuda:0', grad_fn=<AddBackward0>)
0.75080884
tensor(0.3491, device='cuda:0', grad_fn=<AddBackward0>)
0.75074965
tensor(0.3807, device='cuda:0', grad_fn=<AddBackward0>)
0.75067949
tensor(0.3482, device='cuda:0', grad_fn=<AddBackward0>)
0.75091529
tensor(0.2539, device='cuda:0', grad_fn=<AddBackward0>)
0.75106686
tensor(0.3591, device='cuda:0', grad_fn=<AddBackward0>)
0.75110775
tensor(0.3500, device='cuda:0', grad_fn=<AddBackward0>)
0.75137079
tensor(0.3176, device='cuda:0', grad_fn=<AddBackward0>)
0.75134832
tensor(0.3413, device='cuda:0', grad_fn=<AddBackward0>)
0.75132388
tensor(0.3629, device='cuda:0', grad_fn=<AddBackward0>)
0.75165313
tensor(0.3259, device='cuda:0', grad_fn=<AddBackward0>)
0.75169969
tensor(0.3474, device='cuda:0', grad_fn=<AddBackward0>)
0.75164688
tensor(0.2791, device='cuda:0', grad_fn=<AddBackward0>)
0.75158662
tensor(0.2818, device='cuda:0', grad_fn=<AddBackward0>)
0.75157809
tensor(0.3422, device='cuda:0', grad_fn=<AddBackward0>)
0.75184238
tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)
0.75187010
tensor(0.3359, device='cuda:0', grad_fn=<AddBackward0>)
0.75199372
tensor(0.4068, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][   40/  196]   Loss 0.338953   Top1 88.203125   Top5 99.726562   BatchTime 0.352411   LR 0.000224
0.75185978
tensor(0.3815, device='cuda:0', grad_fn=<AddBackward0>)
0.75177741
tensor(0.3335, device='cuda:0', grad_fn=<AddBackward0>)
0.75183475
tensor(0.4307, device='cuda:0', grad_fn=<AddBackward0>)
0.75188416
tensor(0.3559, device='cuda:0', grad_fn=<AddBackward0>)
0.75179154
tensor(0.3666, device='cuda:0', grad_fn=<AddBackward0>)
0.75182116
tensor(0.3000, device='cuda:0', grad_fn=<AddBackward0>)
0.75194126
tensor(0.2750, device='cuda:0', grad_fn=<AddBackward0>)
0.75188082
tensor(0.2374, device='cuda:0', grad_fn=<AddBackward0>)
0.75193089
tensor(0.2860, device='cuda:0', grad_fn=<AddBackward0>)
0.75212455
tensor(0.2557, device='cuda:0', grad_fn=<AddBackward0>)
0.75187963
tensor(0.3801, device='cuda:0', grad_fn=<AddBackward0>)
0.75190693
tensor(0.2759, device='cuda:0', grad_fn=<AddBackward0>)
0.75183523
tensor(0.3648, device='cuda:0', grad_fn=<AddBackward0>)
0.75183064
tensor(0.4003, device='cuda:0', grad_fn=<AddBackward0>)
0.75172657
tensor(0.3853, device='cuda:0', grad_fn=<AddBackward0>)
0.75176209
tensor(0.4493, device='cuda:0', grad_fn=<AddBackward0>)
0.75174749
tensor(0.2298, device='cuda:0', grad_fn=<AddBackward0>)
0.75173253
tensor(0.2845, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][   60/  196]   Loss 0.336778   Top1 88.365885   Top5 99.700521   BatchTime 0.346911   LR 0.000223
0.75173426
tensor(0.2493, device='cuda:0', grad_fn=<AddBackward0>)
0.75179976
tensor(0.3590, device='cuda:0', grad_fn=<AddBackward0>)
0.75191092
tensor(0.3693, device='cuda:0', grad_fn=<AddBackward0>)
0.75186944
tensor(0.3199, device='cuda:0', grad_fn=<AddBackward0>)
0.75188518
tensor(0.2630, device='cuda:0', grad_fn=<AddBackward0>)
0.75156987
tensor(0.3614, device='cuda:0', grad_fn=<AddBackward0>)
0.75159878
tensor(0.2789, device='cuda:0', grad_fn=<AddBackward0>)
0.75182265
tensor(0.3557, device='cuda:0', grad_fn=<AddBackward0>)
0.75201505
tensor(0.3106, device='cuda:0', grad_fn=<AddBackward0>)
0.75200558
tensor(0.3079, device='cuda:0', grad_fn=<AddBackward0>)
0.75187552
tensor(0.3046, device='cuda:0', grad_fn=<AddBackward0>)
0.75194561
tensor(0.2652, device='cuda:0', grad_fn=<AddBackward0>)
0.75172764
tensor(0.2937, device='cuda:0', grad_fn=<AddBackward0>)
0.75162590
tensor(0.2702, device='cuda:0', grad_fn=<AddBackward0>)
0.75170070
tensor(0.3567, device='cuda:0', grad_fn=<AddBackward0>)
0.75171459
tensor(0.3500, device='cuda:0', grad_fn=<AddBackward0>)
0.75180954
tensor(0.3370, device='cuda:0', grad_fn=<AddBackward0>)
0.75193101
tensor(0.5442, device='cuda:0', grad_fn=<AddBackward0>)
0.75188041
tensor(0.3278, device='cuda:0', grad_fn=<AddBackward0>)
0.75162745
tensor(0.3334, device='cuda:0', grad_fn=<AddBackward0>)
0.75155324
tensor(0.3135, device='cuda:0', grad_fn=<AddBackward0>)
0.75166166
tensor(0.3432, device='cuda:0', grad_fn=<AddBackward0>)
0.75169045
tensor(0.2824, device='cuda:0', grad_fn=<AddBackward0>)
0.75175869
tensor(0.3542, device='cuda:0', grad_fn=<AddBackward0>)
0.75205815
tensor(0.3833, device='cuda:0', grad_fn=<AddBackward0>)
0.75208414
tensor(0.3768, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][   80/  196]   Loss 0.335360   Top1 88.544922   Top5 99.692383   BatchTime 0.339039   LR 0.000221
0.75212675
tensor(0.2559, device='cuda:0', grad_fn=<AddBackward0>)
0.75198817
tensor(0.4028, device='cuda:0', grad_fn=<AddBackward0>)
0.75172162
tensor(0.2688, device='cuda:0', grad_fn=<AddBackward0>)
0.75156671
tensor(0.3213, device='cuda:0', grad_fn=<AddBackward0>)
0.75151533
tensor(0.4004, device='cuda:0', grad_fn=<AddBackward0>)
0.75141650
tensor(0.2933, device='cuda:0', grad_fn=<AddBackward0>)
0.75128460
tensor(0.3994, device='cuda:0', grad_fn=<AddBackward0>)
0.75132525
tensor(0.3684, device='cuda:0', grad_fn=<AddBackward0>)
0.75136667
tensor(0.4466, device='cuda:0', grad_fn=<AddBackward0>)
0.75154549
tensor(0.3335, device='cuda:0', grad_fn=<AddBackward0>)
0.75161141
tensor(0.4116, device='cuda:0', grad_fn=<AddBackward0>)
0.75161743
tensor(0.3497, device='cuda:0', grad_fn=<AddBackward0>)
0.75172007
tensor(0.3820, device='cuda:0', grad_fn=<AddBackward0>)
0.75152415
tensor(0.3056, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][  100/  196]   Loss 0.339547   Top1 88.445312   Top5 99.656250   BatchTime 0.327575   LR 0.000220
0.75141841
tensor(0.4467, device='cuda:0', grad_fn=<AddBackward0>)
0.75124514
tensor(0.3951, device='cuda:0', grad_fn=<AddBackward0>)
0.75106674
tensor(0.3165, device='cuda:0', grad_fn=<AddBackward0>)
0.75086504
tensor(0.3397, device='cuda:0', grad_fn=<AddBackward0>)
0.75084680
tensor(0.3374, device='cuda:0', grad_fn=<AddBackward0>)
0.75075060
tensor(0.2935, device='cuda:0', grad_fn=<AddBackward0>)
0.75086194
tensor(0.3280, device='cuda:0', grad_fn=<AddBackward0>)
0.75109679
tensor(0.4393, device='cuda:0', grad_fn=<AddBackward0>)
0.75095063
tensor(0.3095, device='cuda:0', grad_fn=<AddBackward0>)
0.75080413
tensor(0.4343, device='cuda:0', grad_fn=<AddBackward0>)
0.75079781
tensor(0.3483, device='cuda:0', grad_fn=<AddBackward0>)
0.75071907
tensor(0.2938, device='cuda:0', grad_fn=<AddBackward0>)
0.75091684
tensor(0.2842, device='cuda:0', grad_fn=<AddBackward0>)
0.75078708
tensor(0.3538, device='cuda:0', grad_fn=<AddBackward0>)
0.75075412
tensor(0.4430, device='cuda:0', grad_fn=<AddBackward0>)
0.75077295
tensor(0.3354, device='cuda:0', grad_fn=<AddBackward0>)
0.75083596
tensor(0.3288, device='cuda:0', grad_fn=<AddBackward0>)
0.75102890
tensor(0.4281, device='cuda:0', grad_fn=<AddBackward0>)
0.75084841
tensor(0.3130, device='cuda:0', grad_fn=<AddBackward0>)
0.75075102
tensor(0.3446, device='cuda:0', grad_fn=<AddBackward0>)
0.75072998
tensor(0.3678, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][  120/  196]   Loss 0.341571   Top1 88.378906   Top5 99.661458   BatchTime 0.320146   LR 0.000219
0.75052947
tensor(0.3518, device='cuda:0', grad_fn=<AddBackward0>)
0.75051725
tensor(0.4122, device='cuda:0', grad_fn=<AddBackward0>)
0.75050199
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.75033027
tensor(0.4392, device='cuda:0', grad_fn=<AddBackward0>)
0.75063926
tensor(0.3272, device='cuda:0', grad_fn=<AddBackward0>)
0.75105733
tensor(0.3882, device='cuda:0', grad_fn=<AddBackward0>)
0.75142020
tensor(0.3174, device='cuda:0', grad_fn=<AddBackward0>)
0.75130779
tensor(0.3182, device='cuda:0', grad_fn=<AddBackward0>)
0.75100398
tensor(0.3478, device='cuda:0', grad_fn=<AddBackward0>)
0.75082076
tensor(0.4239, device='cuda:0', grad_fn=<AddBackward0>)
0.75089884
tensor(0.4109, device='cuda:0', grad_fn=<AddBackward0>)
0.75078201
tensor(0.2785, device='cuda:0', grad_fn=<AddBackward0>)
0.75050956
tensor(0.3970, device='cuda:0', grad_fn=<AddBackward0>)
0.75037038
tensor(0.3200, device='cuda:0', grad_fn=<AddBackward0>)
0.75031602
tensor(0.4593, device='cuda:0', grad_fn=<AddBackward0>)
0.75018108
tensor(0.3473, device='cuda:0', grad_fn=<AddBackward0>)
0.75013912
tensor(0.3490, device='cuda:0', grad_fn=<AddBackward0>)
0.74998158
tensor(0.3739, device='cuda:0', grad_fn=<AddBackward0>)
0.75024861
tensor(0.3788, device='cuda:0', grad_fn=<AddBackward0>)
0.75024241
tensor(0.3720, device='cuda:0', grad_fn=<AddBackward0>)
0.75016785
tensor(0.4199, device='cuda:0', grad_fn=<AddBackward0>)
0.75019062
tensor(0.2834, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][  140/  196]   Loss 0.344957   Top1 88.278460   Top5 99.659598   BatchTime 0.313363   LR 0.000217
0.74991924
tensor(0.3719, device='cuda:0', grad_fn=<AddBackward0>)
0.74978960
tensor(0.2918, device='cuda:0', grad_fn=<AddBackward0>)
0.74955261
tensor(0.3955, device='cuda:0', grad_fn=<AddBackward0>)
0.74958998
tensor(0.3894, device='cuda:0', grad_fn=<AddBackward0>)
0.74939686
tensor(0.3298, device='cuda:0', grad_fn=<AddBackward0>)
0.74949825
tensor(0.3679, device='cuda:0', grad_fn=<AddBackward0>)
0.74955851
tensor(0.2961, device='cuda:0', grad_fn=<AddBackward0>)
0.74972916
tensor(0.3108, device='cuda:0', grad_fn=<AddBackward0>)
0.74983311
tensor(0.3431, device='cuda:0', grad_fn=<AddBackward0>)
0.74984598
tensor(0.3504, device='cuda:0', grad_fn=<AddBackward0>)
0.74983305
tensor(0.2887, device='cuda:0', grad_fn=<AddBackward0>)
0.74983400
tensor(0.3593, device='cuda:0', grad_fn=<AddBackward0>)
0.74959928
tensor(0.2824, device='cuda:0', grad_fn=<AddBackward0>)
0.74937689
tensor(0.3515, device='cuda:0', grad_fn=<AddBackward0>)
0.74940479
tensor(0.2997, device='cuda:0', grad_fn=<AddBackward0>)
0.74926114
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.74919540
tensor(0.3711, device='cuda:0', grad_fn=<AddBackward0>)
0.74912900
tensor(0.3411, device='cuda:0', grad_fn=<AddBackward0>)
0.74958521
tensor(0.2919, device='cuda:0', grad_fn=<AddBackward0>)
0.74929899
tensor(0.3943, device='cuda:0', grad_fn=<AddBackward0>)
0.74900973
tensor(0.4194, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][  160/  196]   Loss 0.343670   Top1 88.298340   Top5 99.670410   BatchTime 0.310117   LR 0.000216
0.74885648
tensor(0.3303, device='cuda:0', grad_fn=<AddBackward0>)
0.74866903
tensor(0.3205, device='cuda:0', grad_fn=<AddBackward0>)
0.74835157
tensor(0.3464, device='cuda:0', grad_fn=<AddBackward0>)
0.74810696
tensor(0.2626, device='cuda:0', grad_fn=<AddBackward0>)
0.74825883
tensor(0.3534, device='cuda:0', grad_fn=<AddBackward0>)
0.74824232
tensor(0.3615, device='cuda:0', grad_fn=<AddBackward0>)
0.74823546
tensor(0.2735, device='cuda:0', grad_fn=<AddBackward0>)
0.74806976
tensor(0.3440, device='cuda:0', grad_fn=<AddBackward0>)
0.74774057
tensor(0.3437, device='cuda:0', grad_fn=<AddBackward0>)
0.74799812
tensor(0.3704, device='cuda:0', grad_fn=<AddBackward0>)
0.74806225
tensor(0.3259, device='cuda:0', grad_fn=<AddBackward0>)
0.74808180
tensor(0.3207, device='cuda:0', grad_fn=<AddBackward0>)
0.74799025
tensor(0.3012, device='cuda:0', grad_fn=<AddBackward0>)
0.74796528
tensor(0.2840, device='cuda:0', grad_fn=<AddBackward0>)
0.74809027
tensor(0.3391, device='cuda:0', grad_fn=<AddBackward0>)
0.74778253
tensor(0.4453, device='cuda:0', grad_fn=<AddBackward0>)
0.74794364
tensor(0.3255, device='cuda:0', grad_fn=<AddBackward0>)
0.74751109
tensor(0.2705, device='cuda:0', grad_fn=<AddBackward0>)
0.74712288
tensor(0.2450, device='cuda:0', grad_fn=<AddBackward0>)
0.74669266
tensor(0.3379, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [14][  180/  196]   Loss 0.343004   Top1 88.320312   Top5 99.676649   BatchTime 0.309019   LR 0.000215
0.74648845
tensor(0.3715, device='cuda:0', grad_fn=<AddBackward0>)
0.74648196
tensor(0.3314, device='cuda:0', grad_fn=<AddBackward0>)
0.74603844
tensor(0.3561, device='cuda:0', grad_fn=<AddBackward0>)
0.74576914
tensor(0.3911, device='cuda:0', grad_fn=<AddBackward0>)
0.74541748
tensor(0.3319, device='cuda:0', grad_fn=<AddBackward0>)
0.74536860
tensor(0.3216, device='cuda:0', grad_fn=<AddBackward0>)
0.74541807
tensor(0.4111, device='cuda:0', grad_fn=<AddBackward0>)
0.74502522
tensor(0.4244, device='cuda:0', grad_fn=<AddBackward0>)
0.74483031
tensor(0.3655, device='cuda:0', grad_fn=<AddBackward0>)
0.74466920
tensor(0.3845, device='cuda:0', grad_fn=<AddBackward0>)
0.74486601
tensor(0.3343, device='cuda:0', grad_fn=<AddBackward0>)
0.74473965
tensor(0.3729, device='cuda:0', grad_fn=<AddBackward0>)
0.74489492
tensor(0.2344, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 88.334    Top5: 99.664    Loss: 0.343
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [14][   20/   40]   Loss 0.419004   Top1 85.898438   Top5 99.238281   BatchTime 0.112923
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0410)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0981)
features.2.conv.0 tensor(0.0874)
features.2.conv.3 tensor(0.3526)
features.2.conv.6 tensor(0.5223)
features.3.conv.0 tensor(0.0828)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1037)
features.4.conv.0 tensor(0.0605)
features.4.conv.3 tensor(0.3090)
features.4.conv.6 tensor(0.1816)
features.5.conv.0 tensor(0.2912)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.1064)
features.6.conv.0 tensor(0.0558)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0864)
features.7.conv.0 tensor(0.1588)
features.7.conv.3 tensor(0.4641)
features.7.conv.6 tensor(0.1920)
features.8.conv.0 tensor(0.5424)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.1601)
features.9.conv.0 tensor(0.4459)
features.9.conv.3 tensor(0.5616)
features.9.conv.6 tensor(0.1359)
features.10.conv.0 tensor(0.0704)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0987)
features.11.conv.0 tensor(0.7288)
features.11.conv.3 tensor(0.6487)
features.11.conv.6 tensor(0.2041)
features.12.conv.0 tensor(0.7155)
features.12.conv.3 tensor(0.6750)
features.12.conv.6 tensor(0.2303)
features.13.conv.0 tensor(0.2096)
features.13.conv.3 tensor(0.4950)
features.13.conv.6 tensor(0.0889)
features.14.conv.0 tensor(0.8676)
features.14.conv.3 tensor(0.8262)
features.14.conv.6 tensor(0.9504)
features.15.conv.0 tensor(0.8362)
features.15.conv.3 tensor(0.8353)
features.15.conv.6 tensor(0.9539)
features.16.conv.0 tensor(0.6252)
features.16.conv.3 tensor(0.8079)
features.16.conv.6 tensor(0.2449)
conv.0 tensor(0.0938)
tensor(980267.) 2188896.0
INFO - Validation [14][   40/   40]   Loss 0.420385   Top1 85.780000   Top5 99.350000   BatchTime 0.082663
INFO - ==> Top1: 85.780    Top5: 99.350    Loss: 0.420
INFO - ==> Sparsity : 0.448
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
0.74472213
tensor(0.2712, device='cuda:0', grad_fn=<AddBackward0>)
0.74451995
tensor(0.3394, device='cuda:0', grad_fn=<AddBackward0>)
0.74417150
tensor(0.2971, device='cuda:0', grad_fn=<AddBackward0>)
0.74388492
tensor(0.2883, device='cuda:0', grad_fn=<AddBackward0>)
0.74405646
tensor(0.2582, device='cuda:0', grad_fn=<AddBackward0>)
0.74412012
tensor(0.3221, device='cuda:0', grad_fn=<AddBackward0>)
0.74400252
tensor(0.3492, device='cuda:0', grad_fn=<AddBackward0>)
0.74367499
tensor(0.3085, device='cuda:0', grad_fn=<AddBackward0>)
0.74349874
tensor(0.3302, device='cuda:0', grad_fn=<AddBackward0>)
0.74332005
tensor(0.2872, device='cuda:0', grad_fn=<AddBackward0>)
0.74295259
tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)
0.74255711
tensor(0.2477, device='cuda:0', grad_fn=<AddBackward0>)
0.74241203
tensor(0.3537, device='cuda:0', grad_fn=<AddBackward0>)
0.74228799
tensor(0.3900, device='cuda:0', grad_fn=<AddBackward0>)
0.74257886
tensor(0.3482, device='cuda:0', grad_fn=<AddBackward0>)
0.74280965
tensor(0.3042, device='cuda:0', grad_fn=<AddBackward0>)
0.74298924
tensor(0.3179, device='cuda:0', grad_fn=<AddBackward0>)
0.74289507
tensor(0.3517, device='cuda:0', grad_fn=<AddBackward0>)
0.74233299
tensor(0.3311, device='cuda:0', grad_fn=<AddBackward0>)
0.74214649
tensor(0.4186, device='cuda:0', grad_fn=<AddBackward0>)
0.74181724
tensor(0.3032, device='cuda:0', grad_fn=<AddBackward0>)
0.74161112
tensor(0.3069, device='cuda:0', grad_fn=<AddBackward0>)
0.74149895
tensor(0.3694, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][   20/  196]   Loss 0.316513   Top1 89.335938   Top5 99.765625   BatchTime 0.391759   LR 0.000212
0.74119699
tensor(0.3137, device='cuda:0', grad_fn=<AddBackward0>)
0.74111414
tensor(0.2705, device='cuda:0', grad_fn=<AddBackward0>)
0.74108815
tensor(0.2796, device='cuda:0', grad_fn=<AddBackward0>)
0.74088949
tensor(0.3547, device='cuda:0', grad_fn=<AddBackward0>)
0.74079794
tensor(0.2159, device='cuda:0', grad_fn=<AddBackward0>)
0.74072313
tensor(0.3766, device='cuda:0', grad_fn=<AddBackward0>)
0.74044812
tensor(0.3526, device='cuda:0', grad_fn=<AddBackward0>)
0.74033558
tensor(0.2863, device='cuda:0', grad_fn=<AddBackward0>)
0.74041516
tensor(0.3361, device='cuda:0', grad_fn=<AddBackward0>)
0.74063796
tensor(0.2533, device='cuda:0', grad_fn=<AddBackward0>)
0.74052966
tensor(0.3224, device='cuda:0', grad_fn=<AddBackward0>)
0.74023247
tensor(0.3743, device='cuda:0', grad_fn=<AddBackward0>)
0.74023283
tensor(0.3661, device='cuda:0', grad_fn=<AddBackward0>)
0.74029148
tensor(0.3383, device='cuda:0', grad_fn=<AddBackward0>)
0.74049288
tensor(0.2782, device='cuda:0', grad_fn=<AddBackward0>)
0.74061543
tensor(0.3904, device='cuda:0', grad_fn=<AddBackward0>)
0.74054426
tensor(0.3915, device='cuda:0', grad_fn=<AddBackward0>)
0.74038434
tensor(0.2805, device='cuda:0', grad_fn=<AddBackward0>)
0.74032307
tensor(0.3147, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][   40/  196]   Loss 0.320259   Top1 89.091797   Top5 99.687500   BatchTime 0.365862   LR 0.000211
0.74027520
tensor(0.3031, device='cuda:0', grad_fn=<AddBackward0>)
0.74041682
tensor(0.2597, device='cuda:0', grad_fn=<AddBackward0>)
0.74039406
tensor(0.2678, device='cuda:0', grad_fn=<AddBackward0>)
0.74027067
tensor(0.3619, device='cuda:0', grad_fn=<AddBackward0>)
0.74027777
tensor(0.4239, device='cuda:0', grad_fn=<AddBackward0>)
0.74027646
tensor(0.2461, device='cuda:0', grad_fn=<AddBackward0>)
0.74036080
tensor(0.3165, device='cuda:0', grad_fn=<AddBackward0>)
0.74034208
tensor(0.3639, device='cuda:0', grad_fn=<AddBackward0>)
0.74031132
tensor(0.2946, device='cuda:0', grad_fn=<AddBackward0>)
0.74046081
tensor(0.3606, device='cuda:0', grad_fn=<AddBackward0>)
0.74048048
tensor(0.3566, device='cuda:0', grad_fn=<AddBackward0>)
0.74049032
tensor(0.3263, device='cuda:0', grad_fn=<AddBackward0>)
0.74056613
tensor(0.3165, device='cuda:0', grad_fn=<AddBackward0>)
0.74060971
tensor(0.3039, device='cuda:0', grad_fn=<AddBackward0>)
0.74062717
tensor(0.4097, device='cuda:0', grad_fn=<AddBackward0>)
0.74062300
tensor(0.2498, device='cuda:0', grad_fn=<AddBackward0>)
0.74049276
tensor(0.3025, device='cuda:0', grad_fn=<AddBackward0>)
0.74017572
tensor(0.3333, device='cuda:0', grad_fn=<AddBackward0>)
0.74008560
tensor(0.3413, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][   60/  196]   Loss 0.320036   Top1 89.160156   Top5 99.694010   BatchTime 0.346324   LR 0.000209
0.73970222
tensor(0.3583, device='cuda:0', grad_fn=<AddBackward0>)
0.73940724
tensor(0.3325, device='cuda:0', grad_fn=<AddBackward0>)
0.73931521
tensor(0.3449, device='cuda:0', grad_fn=<AddBackward0>)
0.73937172
tensor(0.3183, device='cuda:0', grad_fn=<AddBackward0>)
0.73939794
tensor(0.3909, device='cuda:0', grad_fn=<AddBackward0>)
0.73944598
tensor(0.2671, device='cuda:0', grad_fn=<AddBackward0>)
0.73965758
tensor(0.2584, device='cuda:0', grad_fn=<AddBackward0>)
0.73971218
tensor(0.3589, device='cuda:0', grad_fn=<AddBackward0>)
0.73963213
tensor(0.2726, device='cuda:0', grad_fn=<AddBackward0>)
0.73949689
tensor(0.2614, device='cuda:0', grad_fn=<AddBackward0>)
0.73969316
tensor(0.3483, device='cuda:0', grad_fn=<AddBackward0>)
0.73991275
tensor(0.3234, device='cuda:0', grad_fn=<AddBackward0>)
0.73969328
tensor(0.2704, device='cuda:0', grad_fn=<AddBackward0>)
0.73962265
tensor(0.2811, device='cuda:0', grad_fn=<AddBackward0>)
0.73894548
tensor(0.3575, device='cuda:0', grad_fn=<AddBackward0>)
0.73861718
tensor(0.3330, device='cuda:0', grad_fn=<AddBackward0>)
0.73839402
tensor(0.3085, device='cuda:0', grad_fn=<AddBackward0>)
0.73769420
tensor(0.2395, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][   80/  196]   Loss 0.317860   Top1 89.282227   Top5 99.721680   BatchTime 0.342446   LR 0.000208
0.73741823
tensor(0.2604, device='cuda:0', grad_fn=<AddBackward0>)
0.73725051
tensor(0.3005, device='cuda:0', grad_fn=<AddBackward0>)
0.73719925
tensor(0.3568, device='cuda:0', grad_fn=<AddBackward0>)
0.73712319
tensor(0.4322, device='cuda:0', grad_fn=<AddBackward0>)
0.73716885
tensor(0.3387, device='cuda:0', grad_fn=<AddBackward0>)
0.73708302
tensor(0.2929, device='cuda:0', grad_fn=<AddBackward0>)
0.73719871
tensor(0.4327, device='cuda:0', grad_fn=<AddBackward0>)
0.73697305
tensor(0.2385, device='cuda:0', grad_fn=<AddBackward0>)
0.73664260
tensor(0.2604, device='cuda:0', grad_fn=<AddBackward0>)
0.73665422
tensor(0.3431, device='cuda:0', grad_fn=<AddBackward0>)
0.73641348
tensor(0.3803, device='cuda:0', grad_fn=<AddBackward0>)
0.73649496
tensor(0.4251, device='cuda:0', grad_fn=<AddBackward0>)
0.73620063
tensor(0.3095, device='cuda:0', grad_fn=<AddBackward0>)
0.73629481
tensor(0.3017, device='cuda:0', grad_fn=<AddBackward0>)
0.73645657
tensor(0.3284, device='cuda:0', grad_fn=<AddBackward0>)
0.73638511
tensor(0.3656, device='cuda:0', grad_fn=<AddBackward0>)
0.73654324
tensor(0.2796, device='cuda:0', grad_fn=<AddBackward0>)
0.73690629
tensor(0.3643, device='cuda:0', grad_fn=<AddBackward0>)
0.73687780
tensor(0.3246, device='cuda:0', grad_fn=<AddBackward0>)
0.73668134
tensor(0.3154, device='cuda:0', grad_fn=<AddBackward0>)
0.73675561
tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)
0.73730314
tensor(0.2819, device='cuda:0', grad_fn=<AddBackward0>)
0.73743182
tensor(0.3029, device='cuda:0', grad_fn=<AddBackward0>)
0.73734277
tensor(0.3758, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][  100/  196]   Loss 0.320887   Top1 89.234375   Top5 99.695312   BatchTime 0.341416   LR 0.000206
0.73735982
tensor(0.3402, device='cuda:0', grad_fn=<AddBackward0>)
0.73717970
tensor(0.3344, device='cuda:0', grad_fn=<AddBackward0>)
0.73733771
tensor(0.3373, device='cuda:0', grad_fn=<AddBackward0>)
0.73740196
tensor(0.3730, device='cuda:0', grad_fn=<AddBackward0>)
0.73742640
tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)
0.73784715
tensor(0.3673, device='cuda:0', grad_fn=<AddBackward0>)
0.73802823
tensor(0.3703, device='cuda:0', grad_fn=<AddBackward0>)
0.73816007
tensor(0.3273, device='cuda:0', grad_fn=<AddBackward0>)
0.73852819
tensor(0.3312, device='cuda:0', grad_fn=<AddBackward0>)
0.73894149
tensor(0.3239, device='cuda:0', grad_fn=<AddBackward0>)
0.73943627
tensor(0.3717, device='cuda:0', grad_fn=<AddBackward0>)
0.73969752
tensor(0.5102, device='cuda:0', grad_fn=<AddBackward0>)
0.73964536
tensor(0.2948, device='cuda:0', grad_fn=<AddBackward0>)
0.73955333
tensor(0.3295, device='cuda:0', grad_fn=<AddBackward0>)
0.73950118
tensor(0.3917, device='cuda:0', grad_fn=<AddBackward0>)
0.73937643
tensor(0.3055, device='cuda:0', grad_fn=<AddBackward0>)
0.73925328
tensor(0.3143, device='cuda:0', grad_fn=<AddBackward0>)
0.73949909
tensor(0.2749, device='cuda:0', grad_fn=<AddBackward0>)
0.73948395
tensor(0.3341, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][  120/  196]   Loss 0.324220   Top1 89.134115   Top5 99.690755   BatchTime 0.335865   LR 0.000205
0.73925406
tensor(0.2877, device='cuda:0', grad_fn=<AddBackward0>)
0.73911422
tensor(0.3026, device='cuda:0', grad_fn=<AddBackward0>)
0.73923314
tensor(0.3591, device='cuda:0', grad_fn=<AddBackward0>)
0.73932672
tensor(0.3506, device='cuda:0', grad_fn=<AddBackward0>)
0.73932999
tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)
0.73913330
tensor(0.2695, device='cuda:0', grad_fn=<AddBackward0>)
0.73926175
tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)
0.73965186
tensor(0.3799, device='cuda:0', grad_fn=<AddBackward0>)
0.73968530
tensor(0.2750, device='cuda:0', grad_fn=<AddBackward0>)
0.73954159
tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)
0.73936337
tensor(0.3718, device='cuda:0', grad_fn=<AddBackward0>)
0.73907918
tensor(0.3102, device='cuda:0', grad_fn=<AddBackward0>)
0.73896432
tensor(0.3374, device='cuda:0', grad_fn=<AddBackward0>)
0.73863357
tensor(0.3139, device='cuda:0', grad_fn=<AddBackward0>)
0.73870200
tensor(0.3351, device='cuda:0', grad_fn=<AddBackward0>)
0.73896009
tensor(0.2938, device='cuda:0', grad_fn=<AddBackward0>)
0.73883843
tensor(0.2740, device='cuda:0', grad_fn=<AddBackward0>)
0.73882288
tensor(0.3497, device='cuda:0', grad_fn=<AddBackward0>)
0.73865074
tensor(0.3501, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][  140/  196]   Loss 0.322229   Top1 89.154576   Top5 99.690290   BatchTime 0.334991   LR 0.000203
0.73854840
tensor(0.2977, device='cuda:0', grad_fn=<AddBackward0>)
0.73847246
tensor(0.3174, device='cuda:0', grad_fn=<AddBackward0>)
0.73822570
tensor(0.3094, device='cuda:0', grad_fn=<AddBackward0>)
0.73812842
tensor(0.3842, device='cuda:0', grad_fn=<AddBackward0>)
0.73795480
tensor(0.3978, device='cuda:0', grad_fn=<AddBackward0>)
0.73786187
tensor(0.3591, device='cuda:0', grad_fn=<AddBackward0>)
0.73810410
tensor(0.3647, device='cuda:0', grad_fn=<AddBackward0>)
0.73794705
tensor(0.2805, device='cuda:0', grad_fn=<AddBackward0>)
0.73791593
tensor(0.3193, device='cuda:0', grad_fn=<AddBackward0>)
0.73820633
tensor(0.3150, device='cuda:0', grad_fn=<AddBackward0>)
0.73832345
tensor(0.3144, device='cuda:0', grad_fn=<AddBackward0>)
0.73849970
tensor(0.4076, device='cuda:0', grad_fn=<AddBackward0>)
0.73842800
tensor(0.3207, device='cuda:0', grad_fn=<AddBackward0>)
0.73836112
tensor(0.3084, device='cuda:0', grad_fn=<AddBackward0>)
0.73842865
tensor(0.2803, device='cuda:0', grad_fn=<AddBackward0>)
0.73850137
tensor(0.3367, device='cuda:0', grad_fn=<AddBackward0>)
0.73847592
tensor(0.2924, device='cuda:0', grad_fn=<AddBackward0>)
0.73830473
tensor(0.2925, device='cuda:0', grad_fn=<AddBackward0>)
0.73827660
tensor(0.2976, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][  160/  196]   Loss 0.322861   Top1 89.099121   Top5 99.692383   BatchTime 0.331415   LR 0.000201
0.73839021
tensor(0.3948, device='cuda:0', grad_fn=<AddBackward0>)
0.73832285
tensor(0.3364, device='cuda:0', grad_fn=<AddBackward0>)
0.73827988
tensor(0.3535, device='cuda:0', grad_fn=<AddBackward0>)
0.73822647
tensor(0.2302, device='cuda:0', grad_fn=<AddBackward0>)
0.73829651
tensor(0.2409, device='cuda:0', grad_fn=<AddBackward0>)
0.73815358
tensor(0.2793, device='cuda:0', grad_fn=<AddBackward0>)
0.73800242
tensor(0.3032, device='cuda:0', grad_fn=<AddBackward0>)
0.73795557
tensor(0.2599, device='cuda:0', grad_fn=<AddBackward0>)
0.73799270
tensor(0.3208, device='cuda:0', grad_fn=<AddBackward0>)
0.73833221
tensor(0.2653, device='cuda:0', grad_fn=<AddBackward0>)
0.73809141
tensor(0.3377, device='cuda:0', grad_fn=<AddBackward0>)
0.73780489
tensor(0.3228, device='cuda:0', grad_fn=<AddBackward0>)
0.73775268
tensor(0.3464, device='cuda:0', grad_fn=<AddBackward0>)
0.73784190
tensor(0.3235, device='cuda:0', grad_fn=<AddBackward0>)
0.73810279
tensor(0.3639, device='cuda:0', grad_fn=<AddBackward0>)
0.73817420
tensor(0.2973, device='cuda:0', grad_fn=<AddBackward0>)
0.73824984
tensor(0.3014, device='cuda:0', grad_fn=<AddBackward0>)
0.73762619
tensor(0.2835, device='cuda:0', grad_fn=<AddBackward0>)
0.73730147
tensor(0.3423, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [15][  180/  196]   Loss 0.321418   Top1 89.142795   Top5 99.691840   BatchTime 0.331354   LR 0.000200
0.73721081
tensor(0.2942, device='cuda:0', grad_fn=<AddBackward0>)
0.73733336
tensor(0.3792, device='cuda:0', grad_fn=<AddBackward0>)
0.73725927
tensor(0.3627, device='cuda:0', grad_fn=<AddBackward0>)
0.73701257
tensor(0.3603, device='cuda:0', grad_fn=<AddBackward0>)
0.73697203
tensor(0.4102, device='cuda:0', grad_fn=<AddBackward0>)
0.73707545
tensor(0.3775, device='cuda:0', grad_fn=<AddBackward0>)
0.73705298
tensor(0.3121, device='cuda:0', grad_fn=<AddBackward0>)
0.73691821
tensor(0.3289, device='cuda:0', grad_fn=<AddBackward0>)
0.73718935
tensor(0.3588, device='cuda:0', grad_fn=<AddBackward0>)
0.73696190
tensor(0.3412, device='cuda:0', grad_fn=<AddBackward0>)
0.73666245
tensor(0.3287, device='cuda:0', grad_fn=<AddBackward0>)
0.73655868
tensor(0.3550, device='cuda:0', grad_fn=<AddBackward0>)
0.73687470
tensor(0.3031, device='cuda:0', grad_fn=<AddBackward0>)
0.73670733
tensor(0.3299, device='cuda:0', grad_fn=<AddBackward0>)
0.73672855
tensor(0.3398, device='cuda:0', grad_fn=<AddBackward0>)
0.73673999
tensor(0.3286, device='cuda:0', grad_fn=<AddBackward0>)
0.73673004
tensor(0.4622, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 89.078    Top5: 99.678    Loss: 0.324
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [15][   20/   40]   Loss 0.435706   Top1 85.742188   Top5 99.296875   BatchTime 0.119276
features.0.conv.0 tensor(0.2674)
features.0.conv.3 tensor(0.1426)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0985)
features.2.conv.0 tensor(0.1001)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.5327)
features.3.conv.0 tensor(0.0767)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1044)
features.4.conv.0 tensor(0.0552)
features.4.conv.3 tensor(0.3009)
features.4.conv.6 tensor(0.1859)
features.5.conv.0 tensor(0.2915)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.1045)
features.6.conv.0 tensor(0.0514)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0839)
features.7.conv.0 tensor(0.1619)
features.7.conv.3 tensor(0.4609)
features.7.conv.6 tensor(0.1974)
features.8.conv.0 tensor(0.5590)
features.8.conv.3 tensor(0.5422)
features.8.conv.6 tensor(0.1618)
features.9.conv.0 tensor(0.4764)
features.9.conv.3 tensor(0.5628)
features.9.conv.6 tensor(0.1250)
features.10.conv.0 tensor(0.0634)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0986)
features.11.conv.0 tensor(0.7065)
features.11.conv.3 tensor(0.6439)
features.11.conv.6 tensor(0.2088)
features.12.conv.0 tensor(0.7139)
features.12.conv.3 tensor(0.6742)
features.12.conv.6 tensor(0.2286)
features.13.conv.0 tensor(0.2101)
features.13.conv.3 tensor(0.4958)
features.13.conv.6 tensor(0.0885)
features.14.conv.0 tensor(0.8732)
features.14.conv.3 tensor(0.8262)
features.14.conv.6 tensor(0.9503)
features.15.conv.0 tensor(0.8425)
features.15.conv.3 tensor(0.8353)
features.15.conv.6 tensor(0.9512)
features.16.conv.0 tensor(0.6280)
features.16.conv.3 tensor(0.8069)
features.16.conv.6 tensor(0.3747)
conv.0 tensor(0.0943)
tensor(1021894.) 2188896.0
INFO - Validation [15][   40/   40]   Loss 0.423465   Top1 85.920000   Top5 99.410000   BatchTime 0.087195
INFO - ==> Top1: 85.920    Top5: 99.410    Loss: 0.423
INFO - ==> Sparsity : 0.467
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
0.73673022
tensor(0.3556, device='cuda:0', grad_fn=<AddBackward0>)
0.73658067
tensor(0.2824, device='cuda:0', grad_fn=<AddBackward0>)
0.73689377
tensor(0.2582, device='cuda:0', grad_fn=<AddBackward0>)
0.73715895
tensor(0.2625, device='cuda:0', grad_fn=<AddBackward0>)
0.73714215
tensor(0.3245, device='cuda:0', grad_fn=<AddBackward0>)
0.73715377
tensor(0.3215, device='cuda:0', grad_fn=<AddBackward0>)
0.73731840
tensor(0.3598, device='cuda:0', grad_fn=<AddBackward0>)
0.73768574
tensor(0.4102, device='cuda:0', grad_fn=<AddBackward0>)
0.73758465
tensor(0.3235, device='cuda:0', grad_fn=<AddBackward0>)
0.73761660
tensor(0.2217, device='cuda:0', grad_fn=<AddBackward0>)
0.73772007
tensor(0.2874, device='cuda:0', grad_fn=<AddBackward0>)
0.73775226
tensor(0.2521, device='cuda:0', grad_fn=<AddBackward0>)
0.73745072
tensor(0.2671, device='cuda:0', grad_fn=<AddBackward0>)
0.73762053
tensor(0.2905, device='cuda:0', grad_fn=<AddBackward0>)
0.73802239
tensor(0.2966, device='cuda:0', grad_fn=<AddBackward0>)
0.73776352
tensor(0.4004, device='cuda:0', grad_fn=<AddBackward0>)
0.73792726
tensor(0.2931, device='cuda:0', grad_fn=<AddBackward0>)
0.73825377
tensor(0.2262, device='cuda:0', grad_fn=<AddBackward0>)
0.73827416
tensor(0.3443, device='cuda:0', grad_fn=<AddBackward0>)
0.73832381
tensor(0.3122, device='cuda:0', grad_fn=<AddBackward0>)
0.73816323
tensor(0.2694, device='cuda:0', grad_fn=<AddBackward0>)
0.73801792
tensor(0.2925, device='cuda:0', grad_fn=<AddBackward0>)
0.73802453
tensor(0.3074, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][   20/  196]   Loss 0.304490   Top1 89.746094   Top5 99.707031   BatchTime 0.379682   LR 0.000197
0.73816758
tensor(0.3186, device='cuda:0', grad_fn=<AddBackward0>)
0.73830181
tensor(0.2857, device='cuda:0', grad_fn=<AddBackward0>)
0.73843002
tensor(0.3542, device='cuda:0', grad_fn=<AddBackward0>)
0.73814577
tensor(0.3041, device='cuda:0', grad_fn=<AddBackward0>)
0.73810112
tensor(0.2953, device='cuda:0', grad_fn=<AddBackward0>)
0.73831761
tensor(0.3034, device='cuda:0', grad_fn=<AddBackward0>)
0.73794806
tensor(0.2936, device='cuda:0', grad_fn=<AddBackward0>)
0.73762703
tensor(0.3248, device='cuda:0', grad_fn=<AddBackward0>)
0.73734897
tensor(0.3258, device='cuda:0', grad_fn=<AddBackward0>)
0.73718095
tensor(0.3133, device='cuda:0', grad_fn=<AddBackward0>)
0.73709667
tensor(0.4826, device='cuda:0', grad_fn=<AddBackward0>)
0.73693120
tensor(0.2629, device='cuda:0', grad_fn=<AddBackward0>)
0.73682028
tensor(0.3302, device='cuda:0', grad_fn=<AddBackward0>)
0.73684263
tensor(0.2720, device='cuda:0', grad_fn=<AddBackward0>)
0.73676246
tensor(0.4176, device='cuda:0', grad_fn=<AddBackward0>)
0.73686790
tensor(0.3451, device='cuda:0', grad_fn=<AddBackward0>)
0.73687923
tensor(0.3058, device='cuda:0', grad_fn=<AddBackward0>)
0.73672712
tensor(0.2863, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][   40/  196]   Loss 0.312351   Top1 89.414062   Top5 99.667969   BatchTime 0.356333   LR 0.000195
0.73666590
tensor(0.2487, device='cuda:0', grad_fn=<AddBackward0>)
0.73657012
tensor(0.2429, device='cuda:0', grad_fn=<AddBackward0>)
0.73680162
tensor(0.3263, device='cuda:0', grad_fn=<AddBackward0>)
0.73654222
tensor(0.2918, device='cuda:0', grad_fn=<AddBackward0>)
0.73625052
tensor(0.2626, device='cuda:0', grad_fn=<AddBackward0>)
0.73629004
tensor(0.3290, device='cuda:0', grad_fn=<AddBackward0>)
0.73638886
tensor(0.4094, device='cuda:0', grad_fn=<AddBackward0>)
0.73643261
tensor(0.2469, device='cuda:0', grad_fn=<AddBackward0>)
0.73655856
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
0.73653930
tensor(0.3387, device='cuda:0', grad_fn=<AddBackward0>)
0.73632061
tensor(0.2102, device='cuda:0', grad_fn=<AddBackward0>)
0.73619890
tensor(0.3070, device='cuda:0', grad_fn=<AddBackward0>)
0.73617893
tensor(0.2318, device='cuda:0', grad_fn=<AddBackward0>)
0.73620135
tensor(0.3033, device='cuda:0', grad_fn=<AddBackward0>)
0.73624957
tensor(0.3462, device='cuda:0', grad_fn=<AddBackward0>)
0.73593235
tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)
0.73582035
tensor(0.1967, device='cuda:0', grad_fn=<AddBackward0>)
0.73557830
tensor(0.3640, device='cuda:0', grad_fn=<AddBackward0>)
0.73554420
tensor(0.2385, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][   60/  196]   Loss 0.301649   Top1 89.772135   Top5 99.726562   BatchTime 0.344364   LR 0.000194
0.73539019
tensor(0.3297, device='cuda:0', grad_fn=<AddBackward0>)
0.73546565
tensor(0.3144, device='cuda:0', grad_fn=<AddBackward0>)
0.73522377
tensor(0.3314, device='cuda:0', grad_fn=<AddBackward0>)
0.73531532
tensor(0.2648, device='cuda:0', grad_fn=<AddBackward0>)
0.73527706
tensor(0.3343, device='cuda:0', grad_fn=<AddBackward0>)
0.73508722
tensor(0.2699, device='cuda:0', grad_fn=<AddBackward0>)
0.73502141
tensor(0.3244, device='cuda:0', grad_fn=<AddBackward0>)
0.73513848
tensor(0.3018, device='cuda:0', grad_fn=<AddBackward0>)
0.73506027
tensor(0.2305, device='cuda:0', grad_fn=<AddBackward0>)
0.73492825
tensor(0.2515, device='cuda:0', grad_fn=<AddBackward0>)
0.73482770
tensor(0.3422, device='cuda:0', grad_fn=<AddBackward0>)
0.73492432
tensor(0.2754, device='cuda:0', grad_fn=<AddBackward0>)
0.73490041
tensor(0.3509, device='cuda:0', grad_fn=<AddBackward0>)
0.73472500
tensor(0.3245, device='cuda:0', grad_fn=<AddBackward0>)
0.73485923
tensor(0.3262, device='cuda:0', grad_fn=<AddBackward0>)
0.73473030
tensor(0.2805, device='cuda:0', grad_fn=<AddBackward0>)
0.73476928
tensor(0.3016, device='cuda:0', grad_fn=<AddBackward0>)
0.73459953
tensor(0.3238, device='cuda:0', grad_fn=<AddBackward0>)
0.73477566
tensor(0.2962, device='cuda:0', grad_fn=<AddBackward0>)
0.73434246
tensor(0.3856, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][   80/  196]   Loss 0.303233   Top1 89.829102   Top5 99.711914   BatchTime 0.334703   LR 0.000192
0.73462254
tensor(0.2535, device='cuda:0', grad_fn=<AddBackward0>)
0.73446882
tensor(0.3628, device='cuda:0', grad_fn=<AddBackward0>)
0.73446918
tensor(0.3729, device='cuda:0', grad_fn=<AddBackward0>)
0.73432755
tensor(0.3321, device='cuda:0', grad_fn=<AddBackward0>)
0.73413038
tensor(0.3005, device='cuda:0', grad_fn=<AddBackward0>)
0.73395842
tensor(0.3035, device='cuda:0', grad_fn=<AddBackward0>)
0.73383665
tensor(0.2837, device='cuda:0', grad_fn=<AddBackward0>)
0.73398238
tensor(0.2093, device='cuda:0', grad_fn=<AddBackward0>)
0.73414272
tensor(0.3445, device='cuda:0', grad_fn=<AddBackward0>)
0.73420221
tensor(0.2745, device='cuda:0', grad_fn=<AddBackward0>)
0.73434269
tensor(0.3397, device='cuda:0', grad_fn=<AddBackward0>)
0.73414773
tensor(0.3055, device='cuda:0', grad_fn=<AddBackward0>)
0.73415631
tensor(0.3739, device='cuda:0', grad_fn=<AddBackward0>)
0.73418957
tensor(0.2064, device='cuda:0', grad_fn=<AddBackward0>)
0.73421735
tensor(0.3497, device='cuda:0', grad_fn=<AddBackward0>)
0.73412108
tensor(0.2312, device='cuda:0', grad_fn=<AddBackward0>)
0.73411876
tensor(0.2423, device='cuda:0', grad_fn=<AddBackward0>)
0.73416120
tensor(0.2836, device='cuda:0', grad_fn=<AddBackward0>)
0.73421162
tensor(0.2767, device='cuda:0', grad_fn=<AddBackward0>)
0.73419356
tensor(0.3052, device='cuda:0', grad_fn=<AddBackward0>)
0.73421115
tensor(0.3262, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][  100/  196]   Loss 0.302101   Top1 89.867188   Top5 99.742188   BatchTime 0.321783   LR 0.000190
0.73435116
tensor(0.2468, device='cuda:0', grad_fn=<AddBackward0>)
0.73426205
tensor(0.3627, device='cuda:0', grad_fn=<AddBackward0>)
0.73423135
tensor(0.3487, device='cuda:0', grad_fn=<AddBackward0>)
0.73419672
tensor(0.2552, device='cuda:0', grad_fn=<AddBackward0>)
0.73408157
tensor(0.2568, device='cuda:0', grad_fn=<AddBackward0>)
0.73354030
tensor(0.3459, device='cuda:0', grad_fn=<AddBackward0>)
0.73328257
tensor(0.2337, device='cuda:0', grad_fn=<AddBackward0>)
0.73322946
tensor(0.2601, device='cuda:0', grad_fn=<AddBackward0>)
0.73299932
tensor(0.2861, device='cuda:0', grad_fn=<AddBackward0>)
0.73283893
tensor(0.3655, device='cuda:0', grad_fn=<AddBackward0>)
0.73264533
tensor(0.3178, device='cuda:0', grad_fn=<AddBackward0>)
0.73225749
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
0.73232269
tensor(0.3300, device='cuda:0', grad_fn=<AddBackward0>)
0.73267823
tensor(0.3328, device='cuda:0', grad_fn=<AddBackward0>)
0.73254204
tensor(0.4020, device='cuda:0', grad_fn=<AddBackward0>)
0.73219639
tensor(0.2853, device='cuda:0', grad_fn=<AddBackward0>)
0.73174375
tensor(0.3894, device='cuda:0', grad_fn=<AddBackward0>)
0.73159266
tensor(0.3326, device='cuda:0', grad_fn=<AddBackward0>)
0.73163199
tensor(0.3153, device='cuda:0', grad_fn=<AddBackward0>)
0.73152310
tensor(0.2502, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][  120/  196]   Loss 0.303336   Top1 89.791667   Top5 99.742839   BatchTime 0.319920   LR 0.000188
0.73145920
tensor(0.2653, device='cuda:0', grad_fn=<AddBackward0>)
0.73117775
tensor(0.2594, device='cuda:0', grad_fn=<AddBackward0>)
0.73070967
tensor(0.2795, device='cuda:0', grad_fn=<AddBackward0>)
0.73026311
tensor(0.3627, device='cuda:0', grad_fn=<AddBackward0>)
0.72960812
tensor(0.2857, device='cuda:0', grad_fn=<AddBackward0>)
0.72893018
tensor(0.4712, device='cuda:0', grad_fn=<AddBackward0>)
0.72801173
tensor(0.2648, device='cuda:0', grad_fn=<AddBackward0>)
0.72689497
tensor(0.3126, device='cuda:0', grad_fn=<AddBackward0>)
0.72633946
tensor(0.3284, device='cuda:0', grad_fn=<AddBackward0>)
0.72571921
tensor(0.3121, device='cuda:0', grad_fn=<AddBackward0>)
0.72530872
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.72522414
tensor(0.2458, device='cuda:0', grad_fn=<AddBackward0>)
0.72466594
tensor(0.3782, device='cuda:0', grad_fn=<AddBackward0>)
0.72403550
tensor(0.2848, device='cuda:0', grad_fn=<AddBackward0>)
0.72342026
tensor(0.3024, device='cuda:0', grad_fn=<AddBackward0>)
0.72214305
tensor(0.3381, device='cuda:0', grad_fn=<AddBackward0>)
0.72107482
tensor(0.2868, device='cuda:0', grad_fn=<AddBackward0>)
0.72003412
tensor(0.3243, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][  140/  196]   Loss 0.303950   Top1 89.785156   Top5 99.743304   BatchTime 0.320489   LR 0.000187
0.71928674
tensor(0.3078, device='cuda:0', grad_fn=<AddBackward0>)
0.71827275
tensor(0.2476, device='cuda:0', grad_fn=<AddBackward0>)
0.71734917
tensor(0.2978, device='cuda:0', grad_fn=<AddBackward0>)
0.71628827
tensor(0.2838, device='cuda:0', grad_fn=<AddBackward0>)
0.71537423
tensor(0.3530, device='cuda:0', grad_fn=<AddBackward0>)
0.71477282
tensor(0.3547, device='cuda:0', grad_fn=<AddBackward0>)
0.71373063
tensor(0.2945, device='cuda:0', grad_fn=<AddBackward0>)
0.71296358
tensor(0.3319, device='cuda:0', grad_fn=<AddBackward0>)
0.71212965
tensor(0.3558, device='cuda:0', grad_fn=<AddBackward0>)
0.71116447
tensor(0.2920, device='cuda:0', grad_fn=<AddBackward0>)
0.71053392
tensor(0.3272, device='cuda:0', grad_fn=<AddBackward0>)
0.71000612
tensor(0.2840, device='cuda:0', grad_fn=<AddBackward0>)
0.70984560
tensor(0.3177, device='cuda:0', grad_fn=<AddBackward0>)
0.70925099
tensor(0.2921, device='cuda:0', grad_fn=<AddBackward0>)
0.70896733
tensor(0.2800, device='cuda:0', grad_fn=<AddBackward0>)
0.70856339
tensor(0.3344, device='cuda:0', grad_fn=<AddBackward0>)
0.70839268
tensor(0.3540, device='cuda:0', grad_fn=<AddBackward0>)
0.70791209
tensor(0.3390, device='cuda:0', grad_fn=<AddBackward0>)
0.70773888
tensor(0.2468, device='cuda:0', grad_fn=<AddBackward0>)
0.70730233
tensor(0.3095, device='cuda:0', grad_fn=<AddBackward0>)
0.70701993
tensor(0.3639, device='cuda:0', grad_fn=<AddBackward0>)
0.70689118
tensor(0.3506, device='cuda:0', grad_fn=<AddBackward0>)
0.70665526
tensor(0.2990, device='cuda:0', grad_fn=<AddBackward0>)
0.70667791
tensor(0.2198, device='cuda:0', grad_fn=<AddBackward0>)
0.70650113
tensor(0.3300, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][  160/  196]   Loss 0.305081   Top1 89.743652   Top5 99.738770   BatchTime 0.321063   LR 0.000185
0.70638311
tensor(0.3159, device='cuda:0', grad_fn=<AddBackward0>)
0.70621973
tensor(0.2579, device='cuda:0', grad_fn=<AddBackward0>)
0.70647752
tensor(0.3021, device='cuda:0', grad_fn=<AddBackward0>)
0.70594734
tensor(0.2880, device='cuda:0', grad_fn=<AddBackward0>)
0.70564473
tensor(0.2738, device='cuda:0', grad_fn=<AddBackward0>)
0.70584524
tensor(0.2365, device='cuda:0', grad_fn=<AddBackward0>)
0.70597184
tensor(0.3333, device='cuda:0', grad_fn=<AddBackward0>)
0.70597380
tensor(0.2499, device='cuda:0', grad_fn=<AddBackward0>)
0.70562905
tensor(0.3213, device='cuda:0', grad_fn=<AddBackward0>)
0.70559794
tensor(0.2263, device='cuda:0', grad_fn=<AddBackward0>)
0.70552230
tensor(0.3249, device='cuda:0', grad_fn=<AddBackward0>)
0.70536101
tensor(0.3030, device='cuda:0', grad_fn=<AddBackward0>)
0.70552826
tensor(0.3280, device='cuda:0', grad_fn=<AddBackward0>)
0.70558012
tensor(0.4089, device='cuda:0', grad_fn=<AddBackward0>)
0.70571750
tensor(0.2977, device='cuda:0', grad_fn=<AddBackward0>)
0.70584464
tensor(0.3549, device='cuda:0', grad_fn=<AddBackward0>)
0.70581704
tensor(0.3448, device='cuda:0', grad_fn=<AddBackward0>)
0.70599782
tensor(0.3177, device='cuda:0', grad_fn=<AddBackward0>)
0.70597512
tensor(0.2828, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [16][  180/  196]   Loss 0.304636   Top1 89.780816   Top5 99.733073   BatchTime 0.320743   LR 0.000183
0.70609653
tensor(0.3331, device='cuda:0', grad_fn=<AddBackward0>)
0.70597100
tensor(0.2839, device='cuda:0', grad_fn=<AddBackward0>)
0.70602804
tensor(0.4436, device='cuda:0', grad_fn=<AddBackward0>)
0.70600331
tensor(0.3003, device='cuda:0', grad_fn=<AddBackward0>)
0.70609915
tensor(0.3378, device='cuda:0', grad_fn=<AddBackward0>)
0.70612121
tensor(0.3896, device='cuda:0', grad_fn=<AddBackward0>)
0.70667630
tensor(0.3369, device='cuda:0', grad_fn=<AddBackward0>)
0.70738608
tensor(0.3583, device='cuda:0', grad_fn=<AddBackward0>)
0.70844012
tensor(0.2745, device='cuda:0', grad_fn=<AddBackward0>)
0.70975071
tensor(0.2950, device='cuda:0', grad_fn=<AddBackward0>)
0.71132356
tensor(0.3039, device='cuda:0', grad_fn=<AddBackward0>)
0.71337879
tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)
0.71571887
tensor(0.3352, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 89.730    Top5: 99.724    Loss: 0.306
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [16][   20/   40]   Loss 0.462882   Top1 85.117188   Top5 99.375000   BatchTime 0.121619
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0942)
features.2.conv.0 tensor(0.1079)
features.2.conv.3 tensor(0.3565)
features.2.conv.6 tensor(0.5367)
features.3.conv.0 tensor(0.0833)
features.3.conv.3 tensor(0.0880)
features.3.conv.6 tensor(0.1057)
features.4.conv.0 tensor(0.0535)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1838)
features.5.conv.0 tensor(0.3141)
features.5.conv.3 tensor(0.4230)
features.5.conv.6 tensor(0.1030)
features.6.conv.0 tensor(0.0503)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0898)
features.7.conv.0 tensor(0.1564)
features.7.conv.3 tensor(0.4653)
features.7.conv.6 tensor(0.1960)
features.8.conv.0 tensor(0.5502)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.1623)
features.9.conv.0 tensor(0.4742)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.1297)
features.10.conv.0 tensor(0.0654)
features.10.conv.3 tensor(0.1047)
features.10.conv.6 tensor(0.0980)
features.11.conv.0 tensor(0.7336)
features.11.conv.3 tensor(0.6454)
features.11.conv.6 tensor(0.1834)
features.12.conv.0 tensor(0.7075)
features.12.conv.3 tensor(0.6734)
features.12.conv.6 tensor(0.4248)
features.13.conv.0 tensor(0.2101)
features.13.conv.3 tensor(0.4952)
features.13.conv.6 tensor(0.0890)
features.14.conv.0 tensor(0.8779)
features.14.conv.3 tensor(0.8282)
features.14.conv.6 tensor(0.9499)
features.15.conv.0 tensor(0.8462)
features.15.conv.3 tensor(0.8366)
features.15.conv.6 tensor(0.9524)
features.16.conv.0 tensor(0.6245)
features.16.conv.3 tensor(0.8073)
features.16.conv.6 tensor(0.5150)
conv.0 tensor(0.0927)
tensor(1075931.) 2188896.0
INFO - Validation [16][   40/   40]   Loss 0.450932   Top1 85.370000   Top5 99.420000   BatchTime 0.086818
INFO - ==> Top1: 85.370    Top5: 99.420    Loss: 0.451
INFO - ==> Sparsity : 0.492
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
0.71898842
tensor(0.2688, device='cuda:0', grad_fn=<AddBackward0>)
0.72226870
tensor(0.2729, device='cuda:0', grad_fn=<AddBackward0>)
0.72438389
tensor(0.3133, device='cuda:0', grad_fn=<AddBackward0>)
0.72481549
tensor(0.2628, device='cuda:0', grad_fn=<AddBackward0>)
0.72493792
tensor(0.2736, device='cuda:0', grad_fn=<AddBackward0>)
0.72532421
tensor(0.3146, device='cuda:0', grad_fn=<AddBackward0>)
0.72590917
tensor(0.3239, device='cuda:0', grad_fn=<AddBackward0>)
0.72670776
tensor(0.2451, device='cuda:0', grad_fn=<AddBackward0>)
0.72805399
tensor(0.2479, device='cuda:0', grad_fn=<AddBackward0>)
0.72916883
tensor(0.3458, device='cuda:0', grad_fn=<AddBackward0>)
0.73032528
tensor(0.3501, device='cuda:0', grad_fn=<AddBackward0>)
0.73236775
tensor(0.2765, device='cuda:0', grad_fn=<AddBackward0>)
0.73467863
tensor(0.3397, device='cuda:0', grad_fn=<AddBackward0>)
0.73672831
tensor(0.2076, device='cuda:0', grad_fn=<AddBackward0>)
0.73834991
tensor(0.3200, device='cuda:0', grad_fn=<AddBackward0>)
0.73870951
tensor(0.2767, device='cuda:0', grad_fn=<AddBackward0>)
0.73896909
tensor(0.3003, device='cuda:0', grad_fn=<AddBackward0>)
0.73893404
tensor(0.3286, device='cuda:0', grad_fn=<AddBackward0>)
0.73892438
tensor(0.2979, device='cuda:0', grad_fn=<AddBackward0>)
0.73907518
tensor(0.2894, device='cuda:0', grad_fn=<AddBackward0>)
0.73904437
tensor(0.3035, device='cuda:0', grad_fn=<AddBackward0>)
0.73909914
tensor(0.2585, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][   20/  196]   Loss 0.292772   Top1 90.351562   Top5 99.785156   BatchTime 0.320207   LR 0.000180
0.73929983
tensor(0.2584, device='cuda:0', grad_fn=<AddBackward0>)
0.73978919
tensor(0.2715, device='cuda:0', grad_fn=<AddBackward0>)
0.73967320
tensor(0.3569, device='cuda:0', grad_fn=<AddBackward0>)
0.73951524
tensor(0.3402, device='cuda:0', grad_fn=<AddBackward0>)
0.73953563
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
0.73935455
tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)
0.73901886
tensor(0.2910, device='cuda:0', grad_fn=<AddBackward0>)
0.73886412
tensor(0.2801, device='cuda:0', grad_fn=<AddBackward0>)
0.73886329
tensor(0.2911, device='cuda:0', grad_fn=<AddBackward0>)
0.73866689
tensor(0.3008, device='cuda:0', grad_fn=<AddBackward0>)
0.73847520
tensor(0.3732, device='cuda:0', grad_fn=<AddBackward0>)
0.73843932
tensor(0.2687, device='cuda:0', grad_fn=<AddBackward0>)
0.73902386
tensor(0.3281, device='cuda:0', grad_fn=<AddBackward0>)
0.73881876
tensor(0.2835, device='cuda:0', grad_fn=<AddBackward0>)
0.73869675
tensor(0.2468, device='cuda:0', grad_fn=<AddBackward0>)
0.73844451
tensor(0.2289, device='cuda:0', grad_fn=<AddBackward0>)
0.73827118
tensor(0.3047, device='cuda:0', grad_fn=<AddBackward0>)
0.73817629
tensor(0.3318, device='cuda:0', grad_fn=<AddBackward0>)
0.73826265
tensor(0.2953, device='cuda:0', grad_fn=<AddBackward0>)
0.73793769
tensor(0.3038, device='cuda:0', grad_fn=<AddBackward0>)
0.73769093
tensor(0.3698, device='cuda:0', grad_fn=<AddBackward0>)
0.73754334
tensor(0.3763, device='cuda:0', grad_fn=<AddBackward0>)
0.73740345
tensor(0.2932, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][   40/  196]   Loss 0.291420   Top1 90.253906   Top5 99.794922   BatchTime 0.285106   LR 0.000178
0.73715115
tensor(0.2802, device='cuda:0', grad_fn=<AddBackward0>)
0.73721695
tensor(0.3103, device='cuda:0', grad_fn=<AddBackward0>)
0.73723704
tensor(0.3337, device='cuda:0', grad_fn=<AddBackward0>)
0.73752534
tensor(0.2724, device='cuda:0', grad_fn=<AddBackward0>)
0.73771340
tensor(0.2484, device='cuda:0', grad_fn=<AddBackward0>)
0.73776931
tensor(0.2850, device='cuda:0', grad_fn=<AddBackward0>)
0.73750001
tensor(0.3158, device='cuda:0', grad_fn=<AddBackward0>)
0.73740959
tensor(0.2727, device='cuda:0', grad_fn=<AddBackward0>)
0.73774987
tensor(0.3497, device='cuda:0', grad_fn=<AddBackward0>)
0.73738915
tensor(0.2943, device='cuda:0', grad_fn=<AddBackward0>)
0.73714691
tensor(0.2557, device='cuda:0', grad_fn=<AddBackward0>)
0.73698550
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
0.73706830
tensor(0.2999, device='cuda:0', grad_fn=<AddBackward0>)
0.73709387
tensor(0.2458, device='cuda:0', grad_fn=<AddBackward0>)
0.73718715
tensor(0.2938, device='cuda:0', grad_fn=<AddBackward0>)
0.73711783
tensor(0.2393, device='cuda:0', grad_fn=<AddBackward0>)
0.73708200
tensor(0.2766, device='cuda:0', grad_fn=<AddBackward0>)
0.73685950
tensor(0.2751, device='cuda:0', grad_fn=<AddBackward0>)
0.73646057
tensor(0.3318, device='cuda:0', grad_fn=<AddBackward0>)
0.73615289
tensor(0.2457, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][   60/  196]   Loss 0.292691   Top1 90.182292   Top5 99.765625   BatchTime 0.290595   LR 0.000176
0.73592192
tensor(0.3106, device='cuda:0', grad_fn=<AddBackward0>)
0.73563540
tensor(0.3100, device='cuda:0', grad_fn=<AddBackward0>)
0.73531997
tensor(0.2064, device='cuda:0', grad_fn=<AddBackward0>)
0.73526448
tensor(0.2311, device='cuda:0', grad_fn=<AddBackward0>)
0.73518711
tensor(0.2855, device='cuda:0', grad_fn=<AddBackward0>)
0.73501414
tensor(0.2781, device='cuda:0', grad_fn=<AddBackward0>)
0.73482263
tensor(0.3062, device='cuda:0', grad_fn=<AddBackward0>)
0.73481494
tensor(0.2102, device='cuda:0', grad_fn=<AddBackward0>)
0.73487234
tensor(0.2193, device='cuda:0', grad_fn=<AddBackward0>)
0.73504871
tensor(0.2793, device='cuda:0', grad_fn=<AddBackward0>)
0.73483723
tensor(0.2787, device='cuda:0', grad_fn=<AddBackward0>)
0.73467088
tensor(0.2814, device='cuda:0', grad_fn=<AddBackward0>)
0.73478341
tensor(0.4023, device='cuda:0', grad_fn=<AddBackward0>)
0.73456788
tensor(0.2614, device='cuda:0', grad_fn=<AddBackward0>)
0.73428321
tensor(0.3554, device='cuda:0', grad_fn=<AddBackward0>)
0.73402554
tensor(0.3180, device='cuda:0', grad_fn=<AddBackward0>)
0.73384959
tensor(0.2724, device='cuda:0', grad_fn=<AddBackward0>)
0.73359543
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][   80/  196]   Loss 0.289324   Top1 90.351562   Top5 99.741211   BatchTime 0.299144   LR 0.000175
0.73356080
tensor(0.2602, device='cuda:0', grad_fn=<AddBackward0>)
0.73349392
tensor(0.3028, device='cuda:0', grad_fn=<AddBackward0>)
0.73353744
tensor(0.2901, device='cuda:0', grad_fn=<AddBackward0>)
0.73322958
tensor(0.2516, device='cuda:0', grad_fn=<AddBackward0>)
0.73296613
tensor(0.3415, device='cuda:0', grad_fn=<AddBackward0>)
0.73285443
tensor(0.2905, device='cuda:0', grad_fn=<AddBackward0>)
0.73261702
tensor(0.2527, device='cuda:0', grad_fn=<AddBackward0>)
0.73235524
tensor(0.2542, device='cuda:0', grad_fn=<AddBackward0>)
0.73200923
tensor(0.3552, device='cuda:0', grad_fn=<AddBackward0>)
0.73207873
tensor(0.3241, device='cuda:0', grad_fn=<AddBackward0>)
0.73209023
tensor(0.2915, device='cuda:0', grad_fn=<AddBackward0>)
0.73176843
tensor(0.4015, device='cuda:0', grad_fn=<AddBackward0>)
0.73148459
tensor(0.2570, device='cuda:0', grad_fn=<AddBackward0>)
0.73154980
tensor(0.2722, device='cuda:0', grad_fn=<AddBackward0>)
0.73138469
tensor(0.2639, device='cuda:0', grad_fn=<AddBackward0>)
0.73145968
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.73119712
tensor(0.2826, device='cuda:0', grad_fn=<AddBackward0>)
0.73109764
tensor(0.2628, device='cuda:0', grad_fn=<AddBackward0>)
0.73092234
tensor(0.2831, device='cuda:0', grad_fn=<AddBackward0>)
0.73070461
tensor(0.2493, device='cuda:0', grad_fn=<AddBackward0>)
0.73068470
tensor(0.2510, device='cuda:0', grad_fn=<AddBackward0>)
0.73084468
tensor(0.2924, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][  100/  196]   Loss 0.288128   Top1 90.414062   Top5 99.742188   BatchTime 0.296527   LR 0.000173
0.73107362
tensor(0.3310, device='cuda:0', grad_fn=<AddBackward0>)
0.73118311
tensor(0.2585, device='cuda:0', grad_fn=<AddBackward0>)
0.73096323
tensor(0.2596, device='cuda:0', grad_fn=<AddBackward0>)
0.73108572
tensor(0.2915, device='cuda:0', grad_fn=<AddBackward0>)
0.73096251
tensor(0.3201, device='cuda:0', grad_fn=<AddBackward0>)
0.73083878
tensor(0.3383, device='cuda:0', grad_fn=<AddBackward0>)
0.73079777
tensor(0.3098, device='cuda:0', grad_fn=<AddBackward0>)
0.73058665
tensor(0.3279, device='cuda:0', grad_fn=<AddBackward0>)
0.73041499
tensor(0.2776, device='cuda:0', grad_fn=<AddBackward0>)
0.73035330
tensor(0.4317, device='cuda:0', grad_fn=<AddBackward0>)
0.73030382
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.73037559
tensor(0.3552, device='cuda:0', grad_fn=<AddBackward0>)
0.73021150
tensor(0.2372, device='cuda:0', grad_fn=<AddBackward0>)
0.72994578
tensor(0.2787, device='cuda:0', grad_fn=<AddBackward0>)
0.73007119
tensor(0.3183, device='cuda:0', grad_fn=<AddBackward0>)
0.73019344
tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)
0.73037922
tensor(0.3270, device='cuda:0', grad_fn=<AddBackward0>)
0.73051411
tensor(0.3063, device='cuda:0', grad_fn=<AddBackward0>)
0.73071474
tensor(0.2955, device='cuda:0', grad_fn=<AddBackward0>)
0.73065555
tensor(0.2683, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][  120/  196]   Loss 0.289828   Top1 90.266927   Top5 99.749349   BatchTime 0.296532   LR 0.000171
0.73058927
tensor(0.2981, device='cuda:0', grad_fn=<AddBackward0>)
0.73042160
tensor(0.2975, device='cuda:0', grad_fn=<AddBackward0>)
0.73014426
tensor(0.2377, device='cuda:0', grad_fn=<AddBackward0>)
0.73007292
tensor(0.3595, device='cuda:0', grad_fn=<AddBackward0>)
0.72979510
tensor(0.3045, device='cuda:0', grad_fn=<AddBackward0>)
0.72980535
tensor(0.3792, device='cuda:0', grad_fn=<AddBackward0>)
0.72982144
tensor(0.2386, device='cuda:0', grad_fn=<AddBackward0>)
0.72955155
tensor(0.3640, device='cuda:0', grad_fn=<AddBackward0>)
0.72945642
tensor(0.3217, device='cuda:0', grad_fn=<AddBackward0>)
0.72923535
tensor(0.2432, device='cuda:0', grad_fn=<AddBackward0>)
0.72917634
tensor(0.3376, device='cuda:0', grad_fn=<AddBackward0>)
0.72909212
tensor(0.2960, device='cuda:0', grad_fn=<AddBackward0>)
0.72880608
tensor(0.4113, device='cuda:0', grad_fn=<AddBackward0>)
0.72885478
tensor(0.2969, device='cuda:0', grad_fn=<AddBackward0>)
0.72858936
tensor(0.3045, device='cuda:0', grad_fn=<AddBackward0>)
0.72866297
tensor(0.4276, device='cuda:0', grad_fn=<AddBackward0>)
0.72849590
tensor(0.3145, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][  140/  196]   Loss 0.292158   Top1 90.178571   Top5 99.757254   BatchTime 0.301674   LR 0.000169
0.72870153
tensor(0.4100, device='cuda:0', grad_fn=<AddBackward0>)
0.72837830
tensor(0.2363, device='cuda:0', grad_fn=<AddBackward0>)
0.72827142
tensor(0.2969, device='cuda:0', grad_fn=<AddBackward0>)
0.72812867
tensor(0.3236, device='cuda:0', grad_fn=<AddBackward0>)
0.72803611
tensor(0.3845, device='cuda:0', grad_fn=<AddBackward0>)
0.72805262
tensor(0.3239, device='cuda:0', grad_fn=<AddBackward0>)
0.72814155
tensor(0.2711, device='cuda:0', grad_fn=<AddBackward0>)
0.72847366
tensor(0.3123, device='cuda:0', grad_fn=<AddBackward0>)
0.72814047
tensor(0.2410, device='cuda:0', grad_fn=<AddBackward0>)
0.72805381
tensor(0.2682, device='cuda:0', grad_fn=<AddBackward0>)
0.72799826
tensor(0.2918, device='cuda:0', grad_fn=<AddBackward0>)
0.72781318
tensor(0.2759, device='cuda:0', grad_fn=<AddBackward0>)
0.72758412
tensor(0.2428, device='cuda:0', grad_fn=<AddBackward0>)
0.72781444
tensor(0.2985, device='cuda:0', grad_fn=<AddBackward0>)
0.72779375
tensor(0.3703, device='cuda:0', grad_fn=<AddBackward0>)
0.72785473
tensor(0.2512, device='cuda:0', grad_fn=<AddBackward0>)
0.72768551
tensor(0.3641, device='cuda:0', grad_fn=<AddBackward0>)
0.72781980
tensor(0.2873, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][  160/  196]   Loss 0.294336   Top1 90.117188   Top5 99.736328   BatchTime 0.305913   LR 0.000167
0.72757536
tensor(0.2760, device='cuda:0', grad_fn=<AddBackward0>)
0.72758842
tensor(0.3518, device='cuda:0', grad_fn=<AddBackward0>)
0.72744775
tensor(0.3290, device='cuda:0', grad_fn=<AddBackward0>)
0.72722697
tensor(0.4004, device='cuda:0', grad_fn=<AddBackward0>)
0.72728795
tensor(0.3030, device='cuda:0', grad_fn=<AddBackward0>)
0.72710955
tensor(0.3364, device='cuda:0', grad_fn=<AddBackward0>)
0.72684228
tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)
0.72681004
tensor(0.2417, device='cuda:0', grad_fn=<AddBackward0>)
0.72695619
tensor(0.3157, device='cuda:0', grad_fn=<AddBackward0>)
0.72680390
tensor(0.2817, device='cuda:0', grad_fn=<AddBackward0>)
0.72700715
tensor(0.2452, device='cuda:0', grad_fn=<AddBackward0>)
0.72682548
tensor(0.3735, device='cuda:0', grad_fn=<AddBackward0>)
0.72688150
tensor(0.2495, device='cuda:0', grad_fn=<AddBackward0>)
0.72700208
tensor(0.2990, device='cuda:0', grad_fn=<AddBackward0>)
0.72685951
tensor(0.3200, device='cuda:0', grad_fn=<AddBackward0>)
0.72704464
tensor(0.2431, device='cuda:0', grad_fn=<AddBackward0>)
0.72686112
tensor(0.2942, device='cuda:0', grad_fn=<AddBackward0>)
0.72659415
tensor(0.3265, device='cuda:0', grad_fn=<AddBackward0>)
0.72657341
tensor(0.2802, device='cuda:0', grad_fn=<AddBackward0>)
0.72671521
tensor(0.2522, device='cuda:0', grad_fn=<AddBackward0>)
0.72679532
tensor(0.3167, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [17][  180/  196]   Loss 0.294811   Top1 90.108507   Top5 99.737413   BatchTime 0.304453   LR 0.000165
0.72683614
tensor(0.2784, device='cuda:0', grad_fn=<AddBackward0>)
0.72640646
tensor(0.3519, device='cuda:0', grad_fn=<AddBackward0>)
0.72664195
tensor(0.2446, device='cuda:0', grad_fn=<AddBackward0>)
0.72627282
tensor(0.2943, device='cuda:0', grad_fn=<AddBackward0>)
0.72603440
tensor(0.2742, device='cuda:0', grad_fn=<AddBackward0>)
0.72596705
tensor(0.2743, device='cuda:0', grad_fn=<AddBackward0>)
0.72591478
tensor(0.2486, device='cuda:0', grad_fn=<AddBackward0>)
0.72620010
tensor(0.2520, device='cuda:0', grad_fn=<AddBackward0>)
0.72593701
tensor(0.2624, device='cuda:0', grad_fn=<AddBackward0>)
0.72577322
tensor(0.2860, device='cuda:0', grad_fn=<AddBackward0>)
0.72583544
tensor(0.3291, device='cuda:0', grad_fn=<AddBackward0>)
0.72602099
tensor(0.2846, device='cuda:0', grad_fn=<AddBackward0>)
0.72599906
tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)
0.72588354
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
0.72574490
tensor(0.3181, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 90.152    Top5: 99.734    Loss: 0.293
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0929)
features.2.conv.0 tensor(0.1001)
features.2.conv.3 tensor(0.3511)
features.2.conv.6 tensor(0.5437)
features.3.conv.0 tensor(0.0796)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1098)
features.4.conv.0 tensor(0.0604)
features.4.conv.3 tensor(0.3090)
features.4.conv.6 tensor(0.1828)
features.5.conv.0 tensor(0.2887)
features.5.conv.3 tensor(0.4236)
features.5.conv.6 tensor(0.1037)
features.6.conv.0 tensor(0.0490)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0870)
features.7.conv.0 tensor(0.1668)
features.7.conv.3 tensor(0.4635)
features.7.conv.6 tensor(0.1919)
features.8.conv.0 tensor(0.5848)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.1677)
features.9.conv.0 tensor(0.4789)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.1383)
features.10.conv.0 tensor(0.0649)
features.10.conv.3 tensor(0.1082)
features.10.conv.6 tensor(0.0977)
features.11.conv.0 tensor(0.7155)
features.11.conv.3 tensor(0.6451)
features.11.conv.6 tensor(0.2082)
features.12.conv.0 tensor(0.7297)
features.12.conv.3 tensor(0.6723)
features.12.conv.6 tensor(0.4149)
features.13.conv.0 tensor(0.2233)
features.13.conv.3 tensor(0.4932)
features.13.conv.6 tensor(0.0893)
features.14.conv.0 tensor(0.8823)
features.14.conv.3 tensor(0.8281)
features.14.conv.6 tensor(0.9544)
features.15.conv.0 tensor(0.8519)
features.15.conv.3 tensor(0.8360)
features.15.conv.6 tensor(0.9549)
features.16.conv.0 tensor(0.6311)
features.16.conv.3 tensor(0.8068)
features.16.conv.6 tensor(0.3426)
conv.0 tensor(0.0932)
tensor(1029836.) 2188896.0
INFO - Validation [17][   20/   40]   Loss 0.411379   Top1 86.171875   Top5 99.296875   BatchTime 0.122540
INFO - Validation [17][   40/   40]   Loss 0.397580   Top1 86.440000   Top5 99.450000   BatchTime 0.089502
INFO - ==> Top1: 86.440    Top5: 99.450    Loss: 0.398
INFO - ==> Sparsity : 0.470
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.830   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.72573805
tensor(0.2524, device='cuda:0', grad_fn=<AddBackward0>)
0.72574472
tensor(0.3267, device='cuda:0', grad_fn=<AddBackward0>)
0.72568500
tensor(0.2566, device='cuda:0', grad_fn=<AddBackward0>)
0.72549570
tensor(0.3576, device='cuda:0', grad_fn=<AddBackward0>)
0.72539920
tensor(0.2912, device='cuda:0', grad_fn=<AddBackward0>)
0.72535414
tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)
0.72531664
tensor(0.3125, device='cuda:0', grad_fn=<AddBackward0>)
0.72498184
tensor(0.2777, device='cuda:0', grad_fn=<AddBackward0>)
0.72487336
tensor(0.2569, device='cuda:0', grad_fn=<AddBackward0>)
0.72484344
tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)
0.72494805
tensor(0.2159, device='cuda:0', grad_fn=<AddBackward0>)
0.72515279
tensor(0.3094, device='cuda:0', grad_fn=<AddBackward0>)
0.72509360
tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)
0.72508472
tensor(0.2707, device='cuda:0', grad_fn=<AddBackward0>)
0.72507721
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.72506988
tensor(0.3300, device='cuda:0', grad_fn=<AddBackward0>)
0.72513211
tensor(0.2160, device='cuda:0', grad_fn=<AddBackward0>)
0.72523850
tensor(0.2242, device='cuda:0', grad_fn=<AddBackward0>)
0.72485381
tensor(0.2901, device='cuda:0', grad_fn=<AddBackward0>)
0.72462082
tensor(0.2292, device='cuda:0', grad_fn=<AddBackward0>)
0.72396851
tensor(0.3210, device='cuda:0', grad_fn=<AddBackward0>)
0.72378850
tensor(0.2248, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][   20/  196]   Loss 0.262265   Top1 91.464844   Top5 99.824219   BatchTime 0.361460   LR 0.000162
0.72357404
tensor(0.2530, device='cuda:0', grad_fn=<AddBackward0>)
0.72346783
tensor(0.2682, device='cuda:0', grad_fn=<AddBackward0>)
0.72322345
tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)
0.72312611
tensor(0.3245, device='cuda:0', grad_fn=<AddBackward0>)
0.72351193
tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)
0.72353631
tensor(0.3067, device='cuda:0', grad_fn=<AddBackward0>)
0.72344881
tensor(0.1984, device='cuda:0', grad_fn=<AddBackward0>)
0.72332484
tensor(0.3699, device='cuda:0', grad_fn=<AddBackward0>)
0.72336382
tensor(0.2963, device='cuda:0', grad_fn=<AddBackward0>)
0.72328699
tensor(0.2710, device='cuda:0', grad_fn=<AddBackward0>)
0.72353107
tensor(0.2817, device='cuda:0', grad_fn=<AddBackward0>)
0.72332329
tensor(0.3436, device='cuda:0', grad_fn=<AddBackward0>)
0.72307217
tensor(0.3450, device='cuda:0', grad_fn=<AddBackward0>)
0.72297245
tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)
0.72288167
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
0.72281414
tensor(0.2477, device='cuda:0', grad_fn=<AddBackward0>)
0.72293055
tensor(0.3093, device='cuda:0', grad_fn=<AddBackward0>)
0.72347862
tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)
0.72368205
tensor(0.2400, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][   40/  196]   Loss 0.270489   Top1 91.035156   Top5 99.765625   BatchTime 0.343118   LR 0.000160
0.72380501
tensor(0.2405, device='cuda:0', grad_fn=<AddBackward0>)
0.72374916
tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)
0.72373819
tensor(0.2928, device='cuda:0', grad_fn=<AddBackward0>)
0.72346097
tensor(0.3244, device='cuda:0', grad_fn=<AddBackward0>)
0.72368687
tensor(0.1702, device='cuda:0', grad_fn=<AddBackward0>)
0.72394067
tensor(0.3598, device='cuda:0', grad_fn=<AddBackward0>)
0.72382134
tensor(0.3017, device='cuda:0', grad_fn=<AddBackward0>)
0.72363657
tensor(0.2346, device='cuda:0', grad_fn=<AddBackward0>)
0.72343379
tensor(0.2448, device='cuda:0', grad_fn=<AddBackward0>)
0.72331059
tensor(0.3083, device='cuda:0', grad_fn=<AddBackward0>)
0.72321367
tensor(0.2803, device='cuda:0', grad_fn=<AddBackward0>)
0.72311157
tensor(0.2942, device='cuda:0', grad_fn=<AddBackward0>)
0.72325498
tensor(0.3101, device='cuda:0', grad_fn=<AddBackward0>)
0.72329146
tensor(0.2729, device='cuda:0', grad_fn=<AddBackward0>)
0.72338015
tensor(0.3622, device='cuda:0', grad_fn=<AddBackward0>)
0.72330409
tensor(0.2413, device='cuda:0', grad_fn=<AddBackward0>)
0.72364098
tensor(0.2632, device='cuda:0', grad_fn=<AddBackward0>)
0.72345084
tensor(0.2530, device='cuda:0', grad_fn=<AddBackward0>)
0.72300625
tensor(0.3064, device='cuda:0', grad_fn=<AddBackward0>)
0.72273773
tensor(0.2854, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][   60/  196]   Loss 0.272885   Top1 90.937500   Top5 99.785156   BatchTime 0.330808   LR 0.000158
0.72259086
tensor(0.3193, device='cuda:0', grad_fn=<AddBackward0>)
0.72209859
tensor(0.2521, device='cuda:0', grad_fn=<AddBackward0>)
0.72152680
tensor(0.2297, device='cuda:0', grad_fn=<AddBackward0>)
0.72148263
tensor(0.2013, device='cuda:0', grad_fn=<AddBackward0>)
0.72150677
tensor(0.2647, device='cuda:0', grad_fn=<AddBackward0>)
0.72158051
tensor(0.4015, device='cuda:0', grad_fn=<AddBackward0>)
0.72153842
tensor(0.2269, device='cuda:0', grad_fn=<AddBackward0>)
0.72173011
tensor(0.4003, device='cuda:0', grad_fn=<AddBackward0>)
0.72187907
tensor(0.2247, device='cuda:0', grad_fn=<AddBackward0>)
0.72157085
tensor(0.3093, device='cuda:0', grad_fn=<AddBackward0>)
0.72139090
tensor(0.2954, device='cuda:0', grad_fn=<AddBackward0>)
0.72124809
tensor(0.2407, device='cuda:0', grad_fn=<AddBackward0>)
0.72132009
tensor(0.2645, device='cuda:0', grad_fn=<AddBackward0>)
0.72126192
tensor(0.2927, device='cuda:0', grad_fn=<AddBackward0>)
0.72142982
tensor(0.3386, device='cuda:0', grad_fn=<AddBackward0>)
0.72190684
tensor(0.2145, device='cuda:0', grad_fn=<AddBackward0>)
0.72212774
tensor(0.2072, device='cuda:0', grad_fn=<AddBackward0>)
0.72230780
tensor(0.3478, device='cuda:0', grad_fn=<AddBackward0>)
0.72238666
tensor(0.3166, device='cuda:0', grad_fn=<AddBackward0>)
0.72238553
tensor(0.2379, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][   80/  196]   Loss 0.275079   Top1 90.800781   Top5 99.775391   BatchTime 0.321155   LR 0.000156
0.72223240
tensor(0.2376, device='cuda:0', grad_fn=<AddBackward0>)
0.72233808
tensor(0.2801, device='cuda:0', grad_fn=<AddBackward0>)
0.72273099
tensor(0.2972, device='cuda:0', grad_fn=<AddBackward0>)
0.72270054
tensor(0.2707, device='cuda:0', grad_fn=<AddBackward0>)
0.72248662
tensor(0.3284, device='cuda:0', grad_fn=<AddBackward0>)
0.72251385
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.72225517
tensor(0.2638, device='cuda:0', grad_fn=<AddBackward0>)
0.72198296
tensor(0.2992, device='cuda:0', grad_fn=<AddBackward0>)
0.72178841
tensor(0.3237, device='cuda:0', grad_fn=<AddBackward0>)
0.72168130
tensor(0.3126, device='cuda:0', grad_fn=<AddBackward0>)
0.72182423
tensor(0.2951, device='cuda:0', grad_fn=<AddBackward0>)
0.72189385
tensor(0.2755, device='cuda:0', grad_fn=<AddBackward0>)
0.72200358
tensor(0.3228, device='cuda:0', grad_fn=<AddBackward0>)
0.72195762
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
0.72189760
tensor(0.2964, device='cuda:0', grad_fn=<AddBackward0>)
0.72168428
tensor(0.2938, device='cuda:0', grad_fn=<AddBackward0>)
0.72151530
tensor(0.2786, device='cuda:0', grad_fn=<AddBackward0>)
0.72135878
tensor(0.2222, device='cuda:0', grad_fn=<AddBackward0>)
0.72112477
tensor(0.2326, device='cuda:0', grad_fn=<AddBackward0>)
0.72098351
tensor(0.2765, device='cuda:0', grad_fn=<AddBackward0>)
0.72092694
tensor(0.2313, device='cuda:0', grad_fn=<AddBackward0>)
0.72071332
tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)
0.72045070
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
0.72044712
tensor(0.3219, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][  100/  196]   Loss 0.275616   Top1 90.824219   Top5 99.781250   BatchTime 0.323755   LR 0.000154
0.72052091
tensor(0.2430, device='cuda:0', grad_fn=<AddBackward0>)
0.72046638
tensor(0.2102, device='cuda:0', grad_fn=<AddBackward0>)
0.72044522
tensor(0.2359, device='cuda:0', grad_fn=<AddBackward0>)
0.72018689
tensor(0.2211, device='cuda:0', grad_fn=<AddBackward0>)
0.71993524
tensor(0.3033, device='cuda:0', grad_fn=<AddBackward0>)
0.71987772
tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)
0.71966004
tensor(0.2371, device='cuda:0', grad_fn=<AddBackward0>)
0.71961701
tensor(0.2328, device='cuda:0', grad_fn=<AddBackward0>)
0.71964979
tensor(0.2077, device='cuda:0', grad_fn=<AddBackward0>)
0.71941036
tensor(0.3401, device='cuda:0', grad_fn=<AddBackward0>)
0.71955639
tensor(0.2694, device='cuda:0', grad_fn=<AddBackward0>)
0.71932149
tensor(0.3306, device='cuda:0', grad_fn=<AddBackward0>)
0.71914786
tensor(0.3379, device='cuda:0', grad_fn=<AddBackward0>)
0.71885288
tensor(0.2666, device='cuda:0', grad_fn=<AddBackward0>)
0.71875787
tensor(0.2907, device='cuda:0', grad_fn=<AddBackward0>)
0.71886289
tensor(0.2482, device='cuda:0', grad_fn=<AddBackward0>)
0.71868467
tensor(0.3074, device='cuda:0', grad_fn=<AddBackward0>)
0.71863365
tensor(0.3554, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][  120/  196]   Loss 0.272667   Top1 90.917969   Top5 99.788411   BatchTime 0.327007   LR 0.000152
0.71879703
tensor(0.2976, device='cuda:0', grad_fn=<AddBackward0>)
0.71866173
tensor(0.2197, device='cuda:0', grad_fn=<AddBackward0>)
0.71852779
tensor(0.2513, device='cuda:0', grad_fn=<AddBackward0>)
0.71862096
tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)
0.71876496
tensor(0.2997, device='cuda:0', grad_fn=<AddBackward0>)
0.71884060
tensor(0.3122, device='cuda:0', grad_fn=<AddBackward0>)
0.71892446
tensor(0.2393, device='cuda:0', grad_fn=<AddBackward0>)
0.71902233
tensor(0.3164, device='cuda:0', grad_fn=<AddBackward0>)
0.71879709
tensor(0.3221, device='cuda:0', grad_fn=<AddBackward0>)
0.71908772
tensor(0.3402, device='cuda:0', grad_fn=<AddBackward0>)
0.71873891
tensor(0.2333, device='cuda:0', grad_fn=<AddBackward0>)
0.71859205
tensor(0.3629, device='cuda:0', grad_fn=<AddBackward0>)
0.71844721
tensor(0.2207, device='cuda:0', grad_fn=<AddBackward0>)
0.71850556
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.71831548
tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)
0.71824801
tensor(0.2895, device='cuda:0', grad_fn=<AddBackward0>)
0.71829611
tensor(0.2659, device='cuda:0', grad_fn=<AddBackward0>)
0.71827137
tensor(0.3232, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][  140/  196]   Loss 0.273203   Top1 90.876116   Top5 99.796317   BatchTime 0.326294   LR 0.000150
0.71830809
tensor(0.2667, device='cuda:0', grad_fn=<AddBackward0>)
0.71829861
tensor(0.2786, device='cuda:0', grad_fn=<AddBackward0>)
0.71789062
tensor(0.2763, device='cuda:0', grad_fn=<AddBackward0>)
0.71813661
tensor(0.3013, device='cuda:0', grad_fn=<AddBackward0>)
0.71838230
tensor(0.3072, device='cuda:0', grad_fn=<AddBackward0>)
0.71835208
tensor(0.2191, device='cuda:0', grad_fn=<AddBackward0>)
0.71842676
tensor(0.2167, device='cuda:0', grad_fn=<AddBackward0>)
0.71847588
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.71841645
tensor(0.2107, device='cuda:0', grad_fn=<AddBackward0>)
0.71797168
tensor(0.2842, device='cuda:0', grad_fn=<AddBackward0>)
0.71764797
tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)
0.71777171
tensor(0.2522, device='cuda:0', grad_fn=<AddBackward0>)
0.71749276
tensor(0.2542, device='cuda:0', grad_fn=<AddBackward0>)
0.71732563
tensor(0.2753, device='cuda:0', grad_fn=<AddBackward0>)
0.71740788
tensor(0.2780, device='cuda:0', grad_fn=<AddBackward0>)
0.71725678
tensor(0.2755, device='cuda:0', grad_fn=<AddBackward0>)
0.71705604
tensor(0.2454, device='cuda:0', grad_fn=<AddBackward0>)
0.71703619
tensor(0.2934, device='cuda:0', grad_fn=<AddBackward0>)
0.71705765
tensor(0.3258, device='cuda:0', grad_fn=<AddBackward0>)
0.71719652
tensor(0.2719, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][  160/  196]   Loss 0.272180   Top1 90.859375   Top5 99.794922   BatchTime 0.324084   LR 0.000148
0.71751171
tensor(0.2379, device='cuda:0', grad_fn=<AddBackward0>)
0.71726316
tensor(0.3018, device='cuda:0', grad_fn=<AddBackward0>)
0.71740848
tensor(0.2368, device='cuda:0', grad_fn=<AddBackward0>)
0.71755958
tensor(0.2571, device='cuda:0', grad_fn=<AddBackward0>)
0.71745837
tensor(0.3049, device='cuda:0', grad_fn=<AddBackward0>)
0.71743971
tensor(0.3566, device='cuda:0', grad_fn=<AddBackward0>)
0.71707779
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
0.71685940
tensor(0.3583, device='cuda:0', grad_fn=<AddBackward0>)
0.71661234
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
0.71678972
tensor(0.2786, device='cuda:0', grad_fn=<AddBackward0>)
0.71650654
tensor(0.2767, device='cuda:0', grad_fn=<AddBackward0>)
0.71679330
tensor(0.3408, device='cuda:0', grad_fn=<AddBackward0>)
0.71714431
tensor(0.2129, device='cuda:0', grad_fn=<AddBackward0>)
0.71730763
tensor(0.3015, device='cuda:0', grad_fn=<AddBackward0>)
0.71704829
tensor(0.3366, device='cuda:0', grad_fn=<AddBackward0>)
0.71677834
tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)
0.71678400
tensor(0.2814, device='cuda:0', grad_fn=<AddBackward0>)
0.71686918
tensor(0.2469, device='cuda:0', grad_fn=<AddBackward0>)
0.71671826
tensor(0.2866, device='cuda:0', grad_fn=<AddBackward0>)
0.71643323
tensor(0.3594, device='cuda:0', grad_fn=<AddBackward0>)
0.71632880
tensor(0.2626, device='cuda:0', grad_fn=<AddBackward0>)
0.71617103
tensor(0.2866, device='cuda:0', grad_fn=<AddBackward0>)
0.71647298
tensor(0.3176, device='cuda:0', grad_fn=<AddBackward0>)
0.71658063
tensor(0.2866, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [18][  180/  196]   Loss 0.272523   Top1 90.868056   Top5 99.789497   BatchTime 0.325070   LR 0.000146
0.71634012
tensor(0.2591, device='cuda:0', grad_fn=<AddBackward0>)
0.71627527
tensor(0.2701, device='cuda:0', grad_fn=<AddBackward0>)
0.71560204
tensor(0.2762, device='cuda:0', grad_fn=<AddBackward0>)
0.71596473
tensor(0.3187, device='cuda:0', grad_fn=<AddBackward0>)
0.71551460
tensor(0.2520, device='cuda:0', grad_fn=<AddBackward0>)
0.71518630
tensor(0.2889, device='cuda:0', grad_fn=<AddBackward0>)
0.71521735
tensor(0.2531, device='cuda:0', grad_fn=<AddBackward0>)
0.71548206
tensor(0.2689, device='cuda:0', grad_fn=<AddBackward0>)
0.71558732
tensor(0.2326, device='cuda:0', grad_fn=<AddBackward0>)
0.71548551
tensor(0.2269, device='cuda:0', grad_fn=<AddBackward0>)
0.71566957
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 90.858    Top5: 99.786    Loss: 0.273
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 0.382902   Top1 87.031250   Top5 99.414062   BatchTime 0.114507
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0998)
features.2.conv.0 tensor(0.1256)
features.2.conv.3 tensor(0.3549)
features.2.conv.6 tensor(0.5408)
features.3.conv.0 tensor(0.0807)
features.3.conv.3 tensor(0.0864)
features.3.conv.6 tensor(0.1122)
features.4.conv.0 tensor(0.0566)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1820)
features.5.conv.0 tensor(0.3062)
features.5.conv.3 tensor(0.4219)
features.5.conv.6 tensor(0.1034)
features.6.conv.0 tensor(0.0496)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0855)
features.7.conv.0 tensor(0.1626)
features.7.conv.3 tensor(0.4615)
features.7.conv.6 tensor(0.1914)
features.8.conv.0 tensor(0.5691)
features.8.conv.3 tensor(0.5448)
features.8.conv.6 tensor(0.1692)
features.9.conv.0 tensor(0.4788)
features.9.conv.3 tensor(0.5558)
features.9.conv.6 tensor(0.1349)
features.10.conv.0 tensor(0.0656)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0975)
features.11.conv.0 tensor(0.7258)
features.11.conv.3 tensor(0.6445)
features.11.conv.6 tensor(0.2026)
features.12.conv.0 tensor(0.7308)
features.12.conv.3 tensor(0.6723)
features.12.conv.6 tensor(0.5107)
features.13.conv.0 tensor(0.2226)
features.13.conv.3 tensor(0.4932)
features.13.conv.6 tensor(0.0956)
features.14.conv.0 tensor(0.8869)
features.14.conv.3 tensor(0.8279)
features.14.conv.6 tensor(0.9543)
features.15.conv.0 tensor(0.8534)
features.15.conv.3 tensor(0.8358)
features.15.conv.6 tensor(0.9562)
features.16.conv.0 tensor(0.6344)
features.16.conv.3 tensor(0.8069)
features.16.conv.6 tensor(0.3967)
conv.0 tensor(0.0933)
tensor(1053910.) 2188896.0
INFO - Validation [18][   40/   40]   Loss 0.378056   Top1 87.300000   Top5 99.560000   BatchTime 0.083785
INFO - ==> Top1: 87.300    Top5: 99.560    Loss: 0.378
INFO - ==> Sparsity : 0.481
INFO - Scoreboard best 1 ==> Epoch [18][Top1: 87.300   Top5: 99.560]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 87.090   Top5: 99.500]
0.71547139
tensor(0.2863, device='cuda:0', grad_fn=<AddBackward0>)
0.71538866
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
0.71528512
tensor(0.2830, device='cuda:0', grad_fn=<AddBackward0>)
0.71508753
tensor(0.3152, device='cuda:0', grad_fn=<AddBackward0>)
0.71501887
tensor(0.2572, device='cuda:0', grad_fn=<AddBackward0>)
0.71519953
tensor(0.2769, device='cuda:0', grad_fn=<AddBackward0>)
0.71508902
tensor(0.2672, device='cuda:0', grad_fn=<AddBackward0>)
0.71512848
tensor(0.2051, device='cuda:0', grad_fn=<AddBackward0>)
0.71527117
tensor(0.2601, device='cuda:0', grad_fn=<AddBackward0>)
0.71550715
tensor(0.1878, device='cuda:0', grad_fn=<AddBackward0>)
0.71589035
tensor(0.2559, device='cuda:0', grad_fn=<AddBackward0>)
0.71584803
tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)
0.71565294
tensor(0.2656, device='cuda:0', grad_fn=<AddBackward0>)
0.71555060
tensor(0.2431, device='cuda:0', grad_fn=<AddBackward0>)
0.71500462
tensor(0.2464, device='cuda:0', grad_fn=<AddBackward0>)
0.71468931
tensor(0.2820, device='cuda:0', grad_fn=<AddBackward0>)
0.71520460
tensor(0.2394, device='cuda:0', grad_fn=<AddBackward0>)
0.71451074
tensor(0.2374, device='cuda:0', grad_fn=<AddBackward0>)
0.71476209
tensor(0.3716, device='cuda:0', grad_fn=<AddBackward0>)
0.71435213
tensor(0.2977, device='cuda:0', grad_fn=<AddBackward0>)
0.71460873
tensor(0.2033, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][   20/  196]   Loss 0.260530   Top1 91.210938   Top5 99.707031   BatchTime 0.361222   LR 0.000143
0.71469140
tensor(0.2453, device='cuda:0', grad_fn=<AddBackward0>)
0.71504974
tensor(0.2817, device='cuda:0', grad_fn=<AddBackward0>)
0.71460205
tensor(0.2914, device='cuda:0', grad_fn=<AddBackward0>)
0.71481687
tensor(0.3512, device='cuda:0', grad_fn=<AddBackward0>)
0.71477723
tensor(0.2890, device='cuda:0', grad_fn=<AddBackward0>)
0.71490818
tensor(0.2967, device='cuda:0', grad_fn=<AddBackward0>)
0.71479756
tensor(0.2339, device='cuda:0', grad_fn=<AddBackward0>)
0.71444321
tensor(0.2543, device='cuda:0', grad_fn=<AddBackward0>)
0.71418858
tensor(0.2854, device='cuda:0', grad_fn=<AddBackward0>)
0.71413952
tensor(0.2287, device='cuda:0', grad_fn=<AddBackward0>)
0.71373498
tensor(0.2689, device='cuda:0', grad_fn=<AddBackward0>)
0.71356595
tensor(0.2707, device='cuda:0', grad_fn=<AddBackward0>)
0.71350896
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
0.71358013
tensor(0.2528, device='cuda:0', grad_fn=<AddBackward0>)
0.71334738
tensor(0.2892, device='cuda:0', grad_fn=<AddBackward0>)
0.71352804
tensor(0.2061, device='cuda:0', grad_fn=<AddBackward0>)
0.71357220
tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)
0.71381372
tensor(0.2409, device='cuda:0', grad_fn=<AddBackward0>)
0.71462560
tensor(0.2280, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][   40/  196]   Loss 0.259530   Top1 91.357422   Top5 99.804688   BatchTime 0.343211   LR 0.000141
0.71538550
tensor(0.2448, device='cuda:0', grad_fn=<AddBackward0>)
0.71539980
tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)
0.71511465
tensor(0.2530, device='cuda:0', grad_fn=<AddBackward0>)
0.71480089
tensor(0.2634, device='cuda:0', grad_fn=<AddBackward0>)
0.71482491
tensor(0.2777, device='cuda:0', grad_fn=<AddBackward0>)
0.71504050
tensor(0.2772, device='cuda:0', grad_fn=<AddBackward0>)
0.71509320
tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>)
0.71544635
tensor(0.2626, device='cuda:0', grad_fn=<AddBackward0>)
0.71534973
tensor(0.3163, device='cuda:0', grad_fn=<AddBackward0>)
0.71513855
tensor(0.2266, device='cuda:0', grad_fn=<AddBackward0>)
0.71472985
tensor(0.2788, device='cuda:0', grad_fn=<AddBackward0>)
0.71448475
tensor(0.2767, device='cuda:0', grad_fn=<AddBackward0>)
0.71423203
tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>)
0.71434122
tensor(0.2439, device='cuda:0', grad_fn=<AddBackward0>)
0.71432447
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
0.71406978
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.71381295
tensor(0.2779, device='cuda:0', grad_fn=<AddBackward0>)
0.71408767
tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)
0.71418017
tensor(0.3447, device='cuda:0', grad_fn=<AddBackward0>)
0.71417880
tensor(0.2700, device='cuda:0', grad_fn=<AddBackward0>)
0.71407306
tensor(0.3080, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][   60/  196]   Loss 0.257130   Top1 91.399740   Top5 99.811198   BatchTime 0.325011   LR 0.000139
0.71376860
tensor(0.3315, device='cuda:0', grad_fn=<AddBackward0>)
0.71353579
tensor(0.2421, device='cuda:0', grad_fn=<AddBackward0>)
0.71351814
tensor(0.2542, device='cuda:0', grad_fn=<AddBackward0>)
0.71315056
tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)
0.71294177
tensor(0.2262, device='cuda:0', grad_fn=<AddBackward0>)
0.71278369
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.71282536
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.71298391
tensor(0.2438, device='cuda:0', grad_fn=<AddBackward0>)
0.71293896
tensor(0.3055, device='cuda:0', grad_fn=<AddBackward0>)
0.71276468
tensor(0.2383, device='cuda:0', grad_fn=<AddBackward0>)
0.71285594
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
0.71290904
tensor(0.2297, device='cuda:0', grad_fn=<AddBackward0>)
0.71293586
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.71283275
tensor(0.3387, device='cuda:0', grad_fn=<AddBackward0>)
0.71270472
tensor(0.2605, device='cuda:0', grad_fn=<AddBackward0>)
0.71267515
tensor(0.2516, device='cuda:0', grad_fn=<AddBackward0>)
0.71274257
tensor(0.3129, device='cuda:0', grad_fn=<AddBackward0>)
0.71283990
tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)
0.71255541
tensor(0.2337, device='cuda:0', grad_fn=<AddBackward0>)
0.71268821
tensor(0.2122, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][   80/  196]   Loss 0.256748   Top1 91.347656   Top5 99.804688   BatchTime 0.319514   LR 0.000137
0.71231586
tensor(0.2801, device='cuda:0', grad_fn=<AddBackward0>)
0.71219951
tensor(0.3415, device='cuda:0', grad_fn=<AddBackward0>)
0.71203899
tensor(0.3164, device='cuda:0', grad_fn=<AddBackward0>)
0.71184623
tensor(0.2015, device='cuda:0', grad_fn=<AddBackward0>)
0.71174270
tensor(0.1976, device='cuda:0', grad_fn=<AddBackward0>)
0.71174699
tensor(0.2373, device='cuda:0', grad_fn=<AddBackward0>)
0.71172619
tensor(0.2674, device='cuda:0', grad_fn=<AddBackward0>)
0.71189624
tensor(0.3399, device='cuda:0', grad_fn=<AddBackward0>)
0.71211678
tensor(0.2891, device='cuda:0', grad_fn=<AddBackward0>)
0.71205354
tensor(0.3174, device='cuda:0', grad_fn=<AddBackward0>)
0.71211219
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
0.71228445
tensor(0.2987, device='cuda:0', grad_fn=<AddBackward0>)
0.71197379
tensor(0.2430, device='cuda:0', grad_fn=<AddBackward0>)
0.71200043
tensor(0.2257, device='cuda:0', grad_fn=<AddBackward0>)
0.71202445
tensor(0.2898, device='cuda:0', grad_fn=<AddBackward0>)
0.71202672
tensor(0.2338, device='cuda:0', grad_fn=<AddBackward0>)
0.71208841
tensor(0.2820, device='cuda:0', grad_fn=<AddBackward0>)
0.71200103
tensor(0.2711, device='cuda:0', grad_fn=<AddBackward0>)
0.71198654
tensor(0.2834, device='cuda:0', grad_fn=<AddBackward0>)
0.71207100
tensor(0.2641, device='cuda:0', grad_fn=<AddBackward0>)
0.71177596
tensor(0.3112, device='cuda:0', grad_fn=<AddBackward0>)
0.71173787
tensor(0.3469, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][  100/  196]   Loss 0.259062   Top1 91.269531   Top5 99.824219   BatchTime 0.310109   LR 0.000135
0.71187001
tensor(0.2902, device='cuda:0', grad_fn=<AddBackward0>)
0.71203059
tensor(0.1835, device='cuda:0', grad_fn=<AddBackward0>)
0.71208113
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
0.71203816
tensor(0.2634, device='cuda:0', grad_fn=<AddBackward0>)
0.71198714
tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)
0.71206349
tensor(0.2301, device='cuda:0', grad_fn=<AddBackward0>)
0.71200079
tensor(0.2577, device='cuda:0', grad_fn=<AddBackward0>)
0.71173143
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.71161777
tensor(0.2912, device='cuda:0', grad_fn=<AddBackward0>)
0.71154702
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
0.71147382
tensor(0.2943, device='cuda:0', grad_fn=<AddBackward0>)
0.71171415
tensor(0.2776, device='cuda:0', grad_fn=<AddBackward0>)
0.71183282
tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)
0.71215022
tensor(0.1903, device='cuda:0', grad_fn=<AddBackward0>)
0.71212631
tensor(0.2978, device='cuda:0', grad_fn=<AddBackward0>)
0.71254158
tensor(0.2075, device='cuda:0', grad_fn=<AddBackward0>)
0.71258122
tensor(0.2260, device='cuda:0', grad_fn=<AddBackward0>)
0.71193981
tensor(0.2901, device='cuda:0', grad_fn=<AddBackward0>)
0.71169621
tensor(0.2570, device='cuda:0', grad_fn=<AddBackward0>)
0.71160722
tensor(0.2400, device='cuda:0', grad_fn=<AddBackward0>)
0.71158910
tensor(0.4018, device='cuda:0', grad_fn=<AddBackward0>)
0.71151662
tensor(0.2098, device='cuda:0', grad_fn=<AddBackward0>)
0.71160567
tensor(0.2319, device='cuda:0', grad_fn=<AddBackward0>)
0.71175563
tensor(0.2469, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][  120/  196]   Loss 0.258181   Top1 91.344401   Top5 99.830729   BatchTime 0.300189   LR 0.000133
0.71176475
tensor(0.2730, device='cuda:0', grad_fn=<AddBackward0>)
0.71185952
tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)
0.71193862
tensor(0.3143, device='cuda:0', grad_fn=<AddBackward0>)
0.71218288
tensor(0.2666, device='cuda:0', grad_fn=<AddBackward0>)
0.71202630
tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)
0.71225917
tensor(0.2535, device='cuda:0', grad_fn=<AddBackward0>)
0.71218210
tensor(0.2374, device='cuda:0', grad_fn=<AddBackward0>)
0.71204668
tensor(0.2970, device='cuda:0', grad_fn=<AddBackward0>)
0.71190083
tensor(0.2562, device='cuda:0', grad_fn=<AddBackward0>)
0.71210963
tensor(0.2544, device='cuda:0', grad_fn=<AddBackward0>)
0.71217549
tensor(0.2616, device='cuda:0', grad_fn=<AddBackward0>)
0.71183753
tensor(0.2253, device='cuda:0', grad_fn=<AddBackward0>)
0.71173871
tensor(0.2436, device='cuda:0', grad_fn=<AddBackward0>)
0.71163625
tensor(0.3157, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][  140/  196]   Loss 0.258408   Top1 91.297433   Top5 99.838170   BatchTime 0.296892   LR 0.000131
0.71149176
tensor(0.2977, device='cuda:0', grad_fn=<AddBackward0>)
0.71138877
tensor(0.2541, device='cuda:0', grad_fn=<AddBackward0>)
0.71143264
tensor(0.1760, device='cuda:0', grad_fn=<AddBackward0>)
0.71147966
tensor(0.2938, device='cuda:0', grad_fn=<AddBackward0>)
0.71160364
tensor(0.2098, device='cuda:0', grad_fn=<AddBackward0>)
0.71147466
tensor(0.2879, device='cuda:0', grad_fn=<AddBackward0>)
0.71158999
tensor(0.1952, device='cuda:0', grad_fn=<AddBackward0>)
0.71132898
tensor(0.2264, device='cuda:0', grad_fn=<AddBackward0>)
0.71106875
tensor(0.2624, device='cuda:0', grad_fn=<AddBackward0>)
0.71116215
tensor(0.2145, device='cuda:0', grad_fn=<AddBackward0>)
0.71133268
tensor(0.3374, device='cuda:0', grad_fn=<AddBackward0>)
0.71121001
tensor(0.3474, device='cuda:0', grad_fn=<AddBackward0>)
0.71151030
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
0.71168309
tensor(0.2803, device='cuda:0', grad_fn=<AddBackward0>)
0.71174306
tensor(0.2780, device='cuda:0', grad_fn=<AddBackward0>)
0.71185291
tensor(0.2950, device='cuda:0', grad_fn=<AddBackward0>)
0.71137100
tensor(0.2252, device='cuda:0', grad_fn=<AddBackward0>)
0.71148771
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
0.71143407
tensor(0.3366, device='cuda:0', grad_fn=<AddBackward0>)
0.71138889
tensor(0.2725, device='cuda:0', grad_fn=<AddBackward0>)
0.71130574
tensor(0.2447, device='cuda:0', grad_fn=<AddBackward0>)
0.71124941
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.71130711
tensor(0.2777, device='cuda:0', grad_fn=<AddBackward0>)
0.71127403
tensor(0.2533, device='cuda:0', grad_fn=<AddBackward0>)
0.71100837
tensor(0.2593, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][  160/  196]   Loss 0.258421   Top1 91.303711   Top5 99.831543   BatchTime 0.301146   LR 0.000129
0.71120983
tensor(0.3227, device='cuda:0', grad_fn=<AddBackward0>)
0.71120685
tensor(0.2682, device='cuda:0', grad_fn=<AddBackward0>)
0.71109796
tensor(0.2104, device='cuda:0', grad_fn=<AddBackward0>)
0.71083492
tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)
0.71102333
tensor(0.2316, device='cuda:0', grad_fn=<AddBackward0>)
0.71101600
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
0.71087670
tensor(0.2996, device='cuda:0', grad_fn=<AddBackward0>)
0.71096385
tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)
0.71075708
tensor(0.1785, device='cuda:0', grad_fn=<AddBackward0>)
0.71063614
tensor(0.2379, device='cuda:0', grad_fn=<AddBackward0>)
0.71068168
tensor(0.3368, device='cuda:0', grad_fn=<AddBackward0>)
0.71085054
tensor(0.2521, device='cuda:0', grad_fn=<AddBackward0>)
0.71051949
tensor(0.2284, device='cuda:0', grad_fn=<AddBackward0>)
0.71042711
tensor(0.2514, device='cuda:0', grad_fn=<AddBackward0>)
0.71015757
tensor(0.2716, device='cuda:0', grad_fn=<AddBackward0>)
0.70981842
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
0.70946419
tensor(0.2522, device='cuda:0', grad_fn=<AddBackward0>)
0.70944118
tensor(0.2455, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [19][  180/  196]   Loss 0.257585   Top1 91.323785   Top5 99.839410   BatchTime 0.302078   LR 0.000127
0.70922172
tensor(0.3496, device='cuda:0', grad_fn=<AddBackward0>)
0.70917690
tensor(0.2346, device='cuda:0', grad_fn=<AddBackward0>)
0.70920908
tensor(0.3150, device='cuda:0', grad_fn=<AddBackward0>)
0.70920247
tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)
0.70944262
tensor(0.2723, device='cuda:0', grad_fn=<AddBackward0>)
0.70943606
tensor(0.2703, device='cuda:0', grad_fn=<AddBackward0>)
0.70948982
tensor(0.3326, device='cuda:0', grad_fn=<AddBackward0>)
0.70911270
tensor(0.2220, device='cuda:0', grad_fn=<AddBackward0>)
0.70883077
tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)
0.70876455
tensor(0.2437, device='cuda:0', grad_fn=<AddBackward0>)
0.70865345
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
0.70857877
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 91.340    Top5: 99.842    Loss: 0.257
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [19][   20/   40]   Loss 0.392142   Top1 87.031250   Top5 99.433594   BatchTime 0.110583
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1426)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.1016)
features.2.conv.0 tensor(0.1233)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.5443)
features.3.conv.0 tensor(0.0813)
features.3.conv.3 tensor(0.0903)
features.3.conv.6 tensor(0.1063)
features.4.conv.0 tensor(0.0540)
features.4.conv.3 tensor(0.3050)
features.4.conv.6 tensor(0.1834)
features.5.conv.0 tensor(0.3050)
features.5.conv.3 tensor(0.4225)
features.5.conv.6 tensor(0.1060)
features.6.conv.0 tensor(0.0488)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0856)
features.7.conv.0 tensor(0.1777)
features.7.conv.3 tensor(0.4618)
features.7.conv.6 tensor(0.1923)
features.8.conv.0 tensor(0.5905)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.1724)
features.9.conv.0 tensor(0.4768)
features.9.conv.3 tensor(0.5582)
features.9.conv.6 tensor(0.1395)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0975)
features.11.conv.0 tensor(0.7325)
features.11.conv.3 tensor(0.6414)
features.11.conv.6 tensor(0.2123)
features.12.conv.0 tensor(0.7230)
features.12.conv.3 tensor(0.6736)
features.12.conv.6 tensor(0.5395)
features.13.conv.0 tensor(0.2319)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.0946)
features.14.conv.0 tensor(0.8933)
features.14.conv.3 tensor(0.8275)
features.14.conv.6 tensor(0.9540)
features.15.conv.0 tensor(0.8617)
features.15.conv.3 tensor(0.8355)
features.15.conv.6 tensor(0.9561)
features.16.conv.0 tensor(0.6361)
features.16.conv.3 tensor(0.8074)
features.16.conv.6 tensor(0.5484)
conv.0 tensor(0.0925)
tensor(1106152.) 2188896.0
INFO - Validation [19][   40/   40]   Loss 0.384355   Top1 87.320000   Top5 99.570000   BatchTime 0.084614
INFO - ==> Top1: 87.320    Top5: 99.570    Loss: 0.384
INFO - ==> Sparsity : 0.505
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 87.320   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 87.300   Top5: 99.560]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.70838141
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.70834905
tensor(0.2962, device='cuda:0', grad_fn=<AddBackward0>)
0.70852453
tensor(0.2541, device='cuda:0', grad_fn=<AddBackward0>)
0.70869952
tensor(0.2472, device='cuda:0', grad_fn=<AddBackward0>)
0.70867348
tensor(0.4084, device='cuda:0', grad_fn=<AddBackward0>)
0.70835990
tensor(0.2369, device='cuda:0', grad_fn=<AddBackward0>)
0.70834452
tensor(0.2165, device='cuda:0', grad_fn=<AddBackward0>)
0.70831543
tensor(0.2161, device='cuda:0', grad_fn=<AddBackward0>)
0.70825815
tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)
0.70840859
tensor(0.2375, device='cuda:0', grad_fn=<AddBackward0>)
0.70845807
tensor(0.2106, device='cuda:0', grad_fn=<AddBackward0>)
0.70844322
tensor(0.3171, device='cuda:0', grad_fn=<AddBackward0>)
0.70828778
tensor(0.3188, device='cuda:0', grad_fn=<AddBackward0>)
0.70827651
tensor(0.2922, device='cuda:0', grad_fn=<AddBackward0>)
0.70840812
tensor(0.3022, device='cuda:0', grad_fn=<AddBackward0>)
0.70857203
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.70828789
tensor(0.2846, device='cuda:0', grad_fn=<AddBackward0>)
0.70828193
tensor(0.2846, device='cuda:0', grad_fn=<AddBackward0>)
0.70835555
tensor(0.2318, device='cuda:0', grad_fn=<AddBackward0>)
0.70822918
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.70824105
tensor(0.1967, device='cuda:0', grad_fn=<AddBackward0>)
0.70813030
tensor(0.2621, device='cuda:0', grad_fn=<AddBackward0>)
0.70797038
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][   20/  196]   Loss 0.257367   Top1 91.386719   Top5 99.843750   BatchTime 0.358258   LR 0.000123
0.70812231
tensor(0.2287, device='cuda:0', grad_fn=<AddBackward0>)
0.70776129
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
0.70743382
tensor(0.2138, device='cuda:0', grad_fn=<AddBackward0>)
0.70776343
tensor(0.2507, device='cuda:0', grad_fn=<AddBackward0>)
0.70761514
tensor(0.2157, device='cuda:0', grad_fn=<AddBackward0>)
0.70740759
tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)
0.70710653
tensor(0.2978, device='cuda:0', grad_fn=<AddBackward0>)
0.70700127
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.70707905
tensor(0.2002, device='cuda:0', grad_fn=<AddBackward0>)
0.70737594
tensor(0.2046, device='cuda:0', grad_fn=<AddBackward0>)
0.70707858
tensor(0.2346, device='cuda:0', grad_fn=<AddBackward0>)
0.70733619
tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)
0.70734698
tensor(0.2581, device='cuda:0', grad_fn=<AddBackward0>)
0.70724785
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
0.70703590
tensor(0.2395, device='cuda:0', grad_fn=<AddBackward0>)
0.70683885
tensor(0.3010, device='cuda:0', grad_fn=<AddBackward0>)
0.70655429
tensor(0.2846, device='cuda:0', grad_fn=<AddBackward0>)
0.70647329
tensor(0.2641, device='cuda:0', grad_fn=<AddBackward0>)
0.70653307
tensor(0.2536, device='cuda:0', grad_fn=<AddBackward0>)
0.70632946
tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][   40/  196]   Loss 0.242879   Top1 91.777344   Top5 99.843750   BatchTime 0.332130   LR 0.000121
0.70613456
tensor(0.2647, device='cuda:0', grad_fn=<AddBackward0>)
0.70614243
tensor(0.2883, device='cuda:0', grad_fn=<AddBackward0>)
0.70594460
tensor(0.2513, device='cuda:0', grad_fn=<AddBackward0>)
0.70610058
tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)
0.70639467
tensor(0.2296, device='cuda:0', grad_fn=<AddBackward0>)
0.70590830
tensor(0.2828, device='cuda:0', grad_fn=<AddBackward0>)
0.70576715
tensor(0.2378, device='cuda:0', grad_fn=<AddBackward0>)
0.70598513
tensor(0.2733, device='cuda:0', grad_fn=<AddBackward0>)
0.70621526
tensor(0.2808, device='cuda:0', grad_fn=<AddBackward0>)
0.70616329
tensor(0.2382, device='cuda:0', grad_fn=<AddBackward0>)
0.70597512
tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)
0.70593226
tensor(0.2411, device='cuda:0', grad_fn=<AddBackward0>)
0.70604604
tensor(0.2190, device='cuda:0', grad_fn=<AddBackward0>)
0.70609993
tensor(0.2677, device='cuda:0', grad_fn=<AddBackward0>)
0.70605522
tensor(0.2314, device='cuda:0', grad_fn=<AddBackward0>)
0.70586228
tensor(0.2886, device='cuda:0', grad_fn=<AddBackward0>)
0.70557970
tensor(0.2478, device='cuda:0', grad_fn=<AddBackward0>)
0.70537668
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][   60/  196]   Loss 0.244198   Top1 91.803385   Top5 99.837240   BatchTime 0.333667   LR 0.000119
0.70559388
tensor(0.2354, device='cuda:0', grad_fn=<AddBackward0>)
0.70549440
tensor(0.2294, device='cuda:0', grad_fn=<AddBackward0>)
0.70531327
tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)
0.70523137
tensor(0.2343, device='cuda:0', grad_fn=<AddBackward0>)
0.70519441
tensor(0.2983, device='cuda:0', grad_fn=<AddBackward0>)
0.70514172
tensor(0.2808, device='cuda:0', grad_fn=<AddBackward0>)
0.70505661
tensor(0.2705, device='cuda:0', grad_fn=<AddBackward0>)
0.70489031
tensor(0.2595, device='cuda:0', grad_fn=<AddBackward0>)
0.70483142
tensor(0.2085, device='cuda:0', grad_fn=<AddBackward0>)
0.70446068
tensor(0.2682, device='cuda:0', grad_fn=<AddBackward0>)
0.70438164
tensor(0.2664, device='cuda:0', grad_fn=<AddBackward0>)
0.70441800
tensor(0.3094, device='cuda:0', grad_fn=<AddBackward0>)
0.70479977
tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)
0.70498198
tensor(0.2105, device='cuda:0', grad_fn=<AddBackward0>)
0.70521671
tensor(0.3042, device='cuda:0', grad_fn=<AddBackward0>)
0.70514172
tensor(0.2359, device='cuda:0', grad_fn=<AddBackward0>)
0.70514244
tensor(0.2176, device='cuda:0', grad_fn=<AddBackward0>)
0.70520794
tensor(0.2754, device='cuda:0', grad_fn=<AddBackward0>)
0.70517188
tensor(0.2700, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][   80/  196]   Loss 0.246623   Top1 91.835938   Top5 99.819336   BatchTime 0.329050   LR 0.000117
0.70458108
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.70446181
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
0.70443690
tensor(0.2086, device='cuda:0', grad_fn=<AddBackward0>)
0.70445758
tensor(0.2477, device='cuda:0', grad_fn=<AddBackward0>)
0.70435625
tensor(0.2599, device='cuda:0', grad_fn=<AddBackward0>)
0.70422298
tensor(0.2959, device='cuda:0', grad_fn=<AddBackward0>)
0.70429814
tensor(0.2284, device='cuda:0', grad_fn=<AddBackward0>)
0.70434254
tensor(0.2020, device='cuda:0', grad_fn=<AddBackward0>)
0.70444220
tensor(0.2518, device='cuda:0', grad_fn=<AddBackward0>)
0.70440274
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
0.70437700
tensor(0.2720, device='cuda:0', grad_fn=<AddBackward0>)
0.70447350
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
0.70428252
tensor(0.2108, device='cuda:0', grad_fn=<AddBackward0>)
0.70432204
tensor(0.2772, device='cuda:0', grad_fn=<AddBackward0>)
0.70431507
tensor(0.2065, device='cuda:0', grad_fn=<AddBackward0>)
0.70426595
tensor(0.2457, device='cuda:0', grad_fn=<AddBackward0>)
0.70442289
tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>)
0.70410824
tensor(0.3453, device='cuda:0', grad_fn=<AddBackward0>)
0.70409679
tensor(0.2911, device='cuda:0', grad_fn=<AddBackward0>)
0.70387644
tensor(0.2467, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][  100/  196]   Loss 0.246661   Top1 91.792969   Top5 99.824219   BatchTime 0.323297   LR 0.000115
0.70396119
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
0.70392531
tensor(0.2595, device='cuda:0', grad_fn=<AddBackward0>)
0.70374143
tensor(0.2581, device='cuda:0', grad_fn=<AddBackward0>)
0.70370466
tensor(0.2600, device='cuda:0', grad_fn=<AddBackward0>)
0.70372796
tensor(0.2821, device='cuda:0', grad_fn=<AddBackward0>)
0.70366532
tensor(0.2827, device='cuda:0', grad_fn=<AddBackward0>)
0.70367837
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
0.70366311
tensor(0.2194, device='cuda:0', grad_fn=<AddBackward0>)
0.70364279
tensor(0.2050, device='cuda:0', grad_fn=<AddBackward0>)
0.70372057
tensor(0.1976, device='cuda:0', grad_fn=<AddBackward0>)
0.70371628
tensor(0.2268, device='cuda:0', grad_fn=<AddBackward0>)
0.70365387
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
0.70326406
tensor(0.2254, device='cuda:0', grad_fn=<AddBackward0>)
0.70321518
tensor(0.2093, device='cuda:0', grad_fn=<AddBackward0>)
0.70323431
tensor(0.2010, device='cuda:0', grad_fn=<AddBackward0>)
0.70333940
tensor(0.2868, device='cuda:0', grad_fn=<AddBackward0>)
0.70334566
tensor(0.2130, device='cuda:0', grad_fn=<AddBackward0>)
0.70338488
tensor(0.2528, device='cuda:0', grad_fn=<AddBackward0>)
0.70326501
tensor(0.2323, device='cuda:0', grad_fn=<AddBackward0>)
0.70299464
tensor(0.2657, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][  120/  196]   Loss 0.244478   Top1 91.842448   Top5 99.830729   BatchTime 0.320374   LR 0.000113
0.70295525
tensor(0.3068, device='cuda:0', grad_fn=<AddBackward0>)
0.70301783
tensor(0.2470, device='cuda:0', grad_fn=<AddBackward0>)
0.70297217
tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)
0.70277810
tensor(0.2303, device='cuda:0', grad_fn=<AddBackward0>)
0.70263863
tensor(0.2139, device='cuda:0', grad_fn=<AddBackward0>)
0.70276958
tensor(0.2128, device='cuda:0', grad_fn=<AddBackward0>)
0.70270526
tensor(0.3651, device='cuda:0', grad_fn=<AddBackward0>)
0.70269012
tensor(0.2760, device='cuda:0', grad_fn=<AddBackward0>)
0.70275956
tensor(0.2734, device='cuda:0', grad_fn=<AddBackward0>)
0.70296276
tensor(0.2751, device='cuda:0', grad_fn=<AddBackward0>)
0.70272189
tensor(0.2828, device='cuda:0', grad_fn=<AddBackward0>)
0.70233548
tensor(0.3118, device='cuda:0', grad_fn=<AddBackward0>)
0.70226568
tensor(0.2136, device='cuda:0', grad_fn=<AddBackward0>)
0.70218891
tensor(0.2036, device='cuda:0', grad_fn=<AddBackward0>)
0.70213532
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
0.70186615
tensor(0.2414, device='cuda:0', grad_fn=<AddBackward0>)
0.70180231
tensor(0.3310, device='cuda:0', grad_fn=<AddBackward0>)
0.70186603
tensor(0.2653, device='cuda:0', grad_fn=<AddBackward0>)
0.70164233
tensor(0.3116, device='cuda:0', grad_fn=<AddBackward0>)
0.70150161
tensor(0.2748, device='cuda:0', grad_fn=<AddBackward0>)
0.70161664
tensor(0.2152, device='cuda:0', grad_fn=<AddBackward0>)
0.70158243
tensor(0.2583, device='cuda:0', grad_fn=<AddBackward0>)
0.70153898
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.70151711
tensor(0.2849, device='cuda:0', grad_fn=<AddBackward0>)
0.70163333
tensor(0.2234, device='cuda:0', grad_fn=<AddBackward0>)
0.70166129
tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][  140/  196]   Loss 0.247131   Top1 91.763393   Top5 99.824219   BatchTime 0.319494   LR 0.000111
0.70187300
tensor(0.2573, device='cuda:0', grad_fn=<AddBackward0>)
0.70172280
tensor(0.3024, device='cuda:0', grad_fn=<AddBackward0>)
0.70162416
tensor(0.2631, device='cuda:0', grad_fn=<AddBackward0>)
0.70153147
tensor(0.2569, device='cuda:0', grad_fn=<AddBackward0>)
0.70143682
tensor(0.2549, device='cuda:0', grad_fn=<AddBackward0>)
0.70144588
tensor(0.2579, device='cuda:0', grad_fn=<AddBackward0>)
0.70145059
tensor(0.3823, device='cuda:0', grad_fn=<AddBackward0>)
0.70132905
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
0.70141166
tensor(0.1774, device='cuda:0', grad_fn=<AddBackward0>)
0.70148098
tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)
0.70146978
tensor(0.2701, device='cuda:0', grad_fn=<AddBackward0>)
0.70111465
tensor(0.2554, device='cuda:0', grad_fn=<AddBackward0>)
0.70091963
tensor(0.2447, device='cuda:0', grad_fn=<AddBackward0>)
0.70081019
tensor(0.2204, device='cuda:0', grad_fn=<AddBackward0>)
0.70086056
tensor(0.2437, device='cuda:0', grad_fn=<AddBackward0>)
0.70094573
tensor(0.2879, device='cuda:0', grad_fn=<AddBackward0>)
0.70095021
tensor(0.2546, device='cuda:0', grad_fn=<AddBackward0>)
0.70105380
tensor(0.2252, device='cuda:0', grad_fn=<AddBackward0>)
0.70121080
tensor(0.2268, device='cuda:0', grad_fn=<AddBackward0>)
0.70118254
tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][  160/  196]   Loss 0.247610   Top1 91.730957   Top5 99.821777   BatchTime 0.315659   LR 0.000109
0.70113629
tensor(0.3226, device='cuda:0', grad_fn=<AddBackward0>)
0.70116389
tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)
0.70086437
tensor(0.2554, device='cuda:0', grad_fn=<AddBackward0>)
0.70070380
tensor(0.2473, device='cuda:0', grad_fn=<AddBackward0>)
0.70064390
tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)
0.70071125
tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)
0.70077139
tensor(0.3003, device='cuda:0', grad_fn=<AddBackward0>)
0.70075148
tensor(0.2523, device='cuda:0', grad_fn=<AddBackward0>)
0.70083994
tensor(0.2172, device='cuda:0', grad_fn=<AddBackward0>)
0.70101082
tensor(0.2372, device='cuda:0', grad_fn=<AddBackward0>)
0.70062101
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
0.70046562
tensor(0.2051, device='cuda:0', grad_fn=<AddBackward0>)
0.70026714
tensor(0.2708, device='cuda:0', grad_fn=<AddBackward0>)
0.70013422
tensor(0.2468, device='cuda:0', grad_fn=<AddBackward0>)
0.70013589
tensor(0.2470, device='cuda:0', grad_fn=<AddBackward0>)
0.70024133
tensor(0.2315, device='cuda:0', grad_fn=<AddBackward0>)
0.70045316
tensor(0.2545, device='cuda:0', grad_fn=<AddBackward0>)
0.70060819
tensor(0.2421, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [20][  180/  196]   Loss 0.247051   Top1 91.697049   Top5 99.826389   BatchTime 0.317155   LR 0.000107
0.70041579
tensor(0.3710, device='cuda:0', grad_fn=<AddBackward0>)
0.70034534
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
0.70048231
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
0.70038205
tensor(0.2366, device='cuda:0', grad_fn=<AddBackward0>)
0.70030773
tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)
0.70020926
tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)
0.70019925
tensor(0.2659, device='cuda:0', grad_fn=<AddBackward0>)
0.70029581
tensor(0.2690, device='cuda:0', grad_fn=<AddBackward0>)
0.70022970
tensor(0.2713, device='cuda:0', grad_fn=<AddBackward0>)
0.70004267
tensor(0.2165, device='cuda:0', grad_fn=<AddBackward0>)
0.69993830
tensor(0.2636, device='cuda:0', grad_fn=<AddBackward0>)
0.69984871
tensor(0.2723, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 91.676    Top5: 99.824    Loss: 0.247
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [20][   20/   40]   Loss 0.412248   Top1 86.679688   Top5 99.472656   BatchTime 0.116011
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0916)
features.2.conv.0 tensor(0.1418)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.5466)
features.3.conv.0 tensor(0.0787)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1079)
features.4.conv.0 tensor(0.0607)
features.4.conv.3 tensor(0.3090)
features.4.conv.6 tensor(0.1859)
features.5.conv.0 tensor(0.3109)
features.5.conv.3 tensor(0.4213)
features.5.conv.6 tensor(0.1053)
features.6.conv.0 tensor(0.0503)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0844)
features.7.conv.0 tensor(0.1661)
features.7.conv.3 tensor(0.4586)
features.7.conv.6 tensor(0.1936)
features.8.conv.0 tensor(0.5992)
features.8.conv.3 tensor(0.5446)
features.8.conv.6 tensor(0.1729)
features.9.conv.0 tensor(0.5031)
features.9.conv.3 tensor(0.5582)
features.9.conv.6 tensor(0.1309)
features.10.conv.0 tensor(0.0636)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.0982)
features.11.conv.0 tensor(0.7328)
features.11.conv.3 tensor(0.6412)
features.11.conv.6 tensor(0.2150)
features.12.conv.0 tensor(0.7210)
features.12.conv.3 tensor(0.6728)
features.12.conv.6 tensor(0.6204)
features.13.conv.0 tensor(0.2231)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.0939)
features.14.conv.0 tensor(0.8945)
features.14.conv.3 tensor(0.8277)
features.14.conv.6 tensor(0.9546)
features.15.conv.0 tensor(0.8633)
features.15.conv.3 tensor(0.8356)
features.15.conv.6 tensor(0.9588)
features.16.conv.0 tensor(0.6355)
features.16.conv.3 tensor(0.8075)
features.16.conv.6 tensor(0.6770)
conv.0 tensor(0.0920)
tensor(1150741.) 2188896.0
INFO - Validation [20][   40/   40]   Loss 0.406920   Top1 87.110000   Top5 99.520000   BatchTime 0.084531
INFO - ==> Top1: 87.110    Top5: 99.520    Loss: 0.407
INFO - ==> Sparsity : 0.526
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 87.320   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 87.300   Top5: 99.560]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 87.220   Top5: 99.510]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
0.70003653
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
0.70026612
tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)
0.70021999
tensor(0.2736, device='cuda:0', grad_fn=<AddBackward0>)
0.69993186
tensor(0.2684, device='cuda:0', grad_fn=<AddBackward0>)
0.69993800
tensor(0.2448, device='cuda:0', grad_fn=<AddBackward0>)
0.69961190
tensor(0.2952, device='cuda:0', grad_fn=<AddBackward0>)
0.69963431
tensor(0.1754, device='cuda:0', grad_fn=<AddBackward0>)
0.69964027
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.69958740
tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>)
0.69962138
tensor(0.2490, device='cuda:0', grad_fn=<AddBackward0>)
0.69959456
tensor(0.2492, device='cuda:0', grad_fn=<AddBackward0>)
0.69965982
tensor(0.2225, device='cuda:0', grad_fn=<AddBackward0>)
0.69971442
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
0.69986981
tensor(0.2497, device='cuda:0', grad_fn=<AddBackward0>)
0.69971693
tensor(0.2330, device='cuda:0', grad_fn=<AddBackward0>)
0.69950151
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
0.69952351
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.69935471
tensor(0.2432, device='cuda:0', grad_fn=<AddBackward0>)
0.69939953
tensor(0.2787, device='cuda:0', grad_fn=<AddBackward0>)
0.69907337
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][   20/  196]   Loss 0.222886   Top1 92.031250   Top5 99.980469   BatchTime 0.349766   LR 0.000104
0.69903904
tensor(0.2461, device='cuda:0', grad_fn=<AddBackward0>)
0.69920194
tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)
0.69905812
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.69899815
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
0.69884604
tensor(0.2612, device='cuda:0', grad_fn=<AddBackward0>)
0.69885212
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.69893569
tensor(0.1746, device='cuda:0', grad_fn=<AddBackward0>)
0.69886053
tensor(0.2275, device='cuda:0', grad_fn=<AddBackward0>)
0.69855559
tensor(0.2621, device='cuda:0', grad_fn=<AddBackward0>)
0.69841933
tensor(0.2350, device='cuda:0', grad_fn=<AddBackward0>)
0.69826746
tensor(0.2179, device='cuda:0', grad_fn=<AddBackward0>)
0.69845986
tensor(0.2473, device='cuda:0', grad_fn=<AddBackward0>)
0.69824487
tensor(0.2605, device='cuda:0', grad_fn=<AddBackward0>)
0.69786656
tensor(0.2852, device='cuda:0', grad_fn=<AddBackward0>)
0.69784927
tensor(0.2825, device='cuda:0', grad_fn=<AddBackward0>)
0.69812649
tensor(0.2568, device='cuda:0', grad_fn=<AddBackward0>)
0.69809979
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
0.69811082
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
0.69783175
tensor(0.1900, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][   40/  196]   Loss 0.224942   Top1 92.216797   Top5 99.912109   BatchTime 0.332495   LR 0.000102
0.69794041
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.69792712
tensor(0.2635, device='cuda:0', grad_fn=<AddBackward0>)
0.69778889
tensor(0.2194, device='cuda:0', grad_fn=<AddBackward0>)
0.69749331
tensor(0.2373, device='cuda:0', grad_fn=<AddBackward0>)
0.69728237
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
0.69728363
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.69719607
tensor(0.1914, device='cuda:0', grad_fn=<AddBackward0>)
0.69718605
tensor(0.2509, device='cuda:0', grad_fn=<AddBackward0>)
0.69719785
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
0.69742090
tensor(0.2272, device='cuda:0', grad_fn=<AddBackward0>)
0.69741207
tensor(0.2542, device='cuda:0', grad_fn=<AddBackward0>)
0.69752747
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
0.69748551
tensor(0.2664, device='cuda:0', grad_fn=<AddBackward0>)
0.69730473
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.69728756
tensor(0.2403, device='cuda:0', grad_fn=<AddBackward0>)
0.69730031
tensor(0.2074, device='cuda:0', grad_fn=<AddBackward0>)
0.69750631
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
0.69752556
tensor(0.2142, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][   60/  196]   Loss 0.223983   Top1 92.213542   Top5 99.921875   BatchTime 0.332603   LR 0.000100
0.69717747
tensor(0.2493, device='cuda:0', grad_fn=<AddBackward0>)
0.69690579
tensor(0.2233, device='cuda:0', grad_fn=<AddBackward0>)
0.69684130
tensor(0.2053, device='cuda:0', grad_fn=<AddBackward0>)
0.69642413
tensor(0.2279, device='cuda:0', grad_fn=<AddBackward0>)
0.69606477
tensor(0.2627, device='cuda:0', grad_fn=<AddBackward0>)
0.69595617
tensor(0.2033, device='cuda:0', grad_fn=<AddBackward0>)
0.69573593
tensor(0.2136, device='cuda:0', grad_fn=<AddBackward0>)
0.69577652
tensor(0.3017, device='cuda:0', grad_fn=<AddBackward0>)
0.69560891
tensor(0.2124, device='cuda:0', grad_fn=<AddBackward0>)
0.69531798
tensor(0.2680, device='cuda:0', grad_fn=<AddBackward0>)
0.69535118
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
0.69539666
tensor(0.2426, device='cuda:0', grad_fn=<AddBackward0>)
0.69526368
tensor(0.2318, device='cuda:0', grad_fn=<AddBackward0>)
0.69532460
tensor(0.2287, device='cuda:0', grad_fn=<AddBackward0>)
0.69535625
tensor(0.1857, device='cuda:0', grad_fn=<AddBackward0>)
0.69538474
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
0.69527203
tensor(0.2341, device='cuda:0', grad_fn=<AddBackward0>)
0.69527209
tensor(0.2724, device='cuda:0', grad_fn=<AddBackward0>)
0.69503760
tensor(0.2821, device='cuda:0', grad_fn=<AddBackward0>)
0.69485372
tensor(0.2365, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][   80/  196]   Loss 0.225623   Top1 92.250977   Top5 99.897461   BatchTime 0.326301   LR 0.000098
0.69471228
tensor(0.1717, device='cuda:0', grad_fn=<AddBackward0>)
0.69450343
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
0.69425720
tensor(0.1821, device='cuda:0', grad_fn=<AddBackward0>)
0.69394183
tensor(0.2518, device='cuda:0', grad_fn=<AddBackward0>)
0.69377416
tensor(0.2369, device='cuda:0', grad_fn=<AddBackward0>)
0.69313073
tensor(0.2460, device='cuda:0', grad_fn=<AddBackward0>)
0.69272006
tensor(0.2546, device='cuda:0', grad_fn=<AddBackward0>)
0.69233769
tensor(0.2874, device='cuda:0', grad_fn=<AddBackward0>)
0.69193876
tensor(0.2403, device='cuda:0', grad_fn=<AddBackward0>)
0.69176060
tensor(0.2992, device='cuda:0', grad_fn=<AddBackward0>)
0.69144380
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
0.69087511
tensor(0.2175, device='cuda:0', grad_fn=<AddBackward0>)
0.69069326
tensor(0.2433, device='cuda:0', grad_fn=<AddBackward0>)
0.69047338
tensor(0.2673, device='cuda:0', grad_fn=<AddBackward0>)
0.69016069
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.68999660
tensor(0.2418, device='cuda:0', grad_fn=<AddBackward0>)
0.68967783
tensor(0.2680, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][  100/  196]   Loss 0.228017   Top1 92.226562   Top5 99.890625   BatchTime 0.328183   LR 0.000096
0.68953288
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.68957889
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.68973845
tensor(0.2282, device='cuda:0', grad_fn=<AddBackward0>)
0.68968874
tensor(0.2323, device='cuda:0', grad_fn=<AddBackward0>)
0.68992323
tensor(0.2199, device='cuda:0', grad_fn=<AddBackward0>)
0.68986601
tensor(0.3024, device='cuda:0', grad_fn=<AddBackward0>)
0.68970627
tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)
0.68966538
tensor(0.2611, device='cuda:0', grad_fn=<AddBackward0>)
0.68929023
tensor(0.2183, device='cuda:0', grad_fn=<AddBackward0>)
0.68896204
tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)
0.68880194
tensor(0.3493, device='cuda:0', grad_fn=<AddBackward0>)
0.68866509
tensor(0.1987, device='cuda:0', grad_fn=<AddBackward0>)
0.68868077
tensor(0.2436, device='cuda:0', grad_fn=<AddBackward0>)
0.68883967
tensor(0.2470, device='cuda:0', grad_fn=<AddBackward0>)
0.68890089
tensor(0.2147, device='cuda:0', grad_fn=<AddBackward0>)
0.68910414
tensor(0.2820, device='cuda:0', grad_fn=<AddBackward0>)
0.68895411
tensor(0.2263, device='cuda:0', grad_fn=<AddBackward0>)
0.68903869
tensor(0.1821, device='cuda:0', grad_fn=<AddBackward0>)
0.68879396
tensor(0.1990, device='cuda:0', grad_fn=<AddBackward0>)
0.68827015
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.68784934
tensor(0.2889, device='cuda:0', grad_fn=<AddBackward0>)
0.68740338
tensor(0.1904, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][  120/  196]   Loss 0.228315   Top1 92.242839   Top5 99.892578   BatchTime 0.317254   LR 0.000094
0.68726498
tensor(0.2650, device='cuda:0', grad_fn=<AddBackward0>)
0.68709731
tensor(0.2229, device='cuda:0', grad_fn=<AddBackward0>)
0.68695450
tensor(0.2427, device='cuda:0', grad_fn=<AddBackward0>)
0.68676639
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
0.68638182
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
0.68609685
tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)
0.68581516
tensor(0.2821, device='cuda:0', grad_fn=<AddBackward0>)
0.68561012
tensor(0.2142, device='cuda:0', grad_fn=<AddBackward0>)
0.68543583
tensor(0.1854, device='cuda:0', grad_fn=<AddBackward0>)
0.68500572
tensor(0.1899, device='cuda:0', grad_fn=<AddBackward0>)
0.68504155
tensor(0.2274, device='cuda:0', grad_fn=<AddBackward0>)
0.68463415
tensor(0.3339, device='cuda:0', grad_fn=<AddBackward0>)
0.68435884
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
0.68423969
tensor(0.2990, device='cuda:0', grad_fn=<AddBackward0>)
0.68386149
tensor(0.2161, device='cuda:0', grad_fn=<AddBackward0>)
0.68354756
tensor(0.2468, device='cuda:0', grad_fn=<AddBackward0>)
0.68341762
tensor(0.2566, device='cuda:0', grad_fn=<AddBackward0>)
0.68342513
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
0.68294930
tensor(0.2224, device='cuda:0', grad_fn=<AddBackward0>)
0.68299121
tensor(0.2522, device='cuda:0', grad_fn=<AddBackward0>)
0.68279111
tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)
0.68268627
tensor(0.2674, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][  140/  196]   Loss 0.229870   Top1 92.184710   Top5 99.888393   BatchTime 0.312534   LR 0.000092
0.68238670
tensor(0.2219, device='cuda:0', grad_fn=<AddBackward0>)
0.68222052
tensor(0.2515, device='cuda:0', grad_fn=<AddBackward0>)
0.68207538
tensor(0.2461, device='cuda:0', grad_fn=<AddBackward0>)
0.68203264
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
0.68190914
tensor(0.2788, device='cuda:0', grad_fn=<AddBackward0>)
0.68186194
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
0.68156028
tensor(0.2402, device='cuda:0', grad_fn=<AddBackward0>)
0.68139976
tensor(0.2414, device='cuda:0', grad_fn=<AddBackward0>)
0.68129593
tensor(0.2768, device='cuda:0', grad_fn=<AddBackward0>)
0.68108678
tensor(0.2238, device='cuda:0', grad_fn=<AddBackward0>)
0.68093687
tensor(0.2914, device='cuda:0', grad_fn=<AddBackward0>)
0.68065041
tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)
0.68066591
tensor(0.2610, device='cuda:0', grad_fn=<AddBackward0>)
0.68066132
tensor(0.2243, device='cuda:0', grad_fn=<AddBackward0>)
0.68021941
tensor(0.3296, device='cuda:0', grad_fn=<AddBackward0>)
0.68008810
tensor(0.3116, device='cuda:0', grad_fn=<AddBackward0>)
0.68032098
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
0.68024695
tensor(0.2876, device='cuda:0', grad_fn=<AddBackward0>)
0.68000209
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][  160/  196]   Loss 0.231811   Top1 92.097168   Top5 99.877930   BatchTime 0.312531   LR 0.000090
0.67974079
tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)
0.67971629
tensor(0.2704, device='cuda:0', grad_fn=<AddBackward0>)
0.67959696
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
0.67950988
tensor(0.2056, device='cuda:0', grad_fn=<AddBackward0>)
0.67937279
tensor(0.2531, device='cuda:0', grad_fn=<AddBackward0>)
0.67897302
tensor(0.2199, device='cuda:0', grad_fn=<AddBackward0>)
0.67864114
tensor(0.2929, device='cuda:0', grad_fn=<AddBackward0>)
0.67861485
tensor(0.1999, device='cuda:0', grad_fn=<AddBackward0>)
0.67854273
tensor(0.3049, device='cuda:0', grad_fn=<AddBackward0>)
0.67850429
tensor(0.2183, device='cuda:0', grad_fn=<AddBackward0>)
0.67849976
tensor(0.2165, device='cuda:0', grad_fn=<AddBackward0>)
0.67844379
tensor(0.2580, device='cuda:0', grad_fn=<AddBackward0>)
0.67827088
tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)
0.67817068
tensor(0.3008, device='cuda:0', grad_fn=<AddBackward0>)
0.67822367
tensor(0.3228, device='cuda:0', grad_fn=<AddBackward0>)
0.67817628
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
0.67799395
tensor(0.2356, device='cuda:0', grad_fn=<AddBackward0>)
0.67804724
tensor(0.1847, device='cuda:0', grad_fn=<AddBackward0>)
0.67867404
tensor(0.2973, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [21][  180/  196]   Loss 0.232842   Top1 92.046441   Top5 99.869792   BatchTime 0.313698   LR 0.000088
0.67976904
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
0.67967898
tensor(0.2619, device='cuda:0', grad_fn=<AddBackward0>)
0.67949027
tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>)
0.67933142
tensor(0.2338, device='cuda:0', grad_fn=<AddBackward0>)
0.67938828
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
0.67911488
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
0.67881548
tensor(0.2560, device='cuda:0', grad_fn=<AddBackward0>)
0.67860550
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
0.67872351
tensor(0.2283, device='cuda:0', grad_fn=<AddBackward0>)
0.67865717
tensor(0.2573, device='cuda:0', grad_fn=<AddBackward0>)
0.67833084
tensor(0.2686, device='cuda:0', grad_fn=<AddBackward0>)
0.67846757
tensor(0.2437, device='cuda:0', grad_fn=<AddBackward0>)
0.67846352
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.67848951
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.67864949
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
0.67874014
tensor(0.2080, device='cuda:0', grad_fn=<AddBackward0>)
0.67862564
tensor(0.2255, device='cuda:0', grad_fn=<AddBackward0>)
0.67852312
tensor(0.2409, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 92.130    Top5: 99.866    Loss: 0.231
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.67841518
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
0.67853504
tensor(0.2805, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 0.373534   Top1 88.105469   Top5 99.375000   BatchTime 0.112836
INFO - Validation [21][   40/   40]   Loss 0.371413   Top1 88.200000   Top5 99.530000   BatchTime 0.082122
INFO - ==> Top1: 88.200    Top5: 99.530    Loss: 0.371
INFO - ==> Sparsity : 0.552
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 88.200   Top5: 99.530]
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 87.320   Top5: 99.570]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 87.300   Top5: 99.560]
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.1348)
features.1.conv.0 tensor(0.0618)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0942)
features.2.conv.0 tensor(0.1398)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.5544)
features.3.conv.0 tensor(0.0810)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1111)
features.4.conv.0 tensor(0.0625)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1820)
features.5.conv.0 tensor(0.3188)
features.5.conv.3 tensor(0.4213)
features.5.conv.6 tensor(0.1076)
features.6.conv.0 tensor(0.0485)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0849)
features.7.conv.0 tensor(0.1741)
features.7.conv.3 tensor(0.4586)
features.7.conv.6 tensor(0.1929)
features.8.conv.0 tensor(0.5785)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.1746)
features.9.conv.0 tensor(0.5227)
features.9.conv.3 tensor(0.5584)
features.9.conv.6 tensor(0.1305)
features.10.conv.0 tensor(0.0653)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.0978)
features.11.conv.0 tensor(0.7452)
features.11.conv.3 tensor(0.6414)
features.11.conv.6 tensor(0.5689)
features.12.conv.0 tensor(0.7314)
features.12.conv.3 tensor(0.6728)
features.12.conv.6 tensor(0.6573)
features.13.conv.0 tensor(0.2204)
features.13.conv.3 tensor(0.4921)
features.13.conv.6 tensor(0.0936)
features.14.conv.0 tensor(0.8968)
features.14.conv.3 tensor(0.8265)
features.14.conv.6 tensor(0.9564)
features.15.conv.0 tensor(0.8660)
features.15.conv.3 tensor(0.8365)
features.15.conv.6 tensor(0.9587)
features.16.conv.0 tensor(0.6427)
features.16.conv.3 tensor(0.8073)
features.16.conv.6 tensor(0.7815)
conv.0 tensor(0.0919)
tensor(1207944.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.67825818
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
0.67766529
tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)
0.67749566
tensor(0.2279, device='cuda:0', grad_fn=<AddBackward0>)
0.67740023
tensor(0.2442, device='cuda:0', grad_fn=<AddBackward0>)
0.67734843
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
0.67736793
tensor(0.2364, device='cuda:0', grad_fn=<AddBackward0>)
0.67725539
tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)
0.67739844
tensor(0.2685, device='cuda:0', grad_fn=<AddBackward0>)
0.67712450
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.67720360
tensor(0.2071, device='cuda:0', grad_fn=<AddBackward0>)
0.67703992
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
0.67696953
tensor(0.1996, device='cuda:0', grad_fn=<AddBackward0>)
0.67684180
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
0.67654419
tensor(0.2072, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][   20/  196]   Loss 0.208071   Top1 93.164062   Top5 99.902344   BatchTime 0.334968   LR 0.000085
0.67651683
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
0.67628187
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.67634922
tensor(0.2455, device='cuda:0', grad_fn=<AddBackward0>)
0.67631388
tensor(0.2632, device='cuda:0', grad_fn=<AddBackward0>)
0.67620540
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
0.67575276
tensor(0.2831, device='cuda:0', grad_fn=<AddBackward0>)
0.67515814
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
0.67442966
tensor(0.2526, device='cuda:0', grad_fn=<AddBackward0>)
0.67410821
tensor(0.2255, device='cuda:0', grad_fn=<AddBackward0>)
0.67381370
tensor(0.3119, device='cuda:0', grad_fn=<AddBackward0>)
0.67381507
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
0.67379177
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.67377543
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
0.67340517
tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)
0.67372775
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.67356241
tensor(0.2419, device='cuda:0', grad_fn=<AddBackward0>)
0.67331558
tensor(0.1982, device='cuda:0', grad_fn=<AddBackward0>)
0.67310971
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
0.67314583
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.67298418
tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)
0.67270672
tensor(0.2273, device='cuda:0', grad_fn=<AddBackward0>)
0.67246425
tensor(0.2162, device='cuda:0', grad_fn=<AddBackward0>)
0.67201614
tensor(0.2169, device='cuda:0', grad_fn=<AddBackward0>)
0.67181247
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
0.67182016
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
0.67182446
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][   40/  196]   Loss 0.205114   Top1 92.939453   Top5 99.912109   BatchTime 0.319539   LR 0.000083
0.67193216
tensor(0.2199, device='cuda:0', grad_fn=<AddBackward0>)
0.67161983
tensor(0.2631, device='cuda:0', grad_fn=<AddBackward0>)
0.67150897
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
0.67123932
tensor(0.2059, device='cuda:0', grad_fn=<AddBackward0>)
0.67104584
tensor(0.2202, device='cuda:0', grad_fn=<AddBackward0>)
0.67099053
tensor(0.2423, device='cuda:0', grad_fn=<AddBackward0>)
0.67085129
tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)
0.67051786
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.67033726
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.67026091
tensor(0.2504, device='cuda:0', grad_fn=<AddBackward0>)
0.67005944
tensor(0.3105, device='cuda:0', grad_fn=<AddBackward0>)
0.67001969
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.67002445
tensor(0.1654, device='cuda:0', grad_fn=<AddBackward0>)
0.67004508
tensor(0.2250, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][   60/  196]   Loss 0.209421   Top1 92.968750   Top5 99.895833   BatchTime 0.310365   LR 0.000081
0.67011344
tensor(0.2490, device='cuda:0', grad_fn=<AddBackward0>)
0.67069870
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.67039025
tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)
0.67017484
tensor(0.3092, device='cuda:0', grad_fn=<AddBackward0>)
0.67000937
tensor(0.2561, device='cuda:0', grad_fn=<AddBackward0>)
0.67134035
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
0.67113340
tensor(0.2459, device='cuda:0', grad_fn=<AddBackward0>)
0.67084271
tensor(0.3042, device='cuda:0', grad_fn=<AddBackward0>)
0.67084742
tensor(0.2449, device='cuda:0', grad_fn=<AddBackward0>)
0.67092222
tensor(0.3209, device='cuda:0', grad_fn=<AddBackward0>)
0.67101085
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
0.67072117
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
0.67048192
tensor(0.1979, device='cuda:0', grad_fn=<AddBackward0>)
0.67035723
tensor(0.2045, device='cuda:0', grad_fn=<AddBackward0>)
0.67022949
tensor(0.2868, device='cuda:0', grad_fn=<AddBackward0>)
0.67012304
tensor(0.2643, device='cuda:0', grad_fn=<AddBackward0>)
0.67003137
tensor(0.2704, device='cuda:0', grad_fn=<AddBackward0>)
0.66992730
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
0.67006940
tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)
0.67031151
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
0.67035723
tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)
0.67028797
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][   80/  196]   Loss 0.213073   Top1 92.866211   Top5 99.916992   BatchTime 0.301393   LR 0.000079
0.67036110
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
0.67034423
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
0.67041725
tensor(0.2903, device='cuda:0', grad_fn=<AddBackward0>)
0.67024446
tensor(0.2163, device='cuda:0', grad_fn=<AddBackward0>)
0.67049128
tensor(0.1969, device='cuda:0', grad_fn=<AddBackward0>)
0.67058432
tensor(0.2512, device='cuda:0', grad_fn=<AddBackward0>)
0.67036253
tensor(0.1693, device='cuda:0', grad_fn=<AddBackward0>)
0.67014694
tensor(0.2113, device='cuda:0', grad_fn=<AddBackward0>)
0.67004442
tensor(0.2000, device='cuda:0', grad_fn=<AddBackward0>)
0.67006910
tensor(0.2294, device='cuda:0', grad_fn=<AddBackward0>)
0.66974819
tensor(0.2287, device='cuda:0', grad_fn=<AddBackward0>)
0.66976476
tensor(0.2366, device='cuda:0', grad_fn=<AddBackward0>)
0.66981286
tensor(0.2126, device='cuda:0', grad_fn=<AddBackward0>)
0.66959393
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.66978633
tensor(0.2488, device='cuda:0', grad_fn=<AddBackward0>)
0.66988969
tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)
0.66969478
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
0.66974080
tensor(0.2972, device='cuda:0', grad_fn=<AddBackward0>)
0.66972148
tensor(0.2546, device='cuda:0', grad_fn=<AddBackward0>)
0.66943318
tensor(0.2650, device='cuda:0', grad_fn=<AddBackward0>)
0.66916150
tensor(0.2473, device='cuda:0', grad_fn=<AddBackward0>)
0.66916513
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][  100/  196]   Loss 0.214486   Top1 92.761719   Top5 99.910156   BatchTime 0.295945   LR 0.000077
0.66917455
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
0.66935784
tensor(0.2603, device='cuda:0', grad_fn=<AddBackward0>)
0.66942334
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
0.66923422
tensor(0.3177, device='cuda:0', grad_fn=<AddBackward0>)
0.66919631
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
0.66867739
tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)
0.66868961
tensor(0.2155, device='cuda:0', grad_fn=<AddBackward0>)
0.66888100
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.66874379
tensor(0.2388, device='cuda:0', grad_fn=<AddBackward0>)
0.66879743
tensor(0.2603, device='cuda:0', grad_fn=<AddBackward0>)
0.66855669
tensor(0.2262, device='cuda:0', grad_fn=<AddBackward0>)
0.66866970
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.66856474
tensor(0.2123, device='cuda:0', grad_fn=<AddBackward0>)
0.66864520
tensor(0.2366, device='cuda:0', grad_fn=<AddBackward0>)
0.66884446
tensor(0.2802, device='cuda:0', grad_fn=<AddBackward0>)
0.66881740
tensor(0.2522, device='cuda:0', grad_fn=<AddBackward0>)
0.66863018
tensor(0.2206, device='cuda:0', grad_fn=<AddBackward0>)
0.66874832
tensor(0.2857, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][  120/  196]   Loss 0.216516   Top1 92.714844   Top5 99.905599   BatchTime 0.300440   LR 0.000075
0.66875231
tensor(0.2508, device='cuda:0', grad_fn=<AddBackward0>)
0.66847706
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
0.66823846
tensor(0.3046, device='cuda:0', grad_fn=<AddBackward0>)
0.66810304
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
0.66817260
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
0.66825259
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.66836983
tensor(0.1982, device='cuda:0', grad_fn=<AddBackward0>)
0.66836292
tensor(0.2119, device='cuda:0', grad_fn=<AddBackward0>)
0.66835415
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
0.66816801
tensor(0.2219, device='cuda:0', grad_fn=<AddBackward0>)
0.66786093
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.66774172
tensor(0.2169, device='cuda:0', grad_fn=<AddBackward0>)
0.66775584
tensor(0.2504, device='cuda:0', grad_fn=<AddBackward0>)
0.66765684
tensor(0.2300, device='cuda:0', grad_fn=<AddBackward0>)
0.66762227
tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)
0.66750747
tensor(0.1808, device='cuda:0', grad_fn=<AddBackward0>)
0.66742867
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
0.66764063
tensor(0.2444, device='cuda:0', grad_fn=<AddBackward0>)
0.66781306
tensor(0.2488, device='cuda:0', grad_fn=<AddBackward0>)
0.66775489
tensor(0.2953, device='cuda:0', grad_fn=<AddBackward0>)
0.66769522
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][  140/  196]   Loss 0.216954   Top1 92.731585   Top5 99.896763   BatchTime 0.301245   LR 0.000073
0.66754836
tensor(0.2509, device='cuda:0', grad_fn=<AddBackward0>)
0.66743660
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
0.66740584
tensor(0.2398, device='cuda:0', grad_fn=<AddBackward0>)
0.66745150
tensor(0.2840, device='cuda:0', grad_fn=<AddBackward0>)
0.66734952
tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)
0.66714889
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
0.66699469
tensor(0.2908, device='cuda:0', grad_fn=<AddBackward0>)
0.66709191
tensor(0.2260, device='cuda:0', grad_fn=<AddBackward0>)
0.66740024
tensor(0.2703, device='cuda:0', grad_fn=<AddBackward0>)
0.66715175
tensor(0.2349, device='cuda:0', grad_fn=<AddBackward0>)
0.66703850
tensor(0.2532, device='cuda:0', grad_fn=<AddBackward0>)
0.66727698
tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)
0.66741371
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.66731477
tensor(0.2085, device='cuda:0', grad_fn=<AddBackward0>)
0.66746688
tensor(0.2382, device='cuda:0', grad_fn=<AddBackward0>)
0.66743875
tensor(0.2445, device='cuda:0', grad_fn=<AddBackward0>)
0.66730559
tensor(0.2369, device='cuda:0', grad_fn=<AddBackward0>)
0.66704613
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][  160/  196]   Loss 0.218032   Top1 92.695312   Top5 99.892578   BatchTime 0.303300   LR 0.000072
0.66693795
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
0.66670078
tensor(0.2085, device='cuda:0', grad_fn=<AddBackward0>)
0.66659105
tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)
0.66665816
tensor(0.2358, device='cuda:0', grad_fn=<AddBackward0>)
0.66667449
tensor(0.2328, device='cuda:0', grad_fn=<AddBackward0>)
0.66681093
tensor(0.2660, device='cuda:0', grad_fn=<AddBackward0>)
0.66674191
tensor(0.2376, device='cuda:0', grad_fn=<AddBackward0>)
0.66656911
tensor(0.2987, device='cuda:0', grad_fn=<AddBackward0>)
0.66623890
tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)
0.66606867
tensor(0.3121, device='cuda:0', grad_fn=<AddBackward0>)
0.66601193
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
0.66582733
tensor(0.2203, device='cuda:0', grad_fn=<AddBackward0>)
0.66570979
tensor(0.1930, device='cuda:0', grad_fn=<AddBackward0>)
0.66569835
tensor(0.1995, device='cuda:0', grad_fn=<AddBackward0>)
0.66601086
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
0.66620976
tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)
0.66589713
tensor(0.2107, device='cuda:0', grad_fn=<AddBackward0>)
0.66560757
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
0.66546887
tensor(0.2632, device='cuda:0', grad_fn=<AddBackward0>)
0.66516382
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.66513306
tensor(0.2405, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [22][  180/  196]   Loss 0.218689   Top1 92.688802   Top5 99.884983   BatchTime 0.300231   LR 0.000070
0.66499346
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.66503972
tensor(0.2398, device='cuda:0', grad_fn=<AddBackward0>)
0.66509724
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
0.66523266
tensor(0.2275, device='cuda:0', grad_fn=<AddBackward0>)
0.66557598
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
0.66561133
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.66529536
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
0.66551310
tensor(0.2830, device='cuda:0', grad_fn=<AddBackward0>)
0.66545451
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.66530979
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.66512424
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
0.66504270
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
0.66515499
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.66506296
tensor(0.2517, device='cuda:0', grad_fn=<AddBackward0>)
0.66531920
tensor(0.2093, device='cuda:0', grad_fn=<AddBackward0>)
0.66521686
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 92.758    Top5: 99.888    Loss: 0.217
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.66509676
tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)
0.66486621
tensor(0.2524, device='cuda:0', grad_fn=<AddBackward0>)
0.66470271
tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)
0.66473126
tensor(0.3985, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.379264   Top1 87.714844   Top5 99.472656   BatchTime 0.118893
INFO - Validation [22][   40/   40]   Loss 0.373313   Top1 87.940000   Top5 99.570000   BatchTime 0.086611
features.0.conv.0 tensor(0.3125)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0632)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0929)
features.2.conv.0 tensor(0.1461)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.5570)
features.3.conv.0 tensor(0.0790)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1089)
features.4.conv.0 tensor(0.0592)
features.4.conv.3 tensor(0.2998)
features.4.conv.6 tensor(0.1849)
features.5.conv.0 tensor(0.3431)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.1055)
features.6.conv.0 tensor(0.0461)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.1759)
features.7.conv.3 tensor(0.4566)
features.7.conv.6 tensor(0.1956)
features.8.conv.0 tensor(0.5841)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.2785)
features.9.conv.0 tensor(0.5171)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.1298)
features.10.conv.0 tensor(0.0646)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0987)
features.11.conv.0 tensor(0.7454)
features.11.conv.3 tensor(0.6406)
features.11.conv.6 tensor(0.7129)
features.12.conv.0 tensor(0.7579)
features.12.conv.3 tensor(0.6713)
INFO - ==> Top1: 87.940    Top5: 99.570    Loss: 0.373
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 88.200   Top5: 99.530]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 87.940   Top5: 99.570]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 87.320   Top5: 99.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
features.12.conv.6 tensor(0.6855)
features.13.conv.0 tensor(0.2091)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.0936)
features.14.conv.0 tensor(0.9021)
features.14.conv.3 tensor(0.8260)
features.14.conv.6 tensor(0.9560)
features.15.conv.0 tensor(0.8688)
features.15.conv.3 tensor(0.8362)
features.15.conv.6 tensor(0.9594)
features.16.conv.0 tensor(0.6463)
features.16.conv.3 tensor(0.8061)
features.16.conv.6 tensor(0.8471)
conv.0 tensor(0.0912)
tensor(1242765.) 2188896.0
0.66455728
tensor(0.1985, device='cuda:0', grad_fn=<AddBackward0>)
0.66434789
tensor(0.2443, device='cuda:0', grad_fn=<AddBackward0>)
0.66438699
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.66444790
tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)
0.66452980
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.66434669
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.66446716
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
0.66426134
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
0.66436350
tensor(0.2333, device='cuda:0', grad_fn=<AddBackward0>)
0.66433030
tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)
0.66432381
tensor(0.1770, device='cuda:0', grad_fn=<AddBackward0>)
0.66413999
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
0.66403109
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
0.66376358
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.66363829
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
0.66335297
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
0.66340399
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][   20/  196]   Loss 0.188869   Top1 93.710938   Top5 99.882812   BatchTime 0.349218   LR 0.000067
0.66349286
tensor(0.1833, device='cuda:0', grad_fn=<AddBackward0>)
0.66352230
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
0.66370803
tensor(0.2468, device='cuda:0', grad_fn=<AddBackward0>)
0.66391921
tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>)
0.66387433
tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)
0.66402256
tensor(0.1691, device='cuda:0', grad_fn=<AddBackward0>)
0.66377956
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
0.66364902
tensor(0.2580, device='cuda:0', grad_fn=<AddBackward0>)
0.66372150
tensor(0.2088, device='cuda:0', grad_fn=<AddBackward0>)
0.66358608
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
0.66335106
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
0.66328961
tensor(0.2687, device='cuda:0', grad_fn=<AddBackward0>)
0.66344053
tensor(0.1900, device='cuda:0', grad_fn=<AddBackward0>)
0.66321892
tensor(0.1746, device='cuda:0', grad_fn=<AddBackward0>)
0.66314977
tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)
0.66326296
tensor(0.2442, device='cuda:0', grad_fn=<AddBackward0>)
0.66367412
tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)
0.66338068
tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)
0.66334695
tensor(0.2394, device='cuda:0', grad_fn=<AddBackward0>)
0.66309315
tensor(0.1724, device='cuda:0', grad_fn=<AddBackward0>)
0.66303056
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
0.66331744
tensor(0.2753, device='cuda:0', grad_fn=<AddBackward0>)
0.66335076
tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][   40/  196]   Loss 0.193635   Top1 93.662109   Top5 99.921875   BatchTime 0.348627   LR 0.000065
0.66330707
tensor(0.2339, device='cuda:0', grad_fn=<AddBackward0>)
0.66331464
tensor(0.2519, device='cuda:0', grad_fn=<AddBackward0>)
0.66330385
tensor(0.2923, device='cuda:0', grad_fn=<AddBackward0>)
0.66321546
tensor(0.2772, device='cuda:0', grad_fn=<AddBackward0>)
0.66321588
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
0.66323918
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
0.66325963
tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)
0.66316068
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
0.66338837
tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)
0.66335386
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
0.66290593
tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)
0.66275495
tensor(0.2061, device='cuda:0', grad_fn=<AddBackward0>)
0.66254818
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.66258281
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
0.66261697
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
0.66263455
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.66284633
tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)
0.66298532
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
0.66283530
tensor(0.2546, device='cuda:0', grad_fn=<AddBackward0>)
0.66293955
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][   60/  196]   Loss 0.197706   Top1 93.489583   Top5 99.928385   BatchTime 0.333900   LR 0.000063
0.66267926
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
0.66285479
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.66247636
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.66222912
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
0.66210961
tensor(0.2936, device='cuda:0', grad_fn=<AddBackward0>)
0.66220504
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
0.66222817
tensor(0.2592, device='cuda:0', grad_fn=<AddBackward0>)
0.66222495
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
0.66221595
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.66236532
tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)
0.66229969
tensor(0.2620, device='cuda:0', grad_fn=<AddBackward0>)
0.66200626
tensor(0.2245, device='cuda:0', grad_fn=<AddBackward0>)
0.66191965
tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)
0.66195095
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
0.66176432
tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)
0.66148025
tensor(0.1887, device='cuda:0', grad_fn=<AddBackward0>)
0.66134959
tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)
0.66125637
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
0.66094905
tensor(0.2280, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][   80/  196]   Loss 0.199150   Top1 93.305664   Top5 99.912109   BatchTime 0.328489   LR 0.000061
0.66087204
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.66087949
tensor(0.2358, device='cuda:0', grad_fn=<AddBackward0>)
0.66081041
tensor(0.2564, device='cuda:0', grad_fn=<AddBackward0>)
0.66073364
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
0.66050661
tensor(0.2826, device='cuda:0', grad_fn=<AddBackward0>)
0.66042417
tensor(0.2049, device='cuda:0', grad_fn=<AddBackward0>)
0.66040498
tensor(0.2688, device='cuda:0', grad_fn=<AddBackward0>)
0.66020983
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
0.66031986
tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)
0.66010433
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.66009367
tensor(0.2075, device='cuda:0', grad_fn=<AddBackward0>)
0.66006327
tensor(0.1940, device='cuda:0', grad_fn=<AddBackward0>)
0.66007346
tensor(0.2219, device='cuda:0', grad_fn=<AddBackward0>)
0.66033983
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
0.66021323
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.66003662
tensor(0.2477, device='cuda:0', grad_fn=<AddBackward0>)
0.65984726
tensor(0.2642, device='cuda:0', grad_fn=<AddBackward0>)
0.65980977
tensor(0.1878, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][  100/  196]   Loss 0.204112   Top1 93.144531   Top5 99.910156   BatchTime 0.330253   LR 0.000060
0.66004926
tensor(0.2851, device='cuda:0', grad_fn=<AddBackward0>)
0.66039389
tensor(0.1814, device='cuda:0', grad_fn=<AddBackward0>)
0.66043115
tensor(0.2548, device='cuda:0', grad_fn=<AddBackward0>)
0.66023827
tensor(0.2376, device='cuda:0', grad_fn=<AddBackward0>)
0.66001838
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
0.66051817
tensor(0.2426, device='cuda:0', grad_fn=<AddBackward0>)
0.66021973
tensor(0.2678, device='cuda:0', grad_fn=<AddBackward0>)
0.65974134
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
0.65965909
tensor(0.2521, device='cuda:0', grad_fn=<AddBackward0>)
0.65951145
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
0.65945870
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.65940648
tensor(0.2138, device='cuda:0', grad_fn=<AddBackward0>)
0.65952504
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
0.65934294
tensor(0.2322, device='cuda:0', grad_fn=<AddBackward0>)
0.65919495
tensor(0.2103, device='cuda:0', grad_fn=<AddBackward0>)
0.65927380
tensor(0.2002, device='cuda:0', grad_fn=<AddBackward0>)
0.65910590
tensor(0.1912, device='cuda:0', grad_fn=<AddBackward0>)
0.65891224
tensor(0.1996, device='cuda:0', grad_fn=<AddBackward0>)
0.65887719
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
0.65891504
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
0.65904289
tensor(0.2427, device='cuda:0', grad_fn=<AddBackward0>)
0.65896189
tensor(0.2570, device='cuda:0', grad_fn=<AddBackward0>)
0.65896201
tensor(0.2909, device='cuda:0', grad_fn=<AddBackward0>)
0.65892398
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][  120/  196]   Loss 0.206011   Top1 93.098958   Top5 99.905599   BatchTime 0.331566   LR 0.000058
0.65893734
tensor(0.2743, device='cuda:0', grad_fn=<AddBackward0>)
0.65881002
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.65895802
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
0.65886164
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
0.65888673
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
0.65882033
tensor(0.2421, device='cuda:0', grad_fn=<AddBackward0>)
0.65873462
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
0.65896571
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
0.65903199
tensor(0.2715, device='cuda:0', grad_fn=<AddBackward0>)
0.65881872
tensor(0.2983, device='cuda:0', grad_fn=<AddBackward0>)
0.65844166
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
0.65842414
tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)
0.65833777
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.65832645
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.65823418
tensor(0.2223, device='cuda:0', grad_fn=<AddBackward0>)
0.65827966
tensor(0.1906, device='cuda:0', grad_fn=<AddBackward0>)
0.65817744
tensor(0.1832, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][  140/  196]   Loss 0.205161   Top1 93.119420   Top5 99.905134   BatchTime 0.328759   LR 0.000056
0.65814257
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
0.65809518
tensor(0.2335, device='cuda:0', grad_fn=<AddBackward0>)
0.65857303
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
0.65844792
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
0.65842706
tensor(0.2354, device='cuda:0', grad_fn=<AddBackward0>)
0.65845871
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.65856385
tensor(0.2900, device='cuda:0', grad_fn=<AddBackward0>)
0.65848732
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
0.65838432
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
0.65808922
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
0.65778774
tensor(0.2427, device='cuda:0', grad_fn=<AddBackward0>)
0.65771228
tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)
0.65765780
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
0.65769619
tensor(0.2936, device='cuda:0', grad_fn=<AddBackward0>)
0.65760380
tensor(0.2618, device='cuda:0', grad_fn=<AddBackward0>)
0.65757543
tensor(0.2467, device='cuda:0', grad_fn=<AddBackward0>)
0.65770811
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
0.65832025
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
0.65814346
tensor(0.2014, device='cuda:0', grad_fn=<AddBackward0>)
0.65775269
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][  160/  196]   Loss 0.205862   Top1 93.117676   Top5 99.895020   BatchTime 0.326243   LR 0.000055
0.65765738
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
0.65761322
tensor(0.2328, device='cuda:0', grad_fn=<AddBackward0>)
0.65750515
tensor(0.1832, device='cuda:0', grad_fn=<AddBackward0>)
0.65758258
tensor(0.2365, device='cuda:0', grad_fn=<AddBackward0>)
0.65747702
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.65739495
tensor(0.2098, device='cuda:0', grad_fn=<AddBackward0>)
0.65731031
tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)
0.65745664
tensor(0.2067, device='cuda:0', grad_fn=<AddBackward0>)
0.65764087
tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)
0.65755004
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.65753418
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
0.65739137
tensor(0.2193, device='cuda:0', grad_fn=<AddBackward0>)
0.65754151
tensor(0.2295, device='cuda:0', grad_fn=<AddBackward0>)
0.65742630
tensor(0.2242, device='cuda:0', grad_fn=<AddBackward0>)
0.65748560
tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)
0.65756613
tensor(0.2545, device='cuda:0', grad_fn=<AddBackward0>)
0.65757167
tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)
0.65742850
tensor(0.1984, device='cuda:0', grad_fn=<AddBackward0>)
0.65741587
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
0.65738499
tensor(0.2461, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [23][  180/  196]   Loss 0.205462   Top1 93.116319   Top5 99.898003   BatchTime 0.322980   LR 0.000053
0.65756333
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
0.65771830
tensor(0.2408, device='cuda:0', grad_fn=<AddBackward0>)
0.65751046
tensor(0.2317, device='cuda:0', grad_fn=<AddBackward0>)
0.65751803
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
0.65740836
tensor(0.2295, device='cuda:0', grad_fn=<AddBackward0>)
0.65717566
tensor(0.2530, device='cuda:0', grad_fn=<AddBackward0>)
0.65715897
tensor(0.2176, device='cuda:0', grad_fn=<AddBackward0>)
0.65725923
tensor(0.2551, device='cuda:0', grad_fn=<AddBackward0>)
0.65729159
tensor(0.2603, device='cuda:0', grad_fn=<AddBackward0>)
0.65744615
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.65750676
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.65736759
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
0.65731055
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.65688771
tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 93.100    Top5: 99.892    Loss: 0.206
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.65669405
tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)
0.65659434
tensor(0.2341, device='cuda:0', grad_fn=<AddBackward0>)
0.65642208
tensor(0.2410, device='cuda:0', grad_fn=<AddBackward0>)
0.65639615
tensor(0.2484, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.368765   Top1 88.203125   Top5 99.433594   BatchTime 0.115294
INFO - Validation [23][   40/   40]   Loss 0.362099   Top1 88.520000   Top5 99.590000   BatchTime 0.085492
INFO - ==> Top1: 88.520    Top5: 99.590    Loss: 0.362
INFO - ==> Sparsity : 0.573
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 88.520   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 88.200   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 87.940   Top5: 99.570]
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.1426)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0972)
features.2.conv.0 tensor(0.1505)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.5599)
features.3.conv.0 tensor(0.0833)
features.3.conv.3 tensor(0.0810)
features.3.conv.6 tensor(0.1039)
features.4.conv.0 tensor(0.0592)
features.4.conv.3 tensor(0.2986)
features.4.conv.6 tensor(0.1847)
features.5.conv.0 tensor(0.3387)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.1045)
features.6.conv.0 tensor(0.0467)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.1763)
features.7.conv.3 tensor(0.4563)
features.7.conv.6 tensor(0.1943)
features.8.conv.0 tensor(0.6001)
features.8.conv.3 tensor(0.5434)
features.8.conv.6 tensor(0.4057)
features.9.conv.0 tensor(0.5450)
features.9.conv.3 tensor(0.5573)
features.9.conv.6 tensor(0.1270)
features.10.conv.0 tensor(0.0631)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.0976)
features.11.conv.0 tensor(0.7468)
features.11.conv.3 tensor(0.6416)
features.11.conv.6 tensor(0.7285)
features.12.conv.0 tensor(0.7487)
features.12.conv.3 tensor(0.6721)
features.12.conv.6 tensor(0.6954)
features.13.conv.0 tensor(0.2087)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.0928)
features.14.conv.0 tensor(0.9022)
features.14.conv.3 tensor(0.8262)
features.14.conv.6 tensor(0.9559)
features.15.conv.0 tensor(0.8705)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9597)
features.16.conv.0 tensor(0.6526)
features.16.conv.3 tensor(0.8056)
features.16.conv.6 tensor(0.8649)
conv.0 tensor(0.0912)
tensor(1254438.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.65637034
tensor(0.2156, device='cuda:0', grad_fn=<AddBackward0>)
0.65624803
tensor(0.2140, device='cuda:0', grad_fn=<AddBackward0>)
0.65636116
tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)
0.65666312
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
0.65671748
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.65652710
tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)
0.65653139
tensor(0.2189, device='cuda:0', grad_fn=<AddBackward0>)
0.65640497
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.65638196
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.65625113
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.65616709
tensor(0.1754, device='cuda:0', grad_fn=<AddBackward0>)
0.65620059
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
0.65609461
tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)
0.65604734
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.65581721
tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)
0.65592152
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.65578145
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
0.65545040
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][   20/  196]   Loss 0.186157   Top1 93.593750   Top5 99.960938   BatchTime 0.364336   LR 0.000050
0.65537161
tensor(0.1937, device='cuda:0', grad_fn=<AddBackward0>)
0.65537834
tensor(0.2581, device='cuda:0', grad_fn=<AddBackward0>)
0.65549856
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
0.65552014
tensor(0.2392, device='cuda:0', grad_fn=<AddBackward0>)
0.65548664
tensor(0.2475, device='cuda:0', grad_fn=<AddBackward0>)
0.65543872
tensor(0.2111, device='cuda:0', grad_fn=<AddBackward0>)
0.65582681
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
0.65549403
tensor(0.2012, device='cuda:0', grad_fn=<AddBackward0>)
0.65508944
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.65491635
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
0.65476620
tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)
0.65475655
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
0.65479225
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
0.65418154
tensor(0.2222, device='cuda:0', grad_fn=<AddBackward0>)
0.65363568
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
0.65380239
tensor(0.2290, device='cuda:0', grad_fn=<AddBackward0>)
0.65385365
tensor(0.2137, device='cuda:0', grad_fn=<AddBackward0>)
0.65354395
tensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)
0.65328109
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][   40/  196]   Loss 0.193432   Top1 93.505859   Top5 99.902344   BatchTime 0.351519   LR 0.000048
0.65305376
tensor(0.2709, device='cuda:0', grad_fn=<AddBackward0>)
0.65290105
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
0.65292931
tensor(0.2428, device='cuda:0', grad_fn=<AddBackward0>)
0.65291798
tensor(0.2221, device='cuda:0', grad_fn=<AddBackward0>)
0.65292054
tensor(0.2258, device='cuda:0', grad_fn=<AddBackward0>)
0.65320033
tensor(0.2081, device='cuda:0', grad_fn=<AddBackward0>)
0.65312290
tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)
0.65320212
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
0.65328705
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.65293401
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
0.65285289
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.65302074
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.65272421
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.65233111
tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)
0.65238971
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
0.65230554
tensor(0.2578, device='cuda:0', grad_fn=<AddBackward0>)
0.65246069
tensor(0.1665, device='cuda:0', grad_fn=<AddBackward0>)
0.65218610
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
0.65188193
tensor(0.2004, device='cuda:0', grad_fn=<AddBackward0>)
0.65159851
tensor(0.2401, device='cuda:0', grad_fn=<AddBackward0>)
0.65127653
tensor(0.2499, device='cuda:0', grad_fn=<AddBackward0>)
0.65108609
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][   60/  196]   Loss 0.193628   Top1 93.509115   Top5 99.902344   BatchTime 0.343464   LR 0.000047
INFO - Training [24][   80/  196]   Loss 0.192120   Top1 93.632812   Top5 99.907227   BatchTime 0.335272   LR 0.000045
0.65095103
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
0.65085381
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
0.65081280
tensor(0.2000, device='cuda:0', grad_fn=<AddBackward0>)
0.65094030
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.65106416
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
0.65098071
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
0.65096498
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.65076840
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
0.65062803
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
0.65052813
tensor(0.2243, device='cuda:0', grad_fn=<AddBackward0>)
0.65046871
tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)
0.65064043
tensor(0.2238, device='cuda:0', grad_fn=<AddBackward0>)
0.65075058
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
0.65085888
tensor(0.2301, device='cuda:0', grad_fn=<AddBackward0>)
0.65073216
tensor(0.2632, device='cuda:0', grad_fn=<AddBackward0>)
0.65090346
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.65085846
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
0.65068018
tensor(0.1644, device='cuda:0', grad_fn=<AddBackward0>)
0.65073311
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.65059632
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.65056622
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
0.65064776
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.65043145
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.65028280
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
0.65033650
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
0.65023190
tensor(0.2064, device='cuda:0', grad_fn=<AddBackward0>)
0.65019536
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
0.65017837
tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)
0.65001369
tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)
0.65016592
tensor(0.2220, device='cuda:0', grad_fn=<AddBackward0>)
0.65017384
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
0.65010172
tensor(0.2093, device='cuda:0', grad_fn=<AddBackward0>)
0.64997250
tensor(0.2356, device='cuda:0', grad_fn=<AddBackward0>)
0.64992154
tensor(0.1975, device='cuda:0', grad_fn=<AddBackward0>)
0.64984679
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
0.64996505
tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][  100/  196]   Loss 0.191381   Top1 93.652344   Top5 99.914062   BatchTime 0.324159   LR 0.000044
0.65015334
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.65024734
tensor(0.2326, device='cuda:0', grad_fn=<AddBackward0>)
0.65024209
tensor(0.2247, device='cuda:0', grad_fn=<AddBackward0>)
0.65013695
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.64993352
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.64980733
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.64978564
tensor(0.2071, device='cuda:0', grad_fn=<AddBackward0>)
0.64988846
tensor(0.1937, device='cuda:0', grad_fn=<AddBackward0>)
0.64973134
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
0.64960194
tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)
0.64963734
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
0.64951891
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
0.64944315
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
0.64970368
tensor(0.2516, device='cuda:0', grad_fn=<AddBackward0>)
0.64954114
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
0.64949226
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
0.64958042
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
0.64932555
tensor(0.2210, device='cuda:0', grad_fn=<AddBackward0>)
0.64908522
tensor(0.2459, device='cuda:0', grad_fn=<AddBackward0>)
0.64845425
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][  120/  196]   Loss 0.191328   Top1 93.597005   Top5 99.921875   BatchTime 0.321878   LR 0.000042
0.64812899
tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)
0.64831913
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
0.64831609
tensor(0.2402, device='cuda:0', grad_fn=<AddBackward0>)
0.64815521
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.64797980
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
0.64786845
tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)
0.64763159
tensor(0.2060, device='cuda:0', grad_fn=<AddBackward0>)
0.64769095
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.64762837
tensor(0.2155, device='cuda:0', grad_fn=<AddBackward0>)
0.64772987
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.64762610
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
0.64776665
tensor(0.2283, device='cuda:0', grad_fn=<AddBackward0>)
0.64803773
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.64801002
tensor(0.1957, device='cuda:0', grad_fn=<AddBackward0>)
0.64788550
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.64780229
tensor(0.1887, device='cuda:0', grad_fn=<AddBackward0>)
0.64773154
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
0.64783734
tensor(0.2094, device='cuda:0', grad_fn=<AddBackward0>)
0.64769161
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.64763951
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
0.64765352
tensor(0.2809, device='cuda:0', grad_fn=<AddBackward0>)
0.64762938
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.64743638
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
0.64742202
tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][  140/  196]   Loss 0.190913   Top1 93.596540   Top5 99.927455   BatchTime 0.323654   LR 0.000041
0.64733070
tensor(0.1956, device='cuda:0', grad_fn=<AddBackward0>)
0.64716864
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
0.64696217
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
0.64688659
tensor(0.1890, device='cuda:0', grad_fn=<AddBackward0>)
0.64708400
tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)
0.64731044
tensor(0.1825, device='cuda:0', grad_fn=<AddBackward0>)
0.64742714
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
0.64699590
tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)
0.64708495
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.64715898
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.64734244
tensor(0.2068, device='cuda:0', grad_fn=<AddBackward0>)
0.64728749
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
0.64715004
tensor(0.2262, device='cuda:0', grad_fn=<AddBackward0>)
0.64704043
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.64729494
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.64705127
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.64663494
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
0.64682651
tensor(0.2312, device='cuda:0', grad_fn=<AddBackward0>)
0.64698416
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.64688408
tensor(0.2157, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][  160/  196]   Loss 0.188858   Top1 93.654785   Top5 99.931641   BatchTime 0.320415   LR 0.000039
0.64690965
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.64684331
tensor(0.2435, device='cuda:0', grad_fn=<AddBackward0>)
0.64690220
tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)
0.64678782
tensor(0.2551, device='cuda:0', grad_fn=<AddBackward0>)
0.64677578
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
0.64671046
tensor(0.1677, device='cuda:0', grad_fn=<AddBackward0>)
0.64655358
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
0.64645189
tensor(0.2385, device='cuda:0', grad_fn=<AddBackward0>)
0.64659697
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
0.64646912
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.64640933
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
0.64640838
tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)
0.64651537
tensor(0.2763, device='cuda:0', grad_fn=<AddBackward0>)
0.64618933
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
0.64619511
tensor(0.1824, device='cuda:0', grad_fn=<AddBackward0>)
0.64632004
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
0.64627439
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.64637375
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
0.64642423
tensor(0.2125, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [24][  180/  196]   Loss 0.189512   Top1 93.665365   Top5 99.919705   BatchTime 0.319320   LR 0.000038
0.64641178
tensor(0.2300, device='cuda:0', grad_fn=<AddBackward0>)
0.64648718
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.64629894
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
0.64599580
tensor(0.1954, device='cuda:0', grad_fn=<AddBackward0>)
0.64592540
tensor(0.2366, device='cuda:0', grad_fn=<AddBackward0>)
0.64595032
tensor(0.2085, device='cuda:0', grad_fn=<AddBackward0>)
0.64599508
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.64597547
tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)
0.64582527
tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)
0.64585978
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
0.64585847
tensor(0.2816, device='cuda:0', grad_fn=<AddBackward0>)
0.64580303
tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)
0.64568669
tensor(0.2341, device='cuda:0', grad_fn=<AddBackward0>)
0.64564413
INFO - ==> Top1: 93.618    Top5: 99.918    Loss: 0.191
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.2611, device='cuda:0', grad_fn=<AddBackward0>)
0.64558345
tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)
0.64550418
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.64567465
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.64552754
tensor(0.2454, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [24][   20/   40]   Loss 0.371448   Top1 88.242188   Top5 99.492188   BatchTime 0.116121
INFO - Validation [24][   40/   40]   Loss 0.362181   Top1 88.500000   Top5 99.620000   BatchTime 0.084432
INFO - ==> Top1: 88.500    Top5: 99.620    Loss: 0.362
INFO - ==> Sparsity : 0.579
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 88.520   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 88.500   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 88.200   Top5: 99.530]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0592)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0955)
features.2.conv.0 tensor(0.1539)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.5616)
features.3.conv.0 tensor(0.0830)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1072)
features.4.conv.0 tensor(0.0607)
features.4.conv.3 tensor(0.2986)
features.4.conv.6 tensor(0.1862)
features.5.conv.0 tensor(0.3490)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.1390)
features.6.conv.0 tensor(0.0454)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0856)
features.7.conv.0 tensor(0.1790)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.1929)
features.8.conv.0 tensor(0.6071)
features.8.conv.3 tensor(0.5425)
features.8.conv.6 tensor(0.4709)
features.9.conv.0 tensor(0.5450)
features.9.conv.3 tensor(0.5573)
features.9.conv.6 tensor(0.1359)
features.10.conv.0 tensor(0.0618)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0973)
features.11.conv.0 tensor(0.7496)
features.11.conv.3 tensor(0.6414)
features.11.conv.6 tensor(0.7424)
features.12.conv.0 tensor(0.7598)
features.12.conv.3 tensor(0.6723)
features.12.conv.6 tensor(0.7033)
features.13.conv.0 tensor(0.2084)
features.13.conv.3 tensor(0.4907)
features.13.conv.6 tensor(0.0924)
features.14.conv.0 tensor(0.9027)
features.14.conv.3 tensor(0.8264)
features.14.conv.6 tensor(0.9568)
features.15.conv.0 tensor(0.8717)
features.15.conv.3 tensor(0.8362)
features.15.conv.6 tensor(0.9604)
features.16.conv.0 tensor(0.6563)
features.16.conv.3 tensor(0.8054)
features.16.conv.6 tensor(0.8718)
conv.0 tensor(0.1035)
tensor(1266894.) 2188896.0
0.64558423
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.64563644
tensor(0.2129, device='cuda:0', grad_fn=<AddBackward0>)
0.64545870
tensor(0.1488, device='cuda:0', grad_fn=<AddBackward0>)
0.64546138
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.64538956
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
0.64540267
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
0.64543581
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
0.64542347
tensor(0.2131, device='cuda:0', grad_fn=<AddBackward0>)
0.64537239
tensor(0.2160, device='cuda:0', grad_fn=<AddBackward0>)
0.64522576
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
0.64542508
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
0.64543521
tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)
0.64539856
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
0.64521313
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
0.64495492
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][   20/  196]   Loss 0.171508   Top1 94.296875   Top5 99.902344   BatchTime 0.323422   LR 0.000035
0.64503908
tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)
0.64512694
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
0.64513665
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
0.64496207
tensor(0.1664, device='cuda:0', grad_fn=<AddBackward0>)
0.64497429
tensor(0.2004, device='cuda:0', grad_fn=<AddBackward0>)
0.64467770
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.64453578
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
0.64454991
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.64455944
tensor(0.2473, device='cuda:0', grad_fn=<AddBackward0>)
0.64451945
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
0.64451712
tensor(0.2119, device='cuda:0', grad_fn=<AddBackward0>)
0.64465320
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
0.64460802
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
0.64467132
tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)
0.64440525
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
0.64430493
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
0.64418185
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
0.64403230
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
0.64364135
tensor(0.1846, device='cuda:0', grad_fn=<AddBackward0>)
0.64355397
tensor(0.1961, device='cuda:0', grad_fn=<AddBackward0>)
0.64354718
tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)
0.64341635
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
0.64340097
tensor(0.2486, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][   40/  196]   Loss 0.173591   Top1 94.111328   Top5 99.931641   BatchTime 0.298027   LR 0.000034
0.64349514
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
0.64325309
tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)
0.64292020
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.64245731
tensor(0.2277, device='cuda:0', grad_fn=<AddBackward0>)
0.64213979
tensor(0.2128, device='cuda:0', grad_fn=<AddBackward0>)
0.64198834
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
0.64184290
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
0.64181739
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.64179051
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
0.64171559
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
0.64162242
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
0.64175063
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
0.64171743
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
0.64179176
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.64194590
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
0.64183325
tensor(0.2141, device='cuda:0', grad_fn=<AddBackward0>)
0.64183080
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.64187688
tensor(0.2549, device='cuda:0', grad_fn=<AddBackward0>)
0.64170492
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
0.64177048
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.64183581
tensor(0.1893, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][   60/  196]   Loss 0.175921   Top1 94.205729   Top5 99.908854   BatchTime 0.291900   LR 0.000033
0.64183897
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
0.64169163
tensor(0.1847, device='cuda:0', grad_fn=<AddBackward0>)
0.64167011
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
0.64186651
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
0.64190179
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
0.64166516
tensor(0.2028, device='cuda:0', grad_fn=<AddBackward0>)
0.64125657
tensor(0.2081, device='cuda:0', grad_fn=<AddBackward0>)
0.64131963
tensor(0.1697, device='cuda:0', grad_fn=<AddBackward0>)
0.64136708
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
0.64136398
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.64107424
tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)
0.64090657
tensor(0.2536, device='cuda:0', grad_fn=<AddBackward0>)
0.64087242
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.64098865
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.64060104
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.64048892
tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)
0.64083952
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
0.64063889
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
0.64016891
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
0.63977563
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
0.63960892
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][   80/  196]   Loss 0.176979   Top1 94.091797   Top5 99.921875   BatchTime 0.293420   LR 0.000031
0.63930666
tensor(0.2154, device='cuda:0', grad_fn=<AddBackward0>)
0.63924575
tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)
0.63929898
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
0.63913536
tensor(0.2411, device='cuda:0', grad_fn=<AddBackward0>)
0.63889641
tensor(0.2319, device='cuda:0', grad_fn=<AddBackward0>)
0.63888270
tensor(0.1969, device='cuda:0', grad_fn=<AddBackward0>)
0.63908088
tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)
0.63902068
tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)
0.63890457
tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)
0.63889384
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
0.63875175
tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)
0.63856405
tensor(0.2054, device='cuda:0', grad_fn=<AddBackward0>)
0.63850355
tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)
0.63843584
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
0.63846904
tensor(0.2402, device='cuda:0', grad_fn=<AddBackward0>)
0.63859504
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][  100/  196]   Loss 0.179737   Top1 94.007812   Top5 99.917969   BatchTime 0.284086   LR 0.000030
0.63870829
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
0.63866472
tensor(0.2036, device='cuda:0', grad_fn=<AddBackward0>)
0.63855171
tensor(0.1724, device='cuda:0', grad_fn=<AddBackward0>)
0.63852477
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
0.63855350
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.63846284
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.63841373
tensor(0.2295, device='cuda:0', grad_fn=<AddBackward0>)
0.63841867
tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)
0.63847196
tensor(0.1976, device='cuda:0', grad_fn=<AddBackward0>)
0.63832450
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
0.63832051
tensor(0.2360, device='cuda:0', grad_fn=<AddBackward0>)
0.63830435
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
0.63815194
tensor(0.2348, device='cuda:0', grad_fn=<AddBackward0>)
0.63799220
tensor(0.2280, device='cuda:0', grad_fn=<AddBackward0>)
0.63782173
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
0.63764912
tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)
0.63752037
tensor(0.2176, device='cuda:0', grad_fn=<AddBackward0>)
0.63746548
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
0.63745880
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
0.63753277
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
0.63763034
tensor(0.1814, device='cuda:0', grad_fn=<AddBackward0>)
0.63771534
tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][  120/  196]   Loss 0.181321   Top1 93.955078   Top5 99.925130   BatchTime 0.284312   LR 0.000029
0.63768882
tensor(0.1821, device='cuda:0', grad_fn=<AddBackward0>)
0.63773084
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.63769275
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
0.63759083
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
0.63745093
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.63731688
tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)
0.63730741
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
0.63740206
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
0.63759178
tensor(0.2258, device='cuda:0', grad_fn=<AddBackward0>)
0.63759750
tensor(0.2746, device='cuda:0', grad_fn=<AddBackward0>)
0.63764596
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.63777816
tensor(0.1997, device='cuda:0', grad_fn=<AddBackward0>)
0.63743019
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
0.63724107
tensor(0.1980, device='cuda:0', grad_fn=<AddBackward0>)
0.63703156
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
0.63690048
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
0.63682038
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][  140/  196]   Loss 0.182014   Top1 93.922991   Top5 99.921875   BatchTime 0.292476   LR 0.000027
0.63680428
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
0.63676244
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
0.63685960
tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)
0.63682705
tensor(0.2212, device='cuda:0', grad_fn=<AddBackward0>)
0.63686717
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.63711286
tensor(0.2103, device='cuda:0', grad_fn=<AddBackward0>)
0.63736385
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
0.63725466
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
0.63730526
tensor(0.2100, device='cuda:0', grad_fn=<AddBackward0>)
0.63724589
tensor(0.1976, device='cuda:0', grad_fn=<AddBackward0>)
0.63704467
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
0.63684082
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
0.63671738
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
0.63673359
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
0.63680738
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
0.63682330
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
0.63697791
tensor(0.1644, device='cuda:0', grad_fn=<AddBackward0>)
0.63717693
tensor(0.2145, device='cuda:0', grad_fn=<AddBackward0>)
0.63708842
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
0.63679206
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
0.63641727
tensor(0.2069, device='cuda:0', grad_fn=<AddBackward0>)
0.63580179
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
0.63533056
tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)
0.63526303
tensor(0.1783, device='cuda:0', grad_fn=<AddBackward0>)
0.63512254
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][  160/  196]   Loss 0.181317   Top1 93.981934   Top5 99.907227   BatchTime 0.296955   LR 0.000026
0.63504940
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
0.63504297
tensor(0.2098, device='cuda:0', grad_fn=<AddBackward0>)
0.63516754
tensor(0.2107, device='cuda:0', grad_fn=<AddBackward0>)
0.63516390
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
0.63515049
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.63518614
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.63520157
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.63505971
tensor(0.2174, device='cuda:0', grad_fn=<AddBackward0>)
0.63504189
tensor(0.2734, device='cuda:0', grad_fn=<AddBackward0>)
0.63501602
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
0.63499272
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.63500416
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.63480240
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
0.63473672
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
0.63466907
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
0.63472772
tensor(0.2322, device='cuda:0', grad_fn=<AddBackward0>)
0.63470840
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
0.63475388
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [25][  180/  196]   Loss 0.181211   Top1 93.977865   Top5 99.904514   BatchTime 0.300518   LR 0.000025
0.63461971
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.63461334
tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)
0.63464558
tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)
0.63488847
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
0.63487589
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.63495517
tensor(0.1746, device='cuda:0', grad_fn=<AddBackward0>)
0.63489449
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
0.63496363
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
0.63490731
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
0.63469851
tensor(0.2088, device='cuda:0', grad_fn=<AddBackward0>)
0.63456279
tensor(0.1850, device='cuda:0', grad_fn=<AddBackward0>)
0.63447875
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
0.63451761
tensor(0.2292, device='cuda:0', grad_fn=<AddBackward0>)
0.63456070
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
0.63449472
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
0.63442308
tensor(0.2294, device='cuda:0', grad_fn=<AddBackward0>)
0.63436180
tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)
0.63431311
tensor(0.2944, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 93.986    Top5: 99.906    Loss: 0.181
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 0.356703   Top1 88.730469   Top5 99.492188   BatchTime 0.121731
INFO - Validation [25][   40/   40]   Loss 0.351122   Top1 88.870000   Top5 99.610000   BatchTime 0.087202
INFO - ==> Top1: 88.870    Top5: 99.610    Loss: 0.351
INFO - ==> Sparsity : 0.586
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 88.520   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 88.500   Top5: 99.620]
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.1602)
features.1.conv.0 tensor(0.0579)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0955)
features.2.conv.0 tensor(0.1519)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.5637)
features.3.conv.0 tensor(0.0845)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1085)
features.4.conv.0 tensor(0.0573)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.1860)
features.5.conv.0 tensor(0.3581)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.1507)
features.6.conv.0 tensor(0.0443)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0854)
features.7.conv.0 tensor(0.1800)
features.7.conv.3 tensor(0.4563)
features.7.conv.6 tensor(0.2185)
features.8.conv.0 tensor(0.6106)
features.8.conv.3 tensor(0.5422)
features.8.conv.6 tensor(0.4889)
features.9.conv.0 tensor(0.5798)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.1434)
features.10.conv.0 tensor(0.0624)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0978)
features.11.conv.0 tensor(0.7507)
features.11.conv.3 tensor(0.6418)
features.11.conv.6 tensor(0.7460)
features.12.conv.0 tensor(0.7724)
features.12.conv.3 tensor(0.6726)
features.12.conv.6 tensor(0.7081)
features.13.conv.0 tensor(0.2239)
features.13.conv.3 tensor(0.4894)
features.13.conv.6 tensor(0.1347)
features.14.conv.0 tensor(0.9039)
features.14.conv.3 tensor(0.8262)
features.14.conv.6 tensor(0.9570)
features.15.conv.0 tensor(0.8732)
features.15.conv.3 tensor(0.8361)
features.15.conv.6 tensor(0.9611)
features.16.conv.0 tensor(0.6617)
features.16.conv.3 tensor(0.8052)
features.16.conv.6 tensor(0.8773)
conv.0 tensor(0.1131)
tensor(1282245.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
0.63412982
tensor(0.2315, device='cuda:0', grad_fn=<AddBackward0>)
0.63394129
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
0.63358498
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.63320869
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
0.63295126
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
0.63290542
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
0.63286144
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
0.63279825
tensor(0.2189, device='cuda:0', grad_fn=<AddBackward0>)
0.63288641
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
0.63285595
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
0.63281858
tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)
0.63265979
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
0.63263398
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
0.63260967
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
0.63262886
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
0.63255388
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
0.63244784
tensor(0.2899, device='cuda:0', grad_fn=<AddBackward0>)
0.63240957
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
0.63236278
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][   20/  196]   Loss 0.172121   Top1 94.042969   Top5 99.882812   BatchTime 0.379040   LR 0.000023
0.63234824
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
0.63234454
tensor(0.1834, device='cuda:0', grad_fn=<AddBackward0>)
0.63225383
tensor(0.2034, device='cuda:0', grad_fn=<AddBackward0>)
0.63226980
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
0.63235343
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
0.63236344
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.63225347
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
0.63229173
tensor(0.1949, device='cuda:0', grad_fn=<AddBackward0>)
0.63216490
tensor(0.2059, device='cuda:0', grad_fn=<AddBackward0>)
0.63211960
tensor(0.2573, device='cuda:0', grad_fn=<AddBackward0>)
0.63212812
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.63216817
tensor(0.1975, device='cuda:0', grad_fn=<AddBackward0>)
0.63217193
tensor(0.2010, device='cuda:0', grad_fn=<AddBackward0>)
0.63214761
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
0.63222802
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
0.63240969
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
0.63237041
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
0.63232148
tensor(0.2055, device='cuda:0', grad_fn=<AddBackward0>)
0.63217592
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
0.63210064
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
0.63205230
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][   40/  196]   Loss 0.170522   Top1 94.326172   Top5 99.873047   BatchTime 0.331638   LR 0.000022
0.63200337
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
0.63198620
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
0.63204956
tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)
0.63216537
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
0.63214350
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.63209587
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
0.63215077
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.63203645
tensor(0.2104, device='cuda:0', grad_fn=<AddBackward0>)
0.63191748
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.63185620
tensor(0.1891, device='cuda:0', grad_fn=<AddBackward0>)
0.63183266
tensor(0.2156, device='cuda:0', grad_fn=<AddBackward0>)
0.63184673
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
0.63184720
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
0.63183653
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
0.63192475
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
0.63189340
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
0.63191158
tensor(0.2124, device='cuda:0', grad_fn=<AddBackward0>)
0.63179082
INFO - Training [26][   60/  196]   Loss 0.169792   Top1 94.375000   Top5 99.889323   BatchTime 0.334942   LR 0.000021
tensor(0.1668, device='cuda:0', grad_fn=<AddBackward0>)
0.63166630
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
0.63163388
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
0.63162577
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
0.63169789
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.63168973
tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)
0.63171536
tensor(0.1644, device='cuda:0', grad_fn=<AddBackward0>)
0.63171577
tensor(0.2204, device='cuda:0', grad_fn=<AddBackward0>)
0.63157046
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.63148540
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
0.63140923
tensor(0.2892, device='cuda:0', grad_fn=<AddBackward0>)
0.63127035
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
0.63122052
tensor(0.2658, device='cuda:0', grad_fn=<AddBackward0>)
0.63126224
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.63133264
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
0.63124591
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.63129085
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.63124925
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
0.63119352
tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][   80/  196]   Loss 0.173741   Top1 94.228516   Top5 99.892578   BatchTime 0.332695   LR 0.000019
0.63120621
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
0.63126677
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
0.63136947
tensor(0.2433, device='cuda:0', grad_fn=<AddBackward0>)
0.63150662
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
0.63157016
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
0.63158852
tensor(0.2354, device='cuda:0', grad_fn=<AddBackward0>)
0.63149542
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
0.63141870
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
0.63135737
tensor(0.1947, device='cuda:0', grad_fn=<AddBackward0>)
0.63129568
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
0.63133043
tensor(0.2903, device='cuda:0', grad_fn=<AddBackward0>)
0.63133198
tensor(0.2123, device='cuda:0', grad_fn=<AddBackward0>)
0.63135540
tensor(0.1941, device='cuda:0', grad_fn=<AddBackward0>)
0.63144290
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.63149118
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.63147151
tensor(0.2191, device='cuda:0', grad_fn=<AddBackward0>)
0.63140398
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
0.63133651
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
0.63122058
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
0.63117194
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
0.63114846
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.63112485
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
0.63115871
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][  100/  196]   Loss 0.173141   Top1 94.210938   Top5 99.910156   BatchTime 0.334940   LR 0.000018
0.63124412
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
0.63128316
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
0.63128811
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
0.63129896
tensor(0.2439, device='cuda:0', grad_fn=<AddBackward0>)
0.63131875
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
0.63133895
tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)
0.63131535
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.63119239
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
0.63115984
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.63108623
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
0.63101590
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
0.63101488
tensor(0.2363, device='cuda:0', grad_fn=<AddBackward0>)
0.63101822
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
0.63105339
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.63103729
tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>)
0.63098764
tensor(0.1654, device='cuda:0', grad_fn=<AddBackward0>)
0.63096136
tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)
0.63094330
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
0.63085508
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][  120/  196]   Loss 0.174275   Top1 94.166667   Top5 99.895833   BatchTime 0.334137   LR 0.000017
0.63069141
tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)
0.63051999
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.63016820
tensor(0.1693, device='cuda:0', grad_fn=<AddBackward0>)
0.62981850
tensor(0.2275, device='cuda:0', grad_fn=<AddBackward0>)
0.62983090
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.62976074
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
0.62969834
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.62960941
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
0.62946498
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
0.62941146
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
0.62935841
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.62931478
tensor(0.1984, device='cuda:0', grad_fn=<AddBackward0>)
0.62926251
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
0.62922591
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.62919921
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
0.62922078
tensor(0.2255, device='cuda:0', grad_fn=<AddBackward0>)
0.62926829
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][  140/  196]   Loss 0.172806   Top1 94.229911   Top5 99.896763   BatchTime 0.337603   LR 0.000016
0.62933159
tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)
0.62953502
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
0.62940735
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
0.62935448
tensor(0.1668, device='cuda:0', grad_fn=<AddBackward0>)
0.62935460
tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)
0.62927705
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
0.62921298
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.62912750
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
0.62908554
tensor(0.1774, device='cuda:0', grad_fn=<AddBackward0>)
0.62905771
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
0.62904584
tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)
0.62910032
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.62919235
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
0.62922573
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
0.62907153
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
0.62907255
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
0.62909251
tensor(0.2060, device='cuda:0', grad_fn=<AddBackward0>)
0.62907809
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
0.62911463
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
0.62907332
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][  160/  196]   Loss 0.171591   Top1 94.272461   Top5 99.899902   BatchTime 0.331415   LR 0.000015
0.62903446
tensor(0.1932, device='cuda:0', grad_fn=<AddBackward0>)
0.62891871
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.62887430
tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)
0.62883359
tensor(0.1665, device='cuda:0', grad_fn=<AddBackward0>)
0.62877578
tensor(0.1822, device='cuda:0', grad_fn=<AddBackward0>)
0.62872952
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
0.62869900
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.62868947
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.62869710
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.62873316
tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)
0.62882715
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.62893689
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
0.62875062
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
0.62872273
tensor(0.2067, device='cuda:0', grad_fn=<AddBackward0>)
0.62872875
tensor(0.1866, device='cuda:0', grad_fn=<AddBackward0>)
0.62875175
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.62864310
tensor(0.2528, device='cuda:0', grad_fn=<AddBackward0>)
0.62855643
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
0.62853557
tensor(0.1732, device='cuda:0', grad_fn=<AddBackward0>)
0.62853914
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
0.62858731
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.62863326
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
0.62862492
tensor(0.2219, device='cuda:0', grad_fn=<AddBackward0>)
0.62868363
tensor(0.2413, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [26][  180/  196]   Loss 0.171628   Top1 94.288194   Top5 99.908854   BatchTime 0.331659   LR 0.000014
0.62868285
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
0.62856740
tensor(0.2037, device='cuda:0', grad_fn=<AddBackward0>)
0.62848377
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
0.62845331
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.62842762
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
0.62839544
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
0.62840736
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
0.62843746
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.62840205
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.62843752
tensor(0.2597, device='cuda:0', grad_fn=<AddBackward0>)
0.62845510
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
0.62843293
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.260    Top5: 99.906    Loss: 0.172
0.62848103
tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)
0.62845755
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.62828767
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
0.62829447
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
0.62834710
tensor(0.3320, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [26][   20/   40]   Loss 0.371792   Top1 88.515625   Top5 99.414062   BatchTime 0.114837
INFO - Validation [26][   40/   40]   Loss 0.364212   Top1 88.520000   Top5 99.510000   BatchTime 0.084067
INFO - ==> Top1: 88.520    Top5: 99.510    Loss: 0.364
INFO - ==> Sparsity : 0.592
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 88.520   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 88.520   Top5: 99.510]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.1621)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0968)
features.2.conv.0 tensor(0.1496)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.5642)
features.3.conv.0 tensor(0.0842)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1063)
features.4.conv.0 tensor(0.0586)
features.4.conv.3 tensor(0.2986)
features.4.conv.6 tensor(0.2025)
features.5.conv.0 tensor(0.3612)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.1517)
features.6.conv.0 tensor(0.0444)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0864)
features.7.conv.0 tensor(0.1814)
features.7.conv.3 tensor(0.4572)
features.7.conv.6 tensor(0.2164)
features.8.conv.0 tensor(0.6134)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.4996)
features.9.conv.0 tensor(0.5860)
features.9.conv.3 tensor(0.5573)
features.9.conv.6 tensor(0.1401)
features.10.conv.0 tensor(0.0645)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.1057)
features.11.conv.0 tensor(0.7531)
features.11.conv.3 tensor(0.6406)
features.11.conv.6 tensor(0.7497)
features.12.conv.0 tensor(0.7737)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.7142)
features.13.conv.0 tensor(0.2520)
features.13.conv.3 tensor(0.4894)
features.13.conv.6 tensor(0.1642)
features.14.conv.0 tensor(0.9046)
features.14.conv.3 tensor(0.8257)
features.14.conv.6 tensor(0.9576)
features.15.conv.0 tensor(0.8747)
features.15.conv.3 tensor(0.8359)
features.15.conv.6 tensor(0.9616)
features.16.conv.0 tensor(0.6674)
features.16.conv.3 tensor(0.8052)
features.16.conv.6 tensor(0.8816)
conv.0 tensor(0.1248)
tensor(1295570.) 2188896.0
0.62823480
tensor(0.1923, device='cuda:0', grad_fn=<AddBackward0>)
0.62811637
tensor(0.1997, device='cuda:0', grad_fn=<AddBackward0>)
0.62801462
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
0.62797117
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
0.62797230
tensor(0.2314, device='cuda:0', grad_fn=<AddBackward0>)
0.62799031
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
0.62799227
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
0.62803626
tensor(0.2260, device='cuda:0', grad_fn=<AddBackward0>)
0.62792981
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
0.62791610
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
0.62788099
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
0.62778366
tensor(0.2012, device='cuda:0', grad_fn=<AddBackward0>)
0.62768298
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
0.62762183
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
0.62766123
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.62765914
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][   20/  196]   Loss 0.175434   Top1 94.277344   Top5 99.902344   BatchTime 0.311995   LR 0.000013
0.62767780
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.62769991
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
0.62764871
tensor(0.1914, device='cuda:0', grad_fn=<AddBackward0>)
0.62762773
tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)
0.62766045
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
0.62750942
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.62745541
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
0.62742954
tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)
0.62745225
tensor(0.2396, device='cuda:0', grad_fn=<AddBackward0>)
0.62752652
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
0.62743467
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
0.62736660
tensor(0.1827, device='cuda:0', grad_fn=<AddBackward0>)
0.62739098
tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)
0.62732720
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
0.62729210
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.62721229
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
0.62716669
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
0.62719345
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
0.62725908
tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)
0.62714237
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
0.62716794
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
0.62709862
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
0.62706220
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
0.62703907
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][   40/  196]   Loss 0.172231   Top1 94.394531   Top5 99.902344   BatchTime 0.281835   LR 0.000012
0.62695515
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
0.62690908
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
0.62689447
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
0.62686515
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
0.62686789
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
0.62693113
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.62675333
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
0.62673146
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.62679648
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.62665784
tensor(0.1654, device='cuda:0', grad_fn=<AddBackward0>)
0.62657392
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
0.62653863
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
0.62652308
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
0.62654608
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
0.62651294
tensor(0.2311, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][   60/  196]   Loss 0.167928   Top1 94.485677   Top5 99.908854   BatchTime 0.284052   LR 0.000011
0.62651402
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
0.62660730
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
0.62651008
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
0.62657171
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
0.62646699
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
0.62636703
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.62626892
tensor(0.2122, device='cuda:0', grad_fn=<AddBackward0>)
0.62616605
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
0.62610394
tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)
0.62606287
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
0.62604356
tensor(0.2947, device='cuda:0', grad_fn=<AddBackward0>)
0.62602556
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
0.62601817
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
0.62606865
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
0.62618333
tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)
0.62633348
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.62630236
tensor(0.1981, device='cuda:0', grad_fn=<AddBackward0>)
0.62625194
tensor(0.2243, device='cuda:0', grad_fn=<AddBackward0>)
0.62618244
tensor(0.2434, device='cuda:0', grad_fn=<AddBackward0>)
0.62610865
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.62605399
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
0.62597650
tensor(0.2801, device='cuda:0', grad_fn=<AddBackward0>)
0.62593400
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
0.62590462
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
0.62587649
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][   80/  196]   Loss 0.172818   Top1 94.355469   Top5 99.912109   BatchTime 0.292402   LR 0.000010
0.62589061
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
0.62584680
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
0.62591845
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.62592149
tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)
0.62587124
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
0.62586266
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
0.62581706
tensor(0.1754, device='cuda:0', grad_fn=<AddBackward0>)
0.62579113
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
0.62583804
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
0.62575722
tensor(0.1985, device='cuda:0', grad_fn=<AddBackward0>)
0.62569273
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.62569863
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
0.62563282
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
0.62561059
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
0.62565124
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
0.62564605
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
0.62554795
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
0.62557083
tensor(0.1904, device='cuda:0', grad_fn=<AddBackward0>)
0.62552428
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][  100/  196]   Loss 0.170641   Top1 94.406250   Top5 99.914062   BatchTime 0.296395   LR 0.000009
0.62527877
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
0.62430483
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
0.62403142
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
0.62402266
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
0.62400430
tensor(0.1974, device='cuda:0', grad_fn=<AddBackward0>)
0.62402362
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
0.62396413
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
0.62389058
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
0.62384337
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.62379849
tensor(0.1804, device='cuda:0', grad_fn=<AddBackward0>)
0.62377411
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.62376314
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
0.62374884
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.62372446
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
0.62370747
tensor(0.1884, device='cuda:0', grad_fn=<AddBackward0>)
0.62369424
tensor(0.1697, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][  120/  196]   Loss 0.169712   Top1 94.505208   Top5 99.918620   BatchTime 0.288478   LR 0.000009
0.62367767
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
0.62366718
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
0.62365568
tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)
0.62363982
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
0.62362427
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
0.62360340
tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)
0.62359291
tensor(0.1579, device='cuda:0', grad_fn=<AddBackward0>)
0.62357879
tensor(0.2405, device='cuda:0', grad_fn=<AddBackward0>)
0.62355930
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
0.62354445
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
0.62352979
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
0.62351376
tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)
0.62349796
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
0.62347651
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
0.62346071
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
0.62344372
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
0.62342924
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
0.62341428
tensor(0.2707, device='cuda:0', grad_fn=<AddBackward0>)
0.62339526
tensor(0.1717, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][  140/  196]   Loss 0.171777   Top1 94.411272   Top5 99.916295   BatchTime 0.294030   LR 0.000008
0.62337577
tensor(0.1605, device='cuda:0', grad_fn=<AddBackward0>)
0.62336218
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
0.62334871
tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)
0.62333387
tensor(0.2027, device='cuda:0', grad_fn=<AddBackward0>)
0.62333196
tensor(0.2315, device='cuda:0', grad_fn=<AddBackward0>)
0.62332398
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
0.62330514
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.62329245
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
0.62327611
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
0.62326115
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.62325031
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
0.62323427
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
0.62322807
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
0.62321401
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
0.62320656
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.62318939
tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)
0.62317908
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
0.62316579
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.62315381
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.62314528
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
0.62313515
tensor(0.1643, device='cuda:0', grad_fn=<AddBackward0>)
0.62312287
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
0.62311643
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.62311107
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
0.62310433
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
0.62310231
tensor(0.2178, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][  160/  196]   Loss 0.170022   Top1 94.448242   Top5 99.912109   BatchTime 0.295981   LR 0.000007
0.62309718
tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)
0.62309778
tensor(0.2027, device='cuda:0', grad_fn=<AddBackward0>)
0.62309575
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
0.62309390
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
0.62309384
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.62308377
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
0.62307507
tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)
0.62306762
tensor(0.1940, device='cuda:0', grad_fn=<AddBackward0>)
0.62305731
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
0.62304991
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
0.62304419
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
0.62303287
tensor(0.1963, device='cuda:0', grad_fn=<AddBackward0>)
0.62302911
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
0.62301970
tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)
0.62301302
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
0.62300456
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
0.62299544
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
0.62298441
tensor(0.1999, device='cuda:0', grad_fn=<AddBackward0>)
0.62297177
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [27][  180/  196]   Loss 0.170496   Top1 94.433594   Top5 99.908854   BatchTime 0.298760   LR 0.000007
0.62294680
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.62293887
tensor(0.2093, device='cuda:0', grad_fn=<AddBackward0>)
0.62292445
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.62291944
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
0.62291133
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.62290215
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
0.62290001
tensor(0.2081, device='cuda:0', grad_fn=<AddBackward0>)
0.62288606
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
0.62287915
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.62287939
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
0.62287331
tensor(0.1777, device='cuda:0', grad_fn=<AddBackward0>)
0.62287366
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.414    Top5: 99.910    Loss: 0.171
0.62286478
tensor(0.2318, device='cuda:0', grad_fn=<AddBackward0>)
0.62285256
tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)
0.62283683
tensor(0.2243, device='cuda:0', grad_fn=<AddBackward0>)
0.62282807
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.62282449
tensor(0.2923, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 0.369126   Top1 88.632812   Top5 99.492188   BatchTime 0.145085
INFO - Validation [27][   40/   40]   Loss 0.363576   Top1 88.700000   Top5 99.590000   BatchTime 0.099199
INFO - ==> Top1: 88.700    Top5: 99.590    Loss: 0.364
INFO - ==> Sparsity : 0.595
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 88.700   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 88.520   Top5: 99.590]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0959)
features.2.conv.0 tensor(0.1516)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.5651)
features.3.conv.0 tensor(0.0845)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0596)
features.4.conv.3 tensor(0.3003)
features.4.conv.6 tensor(0.2052)
features.5.conv.0 tensor(0.3621)
features.5.conv.3 tensor(0.4196)
features.5.conv.6 tensor(0.1525)
features.6.conv.0 tensor(0.0454)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0855)
features.7.conv.0 tensor(0.1820)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.2118)
features.8.conv.0 tensor(0.6139)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.5091)
features.9.conv.0 tensor(0.5883)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.1427)
features.10.conv.0 tensor(0.0639)
features.10.conv.3 tensor(0.1027)
features.10.conv.6 tensor(0.1017)
features.11.conv.0 tensor(0.7538)
features.11.conv.3 tensor(0.6410)
features.11.conv.6 tensor(0.7522)
features.12.conv.0 tensor(0.7753)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.7173)
features.13.conv.0 tensor(0.2550)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.2077)
features.14.conv.0 tensor(0.9044)
features.14.conv.3 tensor(0.8258)
features.14.conv.6 tensor(0.9575)
features.15.conv.0 tensor(0.8753)
features.15.conv.3 tensor(0.8361)
features.15.conv.6 tensor(0.9617)
features.16.conv.0 tensor(0.6717)
features.16.conv.3 tensor(0.8053)
features.16.conv.6 tensor(0.8839)
conv.0 tensor(0.1279)
tensor(1303084.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
0.62281275
tensor(0.1970, device='cuda:0', grad_fn=<AddBackward0>)
0.62280917
tensor(0.2111, device='cuda:0', grad_fn=<AddBackward0>)
0.62279707
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
0.62279660
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
0.62279296
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.62278211
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.62277716
tensor(0.2034, device='cuda:0', grad_fn=<AddBackward0>)
0.62277406
tensor(0.1822, device='cuda:0', grad_fn=<AddBackward0>)
0.62276548
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
0.62275684
tensor(0.1811, device='cuda:0', grad_fn=<AddBackward0>)
0.62275362
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.62274575
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.62274224
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
0.62273788
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
0.62272710
tensor(0.2312, device='cuda:0', grad_fn=<AddBackward0>)
0.62271935
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
0.62271076
tensor(0.2251, device='cuda:0', grad_fn=<AddBackward0>)
0.62270522
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][   20/  196]   Loss 0.170735   Top1 94.140625   Top5 99.902344   BatchTime 0.331386   LR 0.000006
0.62269807
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
0.62269384
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.62269646
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
0.62270427
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
0.62269366
tensor(0.2806, device='cuda:0', grad_fn=<AddBackward0>)
0.62269253
tensor(0.2418, device='cuda:0', grad_fn=<AddBackward0>)
0.62269074
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
0.62268895
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
0.62267917
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.62267232
tensor(0.1983, device='cuda:0', grad_fn=<AddBackward0>)
0.62266523
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
0.62266213
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
0.62266123
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
0.62265319
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
0.62264979
tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)
0.62264359
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.62263978
tensor(0.1760, device='cuda:0', grad_fn=<AddBackward0>)
0.62263232
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][   40/  196]   Loss 0.175581   Top1 93.955078   Top5 99.902344   BatchTime 0.327243   LR 0.000005
0.62262481
tensor(0.1963, device='cuda:0', grad_fn=<AddBackward0>)
0.62262219
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.62261719
tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>)
0.62261951
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
0.62261039
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
0.62260586
tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)
0.62260115
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
0.62258971
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
0.62258995
tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)
0.62258017
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
0.62257183
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.62255931
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
0.62254304
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.62253636
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
0.62252861
tensor(0.1960, device='cuda:0', grad_fn=<AddBackward0>)
0.62251240
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
0.62250662
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
0.62250531
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
0.62250417
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.62249434
tensor(0.2160, device='cuda:0', grad_fn=<AddBackward0>)
0.62249947
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
0.62249535
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][   60/  196]   Loss 0.167946   Top1 94.277344   Top5 99.921875   BatchTime 0.314288   LR 0.000004
0.62248522
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
0.62248254
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
0.62247998
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
0.62247235
tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)
0.62246984
tensor(0.1668, device='cuda:0', grad_fn=<AddBackward0>)
0.62246794
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
0.62246871
tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)
0.62245572
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.62245256
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
0.62245530
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
0.62245214
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
0.62244481
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
0.62243712
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
0.62243402
tensor(0.1981, device='cuda:0', grad_fn=<AddBackward0>)
0.62243158
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
0.62242138
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
0.62242061
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
0.62241375
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
0.62241143
tensor(0.1979, device='cuda:0', grad_fn=<AddBackward0>)
0.62240970
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
0.62240320
tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][   80/  196]   Loss 0.165813   Top1 94.409180   Top5 99.931641   BatchTime 0.306317   LR 0.000004
0.62239850
tensor(0.2441, device='cuda:0', grad_fn=<AddBackward0>)
0.62239659
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
0.62239391
tensor(0.2127, device='cuda:0', grad_fn=<AddBackward0>)
0.62239343
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.62239689
tensor(0.1884, device='cuda:0', grad_fn=<AddBackward0>)
0.62238508
tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)
0.62238133
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.62237561
tensor(0.2305, device='cuda:0', grad_fn=<AddBackward0>)
0.62237078
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.62236416
tensor(0.2294, device='cuda:0', grad_fn=<AddBackward0>)
0.62235487
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.62235862
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.62235028
tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)
0.62233782
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][  100/  196]   Loss 0.167198   Top1 94.339844   Top5 99.929688   BatchTime 0.298544   LR 0.000003
0.62233859
tensor(0.2489, device='cuda:0', grad_fn=<AddBackward0>)
0.62233043
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
0.62232494
tensor(0.2104, device='cuda:0', grad_fn=<AddBackward0>)
0.62231928
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
0.62231421
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
0.62230593
tensor(0.2289, device='cuda:0', grad_fn=<AddBackward0>)
0.62230504
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
0.62230003
tensor(0.2550, device='cuda:0', grad_fn=<AddBackward0>)
0.62231237
tensor(0.1938, device='cuda:0', grad_fn=<AddBackward0>)
0.62230021
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.62229830
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
0.62230200
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
0.62229627
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.62229055
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
0.62228012
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
0.62227714
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
0.62227613
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
0.62227398
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
0.62227666
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
0.62226909
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
0.62226266
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.62225318
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][  120/  196]   Loss 0.165287   Top1 94.427083   Top5 99.938151   BatchTime 0.296869   LR 0.000003
0.62225568
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
0.62224948
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
0.62224627
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.62224454
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
0.62224603
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
0.62224525
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.62223375
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
0.62222821
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.62223071
tensor(0.2146, device='cuda:0', grad_fn=<AddBackward0>)
0.62222183
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.62221688
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.62221056
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
0.62220824
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
0.62220418
tensor(0.2139, device='cuda:0', grad_fn=<AddBackward0>)
0.62220091
tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)
0.62220180
tensor(0.2431, device='cuda:0', grad_fn=<AddBackward0>)
0.62219709
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
0.62218833
tensor(0.1733, device='cuda:0', grad_fn=<AddBackward0>)
0.62218434
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][  140/  196]   Loss 0.166036   Top1 94.383371   Top5 99.941406   BatchTime 0.298820   LR 0.000003
0.62218183
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
0.62217605
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
0.62217146
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
0.62216353
tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)
0.62215638
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
0.62215525
tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)
0.62214726
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
0.62213957
tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)
0.62214065
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
0.62214190
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
0.62213951
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
0.62213671
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
0.62213790
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
0.62213373
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
0.62213314
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.62212700
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
0.62212574
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
0.62212616
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
0.62211519
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
0.62211370
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
0.62210786
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
0.62210810
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
0.62210178
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
0.62210035
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.62209713
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][  160/  196]   Loss 0.164356   Top1 94.458008   Top5 99.938965   BatchTime 0.302380   LR 0.000002
0.62209213
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
0.62209445
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
0.62209255
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
0.62208146
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
0.62208265
tensor(0.2380, device='cuda:0', grad_fn=<AddBackward0>)
0.62208122
tensor(0.2218, device='cuda:0', grad_fn=<AddBackward0>)
0.62207425
tensor(0.2075, device='cuda:0', grad_fn=<AddBackward0>)
0.62207347
tensor(0.2100, device='cuda:0', grad_fn=<AddBackward0>)
0.62207180
tensor(0.2069, device='cuda:0', grad_fn=<AddBackward0>)
0.62206596
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
0.62206268
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.62206584
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
0.62206167
tensor(0.1817, device='cuda:0', grad_fn=<AddBackward0>)
0.62206358
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.62206203
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
0.62206548
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.62205952
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
0.62205821
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [28][  180/  196]   Loss 0.165226   Top1 94.396701   Top5 99.941406   BatchTime 0.307659   LR 0.000002
0.62205362
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
0.62205106
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.62205249
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
0.62204629
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.62204343
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
0.62204695
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
0.62204736
tensor(0.1612, device='cuda:0', grad_fn=<AddBackward0>)
0.62204945
tensor(0.2170, device='cuda:0', grad_fn=<AddBackward0>)
0.62205148
tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)
0.62205130
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
0.62205249
tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)
0.62205076
tensor(0.1588, device='cuda:0', grad_fn=<AddBackward0>)
0.62204546
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
0.62204379
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
0.62203723
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.62203199
tensor(0.1702, device='cuda:0', grad_fn=<AddBackward0>)
0.62203062
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.394    Top5: 99.942    Loss: 0.166
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.62202519
tensor(0.2817, device='cuda:0', grad_fn=<AddBackward0>)
0.62202412
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [28][   20/   40]   Loss 0.358750   Top1 88.886719   Top5 99.453125   BatchTime 0.122928
INFO - Validation [28][   40/   40]   Loss 0.353915   Top1 88.790000   Top5 99.580000   BatchTime 0.090267
INFO - ==> Top1: 88.790    Top5: 99.580    Loss: 0.354
INFO - ==> Sparsity : 0.597
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.700   Top5: 99.590]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0955)
features.2.conv.0 tensor(0.1516)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.5651)
features.3.conv.0 tensor(0.0836)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1172)
features.4.conv.0 tensor(0.0596)
features.4.conv.3 tensor(0.2992)
features.4.conv.6 tensor(0.2083)
features.5.conv.0 tensor(0.3634)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.1538)
features.6.conv.0 tensor(0.0459)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0858)
features.7.conv.0 tensor(0.1823)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.2099)
features.8.conv.0 tensor(0.6149)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.5111)
features.9.conv.0 tensor(0.5874)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.1453)
features.10.conv.0 tensor(0.0636)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.1011)
features.11.conv.0 tensor(0.7540)
features.11.conv.3 tensor(0.6414)
features.11.conv.6 tensor(0.7534)
features.12.conv.0 tensor(0.7770)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.7178)
features.13.conv.0 tensor(0.2574)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.2151)
features.14.conv.0 tensor(0.9046)
features.14.conv.3 tensor(0.8257)
features.14.conv.6 tensor(0.9576)
features.15.conv.0 tensor(0.8753)
features.15.conv.3 tensor(0.8361)
features.15.conv.6 tensor(0.9616)
features.16.conv.0 tensor(0.6734)
features.16.conv.3 tensor(0.8056)
features.16.conv.6 tensor(0.8845)
conv.0 tensor(0.1315)
tensor(1306171.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
0.62202275
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.62202090
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
0.62201774
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
0.62202185
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
0.62201995
tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)
0.62201685
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
0.62201411
tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)
0.62201756
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
0.62201506
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
0.62201113
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
0.62201846
tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)
0.62201905
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
0.62201637
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.62201667
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.62201744
tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)
0.62201869
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.62201983
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
0.62201971
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
0.62201744
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][   20/  196]   Loss 0.156211   Top1 94.980469   Top5 99.863281   BatchTime 0.315895   LR 0.000001
0.62201506
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.62201315
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
0.62201500
tensor(0.1836, device='cuda:0', grad_fn=<AddBackward0>)
0.62200779
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.62201107
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
0.62200522
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
0.62200367
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
0.62199509
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.62199110
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.62199235
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
0.62199348
tensor(0.1809, device='cuda:0', grad_fn=<AddBackward0>)
0.62199032
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.62199646
tensor(0.2238, device='cuda:0', grad_fn=<AddBackward0>)
0.62199205
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
0.62199366
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
0.62199414
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
0.62199062
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
0.62199211
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
0.62199199
tensor(0.2323, device='cuda:0', grad_fn=<AddBackward0>)
0.62199539
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
0.62199509
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][   40/  196]   Loss 0.158506   Top1 94.755859   Top5 99.902344   BatchTime 0.306780   LR 0.000001
0.62199724
tensor(0.1903, device='cuda:0', grad_fn=<AddBackward0>)
0.62199724
tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)
0.62199694
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
0.62199020
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.62198573
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
0.62198490
tensor(0.2373, device='cuda:0', grad_fn=<AddBackward0>)
0.62199104
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
0.62199175
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
0.62199414
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
0.62200153
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.62199402
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
0.62199247
tensor(0.2195, device='cuda:0', grad_fn=<AddBackward0>)
0.62199134
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.62199163
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
0.62198663
tensor(0.1839, device='cuda:0', grad_fn=<AddBackward0>)
0.62198347
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][   60/  196]   Loss 0.159877   Top1 94.694010   Top5 99.921875   BatchTime 0.288377   LR 0.000001
0.62198305
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.62197644
tensor(0.2315, device='cuda:0', grad_fn=<AddBackward0>)
0.62197864
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
0.62197965
tensor(0.1741, device='cuda:0', grad_fn=<AddBackward0>)
0.62197882
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.62197971
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
0.62198323
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
0.62196976
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
0.62196940
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
0.62196362
tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)
0.62196809
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
0.62196738
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
0.62196672
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
0.62196654
tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)
0.62196386
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
0.62196213
tensor(0.2112, device='cuda:0', grad_fn=<AddBackward0>)
0.62195718
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.62195605
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
0.62196028
tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)
0.62196696
tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)
0.62196302
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
0.62196159
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][   80/  196]   Loss 0.160447   Top1 94.755859   Top5 99.926758   BatchTime 0.286245   LR 0.000001
0.62196094
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
0.62195742
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
0.62195885
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.62195665
tensor(0.2358, device='cuda:0', grad_fn=<AddBackward0>)
0.62195164
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.62196428
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.62196499
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
0.62195426
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
0.62196350
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
0.62195152
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
0.62195551
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
0.62195444
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
0.62196070
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
0.62195766
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
0.62194568
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
0.62195641
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
0.62195009
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.62194842
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][  100/  196]   Loss 0.162067   Top1 94.765625   Top5 99.906250   BatchTime 0.292664   LR 0.000000
0.62194622
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
0.62194860
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.62195492
tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)
0.62195617
tensor(0.2323, device='cuda:0', grad_fn=<AddBackward0>)
0.62195742
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
0.62195724
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
0.62195563
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
0.62195110
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.62195247
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
0.62194890
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.62194777
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
0.62194926
tensor(0.1827, device='cuda:0', grad_fn=<AddBackward0>)
0.62195182
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
0.62195253
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
0.62194651
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
0.62195253
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
0.62195510
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
0.62195957
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
0.62196022
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
0.62195832
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][  120/  196]   Loss 0.160927   Top1 94.781901   Top5 99.905599   BatchTime 0.297727   LR 0.000000
0.62194937
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.62194401
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
0.62194687
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.62195361
tensor(0.1668, device='cuda:0', grad_fn=<AddBackward0>)
0.62195116
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
0.62195498
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
0.62195390
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
0.62195325
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
0.62195092
tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)
0.62194908
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.62194711
tensor(0.1833, device='cuda:0', grad_fn=<AddBackward0>)
0.62194580
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.62194914
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
0.62194616
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
0.62195045
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
0.62194800
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
0.62194306
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
0.62194175
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.62194514
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.62194097
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
0.62194067
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
0.62193459
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
0.62193507
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][  140/  196]   Loss 0.160748   Top1 94.754464   Top5 99.913504   BatchTime 0.303766   LR 0.000000
0.62193769
tensor(0.2263, device='cuda:0', grad_fn=<AddBackward0>)
0.62193185
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
0.62193388
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
0.62192982
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
0.62193376
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
0.62193233
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.62192690
tensor(0.1573, device='cuda:0', grad_fn=<AddBackward0>)
0.62193322
tensor(0.1926, device='cuda:0', grad_fn=<AddBackward0>)
0.62193543
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
0.62193680
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
0.62194306
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.62194407
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
0.62194306
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
0.62193930
tensor(0.1967, device='cuda:0', grad_fn=<AddBackward0>)
0.62193763
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
0.62194228
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.62194377
tensor(0.2015, device='cuda:0', grad_fn=<AddBackward0>)
0.62194347
tensor(0.1724, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][  160/  196]   Loss 0.160812   Top1 94.755859   Top5 99.916992   BatchTime 0.308984   LR 0.000000
0.62194389
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.62194145
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
0.62194622
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
0.62194675
tensor(0.2350, device='cuda:0', grad_fn=<AddBackward0>)
0.62194026
tensor(0.2140, device='cuda:0', grad_fn=<AddBackward0>)
0.62194228
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
0.62194812
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
0.62194240
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
0.62193996
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
0.62194163
tensor(0.2195, device='cuda:0', grad_fn=<AddBackward0>)
0.62194479
tensor(0.2675, device='cuda:0', grad_fn=<AddBackward0>)
0.62194556
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
0.62193894
tensor(0.2015, device='cuda:0', grad_fn=<AddBackward0>)
0.62193805
tensor(0.1586, device='cuda:0', grad_fn=<AddBackward0>)
0.62193507
tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)
0.62193888
tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)
0.62193793
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
0.62193841
tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)
0.62193131
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
0.62193859
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
0.62193912
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.62193638
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
0.62193120
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
0.62194121
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [29][  180/  196]   Loss 0.162207   Top1 94.707031   Top5 99.913194   BatchTime 0.311560   LR 0.000000
0.62194216
tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>)
0.62193805
tensor(0.2248, device='cuda:0', grad_fn=<AddBackward0>)
0.62193716
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
0.62193829
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
0.62193805
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
0.62194192
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
0.62193334
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.62192941
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
0.62193233
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
0.62193018
tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)
0.62192905
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
0.62193364
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.694    Top5: 99.910    Loss: 0.162
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.62193400
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
0.62194210
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.62194407
tensor(0.2248, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 0.364126   Top1 88.710938   Top5 99.492188   BatchTime 0.127762
INFO - Validation [29][   40/   40]   Loss 0.358271   Top1 88.820000   Top5 99.580000   BatchTime 0.089455
INFO - ==> Top1: 88.820    Top5: 99.580    Loss: 0.358
INFO - ==> Sparsity : 0.597
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0955)
features.2.conv.0 tensor(0.1516)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.5651)
features.3.conv.0 tensor(0.0842)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1176)
features.4.conv.0 tensor(0.0596)
features.4.conv.3 tensor(0.2992)
features.4.conv.6 tensor(0.2085)
features.5.conv.0 tensor(0.3634)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.1538)
features.6.conv.0 tensor(0.0459)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0856)
features.7.conv.0 tensor(0.1823)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.2102)
features.8.conv.0 tensor(0.6152)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.5114)
features.9.conv.0 tensor(0.5874)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.1458)
features.10.conv.0 tensor(0.0638)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.1013)
features.11.conv.0 tensor(0.7542)
features.11.conv.3 tensor(0.6414)
features.11.conv.6 tensor(0.7535)
features.12.conv.0 tensor(0.7770)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.7177)
features.13.conv.0 tensor(0.2573)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.2153)
features.14.conv.0 tensor(0.9046)
features.14.conv.3 tensor(0.8257)
features.14.conv.6 tensor(0.9576)
features.15.conv.0 tensor(0.8754)
features.15.conv.3 tensor(0.8361)
features.15.conv.6 tensor(0.9616)
features.16.conv.0 tensor(0.6736)
features.16.conv.3 tensor(0.8056)
features.16.conv.6 tensor(0.8847)
conv.0 tensor(0.1321)
tensor(1306598.) 2188896.0
0.62192935
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
0.62161106
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
0.62137687
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
0.62113273
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
0.62095845
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
0.62088376
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
0.62093246
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
0.62101936
tensor(0.1643, device='cuda:0', grad_fn=<AddBackward0>)
0.62108380
tensor(0.2101, device='cuda:0', grad_fn=<AddBackward0>)
0.62110227
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
0.62103772
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.62086105
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.62070072
tensor(0.1959, device='cuda:0', grad_fn=<AddBackward0>)
0.62061411
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
0.62059027
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][   20/  196]   Loss 0.165143   Top1 94.492188   Top5 99.902344   BatchTime 0.355764   LR 0.000125
0.62060857
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
0.62059265
tensor(0.2503, device='cuda:0', grad_fn=<AddBackward0>)
0.62058866
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
0.62052304
tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)
0.62045699
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
0.62043989
tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)
0.62039465
tensor(0.2209, device='cuda:0', grad_fn=<AddBackward0>)
0.62031794
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.62028438
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
0.62020057
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
0.62018007
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
0.62006813
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.62001091
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.62002498
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
0.62008315
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
0.62009656
tensor(0.2724, device='cuda:0', grad_fn=<AddBackward0>)
0.61999738
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.61998796
tensor(0.3228, device='cuda:0', grad_fn=<AddBackward0>)
0.62000060
tensor(0.1968, device='cuda:0', grad_fn=<AddBackward0>)
0.61992127
tensor(0.1932, device='cuda:0', grad_fn=<AddBackward0>)
0.61989057
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][   40/  196]   Loss 0.183853   Top1 93.935547   Top5 99.892578   BatchTime 0.318620   LR 0.000125
0.61983973
tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)
0.61979675
tensor(0.2379, device='cuda:0', grad_fn=<AddBackward0>)
0.61972922
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.61966914
tensor(0.2173, device='cuda:0', grad_fn=<AddBackward0>)
0.61974102
tensor(0.2119, device='cuda:0', grad_fn=<AddBackward0>)
0.61973536
tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>)
0.61969399
tensor(0.2140, device='cuda:0', grad_fn=<AddBackward0>)
0.61967427
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
0.61955887
tensor(0.2180, device='cuda:0', grad_fn=<AddBackward0>)
0.61958605
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
0.61952806
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.61934602
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
0.61920977
tensor(0.2050, device='cuda:0', grad_fn=<AddBackward0>)
0.61914110
tensor(0.2340, device='cuda:0', grad_fn=<AddBackward0>)
0.61894310
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.61881536
tensor(0.2638, device='cuda:0', grad_fn=<AddBackward0>)
0.61863267
tensor(0.2350, device='cuda:0', grad_fn=<AddBackward0>)
0.61824852
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
0.61790031
tensor(0.3141, device='cuda:0', grad_fn=<AddBackward0>)
0.61790854
tensor(0.2086, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][   60/  196]   Loss 0.191921   Top1 93.606771   Top5 99.915365   BatchTime 0.314412   LR 0.000125
0.61776960
tensor(0.2378, device='cuda:0', grad_fn=<AddBackward0>)
0.61763507
tensor(0.1913, device='cuda:0', grad_fn=<AddBackward0>)
0.61753333
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.61773133
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
0.61814910
tensor(0.2629, device='cuda:0', grad_fn=<AddBackward0>)
0.61810541
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
0.61794525
tensor(0.2450, device='cuda:0', grad_fn=<AddBackward0>)
0.61775208
tensor(0.2295, device='cuda:0', grad_fn=<AddBackward0>)
0.61753720
tensor(0.2346, device='cuda:0', grad_fn=<AddBackward0>)
0.61724794
tensor(0.2049, device='cuda:0', grad_fn=<AddBackward0>)
0.61697954
tensor(0.1851, device='cuda:0', grad_fn=<AddBackward0>)
0.61679935
tensor(0.3035, device='cuda:0', grad_fn=<AddBackward0>)
0.61661935
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.61711532
tensor(0.2803, device='cuda:0', grad_fn=<AddBackward0>)
0.61774909
tensor(0.2046, device='cuda:0', grad_fn=<AddBackward0>)
0.61744106
tensor(0.2861, device='cuda:0', grad_fn=<AddBackward0>)
0.61692625
tensor(0.3013, device='cuda:0', grad_fn=<AddBackward0>)
0.61658365
tensor(0.2354, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][   80/  196]   Loss 0.201311   Top1 93.154297   Top5 99.902344   BatchTime 0.315794   LR 0.000125
0.61615473
tensor(0.2102, device='cuda:0', grad_fn=<AddBackward0>)
0.61569041
tensor(0.2062, device='cuda:0', grad_fn=<AddBackward0>)
0.61517853
tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)
0.61489254
tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)
0.61452675
tensor(0.3013, device='cuda:0', grad_fn=<AddBackward0>)
0.61431229
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
0.61392289
tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)
0.61361116
tensor(0.2183, device='cuda:0', grad_fn=<AddBackward0>)
0.61333966
tensor(0.2684, device='cuda:0', grad_fn=<AddBackward0>)
0.61287445
tensor(0.2365, device='cuda:0', grad_fn=<AddBackward0>)
0.61268079
tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)
0.61266261
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.61276096
tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)
0.61363655
tensor(0.3068, device='cuda:0', grad_fn=<AddBackward0>)
0.61300433
tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)
0.61259699
tensor(0.2233, device='cuda:0', grad_fn=<AddBackward0>)
0.61252695
tensor(0.3195, device='cuda:0', grad_fn=<AddBackward0>)
0.61243629
tensor(0.2320, device='cuda:0', grad_fn=<AddBackward0>)
0.61229044
tensor(0.2666, device='cuda:0', grad_fn=<AddBackward0>)
0.61203057
tensor(0.2336, device='cuda:0', grad_fn=<AddBackward0>)
0.61192280
tensor(0.2539, device='cuda:0', grad_fn=<AddBackward0>)
0.61203629
tensor(0.2333, device='cuda:0', grad_fn=<AddBackward0>)
0.61208993
tensor(0.1989, device='cuda:0', grad_fn=<AddBackward0>)
0.61177689
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][  100/  196]   Loss 0.207076   Top1 93.027344   Top5 99.882812   BatchTime 0.303498   LR 0.000125
0.61152530
tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)
0.61141741
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
0.61111456
tensor(0.2207, device='cuda:0', grad_fn=<AddBackward0>)
0.61098558
tensor(0.2981, device='cuda:0', grad_fn=<AddBackward0>)
0.61086923
tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)
0.61079395
tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)
0.61068636
tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)
0.61060578
tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)
0.61054409
tensor(0.2094, device='cuda:0', grad_fn=<AddBackward0>)
0.61054206
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
0.61053187
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.61037213
tensor(0.2966, device='cuda:0', grad_fn=<AddBackward0>)
0.61037380
tensor(0.2235, device='cuda:0', grad_fn=<AddBackward0>)
0.61038220
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.61017197
tensor(0.2097, device='cuda:0', grad_fn=<AddBackward0>)
0.61000353
tensor(0.2046, device='cuda:0', grad_fn=<AddBackward0>)
0.60981584
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.60968280
tensor(0.2262, device='cuda:0', grad_fn=<AddBackward0>)
0.60960454
tensor(0.2115, device='cuda:0', grad_fn=<AddBackward0>)
0.60958946
tensor(0.2238, device='cuda:0', grad_fn=<AddBackward0>)
0.60960633
tensor(0.3429, device='cuda:0', grad_fn=<AddBackward0>)
0.60956782
tensor(0.3152, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][  120/  196]   Loss 0.210386   Top1 92.887370   Top5 99.886068   BatchTime 0.298672   LR 0.000125
0.60939568
tensor(0.3452, device='cuda:0', grad_fn=<AddBackward0>)
0.60920280
tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)
0.60924500
tensor(0.2430, device='cuda:0', grad_fn=<AddBackward0>)
0.60926622
tensor(0.3051, device='cuda:0', grad_fn=<AddBackward0>)
0.60906613
tensor(0.2572, device='cuda:0', grad_fn=<AddBackward0>)
0.60888124
tensor(0.2639, device='cuda:0', grad_fn=<AddBackward0>)
0.60881156
tensor(0.2679, device='cuda:0', grad_fn=<AddBackward0>)
0.60870510
tensor(0.2672, device='cuda:0', grad_fn=<AddBackward0>)
0.60857862
tensor(0.3118, device='cuda:0', grad_fn=<AddBackward0>)
0.60849339
tensor(0.3219, device='cuda:0', grad_fn=<AddBackward0>)
0.60842299
tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)
0.60829151
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.60823321
tensor(0.2548, device='cuda:0', grad_fn=<AddBackward0>)
0.60833204
tensor(0.2603, device='cuda:0', grad_fn=<AddBackward0>)
0.60827625
tensor(0.2356, device='cuda:0', grad_fn=<AddBackward0>)
0.60825205
tensor(0.1990, device='cuda:0', grad_fn=<AddBackward0>)
0.60810113
tensor(0.2832, device='cuda:0', grad_fn=<AddBackward0>)
0.60797930
tensor(0.3031, device='cuda:0', grad_fn=<AddBackward0>)
0.60774857
tensor(0.2979, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][  140/  196]   Loss 0.217823   Top1 92.631138   Top5 99.880022   BatchTime 0.301259   LR 0.000125
0.60743803
tensor(0.2629, device='cuda:0', grad_fn=<AddBackward0>)
0.60745853
tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)
0.60723203
tensor(0.2691, device='cuda:0', grad_fn=<AddBackward0>)
0.60703188
tensor(0.2439, device='cuda:0', grad_fn=<AddBackward0>)
0.60696167
tensor(0.2038, device='cuda:0', grad_fn=<AddBackward0>)
0.60682362
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
0.60677207
tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)
0.60641164
tensor(0.2459, device='cuda:0', grad_fn=<AddBackward0>)
0.60595560
tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)
0.60544550
tensor(0.2552, device='cuda:0', grad_fn=<AddBackward0>)
0.60488373
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.60436332
tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)
0.60400313
tensor(0.2223, device='cuda:0', grad_fn=<AddBackward0>)
0.60361457
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
0.60363322
tensor(0.2027, device='cuda:0', grad_fn=<AddBackward0>)
0.60353190
tensor(0.1854, device='cuda:0', grad_fn=<AddBackward0>)
0.60330415
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
0.60300142
tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)
0.60279644
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][  160/  196]   Loss 0.217451   Top1 92.646484   Top5 99.880371   BatchTime 0.303642   LR 0.000125
0.60259473
tensor(0.2313, device='cuda:0', grad_fn=<AddBackward0>)
0.60250479
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
0.60257125
tensor(0.2140, device='cuda:0', grad_fn=<AddBackward0>)
0.60243142
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
0.60238659
tensor(0.2626, device='cuda:0', grad_fn=<AddBackward0>)
0.60238850
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
0.60240847
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
0.60267717
tensor(0.2607, device='cuda:0', grad_fn=<AddBackward0>)
0.60267031
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
0.60293818
tensor(0.2610, device='cuda:0', grad_fn=<AddBackward0>)
0.60299641
tensor(0.3448, device='cuda:0', grad_fn=<AddBackward0>)
0.60304111
tensor(0.2649, device='cuda:0', grad_fn=<AddBackward0>)
0.60326630
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.60324752
tensor(0.2283, device='cuda:0', grad_fn=<AddBackward0>)
0.60326856
tensor(0.2195, device='cuda:0', grad_fn=<AddBackward0>)
0.60349363
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
0.60361344
tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)
0.60366857
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.60387319
tensor(0.2660, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [30][  180/  196]   Loss 0.218288   Top1 92.610677   Top5 99.871962   BatchTime 0.304687   LR 0.000125
0.60391527
tensor(0.2391, device='cuda:0', grad_fn=<AddBackward0>)
0.60394579
tensor(0.2966, device='cuda:0', grad_fn=<AddBackward0>)
0.60397774
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
0.60368603
tensor(0.2712, device='cuda:0', grad_fn=<AddBackward0>)
0.60357618
tensor(0.2279, device='cuda:0', grad_fn=<AddBackward0>)
0.60349900
tensor(0.2682, device='cuda:0', grad_fn=<AddBackward0>)
0.60350466
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
0.60333508
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
0.60319251
tensor(0.2230, device='cuda:0', grad_fn=<AddBackward0>)
0.60325807
tensor(0.2284, device='cuda:0', grad_fn=<AddBackward0>)
0.60319209
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
0.60320258
tensor(0.2338, device='cuda:0', grad_fn=<AddBackward0>)
0.60304654
tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)
0.60304642
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
0.60295337
tensor(0.2664, device='cuda:0', grad_fn=<AddBackward0>)
0.60284412
tensor(0.2113, device='cuda:0', grad_fn=<AddBackward0>)
0.60277289
tensor(0.2730, device='cuda:0', grad_fn=<AddBackward0>)
0.60271853
tensor(0.2302, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 92.572    Top5: 99.872    Loss: 0.219
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.60272986
tensor(0.2852, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [30][   20/   40]   Loss 0.405667   Top1 87.460938   Top5 99.414062   BatchTime 0.121959
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.2402)
features.1.conv.0 tensor(0.0579)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0894)
features.2.conv.0 tensor(0.1279)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.5709)
features.3.conv.0 tensor(0.0767)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1133)
features.4.conv.0 tensor(0.0544)
features.4.conv.3 tensor(0.3009)
features.4.conv.6 tensor(0.2056)
features.5.conv.0 tensor(0.3532)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.2671)
features.6.conv.0 tensor(0.0496)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0889)
features.7.conv.0 tensor(0.1670)
features.7.conv.3 tensor(0.4572)
features.7.conv.6 tensor(0.1971)
features.8.conv.0 tensor(0.5940)
features.8.conv.3 tensor(0.5437)
features.8.conv.6 tensor(0.5219)
features.9.conv.0 tensor(0.5413)
features.9.conv.3 tensor(0.5561)
features.9.conv.6 tensor(0.4466)
features.10.conv.0 tensor(0.0651)
features.10.conv.3 tensor(0.1027)
features.10.conv.6 tensor(0.0971)
features.11.conv.0 tensor(0.7436)
features.11.conv.3 tensor(0.6408)
features.11.conv.6 tensor(0.7613)
features.12.conv.0 tensor(0.7451)
features.12.conv.3 tensor(0.6732)
features.12.conv.6 tensor(0.7326)
features.13.conv.0 tensor(0.2451)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.2063)
features.14.conv.0 tensor(0.9071)
features.14.conv.3 tensor(0.8260)
features.14.conv.6 tensor(0.9553)
features.15.conv.0 tensor(0.8760)
features.15.conv.3 tensor(0.8361)
features.15.conv.6 tensor(0.9602)
features.16.conv.0 tensor(0.6526)
features.16.conv.3 tensor(0.8039)
features.16.conv.6 tensor(0.8828)
conv.0 tensor(0.0946)
tensor(1290451.) 2188896.0
INFO - Validation [30][   40/   40]   Loss 0.399435   Top1 87.590000   Top5 99.530000   BatchTime 0.090186
INFO - ==> Top1: 87.590    Top5: 99.530    Loss: 0.399
INFO - ==> Sparsity : 0.590
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
0.60264456
tensor(0.1987, device='cuda:0', grad_fn=<AddBackward0>)
0.60272330
tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)
0.60279518
tensor(0.2137, device='cuda:0', grad_fn=<AddBackward0>)
0.60267711
tensor(0.2581, device='cuda:0', grad_fn=<AddBackward0>)
0.60259211
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
0.60251230
tensor(0.3008, device='cuda:0', grad_fn=<AddBackward0>)
0.60246062
tensor(0.2435, device='cuda:0', grad_fn=<AddBackward0>)
0.60240060
tensor(0.2751, device='cuda:0', grad_fn=<AddBackward0>)
0.60238183
tensor(0.2001, device='cuda:0', grad_fn=<AddBackward0>)
0.60234183
tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)
0.60223347
tensor(0.2203, device='cuda:0', grad_fn=<AddBackward0>)
0.60219830
tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)
0.60227638
tensor(0.2566, device='cuda:0', grad_fn=<AddBackward0>)
0.60230500
tensor(0.2611, device='cuda:0', grad_fn=<AddBackward0>)
0.60227710
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
0.60224313
tensor(0.2271, device='cuda:0', grad_fn=<AddBackward0>)
0.60234308
tensor(0.2322, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][   20/  196]   Loss 0.223001   Top1 92.910156   Top5 99.765625   BatchTime 0.346937   LR 0.000125
0.60224491
tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)
0.60224164
tensor(0.2377, device='cuda:0', grad_fn=<AddBackward0>)
0.60217386
tensor(0.1817, device='cuda:0', grad_fn=<AddBackward0>)
0.60213912
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.60212821
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
0.60206169
tensor(0.2133, device='cuda:0', grad_fn=<AddBackward0>)
0.60209137
tensor(0.2410, device='cuda:0', grad_fn=<AddBackward0>)
0.60203224
tensor(0.2195, device='cuda:0', grad_fn=<AddBackward0>)
0.60190022
tensor(0.2415, device='cuda:0', grad_fn=<AddBackward0>)
0.60176271
tensor(0.2945, device='cuda:0', grad_fn=<AddBackward0>)
0.60178983
tensor(0.3033, device='cuda:0', grad_fn=<AddBackward0>)
0.60166782
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.60162395
tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)
0.60168481
tensor(0.2753, device='cuda:0', grad_fn=<AddBackward0>)
0.60166717
tensor(0.2391, device='cuda:0', grad_fn=<AddBackward0>)
0.60179240
tensor(0.2678, device='cuda:0', grad_fn=<AddBackward0>)
0.60174692
tensor(0.2180, device='cuda:0', grad_fn=<AddBackward0>)
0.60172570
tensor(0.2508, device='cuda:0', grad_fn=<AddBackward0>)
0.60176623
tensor(0.2524, device='cuda:0', grad_fn=<AddBackward0>)
0.60194981
tensor(0.2465, device='cuda:0', grad_fn=<AddBackward0>)
0.60180950
tensor(0.2137, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][   40/  196]   Loss 0.226662   Top1 92.382812   Top5 99.824219   BatchTime 0.321833   LR 0.000125
0.60187691
tensor(0.2435, device='cuda:0', grad_fn=<AddBackward0>)
0.60196304
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
0.60190952
tensor(0.2387, device='cuda:0', grad_fn=<AddBackward0>)
0.60193282
tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)
0.60208374
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
0.60230356
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.60335350
tensor(0.2053, device='cuda:0', grad_fn=<AddBackward0>)
0.60318756
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
0.60313541
tensor(0.2235, device='cuda:0', grad_fn=<AddBackward0>)
0.60306281
tensor(0.2192, device='cuda:0', grad_fn=<AddBackward0>)
0.60307550
tensor(0.2241, device='cuda:0', grad_fn=<AddBackward0>)
0.60300726
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
0.60286039
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.60285550
tensor(0.1890, device='cuda:0', grad_fn=<AddBackward0>)
0.60279238
tensor(0.1899, device='cuda:0', grad_fn=<AddBackward0>)
0.60281169
tensor(0.2083, device='cuda:0', grad_fn=<AddBackward0>)
0.60283494
tensor(0.2378, device='cuda:0', grad_fn=<AddBackward0>)
0.60275638
tensor(0.2255, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][   60/  196]   Loss 0.218452   Top1 92.675781   Top5 99.850260   BatchTime 0.324754   LR 0.000125
0.60277224
tensor(0.2482, device='cuda:0', grad_fn=<AddBackward0>)
0.60289639
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
0.60311127
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
0.60312319
tensor(0.2076, device='cuda:0', grad_fn=<AddBackward0>)
0.60317832
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.60319686
tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)
0.60317260
tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)
0.60319179
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.60330594
tensor(0.2309, device='cuda:0', grad_fn=<AddBackward0>)
0.60341793
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
0.60339069
tensor(0.2552, device='cuda:0', grad_fn=<AddBackward0>)
0.60345036
tensor(0.2816, device='cuda:0', grad_fn=<AddBackward0>)
0.60348815
tensor(0.2335, device='cuda:0', grad_fn=<AddBackward0>)
0.60355592
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.60376012
tensor(0.2750, device='cuda:0', grad_fn=<AddBackward0>)
0.60400587
tensor(0.2704, device='cuda:0', grad_fn=<AddBackward0>)
0.60391372
tensor(0.2709, device='cuda:0', grad_fn=<AddBackward0>)
0.60375470
tensor(0.2251, device='cuda:0', grad_fn=<AddBackward0>)
0.60380185
tensor(0.2660, device='cuda:0', grad_fn=<AddBackward0>)
0.60377824
tensor(0.1947, device='cuda:0', grad_fn=<AddBackward0>)
0.60363454
tensor(0.2370, device='cuda:0', grad_fn=<AddBackward0>)
0.60353839
tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][   80/  196]   Loss 0.219192   Top1 92.587891   Top5 99.858398   BatchTime 0.305889   LR 0.000125
0.60359442
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
0.60368365
tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)
0.60357720
tensor(0.2186, device='cuda:0', grad_fn=<AddBackward0>)
0.60364151
tensor(0.2861, device='cuda:0', grad_fn=<AddBackward0>)
0.60365218
tensor(0.2141, device='cuda:0', grad_fn=<AddBackward0>)
0.60364586
tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)
0.60358238
tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)
0.60350960
tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)
0.60345888
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
0.60338622
tensor(0.1732, device='cuda:0', grad_fn=<AddBackward0>)
0.60328972
tensor(0.2337, device='cuda:0', grad_fn=<AddBackward0>)
0.60328281
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.60330641
tensor(0.1969, device='cuda:0', grad_fn=<AddBackward0>)
0.60340220
tensor(0.2489, device='cuda:0', grad_fn=<AddBackward0>)
0.60346437
tensor(0.2606, device='cuda:0', grad_fn=<AddBackward0>)
0.60340703
tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)
0.60344911
tensor(0.2561, device='cuda:0', grad_fn=<AddBackward0>)
0.60342103
tensor(0.2534, device='cuda:0', grad_fn=<AddBackward0>)
0.60336298
tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)
0.60338861
tensor(0.2800, device='cuda:0', grad_fn=<AddBackward0>)
0.60346442
tensor(0.2658, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][  100/  196]   Loss 0.219949   Top1 92.582031   Top5 99.863281   BatchTime 0.305673   LR 0.000125
0.60339129
tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>)
0.60334575
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
0.60345358
tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)
0.60352188
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.60365599
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.60376030
tensor(0.1981, device='cuda:0', grad_fn=<AddBackward0>)
0.60368234
tensor(0.2124, device='cuda:0', grad_fn=<AddBackward0>)
0.60364830
tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)
0.60362250
tensor(0.2779, device='cuda:0', grad_fn=<AddBackward0>)
0.60359627
tensor(0.1631, device='cuda:0', grad_fn=<AddBackward0>)
0.60346287
tensor(0.3298, device='cuda:0', grad_fn=<AddBackward0>)
0.60334063
tensor(0.2102, device='cuda:0', grad_fn=<AddBackward0>)
0.60317397
tensor(0.2888, device='cuda:0', grad_fn=<AddBackward0>)
0.60326219
tensor(0.1914, device='cuda:0', grad_fn=<AddBackward0>)
0.60344589
tensor(0.2367, device='cuda:0', grad_fn=<AddBackward0>)
0.60330850
tensor(0.2010, device='cuda:0', grad_fn=<AddBackward0>)
0.60319942
tensor(0.2582, device='cuda:0', grad_fn=<AddBackward0>)
0.60321999
tensor(0.1917, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][  120/  196]   Loss 0.218820   Top1 92.604167   Top5 99.876302   BatchTime 0.308979   LR 0.000125
0.60301197
tensor(0.1957, device='cuda:0', grad_fn=<AddBackward0>)
0.60291338
tensor(0.1834, device='cuda:0', grad_fn=<AddBackward0>)
0.60285336
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
0.60276467
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
0.60268837
tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)
0.60258955
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
0.60242164
tensor(0.2732, device='cuda:0', grad_fn=<AddBackward0>)
0.60236484
tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)
0.60243088
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.60221696
tensor(0.2192, device='cuda:0', grad_fn=<AddBackward0>)
0.60225183
tensor(0.2156, device='cuda:0', grad_fn=<AddBackward0>)
0.60209709
tensor(0.2153, device='cuda:0', grad_fn=<AddBackward0>)
0.60212857
tensor(0.2234, device='cuda:0', grad_fn=<AddBackward0>)
0.60203117
tensor(0.1891, device='cuda:0', grad_fn=<AddBackward0>)
0.60210949
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
0.60213012
tensor(0.2250, device='cuda:0', grad_fn=<AddBackward0>)
0.60228550
tensor(0.1963, device='cuda:0', grad_fn=<AddBackward0>)
0.60256022
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.60247517
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][  140/  196]   Loss 0.218326   Top1 92.589286   Top5 99.880022   BatchTime 0.309249   LR 0.000124
0.60221595
tensor(0.2619, device='cuda:0', grad_fn=<AddBackward0>)
0.60212398
tensor(0.3605, device='cuda:0', grad_fn=<AddBackward0>)
0.60212225
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
0.60212541
tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)
0.60195845
tensor(0.2205, device='cuda:0', grad_fn=<AddBackward0>)
0.60197878
tensor(0.2139, device='cuda:0', grad_fn=<AddBackward0>)
0.60190558
tensor(0.2088, device='cuda:0', grad_fn=<AddBackward0>)
0.60210735
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
0.60225332
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.60230303
tensor(0.2390, device='cuda:0', grad_fn=<AddBackward0>)
0.60212046
tensor(0.2242, device='cuda:0', grad_fn=<AddBackward0>)
0.60210943
tensor(0.2425, device='cuda:0', grad_fn=<AddBackward0>)
0.60221368
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
0.60219568
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.60201430
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
0.60184747
tensor(0.3103, device='cuda:0', grad_fn=<AddBackward0>)
0.60182726
tensor(0.2343, device='cuda:0', grad_fn=<AddBackward0>)
0.60177636
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
0.60185546
tensor(0.2878, device='cuda:0', grad_fn=<AddBackward0>)
0.60172182
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
0.60170203
tensor(0.3068, device='cuda:0', grad_fn=<AddBackward0>)
0.60172588
tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>)
0.60162318
tensor(0.2096, device='cuda:0', grad_fn=<AddBackward0>)
0.60160792
tensor(0.2257, device='cuda:0', grad_fn=<AddBackward0>)
0.60150963
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
0.60149866
tensor(0.2033, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][  160/  196]   Loss 0.217562   Top1 92.614746   Top5 99.868164   BatchTime 0.310888   LR 0.000124
0.60158497
tensor(0.1846, device='cuda:0', grad_fn=<AddBackward0>)
0.60150009
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
0.60147083
tensor(0.2634, device='cuda:0', grad_fn=<AddBackward0>)
0.60135961
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
0.60142708
tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)
0.60155475
tensor(0.2004, device='cuda:0', grad_fn=<AddBackward0>)
0.60152662
tensor(0.1783, device='cuda:0', grad_fn=<AddBackward0>)
0.60165346
tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)
0.60164350
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
0.60174954
tensor(0.3135, device='cuda:0', grad_fn=<AddBackward0>)
0.60169297
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.60168493
tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)
0.60170692
tensor(0.2397, device='cuda:0', grad_fn=<AddBackward0>)
0.60159785
tensor(0.2713, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [31][  180/  196]   Loss 0.216080   Top1 92.645399   Top5 99.874132   BatchTime 0.308169   LR 0.000124
0.60162598
tensor(0.1878, device='cuda:0', grad_fn=<AddBackward0>)
0.60152513
tensor(0.1834, device='cuda:0', grad_fn=<AddBackward0>)
0.60147363
tensor(0.2489, device='cuda:0', grad_fn=<AddBackward0>)
0.60154444
tensor(0.1889, device='cuda:0', grad_fn=<AddBackward0>)
0.60152525
tensor(0.2996, device='cuda:0', grad_fn=<AddBackward0>)
0.60161424
tensor(0.1733, device='cuda:0', grad_fn=<AddBackward0>)
0.60160196
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.60166270
tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)
0.60168034
tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)
0.60151088
tensor(0.3342, device='cuda:0', grad_fn=<AddBackward0>)
0.60142702
tensor(0.1947, device='cuda:0', grad_fn=<AddBackward0>)
0.60133171
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
0.60130656
tensor(0.2377, device='cuda:0', grad_fn=<AddBackward0>)
0.60139126
tensor(0.2250, device='cuda:0', grad_fn=<AddBackward0>)
0.60127330
tensor(0.2446, device='cuda:0', grad_fn=<AddBackward0>)
0.60131681
tensor(0.2411, device='cuda:0', grad_fn=<AddBackward0>)
0.60128820
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
0.60144705
tensor(0.2421, device='cuda:0', grad_fn=<AddBackward0>)
0.60149777
tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)
0.60126978
tensor(0.3544, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 92.630    Top5: 99.876    Loss: 0.217
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [31][   20/   40]   Loss 0.407777   Top1 87.187500   Top5 99.570312   BatchTime 0.124157
INFO - Validation [31][   40/   40]   Loss 0.401168   Top1 87.420000   Top5 99.610000   BatchTime 0.087470
INFO - ==> Top1: 87.420    Top5: 99.610    Loss: 0.401
INFO - ==> Sparsity : 0.591
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.2461)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.1221)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.5700)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1128)
features.4.conv.0 tensor(0.0627)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.2144)
features.5.conv.0 tensor(0.3685)
features.5.conv.3 tensor(0.4172)
features.5.conv.6 tensor(0.3091)
features.6.conv.0 tensor(0.0441)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0852)
features.7.conv.0 tensor(0.1630)
features.7.conv.3 tensor(0.4606)
features.7.conv.6 tensor(0.2048)
features.8.conv.0 tensor(0.5816)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.5258)
features.9.conv.0 tensor(0.5247)
features.9.conv.3 tensor(0.5582)
features.9.conv.6 tensor(0.4686)
features.10.conv.0 tensor(0.0703)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0953)
features.11.conv.0 tensor(0.7440)
features.11.conv.3 tensor(0.6410)
features.11.conv.6 tensor(0.7678)
features.12.conv.0 tensor(0.7425)
features.12.conv.3 tensor(0.6734)
features.12.conv.6 tensor(0.7669)
features.13.conv.0 tensor(0.2349)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.2111)
features.14.conv.0 tensor(0.9089)
features.14.conv.3 tensor(0.8269)
features.14.conv.6 tensor(0.9588)
features.15.conv.0 tensor(0.8777)
features.15.conv.3 tensor(0.8358)
features.15.conv.6 tensor(0.9624)
features.16.conv.0 tensor(0.6540)
features.16.conv.3 tensor(0.8044)
features.16.conv.6 tensor(0.8837)
conv.0 tensor(0.0930)
tensor(1294157.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
0.60120833
tensor(0.2110, device='cuda:0', grad_fn=<AddBackward0>)
0.60119778
tensor(0.2475, device='cuda:0', grad_fn=<AddBackward0>)
0.60128170
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
0.60137337
tensor(0.2036, device='cuda:0', grad_fn=<AddBackward0>)
0.60129052
tensor(0.3269, device='cuda:0', grad_fn=<AddBackward0>)
0.60116625
tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)
0.60126698
tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)
0.60170019
tensor(0.2276, device='cuda:0', grad_fn=<AddBackward0>)
0.60214716
tensor(0.2674, device='cuda:0', grad_fn=<AddBackward0>)
0.60253823
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
0.60266739
tensor(0.2095, device='cuda:0', grad_fn=<AddBackward0>)
0.60269380
tensor(0.2708, device='cuda:0', grad_fn=<AddBackward0>)
0.60279930
tensor(0.2374, device='cuda:0', grad_fn=<AddBackward0>)
0.60277075
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
0.60292232
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
0.60281372
tensor(0.2223, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][   20/  196]   Loss 0.210419   Top1 93.007812   Top5 99.921875   BatchTime 0.393925   LR 0.000124
0.60271466
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.60258490
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
0.60249418
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.60253972
tensor(0.2028, device='cuda:0', grad_fn=<AddBackward0>)
0.60243583
tensor(0.2152, device='cuda:0', grad_fn=<AddBackward0>)
0.60233742
tensor(0.2269, device='cuda:0', grad_fn=<AddBackward0>)
0.60229754
tensor(0.2309, device='cuda:0', grad_fn=<AddBackward0>)
0.60237187
tensor(0.1938, device='cuda:0', grad_fn=<AddBackward0>)
0.60222107
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.60224003
tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)
0.60214156
tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)
0.60215551
tensor(0.2374, device='cuda:0', grad_fn=<AddBackward0>)
0.60203183
tensor(0.2865, device='cuda:0', grad_fn=<AddBackward0>)
0.60182518
tensor(0.1820, device='cuda:0', grad_fn=<AddBackward0>)
0.60162938
tensor(0.2195, device='cuda:0', grad_fn=<AddBackward0>)
0.60173416
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
0.60194260
tensor(0.2554, device='cuda:0', grad_fn=<AddBackward0>)
0.60184854
tensor(0.2231, device='cuda:0', grad_fn=<AddBackward0>)
0.60185295
tensor(0.2130, device='cuda:0', grad_fn=<AddBackward0>)
0.60188806
tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)
0.60185039
tensor(0.2211, device='cuda:0', grad_fn=<AddBackward0>)
0.60181010
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.60174608
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.60168189
tensor(0.1951, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][   40/  196]   Loss 0.214537   Top1 92.832031   Top5 99.912109   BatchTime 0.364828   LR 0.000124
0.60164654
tensor(0.2452, device='cuda:0', grad_fn=<AddBackward0>)
0.60168248
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
0.60172516
tensor(0.2597, device='cuda:0', grad_fn=<AddBackward0>)
0.60179210
tensor(0.2754, device='cuda:0', grad_fn=<AddBackward0>)
0.60200489
tensor(0.2231, device='cuda:0', grad_fn=<AddBackward0>)
0.60188282
tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)
0.60177928
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
0.60171938
tensor(0.2213, device='cuda:0', grad_fn=<AddBackward0>)
0.60161924
tensor(0.2560, device='cuda:0', grad_fn=<AddBackward0>)
0.60141551
tensor(0.2296, device='cuda:0', grad_fn=<AddBackward0>)
0.60134500
tensor(0.2334, device='cuda:0', grad_fn=<AddBackward0>)
0.60129625
tensor(0.2093, device='cuda:0', grad_fn=<AddBackward0>)
0.60132939
tensor(0.1880, device='cuda:0', grad_fn=<AddBackward0>)
0.60124463
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.60128266
tensor(0.2210, device='cuda:0', grad_fn=<AddBackward0>)
0.60155702
tensor(0.2920, device='cuda:0', grad_fn=<AddBackward0>)
0.60142881
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][   60/  196]   Loss 0.215885   Top1 92.773438   Top5 99.895833   BatchTime 0.357796   LR 0.000124
0.60131782
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
0.60104311
tensor(0.2078, device='cuda:0', grad_fn=<AddBackward0>)
0.60090977
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.60079575
tensor(0.2788, device='cuda:0', grad_fn=<AddBackward0>)
0.60072541
tensor(0.2202, device='cuda:0', grad_fn=<AddBackward0>)
0.60066372
tensor(0.2599, device='cuda:0', grad_fn=<AddBackward0>)
0.60056943
tensor(0.2319, device='cuda:0', grad_fn=<AddBackward0>)
0.60045093
tensor(0.2565, device='cuda:0', grad_fn=<AddBackward0>)
0.60045558
tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>)
0.60044456
tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)
0.60050178
tensor(0.1980, device='cuda:0', grad_fn=<AddBackward0>)
0.60066640
tensor(0.2051, device='cuda:0', grad_fn=<AddBackward0>)
0.60076702
tensor(0.2656, device='cuda:0', grad_fn=<AddBackward0>)
0.60086799
tensor(0.2485, device='cuda:0', grad_fn=<AddBackward0>)
0.60096234
tensor(0.2288, device='cuda:0', grad_fn=<AddBackward0>)
0.60118699
tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)
0.60140496
tensor(0.2087, device='cuda:0', grad_fn=<AddBackward0>)
0.60157257
tensor(0.1926, device='cuda:0', grad_fn=<AddBackward0>)
0.60183960
tensor(0.2210, device='cuda:0', grad_fn=<AddBackward0>)
0.60207301
tensor(0.2383, device='cuda:0', grad_fn=<AddBackward0>)
0.60220093
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.60227579
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][   80/  196]   Loss 0.218460   Top1 92.729492   Top5 99.877930   BatchTime 0.337516   LR 0.000124
0.60234886
tensor(0.2542, device='cuda:0', grad_fn=<AddBackward0>)
0.60237479
tensor(0.2010, device='cuda:0', grad_fn=<AddBackward0>)
0.60241854
tensor(0.2340, device='cuda:0', grad_fn=<AddBackward0>)
0.60248244
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
0.60249937
tensor(0.2214, device='cuda:0', grad_fn=<AddBackward0>)
0.60241145
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.60236192
tensor(0.2197, device='cuda:0', grad_fn=<AddBackward0>)
0.60245770
tensor(0.2483, device='cuda:0', grad_fn=<AddBackward0>)
0.60250092
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
0.60252964
tensor(0.2389, device='cuda:0', grad_fn=<AddBackward0>)
0.60238266
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.60223043
tensor(0.2142, device='cuda:0', grad_fn=<AddBackward0>)
0.60217214
tensor(0.2624, device='cuda:0', grad_fn=<AddBackward0>)
0.60226011
tensor(0.3117, device='cuda:0', grad_fn=<AddBackward0>)
0.60233444
tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)
0.60206509
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
0.60203350
tensor(0.2569, device='cuda:0', grad_fn=<AddBackward0>)
0.60196507
tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)
0.60184151
tensor(0.2206, device='cuda:0', grad_fn=<AddBackward0>)
0.60179126
tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)
0.60170454
tensor(0.2260, device='cuda:0', grad_fn=<AddBackward0>)
0.60169429
tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][  100/  196]   Loss 0.217456   Top1 92.691406   Top5 99.882812   BatchTime 0.324305   LR 0.000124
0.60151201
tensor(0.2197, device='cuda:0', grad_fn=<AddBackward0>)
0.60148847
tensor(0.2631, device='cuda:0', grad_fn=<AddBackward0>)
0.60149181
tensor(0.1984, device='cuda:0', grad_fn=<AddBackward0>)
0.60134721
tensor(0.2319, device='cuda:0', grad_fn=<AddBackward0>)
0.60127866
tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)
0.60123742
tensor(0.2448, device='cuda:0', grad_fn=<AddBackward0>)
0.60128170
tensor(0.2157, device='cuda:0', grad_fn=<AddBackward0>)
0.60122973
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
0.60118067
tensor(0.1691, device='cuda:0', grad_fn=<AddBackward0>)
0.60111082
tensor(0.2807, device='cuda:0', grad_fn=<AddBackward0>)
0.60099459
tensor(0.2451, device='cuda:0', grad_fn=<AddBackward0>)
0.60079324
tensor(0.2634, device='cuda:0', grad_fn=<AddBackward0>)
0.60070026
tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)
0.60071570
tensor(0.2754, device='cuda:0', grad_fn=<AddBackward0>)
0.60063076
tensor(0.2545, device='cuda:0', grad_fn=<AddBackward0>)
0.60050493
tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)
0.60054410
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
0.60055757
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
0.60062051
tensor(0.2098, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][  120/  196]   Loss 0.217795   Top1 92.666016   Top5 99.886068   BatchTime 0.321386   LR 0.000124
0.60065866
tensor(0.2886, device='cuda:0', grad_fn=<AddBackward0>)
0.60053301
tensor(0.2185, device='cuda:0', grad_fn=<AddBackward0>)
0.60047531
tensor(0.2387, device='cuda:0', grad_fn=<AddBackward0>)
0.60050541
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
0.60041666
tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)
0.60041064
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.60039729
tensor(0.2404, device='cuda:0', grad_fn=<AddBackward0>)
0.60040146
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
0.60024095
tensor(0.1997, device='cuda:0', grad_fn=<AddBackward0>)
0.60018712
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
0.60007250
tensor(0.2742, device='cuda:0', grad_fn=<AddBackward0>)
0.60000277
tensor(0.2586, device='cuda:0', grad_fn=<AddBackward0>)
0.60007256
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
0.60002190
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
0.59996730
tensor(0.2860, device='cuda:0', grad_fn=<AddBackward0>)
0.59983683
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
0.59975803
tensor(0.2329, device='cuda:0', grad_fn=<AddBackward0>)
0.59970677
tensor(0.2491, device='cuda:0', grad_fn=<AddBackward0>)
0.59964442
tensor(0.2155, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][  140/  196]   Loss 0.217247   Top1 92.675781   Top5 99.893973   BatchTime 0.322328   LR 0.000124
0.59948426
tensor(0.1995, device='cuda:0', grad_fn=<AddBackward0>)
0.59946668
tensor(0.2193, device='cuda:0', grad_fn=<AddBackward0>)
0.59947878
tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)
0.59951943
tensor(0.2012, device='cuda:0', grad_fn=<AddBackward0>)
0.59953564
tensor(0.2514, device='cuda:0', grad_fn=<AddBackward0>)
0.59934580
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
0.59921318
tensor(0.2341, device='cuda:0', grad_fn=<AddBackward0>)
0.59928834
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
0.59919494
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
0.59910262
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
0.59908944
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
0.59904355
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.59905660
tensor(0.2380, device='cuda:0', grad_fn=<AddBackward0>)
0.59912163
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
0.59926182
tensor(0.2485, device='cuda:0', grad_fn=<AddBackward0>)
0.60034591
tensor(0.1957, device='cuda:0', grad_fn=<AddBackward0>)
0.60284996
tensor(0.2108, device='cuda:0', grad_fn=<AddBackward0>)
0.60433900
tensor(0.2367, device='cuda:0', grad_fn=<AddBackward0>)
0.60423106
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][  160/  196]   Loss 0.215428   Top1 92.690430   Top5 99.902344   BatchTime 0.321187   LR 0.000123
0.60426581
tensor(0.2137, device='cuda:0', grad_fn=<AddBackward0>)
0.60407728
tensor(0.2176, device='cuda:0', grad_fn=<AddBackward0>)
0.60395950
tensor(0.1957, device='cuda:0', grad_fn=<AddBackward0>)
0.60376245
tensor(0.3018, device='cuda:0', grad_fn=<AddBackward0>)
0.60378486
tensor(0.1919, device='cuda:0', grad_fn=<AddBackward0>)
0.60368222
tensor(0.2038, device='cuda:0', grad_fn=<AddBackward0>)
0.60355020
tensor(0.2825, device='cuda:0', grad_fn=<AddBackward0>)
0.60341489
tensor(0.2535, device='cuda:0', grad_fn=<AddBackward0>)
0.60323441
tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>)
0.60319668
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
0.60328496
tensor(0.2691, device='cuda:0', grad_fn=<AddBackward0>)
0.60330951
tensor(0.2012, device='cuda:0', grad_fn=<AddBackward0>)
0.60340667
tensor(0.2474, device='cuda:0', grad_fn=<AddBackward0>)
0.60336369
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
0.60329694
tensor(0.2635, device='cuda:0', grad_fn=<AddBackward0>)
0.60326689
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
0.60319948
tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)
0.60316801
tensor(0.2735, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [32][  180/  196]   Loss 0.216587   Top1 92.638889   Top5 99.902344   BatchTime 0.321761   LR 0.000123
0.60323048
tensor(0.1910, device='cuda:0', grad_fn=<AddBackward0>)
0.60330170
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
0.60333186
tensor(0.2292, device='cuda:0', grad_fn=<AddBackward0>)
0.60340786
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.60327721
tensor(0.2970, device='cuda:0', grad_fn=<AddBackward0>)
0.60313171
tensor(0.2671, device='cuda:0', grad_fn=<AddBackward0>)
0.60305977
tensor(0.2221, device='cuda:0', grad_fn=<AddBackward0>)
0.60300803
tensor(0.2390, device='cuda:0', grad_fn=<AddBackward0>)
0.60305381
tensor(0.2884, device='cuda:0', grad_fn=<AddBackward0>)
0.60303384
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
0.60297823
tensor(0.2245, device='cuda:0', grad_fn=<AddBackward0>)
0.60316557
tensor(0.2364, device='cuda:0', grad_fn=<AddBackward0>)
0.60324180
tensor(0.2100, device='cuda:0', grad_fn=<AddBackward0>)
0.60310411
tensor(0.2388, device='cuda:0', grad_fn=<AddBackward0>)
0.60310680
tensor(0.2565, device='cuda:0', grad_fn=<AddBackward0>)
0.60319310
tensor(0.2562, device='cuda:0', grad_fn=<AddBackward0>)
0.60311013
tensor(0.2213, device='cuda:0', grad_fn=<AddBackward0>)
0.60314626
tensor(0.2049, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 92.572    Top5: 99.908    Loss: 0.219
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.60306954
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.60295916
tensor(0.3574, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [32][   20/   40]   Loss 0.411099   Top1 87.128906   Top5 99.453125   BatchTime 0.122003
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0579)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0911)
features.2.conv.0 tensor(0.1105)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.5700)
features.3.conv.0 tensor(0.0851)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1107)
features.4.conv.0 tensor(0.0553)
features.4.conv.3 tensor(0.2998)
features.4.conv.6 tensor(0.2218)
features.5.conv.0 tensor(0.3586)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.3594)
features.6.conv.0 tensor(0.0492)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0843)
features.7.conv.0 tensor(0.1626)
features.7.conv.3 tensor(0.4612)
features.7.conv.6 tensor(0.2056)
features.8.conv.0 tensor(0.5892)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.5307)
features.9.conv.0 tensor(0.5262)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.5492)
features.10.conv.0 tensor(0.0679)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0959)
features.11.conv.0 tensor(0.7457)
features.11.conv.3 tensor(0.6416)
features.11.conv.6 tensor(0.7791)
features.12.conv.0 tensor(0.7405)
features.12.conv.3 tensor(0.6734)
features.12.conv.6 tensor(0.7741)
features.13.conv.0 tensor(0.2585)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.1106)
features.14.conv.0 tensor(0.9084)
features.14.conv.3 tensor(0.8264)
features.14.conv.6 tensor(0.9579)
features.15.conv.0 tensor(0.8773)
features.15.conv.3 tensor(0.8351)
features.15.conv.6 tensor(0.9630)
features.16.conv.0 tensor(0.6530)
features.16.conv.3 tensor(0.8041)
features.16.conv.6 tensor(0.8839)
conv.0 tensor(0.0928)
tensor(1289392.) 2188896.0
INFO - Validation [32][   40/   40]   Loss 0.394178   Top1 87.410000   Top5 99.510000   BatchTime 0.088479
INFO - ==> Top1: 87.410    Top5: 99.510    Loss: 0.394
INFO - ==> Sparsity : 0.589
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
0.60289115
tensor(0.1857, device='cuda:0', grad_fn=<AddBackward0>)
0.60291630
tensor(0.2041, device='cuda:0', grad_fn=<AddBackward0>)
0.60294122
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
0.60296363
tensor(0.1731, device='cuda:0', grad_fn=<AddBackward0>)
0.60275608
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
0.60282314
tensor(0.2423, device='cuda:0', grad_fn=<AddBackward0>)
0.60285097
tensor(0.2472, device='cuda:0', grad_fn=<AddBackward0>)
0.60297000
tensor(0.2701, device='cuda:0', grad_fn=<AddBackward0>)
0.60284078
tensor(0.2183, device='cuda:0', grad_fn=<AddBackward0>)
0.60271758
tensor(0.2449, device='cuda:0', grad_fn=<AddBackward0>)
0.60272914
tensor(0.3068, device='cuda:0', grad_fn=<AddBackward0>)
0.60272735
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
0.60275078
tensor(0.1989, device='cuda:0', grad_fn=<AddBackward0>)
0.60262626
tensor(0.2313, device='cuda:0', grad_fn=<AddBackward0>)
0.60245663
tensor(0.2197, device='cuda:0', grad_fn=<AddBackward0>)
0.60247856
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
0.60247660
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
0.60244101
tensor(0.3021, device='cuda:0', grad_fn=<AddBackward0>)
0.60242373
tensor(0.2290, device='cuda:0', grad_fn=<AddBackward0>)
0.60244608
tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)
0.60240078
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
0.60230178
tensor(0.2191, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][   20/  196]   Loss 0.219194   Top1 92.617188   Top5 99.824219   BatchTime 0.379304   LR 0.000123
0.60217130
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
0.60206681
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
0.60200983
tensor(0.2069, device='cuda:0', grad_fn=<AddBackward0>)
0.60192782
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
0.60188001
tensor(0.2296, device='cuda:0', grad_fn=<AddBackward0>)
0.60188210
tensor(0.1932, device='cuda:0', grad_fn=<AddBackward0>)
0.60208946
tensor(0.2076, device='cuda:0', grad_fn=<AddBackward0>)
0.60216290
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.60207301
tensor(0.2399, device='cuda:0', grad_fn=<AddBackward0>)
0.60220534
tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)
0.60211688
tensor(0.2433, device='cuda:0', grad_fn=<AddBackward0>)
0.60200775
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
0.60203582
tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)
0.60180920
tensor(0.2310, device='cuda:0', grad_fn=<AddBackward0>)
0.60191095
tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)
0.60191578
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
0.60188919
tensor(0.2334, device='cuda:0', grad_fn=<AddBackward0>)
0.60195833
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.60200882
INFO - Training [33][   40/  196]   Loss 0.214230   Top1 92.714844   Top5 99.882812   BatchTime 0.348131   LR 0.000123
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
0.60199153
tensor(0.2257, device='cuda:0', grad_fn=<AddBackward0>)
0.60227013
tensor(0.1857, device='cuda:0', grad_fn=<AddBackward0>)
0.60219091
tensor(0.2396, device='cuda:0', grad_fn=<AddBackward0>)
0.60229284
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
0.60219872
tensor(0.2378, device='cuda:0', grad_fn=<AddBackward0>)
0.60210037
tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)
0.60255158
tensor(0.2483, device='cuda:0', grad_fn=<AddBackward0>)
0.60447031
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
0.60452771
tensor(0.2399, device='cuda:0', grad_fn=<AddBackward0>)
0.60454834
tensor(0.1878, device='cuda:0', grad_fn=<AddBackward0>)
0.60475975
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
0.60485578
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
0.60474205
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
0.60467041
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
0.60461605
tensor(0.2002, device='cuda:0', grad_fn=<AddBackward0>)
0.60465747
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
0.60473216
tensor(0.2593, device='cuda:0', grad_fn=<AddBackward0>)
0.60493898
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][   60/  196]   Loss 0.206499   Top1 93.020833   Top5 99.902344   BatchTime 0.339444   LR 0.000123
0.60488892
tensor(0.1776, device='cuda:0', grad_fn=<AddBackward0>)
0.60495102
tensor(0.1878, device='cuda:0', grad_fn=<AddBackward0>)
0.60478759
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
0.60469717
tensor(0.2013, device='cuda:0', grad_fn=<AddBackward0>)
0.60457438
tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)
0.60457236
tensor(0.2404, device='cuda:0', grad_fn=<AddBackward0>)
0.60470140
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.60473692
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
0.60476738
tensor(0.1932, device='cuda:0', grad_fn=<AddBackward0>)
0.60487878
tensor(0.2169, device='cuda:0', grad_fn=<AddBackward0>)
0.60493869
tensor(0.2178, device='cuda:0', grad_fn=<AddBackward0>)
0.60493100
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
0.60485142
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.60481650
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
0.60476452
tensor(0.2152, device='cuda:0', grad_fn=<AddBackward0>)
0.60478652
tensor(0.2980, device='cuda:0', grad_fn=<AddBackward0>)
0.60468471
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
0.60462773
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][   80/  196]   Loss 0.202928   Top1 93.188477   Top5 99.907227   BatchTime 0.340061   LR 0.000123
0.60457408
tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)
0.60459882
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.60463279
tensor(0.2321, device='cuda:0', grad_fn=<AddBackward0>)
0.60469425
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
0.60475063
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.60499978
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
0.60500729
tensor(0.2347, device='cuda:0', grad_fn=<AddBackward0>)
0.60484123
tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)
0.60478848
tensor(0.2224, device='cuda:0', grad_fn=<AddBackward0>)
0.60480219
tensor(0.2234, device='cuda:0', grad_fn=<AddBackward0>)
0.60473871
tensor(0.1982, device='cuda:0', grad_fn=<AddBackward0>)
0.60449195
tensor(0.2343, device='cuda:0', grad_fn=<AddBackward0>)
0.60435903
tensor(0.1906, device='cuda:0', grad_fn=<AddBackward0>)
0.60438710
tensor(0.2126, device='cuda:0', grad_fn=<AddBackward0>)
0.60445112
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
0.60458142
tensor(0.2732, device='cuda:0', grad_fn=<AddBackward0>)
0.60453427
tensor(0.2776, device='cuda:0', grad_fn=<AddBackward0>)
0.60457522
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.60478067
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
0.60486954
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.60486221
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.60471970
tensor(0.2259, device='cuda:0', grad_fn=<AddBackward0>)
0.60462964
tensor(0.2173, device='cuda:0', grad_fn=<AddBackward0>)
0.60453534
tensor(0.2170, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][  100/  196]   Loss 0.204400   Top1 93.042969   Top5 99.902344   BatchTime 0.337833   LR 0.000123
0.60455716
tensor(0.2947, device='cuda:0', grad_fn=<AddBackward0>)
0.60470593
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.60485125
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.60489279
tensor(0.2576, device='cuda:0', grad_fn=<AddBackward0>)
0.60508674
tensor(0.2611, device='cuda:0', grad_fn=<AddBackward0>)
0.60482550
tensor(0.2456, device='cuda:0', grad_fn=<AddBackward0>)
0.60471541
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.60442835
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.60425621
tensor(0.2334, device='cuda:0', grad_fn=<AddBackward0>)
0.60415238
tensor(0.1906, device='cuda:0', grad_fn=<AddBackward0>)
0.60409319
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
0.60411584
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
0.60402918
tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)
0.60403377
tensor(0.1917, device='cuda:0', grad_fn=<AddBackward0>)
0.60411912
tensor(0.2184, device='cuda:0', grad_fn=<AddBackward0>)
0.60411596
tensor(0.2234, device='cuda:0', grad_fn=<AddBackward0>)
0.60414296
tensor(0.2391, device='cuda:0', grad_fn=<AddBackward0>)
0.60414648
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
0.60417366
tensor(0.1839, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][  120/  196]   Loss 0.205815   Top1 92.985026   Top5 99.899089   BatchTime 0.334524   LR 0.000123
0.60420907
tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)
0.60430640
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
0.60439378
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
0.60453212
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
0.60467929
tensor(0.2348, device='cuda:0', grad_fn=<AddBackward0>)
0.60473472
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.60462123
tensor(0.2062, device='cuda:0', grad_fn=<AddBackward0>)
0.60445613
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.60443091
tensor(0.3139, device='cuda:0', grad_fn=<AddBackward0>)
0.60455561
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
0.60451561
tensor(0.2313, device='cuda:0', grad_fn=<AddBackward0>)
0.60458606
tensor(0.2762, device='cuda:0', grad_fn=<AddBackward0>)
0.60468966
tensor(0.2995, device='cuda:0', grad_fn=<AddBackward0>)
0.60460663
tensor(0.1979, device='cuda:0', grad_fn=<AddBackward0>)
0.60455406
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
0.60481030
tensor(0.2157, device='cuda:0', grad_fn=<AddBackward0>)
0.60490173
tensor(0.2515, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][  140/  196]   Loss 0.207742   Top1 92.957589   Top5 99.891183   BatchTime 0.335914   LR 0.000122
0.60483444
tensor(0.2298, device='cuda:0', grad_fn=<AddBackward0>)
0.60467631
tensor(0.2202, device='cuda:0', grad_fn=<AddBackward0>)
0.60460651
tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>)
0.60458267
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.60452008
tensor(0.2331, device='cuda:0', grad_fn=<AddBackward0>)
0.60440904
tensor(0.2187, device='cuda:0', grad_fn=<AddBackward0>)
0.60428822
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
0.60432130
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.60438764
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
0.60461897
tensor(0.2772, device='cuda:0', grad_fn=<AddBackward0>)
0.60458714
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
0.60453898
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.60441828
tensor(0.2484, device='cuda:0', grad_fn=<AddBackward0>)
0.60437858
tensor(0.2205, device='cuda:0', grad_fn=<AddBackward0>)
0.60433102
tensor(0.2791, device='cuda:0', grad_fn=<AddBackward0>)
0.60425234
tensor(0.2417, device='cuda:0', grad_fn=<AddBackward0>)
0.60426569
tensor(0.2122, device='cuda:0', grad_fn=<AddBackward0>)
0.60448188
tensor(0.2915, device='cuda:0', grad_fn=<AddBackward0>)
0.60449928
tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)
0.60460907
tensor(0.2071, device='cuda:0', grad_fn=<AddBackward0>)
0.60459572
tensor(0.2045, device='cuda:0', grad_fn=<AddBackward0>)
0.60446292
tensor(0.2247, device='cuda:0', grad_fn=<AddBackward0>)
0.60440296
tensor(0.1975, device='cuda:0', grad_fn=<AddBackward0>)
0.60440230
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][  160/  196]   Loss 0.208320   Top1 92.990723   Top5 99.885254   BatchTime 0.335499   LR 0.000122
0.60431439
tensor(0.2146, device='cuda:0', grad_fn=<AddBackward0>)
0.60432637
tensor(0.2706, device='cuda:0', grad_fn=<AddBackward0>)
0.60428035
tensor(0.2342, device='cuda:0', grad_fn=<AddBackward0>)
0.60418761
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
0.60415322
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.60409564
tensor(0.2879, device='cuda:0', grad_fn=<AddBackward0>)
0.60420567
tensor(0.2635, device='cuda:0', grad_fn=<AddBackward0>)
0.60430491
tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)
0.60434139
tensor(0.2838, device='cuda:0', grad_fn=<AddBackward0>)
0.60438251
tensor(0.2074, device='cuda:0', grad_fn=<AddBackward0>)
0.60442317
tensor(0.2088, device='cuda:0', grad_fn=<AddBackward0>)
0.60447943
tensor(0.2583, device='cuda:0', grad_fn=<AddBackward0>)
0.60416007
tensor(0.2125, device='cuda:0', grad_fn=<AddBackward0>)
0.60405988
tensor(0.2460, device='cuda:0', grad_fn=<AddBackward0>)
0.60398024
tensor(0.2586, device='cuda:0', grad_fn=<AddBackward0>)
0.60387564
tensor(0.2156, device='cuda:0', grad_fn=<AddBackward0>)
0.60392112
tensor(0.2094, device='cuda:0', grad_fn=<AddBackward0>)
0.60390365
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
0.60393113
tensor(0.2338, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [33][  180/  196]   Loss 0.210551   Top1 92.918837   Top5 99.891493   BatchTime 0.335088   LR 0.000122
0.60395485
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
0.60395253
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
0.60400605
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.60393190
tensor(0.3111, device='cuda:0', grad_fn=<AddBackward0>)
0.60383230
tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)
0.60379434
tensor(0.2619, device='cuda:0', grad_fn=<AddBackward0>)
0.60386115
tensor(0.2825, device='cuda:0', grad_fn=<AddBackward0>)
0.60391915
tensor(0.2414, device='cuda:0', grad_fn=<AddBackward0>)
0.60371780
tensor(0.2226, device='cuda:0', grad_fn=<AddBackward0>)
0.60361737
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.60369587
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
0.60370088
tensor(0.2089, device='cuda:0', grad_fn=<AddBackward0>)
0.60360456
tensor(0.1644, device='cuda:0', grad_fn=<AddBackward0>)
0.60333884
tensor(0.2402, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 92.886    Top5: 99.886    Loss: 0.211
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.60319984
tensor(0.3413, device='cuda:0', grad_fn=<AddBackward0>)
0.60286671
tensor(0.3074, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 0.395951   Top1 87.753906   Top5 99.433594   BatchTime 0.119044
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.2676)
features.1.conv.0 tensor(0.0632)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0946)
features.2.conv.0 tensor(0.1059)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.5703)
features.3.conv.0 tensor(0.0822)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1157)
features.4.conv.0 tensor(0.0557)
features.4.conv.3 tensor(0.2986)
features.4.conv.6 tensor(0.2201)
features.5.conv.0 tensor(0.3465)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.3911)
features.6.conv.0 tensor(0.0475)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0841)
features.7.conv.0 tensor(0.1717)
features.7.conv.3 tensor(0.4633)
features.7.conv.6 tensor(0.2056)
features.8.conv.0 tensor(0.5802)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.5304)
features.9.conv.0 tensor(0.4978)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.5688)
features.10.conv.0 tensor(0.0629)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0963)
features.11.conv.0 tensor(0.7542)
features.11.conv.3 tensor(0.6420)
features.11.conv.6 tensor(0.7879)
features.12.conv.0 tensor(0.7383)
features.12.conv.3 tensor(0.6740)
features.12.conv.6 tensor(0.7801)
features.13.conv.0 tensor(0.2178)
features.13.conv.3 tensor(0.4923)
features.13.conv.6 tensor(0.1033)
features.14.conv.0 tensor(0.9080)
features.14.conv.3 tensor(0.8267)
features.14.conv.6 tensor(0.9575)
features.15.conv.0 tensor(0.8793)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9627)
features.16.conv.0 tensor(0.6556)
features.16.conv.3 tensor(0.8046)
features.16.conv.6 tensor(0.8845)
conv.0 tensor(0.0914)
tensor(1287616.) 2188896.0
INFO - Validation [33][   40/   40]   Loss 0.387605   Top1 87.750000   Top5 99.510000   BatchTime 0.086448
INFO - ==> Top1: 87.750    Top5: 99.510    Loss: 0.388
INFO - ==> Sparsity : 0.588
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
0.60267025
tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)
0.60271984
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.60269910
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
0.60261875
tensor(0.2106, device='cuda:0', grad_fn=<AddBackward0>)
0.60263407
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.60258478
tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)
0.60252976
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
0.60264945
tensor(0.2417, device='cuda:0', grad_fn=<AddBackward0>)
0.60270590
tensor(0.2125, device='cuda:0', grad_fn=<AddBackward0>)
0.60269773
tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)
0.60249656
tensor(0.2244, device='cuda:0', grad_fn=<AddBackward0>)
0.60247034
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
0.60245180
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
0.60242224
tensor(0.2141, device='cuda:0', grad_fn=<AddBackward0>)
0.60233283
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
0.60241723
tensor(0.2207, device='cuda:0', grad_fn=<AddBackward0>)
0.60248995
tensor(0.1879, device='cuda:0', grad_fn=<AddBackward0>)
0.60247326
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
0.60241222
tensor(0.1794, device='cuda:0', grad_fn=<AddBackward0>)
0.60237896
tensor(0.2368, device='cuda:0', grad_fn=<AddBackward0>)
0.60243762
tensor(0.2773, device='cuda:0', grad_fn=<AddBackward0>)
0.60245156
tensor(0.2100, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][   20/  196]   Loss 0.197557   Top1 93.535156   Top5 99.882812   BatchTime 0.362790   LR 0.000122
0.60234618
tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>)
0.60234004
tensor(0.2062, device='cuda:0', grad_fn=<AddBackward0>)
0.60222203
tensor(0.2284, device='cuda:0', grad_fn=<AddBackward0>)
0.60199916
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
0.60192299
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.60183728
tensor(0.2314, device='cuda:0', grad_fn=<AddBackward0>)
0.60174257
tensor(0.2288, device='cuda:0', grad_fn=<AddBackward0>)
0.60169035
tensor(0.2622, device='cuda:0', grad_fn=<AddBackward0>)
0.60165745
tensor(0.1995, device='cuda:0', grad_fn=<AddBackward0>)
0.60135472
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
0.60140002
tensor(0.2413, device='cuda:0', grad_fn=<AddBackward0>)
0.60151267
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
0.60130829
tensor(0.2258, device='cuda:0', grad_fn=<AddBackward0>)
0.60110891
tensor(0.2599, device='cuda:0', grad_fn=<AddBackward0>)
0.60103488
tensor(0.2307, device='cuda:0', grad_fn=<AddBackward0>)
0.60095924
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
0.60108376
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.60101193
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.60087162
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][   40/  196]   Loss 0.203903   Top1 93.164062   Top5 99.892578   BatchTime 0.342840   LR 0.000122
0.60060799
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.60037905
tensor(0.2583, device='cuda:0', grad_fn=<AddBackward0>)
0.60030615
tensor(0.2128, device='cuda:0', grad_fn=<AddBackward0>)
0.60021383
tensor(0.1938, device='cuda:0', grad_fn=<AddBackward0>)
0.60018057
tensor(0.1891, device='cuda:0', grad_fn=<AddBackward0>)
0.60010576
tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)
0.60000771
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
0.60000235
tensor(0.1964, device='cuda:0', grad_fn=<AddBackward0>)
0.59989947
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
0.59973121
tensor(0.2212, device='cuda:0', grad_fn=<AddBackward0>)
0.59965748
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.59962839
tensor(0.2770, device='cuda:0', grad_fn=<AddBackward0>)
0.59962308
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
0.59959078
tensor(0.2926, device='cuda:0', grad_fn=<AddBackward0>)
0.59953314
tensor(0.2025, device='cuda:0', grad_fn=<AddBackward0>)
0.59954286
tensor(0.2159, device='cuda:0', grad_fn=<AddBackward0>)
0.59941655
tensor(0.2528, device='cuda:0', grad_fn=<AddBackward0>)
0.59947145
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
0.59957850
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][   60/  196]   Loss 0.206187   Top1 92.981771   Top5 99.889323   BatchTime 0.333063   LR 0.000121
0.59961802
tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)
0.59928459
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
0.59929228
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
0.59921992
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
0.59920359
tensor(0.2591, device='cuda:0', grad_fn=<AddBackward0>)
0.59913939
tensor(0.2633, device='cuda:0', grad_fn=<AddBackward0>)
0.59905815
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
0.59908974
tensor(0.2246, device='cuda:0', grad_fn=<AddBackward0>)
0.59908485
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
0.59917122
tensor(0.1917, device='cuda:0', grad_fn=<AddBackward0>)
0.59938020
tensor(0.2215, device='cuda:0', grad_fn=<AddBackward0>)
0.59940487
tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)
0.59956026
tensor(0.2089, device='cuda:0', grad_fn=<AddBackward0>)
0.59963709
tensor(0.2277, device='cuda:0', grad_fn=<AddBackward0>)
0.59960848
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
0.59947270
tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)
0.59909266
tensor(0.2441, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][   80/  196]   Loss 0.205778   Top1 93.041992   Top5 99.892578   BatchTime 0.336483   LR 0.000121
0.59888798
tensor(0.2753, device='cuda:0', grad_fn=<AddBackward0>)
0.59884840
tensor(0.2339, device='cuda:0', grad_fn=<AddBackward0>)
0.59893727
tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)
0.59918529
tensor(0.2262, device='cuda:0', grad_fn=<AddBackward0>)
0.59923512
tensor(0.2849, device='cuda:0', grad_fn=<AddBackward0>)
0.59934175
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.59924066
tensor(0.2822, device='cuda:0', grad_fn=<AddBackward0>)
0.59911776
tensor(0.2262, device='cuda:0', grad_fn=<AddBackward0>)
0.59901673
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
0.59893090
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.59881884
tensor(0.1949, device='cuda:0', grad_fn=<AddBackward0>)
0.59869820
tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)
0.59866977
tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)
0.59867477
tensor(0.2435, device='cuda:0', grad_fn=<AddBackward0>)
0.59876251
tensor(0.2815, device='cuda:0', grad_fn=<AddBackward0>)
0.59878677
tensor(0.2152, device='cuda:0', grad_fn=<AddBackward0>)
0.59882736
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
0.59879899
tensor(0.2472, device='cuda:0', grad_fn=<AddBackward0>)
0.59873831
tensor(0.2362, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][  100/  196]   Loss 0.208471   Top1 92.890625   Top5 99.882812   BatchTime 0.330887   LR 0.000121
0.59867102
tensor(0.2467, device='cuda:0', grad_fn=<AddBackward0>)
0.59866190
tensor(0.2400, device='cuda:0', grad_fn=<AddBackward0>)
0.59859800
tensor(0.1562, device='cuda:0', grad_fn=<AddBackward0>)
0.59848243
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.59825194
tensor(0.2429, device='cuda:0', grad_fn=<AddBackward0>)
0.59821302
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
0.59834951
tensor(0.2978, device='cuda:0', grad_fn=<AddBackward0>)
0.59837377
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
0.59836584
tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)
0.59835964
tensor(0.1724, device='cuda:0', grad_fn=<AddBackward0>)
0.59825718
tensor(0.1943, device='cuda:0', grad_fn=<AddBackward0>)
0.59814399
tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)
0.59801960
tensor(0.2315, device='cuda:0', grad_fn=<AddBackward0>)
0.59797341
tensor(0.1903, device='cuda:0', grad_fn=<AddBackward0>)
0.59800577
tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)
0.59794956
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
0.59791720
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
0.59794968
tensor(0.2150, device='cuda:0', grad_fn=<AddBackward0>)
0.59814131
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
0.59825766
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.59831440
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
0.59818536
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
0.59816664
tensor(0.1889, device='cuda:0', grad_fn=<AddBackward0>)
0.59824830
tensor(0.2333, device='cuda:0', grad_fn=<AddBackward0>)
0.59829813
tensor(0.2625, device='cuda:0', grad_fn=<AddBackward0>)
0.59858829
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.59858727
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][  120/  196]   Loss 0.207185   Top1 92.942708   Top5 99.889323   BatchTime 0.327474   LR 0.000121
0.59846735
tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)
0.59852409
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
0.59854084
tensor(0.2642, device='cuda:0', grad_fn=<AddBackward0>)
0.59844846
tensor(0.1959, device='cuda:0', grad_fn=<AddBackward0>)
0.59840316
tensor(0.1866, device='cuda:0', grad_fn=<AddBackward0>)
0.59859508
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.59858739
tensor(0.2332, device='cuda:0', grad_fn=<AddBackward0>)
0.59860122
tensor(0.2059, device='cuda:0', grad_fn=<AddBackward0>)
0.59851587
tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)
0.59855425
tensor(0.1785, device='cuda:0', grad_fn=<AddBackward0>)
0.59860814
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
0.59865659
tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)
0.59856856
tensor(0.2517, device='cuda:0', grad_fn=<AddBackward0>)
0.59852940
tensor(0.2724, device='cuda:0', grad_fn=<AddBackward0>)
0.59845173
tensor(0.2766, device='cuda:0', grad_fn=<AddBackward0>)
0.59850425
tensor(0.3022, device='cuda:0', grad_fn=<AddBackward0>)
0.59857738
tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][  140/  196]   Loss 0.208321   Top1 92.865513   Top5 99.888393   BatchTime 0.328187   LR 0.000121
0.59849846
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
0.59843159
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
0.59837216
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
0.59844261
tensor(0.2557, device='cuda:0', grad_fn=<AddBackward0>)
0.59849495
tensor(0.2756, device='cuda:0', grad_fn=<AddBackward0>)
0.59868330
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
0.59865391
tensor(0.3367, device='cuda:0', grad_fn=<AddBackward0>)
0.59851176
tensor(0.1787, device='cuda:0', grad_fn=<AddBackward0>)
0.59849370
tensor(0.2182, device='cuda:0', grad_fn=<AddBackward0>)
0.59849524
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
0.59843814
tensor(0.2113, device='cuda:0', grad_fn=<AddBackward0>)
0.59830028
tensor(0.1868, device='cuda:0', grad_fn=<AddBackward0>)
0.59831226
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
0.59805769
tensor(0.2516, device='cuda:0', grad_fn=<AddBackward0>)
0.59802896
tensor(0.2215, device='cuda:0', grad_fn=<AddBackward0>)
0.59810895
tensor(0.2422, device='cuda:0', grad_fn=<AddBackward0>)
0.59814417
tensor(0.2734, device='cuda:0', grad_fn=<AddBackward0>)
0.59817564
tensor(0.2661, device='cuda:0', grad_fn=<AddBackward0>)
0.59822148
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
0.59826159
tensor(0.2341, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][  160/  196]   Loss 0.208886   Top1 92.895508   Top5 99.887695   BatchTime 0.325819   LR 0.000121
0.59822798
tensor(0.2244, device='cuda:0', grad_fn=<AddBackward0>)
0.59823185
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.59818381
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
0.59810179
tensor(0.1893, device='cuda:0', grad_fn=<AddBackward0>)
0.59806150
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
0.59812486
tensor(0.2234, device='cuda:0', grad_fn=<AddBackward0>)
0.59815723
tensor(0.2174, device='cuda:0', grad_fn=<AddBackward0>)
0.59813231
tensor(0.2527, device='cuda:0', grad_fn=<AddBackward0>)
0.59799474
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
0.59810406
tensor(0.2662, device='cuda:0', grad_fn=<AddBackward0>)
0.59827936
tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)
0.59818590
tensor(0.2473, device='cuda:0', grad_fn=<AddBackward0>)
0.59810120
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.59800518
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
0.59799421
tensor(0.2211, device='cuda:0', grad_fn=<AddBackward0>)
0.59811747
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.59794956
tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>)
0.59785807
tensor(0.2209, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [34][  180/  196]   Loss 0.208764   Top1 92.894965   Top5 99.893663   BatchTime 0.327102   LR 0.000120
0.59773391
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
0.59760559
tensor(0.2020, device='cuda:0', grad_fn=<AddBackward0>)
0.59760368
tensor(0.2444, device='cuda:0', grad_fn=<AddBackward0>)
0.59757853
tensor(0.2180, device='cuda:0', grad_fn=<AddBackward0>)
0.59756839
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.59768903
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
0.59771824
tensor(0.2207, device='cuda:0', grad_fn=<AddBackward0>)
0.59777904
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
0.59784597
tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)
0.59780347
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
0.59772289
tensor(0.2504, device='cuda:0', grad_fn=<AddBackward0>)
0.59769070
tensor(0.2858, device='cuda:0', grad_fn=<AddBackward0>)
0.59772748
tensor(0.2543, device='cuda:0', grad_fn=<AddBackward0>)
0.59777880
tensor(0.2027, device='cuda:0', grad_fn=<AddBackward0>)
0.59778392
tensor(0.2867, device='cuda:0', grad_fn=<AddBackward0>)
0.59769708
tensor(0.1857, device='cuda:0', grad_fn=<AddBackward0>)
0.59772438
tensor(0.2007, device='cuda:0', grad_fn=<AddBackward0>)
0.59788513
tensor(0.1741, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 92.886    Top5: 99.894    Loss: 0.209
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.393624   Top1 87.832031   Top5 99.335938   BatchTime 0.119368
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.2871)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0951)
features.2.conv.0 tensor(0.1264)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.5715)
features.3.conv.0 tensor(0.0868)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1144)
features.4.conv.0 tensor(0.0586)
features.4.conv.3 tensor(0.3009)
features.4.conv.6 tensor(0.2350)
features.5.conv.0 tensor(0.3545)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.4634)
features.6.conv.0 tensor(0.0518)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0838)
features.7.conv.0 tensor(0.1618)
features.7.conv.3 tensor(0.4589)
features.7.conv.6 tensor(0.2113)
features.8.conv.0 tensor(0.5832)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.5482)
features.9.conv.0 tensor(0.5363)
features.9.conv.3 tensor(0.5561)
features.9.conv.6 tensor(0.5871)
features.10.conv.0 tensor(0.0676)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0957)
features.11.conv.0 tensor(0.7462)
features.11.conv.3 tensor(0.6443)
features.11.conv.6 tensor(0.7865)
features.12.conv.0 tensor(0.7404)
features.12.conv.3 tensor(0.6740)
features.12.conv.6 tensor(0.7840)
features.13.conv.0 tensor(0.2180)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.1012)
features.14.conv.0 tensor(0.9106)
features.14.conv.3 tensor(0.8257)
features.14.conv.6 tensor(0.9577)
features.15.conv.0 tensor(0.8817)
features.15.conv.3 tensor(0.8361)
features.15.conv.6 tensor(0.9634)
features.16.conv.0 tensor(0.6554)
features.16.conv.3 tensor(0.8038)
features.16.conv.6 tensor(0.8852)
conv.0 tensor(0.0906)
tensor(1290595.) 2188896.0
INFO - Validation [34][   40/   40]   Loss 0.385077   Top1 87.910000   Top5 99.580000   BatchTime 0.087339
INFO - ==> Top1: 87.910    Top5: 99.580    Loss: 0.385
INFO - ==> Sparsity : 0.590
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
0.59775341
tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)
0.59763646
tensor(0.2179, device='cuda:0', grad_fn=<AddBackward0>)
0.59773988
tensor(0.2028, device='cuda:0', grad_fn=<AddBackward0>)
0.59799683
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
0.59793967
tensor(0.2180, device='cuda:0', grad_fn=<AddBackward0>)
0.59792274
tensor(0.2369, device='cuda:0', grad_fn=<AddBackward0>)
0.59781748
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
0.59779942
tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)
0.59775954
tensor(0.2067, device='cuda:0', grad_fn=<AddBackward0>)
0.59782141
tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)
0.59777623
tensor(0.2401, device='cuda:0', grad_fn=<AddBackward0>)
0.59782970
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.59786713
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
0.59792191
tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)
0.59796369
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
0.59784263
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
0.59789526
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.59794956
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.59798127
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.59782928
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
0.59782106
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
0.59794480
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][   20/  196]   Loss 0.184891   Top1 93.671875   Top5 99.902344   BatchTime 0.382667   LR 0.000120
0.59802002
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.59800136
tensor(0.2095, device='cuda:0', grad_fn=<AddBackward0>)
0.59798479
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
0.59792095
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.59788895
tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)
0.59784484
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
0.59775746
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
0.59765726
tensor(0.2103, device='cuda:0', grad_fn=<AddBackward0>)
0.59780103
tensor(0.2446, device='cuda:0', grad_fn=<AddBackward0>)
0.59763998
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
0.59752125
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
0.59759414
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
0.59760016
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.59759516
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.59752780
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
0.59749347
tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)
0.59740657
tensor(0.2177, device='cuda:0', grad_fn=<AddBackward0>)
0.59738880
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
0.59738159
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][   40/  196]   Loss 0.179389   Top1 93.984375   Top5 99.912109   BatchTime 0.345328   LR 0.000120
0.59741592
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
0.59742153
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
0.59739244
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
0.59740013
tensor(0.1983, device='cuda:0', grad_fn=<AddBackward0>)
0.59749210
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
0.59750289
tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)
0.59740627
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
0.59726143
tensor(0.2604, device='cuda:0', grad_fn=<AddBackward0>)
0.59728980
tensor(0.1938, device='cuda:0', grad_fn=<AddBackward0>)
0.59727323
tensor(0.2919, device='cuda:0', grad_fn=<AddBackward0>)
0.59712559
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
0.59701562
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.59709710
tensor(0.1964, device='cuda:0', grad_fn=<AddBackward0>)
0.59703833
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
0.59691238
tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)
0.59667522
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.59641904
tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)
0.59671724
tensor(0.2302, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][   60/  196]   Loss 0.183309   Top1 93.860677   Top5 99.889323   BatchTime 0.341553   LR 0.000120
0.59676021
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
0.59669888
tensor(0.2089, device='cuda:0', grad_fn=<AddBackward0>)
0.59661436
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
0.59661269
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
0.59660107
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.59633023
tensor(0.2493, device='cuda:0', grad_fn=<AddBackward0>)
0.59626675
tensor(0.2681, device='cuda:0', grad_fn=<AddBackward0>)
0.59637082
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
0.59639281
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.59612107
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.59588248
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.59567332
tensor(0.2436, device='cuda:0', grad_fn=<AddBackward0>)
0.59576923
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
0.59572649
tensor(0.1890, device='cuda:0', grad_fn=<AddBackward0>)
0.59573400
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
0.59584385
tensor(0.2918, device='cuda:0', grad_fn=<AddBackward0>)
0.59585595
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.59586841
tensor(0.2176, device='cuda:0', grad_fn=<AddBackward0>)
0.59576720
tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)
0.59582573
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][   80/  196]   Loss 0.187035   Top1 93.686523   Top5 99.887695   BatchTime 0.331058   LR 0.000119
0.59582192
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
0.59579432
tensor(0.2077, device='cuda:0', grad_fn=<AddBackward0>)
0.59570044
tensor(0.2087, device='cuda:0', grad_fn=<AddBackward0>)
0.59573179
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.59575635
tensor(0.1719, device='cuda:0', grad_fn=<AddBackward0>)
0.59566087
tensor(0.1814, device='cuda:0', grad_fn=<AddBackward0>)
0.59566230
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.59577811
tensor(0.2231, device='cuda:0', grad_fn=<AddBackward0>)
0.59573102
tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)
0.59589791
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
0.59586698
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
0.59583282
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
0.59599477
tensor(0.2659, device='cuda:0', grad_fn=<AddBackward0>)
0.59603310
tensor(0.2111, device='cuda:0', grad_fn=<AddBackward0>)
0.59596437
tensor(0.2444, device='cuda:0', grad_fn=<AddBackward0>)
0.59587604
tensor(0.2594, device='cuda:0', grad_fn=<AddBackward0>)
0.59575450
tensor(0.1656, device='cuda:0', grad_fn=<AddBackward0>)
0.59569824
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
0.59564817
tensor(0.2347, device='cuda:0', grad_fn=<AddBackward0>)
0.59554100
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
0.59551483
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][  100/  196]   Loss 0.189361   Top1 93.554688   Top5 99.898438   BatchTime 0.321832   LR 0.000119
0.59566551
tensor(0.2062, device='cuda:0', grad_fn=<AddBackward0>)
0.59568882
tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)
0.59560722
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.59568650
tensor(0.2177, device='cuda:0', grad_fn=<AddBackward0>)
0.59569293
tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)
0.59556311
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
0.59547371
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
0.59542447
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
0.59542710
tensor(0.2062, device='cuda:0', grad_fn=<AddBackward0>)
0.59544748
tensor(0.2270, device='cuda:0', grad_fn=<AddBackward0>)
0.59546423
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
0.59550875
tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)
0.59560257
tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)
0.59569991
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
0.59577179
tensor(0.2124, device='cuda:0', grad_fn=<AddBackward0>)
0.59576273
tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)
0.59562069
tensor(0.2470, device='cuda:0', grad_fn=<AddBackward0>)
0.59571642
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
0.59550822
tensor(0.2297, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][  120/  196]   Loss 0.191193   Top1 93.564453   Top5 99.895833   BatchTime 0.321533   LR 0.000119
0.59564263
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.59565198
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
0.59561193
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
0.59562218
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
0.59569663
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
0.59566534
tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)
0.59569818
tensor(0.1693, device='cuda:0', grad_fn=<AddBackward0>)
0.59552395
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
0.59555280
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
0.59565264
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
0.59574491
tensor(0.2393, device='cuda:0', grad_fn=<AddBackward0>)
0.59566128
tensor(0.2718, device='cuda:0', grad_fn=<AddBackward0>)
0.59559005
tensor(0.2251, device='cuda:0', grad_fn=<AddBackward0>)
0.59554839
tensor(0.2430, device='cuda:0', grad_fn=<AddBackward0>)
0.59553349
tensor(0.2250, device='cuda:0', grad_fn=<AddBackward0>)
0.59548503
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
0.59543812
tensor(0.2110, device='cuda:0', grad_fn=<AddBackward0>)
0.59540683
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
0.59540558
tensor(0.2516, device='cuda:0', grad_fn=<AddBackward0>)
0.59537673
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][  140/  196]   Loss 0.191162   Top1 93.596540   Top5 99.905134   BatchTime 0.318350   LR 0.000119
0.59538418
tensor(0.2104, device='cuda:0', grad_fn=<AddBackward0>)
0.59532022
tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)
0.59534031
tensor(0.2274, device='cuda:0', grad_fn=<AddBackward0>)
0.59528220
tensor(0.2351, device='cuda:0', grad_fn=<AddBackward0>)
0.59529722
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
0.59518206
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
0.59517944
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
0.59498352
tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)
0.59486425
tensor(0.2176, device='cuda:0', grad_fn=<AddBackward0>)
0.59491032
tensor(0.2526, device='cuda:0', grad_fn=<AddBackward0>)
0.59506822
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
0.59496355
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.59488350
tensor(0.2014, device='cuda:0', grad_fn=<AddBackward0>)
0.59468085
tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)
0.59458798
tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)
0.59464908
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.59464753
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.59463555
tensor(0.2165, device='cuda:0', grad_fn=<AddBackward0>)
0.59475285
tensor(0.2692, device='cuda:0', grad_fn=<AddBackward0>)
0.59480566
tensor(0.2300, device='cuda:0', grad_fn=<AddBackward0>)
0.59491068
tensor(0.2241, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][  160/  196]   Loss 0.192421   Top1 93.554688   Top5 99.904785   BatchTime 0.314634   LR 0.000119
0.59461439
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
0.59444958
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
0.59438378
tensor(0.2565, device='cuda:0', grad_fn=<AddBackward0>)
0.59434086
tensor(0.2028, device='cuda:0', grad_fn=<AddBackward0>)
0.59426558
tensor(0.1964, device='cuda:0', grad_fn=<AddBackward0>)
0.59428000
tensor(0.2108, device='cuda:0', grad_fn=<AddBackward0>)
0.59431839
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
0.59439862
tensor(0.2063, device='cuda:0', grad_fn=<AddBackward0>)
0.59448379
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
0.59456104
tensor(0.2220, device='cuda:0', grad_fn=<AddBackward0>)
0.59447908
tensor(0.2386, device='cuda:0', grad_fn=<AddBackward0>)
0.59427285
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
0.59428364
tensor(0.1917, device='cuda:0', grad_fn=<AddBackward0>)
0.59447092
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
0.59440768
tensor(0.2110, device='cuda:0', grad_fn=<AddBackward0>)
0.59428698
tensor(0.2600, device='cuda:0', grad_fn=<AddBackward0>)
0.59438282
tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)
0.59437561
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
0.59437257
tensor(0.2519, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [35][  180/  196]   Loss 0.193589   Top1 93.511285   Top5 99.900174   BatchTime 0.315772   LR 0.000118
0.59447521
tensor(0.2041, device='cuda:0', grad_fn=<AddBackward0>)
0.59420437
tensor(0.2981, device='cuda:0', grad_fn=<AddBackward0>)
0.59423816
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
0.59419793
tensor(0.2511, device='cuda:0', grad_fn=<AddBackward0>)
0.59409338
tensor(0.2492, device='cuda:0', grad_fn=<AddBackward0>)
0.59399378
tensor(0.2140, device='cuda:0', grad_fn=<AddBackward0>)
0.59404200
tensor(0.2058, device='cuda:0', grad_fn=<AddBackward0>)
0.59392089
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
0.59392583
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
0.59392744
tensor(0.2463, device='cuda:0', grad_fn=<AddBackward0>)
0.59407765
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
0.59407961
tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)
0.59400189
tensor(0.1612, device='cuda:0', grad_fn=<AddBackward0>)
0.59403402
tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)
0.59385210
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.59387475
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
0.59379750
tensor(0.3618, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 93.482    Top5: 99.900    Loss: 0.194
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.425656   Top1 87.128906   Top5 99.296875   BatchTime 0.136454
INFO - Validation [35][   40/   40]   Loss 0.426009   Top1 86.960000   Top5 99.510000   BatchTime 0.096155
INFO - ==> Top1: 86.960    Top5: 99.510    Loss: 0.426
INFO - ==> Sparsity : 0.590
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.2910)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0924)
features.2.conv.0 tensor(0.1166)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.5686)
features.3.conv.0 tensor(0.0796)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1176)
features.4.conv.0 tensor(0.0584)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.2383)
features.5.conv.0 tensor(0.3494)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.4738)
features.6.conv.0 tensor(0.0482)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0862)
features.7.conv.0 tensor(0.1740)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.2548)
features.8.conv.0 tensor(0.5786)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.5577)
features.9.conv.0 tensor(0.5298)
features.9.conv.3 tensor(0.5587)
features.9.conv.6 tensor(0.5992)
features.10.conv.0 tensor(0.0613)
features.10.conv.3 tensor(0.1027)
features.10.conv.6 tensor(0.0963)
features.11.conv.0 tensor(0.7472)
features.11.conv.3 tensor(0.6472)
features.11.conv.6 tensor(0.7845)
features.12.conv.0 tensor(0.7368)
features.12.conv.3 tensor(0.6740)
features.12.conv.6 tensor(0.7878)
features.13.conv.0 tensor(0.2169)
features.13.conv.3 tensor(0.4886)
features.13.conv.6 tensor(0.1004)
features.14.conv.0 tensor(0.9091)
features.14.conv.3 tensor(0.8262)
features.14.conv.6 tensor(0.9575)
features.15.conv.0 tensor(0.8826)
features.15.conv.3 tensor(0.8363)
features.15.conv.6 tensor(0.9636)
features.16.conv.0 tensor(0.6564)
features.16.conv.3 tensor(0.8039)
features.16.conv.6 tensor(0.8866)
conv.0 tensor(0.0904)
tensor(1292329.) 2188896.0
0.59371245
tensor(0.2220, device='cuda:0', grad_fn=<AddBackward0>)
0.59353358
tensor(0.2326, device='cuda:0', grad_fn=<AddBackward0>)
0.59346539
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
0.59350860
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
0.59361178
tensor(0.2062, device='cuda:0', grad_fn=<AddBackward0>)
0.59348613
tensor(0.2489, device='cuda:0', grad_fn=<AddBackward0>)
0.59347969
tensor(0.2716, device='cuda:0', grad_fn=<AddBackward0>)
0.59343976
tensor(0.2602, device='cuda:0', grad_fn=<AddBackward0>)
0.59335393
tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)
0.59326822
tensor(0.2027, device='cuda:0', grad_fn=<AddBackward0>)
0.59321594
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
0.59313560
tensor(0.2191, device='cuda:0', grad_fn=<AddBackward0>)
0.59304988
tensor(0.2734, device='cuda:0', grad_fn=<AddBackward0>)
0.59294754
tensor(0.2760, device='cuda:0', grad_fn=<AddBackward0>)
0.59281182
tensor(0.2525, device='cuda:0', grad_fn=<AddBackward0>)
0.59257096
tensor(0.1954, device='cuda:0', grad_fn=<AddBackward0>)
0.59238172
tensor(0.2466, device='cuda:0', grad_fn=<AddBackward0>)
0.59223008
tensor(0.2398, device='cuda:0', grad_fn=<AddBackward0>)
0.59198815
tensor(0.2371, device='cuda:0', grad_fn=<AddBackward0>)
0.59172469
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.59140116
tensor(0.1975, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][   20/  196]   Loss 0.228916   Top1 92.089844   Top5 99.765625   BatchTime 0.343229   LR 0.000118
0.59113950
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
0.59091681
tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)
0.59086025
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
0.59064049
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.59047520
tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)
0.59020656
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
0.59017003
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.59008557
tensor(0.1913, device='cuda:0', grad_fn=<AddBackward0>)
0.58986181
tensor(0.2110, device='cuda:0', grad_fn=<AddBackward0>)
0.58991134
tensor(0.1851, device='cuda:0', grad_fn=<AddBackward0>)
0.58985567
tensor(0.3014, device='cuda:0', grad_fn=<AddBackward0>)
0.58979815
tensor(0.2704, device='cuda:0', grad_fn=<AddBackward0>)
0.58978707
tensor(0.1852, device='cuda:0', grad_fn=<AddBackward0>)
0.58965427
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.58963168
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
0.58958662
tensor(0.2306, device='cuda:0', grad_fn=<AddBackward0>)
0.58969957
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
0.58963060
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][   40/  196]   Loss 0.215392   Top1 92.724609   Top5 99.843750   BatchTime 0.338368   LR 0.000118
0.58961272
tensor(0.2250, device='cuda:0', grad_fn=<AddBackward0>)
0.58964080
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
0.58957481
tensor(0.2598, device='cuda:0', grad_fn=<AddBackward0>)
0.58947992
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.58947277
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
0.58987981
tensor(0.2676, device='cuda:0', grad_fn=<AddBackward0>)
0.58958596
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
0.58966047
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.58976823
tensor(0.2132, device='cuda:0', grad_fn=<AddBackward0>)
0.58971745
tensor(0.2412, device='cuda:0', grad_fn=<AddBackward0>)
0.58960968
tensor(0.2088, device='cuda:0', grad_fn=<AddBackward0>)
0.58927840
tensor(0.2379, device='cuda:0', grad_fn=<AddBackward0>)
0.58933055
tensor(0.1579, device='cuda:0', grad_fn=<AddBackward0>)
0.58933920
tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)
0.58929813
tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)
0.58927357
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.58925045
tensor(0.2350, device='cuda:0', grad_fn=<AddBackward0>)
0.58919489
tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][   60/  196]   Loss 0.209597   Top1 92.936198   Top5 99.869792   BatchTime 0.332888   LR 0.000117
0.58928651
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.58930588
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
0.58917004
tensor(0.3040, device='cuda:0', grad_fn=<AddBackward0>)
0.58904904
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
0.58898240
tensor(0.2445, device='cuda:0', grad_fn=<AddBackward0>)
0.58889687
tensor(0.2746, device='cuda:0', grad_fn=<AddBackward0>)
0.58879656
tensor(0.2176, device='cuda:0', grad_fn=<AddBackward0>)
0.58873159
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
0.58864248
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
0.58869791
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
0.58870530
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
0.58860791
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.58803374
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
0.58761495
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
0.58759034
tensor(0.2246, device='cuda:0', grad_fn=<AddBackward0>)
0.58746159
tensor(0.1941, device='cuda:0', grad_fn=<AddBackward0>)
0.58745599
tensor(0.2153, device='cuda:0', grad_fn=<AddBackward0>)
0.58729559
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
0.58705032
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
0.58693177
tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)
0.58680546
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
0.58678895
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
0.58676809
tensor(0.3277, device='cuda:0', grad_fn=<AddBackward0>)
0.58663112
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
0.58651811
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][   80/  196]   Loss 0.206867   Top1 93.061523   Top5 99.877930   BatchTime 0.331399   LR 0.000117
0.58656031
tensor(0.2107, device='cuda:0', grad_fn=<AddBackward0>)
0.58654654
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
0.58647817
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
0.58645493
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.58630288
tensor(0.2312, device='cuda:0', grad_fn=<AddBackward0>)
0.58624405
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
0.58622015
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
0.58617979
tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)
0.58608842
tensor(0.2224, device='cuda:0', grad_fn=<AddBackward0>)
0.58603859
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
0.58593267
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.58590454
tensor(0.2615, device='cuda:0', grad_fn=<AddBackward0>)
0.58592200
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
0.58578265
tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)
0.58581197
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.58578169
tensor(0.2208, device='cuda:0', grad_fn=<AddBackward0>)
0.58587629
tensor(0.2233, device='cuda:0', grad_fn=<AddBackward0>)
0.58585209
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][  100/  196]   Loss 0.203208   Top1 93.183594   Top5 99.878906   BatchTime 0.331457   LR 0.000117
0.58566833
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
0.58554679
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.58556265
tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>)
0.58553922
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
0.58558720
tensor(0.2417, device='cuda:0', grad_fn=<AddBackward0>)
0.58558309
tensor(0.1989, device='cuda:0', grad_fn=<AddBackward0>)
0.58553970
tensor(0.2896, device='cuda:0', grad_fn=<AddBackward0>)
0.58538872
tensor(0.2022, device='cuda:0', grad_fn=<AddBackward0>)
0.58544886
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
0.58528972
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
0.58508390
tensor(0.2157, device='cuda:0', grad_fn=<AddBackward0>)
0.58492714
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
0.58484203
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
0.58476502
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
0.58471763
tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)
0.58468425
tensor(0.2509, device='cuda:0', grad_fn=<AddBackward0>)
0.58471328
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
0.58475399
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][  120/  196]   Loss 0.200672   Top1 93.268229   Top5 99.879557   BatchTime 0.332001   LR 0.000117
0.58486229
tensor(0.2204, device='cuda:0', grad_fn=<AddBackward0>)
0.58486873
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
0.58491850
tensor(0.1982, device='cuda:0', grad_fn=<AddBackward0>)
0.58493167
tensor(0.1859, device='cuda:0', grad_fn=<AddBackward0>)
0.58496982
tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)
0.58497006
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
0.58502203
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
0.58499199
tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)
0.58498156
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
0.58504242
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
0.58494329
tensor(0.1879, device='cuda:0', grad_fn=<AddBackward0>)
0.58483148
tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)
0.58481568
tensor(0.2083, device='cuda:0', grad_fn=<AddBackward0>)
0.58474261
tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)
0.58460844
tensor(0.1808, device='cuda:0', grad_fn=<AddBackward0>)
0.58454877
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.58449680
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.58450663
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.58443117
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][  140/  196]   Loss 0.198845   Top1 93.339844   Top5 99.896763   BatchTime 0.329919   LR 0.000117
0.58451802
tensor(0.2119, device='cuda:0', grad_fn=<AddBackward0>)
0.58456916
tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)
0.58451778
tensor(0.2431, device='cuda:0', grad_fn=<AddBackward0>)
0.58447063
tensor(0.2002, device='cuda:0', grad_fn=<AddBackward0>)
0.58435541
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
0.58436018
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
0.58441317
tensor(0.1960, device='cuda:0', grad_fn=<AddBackward0>)
0.58449435
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
0.58439624
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
0.58440328
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
0.58456445
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
0.58474678
tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)
0.58489025
tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)
0.58492798
tensor(0.2440, device='cuda:0', grad_fn=<AddBackward0>)
0.58474934
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
0.58481294
tensor(0.2234, device='cuda:0', grad_fn=<AddBackward0>)
0.58475709
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.58460641
tensor(0.2553, device='cuda:0', grad_fn=<AddBackward0>)
0.58418524
tensor(0.2487, device='cuda:0', grad_fn=<AddBackward0>)
0.58387917
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.58382034
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
0.58380258
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
0.58364505
tensor(0.2083, device='cuda:0', grad_fn=<AddBackward0>)
0.58352906
tensor(0.2307, device='cuda:0', grad_fn=<AddBackward0>)
0.58339638
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
0.58328754
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][  160/  196]   Loss 0.198025   Top1 93.395996   Top5 99.899902   BatchTime 0.327375   LR 0.000116
0.58328205
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
0.58325273
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
0.58330971
tensor(0.2162, device='cuda:0', grad_fn=<AddBackward0>)
0.58333433
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
0.58334136
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
0.58329409
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
0.58331639
tensor(0.1983, device='cuda:0', grad_fn=<AddBackward0>)
0.58327830
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.58322978
tensor(0.2455, device='cuda:0', grad_fn=<AddBackward0>)
0.58314663
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
0.58304787
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
0.58301210
tensor(0.1923, device='cuda:0', grad_fn=<AddBackward0>)
0.58308047
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
0.58312905
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
0.58315402
tensor(0.2494, device='cuda:0', grad_fn=<AddBackward0>)
0.58322293
tensor(0.2122, device='cuda:0', grad_fn=<AddBackward0>)
0.58321965
tensor(0.2347, device='cuda:0', grad_fn=<AddBackward0>)
0.58313888
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [36][  180/  196]   Loss 0.196347   Top1 93.457031   Top5 99.900174   BatchTime 0.327765   LR 0.000116
0.58309591
tensor(0.2097, device='cuda:0', grad_fn=<AddBackward0>)
0.58299732
tensor(0.2284, device='cuda:0', grad_fn=<AddBackward0>)
0.58295923
tensor(0.2291, device='cuda:0', grad_fn=<AddBackward0>)
0.58285177
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
0.58287406
tensor(0.2687, device='cuda:0', grad_fn=<AddBackward0>)
0.58293062
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.58302766
tensor(0.2282, device='cuda:0', grad_fn=<AddBackward0>)
0.58298939
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.58314091
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
0.58308172
tensor(0.2144, device='cuda:0', grad_fn=<AddBackward0>)
0.58301443
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
0.58305979
tensor(0.2198, device='cuda:0', grad_fn=<AddBackward0>)
0.58302081
tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)
0.58293766
tensor(0.2417, device='cuda:0', grad_fn=<AddBackward0>)
0.58295757
tensor(0.3056, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 93.408    Top5: 99.902    Loss: 0.198
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [36][   20/   40]   Loss 0.429981   Top1 87.304688   Top5 99.414062   BatchTime 0.119631
INFO - Validation [36][   40/   40]   Loss 0.415008   Top1 87.370000   Top5 99.540000   BatchTime 0.086126
INFO - ==> Top1: 87.370    Top5: 99.540    Loss: 0.415
INFO - ==> Sparsity : 0.595
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.3047)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0946)
features.2.conv.0 tensor(0.1389)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.5712)
features.3.conv.0 tensor(0.0856)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1172)
features.4.conv.0 tensor(0.0573)
features.4.conv.3 tensor(0.3050)
features.4.conv.6 tensor(0.2399)
features.5.conv.0 tensor(0.3527)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.4767)
features.6.conv.0 tensor(0.0480)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.1709)
features.7.conv.3 tensor(0.4633)
features.7.conv.6 tensor(0.3825)
features.8.conv.0 tensor(0.5829)
features.8.conv.3 tensor(0.5434)
features.8.conv.6 tensor(0.5468)
features.9.conv.0 tensor(0.4866)
features.9.conv.3 tensor(0.5573)
features.9.conv.6 tensor(0.5940)
features.10.conv.0 tensor(0.0595)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.0961)
features.11.conv.0 tensor(0.7441)
features.11.conv.3 tensor(0.6433)
features.11.conv.6 tensor(0.7899)
features.12.conv.0 tensor(0.7399)
features.12.conv.3 tensor(0.6723)
features.12.conv.6 tensor(0.7954)
features.13.conv.0 tensor(0.2314)
features.13.conv.3 tensor(0.4871)
features.13.conv.6 tensor(0.0991)
features.14.conv.0 tensor(0.9083)
features.14.conv.3 tensor(0.8263)
features.14.conv.6 tensor(0.9576)
features.15.conv.0 tensor(0.8845)
features.15.conv.3 tensor(0.8363)
features.15.conv.6 tensor(0.9639)
features.16.conv.0 tensor(0.6575)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.8875)
conv.0 tensor(0.1043)
tensor(1301889.) 2188896.0
0.58305556
tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)
0.58316350
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.58318192
tensor(0.2312, device='cuda:0', grad_fn=<AddBackward0>)
0.58320248
tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)
0.58320075
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
0.58324569
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.58310890
tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>)
0.58304328
tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)
0.58300060
tensor(0.2445, device='cuda:0', grad_fn=<AddBackward0>)
0.58299202
tensor(0.2365, device='cuda:0', grad_fn=<AddBackward0>)
0.58295113
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
0.58300972
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
0.58306068
tensor(0.2605, device='cuda:0', grad_fn=<AddBackward0>)
0.58309388
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
0.58310288
tensor(0.2186, device='cuda:0', grad_fn=<AddBackward0>)
0.58318084
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.58309108
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
0.58304954
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.58295649
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][   20/  196]   Loss 0.188318   Top1 93.417969   Top5 99.960938   BatchTime 0.302198   LR 0.000116
0.58291459
tensor(0.2182, device='cuda:0', grad_fn=<AddBackward0>)
0.58296341
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
0.58285660
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
0.58285290
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
0.58282763
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.58286923
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
0.58283108
tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)
0.58285886
tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)
0.58274925
tensor(0.2663, device='cuda:0', grad_fn=<AddBackward0>)
0.58272809
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.58290678
tensor(0.1586, device='cuda:0', grad_fn=<AddBackward0>)
0.58289361
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
0.58272243
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
0.58283186
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.58291167
tensor(0.2223, device='cuda:0', grad_fn=<AddBackward0>)
0.58287638
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
0.58282435
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
0.58275437
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
0.58265597
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
0.58267742
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.58268368
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
0.58263588
tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)
0.58267641
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][   40/  196]   Loss 0.185405   Top1 93.720703   Top5 99.912109   BatchTime 0.282960   LR 0.000115
0.58280212
tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)
0.58269691
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
0.58257431
tensor(0.2320, device='cuda:0', grad_fn=<AddBackward0>)
0.58244067
tensor(0.2873, device='cuda:0', grad_fn=<AddBackward0>)
0.58230871
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
0.58228111
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.58218992
tensor(0.2173, device='cuda:0', grad_fn=<AddBackward0>)
0.58198100
tensor(0.2209, device='cuda:0', grad_fn=<AddBackward0>)
0.58175683
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
0.58156383
tensor(0.2211, device='cuda:0', grad_fn=<AddBackward0>)
0.58143020
tensor(0.2252, device='cuda:0', grad_fn=<AddBackward0>)
0.58126110
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.58099359
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
0.58066428
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.58051407
tensor(0.1770, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][   60/  196]   Loss 0.189681   Top1 93.548177   Top5 99.921875   BatchTime 0.275646   LR 0.000115
0.58044171
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
0.58030730
tensor(0.2657, device='cuda:0', grad_fn=<AddBackward0>)
0.58019572
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
0.58010125
tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)
0.58005422
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.58004731
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.58003634
tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)
0.57999671
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
0.57997704
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
0.57998329
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.57987267
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.57980502
tensor(0.2133, device='cuda:0', grad_fn=<AddBackward0>)
0.57970262
tensor(0.2045, device='cuda:0', grad_fn=<AddBackward0>)
0.57955605
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
0.57951474
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
0.57953316
tensor(0.2448, device='cuda:0', grad_fn=<AddBackward0>)
0.57947099
tensor(0.1917, device='cuda:0', grad_fn=<AddBackward0>)
0.57941920
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.57937205
tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)
0.57937294
tensor(0.2098, device='cuda:0', grad_fn=<AddBackward0>)
0.57940632
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.57938641
tensor(0.2014, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][   80/  196]   Loss 0.188313   Top1 93.554688   Top5 99.921875   BatchTime 0.277014   LR 0.000115
0.57926726
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.57911861
tensor(0.1957, device='cuda:0', grad_fn=<AddBackward0>)
0.57905769
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
0.57896602
tensor(0.1959, device='cuda:0', grad_fn=<AddBackward0>)
0.57889515
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
0.57878619
tensor(0.2213, device='cuda:0', grad_fn=<AddBackward0>)
0.57876283
tensor(0.1952, device='cuda:0', grad_fn=<AddBackward0>)
0.57879192
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
0.57877731
tensor(0.2561, device='cuda:0', grad_fn=<AddBackward0>)
0.57870197
tensor(0.2125, device='cuda:0', grad_fn=<AddBackward0>)
0.57857132
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
0.57845700
tensor(0.2094, device='cuda:0', grad_fn=<AddBackward0>)
0.57832301
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.57830656
tensor(0.2350, device='cuda:0', grad_fn=<AddBackward0>)
0.57827961
tensor(0.1762, device='cuda:0', grad_fn=<AddBackward0>)
0.57812256
tensor(0.1968, device='cuda:0', grad_fn=<AddBackward0>)
0.57810783
tensor(0.2175, device='cuda:0', grad_fn=<AddBackward0>)
0.57814807
tensor(0.2163, device='cuda:0', grad_fn=<AddBackward0>)
0.57810289
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][  100/  196]   Loss 0.190667   Top1 93.445312   Top5 99.910156   BatchTime 0.283731   LR 0.000114
0.57805014
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.57797098
tensor(0.2394, device='cuda:0', grad_fn=<AddBackward0>)
0.57783914
tensor(0.2606, device='cuda:0', grad_fn=<AddBackward0>)
0.57785785
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
0.57786089
tensor(0.2485, device='cuda:0', grad_fn=<AddBackward0>)
0.57779688
tensor(0.1979, device='cuda:0', grad_fn=<AddBackward0>)
0.57769787
tensor(0.2175, device='cuda:0', grad_fn=<AddBackward0>)
0.57756197
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
0.57757312
tensor(0.2350, device='cuda:0', grad_fn=<AddBackward0>)
0.57762140
tensor(0.2095, device='cuda:0', grad_fn=<AddBackward0>)
0.57754707
tensor(0.2277, device='cuda:0', grad_fn=<AddBackward0>)
0.57743609
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
0.57737666
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
0.57736433
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
0.57728219
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
0.57719028
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.57715642
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
0.57709062
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
0.57714486
tensor(0.2360, device='cuda:0', grad_fn=<AddBackward0>)
0.57720494
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][  120/  196]   Loss 0.190915   Top1 93.430990   Top5 99.895833   BatchTime 0.285744   LR 0.000114
0.57717216
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
0.57720464
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
0.57720691
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.57722217
tensor(0.2625, device='cuda:0', grad_fn=<AddBackward0>)
0.57723874
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
0.57724679
tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)
0.57727689
tensor(0.1997, device='cuda:0', grad_fn=<AddBackward0>)
0.57728231
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
0.57726318
tensor(0.2439, device='cuda:0', grad_fn=<AddBackward0>)
0.57715052
tensor(0.1562, device='cuda:0', grad_fn=<AddBackward0>)
0.57714927
tensor(0.1827, device='cuda:0', grad_fn=<AddBackward0>)
0.57705259
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
0.57700402
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.57702601
tensor(0.1949, device='cuda:0', grad_fn=<AddBackward0>)
0.57704645
tensor(0.2531, device='cuda:0', grad_fn=<AddBackward0>)
0.57713473
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.57710111
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
0.57701832
tensor(0.2934, device='cuda:0', grad_fn=<AddBackward0>)
0.57708174
tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)
0.57709837
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][  140/  196]   Loss 0.191127   Top1 93.443080   Top5 99.893973   BatchTime 0.288575   LR 0.000114
0.57707238
tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)
0.57700080
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
0.57710814
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.57708752
tensor(0.2503, device='cuda:0', grad_fn=<AddBackward0>)
0.57704890
tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)
0.57703286
tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>)
0.57701635
tensor(0.2375, device='cuda:0', grad_fn=<AddBackward0>)
0.57694751
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
0.57698464
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
0.57705563
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
0.57715148
tensor(0.2189, device='cuda:0', grad_fn=<AddBackward0>)
0.57714772
tensor(0.1964, device='cuda:0', grad_fn=<AddBackward0>)
0.57720107
tensor(0.1954, device='cuda:0', grad_fn=<AddBackward0>)
0.57713276
tensor(0.2095, device='cuda:0', grad_fn=<AddBackward0>)
0.57726240
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
0.57730603
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
0.57738680
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
0.57754350
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
0.57756412
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][  160/  196]   Loss 0.190577   Top1 93.449707   Top5 99.890137   BatchTime 0.291956   LR 0.000114
0.57765383
tensor(0.2720, device='cuda:0', grad_fn=<AddBackward0>)
0.57767612
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
0.57752573
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
0.57754952
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.57766354
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
0.57760966
tensor(0.2185, device='cuda:0', grad_fn=<AddBackward0>)
0.57744992
tensor(0.2014, device='cuda:0', grad_fn=<AddBackward0>)
0.57749420
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
0.57753295
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
0.57752419
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.57750893
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.57752454
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.57750130
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
0.57742691
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.57743120
tensor(0.2086, device='cuda:0', grad_fn=<AddBackward0>)
0.57747424
tensor(0.2606, device='cuda:0', grad_fn=<AddBackward0>)
0.57748008
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
0.57739210
tensor(0.2791, device='cuda:0', grad_fn=<AddBackward0>)
0.57721329
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
0.57721758
tensor(0.2294, device='cuda:0', grad_fn=<AddBackward0>)
0.57715017
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.57710373
tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)
0.57709247
tensor(0.1968, device='cuda:0', grad_fn=<AddBackward0>)
0.57713264
tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [37][  180/  196]   Loss 0.190436   Top1 93.467882   Top5 99.898003   BatchTime 0.287705   LR 0.000113
0.57717240
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
0.57720840
tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)
0.57717401
tensor(0.2210, device='cuda:0', grad_fn=<AddBackward0>)
0.57712376
tensor(0.2130, device='cuda:0', grad_fn=<AddBackward0>)
0.57708704
tensor(0.2360, device='cuda:0', grad_fn=<AddBackward0>)
0.57703143
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
0.57704443
tensor(0.2428, device='cuda:0', grad_fn=<AddBackward0>)
0.57701355
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.57695836
tensor(0.2132, device='cuda:0', grad_fn=<AddBackward0>)
0.57688510
tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)
0.57680255
tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)
0.57675558
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
0.57675052
tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)
0.57673198
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
0.57673109
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 93.480    Top5: 99.904    Loss: 0.190
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.420303   Top1 87.578125   Top5 99.531250   BatchTime 0.121806
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.3066)
features.1.conv.0 tensor(0.0651)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0933)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.5720)
features.3.conv.0 tensor(0.0813)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1220)
features.4.conv.0 tensor(0.0544)
features.4.conv.3 tensor(0.3003)
features.4.conv.6 tensor(0.2410)
features.5.conv.0 tensor(0.3530)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.4845)
features.6.conv.0 tensor(0.0449)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0863)
features.7.conv.0 tensor(0.1705)
features.7.conv.3 tensor(0.4583)
features.7.conv.6 tensor(0.4026)
features.8.conv.0 tensor(0.5885)
features.8.conv.3 tensor(0.5434)
features.8.conv.6 tensor(0.5465)
features.9.conv.0 tensor(0.5084)
features.9.conv.3 tensor(0.5556)
features.9.conv.6 tensor(0.6027)
features.10.conv.0 tensor(0.0581)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0939)
features.11.conv.0 tensor(0.7426)
features.11.conv.3 tensor(0.6422)
features.11.conv.6 tensor(0.7925)
features.12.conv.0 tensor(0.7403)
features.12.conv.3 tensor(0.6707)
features.12.conv.6 tensor(0.8007)
features.13.conv.0 tensor(0.2410)
features.13.conv.3 tensor(0.4884)
features.13.conv.6 tensor(0.1518)
features.14.conv.0 tensor(0.9102)
features.14.conv.3 tensor(0.8270)
features.14.conv.6 tensor(0.9581)
features.15.conv.0 tensor(0.8854)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9647)
features.16.conv.0 tensor(0.6600)
features.16.conv.3 tensor(0.8037)
features.16.conv.6 tensor(0.8880)
conv.0 tensor(0.1157)
tensor(1314762.) 2188896.0
INFO - Validation [37][   40/   40]   Loss 0.409032   Top1 87.560000   Top5 99.610000   BatchTime 0.089683
INFO - ==> Top1: 87.560    Top5: 99.610    Loss: 0.409
INFO - ==> Sparsity : 0.601
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
0.57673067
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
0.57673824
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
0.57674253
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
0.57669890
tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)
0.57664412
tensor(0.2378, device='cuda:0', grad_fn=<AddBackward0>)
0.57662582
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
0.57662290
tensor(0.2225, device='cuda:0', grad_fn=<AddBackward0>)
0.57661837
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
0.57662511
tensor(0.2090, device='cuda:0', grad_fn=<AddBackward0>)
0.57658362
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
0.57659531
tensor(0.2077, device='cuda:0', grad_fn=<AddBackward0>)
0.57660288
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.57655966
tensor(0.1974, device='cuda:0', grad_fn=<AddBackward0>)
0.57650870
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
0.57648450
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
0.57646173
tensor(0.2251, device='cuda:0', grad_fn=<AddBackward0>)
0.57649100
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
0.57649714
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
0.57646155
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][   20/  196]   Loss 0.179997   Top1 93.906250   Top5 99.980469   BatchTime 0.358331   LR 0.000113
0.57647276
tensor(0.2299, device='cuda:0', grad_fn=<AddBackward0>)
0.57648063
tensor(0.2154, device='cuda:0', grad_fn=<AddBackward0>)
0.57665741
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.57679522
tensor(0.2519, device='cuda:0', grad_fn=<AddBackward0>)
0.57748765
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
0.57765979
tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)
0.57764256
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.57762337
tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)
0.57755977
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
0.57745707
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
0.57736534
tensor(0.2127, device='cuda:0', grad_fn=<AddBackward0>)
0.57731366
tensor(0.2071, device='cuda:0', grad_fn=<AddBackward0>)
0.57726246
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.57719207
tensor(0.2515, device='cuda:0', grad_fn=<AddBackward0>)
0.57710391
tensor(0.2006, device='cuda:0', grad_fn=<AddBackward0>)
0.57703197
tensor(0.2635, device='cuda:0', grad_fn=<AddBackward0>)
0.57699013
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
0.57697827
tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)
0.57697105
tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)
0.57693475
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
0.57686526
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
0.57686794
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
0.57687837
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][   40/  196]   Loss 0.184865   Top1 93.857422   Top5 99.882812   BatchTime 0.351046   LR 0.000112
0.57683629
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
0.57677799
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
0.57677513
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
0.57678467
tensor(0.1951, device='cuda:0', grad_fn=<AddBackward0>)
0.57687706
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
0.57698566
tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)
0.57699108
tensor(0.1801, device='cuda:0', grad_fn=<AddBackward0>)
0.57703847
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.57710159
tensor(0.1423, device='cuda:0', grad_fn=<AddBackward0>)
0.57717758
tensor(0.2336, device='cuda:0', grad_fn=<AddBackward0>)
0.57707053
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
0.57707238
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
0.57697523
tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)
0.57693279
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
0.57694042
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
0.57690704
tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)
0.57688481
tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)
0.57682747
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][   60/  196]   Loss 0.182092   Top1 93.932292   Top5 99.902344   BatchTime 0.345706   LR 0.000112
0.57682925
tensor(0.2006, device='cuda:0', grad_fn=<AddBackward0>)
0.57686681
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
0.57700616
tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)
0.57699066
tensor(0.2068, device='cuda:0', grad_fn=<AddBackward0>)
0.57689804
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
0.57695907
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
0.57695943
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
0.57689095
tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)
0.57682002
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
0.57676375
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
0.57676470
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.57673562
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.57670850
tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)
0.57661235
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.57652903
tensor(0.2083, device='cuda:0', grad_fn=<AddBackward0>)
0.57643914
tensor(0.1498, device='cuda:0', grad_fn=<AddBackward0>)
0.57640779
tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)
0.57632339
tensor(0.2242, device='cuda:0', grad_fn=<AddBackward0>)
0.57621956
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][   80/  196]   Loss 0.180664   Top1 93.969727   Top5 99.916992   BatchTime 0.338276   LR 0.000112
0.57621258
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
0.57627636
tensor(0.2459, device='cuda:0', grad_fn=<AddBackward0>)
0.57627195
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
0.57622218
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
0.57628673
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.57616764
tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)
0.57609898
tensor(0.1954, device='cuda:0', grad_fn=<AddBackward0>)
0.57611215
tensor(0.2128, device='cuda:0', grad_fn=<AddBackward0>)
0.57609439
tensor(0.2501, device='cuda:0', grad_fn=<AddBackward0>)
0.57599926
tensor(0.2627, device='cuda:0', grad_fn=<AddBackward0>)
0.57574397
tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)
0.57555431
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
0.57556736
tensor(0.2771, device='cuda:0', grad_fn=<AddBackward0>)
0.57566351
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.57554328
tensor(0.1588, device='cuda:0', grad_fn=<AddBackward0>)
0.57538539
tensor(0.2054, device='cuda:0', grad_fn=<AddBackward0>)
0.57528168
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
0.57529664
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.57527310
tensor(0.2417, device='cuda:0', grad_fn=<AddBackward0>)
0.57522207
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][  100/  196]   Loss 0.183936   Top1 93.859375   Top5 99.906250   BatchTime 0.332120   LR 0.000112
0.57526290
tensor(0.1910, device='cuda:0', grad_fn=<AddBackward0>)
0.57528985
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
0.57520121
tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)
0.57512909
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
0.57504475
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
0.57503343
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
0.57504296
tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)
0.57499635
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
0.57495648
tensor(0.2354, device='cuda:0', grad_fn=<AddBackward0>)
0.57490116
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.57490784
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.57495576
tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)
0.57489705
tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)
0.57492083
tensor(0.2625, device='cuda:0', grad_fn=<AddBackward0>)
0.57489842
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.57479489
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.57468176
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.57462835
tensor(0.1832, device='cuda:0', grad_fn=<AddBackward0>)
0.57463735
tensor(0.1923, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][  120/  196]   Loss 0.184997   Top1 93.873698   Top5 99.895833   BatchTime 0.330046   LR 0.000111
0.57459396
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
0.57459503
tensor(0.2022, device='cuda:0', grad_fn=<AddBackward0>)
0.57457620
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
0.57467514
tensor(0.2644, device='cuda:0', grad_fn=<AddBackward0>)
0.57451159
tensor(0.1801, device='cuda:0', grad_fn=<AddBackward0>)
0.57444012
tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)
0.57434297
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
0.57430756
tensor(0.1879, device='cuda:0', grad_fn=<AddBackward0>)
0.57428294
tensor(0.2036, device='cuda:0', grad_fn=<AddBackward0>)
0.57422215
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
0.57423693
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
0.57414216
tensor(0.2203, device='cuda:0', grad_fn=<AddBackward0>)
0.57404733
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
0.57399017
tensor(0.2527, device='cuda:0', grad_fn=<AddBackward0>)
0.57397050
tensor(0.1930, device='cuda:0', grad_fn=<AddBackward0>)
0.57388908
tensor(0.2320, device='cuda:0', grad_fn=<AddBackward0>)
0.57379043
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
0.57371897
tensor(0.1964, device='cuda:0', grad_fn=<AddBackward0>)
0.57370883
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
0.57363236
tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>)
0.57354218
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
0.57348990
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][  140/  196]   Loss 0.185538   Top1 93.833705   Top5 99.905134   BatchTime 0.335259   LR 0.000111
0.57349271
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
0.57342839
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.57333487
tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>)
0.57335395
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
0.57338226
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.57325476
tensor(0.2372, device='cuda:0', grad_fn=<AddBackward0>)
0.57321221
tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)
0.57318223
tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)
0.57303947
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
0.57296336
tensor(0.2594, device='cuda:0', grad_fn=<AddBackward0>)
0.57295328
tensor(0.1835, device='cuda:0', grad_fn=<AddBackward0>)
0.57294017
tensor(0.2173, device='cuda:0', grad_fn=<AddBackward0>)
0.57290143
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.57282692
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
0.57273024
tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)
0.57271129
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
0.57267433
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.57258517
tensor(0.2034, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][  160/  196]   Loss 0.185661   Top1 93.793945   Top5 99.904785   BatchTime 0.335203   LR 0.000111
0.57250172
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
0.57245654
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
0.57242751
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.57241833
tensor(0.2215, device='cuda:0', grad_fn=<AddBackward0>)
0.57240731
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
0.57237369
tensor(0.2408, device='cuda:0', grad_fn=<AddBackward0>)
0.57238716
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
0.57244325
tensor(0.2377, device='cuda:0', grad_fn=<AddBackward0>)
0.57244349
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
0.57241178
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
0.57240498
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.57233769
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.57227975
tensor(0.1989, device='cuda:0', grad_fn=<AddBackward0>)
0.57233161
tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)
0.57233799
tensor(0.2292, device='cuda:0', grad_fn=<AddBackward0>)
0.57236278
tensor(0.2222, device='cuda:0', grad_fn=<AddBackward0>)
0.57234681
tensor(0.2123, device='cuda:0', grad_fn=<AddBackward0>)
0.57228392
tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)
0.57217318
tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)
0.57214195
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
0.57213485
tensor(0.1832, device='cuda:0', grad_fn=<AddBackward0>)
0.57207477
tensor(0.2322, device='cuda:0', grad_fn=<AddBackward0>)
0.57202005
tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)
0.57196957
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [38][  180/  196]   Loss 0.187383   Top1 93.728299   Top5 99.902344   BatchTime 0.334837   LR 0.000110
0.57193404
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.57195324
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
0.57203227
tensor(0.2100, device='cuda:0', grad_fn=<AddBackward0>)
0.57205194
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
0.57201612
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
0.57204372
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
0.57192343
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
0.57180554
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
0.57176518
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
0.57178551
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
0.57181621
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.57190305
tensor(0.2548, device='cuda:0', grad_fn=<AddBackward0>)
0.57197636
tensor(0.2404, device='cuda:0', grad_fn=<AddBackward0>)
0.57290608
tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 93.708    Top5: 99.910    Loss: 0.187
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [38][   20/   40]   Loss 0.416638   Top1 87.363281   Top5 99.453125   BatchTime 0.116651
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.3145)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0916)
features.2.conv.0 tensor(0.1322)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.5729)
features.3.conv.0 tensor(0.0856)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1139)
features.4.conv.0 tensor(0.0537)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.2417)
features.5.conv.0 tensor(0.3831)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.4862)
features.6.conv.0 tensor(0.0475)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0849)
features.7.conv.0 tensor(0.1715)
features.7.conv.3 tensor(0.4552)
features.7.conv.6 tensor(0.4553)
features.8.conv.0 tensor(0.6027)
features.8.conv.3 tensor(0.5451)
features.8.conv.6 tensor(0.5578)
features.9.conv.0 tensor(0.5312)
features.9.conv.3 tensor(0.5558)
features.9.conv.6 tensor(0.6285)
features.10.conv.0 tensor(0.0627)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0944)
features.11.conv.0 tensor(0.7454)
features.11.conv.3 tensor(0.6408)
features.11.conv.6 tensor(0.7943)
features.12.conv.0 tensor(0.7280)
features.12.conv.3 tensor(0.6726)
features.12.conv.6 tensor(0.8120)
features.13.conv.0 tensor(0.2407)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.1787)
features.14.conv.0 tensor(0.9124)
features.14.conv.3 tensor(0.8273)
features.14.conv.6 tensor(0.9568)
features.15.conv.0 tensor(0.8879)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9644)
features.16.conv.0 tensor(0.6584)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.8874)
conv.0 tensor(0.1228)
tensor(1323828.) 2188896.0
INFO - Validation [38][   40/   40]   Loss 0.405427   Top1 87.680000   Top5 99.540000   BatchTime 0.081579
INFO - ==> Top1: 87.680    Top5: 99.540    Loss: 0.405
INFO - ==> Sparsity : 0.605
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
0.57382101
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
0.57373857
tensor(0.2146, device='cuda:0', grad_fn=<AddBackward0>)
0.57366395
tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)
0.57360184
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
0.57358724
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
0.57354599
tensor(0.2728, device='cuda:0', grad_fn=<AddBackward0>)
0.57352144
tensor(0.2423, device='cuda:0', grad_fn=<AddBackward0>)
0.57358730
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
0.57362306
tensor(0.2804, device='cuda:0', grad_fn=<AddBackward0>)
0.57360637
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.57361507
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.57364011
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.57362545
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.57364273
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.57366419
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
0.57374382
tensor(0.1760, device='cuda:0', grad_fn=<AddBackward0>)
0.57368332
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
0.57362980
tensor(0.2104, device='cuda:0', grad_fn=<AddBackward0>)
0.57366711
tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>)
0.57371837
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
0.57367110
tensor(0.2036, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][   20/  196]   Loss 0.188447   Top1 93.652344   Top5 99.941406   BatchTime 0.381629   LR 0.000110
0.57366043
tensor(0.2119, device='cuda:0', grad_fn=<AddBackward0>)
0.57371366
tensor(0.1634, device='cuda:0', grad_fn=<AddBackward0>)
0.57374978
tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)
0.57375264
tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)
0.57378364
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
0.57366914
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
0.57360721
tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)
0.57358754
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.57356375
tensor(0.1588, device='cuda:0', grad_fn=<AddBackward0>)
0.57355732
tensor(0.1996, device='cuda:0', grad_fn=<AddBackward0>)
0.57359999
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
0.57356083
tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)
0.57359934
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.57353348
tensor(0.2064, device='cuda:0', grad_fn=<AddBackward0>)
0.57352805
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
0.57351059
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
0.57345480
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
0.57336617
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
0.57337403
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.57342798
tensor(0.1544, device='cuda:0', grad_fn=<AddBackward0>)
0.57343698
tensor(0.2221, device='cuda:0', grad_fn=<AddBackward0>)
0.57342744
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][   40/  196]   Loss 0.179852   Top1 93.916016   Top5 99.941406   BatchTime 0.324485   LR 0.000109
0.57346213
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
0.57334411
tensor(0.2020, device='cuda:0', grad_fn=<AddBackward0>)
0.57337999
tensor(0.1741, device='cuda:0', grad_fn=<AddBackward0>)
0.57330096
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
0.57322145
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
0.57318634
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.57318777
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.57320273
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
0.57327133
tensor(0.1717, device='cuda:0', grad_fn=<AddBackward0>)
0.57325363
tensor(0.2033, device='cuda:0', grad_fn=<AddBackward0>)
0.57333350
tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)
0.57334507
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.57332695
tensor(0.1825, device='cuda:0', grad_fn=<AddBackward0>)
0.57326180
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
0.57322383
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.57318735
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][   60/  196]   Loss 0.174988   Top1 94.134115   Top5 99.947917   BatchTime 0.306950   LR 0.000109
0.57320952
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
0.57325369
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
0.57323635
tensor(0.2087, device='cuda:0', grad_fn=<AddBackward0>)
0.57327348
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
0.57327133
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.57331789
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
0.57337719
tensor(0.1646, device='cuda:0', grad_fn=<AddBackward0>)
0.57355076
tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)
0.57441318
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
0.57540756
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
0.57574451
tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)
0.57598102
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.57608545
tensor(0.1787, device='cuda:0', grad_fn=<AddBackward0>)
0.57602781
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.57591856
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
0.57590318
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.57586020
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
0.57573760
tensor(0.2282, device='cuda:0', grad_fn=<AddBackward0>)
0.57561195
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
0.57554358
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
0.57551157
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
0.57555580
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][   80/  196]   Loss 0.174535   Top1 94.057617   Top5 99.946289   BatchTime 0.296691   LR 0.000109
0.57556242
tensor(0.2328, device='cuda:0', grad_fn=<AddBackward0>)
0.57543778
tensor(0.2332, device='cuda:0', grad_fn=<AddBackward0>)
0.57532632
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
0.57529867
tensor(0.2160, device='cuda:0', grad_fn=<AddBackward0>)
0.57524955
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
0.57514560
tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)
0.57504982
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
0.57499999
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
0.57498354
tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)
0.57498276
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.57497269
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
0.57489538
tensor(0.2545, device='cuda:0', grad_fn=<AddBackward0>)
0.57485658
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.57489675
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
0.57479793
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
0.57473451
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.57472664
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.57465321
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
0.57467103
tensor(0.2184, device='cuda:0', grad_fn=<AddBackward0>)
0.57464516
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
0.57458770
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
0.57447928
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][  100/  196]   Loss 0.176194   Top1 93.980469   Top5 99.941406   BatchTime 0.293621   LR 0.000108
0.57444978
tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)
0.57443213
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
0.57439572
tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)
0.57436866
tensor(0.1857, device='cuda:0', grad_fn=<AddBackward0>)
0.57421374
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
0.57404965
tensor(0.2234, device='cuda:0', grad_fn=<AddBackward0>)
0.57393700
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
0.57377869
tensor(0.2174, device='cuda:0', grad_fn=<AddBackward0>)
0.57366616
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
0.57355922
tensor(0.2266, device='cuda:0', grad_fn=<AddBackward0>)
0.57347834
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
0.57347971
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
0.57352751
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
0.57348114
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
0.57345176
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
0.57339054
tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)
0.57335281
tensor(0.1889, device='cuda:0', grad_fn=<AddBackward0>)
0.57330531
tensor(0.2033, device='cuda:0', grad_fn=<AddBackward0>)
0.57325071
tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)
0.57317984
tensor(0.1910, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][  120/  196]   Loss 0.176492   Top1 93.945312   Top5 99.921875   BatchTime 0.294054   LR 0.000108
0.57316130
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
0.57319450
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.57319891
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
0.57326770
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
0.57330763
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
0.57330459
tensor(0.1814, device='cuda:0', grad_fn=<AddBackward0>)
0.57323229
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
0.57318127
tensor(0.1414, device='cuda:0', grad_fn=<AddBackward0>)
0.57301974
tensor(0.2791, device='cuda:0', grad_fn=<AddBackward0>)
0.57299638
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
0.57296300
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
0.57293385
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.57291806
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.57295382
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
0.57301193
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.57300240
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
0.57298905
tensor(0.2229, device='cuda:0', grad_fn=<AddBackward0>)
0.57294023
tensor(0.2612, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][  140/  196]   Loss 0.176819   Top1 93.914621   Top5 99.921875   BatchTime 0.298659   LR 0.000108
0.57295418
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.57300490
tensor(0.1961, device='cuda:0', grad_fn=<AddBackward0>)
0.57287419
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.57283747
tensor(0.1903, device='cuda:0', grad_fn=<AddBackward0>)
0.57283258
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.57280606
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
0.57274908
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
0.57275075
tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)
0.57280767
tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)
0.57283217
tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)
0.57276642
tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)
0.57272309
tensor(0.2346, device='cuda:0', grad_fn=<AddBackward0>)
0.57266021
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
0.57256526
tensor(0.2122, device='cuda:0', grad_fn=<AddBackward0>)
0.57249981
tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)
0.57247722
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
0.57246310
tensor(0.2172, device='cuda:0', grad_fn=<AddBackward0>)
0.57240766
tensor(0.2298, device='cuda:0', grad_fn=<AddBackward0>)
0.57235491
tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)
0.57231420
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
0.57235420
tensor(0.2973, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][  160/  196]   Loss 0.179312   Top1 93.867188   Top5 99.914551   BatchTime 0.296454   LR 0.000107
0.57234925
tensor(0.2244, device='cuda:0', grad_fn=<AddBackward0>)
0.57227790
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
0.57227242
tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)
0.57227242
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
0.57225150
tensor(0.1926, device='cuda:0', grad_fn=<AddBackward0>)
0.57214314
tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)
0.57212991
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
0.57215798
tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)
0.57212663
tensor(0.1855, device='cuda:0', grad_fn=<AddBackward0>)
0.57208288
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
0.57209730
tensor(0.2877, device='cuda:0', grad_fn=<AddBackward0>)
0.57199681
tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)
0.57203680
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
0.57200104
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
0.57197988
tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)
0.57193702
tensor(0.2351, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [39][  180/  196]   Loss 0.180873   Top1 93.823785   Top5 99.919705   BatchTime 0.292838   LR 0.000107
0.57200480
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
0.57204509
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
0.57202977
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.57202619
tensor(0.2290, device='cuda:0', grad_fn=<AddBackward0>)
0.57204854
tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)
0.57202893
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
0.57204843
tensor(0.2226, device='cuda:0', grad_fn=<AddBackward0>)
0.57211018
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
0.57219112
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
0.57225305
tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)
0.57230639
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
0.57231599
tensor(0.2496, device='cuda:0', grad_fn=<AddBackward0>)
0.57236779
tensor(0.1691, device='cuda:0', grad_fn=<AddBackward0>)
0.57235342
tensor(0.2074, device='cuda:0', grad_fn=<AddBackward0>)
0.57241428
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.57259721
tensor(0.2250, device='cuda:0', grad_fn=<AddBackward0>)
0.57259405
tensor(0.2107, device='cuda:0', grad_fn=<AddBackward0>)
0.57252347
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 93.798    Top5: 99.912    Loss: 0.182
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [39][   20/   40]   Loss 0.402868   Top1 87.949219   Top5 99.550781   BatchTime 0.115848
INFO - Validation [39][   40/   40]   Loss 0.392631   Top1 87.920000   Top5 99.640000   BatchTime 0.085962
INFO - ==> Top1: 87.920    Top5: 99.640    Loss: 0.393
INFO - ==> Sparsity : 0.607
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.3145)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0981)
features.2.conv.0 tensor(0.1227)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.5767)
features.3.conv.0 tensor(0.0848)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.0601)
features.4.conv.3 tensor(0.3056)
features.4.conv.6 tensor(0.2443)
features.5.conv.0 tensor(0.3773)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.4884)
features.6.conv.0 tensor(0.0487)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0854)
features.7.conv.0 tensor(0.1687)
features.7.conv.3 tensor(0.4557)
features.7.conv.6 tensor(0.4567)
features.8.conv.0 tensor(0.5965)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.5697)
features.9.conv.0 tensor(0.5089)
features.9.conv.3 tensor(0.5587)
features.9.conv.6 tensor(0.6357)
features.10.conv.0 tensor(0.0638)
features.10.conv.3 tensor(0.1071)
features.10.conv.6 tensor(0.0950)
features.11.conv.0 tensor(0.7433)
features.11.conv.3 tensor(0.6412)
features.11.conv.6 tensor(0.7940)
features.12.conv.0 tensor(0.7379)
features.12.conv.3 tensor(0.6709)
features.12.conv.6 tensor(0.8178)
features.13.conv.0 tensor(0.2409)
features.13.conv.3 tensor(0.4882)
features.13.conv.6 tensor(0.1750)
features.14.conv.0 tensor(0.9130)
features.14.conv.3 tensor(0.8271)
features.14.conv.6 tensor(0.9585)
features.15.conv.0 tensor(0.8873)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.6611)
features.16.conv.3 tensor(0.8039)
features.16.conv.6 tensor(0.8885)
conv.0 tensor(0.1301)
tensor(1328248.) 2188896.0
0.57244831
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
0.57239109
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.57241976
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
0.57229573
tensor(0.2446, device='cuda:0', grad_fn=<AddBackward0>)
0.57226193
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
0.57234925
tensor(0.2471, device='cuda:0', grad_fn=<AddBackward0>)
0.57245421
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.57245189
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.57230860
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
0.57225406
tensor(0.2581, device='cuda:0', grad_fn=<AddBackward0>)
0.57216400
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.57209688
tensor(0.1923, device='cuda:0', grad_fn=<AddBackward0>)
0.57201451
tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)
0.57189894
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
0.57186574
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
0.57184190
tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)
0.57187063
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
0.57192856
tensor(0.2224, device='cuda:0', grad_fn=<AddBackward0>)
0.57202339
tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)
0.57205385
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.57214504
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][   20/  196]   Loss 0.178451   Top1 94.121094   Top5 99.902344   BatchTime 0.357645   LR 0.000106
0.57217640
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
0.57209331
tensor(0.1903, device='cuda:0', grad_fn=<AddBackward0>)
0.57209301
tensor(0.1979, device='cuda:0', grad_fn=<AddBackward0>)
0.57207835
tensor(0.1832, device='cuda:0', grad_fn=<AddBackward0>)
0.57206404
tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)
0.57204884
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.57204735
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.57200670
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
0.57205129
tensor(0.2577, device='cuda:0', grad_fn=<AddBackward0>)
0.57199186
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
0.57177961
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
0.57170540
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
0.57171404
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
0.57171506
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
0.57176298
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
0.57166535
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.57161164
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
0.57168782
tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)
0.57179147
tensor(0.1822, device='cuda:0', grad_fn=<AddBackward0>)
0.57187253
tensor(0.2567, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][   40/  196]   Loss 0.174120   Top1 94.238281   Top5 99.892578   BatchTime 0.326968   LR 0.000106
0.57185513
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.57200265
tensor(0.1974, device='cuda:0', grad_fn=<AddBackward0>)
0.57190210
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.57191044
tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)
0.57206780
tensor(0.2356, device='cuda:0', grad_fn=<AddBackward0>)
0.57426274
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.57719386
tensor(0.2020, device='cuda:0', grad_fn=<AddBackward0>)
0.57720053
tensor(0.2609, device='cuda:0', grad_fn=<AddBackward0>)
0.57720077
tensor(0.2349, device='cuda:0', grad_fn=<AddBackward0>)
0.57734364
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
0.57729161
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
0.57730550
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
0.57727867
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
0.57723618
tensor(0.2131, device='cuda:0', grad_fn=<AddBackward0>)
0.57717854
tensor(0.2253, device='cuda:0', grad_fn=<AddBackward0>)
0.57714659
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
0.57711828
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.57710803
tensor(0.2395, device='cuda:0', grad_fn=<AddBackward0>)
0.57712877
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
0.57718199
tensor(0.1820, device='cuda:0', grad_fn=<AddBackward0>)
0.57724470
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][   60/  196]   Loss 0.179055   Top1 94.016927   Top5 99.908854   BatchTime 0.315808   LR 0.000106
0.57740688
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
0.57740802
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.57745826
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
0.57738674
tensor(0.2296, device='cuda:0', grad_fn=<AddBackward0>)
0.57734108
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
0.57740062
tensor(0.1983, device='cuda:0', grad_fn=<AddBackward0>)
0.57736987
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.57744634
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.57767850
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
0.57787311
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.57805979
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
0.57834393
tensor(0.2193, device='cuda:0', grad_fn=<AddBackward0>)
0.57847750
tensor(0.2078, device='cuda:0', grad_fn=<AddBackward0>)
0.57860470
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
0.57888556
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
0.57902247
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
0.57913917
tensor(0.1664, device='cuda:0', grad_fn=<AddBackward0>)
0.57920611
tensor(0.2243, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][   80/  196]   Loss 0.176022   Top1 94.052734   Top5 99.916992   BatchTime 0.317289   LR 0.000105
0.57922232
tensor(0.2154, device='cuda:0', grad_fn=<AddBackward0>)
0.57907850
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
0.57889241
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.57868862
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
0.57853019
tensor(0.2167, device='cuda:0', grad_fn=<AddBackward0>)
0.57851577
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
0.57852328
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
0.57849103
tensor(0.1820, device='cuda:0', grad_fn=<AddBackward0>)
0.57857579
tensor(0.2238, device='cuda:0', grad_fn=<AddBackward0>)
0.57863301
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.57874346
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
0.57850617
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
0.57841927
tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)
0.57836014
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.57833827
tensor(0.2141, device='cuda:0', grad_fn=<AddBackward0>)
0.57835144
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
0.57835758
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
0.57826257
tensor(0.1472, device='cuda:0', grad_fn=<AddBackward0>)
0.57833314
tensor(0.2229, device='cuda:0', grad_fn=<AddBackward0>)
0.57834488
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][  100/  196]   Loss 0.176127   Top1 94.027344   Top5 99.914062   BatchTime 0.314444   LR 0.000105
0.57820970
tensor(0.1804, device='cuda:0', grad_fn=<AddBackward0>)
0.57809293
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.57799011
tensor(0.2203, device='cuda:0', grad_fn=<AddBackward0>)
0.57791412
tensor(0.2000, device='cuda:0', grad_fn=<AddBackward0>)
0.57783121
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
0.57772940
tensor(0.2072, device='cuda:0', grad_fn=<AddBackward0>)
0.57768005
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
0.57768357
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
0.57768339
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.57786870
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.57806957
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
0.57820129
tensor(0.1748, device='cuda:0', grad_fn=<AddBackward0>)
0.57802075
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
0.57800537
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
0.57785594
tensor(0.1987, device='cuda:0', grad_fn=<AddBackward0>)
0.57776272
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.57765472
tensor(0.2053, device='cuda:0', grad_fn=<AddBackward0>)
0.57758409
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
0.57756293
tensor(0.1937, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][  120/  196]   Loss 0.175672   Top1 94.013672   Top5 99.905599   BatchTime 0.315103   LR 0.000105
0.57758617
tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)
0.57761103
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.57758492
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
0.57763439
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.57764983
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
0.57763523
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.57773322
tensor(0.2130, device='cuda:0', grad_fn=<AddBackward0>)
0.57786363
tensor(0.1719, device='cuda:0', grad_fn=<AddBackward0>)
0.57779557
tensor(0.2179, device='cuda:0', grad_fn=<AddBackward0>)
0.57786661
tensor(0.1887, device='cuda:0', grad_fn=<AddBackward0>)
0.57781833
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
0.57769209
tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)
0.57760984
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.57751811
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
0.57742316
tensor(0.1834, device='cuda:0', grad_fn=<AddBackward0>)
0.57737929
tensor(0.1746, device='cuda:0', grad_fn=<AddBackward0>)
0.57735217
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
0.57736206
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
0.57744122
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.57749420
tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)
0.57752800
tensor(0.2244, device='cuda:0', grad_fn=<AddBackward0>)
0.57760686
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
0.57773519
tensor(0.1947, device='cuda:0', grad_fn=<AddBackward0>)
0.57772946
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
0.57774645
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][  140/  196]   Loss 0.174545   Top1 94.040179   Top5 99.913504   BatchTime 0.315964   LR 0.000104
0.57787752
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
0.57782084
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
0.57775873
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
0.57771742
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.57774508
tensor(0.1822, device='cuda:0', grad_fn=<AddBackward0>)
0.57773674
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
0.57769102
tensor(0.1643, device='cuda:0', grad_fn=<AddBackward0>)
0.57764995
tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)
0.57757974
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
0.57765412
tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)
0.57767600
tensor(0.2210, device='cuda:0', grad_fn=<AddBackward0>)
0.57775980
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.57780439
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.57778287
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.57781106
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
0.57765347
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
0.57756853
tensor(0.2183, device='cuda:0', grad_fn=<AddBackward0>)
0.57753998
tensor(0.2046, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][  160/  196]   Loss 0.174192   Top1 94.052734   Top5 99.912109   BatchTime 0.318785   LR 0.000104
0.57762289
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
0.57749104
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
0.57742274
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
0.57741791
tensor(0.1919, device='cuda:0', grad_fn=<AddBackward0>)
0.57745516
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
0.57760179
tensor(0.2207, device='cuda:0', grad_fn=<AddBackward0>)
0.57777435
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
0.57768613
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
0.57760960
tensor(0.1981, device='cuda:0', grad_fn=<AddBackward0>)
0.57747465
tensor(0.1777, device='cuda:0', grad_fn=<AddBackward0>)
0.57743019
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
0.57738584
tensor(0.1990, device='cuda:0', grad_fn=<AddBackward0>)
0.57736909
tensor(0.2523, device='cuda:0', grad_fn=<AddBackward0>)
0.57734913
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
0.57735175
tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)
0.57735497
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
0.57743543
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
0.57753068
tensor(0.2192, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [40][  180/  196]   Loss 0.175333   Top1 93.993056   Top5 99.911024   BatchTime 0.320688   LR 0.000103
0.57747698
tensor(0.1810, device='cuda:0', grad_fn=<AddBackward0>)
0.57733297
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
0.57730198
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.57730919
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.57724178
tensor(0.2375, device='cuda:0', grad_fn=<AddBackward0>)
0.57721323
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
0.57717067
tensor(0.2229, device='cuda:0', grad_fn=<AddBackward0>)
0.57719803
tensor(0.2736, device='cuda:0', grad_fn=<AddBackward0>)
0.57724851
tensor(0.2036, device='cuda:0', grad_fn=<AddBackward0>)
0.57737166
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.57740366
tensor(0.2186, device='cuda:0', grad_fn=<AddBackward0>)
0.57769102
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
0.57756054
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
0.57748091
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
0.57748616
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 93.972    Top5: 99.912    Loss: 0.175
0.57753599
tensor(0.1980, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.421467   Top1 87.636719   Top5 99.414062   BatchTime 0.122376
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.3125)
features.1.conv.0 tensor(0.0703)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0885)
features.2.conv.0 tensor(0.1302)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.5686)
features.3.conv.0 tensor(0.0859)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1152)
features.4.conv.0 tensor(0.0654)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.2371)
features.5.conv.0 tensor(0.3815)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.4909)
features.6.conv.0 tensor(0.0457)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0862)
features.7.conv.0 tensor(0.1596)
features.7.conv.3 tensor(0.4580)
features.7.conv.6 tensor(0.4564)
features.8.conv.0 tensor(0.5935)
features.8.conv.3 tensor(0.5451)
features.8.conv.6 tensor(0.5633)
features.9.conv.0 tensor(0.5325)
features.9.conv.3 tensor(0.5556)
features.9.conv.6 tensor(0.6433)
features.10.conv.0 tensor(0.0589)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0950)
features.11.conv.0 tensor(0.7446)
features.11.conv.3 tensor(0.6404)
features.11.conv.6 tensor(0.7949)
features.12.conv.0 tensor(0.7372)
features.12.conv.3 tensor(0.6713)
features.12.conv.6 tensor(0.8173)
features.13.conv.0 tensor(0.2495)
features.13.conv.3 tensor(0.4863)
features.13.conv.6 tensor(0.0931)
features.14.conv.0 tensor(0.9136)
features.14.conv.3 tensor(0.8264)
features.14.conv.6 tensor(0.9589)
features.15.conv.0 tensor(0.8887)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9657)
features.16.conv.0 tensor(0.6628)
features.16.conv.3 tensor(0.8035)
features.16.conv.6 tensor(0.8888)
conv.0 tensor(0.1356)
tensor(1324408.) 2188896.0
INFO - Validation [40][   40/   40]   Loss 0.408596   Top1 87.900000   Top5 99.510000   BatchTime 0.088442
INFO - ==> Top1: 87.900    Top5: 99.510    Loss: 0.409
INFO - ==> Sparsity : 0.605
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
0.57752168
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
0.57742119
tensor(0.2067, device='cuda:0', grad_fn=<AddBackward0>)
0.57751828
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.57766622
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.57764333
tensor(0.2058, device='cuda:0', grad_fn=<AddBackward0>)
0.57761776
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
0.57760197
tensor(0.2000, device='cuda:0', grad_fn=<AddBackward0>)
0.57765925
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.57766801
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
0.57766843
tensor(0.2336, device='cuda:0', grad_fn=<AddBackward0>)
0.57772130
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
0.57766944
tensor(0.1970, device='cuda:0', grad_fn=<AddBackward0>)
0.57770294
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
0.57770813
tensor(0.2373, device='cuda:0', grad_fn=<AddBackward0>)
0.57760823
tensor(0.1884, device='cuda:0', grad_fn=<AddBackward0>)
0.57761544
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
0.57757980
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
0.57752019
tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)
0.57755589
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
0.57763505
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
0.57761663
tensor(0.1947, device='cuda:0', grad_fn=<AddBackward0>)
0.57749891
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
0.57751286
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][   20/  196]   Loss 0.169991   Top1 94.277344   Top5 99.863281   BatchTime 0.385799   LR 0.000103
0.57746404
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
0.57742268
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
0.57745397
tensor(0.1612, device='cuda:0', grad_fn=<AddBackward0>)
0.57733518
tensor(0.2231, device='cuda:0', grad_fn=<AddBackward0>)
0.57722914
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
0.57720590
tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)
0.57719296
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.57718885
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.57724035
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.57743365
tensor(0.1820, device='cuda:0', grad_fn=<AddBackward0>)
0.57752001
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
0.57766688
tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)
0.57739449
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
0.57724327
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
0.57719523
tensor(0.2075, device='cuda:0', grad_fn=<AddBackward0>)
0.57709897
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
0.57701224
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
0.57691973
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][   40/  196]   Loss 0.164439   Top1 94.541016   Top5 99.863281   BatchTime 0.357084   LR 0.000102
0.57687110
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
0.57675731
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
0.57675123
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
0.57682002
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
0.57684773
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
0.57695889
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
0.57691181
tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)
0.57685220
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
0.57684696
tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>)
0.57687354
tensor(0.2471, device='cuda:0', grad_fn=<AddBackward0>)
0.57682735
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
0.57697684
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.57719320
tensor(0.2360, device='cuda:0', grad_fn=<AddBackward0>)
0.57739383
tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)
0.57729876
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
0.57716179
tensor(0.1774, device='cuda:0', grad_fn=<AddBackward0>)
0.57706910
tensor(0.2122, device='cuda:0', grad_fn=<AddBackward0>)
0.57707381
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.57697791
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][   60/  196]   Loss 0.164424   Top1 94.563802   Top5 99.882812   BatchTime 0.344613   LR 0.000102
0.57684571
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.57685405
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
0.57693011
tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)
0.57705343
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.57710177
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
0.57727039
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.57726598
tensor(0.2402, device='cuda:0', grad_fn=<AddBackward0>)
0.57715803
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
0.57721394
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
0.57704544
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
0.57691330
tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)
0.57696342
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
0.57697219
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.57700974
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
0.57699203
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
0.57695729
tensor(0.2170, device='cuda:0', grad_fn=<AddBackward0>)
0.57698703
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
0.57704979
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
0.57713068
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
0.57722938
tensor(0.2230, device='cuda:0', grad_fn=<AddBackward0>)
0.57709539
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][   80/  196]   Loss 0.164760   Top1 94.589844   Top5 99.907227   BatchTime 0.329425   LR 0.000102
0.57701111
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
0.57693297
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.57689244
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
0.57692295
tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)
0.57685095
tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)
0.57681769
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
0.57690227
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
0.57706219
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.57702422
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
0.57709873
tensor(0.2213, device='cuda:0', grad_fn=<AddBackward0>)
0.57701963
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
0.57700497
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
0.57698911
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
0.57695687
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.57696253
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
0.57688457
tensor(0.2096, device='cuda:0', grad_fn=<AddBackward0>)
0.57692432
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
0.57706195
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.57695055
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][  100/  196]   Loss 0.165490   Top1 94.527344   Top5 99.914062   BatchTime 0.325931   LR 0.000101
0.57697517
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
0.57700974
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
0.57708430
tensor(0.1733, device='cuda:0', grad_fn=<AddBackward0>)
0.57703996
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
0.57694167
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.57684028
tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)
0.57673395
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.57673854
tensor(0.1634, device='cuda:0', grad_fn=<AddBackward0>)
0.57671791
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.57674003
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
0.57676303
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
0.57683492
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
0.57694775
tensor(0.2457, device='cuda:0', grad_fn=<AddBackward0>)
0.57706875
tensor(0.2101, device='cuda:0', grad_fn=<AddBackward0>)
0.57703704
tensor(0.1910, device='cuda:0', grad_fn=<AddBackward0>)
0.57694501
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
0.57703835
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
0.57707804
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
0.57720202
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][  120/  196]   Loss 0.166016   Top1 94.492188   Top5 99.902344   BatchTime 0.324124   LR 0.000101
0.57730669
tensor(0.2086, device='cuda:0', grad_fn=<AddBackward0>)
0.57729238
tensor(0.1748, device='cuda:0', grad_fn=<AddBackward0>)
0.57731277
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
0.57734817
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
0.57730597
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
0.57719344
tensor(0.2192, device='cuda:0', grad_fn=<AddBackward0>)
0.57711983
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
0.57698262
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
0.57686323
tensor(0.2324, device='cuda:0', grad_fn=<AddBackward0>)
0.57686663
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
0.57686335
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.57687312
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
0.57681006
tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)
0.57677943
tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)
0.57677704
tensor(0.1804, device='cuda:0', grad_fn=<AddBackward0>)
0.57684511
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
0.57687932
tensor(0.2548, device='cuda:0', grad_fn=<AddBackward0>)
0.57681882
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
0.57670909
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
0.57671750
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][  140/  196]   Loss 0.166862   Top1 94.450335   Top5 99.910714   BatchTime 0.321629   LR 0.000100
0.57679111
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
0.57676291
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
0.57665688
tensor(0.2044, device='cuda:0', grad_fn=<AddBackward0>)
0.57669502
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
0.57679766
tensor(0.2169, device='cuda:0', grad_fn=<AddBackward0>)
0.57673889
tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)
0.57660735
tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)
0.57655793
tensor(0.2293, device='cuda:0', grad_fn=<AddBackward0>)
0.57642901
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.57648617
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.57646108
tensor(0.2679, device='cuda:0', grad_fn=<AddBackward0>)
0.57646525
tensor(0.2788, device='cuda:0', grad_fn=<AddBackward0>)
0.57649112
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.57635647
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
0.57634127
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.57636505
tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)
0.57638538
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.57634443
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.57635552
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
0.57631379
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][  160/  196]   Loss 0.168570   Top1 94.382324   Top5 99.914551   BatchTime 0.319490   LR 0.000100
0.57625133
tensor(0.1423, device='cuda:0', grad_fn=<AddBackward0>)
0.57617539
tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)
0.57615930
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.57614893
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.57612902
tensor(0.2065, device='cuda:0', grad_fn=<AddBackward0>)
0.57598788
tensor(0.2495, device='cuda:0', grad_fn=<AddBackward0>)
0.57589823
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.57589090
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
0.57588726
tensor(0.1854, device='cuda:0', grad_fn=<AddBackward0>)
0.57590413
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
0.57573092
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
0.57571125
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
0.57560396
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.57556599
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
0.57549119
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
0.57540244
tensor(0.2229, device='cuda:0', grad_fn=<AddBackward0>)
0.57533562
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
0.57530975
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.57525474
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
0.57524246
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [41][  180/  196]   Loss 0.168723   Top1 94.379340   Top5 99.915365   BatchTime 0.314095   LR 0.000100
0.57519859
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.57535416
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
0.57523435
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
0.57508427
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.57494432
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
0.57485199
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
0.57471400
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
0.57458192
tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)
0.57449567
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.57445872
tensor(0.1664, device='cuda:0', grad_fn=<AddBackward0>)
0.57439691
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
0.57438773
tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)
0.57441264
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
0.57450759
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
0.57437736
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
0.57431620
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.414    Top5: 99.918    Loss: 0.168
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.57415533
tensor(0.2622, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [41][   20/   40]   Loss 0.397664   Top1 88.359375   Top5 99.550781   BatchTime 0.118232
INFO - Validation [41][   40/   40]   Loss 0.388085   Top1 88.310000   Top5 99.690000   BatchTime 0.086319
INFO - ==> Top1: 88.310    Top5: 99.690    Loss: 0.388
INFO - ==> Sparsity : 0.606
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.3145)
features.1.conv.0 tensor(0.0671)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0911)
features.2.conv.0 tensor(0.1262)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.5712)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1122)
features.4.conv.0 tensor(0.0620)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.2918)
features.5.conv.0 tensor(0.3771)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.4953)
features.6.conv.0 tensor(0.0480)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0848)
features.7.conv.0 tensor(0.1637)
features.7.conv.3 tensor(0.4566)
features.7.conv.6 tensor(0.4630)
features.8.conv.0 tensor(0.6059)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.5764)
features.9.conv.0 tensor(0.5108)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6430)
features.10.conv.0 tensor(0.0587)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0939)
features.11.conv.0 tensor(0.7514)
features.11.conv.3 tensor(0.6395)
features.11.conv.6 tensor(0.7959)
features.12.conv.0 tensor(0.7309)
features.12.conv.3 tensor(0.6723)
features.12.conv.6 tensor(0.8186)
features.13.conv.0 tensor(0.2427)
features.13.conv.3 tensor(0.4850)
features.13.conv.6 tensor(0.0907)
features.14.conv.0 tensor(0.9146)
features.14.conv.3 tensor(0.8272)
features.14.conv.6 tensor(0.9589)
features.15.conv.0 tensor(0.8906)
features.15.conv.3 tensor(0.8360)
features.15.conv.6 tensor(0.9667)
features.16.conv.0 tensor(0.6637)
features.16.conv.3 tensor(0.8035)
features.16.conv.6 tensor(0.8898)
conv.0 tensor(0.1400)
tensor(1327322.) 2188896.0
0.57410693
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
0.57400858
tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)
0.57403636
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
0.57401949
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
0.57403171
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.57399243
tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)
0.57402569
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.57403684
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
0.57388663
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.57371664
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
0.57357180
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
0.57352406
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
0.57350993
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
0.57345253
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
0.57346493
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.57351786
tensor(0.2536, device='cuda:0', grad_fn=<AddBackward0>)
0.57361513
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
0.57369900
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
0.57372242
tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)
0.57370049
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.57366765
tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)
0.57353252
tensor(0.1855, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][   20/  196]   Loss 0.161167   Top1 94.433594   Top5 99.882812   BatchTime 0.345875   LR 0.000099
0.57341623
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
0.57334948
tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)
0.57336837
tensor(0.1665, device='cuda:0', grad_fn=<AddBackward0>)
0.57339925
tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)
0.57356483
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.57363135
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
0.57358170
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
0.57342732
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.57338345
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
0.57336551
tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)
0.57324582
tensor(0.1404, device='cuda:0', grad_fn=<AddBackward0>)
0.57320237
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
0.57307714
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.57300401
tensor(0.2140, device='cuda:0', grad_fn=<AddBackward0>)
0.57300603
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
0.57299662
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.57299060
tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)
0.57299525
tensor(0.1677, device='cuda:0', grad_fn=<AddBackward0>)
0.57305682
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
0.57317495
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][   40/  196]   Loss 0.164565   Top1 94.335938   Top5 99.912109   BatchTime 0.327447   LR 0.000098
0.57325912
tensor(0.2101, device='cuda:0', grad_fn=<AddBackward0>)
0.57331997
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
0.57333934
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
0.57338792
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.57335103
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.57338816
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
0.57339209
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
0.57329571
tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)
0.57327032
tensor(0.2123, device='cuda:0', grad_fn=<AddBackward0>)
0.57328314
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
0.57325149
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
0.57329750
tensor(0.2325, device='cuda:0', grad_fn=<AddBackward0>)
0.57342225
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.57338560
tensor(0.2396, device='cuda:0', grad_fn=<AddBackward0>)
0.57338649
tensor(0.1732, device='cuda:0', grad_fn=<AddBackward0>)
0.57341290
tensor(0.2269, device='cuda:0', grad_fn=<AddBackward0>)
0.57330579
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
0.57330638
tensor(0.1770, device='cuda:0', grad_fn=<AddBackward0>)
0.57311767
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][   60/  196]   Loss 0.168304   Top1 94.199219   Top5 99.915365   BatchTime 0.323757   LR 0.000098
0.57301801
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
0.57291186
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
0.57293916
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
0.57296103
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.57296246
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.57297361
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
0.57313704
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
0.57324088
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
0.57322562
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
0.57316405
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
0.57315499
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
0.57304889
tensor(0.1809, device='cuda:0', grad_fn=<AddBackward0>)
0.57300651
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.57289773
tensor(0.2150, device='cuda:0', grad_fn=<AddBackward0>)
0.57287651
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
0.57293868
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
0.57301825
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
0.57313430
tensor(0.1964, device='cuda:0', grad_fn=<AddBackward0>)
0.57310218
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
0.57297260
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][   80/  196]   Loss 0.170014   Top1 94.213867   Top5 99.912109   BatchTime 0.317354   LR 0.000098
0.57306230
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.57307953
tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)
0.57295758
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
0.57288188
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
0.57298726
tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)
0.57304919
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
0.57296109
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
0.57299179
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
0.57294983
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
0.57293671
tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)
0.57295549
tensor(0.2524, device='cuda:0', grad_fn=<AddBackward0>)
0.57296801
tensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)
0.57296646
tensor(0.2344, device='cuda:0', grad_fn=<AddBackward0>)
0.57298344
tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)
0.57292730
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
0.57292593
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
0.57296538
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.57293707
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
0.57295322
tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)
0.57295698
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
0.57305485
tensor(0.1941, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][  100/  196]   Loss 0.169430   Top1 94.281250   Top5 99.921875   BatchTime 0.312681   LR 0.000097
0.57313186
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
0.57315409
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
0.57302028
tensor(0.1976, device='cuda:0', grad_fn=<AddBackward0>)
0.57303900
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
0.57318145
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
0.57312506
tensor(0.2230, device='cuda:0', grad_fn=<AddBackward0>)
0.57291365
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
0.57278937
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
0.57273370
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
0.57280612
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
0.57287455
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.57278490
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
0.57279843
tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)
0.57282627
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
0.57299656
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
0.57309115
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.57298905
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.57279414
tensor(0.1913, device='cuda:0', grad_fn=<AddBackward0>)
0.57268959
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
0.57279432
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][  120/  196]   Loss 0.167667   Top1 94.316406   Top5 99.912109   BatchTime 0.310038   LR 0.000097
0.57274371
tensor(0.1879, device='cuda:0', grad_fn=<AddBackward0>)
0.57269025
tensor(0.1808, device='cuda:0', grad_fn=<AddBackward0>)
0.57274592
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
0.57276279
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.57276917
tensor(0.2411, device='cuda:0', grad_fn=<AddBackward0>)
0.57266068
tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)
0.57267928
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
0.57265997
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
0.57264555
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
0.57263160
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
0.57267660
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
0.57280517
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
0.57264209
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
0.57267869
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.57275355
tensor(0.2022, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][  140/  196]   Loss 0.167538   Top1 94.321987   Top5 99.921875   BatchTime 0.304475   LR 0.000096
0.57279688
tensor(0.2106, device='cuda:0', grad_fn=<AddBackward0>)
0.57282102
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
0.57272977
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
0.57269323
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
0.57276022
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
0.57269126
tensor(0.2506, device='cuda:0', grad_fn=<AddBackward0>)
0.57269531
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.57261086
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
0.57264966
tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)
0.57263052
tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)
0.57260913
tensor(0.2013, device='cuda:0', grad_fn=<AddBackward0>)
0.57264411
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
0.57273567
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
0.57266843
tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)
0.57271767
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
0.57269287
tensor(0.2315, device='cuda:0', grad_fn=<AddBackward0>)
0.57261682
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.57262486
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
0.57259178
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.57255518
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
0.57255280
tensor(0.2460, device='cuda:0', grad_fn=<AddBackward0>)
0.57264221
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
0.57273251
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
0.57274765
tensor(0.2219, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][  160/  196]   Loss 0.169302   Top1 94.267578   Top5 99.912109   BatchTime 0.298210   LR 0.000096
0.57278532
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
0.57278591
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
0.57265729
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
0.57264316
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
0.57262141
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
0.57273358
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
0.57257807
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
0.57255620
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
0.57253551
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.57249081
tensor(0.1834, device='cuda:0', grad_fn=<AddBackward0>)
0.57240534
tensor(0.2129, device='cuda:0', grad_fn=<AddBackward0>)
0.57242322
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
0.57249755
tensor(0.2587, device='cuda:0', grad_fn=<AddBackward0>)
0.57269347
tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)
0.57265079
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
0.57270449
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
0.57275259
tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [42][  180/  196]   Loss 0.170796   Top1 94.249132   Top5 99.911024   BatchTime 0.291297   LR 0.000096
0.57274336
tensor(0.2184, device='cuda:0', grad_fn=<AddBackward0>)
0.57261431
tensor(0.2692, device='cuda:0', grad_fn=<AddBackward0>)
0.57249296
tensor(0.1754, device='cuda:0', grad_fn=<AddBackward0>)
0.57240182
tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)
0.57240468
tensor(0.2377, device='cuda:0', grad_fn=<AddBackward0>)
0.57236588
tensor(0.1801, device='cuda:0', grad_fn=<AddBackward0>)
0.57231063
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
0.57227564
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.57241666
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.57252568
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
0.57257622
tensor(0.1938, device='cuda:0', grad_fn=<AddBackward0>)
0.57261139
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
0.57262594
tensor(0.2152, device='cuda:0', grad_fn=<AddBackward0>)
0.57258618
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
0.57259607
tensor(0.2616, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.178    Top5: 99.918    Loss: 0.172
0.57260728
tensor(0.2362, device='cuda:0', grad_fn=<AddBackward0>)
0.57256442
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
0.57251495
tensor(0.1776, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [42][   20/   40]   Loss 0.404831   Top1 87.988281   Top5 99.414062   BatchTime 0.125015
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.3164)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0920)
features.2.conv.0 tensor(0.1143)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.5741)
features.3.conv.0 tensor(0.0741)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.1139)
features.4.conv.0 tensor(0.0612)
features.4.conv.3 tensor(0.2992)
features.4.conv.6 tensor(0.2980)
features.5.conv.0 tensor(0.3892)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.4979)
features.6.conv.0 tensor(0.0461)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0870)
features.7.conv.0 tensor(0.1743)
features.7.conv.3 tensor(0.4572)
features.7.conv.6 tensor(0.4653)
features.8.conv.0 tensor(0.6030)
features.8.conv.3 tensor(0.5437)
features.8.conv.6 tensor(0.5871)
features.9.conv.0 tensor(0.5156)
features.9.conv.3 tensor(0.5544)
features.9.conv.6 tensor(0.6441)
features.10.conv.0 tensor(0.0591)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0936)
features.11.conv.0 tensor(0.7564)
features.11.conv.3 tensor(0.6410)
features.11.conv.6 tensor(0.8017)
features.12.conv.0 tensor(0.7373)
features.12.conv.3 tensor(0.6717)
features.12.conv.6 tensor(0.8205)
features.13.conv.0 tensor(0.2512)
features.13.conv.3 tensor(0.4871)
features.13.conv.6 tensor(0.0896)
features.14.conv.0 tensor(0.9151)
features.14.conv.3 tensor(0.8279)
features.14.conv.6 tensor(0.9594)
features.15.conv.0 tensor(0.8904)
features.15.conv.3 tensor(0.8363)
features.15.conv.6 tensor(0.9661)
features.16.conv.0 tensor(0.6610)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.8896)
conv.0 tensor(0.1460)
tensor(1331554.) 2188896.0
INFO - Validation [42][   40/   40]   Loss 0.390583   Top1 87.990000   Top5 99.590000   BatchTime 0.088700
INFO - ==> Top1: 87.990    Top5: 99.590    Loss: 0.391
INFO - ==> Sparsity : 0.608
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
0.57243490
tensor(0.1910, device='cuda:0', grad_fn=<AddBackward0>)
0.57250130
tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)
0.57249653
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.57247907
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
0.57252198
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
0.57241249
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
0.57234323
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
0.57240194
tensor(0.2541, device='cuda:0', grad_fn=<AddBackward0>)
0.57241708
tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)
0.57242447
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
0.57243335
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.57235956
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
0.57244176
tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)
0.57245666
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
0.57240051
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
0.57236910
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
0.57244736
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
0.57239431
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
0.57236445
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
0.57242966
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][   20/  196]   Loss 0.153203   Top1 95.019531   Top5 99.960938   BatchTime 0.405071   LR 0.000095
0.57236379
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.57227796
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.57220048
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.57223284
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
0.57233024
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
0.57230300
tensor(0.1820, device='cuda:0', grad_fn=<AddBackward0>)
0.57230163
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.57231939
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
0.57227951
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
0.57223874
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
0.57225251
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
0.57226551
tensor(0.1643, device='cuda:0', grad_fn=<AddBackward0>)
0.57225841
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
0.57234102
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
0.57225126
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
0.57221860
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
0.57218987
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.57225531
tensor(0.1748, device='cuda:0', grad_fn=<AddBackward0>)
0.57239985
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.57248896
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
0.57239085
tensor(0.2409, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][   40/  196]   Loss 0.153021   Top1 95.087891   Top5 99.941406   BatchTime 0.351705   LR 0.000094
0.57248873
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
0.57237011
tensor(0.2126, device='cuda:0', grad_fn=<AddBackward0>)
0.57230663
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.57221246
tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)
0.57216769
tensor(0.1919, device='cuda:0', grad_fn=<AddBackward0>)
0.57220113
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
0.57213199
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
0.57213455
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
0.57203293
tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)
0.57202250
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
0.57213300
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
0.57213521
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
0.57214421
tensor(0.2002, device='cuda:0', grad_fn=<AddBackward0>)
0.57211620
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
0.57227236
tensor(0.2153, device='cuda:0', grad_fn=<AddBackward0>)
0.57226366
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.57222497
tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)
0.57210124
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
0.57218003
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.57212752
tensor(0.1794, device='cuda:0', grad_fn=<AddBackward0>)
0.57207769
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.57207656
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][   60/  196]   Loss 0.159667   Top1 94.837240   Top5 99.960938   BatchTime 0.322166   LR 0.000094
0.57219785
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.57212132
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
0.57199550
tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)
0.57196283
tensor(0.1454, device='cuda:0', grad_fn=<AddBackward0>)
0.57193762
tensor(0.1598, device='cuda:0', grad_fn=<AddBackward0>)
0.57198197
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
0.57206053
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
0.57213455
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
0.57229930
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
0.57221842
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
0.57210672
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.57213950
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
0.57216185
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
0.57218677
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
0.57205081
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
0.57196677
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
0.57201451
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
0.57211030
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
0.57207459
tensor(0.1850, device='cuda:0', grad_fn=<AddBackward0>)
0.57206094
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][   80/  196]   Loss 0.157612   Top1 94.868164   Top5 99.956055   BatchTime 0.316414   LR 0.000093
0.57205296
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.57202184
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
0.57208306
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
0.57215047
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
0.57215178
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.57208687
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.57202095
tensor(0.2007, device='cuda:0', grad_fn=<AddBackward0>)
0.57208711
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
0.57211179
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
0.57196194
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
0.57189894
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
0.57182771
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
0.57183546
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
0.57186437
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
0.57200378
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
0.57207918
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
0.57209057
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
0.57208526
tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>)
0.57215244
tensor(0.1579, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][  100/  196]   Loss 0.156834   Top1 94.839844   Top5 99.957031   BatchTime 0.316801   LR 0.000093
0.57200289
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.57195634
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
0.57191139
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
0.57190078
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
0.57193595
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
0.57187188
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
0.57192093
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.57196248
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.57200664
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
0.57194668
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.57194215
tensor(0.2119, device='cuda:0', grad_fn=<AddBackward0>)
0.57194531
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
0.57190442
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
0.57186019
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
0.57187825
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
0.57189584
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
0.57194841
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
0.57203907
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.57189560
tensor(0.1910, device='cuda:0', grad_fn=<AddBackward0>)
0.57177621
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
0.57185012
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.57181603
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][  120/  196]   Loss 0.155599   Top1 94.886068   Top5 99.957682   BatchTime 0.310985   LR 0.000093
0.57177657
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
0.57181740
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.57192993
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
0.57195359
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.57193214
tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)
0.57175851
tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)
0.57171094
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
0.57171416
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
0.57173318
tensor(0.2323, device='cuda:0', grad_fn=<AddBackward0>)
0.57175064
tensor(0.1562, device='cuda:0', grad_fn=<AddBackward0>)
0.57171708
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
0.57167614
tensor(0.1889, device='cuda:0', grad_fn=<AddBackward0>)
0.57177746
tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)
0.57169110
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
0.57171082
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.57173508
tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)
0.57173753
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
0.57173687
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][  140/  196]   Loss 0.155804   Top1 94.810268   Top5 99.955357   BatchTime 0.309859   LR 0.000092
0.57169712
tensor(0.2518, device='cuda:0', grad_fn=<AddBackward0>)
0.57169187
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
0.57170552
tensor(0.1951, device='cuda:0', grad_fn=<AddBackward0>)
0.57163453
tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)
0.57166880
tensor(0.2465, device='cuda:0', grad_fn=<AddBackward0>)
0.57166219
tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)
0.57173425
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
0.57181776
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
0.57168853
tensor(0.1990, device='cuda:0', grad_fn=<AddBackward0>)
0.57166308
tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)
0.57153934
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
0.57154894
tensor(0.1949, device='cuda:0', grad_fn=<AddBackward0>)
0.57157952
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
0.57153797
tensor(0.1833, device='cuda:0', grad_fn=<AddBackward0>)
0.57159579
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
0.57163173
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
0.57149041
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
0.57149273
tensor(0.1306, device='cuda:0', grad_fn=<AddBackward0>)
0.57157671
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][  160/  196]   Loss 0.157570   Top1 94.704590   Top5 99.946289   BatchTime 0.311392   LR 0.000092
0.57147038
tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)
0.57147360
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.57148325
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
0.57127595
tensor(0.2419, device='cuda:0', grad_fn=<AddBackward0>)
0.57119608
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.57110542
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
0.57106704
tensor(0.2189, device='cuda:0', grad_fn=<AddBackward0>)
0.57098556
tensor(0.2351, device='cuda:0', grad_fn=<AddBackward0>)
0.57100815
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.57099378
tensor(0.1529, device='cuda:0', grad_fn=<AddBackward0>)
0.57095104
tensor(0.2417, device='cuda:0', grad_fn=<AddBackward0>)
0.57091725
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.57094574
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
0.57086688
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.57076132
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
0.57079822
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.57069856
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
0.57059592
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [43][  180/  196]   Loss 0.159210   Top1 94.663628   Top5 99.934896   BatchTime 0.314008   LR 0.000091
0.57055598
tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)
0.57049233
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
0.57036984
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.57027203
tensor(0.1498, device='cuda:0', grad_fn=<AddBackward0>)
0.57015228
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
0.57006860
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
0.56988549
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
0.56982392
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
0.56976366
tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)
0.56966412
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
0.56970316
tensor(0.2160, device='cuda:0', grad_fn=<AddBackward0>)
0.56973165
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
0.56967175
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
0.56970119
tensor(0.2078, device='cuda:0', grad_fn=<AddBackward0>)
0.56968528
tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)
0.56965274
tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)
0.56955642
tensor(0.2122, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.686    Top5: 99.936    Loss: 0.160
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [43][   20/   40]   Loss 0.402199   Top1 87.675781   Top5 99.628906   BatchTime 0.121403
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.3203)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0907)
features.2.conv.0 tensor(0.1319)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.5712)
features.3.conv.0 tensor(0.0732)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.1144)
features.4.conv.0 tensor(0.0566)
features.4.conv.3 tensor(0.3015)
features.4.conv.6 tensor(0.3491)
features.5.conv.0 tensor(0.4036)
features.5.conv.3 tensor(0.4184)
features.5.conv.6 tensor(0.4982)
features.6.conv.0 tensor(0.0519)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0860)
features.7.conv.0 tensor(0.1656)
features.7.conv.3 tensor(0.4580)
features.7.conv.6 tensor(0.4773)
features.8.conv.0 tensor(0.6030)
features.8.conv.3 tensor(0.5434)
features.8.conv.6 tensor(0.6188)
features.9.conv.0 tensor(0.5168)
features.9.conv.3 tensor(0.5544)
features.9.conv.6 tensor(0.6448)
features.10.conv.0 tensor(0.0553)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0930)
features.11.conv.0 tensor(0.7525)
features.11.conv.3 tensor(0.6393)
features.11.conv.6 tensor(0.8014)
features.12.conv.0 tensor(0.7363)
features.12.conv.3 tensor(0.6696)
features.12.conv.6 tensor(0.8218)
features.13.conv.0 tensor(0.2642)
features.13.conv.3 tensor(0.4867)
features.13.conv.6 tensor(0.0887)
features.14.conv.0 tensor(0.9155)
features.14.conv.3 tensor(0.8277)
features.14.conv.6 tensor(0.9593)
features.15.conv.0 tensor(0.8916)
features.15.conv.3 tensor(0.8355)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.6669)
features.16.conv.3 tensor(0.8036)
features.16.conv.6 tensor(0.8914)
conv.0 tensor(0.1529)
tensor(1337584.) 2188896.0
INFO - Validation [43][   40/   40]   Loss 0.386736   Top1 88.420000   Top5 99.700000   BatchTime 0.087623
INFO - ==> Top1: 88.420    Top5: 99.700    Loss: 0.387
INFO - ==> Sparsity : 0.611
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
0.56953377
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.56949431
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
0.56941891
tensor(0.2193, device='cuda:0', grad_fn=<AddBackward0>)
0.56933600
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
0.56923026
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
0.56919521
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
0.56919557
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.56915992
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
0.56909269
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.56911302
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.56907350
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
0.56910527
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.56905812
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.56909621
tensor(0.1851, device='cuda:0', grad_fn=<AddBackward0>)
0.56909990
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
0.56902057
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
0.56903762
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.56892145
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.56877404
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.56868577
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.56854475
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.56831813
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][   20/  196]   Loss 0.135496   Top1 95.449219   Top5 99.980469   BatchTime 0.403536   LR 0.000090
0.56825763
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
0.56824160
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
0.56805581
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
0.56792247
tensor(0.1643, device='cuda:0', grad_fn=<AddBackward0>)
0.56775665
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
0.56761789
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
0.56748909
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
0.56736165
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.56734991
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.56712395
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
0.56696278
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.56696224
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
0.56698173
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
0.56689578
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.56685579
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
0.56685275
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
0.56690651
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.56667554
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
0.56664288
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.56666565
tensor(0.2010, device='cuda:0', grad_fn=<AddBackward0>)
0.56665391
tensor(0.1801, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][   40/  196]   Loss 0.138572   Top1 95.302734   Top5 99.980469   BatchTime 0.351334   LR 0.000090
0.56666720
tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)
0.56670839
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
0.56662440
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
0.56650454
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
0.56642395
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
0.56637228
tensor(0.1811, device='cuda:0', grad_fn=<AddBackward0>)
0.56638211
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
0.56631619
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
0.56630868
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.56622368
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
0.56615913
tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)
0.56615460
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.56621522
tensor(0.2045, device='cuda:0', grad_fn=<AddBackward0>)
0.56622380
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
0.56623447
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
0.56617880
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
0.56614244
tensor(0.2177, device='cuda:0', grad_fn=<AddBackward0>)
0.56606066
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][   60/  196]   Loss 0.144504   Top1 95.123698   Top5 99.967448   BatchTime 0.341863   LR 0.000090
0.56597000
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
0.56588870
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
0.56581903
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
0.56579173
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
0.56576973
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
0.56585747
tensor(0.1801, device='cuda:0', grad_fn=<AddBackward0>)
0.56584901
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
0.56577605
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
0.56556773
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
0.56542009
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.56523329
tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)
0.56506866
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
0.56495333
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
0.56479317
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
0.56465572
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.56452501
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
0.56443518
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
0.56440216
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
0.56442565
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][   80/  196]   Loss 0.144300   Top1 95.195312   Top5 99.970703   BatchTime 0.335255   LR 0.000089
0.56437725
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.56437564
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
0.56416333
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
0.56397188
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
0.56382841
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
0.56372231
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
0.56360793
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
0.56352419
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
0.56338727
tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)
0.56333542
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.56333238
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.56345153
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
0.56323302
tensor(0.2826, device='cuda:0', grad_fn=<AddBackward0>)
0.56295067
tensor(0.2677, device='cuda:0', grad_fn=<AddBackward0>)
0.56254470
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
0.56212682
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
0.56165147
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
0.56118840
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.56074935
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.56047428
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
0.56005746
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
0.55973446
tensor(0.1852, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][  100/  196]   Loss 0.147364   Top1 95.062500   Top5 99.968750   BatchTime 0.325205   LR 0.000089
0.55943888
tensor(0.1733, device='cuda:0', grad_fn=<AddBackward0>)
0.55913597
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
0.55885816
tensor(0.1889, device='cuda:0', grad_fn=<AddBackward0>)
0.55860442
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
0.55846542
tensor(0.1822, device='cuda:0', grad_fn=<AddBackward0>)
0.55842716
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
0.55830550
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
0.55817634
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
0.55802500
tensor(0.2062, device='cuda:0', grad_fn=<AddBackward0>)
0.55789047
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.55775315
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
0.55764812
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.55773389
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
0.55768800
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.55753148
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
0.55739313
tensor(0.2058, device='cuda:0', grad_fn=<AddBackward0>)
0.55733418
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.55725557
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
0.55713433
tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)
0.55696368
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
0.55656624
tensor(0.2559, device='cuda:0', grad_fn=<AddBackward0>)
0.55649227
tensor(0.2185, device='cuda:0', grad_fn=<AddBackward0>)
0.55675316
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][  120/  196]   Loss 0.149498   Top1 94.967448   Top5 99.960938   BatchTime 0.314734   LR 0.000088
0.55679220
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
0.55678117
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.55669236
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.55655056
tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)
0.55639988
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
0.55623358
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
0.55609161
tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)
0.55598605
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.55584466
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
0.55567032
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
0.55545026
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
0.55535668
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
0.55522686
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
0.55515778
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][  140/  196]   Loss 0.151053   Top1 94.913504   Top5 99.958147   BatchTime 0.308203   LR 0.000088
0.55510825
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
0.55505091
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.55493027
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.55482256
tensor(0.2136, device='cuda:0', grad_fn=<AddBackward0>)
0.55480713
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
0.55467379
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
0.55465847
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
0.55462593
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
0.55460805
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
0.55458146
tensor(0.2083, device='cuda:0', grad_fn=<AddBackward0>)
0.55456489
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.55445373
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
0.55440593
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
0.55442947
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.55436409
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
0.55424005
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
0.55418360
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
0.55413389
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
0.55408132
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][  160/  196]   Loss 0.150598   Top1 94.914551   Top5 99.953613   BatchTime 0.309331   LR 0.000087
0.55399853
tensor(0.2440, device='cuda:0', grad_fn=<AddBackward0>)
0.55395311
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
0.55389816
tensor(0.2317, device='cuda:0', grad_fn=<AddBackward0>)
0.55388838
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.55382198
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
0.55367929
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
0.55360359
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
0.55356991
tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)
0.55355781
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
0.55358517
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
0.55362380
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
0.55371350
tensor(0.1817, device='cuda:0', grad_fn=<AddBackward0>)
0.55375844
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
0.55375618
tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)
0.55371183
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
0.55371350
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
0.55375826
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
0.55371511
tensor(0.1644, device='cuda:0', grad_fn=<AddBackward0>)
0.55368584
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
0.55376500
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
0.55380762
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
0.55378580
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
0.55382621
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
0.55377728
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.55378753
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
0.55382377
tensor(0.2675, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [44][  180/  196]   Loss 0.151114   Top1 94.895833   Top5 99.950087   BatchTime 0.309753   LR 0.000087
0.55390590
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.55393779
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
0.55400425
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
0.55400187
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
0.55402625
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
0.55401886
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
0.55401856
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
0.55396688
tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)
0.55397874
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
0.55392051
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
0.55395633
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
0.55401587
tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 94.864    Top5: 99.950    Loss: 0.152
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [44][   20/   40]   Loss 0.413549   Top1 87.695312   Top5 99.531250   BatchTime 0.130964
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.3203)
features.1.conv.0 tensor(0.0605)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0911)
features.2.conv.0 tensor(0.1152)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.5752)
features.3.conv.0 tensor(0.0732)
features.3.conv.3 tensor(0.0710)
features.3.conv.6 tensor(0.1157)
features.4.conv.0 tensor(0.0609)
features.4.conv.3 tensor(0.3009)
features.4.conv.6 tensor(0.3571)
features.5.conv.0 tensor(0.4152)
features.5.conv.3 tensor(0.4190)
features.5.conv.6 tensor(0.4966)
features.6.conv.0 tensor(0.0470)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0836)
features.7.conv.0 tensor(0.1735)
features.7.conv.3 tensor(0.4531)
features.7.conv.6 tensor(0.4772)
features.8.conv.0 tensor(0.6003)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.6219)
features.9.conv.0 tensor(0.5138)
features.9.conv.3 tensor(0.5582)
features.9.conv.6 tensor(0.6470)
features.10.conv.0 tensor(0.0588)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0939)
features.11.conv.0 tensor(0.7561)
features.11.conv.3 tensor(0.6377)
features.11.conv.6 tensor(0.8070)
features.12.conv.0 tensor(0.7490)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.8252)
features.13.conv.0 tensor(0.2549)
features.13.conv.3 tensor(0.4851)
features.13.conv.6 tensor(0.3209)
features.14.conv.0 tensor(0.9160)
features.14.conv.3 tensor(0.8272)
features.14.conv.6 tensor(0.9595)
features.15.conv.0 tensor(0.8936)
features.15.conv.3 tensor(0.8359)
features.15.conv.6 tensor(0.9661)
features.16.conv.0 tensor(0.6674)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.8911)
conv.0 tensor(0.1583)
tensor(1362935.) 2188896.0
INFO - Validation [44][   40/   40]   Loss 0.398916   Top1 88.080000   Top5 99.620000   BatchTime 0.091961
INFO - ==> Top1: 88.080    Top5: 99.620    Loss: 0.399
INFO - ==> Sparsity : 0.623
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
0.55402350
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
0.55393040
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
0.55389810
tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)
0.55390412
tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>)
0.55392164
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.55385971
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
0.55381149
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
0.55380857
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
0.55388469
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.55391699
tensor(0.2333, device='cuda:0', grad_fn=<AddBackward0>)
0.55392736
tensor(0.2762, device='cuda:0', grad_fn=<AddBackward0>)
0.55390215
tensor(0.1833, device='cuda:0', grad_fn=<AddBackward0>)
0.55385458
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
0.55385816
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
0.55390155
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
0.55407453
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
0.55448097
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
0.55532181
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
0.55540365
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
0.55537921
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
0.55536765
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
0.55530745
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.55523747
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][   20/  196]   Loss 0.158253   Top1 94.843750   Top5 99.921875   BatchTime 0.378030   LR 0.000086
0.55522585
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.55523109
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
0.55521661
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
0.55519927
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.55512333
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
0.55505806
tensor(0.2205, device='cuda:0', grad_fn=<AddBackward0>)
0.55502713
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
0.55496180
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
0.55493522
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
0.55493212
tensor(0.1785, device='cuda:0', grad_fn=<AddBackward0>)
0.55499661
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.55492920
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.55491436
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
0.55490261
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
0.55493790
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
0.55499685
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.55510765
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.55500305
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
0.55496520
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][   40/  196]   Loss 0.146981   Top1 95.156250   Top5 99.951172   BatchTime 0.348709   LR 0.000086
0.55491459
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.55488944
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.55479324
tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)
0.55472964
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
0.55469972
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
0.55462915
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.55464590
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
0.55474967
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
0.55485153
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
0.55489933
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
0.55495018
tensor(0.2208, device='cuda:0', grad_fn=<AddBackward0>)
0.55478793
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
0.55468678
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
0.55466527
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
0.55464977
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
0.55456966
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
0.55453235
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
0.55463767
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
0.55471116
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.55477232
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.55473101
tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][   60/  196]   Loss 0.145962   Top1 95.091146   Top5 99.967448   BatchTime 0.329073   LR 0.000085
0.55481631
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.55474049
tensor(0.1810, device='cuda:0', grad_fn=<AddBackward0>)
0.55461502
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
0.55453259
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
0.55449957
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
0.55445641
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
0.55448121
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
0.55460125
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
0.55476892
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
0.55478626
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
0.55463660
tensor(0.2001, device='cuda:0', grad_fn=<AddBackward0>)
0.55460894
tensor(0.1631, device='cuda:0', grad_fn=<AddBackward0>)
0.55451345
tensor(0.2721, device='cuda:0', grad_fn=<AddBackward0>)
0.55439377
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
0.55430466
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
0.55426860
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
0.55431825
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
0.55436945
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
0.55458808
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][   80/  196]   Loss 0.149586   Top1 94.941406   Top5 99.956055   BatchTime 0.323964   LR 0.000085
0.55450487
tensor(0.1821, device='cuda:0', grad_fn=<AddBackward0>)
0.55467290
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.55476052
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
0.55471170
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
0.55467820
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
0.55470794
tensor(0.1835, device='cuda:0', grad_fn=<AddBackward0>)
0.55471343
tensor(0.2010, device='cuda:0', grad_fn=<AddBackward0>)
0.55485684
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
0.55483603
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
0.55482763
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
0.55480570
tensor(0.2010, device='cuda:0', grad_fn=<AddBackward0>)
0.55470830
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
0.55466688
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
0.55458999
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.55455291
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
0.55449241
tensor(0.2259, device='cuda:0', grad_fn=<AddBackward0>)
0.55448955
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
0.55456215
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
0.55458450
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.55459249
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][  100/  196]   Loss 0.148351   Top1 95.039062   Top5 99.957031   BatchTime 0.320707   LR 0.000084
0.55433398
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
0.55434531
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
0.55443102
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
0.55433893
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
0.55430388
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
0.55432051
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.55427563
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
0.55431515
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.55438232
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
0.55434448
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
0.55424923
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
0.55415779
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.55407828
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
0.55403394
tensor(0.1414, device='cuda:0', grad_fn=<AddBackward0>)
0.55398971
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
0.55399299
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
0.55397803
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.55394769
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.55395722
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][  120/  196]   Loss 0.146397   Top1 95.117188   Top5 99.960938   BatchTime 0.320417   LR 0.000084
0.55396396
tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)
0.55402768
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
0.55395418
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
0.55391175
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
0.55398750
tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)
0.55400991
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
0.55388212
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
0.55376726
tensor(0.1836, device='cuda:0', grad_fn=<AddBackward0>)
0.55365968
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
0.55360550
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
0.55357385
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
0.55359036
tensor(0.2471, device='cuda:0', grad_fn=<AddBackward0>)
0.55361950
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
0.55366951
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
0.55375654
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
0.55377322
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
0.55379236
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.55371368
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.55364174
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
0.55361056
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][  140/  196]   Loss 0.147292   Top1 95.108817   Top5 99.944196   BatchTime 0.317645   LR 0.000083
0.55360657
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
0.55362636
tensor(0.2207, device='cuda:0', grad_fn=<AddBackward0>)
0.55360192
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
0.55370665
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
0.55366552
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
0.55364358
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.55361819
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
0.55358452
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
0.55357111
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
0.55362219
tensor(0.1993, device='cuda:0', grad_fn=<AddBackward0>)
0.55370021
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
0.55346859
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
0.55350888
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
0.55363399
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
0.55370641
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.55354816
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
0.55350149
tensor(0.2076, device='cuda:0', grad_fn=<AddBackward0>)
0.55350709
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
0.55359548
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][  160/  196]   Loss 0.147214   Top1 95.087891   Top5 99.938965   BatchTime 0.317082   LR 0.000083
0.55332321
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
0.55330592
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.55329132
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
0.55333441
tensor(0.1774, device='cuda:0', grad_fn=<AddBackward0>)
0.55341870
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.55331415
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
0.55322534
tensor(0.2276, device='cuda:0', grad_fn=<AddBackward0>)
0.55322421
tensor(0.2028, device='cuda:0', grad_fn=<AddBackward0>)
0.55338198
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
0.55328476
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
0.55314529
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.55307347
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
0.55304486
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.55298001
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.55292135
tensor(0.1631, device='cuda:0', grad_fn=<AddBackward0>)
0.55293268
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
0.55299181
tensor(0.2190, device='cuda:0', grad_fn=<AddBackward0>)
0.55308592
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.55321223
tensor(0.1472, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [45][  180/  196]   Loss 0.148111   Top1 95.036892   Top5 99.941406   BatchTime 0.317209   LR 0.000082
0.55318308
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
0.55309647
tensor(0.2110, device='cuda:0', grad_fn=<AddBackward0>)
0.55310434
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
0.55308843
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
0.55313975
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
0.55315918
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
0.55315232
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
0.55306691
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
0.55308008
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.55315298
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.55308968
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
0.55304402
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
0.55311072
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
0.55317682
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
0.55337948
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.55320251
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
0.55310196
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 95.018    Top5: 99.940    Loss: 0.149
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.408868   Top1 88.203125   Top5 99.433594   BatchTime 0.121189
features.0.conv.0 tensor(0.3229)
features.0.conv.3 tensor(0.3301)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0985)
features.2.conv.0 tensor(0.1270)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.5784)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.1159)
features.4.conv.0 tensor(0.0646)
features.4.conv.3 tensor(0.3044)
features.4.conv.6 tensor(0.3641)
features.5.conv.0 tensor(0.3883)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.5000)
features.6.conv.0 tensor(0.0531)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0851)
features.7.conv.0 tensor(0.1710)
features.7.conv.3 tensor(0.4589)
features.7.conv.6 tensor(0.4849)
features.8.conv.0 tensor(0.6093)
features.8.conv.3 tensor(0.5434)
features.8.conv.6 tensor(0.6286)
features.9.conv.0 tensor(0.5229)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.6493)
features.10.conv.0 tensor(0.0570)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0950)
features.11.conv.0 tensor(0.7568)
features.11.conv.3 tensor(0.6375)
features.11.conv.6 tensor(0.8070)
features.12.conv.0 tensor(0.7456)
features.12.conv.3 tensor(0.6703)
features.12.conv.6 tensor(0.8317)
features.13.conv.0 tensor(0.2547)
features.13.conv.3 tensor(0.4846)
features.13.conv.6 tensor(0.3461)
features.14.conv.0 tensor(0.9170)
features.14.conv.3 tensor(0.8271)
features.14.conv.6 tensor(0.9601)
features.15.conv.0 tensor(0.8935)
features.15.conv.3 tensor(0.8359)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.6685)
features.16.conv.3 tensor(0.8036)
features.16.conv.6 tensor(0.8918)
conv.0 tensor(0.1624)
tensor(1368532.) 2188896.0
INFO - Validation [45][   40/   40]   Loss 0.396730   Top1 88.290000   Top5 99.550000   BatchTime 0.089212
INFO - ==> Top1: 88.290    Top5: 99.550    Loss: 0.397
INFO - ==> Sparsity : 0.625
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
0.55318683
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
0.55323994
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
0.55318987
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
0.55319935
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
0.55320507
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
0.55316591
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.55317825
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
0.55325347
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
0.55337536
tensor(0.2440, device='cuda:0', grad_fn=<AddBackward0>)
0.55334711
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
0.55349606
tensor(0.1976, device='cuda:0', grad_fn=<AddBackward0>)
0.55326778
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
0.55319911
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
0.55322826
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
0.55317932
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
0.55318052
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
0.55325937
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
0.55322772
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.55326974
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
0.55328929
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.55323577
tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)
0.55316752
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][   20/  196]   Loss 0.145724   Top1 95.273438   Top5 99.921875   BatchTime 0.404626   LR 0.000081
0.55312413
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
0.55309725
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.55315220
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.55317599
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
0.55321246
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
0.55325645
tensor(0.1997, device='cuda:0', grad_fn=<AddBackward0>)
0.55322027
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
0.55311185
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.55313575
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
0.55318689
tensor(0.2105, device='cuda:0', grad_fn=<AddBackward0>)
0.55313647
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.55312622
tensor(0.1887, device='cuda:0', grad_fn=<AddBackward0>)
0.55317789
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.55312657
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.55322814
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
0.55309319
tensor(0.1824, device='cuda:0', grad_fn=<AddBackward0>)
0.55309254
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.55308646
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
0.55296391
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][   40/  196]   Loss 0.146043   Top1 95.195312   Top5 99.941406   BatchTime 0.357091   LR 0.000081
0.55296707
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.55297333
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.55301523
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
0.55310518
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.55308777
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.55306917
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
0.55311030
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
0.55305058
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
0.55300224
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
0.55292720
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
0.55287987
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
0.55299592
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
0.55298090
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
0.55292344
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
0.55292344
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
0.55301893
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
0.55308914
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
0.55309653
tensor(0.2006, device='cuda:0', grad_fn=<AddBackward0>)
0.55318183
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
0.55326372
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.55308890
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.55297405
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][   60/  196]   Loss 0.140800   Top1 95.338542   Top5 99.960938   BatchTime 0.333915   LR 0.000080
0.55292779
tensor(0.1564, device='cuda:0', grad_fn=<AddBackward0>)
0.55288470
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
0.55286700
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
0.55289549
tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)
0.55288494
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
0.55281985
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
0.55290961
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
0.55303681
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
0.55309319
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
0.55321383
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
0.55305809
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.55288517
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.55279022
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
0.55269551
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
0.55264109
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
0.55263621
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
0.55268300
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.55274874
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][   80/  196]   Loss 0.142914   Top1 95.327148   Top5 99.960938   BatchTime 0.328225   LR 0.000080
0.55287236
tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)
0.55290496
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
0.55298668
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
0.55256933
tensor(0.2363, device='cuda:0', grad_fn=<AddBackward0>)
0.55254793
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
0.55252314
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.55247927
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
0.55240655
tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>)
0.55236119
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
0.55234337
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
0.55233777
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
0.55239248
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
0.55260342
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
0.55258727
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
0.55252945
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
0.55259234
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.55271721
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
0.55247802
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][  100/  196]   Loss 0.141699   Top1 95.292969   Top5 99.960938   BatchTime 0.332628   LR 0.000079
0.55248153
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
0.55254227
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
0.55239636
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.55233902
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.55245936
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
0.55262023
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
0.55237353
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
0.55231172
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
0.55241358
tensor(0.2248, device='cuda:0', grad_fn=<AddBackward0>)
0.55253786
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.55253232
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.55267787
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
0.55281711
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.55260456
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
0.55250460
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
0.55239344
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.55233920
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.55234200
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.55240005
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
0.55242985
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.55236626
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
0.55251592
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
0.55241483
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
0.55222625
tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)
0.55228972
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][  120/  196]   Loss 0.139622   Top1 95.358073   Top5 99.967448   BatchTime 0.329848   LR 0.000079
0.55228233
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
0.55212694
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
0.55211413
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
0.55214918
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
0.55218452
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
0.55228984
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
0.55239058
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
0.55244833
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
0.55236131
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
0.55239213
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
0.55247343
tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)
0.55241090
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
0.55236542
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.55239600
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
0.55237335
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
0.55242413
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
0.55245596
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.55251998
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
0.55254811
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.55250090
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
0.55247551
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][  140/  196]   Loss 0.140698   Top1 95.290179   Top5 99.966518   BatchTime 0.324955   LR 0.000078
0.55247939
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.55245185
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
0.55251932
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.55251008
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
0.55265653
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.55243659
tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)
0.55243188
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.55259323
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
0.55260974
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
0.55249554
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.55253392
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
0.55255479
tensor(0.2130, device='cuda:0', grad_fn=<AddBackward0>)
0.55243897
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
0.55243635
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][  160/  196]   Loss 0.140071   Top1 95.341797   Top5 99.965820   BatchTime 0.318647   LR 0.000078
0.55246311
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
0.55252755
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.55248564
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
0.55254495
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.55250841
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
0.55254108
tensor(0.1717, device='cuda:0', grad_fn=<AddBackward0>)
0.55248624
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.55244589
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
0.55247068
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.55250156
tensor(0.2225, device='cuda:0', grad_fn=<AddBackward0>)
0.55265868
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.55282348
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
0.55271024
tensor(0.1665, device='cuda:0', grad_fn=<AddBackward0>)
0.55269277
tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)
0.55267882
tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)
0.55268818
tensor(0.2022, device='cuda:0', grad_fn=<AddBackward0>)
0.55273753
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.55255091
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
0.55253333
tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)
0.55248755
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
0.55253839
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
0.55259269
tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)
0.55251110
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
0.55246228
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
0.55252576
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
0.55255091
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [46][  180/  196]   Loss 0.140889   Top1 95.314670   Top5 99.967448   BatchTime 0.319278   LR 0.000077
0.55253750
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
0.55267763
tensor(0.2068, device='cuda:0', grad_fn=<AddBackward0>)
0.55251193
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
0.55232733
tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)
0.55228680
tensor(0.2105, device='cuda:0', grad_fn=<AddBackward0>)
0.55227250
tensor(0.2063, device='cuda:0', grad_fn=<AddBackward0>)
0.55229825
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
0.55235791
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
0.55245918
tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)
0.55251521
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.55260754
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 95.276    Top5: 99.966    Loss: 0.142
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [46][   20/   40]   Loss 0.416447   Top1 87.812500   Top5 99.394531   BatchTime 0.125620
INFO - Validation [46][   40/   40]   Loss 0.401948   Top1 88.000000   Top5 99.560000   BatchTime 0.088371
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.3340)
features.1.conv.0 tensor(0.0638)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0916)
features.2.conv.0 tensor(0.1108)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.5799)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1187)
features.4.conv.0 tensor(0.0568)
features.4.conv.3 tensor(0.3003)
features.4.conv.6 tensor(0.3604)
features.5.conv.0 tensor(0.4079)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.5023)
features.6.conv.0 tensor(0.0534)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.1679)
features.7.conv.3 tensor(0.4589)
features.7.conv.6 tensor(0.4886)
features.8.conv.0 tensor(0.6033)
features.8.conv.3 tensor(0.5451)
features.8.conv.6 tensor(0.6335)
features.9.conv.0 tensor(0.5285)
features.9.conv.3 tensor(0.5544)
features.9.conv.6 tensor(0.6513)
features.10.conv.0 tensor(0.0564)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.0952)
features.11.conv.0 tensor(0.7535)
features.11.conv.3 tensor(0.6370)
features.11.conv.6 tensor(0.8129)
features.12.conv.0 tensor(0.7442)
features.12.conv.3 tensor(0.6707)
features.12.conv.6 tensor(0.8336)
features.13.conv.0 tensor(0.2562)
features.13.conv.3 tensor(0.4857)
features.13.conv.6 tensor(0.3503)
features.14.conv.0 tensor(0.9179)
features.14.conv.3 tensor(0.8278)
features.14.conv.6 tensor(0.9602)
features.15.conv.0 tensor(0.8965)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9660)
features.16.conv.0 tensor(0.6698)
features.16.conv.3 tensor(0.8038)
features.16.conv.6 tensor(0.8928)
conv.0 tensor(0.1657)
tensor(1371861.) 2188896.0
INFO - ==> Top1: 88.000    Top5: 99.560    Loss: 0.402
INFO - ==> Sparsity : 0.627
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
0.55255127
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.55249292
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
0.55240184
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.55244666
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
0.55234480
tensor(0.1855, device='cuda:0', grad_fn=<AddBackward0>)
0.55225283
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
0.55216861
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
0.55213523
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
0.55218995
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.55220276
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
0.55216634
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
0.55226809
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
0.55226630
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
0.55244255
tensor(0.1927, device='cuda:0', grad_fn=<AddBackward0>)
0.55220205
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
0.55210108
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
0.55208212
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.55209243
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
0.55203718
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
0.55200756
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.55204225
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
0.55212456
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][   20/  196]   Loss 0.132937   Top1 95.742188   Top5 99.960938   BatchTime 0.344114   LR 0.000077
0.55220592
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.55223733
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
0.55201322
tensor(0.2037, device='cuda:0', grad_fn=<AddBackward0>)
0.55199862
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.55198491
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
0.55197465
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
0.55200535
tensor(0.2291, device='cuda:0', grad_fn=<AddBackward0>)
0.55204147
tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)
0.55190480
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
0.55189115
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.55191636
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.55198514
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
0.55200732
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
0.55207038
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
0.55215389
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.55222684
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
0.55220973
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
0.55213410
tensor(0.1777, device='cuda:0', grad_fn=<AddBackward0>)
0.55204391
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
0.55203485
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
0.55203575
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][   40/  196]   Loss 0.142011   Top1 95.244141   Top5 99.980469   BatchTime 0.313858   LR 0.000076
0.55200464
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.55193716
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
0.55200034
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
0.55208105
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.55211598
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.55209213
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.55202866
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
0.55199176
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.55205983
tensor(0.1731, device='cuda:0', grad_fn=<AddBackward0>)
0.55205715
tensor(0.1956, device='cuda:0', grad_fn=<AddBackward0>)
0.55206901
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
0.55201441
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
0.55203629
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
0.55204093
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.55186635
tensor(0.1664, device='cuda:0', grad_fn=<AddBackward0>)
0.55183625
tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)
0.55178738
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][   60/  196]   Loss 0.140462   Top1 95.260417   Top5 99.986979   BatchTime 0.320623   LR 0.000076
0.55171072
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
0.55174965
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
0.55197090
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
0.55181414
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
0.55175805
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.55184311
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
0.55191690
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
0.55186993
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.55183256
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
0.55176353
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
0.55173737
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
0.55167729
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
0.55173010
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
0.55159241
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
0.55164784
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
0.55192459
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
0.55191714
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
0.55164963
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
0.55166727
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.55160642
tensor(0.1912, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][   80/  196]   Loss 0.137265   Top1 95.405273   Top5 99.960938   BatchTime 0.319027   LR 0.000075
0.55163497
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
0.55176872
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
0.55178511
tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)
0.55171412
tensor(0.1526, device='cuda:0', grad_fn=<AddBackward0>)
0.55162877
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
0.55150402
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
0.55145723
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
0.55141592
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
0.55139542
tensor(0.1868, device='cuda:0', grad_fn=<AddBackward0>)
0.55149955
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
0.55150008
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
0.55139995
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
0.55140293
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
0.55147898
tensor(0.1822, device='cuda:0', grad_fn=<AddBackward0>)
0.55158108
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
0.55161572
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
0.55179811
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.55181140
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
0.55176383
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
0.55173659
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][  100/  196]   Loss 0.138728   Top1 95.335938   Top5 99.953125   BatchTime 0.313848   LR 0.000075
0.55169404
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
0.55165648
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
0.55168927
tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)
0.55176127
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.55172533
tensor(0.2261, device='cuda:0', grad_fn=<AddBackward0>)
0.55175006
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
0.55166209
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.55167598
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
0.55177736
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
0.55194485
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
0.55168879
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
0.55172443
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.55173326
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
0.55181366
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
0.55184621
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
0.55194968
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.55194813
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.55185455
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.55185747
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
0.55185658
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
0.55186820
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
0.55203730
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
0.55193126
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
0.55196494
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
0.55178607
tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)
0.55175334
tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][  120/  196]   Loss 0.139852   Top1 95.292969   Top5 99.954427   BatchTime 0.315520   LR 0.000074
0.55179876
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
0.55209798
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
0.55160803
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.55153692
tensor(0.2311, device='cuda:0', grad_fn=<AddBackward0>)
0.55148876
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.55155855
tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)
0.55165267
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
0.55164808
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.55166107
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.55160576
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
0.55157411
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
0.55153561
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.55147070
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.55143207
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
0.55143279
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][  140/  196]   Loss 0.139559   Top1 95.290179   Top5 99.952567   BatchTime 0.308747   LR 0.000074
0.55141842
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
0.55143493
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
0.55150813
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
0.55148280
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
0.55157435
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.55145484
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
0.55162835
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
0.55170000
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
0.55158663
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.55161881
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
0.55152500
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
0.55146527
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
0.55170667
tensor(0.1598, device='cuda:0', grad_fn=<AddBackward0>)
0.55129129
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
0.55126965
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
0.55129129
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
0.55132824
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
0.55136937
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.55145800
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][  160/  196]   Loss 0.139820   Top1 95.290527   Top5 99.956055   BatchTime 0.309339   LR 0.000073
0.55162406
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.55171889
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
0.55165797
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
0.55162567
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
0.55148852
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.55143273
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
0.55136943
tensor(0.1404, device='cuda:0', grad_fn=<AddBackward0>)
0.55135626
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
0.55133659
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
0.55134827
tensor(0.1825, device='cuda:0', grad_fn=<AddBackward0>)
0.55135006
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
0.55136693
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
0.55138940
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
0.55158293
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
0.55145961
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
0.55127048
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.55123979
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
0.55125248
tensor(0.2004, device='cuda:0', grad_fn=<AddBackward0>)
0.55123132
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.55119836
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
0.55122936
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
0.55130082
tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>)
0.55130821
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.55133986
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [47][  180/  196]   Loss 0.139893   Top1 95.269097   Top5 99.950087   BatchTime 0.312261   LR 0.000073
0.55140644
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
0.55140752
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
0.55141342
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
0.55141640
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
0.55144453
tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)
0.55135471
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
0.55134875
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.55137116
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
0.55133778
tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)
0.55132455
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
0.55148149
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
0.55166471
tensor(0.2454, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 95.254    Top5: 99.950    Loss: 0.140
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [47][   20/   40]   Loss 0.400892   Top1 88.691406   Top5 99.472656   BatchTime 0.117784
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.3340)
features.1.conv.0 tensor(0.0573)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1039)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5773)
features.3.conv.0 tensor(0.0833)
features.3.conv.3 tensor(0.0710)
features.3.conv.6 tensor(0.1213)
features.4.conv.0 tensor(0.0682)
features.4.conv.3 tensor(0.3044)
features.4.conv.6 tensor(0.3610)
features.5.conv.0 tensor(0.4211)
features.5.conv.3 tensor(0.4167)
features.5.conv.6 tensor(0.5031)
features.6.conv.0 tensor(0.0506)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0868)
features.7.conv.0 tensor(0.1690)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.4920)
features.8.conv.0 tensor(0.5989)
features.8.conv.3 tensor(0.5469)
features.8.conv.6 tensor(0.6370)
features.9.conv.0 tensor(0.5407)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6516)
features.10.conv.0 tensor(0.0579)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0939)
features.11.conv.0 tensor(0.7514)
features.11.conv.3 tensor(0.6341)
features.11.conv.6 tensor(0.8150)
features.12.conv.0 tensor(0.7459)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.8347)
features.13.conv.0 tensor(0.2615)
features.13.conv.3 tensor(0.4850)
features.13.conv.6 tensor(0.3541)
features.14.conv.0 tensor(0.9189)
features.14.conv.3 tensor(0.8279)
features.14.conv.6 tensor(0.9607)
features.15.conv.0 tensor(0.8975)
features.15.conv.3 tensor(0.8368)
features.15.conv.6 tensor(0.9662)
features.16.conv.0 tensor(0.6704)
features.16.conv.3 tensor(0.8036)
features.16.conv.6 tensor(0.8946)
conv.0 tensor(0.1699)
tensor(1375989.) 2188896.0
INFO - Validation [47][   40/   40]   Loss 0.382679   Top1 88.690000   Top5 99.660000   BatchTime 0.087051
INFO - ==> Top1: 88.690    Top5: 99.660    Loss: 0.383
INFO - ==> Sparsity : 0.629
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.790   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
0.55154628
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
0.55156183
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
0.55144930
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.55153722
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
0.55112135
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
0.55110472
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.55110747
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
0.55108011
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.55107170
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
0.55114675
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
0.55117542
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
0.55116713
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.55114430
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
0.55114657
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
0.55129981
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.55119187
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
0.55107802
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
0.55101776
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
0.55112845
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.55096918
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
0.55103785
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.55134422
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
0.55130059
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
0.55115497
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
0.55106527
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][   20/  196]   Loss 0.118078   Top1 95.957031   Top5 99.960938   BatchTime 0.366197   LR 0.000072
0.55098414
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
0.55091822
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.55088693
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.55099684
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.55080163
tensor(0.2466, device='cuda:0', grad_fn=<AddBackward0>)
0.55070484
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
0.55070120
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.55070150
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
0.55074704
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
0.55081606
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.55106997
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
0.55124664
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
0.55103582
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.55085295
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.55079705
tensor(0.1866, device='cuda:0', grad_fn=<AddBackward0>)
0.55083233
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
0.55113524
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.55064440
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
0.55053866
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
0.55053025
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
0.55060059
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][   40/  196]   Loss 0.122947   Top1 95.859375   Top5 99.980469   BatchTime 0.320211   LR 0.000071
0.55074781
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.55064267
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
0.55055076
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
0.55049270
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
0.55044770
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
0.55044025
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
0.55045992
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
0.55065042
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
0.55050242
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
0.55058014
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
0.55086517
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
0.55147755
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.55090082
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
0.55076712
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
0.55072165
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.55069709
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][   60/  196]   Loss 0.126623   Top1 95.748698   Top5 99.967448   BatchTime 0.300951   LR 0.000071
0.55067784
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.55066818
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
0.55068439
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
0.55075914
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
0.55091524
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
0.55086654
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.55078149
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
0.55065376
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
0.55062973
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
0.55082995
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
0.55077976
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
0.55089515
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
0.55099767
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
0.55094826
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
0.55099750
tensor(0.1776, device='cuda:0', grad_fn=<AddBackward0>)
0.55118996
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
0.55086476
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
0.55081314
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
0.55084646
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][   80/  196]   Loss 0.128945   Top1 95.678711   Top5 99.951172   BatchTime 0.304314   LR 0.000070
0.55099827
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
0.55120957
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.55103815
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.55098557
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.55106944
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
0.55114627
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
0.55124867
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
0.55120254
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
0.55104434
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.55094552
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
0.55096602
tensor(0.2141, device='cuda:0', grad_fn=<AddBackward0>)
0.55088395
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
0.55086815
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
0.55080491
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
0.55079144
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
0.55080110
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
0.55088156
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.55096412
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
0.55095708
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.55082023
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
0.55090457
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
0.55091280
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
0.55079049
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][  100/  196]   Loss 0.130110   Top1 95.625000   Top5 99.960938   BatchTime 0.311380   LR 0.000070
0.55072832
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
0.55079484
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
0.55090356
tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)
0.55085999
tensor(0.1958, device='cuda:0', grad_fn=<AddBackward0>)
0.55097389
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
0.55082428
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.55064684
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.55060709
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.55051714
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
0.55049258
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
0.55051631
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.55051011
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
0.55047542
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
0.55048889
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
0.55050510
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.55054969
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
0.55064189
tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)
0.55075026
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][  120/  196]   Loss 0.130728   Top1 95.589193   Top5 99.967448   BatchTime 0.314556   LR 0.000069
0.55052304
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.55047274
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
0.55041331
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
0.55030990
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
0.55023092
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.55018741
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
0.55016458
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
0.55016208
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.55018574
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.55028009
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
0.55049187
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.55034512
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.55044556
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
0.55034691
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
0.55049568
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.55054933
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.55038553
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
0.55038577
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
0.55042619
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.55046195
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
0.55041176
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][  140/  196]   Loss 0.130611   Top1 95.622210   Top5 99.963728   BatchTime 0.312912   LR 0.000069
0.55039352
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
0.55038971
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
0.55050886
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
0.55054730
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.55048651
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
0.55045015
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
0.55037981
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.55032665
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.55032980
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
0.55034548
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
0.55040985
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
0.55043083
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
0.55034888
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
0.55045080
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
0.55050296
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.55061036
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
0.55030507
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
0.55030787
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
0.55038184
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][  160/  196]   Loss 0.130789   Top1 95.664062   Top5 99.960938   BatchTime 0.312340   LR 0.000068
0.55047745
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
0.55046016
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
0.55042297
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.55044127
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
0.55062181
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
0.55047655
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
0.55045980
tensor(0.2006, device='cuda:0', grad_fn=<AddBackward0>)
0.55050576
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
0.55048043
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.55047184
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
0.55043441
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.55055982
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
0.55060285
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
0.55046505
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.55063587
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
0.55052078
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
0.55057192
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
0.55049896
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.55058676
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
0.55064136
tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)
0.55063486
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [48][  180/  196]   Loss 0.131142   Top1 95.642361   Top5 99.958767   BatchTime 0.308970   LR 0.000068
0.55068523
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
0.55073369
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
0.55073410
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
0.55061626
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
0.55059105
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
0.55053353
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.55049914
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
0.55061662
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
0.55064952
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
0.55066031
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
0.55059135
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
0.55053848
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
0.55049217
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 95.592    Top5: 99.958    Loss: 0.133
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.3320)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0990)
features.2.conv.0 tensor(0.0998)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.5787)
features.3.conv.0 tensor(0.0842)
features.3.conv.3 tensor(0.0710)
features.3.conv.6 tensor(0.1137)
features.4.conv.0 tensor(0.0701)
features.4.conv.3 tensor(0.3032)
features.4.conv.6 tensor(0.3621)
features.5.conv.0 tensor(0.4067)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.5055)
features.6.conv.0 tensor(0.0488)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0854)
features.7.conv.0 tensor(0.1763)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.4864)
features.8.conv.0 tensor(0.6104)
features.8.conv.3 tensor(0.5446)
features.8.conv.6 tensor(0.6368)
features.9.conv.0 tensor(0.5418)
features.9.conv.3 tensor(0.5544)
features.9.conv.6 tensor(0.6523)
features.10.conv.0 tensor(0.0571)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0943)
features.11.conv.0 tensor(0.7503)
features.11.conv.3 tensor(0.6350)
features.11.conv.6 tensor(0.8155)
features.12.conv.0 tensor(0.7421)
features.12.conv.3 tensor(0.6698)
features.12.conv.6 tensor(0.8359)
features.13.conv.0 tensor(0.2632)
features.13.conv.3 tensor(0.4830)
features.13.conv.6 tensor(0.3602)
features.14.conv.0 tensor(0.9189)
features.14.conv.3 tensor(0.8263)
features.14.conv.6 tensor(0.9611)
features.15.conv.0 tensor(0.8985)
features.15.conv.3 tensor(0.8368)
features.15.conv.6 tensor(0.9666)
features.16.conv.0 tensor(0.6738)
features.16.conv.3 tensor(0.8041)
features.16.conv.6 tensor(0.8958)
conv.0 tensor(0.1726)
tensor(1378928.) 2188896.0
INFO - Validation [48][   20/   40]   Loss 0.405965   Top1 88.593750   Top5 99.511719   BatchTime 0.123230
INFO - Validation [48][   40/   40]   Loss 0.386989   Top1 88.960000   Top5 99.620000   BatchTime 0.088001
INFO - ==> Top1: 88.960    Top5: 99.620    Loss: 0.387
INFO - ==> Sparsity : 0.630
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
0.55052131
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
0.55052781
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
0.55049157
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
0.55054051
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
0.55054110
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.55043358
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
0.55039364
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
0.55042386
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
0.55043751
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.55044019
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
0.55041456
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
0.55048901
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.55043125
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
0.55037051
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.55039912
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.55043232
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
0.55051684
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
0.55081242
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
0.55068099
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
0.55068272
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
0.55073029
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
0.55074686
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
0.55073220
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][   20/  196]   Loss 0.139352   Top1 95.390625   Top5 99.980469   BatchTime 0.331926   LR 0.000067
0.55062729
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
0.55058724
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.55064237
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
0.55064428
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
0.55073911
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
0.55061316
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
0.55053365
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
0.55050403
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
0.55049741
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
0.55049974
tensor(0.1588, device='cuda:0', grad_fn=<AddBackward0>)
0.55048800
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.55053556
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
0.55074209
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.55085278
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
0.55067784
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.55062962
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
0.55059773
tensor(0.2036, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][   40/  196]   Loss 0.133553   Top1 95.683594   Top5 99.970703   BatchTime 0.336552   LR 0.000066
0.55049866
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.55038583
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
0.55036348
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
0.55038494
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
0.55043948
tensor(0.1801, device='cuda:0', grad_fn=<AddBackward0>)
0.55042541
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
0.55054915
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
0.55047482
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.55044544
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.55029422
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
0.55022985
tensor(0.1865, device='cuda:0', grad_fn=<AddBackward0>)
0.55016226
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.55012459
tensor(0.2112, device='cuda:0', grad_fn=<AddBackward0>)
0.55013126
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
0.55022609
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.55029827
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.55049592
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
0.55020171
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.55022734
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.55015212
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
0.55010176
tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)
0.55005300
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
0.54999852
tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)
0.54994351
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][   60/  196]   Loss 0.135404   Top1 95.501302   Top5 99.980469   BatchTime 0.338986   LR 0.000066
0.54994065
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.54992968
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
0.54990917
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
0.54994720
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
0.54998636
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
0.54999912
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
0.54993033
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
0.54986203
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.54980505
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
0.54979450
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
0.54981387
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.54985946
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
0.54993433
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
0.54990512
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
0.54989362
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
0.54977709
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.54974872
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
0.54979557
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.54974157
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][   80/  196]   Loss 0.134522   Top1 95.483398   Top5 99.960938   BatchTime 0.332835   LR 0.000065
0.54977089
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
0.54988372
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
0.54986751
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
0.54979748
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
0.54988509
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
0.54981297
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.54977566
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
0.54983419
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
0.54993892
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.54977113
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
0.54970980
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
0.54969138
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
0.54971486
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
0.54983056
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
0.54984093
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
0.54953933
tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)
0.54948831
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
0.54951847
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][  100/  196]   Loss 0.132097   Top1 95.585938   Top5 99.968750   BatchTime 0.333491   LR 0.000065
0.54949868
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
0.54948705
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.54954201
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.54977316
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
0.54956871
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
0.54949600
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
0.54954773
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
0.54964608
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.54959702
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
0.54953629
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.54950303
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.54943353
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
0.54943913
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.54932898
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
0.54936284
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.54943240
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
0.54933506
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
0.54932505
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.54935616
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
0.54936916
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
0.54950207
tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][  120/  196]   Loss 0.129008   Top1 95.712891   Top5 99.970703   BatchTime 0.322996   LR 0.000064
0.54953802
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.54958546
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
0.54957217
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
0.54942060
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.54953927
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
0.54975176
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
0.54975140
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
0.54982352
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
0.54962206
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
0.54954463
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
0.54948473
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
0.54949492
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
0.54947740
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
0.54941398
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.54950440
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
0.54953653
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
0.54952186
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
0.54941535
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
0.54931635
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.54927647
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.54914051
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.54903942
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][  140/  196]   Loss 0.130479   Top1 95.661272   Top5 99.966518   BatchTime 0.319172   LR 0.000064
0.54898685
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
0.54900879
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
0.54895043
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
0.54893589
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
0.54896033
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
0.54909527
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
0.54908651
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.54911286
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
0.54901022
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.54888046
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
0.54892540
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
0.54909092
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
0.54886121
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
0.54871202
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.54869318
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
0.54870784
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
0.54878390
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
0.54876393
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
0.54872042
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
0.54861313
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][  160/  196]   Loss 0.129496   Top1 95.661621   Top5 99.968262   BatchTime 0.313625   LR 0.000063
0.54853529
tensor(0.1564, device='cuda:0', grad_fn=<AddBackward0>)
0.54850680
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
0.54851192
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.54856002
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
0.54867935
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
0.54884654
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
0.54875028
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
0.54892045
tensor(0.2004, device='cuda:0', grad_fn=<AddBackward0>)
0.54895926
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
0.54907036
tensor(0.2263, device='cuda:0', grad_fn=<AddBackward0>)
0.54900116
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.54894894
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.54890144
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.54880536
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
0.54877734
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
0.54870635
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
0.54869139
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.54876751
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
0.54897392
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [49][  180/  196]   Loss 0.130176   Top1 95.625000   Top5 99.969618   BatchTime 0.315477   LR 0.000063
0.54881221
tensor(0.1668, device='cuda:0', grad_fn=<AddBackward0>)
0.54906964
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.54907566
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
0.54902065
tensor(0.1586, device='cuda:0', grad_fn=<AddBackward0>)
0.54894912
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
0.54888308
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
0.54888874
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
0.54876208
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
0.54873914
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
0.54875958
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
0.54887456
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.54903316
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
0.54887581
tensor(0.2761, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 95.608    Top5: 99.970    Loss: 0.130
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0618)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0959)
features.2.conv.0 tensor(0.1024)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5790)
features.3.conv.0 tensor(0.0839)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.1120)
features.4.conv.0 tensor(0.0710)
features.4.conv.3 tensor(0.2998)
features.4.conv.6 tensor(0.3652)
features.5.conv.0 tensor(0.3923)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.5085)
features.6.conv.0 tensor(0.0492)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0864)
features.7.conv.0 tensor(0.1761)
features.7.conv.3 tensor(0.4563)
features.7.conv.6 tensor(0.4936)
features.8.conv.0 tensor(0.6006)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.6370)
features.9.conv.0 tensor(0.5500)
features.9.conv.3 tensor(0.5558)
features.9.conv.6 tensor(0.6549)
features.10.conv.0 tensor(0.0549)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0937)
features.11.conv.0 tensor(0.7501)
features.11.conv.3 tensor(0.6354)
features.11.conv.6 tensor(0.8175)
features.12.conv.0 tensor(0.7441)
features.12.conv.3 tensor(0.6705)
features.12.conv.6 tensor(0.8390)
features.13.conv.0 tensor(0.2685)
features.13.conv.3 tensor(0.4828)
features.13.conv.6 tensor(0.3776)
features.14.conv.0 tensor(0.9206)
features.14.conv.3 tensor(0.8281)
features.14.conv.6 tensor(0.9610)
features.15.conv.0 tensor(0.8998)
features.15.conv.3 tensor(0.8369)
features.15.conv.6 tensor(0.9665)
features.16.conv.0 tensor(0.6752)
features.16.conv.3 tensor(0.8039)
features.16.conv.6 tensor(0.8969)
conv.0 tensor(0.1796)
tensor(1385235.) 2188896.0
INFO - Validation [49][   20/   40]   Loss 0.392050   Top1 88.593750   Top5 99.589844   BatchTime 0.113516
INFO - Validation [49][   40/   40]   Loss 0.381337   Top1 88.710000   Top5 99.670000   BatchTime 0.082393
INFO - ==> Top1: 88.710    Top5: 99.670    Loss: 0.381
INFO - ==> Sparsity : 0.633
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
0.54880387
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
0.54874420
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
0.54869908
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
0.54867983
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
0.54868102
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
0.54865617
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.54865700
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
0.54874784
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
0.54881048
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
0.54909742
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
0.54923743
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.54907858
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.54916149
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
0.54920530
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
0.54913765
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
0.54903811
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
0.54906332
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
0.54899335
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.54897398
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.54890245
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.54892033
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
0.54900879
tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>)
0.54901594
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
0.54898953
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][   20/  196]   Loss 0.118484   Top1 96.250000   Top5 99.941406   BatchTime 0.351344   LR 0.000062
0.54906857
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.54900366
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.54898590
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
0.54894292
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.54902035
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
0.54914063
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
0.54911190
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
0.54901099
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
0.54895031
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
0.54887664
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
0.54887635
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
0.54877549
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
0.54870689
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.54866725
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.54869592
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
0.54883617
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
0.54899776
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
0.54892552
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
0.54890275
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][   40/  196]   Loss 0.124635   Top1 95.957031   Top5 99.941406   BatchTime 0.328704   LR 0.000062
0.54891700
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.54894352
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
0.54908931
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
0.54916221
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
0.54925448
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
0.54934788
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
0.54928058
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
0.54907852
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.54889590
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
0.54886240
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
0.54880983
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.54873228
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
0.54868770
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
0.54866207
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
0.54865658
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
0.54873759
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
0.54882634
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
0.54889512
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
0.54882848
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.54890609
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][   60/  196]   Loss 0.121789   Top1 95.944010   Top5 99.960938   BatchTime 0.316548   LR 0.000061
0.54901922
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
0.54918963
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.54907823
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
0.54896939
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
0.54892176
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
0.54899615
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
0.54897588
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
0.54892516
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
0.54898208
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
0.54896647
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
0.54903930
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.54900283
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
0.54898781
tensor(0.1404, device='cuda:0', grad_fn=<AddBackward0>)
0.54892385
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
0.54897487
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
0.54900330
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.54892349
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.54885876
tensor(0.1656, device='cuda:0', grad_fn=<AddBackward0>)
0.54889232
tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)
0.54874545
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
0.54867750
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
0.54867512
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][   80/  196]   Loss 0.121580   Top1 96.000977   Top5 99.960938   BatchTime 0.308072   LR 0.000061
0.54863471
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.54873747
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.54875356
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.54868287
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
0.54866725
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.54864293
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
0.54879695
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.54886103
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
0.54872817
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
0.54864192
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
0.54875129
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
0.54871035
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
0.54857498
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
0.54856420
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
0.54852957
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
0.54855138
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
0.54840463
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][  100/  196]   Loss 0.122054   Top1 95.984375   Top5 99.964844   BatchTime 0.313134   LR 0.000060
0.54830593
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
0.54824191
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
0.54828107
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.54833913
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
0.54840940
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.54838616
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
0.54831874
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
0.54830158
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.54823703
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
0.54815763
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
0.54814029
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
0.54817736
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
0.54808068
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
0.54804665
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.54811245
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
0.54813725
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
0.54822856
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.54826438
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
0.54827219
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][  120/  196]   Loss 0.123920   Top1 95.911458   Top5 99.967448   BatchTime 0.315638   LR 0.000060
0.54816347
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.54812467
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
0.54810959
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
0.54812545
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
0.54817510
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.54812723
tensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)
0.54813451
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
0.54810548
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
0.54814923
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
0.54802799
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
0.54800338
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
0.54810178
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
0.54803920
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
0.54794395
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.54802614
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
0.54801649
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
0.54799616
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
0.54799324
tensor(0.2501, device='cuda:0', grad_fn=<AddBackward0>)
0.54811072
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
0.54810345
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.54808486
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
0.54815084
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.54809111
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
0.54804850
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][  140/  196]   Loss 0.123891   Top1 95.895647   Top5 99.966518   BatchTime 0.319140   LR 0.000059
0.54799789
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
0.54806048
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
0.54813761
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
0.54816896
tensor(0.1579, device='cuda:0', grad_fn=<AddBackward0>)
0.54825950
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
0.54802358
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
0.54786706
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
0.54781377
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.54779816
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.54779267
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.54778987
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.54782706
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
0.54770970
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
0.54771191
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
0.54770237
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
0.54766876
tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>)
0.54766691
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.54771954
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.54785311
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][  160/  196]   Loss 0.122412   Top1 95.937500   Top5 99.965820   BatchTime 0.318516   LR 0.000059
0.54786968
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
0.54788649
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.54786938
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
0.54783857
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
0.54795223
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
0.54781377
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
0.54773426
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
0.54773486
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
0.54776329
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
0.54785168
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
0.54790103
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
0.54795432
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.54790610
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
0.54792237
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
0.54792911
tensor(0.1693, device='cuda:0', grad_fn=<AddBackward0>)
0.54784733
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.54776460
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
0.54770875
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
0.54769254
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
0.54772633
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [50][  180/  196]   Loss 0.122840   Top1 95.928819   Top5 99.963108   BatchTime 0.316875   LR 0.000058
0.54774755
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
0.54772151
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
0.54764807
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
0.54760653
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.54752618
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.54751045
tensor(0.2215, device='cuda:0', grad_fn=<AddBackward0>)
0.54754013
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.54758418
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
0.54764515
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
0.54771280
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.54768848
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.54775554
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 95.930    Top5: 99.964    Loss: 0.123
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [50][   20/   40]   Loss 0.403844   Top1 88.613281   Top5 99.531250   BatchTime 0.119390
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.3438)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0929)
features.2.conv.0 tensor(0.1068)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.5807)
features.3.conv.0 tensor(0.0822)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1100)
features.4.conv.0 tensor(0.0706)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.3696)
features.5.conv.0 tensor(0.3783)
features.5.conv.3 tensor(0.4132)
features.5.conv.6 tensor(0.5090)
features.6.conv.0 tensor(0.0521)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0862)
features.7.conv.0 tensor(0.1701)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.5000)
features.8.conv.0 tensor(0.5916)
features.8.conv.3 tensor(0.5446)
features.8.conv.6 tensor(0.6417)
features.9.conv.0 tensor(0.5437)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6588)
features.10.conv.0 tensor(0.0550)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0945)
features.11.conv.0 tensor(0.7547)
features.11.conv.3 tensor(0.6345)
features.11.conv.6 tensor(0.8192)
features.12.conv.0 tensor(0.7451)
features.12.conv.3 tensor(0.6703)
features.12.conv.6 tensor(0.8415)
features.13.conv.0 tensor(0.2668)
features.13.conv.3 tensor(0.4838)
features.13.conv.6 tensor(0.3940)
features.14.conv.0 tensor(0.9205)
features.14.conv.3 tensor(0.8282)
features.14.conv.6 tensor(0.9610)
features.15.conv.0 tensor(0.9008)
features.15.conv.3 tensor(0.8372)
features.15.conv.6 tensor(0.9668)
features.16.conv.0 tensor(0.6778)
features.16.conv.3 tensor(0.8022)
features.16.conv.6 tensor(0.8984)
conv.0 tensor(0.1813)
tensor(1388733.) 2188896.0
INFO - Validation [50][   40/   40]   Loss 0.390018   Top1 88.710000   Top5 99.670000   BatchTime 0.088204
INFO - ==> Top1: 88.710    Top5: 99.670    Loss: 0.390
INFO - ==> Sparsity : 0.634
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 88.820   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
0.54788017
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
0.54792666
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
0.54780233
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
0.54770744
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.54771143
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.54766834
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
0.54759872
tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)
0.54760396
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
0.54756981
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.54756391
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
0.54767925
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
0.54786301
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
0.54785335
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
0.54782629
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
0.54788625
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
0.54774630
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
0.54771495
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
0.54775053
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
0.54767907
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
0.54766774
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.54781687
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
0.54784191
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
0.54784298
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
0.54779285
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.54775566
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][   20/  196]   Loss 0.113967   Top1 95.878906   Top5 99.960938   BatchTime 0.368916   LR 0.000057
0.54769439
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
0.54767632
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
0.54768145
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
0.54763550
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.54770797
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
0.54768729
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.54763877
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
0.54767621
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
0.54776442
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
0.54780602
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.54784149
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
0.54783726
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
0.54772210
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
0.54763645
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
0.54764122
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
0.54770321
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.54774660
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
0.54778248
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
0.54775542
tensor(0.1644, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][   40/  196]   Loss 0.110065   Top1 96.259766   Top5 99.980469   BatchTime 0.346139   LR 0.000057
0.54773456
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
0.54772252
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
0.54769009
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.54768229
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
0.54767233
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
0.54766852
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
0.54777706
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
0.54760098
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
0.54760087
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.54757768
tensor(0.1836, device='cuda:0', grad_fn=<AddBackward0>)
0.54752398
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
0.54750913
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
0.54748458
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.54753387
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
0.54753095
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
0.54755145
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
0.54767567
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.54757887
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][   60/  196]   Loss 0.113826   Top1 96.080729   Top5 99.980469   BatchTime 0.342176   LR 0.000056
0.54754281
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
0.54751343
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.54755074
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
0.54756123
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
0.54757863
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
0.54761022
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
0.54760164
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
0.54765791
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
0.54778510
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.54781026
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
0.54780656
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.54774040
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.54766947
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
0.54760635
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
0.54753906
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
0.54754275
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
0.54761785
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.54769695
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.54758430
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
0.54752511
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.54748613
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][   80/  196]   Loss 0.114981   Top1 96.098633   Top5 99.956055   BatchTime 0.328749   LR 0.000056
0.54744023
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.54743981
tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)
0.54743922
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
0.54744065
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
0.54749221
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.54753143
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
0.54759568
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
0.54762608
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
0.54757333
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
0.54740238
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
0.54735124
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
0.54734159
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
0.54734468
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.54742157
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
0.54733074
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.54730874
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
0.54725331
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
0.54728651
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
0.54736173
tensor(0.1801, device='cuda:0', grad_fn=<AddBackward0>)
0.54746175
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
0.54750085
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][  100/  196]   Loss 0.115595   Top1 96.042969   Top5 99.957031   BatchTime 0.318215   LR 0.000055
0.54757953
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
0.54751992
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.54767060
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
0.54749227
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.54745090
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
0.54745239
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
0.54739809
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
0.54731125
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
0.54720050
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
0.54711181
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
0.54716021
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
0.54726523
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
0.54736084
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
0.54731244
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
0.54726624
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.54720539
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
0.54712874
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
0.54709458
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.54704791
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.54702622
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
0.54699099
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][  120/  196]   Loss 0.117547   Top1 95.986328   Top5 99.957682   BatchTime 0.313035   LR 0.000055
0.54698426
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
0.54702115
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
0.54696029
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.54696596
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.54700202
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
0.54709649
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
0.54705846
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
0.54707867
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
0.54711884
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
0.54717499
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
0.54720390
tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)
0.54724413
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
0.54718679
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.54715747
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][  140/  196]   Loss 0.117964   Top1 95.987723   Top5 99.960938   BatchTime 0.311116   LR 0.000054
0.54713213
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
0.54704106
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
0.54693180
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
0.54686022
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.54686707
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
0.54690027
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
0.54691941
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
0.54689205
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
0.54697090
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.54711396
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
0.54702175
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
0.54695201
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.54691565
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
0.54688019
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
0.54682344
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
0.54674375
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
0.54678297
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.54675347
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
0.54681349
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][  160/  196]   Loss 0.118929   Top1 95.957031   Top5 99.965820   BatchTime 0.310539   LR 0.000054
0.54687446
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
0.54678738
tensor(0.2146, device='cuda:0', grad_fn=<AddBackward0>)
0.54670221
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.54667830
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
0.54663682
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.54654920
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
0.54645938
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.54640502
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
0.54641402
tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)
0.54644912
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
0.54649705
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.54656529
tensor(0.1423, device='cuda:0', grad_fn=<AddBackward0>)
0.54664093
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.54671931
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
0.54684651
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.54685187
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
0.54680842
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
0.54672295
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
0.54666924
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [51][  180/  196]   Loss 0.119016   Top1 95.948351   Top5 99.969618   BatchTime 0.312377   LR 0.000053
0.54656166
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
0.54649359
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
0.54645443
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
0.54646939
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
0.54651129
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
0.54658800
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
0.54660755
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.54660594
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.54665035
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
0.54663408
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
0.54649228
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
0.54651350
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
0.54657489
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
0.54667825
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
0.54676753
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
0.54668784
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.54659992
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
0.54664570
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 95.944    Top5: 99.970    Loss: 0.119
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.54660851
tensor(0.1488, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [51][   20/   40]   Loss 0.394117   Top1 88.847656   Top5 99.511719   BatchTime 0.117898
INFO - Validation [51][   40/   40]   Loss 0.386649   Top1 89.070000   Top5 99.640000   BatchTime 0.086524
features.0.conv.0 tensor(0.3125)
features.0.conv.3 tensor(0.3438)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0920)
features.2.conv.0 tensor(0.1111)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5802)
features.3.conv.0 tensor(0.0833)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1081)
features.4.conv.0 tensor(0.0825)
features.4.conv.3 tensor(0.2980)
features.4.conv.6 tensor(0.3654)
features.5.conv.0 tensor(0.3890)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.5086)
features.6.conv.0 tensor(0.0537)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.1713)
features.7.conv.3 tensor(0.4583)
features.7.conv.6 tensor(0.5011)
features.8.conv.0 tensor(0.6084)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.6427)
features.9.conv.0 tensor(0.5422)
features.9.conv.3 tensor(0.5550)
features.9.conv.6 tensor(0.6631)
features.10.conv.0 tensor(0.0559)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0945)
features.11.conv.0 tensor(0.7604)
features.11.conv.3 tensor(0.6352)
features.11.conv.6 tensor(0.8204)
features.12.conv.0 tensor(0.7531)
features.12.conv.3 tensor(0.6701)
features.12.conv.6 tensor(0.8416)
features.13.conv.0 tensor(0.2739)
features.13.conv.3 tensor(0.4836)
features.13.conv.6 tensor(0.4104)
features.14.conv.0 tensor(0.9218)
features.14.conv.3 tensor(0.8287)
features.14.conv.6 tensor(0.9614)
features.15.conv.0 tensor(0.9019)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.6821)
features.16.conv.3 tensor(0.8021)
INFO - ==> Top1: 89.070    Top5: 99.640    Loss: 0.387
INFO - ==> Sparsity : 0.637
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
features.16.conv.6 tensor(0.8996)
conv.0 tensor(0.1838)
tensor(1394657.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
0.54647201
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
0.54638261
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
0.54632723
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
0.54633784
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
0.54632795
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
0.54640299
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.54651296
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
0.54659933
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
0.54657018
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
0.54665238
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
0.54663503
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.54658872
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.54645306
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.54639721
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
0.54643315
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][   20/  196]   Loss 0.108758   Top1 96.464844   Top5 100.000000   BatchTime 0.326779   LR 0.000052
0.54652488
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
0.54664093
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
0.54659104
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.54650980
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
0.54662335
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.54646683
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
0.54635155
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
0.54621542
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
0.54613793
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
0.54610682
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
0.54609656
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
0.54607785
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
0.54614675
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
0.54609340
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
0.54613876
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.54617518
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
0.54624778
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
0.54623741
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
0.54623944
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
0.54633766
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
0.54630280
tensor(0.1938, device='cuda:0', grad_fn=<AddBackward0>)
0.54639649
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
0.54637182
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][   40/  196]   Loss 0.119290   Top1 95.966797   Top5 99.990234   BatchTime 0.293020   LR 0.000052
0.54622144
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.54612148
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
0.54617709
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
0.54615289
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
0.54605824
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
0.54605699
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
0.54609722
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.54624575
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
0.54629987
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
0.54619038
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
0.54616916
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.54617620
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
0.54608613
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
0.54609561
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
0.54605597
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
0.54607147
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.54612869
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
0.54608399
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
0.54615659
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
0.54608500
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
0.54610664
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][   60/  196]   Loss 0.117626   Top1 96.035156   Top5 99.986979   BatchTime 0.295077   LR 0.000051
0.54610366
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
0.54613549
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
0.54612082
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.54626429
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
0.54624975
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.54626822
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
0.54614246
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
0.54605979
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
0.54601794
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
0.54597336
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
0.54599124
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
0.54600340
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
0.54594654
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
0.54600728
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
0.54588962
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.54589295
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
0.54595655
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
0.54593581
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
0.54594189
tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][   80/  196]   Loss 0.116130   Top1 96.069336   Top5 99.975586   BatchTime 0.297268   LR 0.000051
0.54603910
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
0.54612488
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.54618329
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
0.54614258
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.54611707
tensor(0.1783, device='cuda:0', grad_fn=<AddBackward0>)
0.54603463
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
0.54596096
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
0.54593986
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
0.54594570
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.54598290
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
0.54600835
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
0.54602075
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
0.54601336
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.54605252
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
0.54600519
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
0.54588878
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
0.54581779
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
0.54576528
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
0.54582983
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.54586488
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
0.54592282
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][  100/  196]   Loss 0.116350   Top1 96.015625   Top5 99.964844   BatchTime 0.295325   LR 0.000050
0.54590583
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
0.54588038
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
0.54597771
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
0.54601061
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.54590762
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
0.54581797
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
0.54574293
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
0.54568863
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
0.54565400
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
0.54570037
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
0.54569459
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
0.54568046
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
0.54563087
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.54564655
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
0.54575646
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
0.54579479
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
0.54583126
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
0.54585612
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.54588664
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.54593772
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
0.54593623
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][  120/  196]   Loss 0.117351   Top1 96.009115   Top5 99.967448   BatchTime 0.293689   LR 0.000050
0.54579890
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
0.54578882
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
0.54585242
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.54579586
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
0.54574764
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.54565096
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
0.54563487
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
0.54566085
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
0.54571325
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
0.54562342
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
0.54553753
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
0.54554963
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.54558945
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
0.54553580
tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)
0.54549444
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
0.54535151
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
0.54520327
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.54509795
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
0.54498976
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
0.54475588
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][  140/  196]   Loss 0.117242   Top1 96.035156   Top5 99.969308   BatchTime 0.295251   LR 0.000049
0.54441428
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
0.54438120
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.54455614
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
0.54450870
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.54441732
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.54425490
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
0.54421514
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.54413277
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
0.54410875
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
0.54413843
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
0.54413027
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
0.54416311
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
0.54414010
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.54408431
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
0.54408139
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
0.54403412
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
0.54398662
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.54395652
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][  160/  196]   Loss 0.115012   Top1 96.127930   Top5 99.973145   BatchTime 0.298792   LR 0.000049
0.54394430
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.54399133
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
0.54402941
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
0.54401177
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
0.54397464
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
0.54401702
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
0.54395366
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
0.54391199
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
0.54392725
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
0.54394835
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
0.54393536
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.54393166
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
0.54396307
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
0.54407847
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
0.54410481
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.54415387
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
0.54420465
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
0.54415286
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
0.54414070
tensor(0.1472, device='cuda:0', grad_fn=<AddBackward0>)
0.54406404
tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [52][  180/  196]   Loss 0.115766   Top1 96.117622   Top5 99.976128   BatchTime 0.300385   LR 0.000048
0.54402196
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
0.54389530
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
0.54383332
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.54382461
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.54380864
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
0.54382837
tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)
0.54386741
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
0.54391134
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
0.54385108
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
0.54381233
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
0.54370791
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
0.54366916
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
0.54361010
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
0.54357415
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.54355758
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
0.54355502
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
0.54353678
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
0.54354668
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.126    Top5: 99.978    Loss: 0.116
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [52][   20/   40]   Loss 0.408992   Top1 88.730469   Top5 99.492188   BatchTime 0.135635
INFO - Validation [52][   40/   40]   Loss 0.391317   Top1 88.840000   Top5 99.610000   BatchTime 0.095785
INFO - ==> Top1: 88.840    Top5: 99.610    Loss: 0.391
INFO - ==> Sparsity : 0.639
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0579)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.1160)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.5848)
features.3.conv.0 tensor(0.0825)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.1120)
features.4.conv.0 tensor(0.0781)
features.4.conv.3 tensor(0.3003)
features.4.conv.6 tensor(0.3704)
features.5.conv.0 tensor(0.3934)
features.5.conv.3 tensor(0.4132)
features.5.conv.6 tensor(0.5088)
features.6.conv.0 tensor(0.0509)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0862)
features.7.conv.0 tensor(0.1759)
features.7.conv.3 tensor(0.4557)
features.7.conv.6 tensor(0.5034)
features.8.conv.0 tensor(0.6128)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.6447)
features.9.conv.0 tensor(0.5548)
features.9.conv.3 tensor(0.5561)
features.9.conv.6 tensor(0.6691)
features.10.conv.0 tensor(0.0547)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0946)
features.11.conv.0 tensor(0.7641)
features.11.conv.3 tensor(0.6345)
features.11.conv.6 tensor(0.8203)
features.12.conv.0 tensor(0.7562)
features.12.conv.3 tensor(0.6703)
features.12.conv.6 tensor(0.8474)
features.13.conv.0 tensor(0.2599)
features.13.conv.3 tensor(0.4844)
features.13.conv.6 tensor(0.4167)
features.14.conv.0 tensor(0.9225)
features.14.conv.3 tensor(0.8289)
features.14.conv.6 tensor(0.9621)
features.15.conv.0 tensor(0.9031)
features.15.conv.3 tensor(0.8370)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.6849)
features.16.conv.3 tensor(0.8027)
features.16.conv.6 tensor(0.9018)
conv.0 tensor(0.1877)
tensor(1399111.) 2188896.0
0.54359674
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
0.54369897
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
0.54375988
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.54369944
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.54365975
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.54359955
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
0.54356927
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
0.54355383
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.54354429
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
0.54356241
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
0.54358429
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
0.54368907
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
0.54370970
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
0.54362184
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
0.54362535
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
0.54358536
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][   20/  196]   Loss 0.101889   Top1 96.542969   Top5 100.000000   BatchTime 0.378837   LR 0.000047
0.54353637
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
0.54351479
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
0.54349285
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
0.54348087
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
0.54347664
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
0.54352450
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
0.54356372
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.54354632
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
0.54346931
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
0.54343593
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
0.54338551
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.54333854
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
0.54329413
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
0.54326671
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
0.54323524
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
0.54322976
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
0.54324692
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
0.54330200
tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][   40/  196]   Loss 0.109039   Top1 96.318359   Top5 99.970703   BatchTime 0.345801   LR 0.000047
0.54335922
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
0.54344726
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
0.54337555
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
0.54339135
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
0.54333496
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
0.54331380
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
0.54330599
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
0.54332680
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
0.54336560
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
0.54341888
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
0.54334003
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
0.54333967
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
0.54333460
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
0.54326439
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
0.54322988
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
0.54323179
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.54323667
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
0.54326105
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
0.54329699
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
0.54330689
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
0.54326576
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
0.54327148
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
0.54324156
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][   60/  196]   Loss 0.108390   Top1 96.419271   Top5 99.960938   BatchTime 0.315854   LR 0.000046
0.54322541
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
0.54326320
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
0.54327685
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
0.54326487
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
0.54327846
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.54326296
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
0.54324263
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
0.54321378
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
0.54316622
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
0.54312921
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.54313904
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
0.54318196
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
0.54325110
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
0.54324698
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
0.54315907
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.54313022
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
0.54309881
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
0.54310352
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.54307193
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
0.54305279
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
0.54308748
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
0.54316860
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
0.54316002
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][   80/  196]   Loss 0.108594   Top1 96.381836   Top5 99.960938   BatchTime 0.303839   LR 0.000046
0.54306120
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
0.54307288
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
0.54304296
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
0.54302496
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
0.54302990
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
0.54303890
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
0.54308975
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
0.54315293
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.54314125
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
0.54309171
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
0.54299384
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.54295588
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
0.54292500
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
0.54290807
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.54291552
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][  100/  196]   Loss 0.108735   Top1 96.296875   Top5 99.957031   BatchTime 0.297622   LR 0.000046
0.54291141
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.54295224
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
0.54300690
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
0.54306257
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
0.54303372
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
0.54294008
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
0.54292738
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
0.54292732
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
0.54292321
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.54293096
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.54291040
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
0.54293126
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.54299784
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
0.54307765
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
0.54298520
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
0.54303223
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
0.54300356
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
0.54299933
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
0.54300511
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
0.54300678
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
0.54299265
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][  120/  196]   Loss 0.108473   Top1 96.360677   Top5 99.957682   BatchTime 0.295863   LR 0.000045
0.54299110
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
0.54304528
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.54303056
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
0.54304677
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
0.54305995
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
0.54305130
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
0.54307055
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.54311103
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
0.54313904
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
0.54315966
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
0.54303098
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
0.54299110
tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)
0.54299343
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
0.54301083
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
0.54296887
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
0.54293722
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
0.54292935
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
0.54298365
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.54304022
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
0.54288995
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][  140/  196]   Loss 0.109904   Top1 96.319754   Top5 99.960938   BatchTime 0.298971   LR 0.000045
0.54288226
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
0.54289496
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.54286247
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
0.54285300
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.54284835
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
0.54284835
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
0.54287744
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
0.54294205
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
0.54296470
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
0.54293191
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
0.54291952
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
0.54290724
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
0.54292703
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.54287863
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
0.54285330
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
0.54283434
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
0.54285985
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.54292423
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.54288143
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.54286700
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
0.54288274
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
0.54282635
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.54282510
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][  160/  196]   Loss 0.110929   Top1 96.240234   Top5 99.963379   BatchTime 0.303160   LR 0.000044
0.54284203
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
0.54285938
tensor(0.1617, device='cuda:0', grad_fn=<AddBackward0>)
0.54290545
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
0.54287452
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.54280692
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
0.54282594
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.54283398
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
0.54280698
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
0.54277331
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
0.54268146
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.54266280
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
0.54266524
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.54268318
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
0.54270780
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
0.54275852
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
0.54277885
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.54285979
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
0.54283643
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [53][  180/  196]   Loss 0.111794   Top1 96.213108   Top5 99.967448   BatchTime 0.306720   LR 0.000044
0.54278541
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
0.54270315
tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)
0.54267609
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.54265523
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.54260218
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.54260296
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
0.54262477
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
0.54268521
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
0.54281557
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
0.54276335
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
0.54279315
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
0.54270208
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
0.54266268
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
0.54264498
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
0.54260582
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
0.54258925
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
0.54258257
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
0.54259610
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.240    Top5: 99.968    Loss: 0.111
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
0.54261774
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [53][   20/   40]   Loss 0.417422   Top1 88.554688   Top5 99.609375   BatchTime 0.127745
INFO - Validation [53][   40/   40]   Loss 0.406450   Top1 88.410000   Top5 99.690000   BatchTime 0.091836
INFO - ==> Top1: 88.410    Top5: 99.690    Loss: 0.406
INFO - ==> Sparsity : 0.641
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.3477)
features.1.conv.0 tensor(0.0632)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1175)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.5845)
features.3.conv.0 tensor(0.0816)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1174)
features.4.conv.0 tensor(0.0796)
features.4.conv.3 tensor(0.2992)
features.4.conv.6 tensor(0.3734)
features.5.conv.0 tensor(0.3955)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.5099)
features.6.conv.0 tensor(0.0522)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0854)
features.7.conv.0 tensor(0.1805)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.5080)
features.8.conv.0 tensor(0.6088)
features.8.conv.3 tensor(0.5408)
features.8.conv.6 tensor(0.6450)
features.9.conv.0 tensor(0.5621)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6710)
features.10.conv.0 tensor(0.0563)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.0948)
features.11.conv.0 tensor(0.7642)
features.11.conv.3 tensor(0.6352)
features.11.conv.6 tensor(0.8192)
features.12.conv.0 tensor(0.7560)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.8493)
features.13.conv.0 tensor(0.2669)
features.13.conv.3 tensor(0.4840)
features.13.conv.6 tensor(0.4216)
features.14.conv.0 tensor(0.9223)
features.14.conv.3 tensor(0.8293)
features.14.conv.6 tensor(0.9623)
features.15.conv.0 tensor(0.9031)
features.15.conv.3 tensor(0.8368)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.6887)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.9037)
conv.0 tensor(0.1906)
tensor(1402829.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
0.54267234
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
0.54274869
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.54278338
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
0.54276645
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
0.54268849
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
0.54265499
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
0.54264045
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
0.54263741
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
0.54261792
tensor(0.1851, device='cuda:0', grad_fn=<AddBackward0>)
0.54261971
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
0.54263735
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
0.54270142
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
0.54279739
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
0.54268330
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
0.54265249
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
0.54266942
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
0.54267448
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
0.54266852
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
0.54268193
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][   20/  196]   Loss 0.116082   Top1 96.210938   Top5 99.941406   BatchTime 0.332750   LR 0.000043
0.54269975
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
0.54274809
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
0.54269964
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
0.54262745
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.54262173
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
0.54263002
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.54264402
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
0.54265261
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.54266256
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.54273820
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
0.54266715
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
0.54263651
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.54263371
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.54262823
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][   40/  196]   Loss 0.109795   Top1 96.435547   Top5 99.960938   BatchTime 0.298821   LR 0.000042
0.54262227
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.54265797
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
0.54268378
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
0.54269356
tensor(0.1656, device='cuda:0', grad_fn=<AddBackward0>)
0.54261506
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
0.54255778
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
0.54253447
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.54252678
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
0.54253656
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
0.54251426
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.54251581
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
0.54256970
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
0.54252321
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.54250449
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
0.54249442
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.54245335
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
0.54242986
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
0.54243612
tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)
0.54239428
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
0.54240674
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.54245955
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
0.54252076
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
0.54250354
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][   60/  196]   Loss 0.109369   Top1 96.419271   Top5 99.967448   BatchTime 0.290415   LR 0.000042
0.54250133
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.54248101
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.54250830
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.54247212
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
0.54243940
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
0.54244763
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
0.54248273
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.54249036
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
0.54251361
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
0.54257160
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
0.54252994
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
0.54246706
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
0.54240805
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.54238200
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
0.54233438
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
0.54231250
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
0.54230368
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
0.54228491
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
0.54230446
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
0.54237098
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
0.54246718
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][   80/  196]   Loss 0.107123   Top1 96.503906   Top5 99.975586   BatchTime 0.291939   LR 0.000041
0.54238397
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
0.54242122
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
0.54235893
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
0.54231739
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.54230052
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
0.54229122
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
0.54227144
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
0.54227853
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.54232883
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
0.54232556
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
0.54230171
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
0.54236370
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.54236227
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
0.54237777
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.54245108
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.54243708
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
0.54240537
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
0.54244232
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
0.54241842
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
0.54240590
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
0.54237109
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
0.54232323
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
0.54232013
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
0.54225445
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][  100/  196]   Loss 0.107211   Top1 96.484375   Top5 99.980469   BatchTime 0.302129   LR 0.000041
0.54226452
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
0.54226071
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
0.54227233
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
0.54222834
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
0.54226357
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
0.54218066
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
0.54215324
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.54218340
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
0.54218143
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
0.54213762
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
0.54214567
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.54210430
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
0.54210275
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
0.54210287
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
0.54203278
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
0.54198784
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
0.54197478
tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][  120/  196]   Loss 0.106965   Top1 96.477865   Top5 99.980469   BatchTime 0.308989   LR 0.000040
0.54196125
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.54195189
tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)
0.54197842
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.54199731
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.54203975
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
0.54204410
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.54197931
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
0.54195607
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
0.54194242
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
0.54190850
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.54188293
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.54191387
tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)
0.54200363
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
0.54197341
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
0.54196763
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
0.54202777
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
0.54201329
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
0.54200959
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.54202503
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][  140/  196]   Loss 0.107395   Top1 96.470424   Top5 99.977679   BatchTime 0.308539   LR 0.000040
0.54202229
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.54201323
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
0.54196262
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
0.54196155
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
0.54199737
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
0.54200244
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
0.54200810
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
0.54199094
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
0.54195058
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
0.54192311
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
0.54185331
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.54180986
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
0.54178786
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.54178375
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
0.54175591
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
0.54174292
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
0.54175317
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
0.54181820
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
0.54186803
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
0.54193521
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
0.54189402
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.54183680
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
0.54178339
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
0.54174143
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][  160/  196]   Loss 0.107180   Top1 96.489258   Top5 99.980469   BatchTime 0.313140   LR 0.000039
0.54170215
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
0.54167414
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
0.54164177
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.54164559
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.54166448
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
0.54169440
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
0.54183114
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
0.54184347
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
0.54174972
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
0.54170465
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
0.54167700
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
0.54166192
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
0.54162073
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
0.54158634
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
0.54160404
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.54160511
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
0.54164910
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
0.54160893
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [54][  180/  196]   Loss 0.107819   Top1 96.475694   Top5 99.982639   BatchTime 0.316202   LR 0.000039
0.54162800
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
0.54167414
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
0.54166466
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
0.54161918
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
0.54158062
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
0.54157686
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
0.54159951
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
0.54154587
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
0.54154241
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
0.54151338
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.54154164
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
0.54156923
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
0.54148871
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
0.54146624
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
0.54150170
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.54146630
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
0.54145771
tensor(0.1731, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.510    Top5: 99.984    Loss: 0.107
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [54][   20/   40]   Loss 0.411212   Top1 88.554688   Top5 99.550781   BatchTime 0.136207
INFO - Validation [54][   40/   40]   Loss 0.400078   Top1 88.690000   Top5 99.610000   BatchTime 0.095700
INFO - ==> Top1: 88.690    Top5: 99.610    Loss: 0.400
INFO - ==> Sparsity : 0.643
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.870   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3160)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.1160)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.5856)
features.3.conv.0 tensor(0.0813)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1111)
features.4.conv.0 tensor(0.0827)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.3761)
features.5.conv.0 tensor(0.3952)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.5112)
features.6.conv.0 tensor(0.0519)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.1766)
features.7.conv.3 tensor(0.4583)
features.7.conv.6 tensor(0.5117)
features.8.conv.0 tensor(0.6099)
features.8.conv.3 tensor(0.5408)
features.8.conv.6 tensor(0.6491)
features.9.conv.0 tensor(0.5615)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6744)
features.10.conv.0 tensor(0.0549)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0940)
features.11.conv.0 tensor(0.7643)
features.11.conv.3 tensor(0.6358)
features.11.conv.6 tensor(0.8214)
features.12.conv.0 tensor(0.7567)
features.12.conv.3 tensor(0.6717)
features.12.conv.6 tensor(0.8492)
features.13.conv.0 tensor(0.2823)
features.13.conv.3 tensor(0.4838)
features.13.conv.6 tensor(0.4340)
features.14.conv.0 tensor(0.9222)
features.14.conv.3 tensor(0.8289)
features.14.conv.6 tensor(0.9622)
features.15.conv.0 tensor(0.9038)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.6904)
features.16.conv.3 tensor(0.8028)
features.16.conv.6 tensor(0.9064)
conv.0 tensor(0.1950)
tensor(1408097.) 2188896.0
0.54147494
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
0.54149401
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.54143459
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
0.54137397
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
0.54135382
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
0.54131573
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
0.54125911
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
0.54121971
tensor(0.1573, device='cuda:0', grad_fn=<AddBackward0>)
0.54118866
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
0.54120910
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
0.54128206
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
0.54136759
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
0.54130429
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
0.54129380
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.54121709
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
0.54114777
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
0.54108584
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
0.54102832
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][   20/  196]   Loss 0.107348   Top1 96.562500   Top5 99.980469   BatchTime 0.359858   LR 0.000038
0.54099852
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.54097629
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
0.54097772
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
0.54099613
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
0.54106951
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
0.54114467
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.54109484
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
0.54104412
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.54103762
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
0.54095715
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.54091430
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
0.54088503
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
0.54085577
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
0.54082286
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.54083943
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
0.54085082
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
0.54081708
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
0.54081184
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][   40/  196]   Loss 0.104980   Top1 96.513672   Top5 99.970703   BatchTime 0.345394   LR 0.000038
0.54074150
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.54072851
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
0.54072690
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
0.54078156
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
0.54076058
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
0.54079652
tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)
0.54067105
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
0.54061264
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.54059547
tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)
0.54057431
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
0.54053372
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
0.54049563
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
0.54048842
tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)
0.54049319
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
0.54053873
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
0.54046595
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.54046589
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
0.54047674
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
0.54044706
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
0.54039681
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
0.54037589
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
0.54041928
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
0.54040468
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
0.54038215
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.54039484
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][   60/  196]   Loss 0.105303   Top1 96.523438   Top5 99.980469   BatchTime 0.341419   LR 0.000037
0.54037958
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
0.54032212
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
0.54030102
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
0.54025936
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
0.54020143
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
0.54018241
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
0.54018265
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
0.54019219
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
0.54020160
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
0.54020375
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.54022193
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
0.54027891
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
0.54024059
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.54025489
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][   80/  196]   Loss 0.104943   Top1 96.484375   Top5 99.980469   BatchTime 0.323334   LR 0.000037
0.54027504
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
0.54018009
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
0.54008329
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
0.54005820
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
0.54003441
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
0.54001182
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
0.53999096
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
0.53996742
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
0.53996372
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
0.53998822
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.53999406
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
0.54007226
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.54005456
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
0.54005778
tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)
0.54009062
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
0.54015642
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
0.54002929
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
0.53999138
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
0.54000819
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.54003626
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
0.53998321
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
0.53996289
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][  100/  196]   Loss 0.104331   Top1 96.531250   Top5 99.976562   BatchTime 0.316564   LR 0.000036
0.53994447
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
0.53991967
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
0.53995335
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
0.53994852
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
0.53992426
tensor(0.1943, device='cuda:0', grad_fn=<AddBackward0>)
0.53995252
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
0.53997779
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
0.53995150
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.53990871
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
0.53983825
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
0.53976208
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
0.53971404
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
0.53968769
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.53967881
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
0.53966701
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
0.53966576
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
0.53965718
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.53966880
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
0.53973198
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
0.53979170
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
0.53977436
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
0.53969955
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
0.53962809
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
0.53958011
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][  120/  196]   Loss 0.104301   Top1 96.523438   Top5 99.973958   BatchTime 0.318609   LR 0.000036
0.53955519
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
0.53950167
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.53944612
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
0.53940797
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
0.53939891
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
0.53941840
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
0.53947407
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
0.53956109
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.53948247
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
0.53949684
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
0.53946978
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
0.53940260
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
0.53931493
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
0.53927618
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][  140/  196]   Loss 0.104412   Top1 96.529018   Top5 99.972098   BatchTime 0.315678   LR 0.000035
0.53927130
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.53929287
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
0.53932047
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
0.53932887
tensor(0.1404, device='cuda:0', grad_fn=<AddBackward0>)
0.53934056
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
0.53930020
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
0.53928566
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
0.53924328
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
0.53918171
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
0.53913671
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
0.53911936
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
0.53908277
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
0.53904945
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
0.53907359
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
0.53913218
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.53915882
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
0.53915238
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
0.53905654
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.53900349
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
0.53897017
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
0.53893417
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
0.53890806
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
0.53889519
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.53890789
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
0.53898638
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.53904247
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][  160/  196]   Loss 0.103469   Top1 96.582031   Top5 99.975586   BatchTime 0.315867   LR 0.000035
0.53899753
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.53888166
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
0.53879535
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.53874218
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.53868920
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
0.53864115
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.53859216
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.53856617
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.53855920
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
0.53857881
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
0.53857762
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
0.53859890
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
0.53861761
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
0.53865886
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [55][  180/  196]   Loss 0.103113   Top1 96.586372   Top5 99.978299   BatchTime 0.311620   LR 0.000034
0.53874880
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
0.53870547
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.53866833
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
0.53870881
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
0.53861952
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
0.53859442
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.53858346
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.53861946
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
0.53866166
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
0.53860450
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
0.53862637
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
0.53853643
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.53847998
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.53845388
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.576    Top5: 99.978    Loss: 0.103
0.53844261
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
0.53843594
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
0.53844362
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
0.53851020
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
0.53860563
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
0.53845733
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
0.53841341
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [55][   20/   40]   Loss 0.403969   Top1 89.140625   Top5 99.589844   BatchTime 0.120539
INFO - Validation [55][   40/   40]   Loss 0.395597   Top1 89.050000   Top5 99.670000   BatchTime 0.088532
INFO - ==> Top1: 89.050    Top5: 99.670    Loss: 0.396
INFO - ==> Sparsity : 0.647
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 89.050   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3125)
features.0.conv.3 tensor(0.3438)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0916)
features.2.conv.0 tensor(0.1157)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.5885)
features.3.conv.0 tensor(0.0796)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1135)
features.4.conv.0 tensor(0.0806)
features.4.conv.3 tensor(0.2980)
features.4.conv.6 tensor(0.3783)
features.5.conv.0 tensor(0.4032)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.5133)
features.6.conv.0 tensor(0.0514)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0848)
features.7.conv.0 tensor(0.1858)
features.7.conv.3 tensor(0.4546)
features.7.conv.6 tensor(0.5125)
features.8.conv.0 tensor(0.6129)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.6502)
features.9.conv.0 tensor(0.5712)
features.9.conv.3 tensor(0.5561)
features.9.conv.6 tensor(0.6780)
features.10.conv.0 tensor(0.0546)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0945)
features.11.conv.0 tensor(0.7660)
features.11.conv.3 tensor(0.6358)
features.11.conv.6 tensor(0.8220)
features.12.conv.0 tensor(0.7628)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.8517)
features.13.conv.0 tensor(0.2841)
features.13.conv.3 tensor(0.4842)
features.13.conv.6 tensor(0.4677)
features.14.conv.0 tensor(0.9228)
features.14.conv.3 tensor(0.8294)
features.14.conv.6 tensor(0.9619)
features.15.conv.0 tensor(0.9047)
features.15.conv.3 tensor(0.8370)
features.15.conv.6 tensor(0.9670)
features.16.conv.0 tensor(0.6944)
features.16.conv.3 tensor(0.8021)
features.16.conv.6 tensor(0.9086)
conv.0 tensor(0.2017)
tensor(1416851.) 2188896.0
0.53839308
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
0.53838605
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
0.53836924
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
0.53835458
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
0.53837752
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.53841937
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.53831708
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
0.53831053
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
0.53832251
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.53826195
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
0.53819156
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
0.53815699
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.53814453
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
0.53815556
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
0.53817630
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
0.53820211
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.53819722
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][   20/  196]   Loss 0.084076   Top1 97.480469   Top5 100.000000   BatchTime 0.402951   LR 0.000034
0.53825861
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
0.53826785
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
0.53819805
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
0.53818500
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
0.53819782
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
0.53816468
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
0.53810328
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
0.53809029
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
0.53811073
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.53813756
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.53815335
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
0.53824633
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
0.53817534
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
0.53819633
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
0.53819245
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
0.53818285
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
0.53808999
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.53807449
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][   40/  196]   Loss 0.090967   Top1 97.070312   Top5 99.980469   BatchTime 0.369219   LR 0.000033
0.53808564
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.53812158
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
0.53810489
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.53812939
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.53821331
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
0.53815514
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
0.53810829
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
0.53812373
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.53812069
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
0.53815341
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
0.53814405
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
0.53814036
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
0.53814912
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
0.53813797
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.53808415
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
0.53809607
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
0.53812367
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
0.53808111
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.53804129
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
0.53804851
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
0.53810555
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
0.53810954
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
0.53811419
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
0.53814733
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][   60/  196]   Loss 0.096649   Top1 96.783854   Top5 99.986979   BatchTime 0.356728   LR 0.000033
0.53811210
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
0.53811944
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
0.53811795
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.53808558
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
0.53806865
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
0.53809583
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
0.53812116
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
0.53807080
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
0.53808159
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.53813571
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
0.53821474
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
0.53813612
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.53812259
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
0.53814805
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
0.53809953
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
0.53809088
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
0.53807139
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
0.53808093
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][   80/  196]   Loss 0.097882   Top1 96.713867   Top5 99.990234   BatchTime 0.349697   LR 0.000032
0.53811550
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
0.53814489
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
0.53803933
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
0.53802335
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
0.53803533
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
0.53798276
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
0.53795499
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.53796071
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
0.53799725
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
0.53800827
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
0.53798407
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.53800005
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.53801799
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
0.53797084
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
0.53793502
tensor(0.1617, device='cuda:0', grad_fn=<AddBackward0>)
0.53793103
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
0.53794485
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
0.53789485
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][  100/  196]   Loss 0.098670   Top1 96.671875   Top5 99.992188   BatchTime 0.345094   LR 0.000032
0.53784961
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
0.53783303
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
0.53784537
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
0.53786099
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
0.53788424
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
0.53793204
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
0.53783375
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
0.53782588
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
0.53784537
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
0.53782505
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
0.53779989
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.53777570
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
0.53776979
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.53775138
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.53775001
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
0.53768885
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.53758687
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
0.53752184
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
0.53746128
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
0.53741330
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][  120/  196]   Loss 0.098338   Top1 96.666667   Top5 99.990234   BatchTime 0.340123   LR 0.000031
0.53737915
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
0.53735566
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
0.53734845
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
0.53735334
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.53736538
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.53735834
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
0.53737944
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.53748125
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
0.53746206
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.53744745
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.53735232
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.53732300
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.53726327
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.53723782
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.53721321
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
0.53720140
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
0.53722429
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.53724980
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][  140/  196]   Loss 0.098309   Top1 96.693638   Top5 99.988839   BatchTime 0.335858   LR 0.000031
0.53730613
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
0.53728127
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
0.53714114
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
0.53709298
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.53703463
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.53698039
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
0.53694755
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
0.53692073
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
0.53690219
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
0.53688085
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
0.53686953
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
0.53686476
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
0.53687143
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
0.53690201
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.53697300
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.53705329
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
0.53713226
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
0.53713095
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.53711033
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
0.53709412
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
0.53703284
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
0.53698403
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
0.53696024
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
0.53696990
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
0.53696477
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][  160/  196]   Loss 0.098533   Top1 96.684570   Top5 99.990234   BatchTime 0.335765   LR 0.000031
0.53700250
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
0.53701466
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.53705555
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
0.53704602
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
0.53702444
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
0.53701884
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
0.53702694
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
0.53703809
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
0.53703904
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
0.53702086
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.53697407
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
0.53699261
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.53684390
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
0.53679425
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
0.53675497
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
0.53673238
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
0.53671825
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
0.53672194
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [56][  180/  196]   Loss 0.097667   Top1 96.710069   Top5 99.991319   BatchTime 0.335767   LR 0.000030
0.53676492
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
0.53679913
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.53684098
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
0.53680700
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
0.53677803
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
0.53673673
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
0.53672385
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
0.53668630
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
0.53665239
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.53667861
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
0.53666168
tensor(0.1961, device='cuda:0', grad_fn=<AddBackward0>)
0.53669226
tensor(0.1562, device='cuda:0', grad_fn=<AddBackward0>)
0.53669369
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
0.53669155
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
0.53665107
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
0.53657490
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
0.53655714
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
0.53651643
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.672    Top5: 99.992    Loss: 0.098
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.53648067
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
0.53648055
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [56][   20/   40]   Loss 0.402256   Top1 88.691406   Top5 99.628906   BatchTime 0.121505
INFO - Validation [56][   40/   40]   Loss 0.392772   Top1 88.910000   Top5 99.700000   BatchTime 0.087773
INFO - ==> Top1: 88.910    Top5: 99.700    Loss: 0.393
INFO - ==> Sparsity : 0.650
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 89.050   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3229)
features.0.conv.3 tensor(0.3438)
features.1.conv.0 tensor(0.0605)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0938)
features.2.conv.0 tensor(0.1114)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5874)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1120)
features.4.conv.0 tensor(0.0796)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.3846)
features.5.conv.0 tensor(0.4007)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.5153)
features.6.conv.0 tensor(0.0506)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.1895)
features.7.conv.3 tensor(0.4560)
features.7.conv.6 tensor(0.5172)
features.8.conv.0 tensor(0.6147)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.6521)
features.9.conv.0 tensor(0.5710)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.6789)
features.10.conv.0 tensor(0.0529)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0945)
features.11.conv.0 tensor(0.7663)
features.11.conv.3 tensor(0.6356)
features.11.conv.6 tensor(0.8233)
features.12.conv.0 tensor(0.7671)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.8533)
features.13.conv.0 tensor(0.2903)
features.13.conv.3 tensor(0.4834)
features.13.conv.6 tensor(0.5012)
features.14.conv.0 tensor(0.9231)
features.14.conv.3 tensor(0.8291)
features.14.conv.6 tensor(0.9619)
features.15.conv.0 tensor(0.9050)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9670)
features.16.conv.0 tensor(0.6974)
features.16.conv.3 tensor(0.8023)
features.16.conv.6 tensor(0.9098)
conv.0 tensor(0.2059)
tensor(1423640.) 2188896.0
0.53648704
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
0.53651482
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
0.53654414
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.53658921
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
0.53652793
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.53648502
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
0.53638607
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.53633267
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
0.53631675
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.53630537
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
0.53627956
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.53626913
tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)
0.53630096
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
0.53641051
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
0.53637987
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][   20/  196]   Loss 0.082757   Top1 97.441406   Top5 99.960938   BatchTime 0.360158   LR 0.000029
0.53641331
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
0.53637540
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
0.53632486
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
0.53629971
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.53632390
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
0.53636682
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
0.53638226
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
0.53633171
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
0.53627515
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
0.53627002
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
0.53623593
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
0.53620493
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
0.53620523
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
0.53621823
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
0.53622836
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
0.53626877
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
0.53623354
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
0.53613389
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
0.53612375
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
0.53608513
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
0.53603989
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.53598899
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
0.53597021
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.53600293
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
0.53608358
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][   40/  196]   Loss 0.090245   Top1 97.167969   Top5 99.970703   BatchTime 0.344132   LR 0.000029
0.53601658
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.53605634
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
0.53596836
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
0.53594363
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.53590322
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.53588206
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
0.53590739
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.53599966
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.53595835
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
0.53598261
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
0.53592211
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
0.53588778
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
0.53586519
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
0.53586465
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
0.53580922
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
0.53575057
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
0.53574967
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
0.53577411
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][   60/  196]   Loss 0.092628   Top1 97.128906   Top5 99.967448   BatchTime 0.340427   LR 0.000029
0.53579670
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
0.53576976
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
0.53580832
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.53575528
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
0.53577805
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
0.53579867
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
0.53570604
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.53567988
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.53570390
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
0.53570217
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.53567994
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
0.53563678
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
0.53558242
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.53557539
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.53556371
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
0.53550392
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
0.53547984
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
0.53549969
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
0.53552681
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][   80/  196]   Loss 0.092907   Top1 97.070312   Top5 99.970703   BatchTime 0.334398   LR 0.000028
0.53551686
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
0.53556430
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.53553265
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.53553426
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.53546160
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
0.53541946
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
0.53541470
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
0.53536761
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
0.53534287
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
0.53532010
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
0.53531700
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.53536564
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
0.53534168
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
0.53539342
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
0.53532612
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
0.53534889
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
0.53525454
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
0.53523344
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
0.53525698
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][  100/  196]   Loss 0.094629   Top1 97.031250   Top5 99.976562   BatchTime 0.328989   LR 0.000028
0.53526872
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
0.53523493
tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)
0.53524435
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.53532445
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.53531182
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
0.53523594
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.53517139
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.53514051
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.53513169
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
0.53511530
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
0.53510147
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.53513336
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
0.53526789
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
0.53527516
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.53522354
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
0.53523648
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
0.53515637
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
0.53510690
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
0.53507954
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
0.53507036
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
0.53508121
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][  120/  196]   Loss 0.095114   Top1 96.992188   Top5 99.977214   BatchTime 0.322736   LR 0.000027
0.53508335
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.53512293
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
0.53519070
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
0.53512341
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
0.53514123
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.53520060
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
0.53518355
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
0.53517073
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.53514546
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
0.53513616
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
0.53508186
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
0.53504598
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
0.53503078
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.53502172
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
0.53506547
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
0.53513402
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
0.53512025
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
0.53508621
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
0.53502780
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.53500664
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
0.53499478
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][  140/  196]   Loss 0.095672   Top1 96.947545   Top5 99.969308   BatchTime 0.317868   LR 0.000027
0.53498542
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
0.53497511
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
0.53502518
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.53505826
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
0.53505975
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
0.53512090
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
0.53506458
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
0.53505635
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
0.53508610
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
0.53503352
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.53502840
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
0.53504294
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
0.53500390
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.53496891
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
0.53493959
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
0.53490639
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
0.53490460
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
0.53488463
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
0.53486848
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][  160/  196]   Loss 0.095404   Top1 96.928711   Top5 99.970703   BatchTime 0.315486   LR 0.000027
0.53487635
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
0.53494567
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
0.53501463
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.53499073
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
0.53497845
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
0.53495401
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
0.53492504
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.53489214
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.53489280
tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)
0.53488684
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
0.53493166
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
0.53494763
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.53490722
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.53484648
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.53485394
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.53482586
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
0.53478926
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
0.53477603
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.53478283
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [57][  180/  196]   Loss 0.096588   Top1 96.872830   Top5 99.971788   BatchTime 0.316822   LR 0.000026
0.53477335
tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)
0.53477991
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
0.53483045
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
0.53472674
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
0.53469580
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
0.53468108
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
0.53463757
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
0.53460628
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
0.53459316
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
0.53461701
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
0.53467113
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
0.53477192
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
0.53474838
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
0.53465116
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.880    Top5: 99.972    Loss: 0.096
0.53461748
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
0.53461814
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
0.53461581
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.53459638
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
0.53460556
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
0.53463995
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [57][   20/   40]   Loss 0.418262   Top1 88.515625   Top5 99.589844   BatchTime 0.120217
INFO - Validation [57][   40/   40]   Loss 0.403672   Top1 88.750000   Top5 99.690000   BatchTime 0.089008
INFO - ==> Top1: 88.750    Top5: 99.690    Loss: 0.404
INFO - ==> Sparsity : 0.653
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 89.050   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3125)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0946)
features.2.conv.0 tensor(0.1192)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.5880)
features.3.conv.0 tensor(0.0767)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.1146)
features.4.conv.0 tensor(0.0845)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.3856)
features.5.conv.0 tensor(0.4056)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.5168)
features.6.conv.0 tensor(0.0513)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0834)
features.7.conv.0 tensor(0.1920)
features.7.conv.3 tensor(0.4572)
features.7.conv.6 tensor(0.5228)
features.8.conv.0 tensor(0.6192)
features.8.conv.3 tensor(0.5425)
features.8.conv.6 tensor(0.6531)
features.9.conv.0 tensor(0.5788)
features.9.conv.3 tensor(0.5564)
features.9.conv.6 tensor(0.6803)
features.10.conv.0 tensor(0.0559)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0942)
features.11.conv.0 tensor(0.7691)
features.11.conv.3 tensor(0.6352)
features.11.conv.6 tensor(0.8239)
features.12.conv.0 tensor(0.7662)
features.12.conv.3 tensor(0.6713)
features.12.conv.6 tensor(0.8541)
features.13.conv.0 tensor(0.2882)
features.13.conv.3 tensor(0.4844)
features.13.conv.6 tensor(0.5190)
features.14.conv.0 tensor(0.9239)
features.14.conv.3 tensor(0.8294)
features.14.conv.6 tensor(0.9624)
features.15.conv.0 tensor(0.9048)
features.15.conv.3 tensor(0.8369)
features.15.conv.6 tensor(0.9669)
features.16.conv.0 tensor(0.7006)
features.16.conv.3 tensor(0.8023)
features.16.conv.6 tensor(0.9112)
conv.0 tensor(0.2116)
tensor(1429440.) 2188896.0
0.53470194
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.53465772
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
0.53465062
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
0.53459257
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
0.53455961
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.53452182
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
0.53448242
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
0.53449726
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
0.53455973
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
0.53458637
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
0.53452235
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
0.53446966
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.53444636
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
0.53442127
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
0.53439665
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
0.53438145
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
0.53439552
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
0.53441375
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][   20/  196]   Loss 0.095167   Top1 96.972656   Top5 99.960938   BatchTime 0.374135   LR 0.000025
0.53445208
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.53445971
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
0.53448153
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
0.53442842
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.53432173
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
0.53427315
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
0.53424746
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.53421950
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.53420383
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.53420538
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
0.53425699
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.53436714
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
0.53435224
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
0.53435862
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
0.53430492
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
0.53424060
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
0.53421170
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][   40/  196]   Loss 0.096008   Top1 96.787109   Top5 99.970703   BatchTime 0.359620   LR 0.000025
0.53420359
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
0.53420687
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
0.53425848
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.53431982
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
0.53424162
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
0.53424925
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
0.53419924
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.53417873
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
0.53414780
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
0.53413540
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
0.53416699
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
0.53408456
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.53409582
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.53413731
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
0.53412819
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
0.53409433
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
0.53411442
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
0.53415281
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
0.53416705
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
0.53412551
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
0.53406870
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
0.53407031
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
0.53406864
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
0.53405273
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
0.53403914
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
0.53406107
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][   60/  196]   Loss 0.095782   Top1 96.881510   Top5 99.973958   BatchTime 0.342706   LR 0.000025
0.53405845
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
0.53398085
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
0.53400832
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
0.53406674
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.53395277
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.53392237
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
0.53393567
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.53392130
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.53393936
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
0.53400749
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
0.53400636
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.53392369
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
0.53387260
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
0.53385031
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][   80/  196]   Loss 0.092834   Top1 96.982422   Top5 99.980469   BatchTime 0.328263   LR 0.000024
0.53377879
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
0.53371590
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
0.53362745
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
0.53350377
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.53331858
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
0.53287506
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
0.53261364
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
0.53254849
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
0.53250629
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
0.53245121
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
0.53244853
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
0.53242606
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
0.53240818
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
0.53240079
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
0.53239483
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
0.53238583
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
0.53239018
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.53239095
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
0.53239679
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.53239876
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][  100/  196]   Loss 0.095666   Top1 96.808594   Top5 99.984375   BatchTime 0.323454   LR 0.000024
0.53239292
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
0.53238964
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
0.53238803
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.53238314
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.53237695
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.53238451
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
0.53238231
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
0.53238231
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
0.53238708
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
0.53237969
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
0.53238875
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
0.53238767
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
0.53238839
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
0.53238904
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.53237981
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
0.53237069
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.53236794
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.53236014
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
0.53235972
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
0.53236008
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
0.53235972
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
0.53235966
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
0.53236502
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
0.53235269
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][  120/  196]   Loss 0.094739   Top1 96.852214   Top5 99.986979   BatchTime 0.326185   LR 0.000023
0.53235322
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.53235155
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
0.53234559
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
0.53233993
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
0.53233564
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.53232622
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
0.53231847
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
0.53231353
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
0.53230160
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
0.53228647
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
0.53227460
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
0.53226060
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
0.53224766
tensor(0.1573, device='cuda:0', grad_fn=<AddBackward0>)
0.53222561
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.53221577
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
0.53219730
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.53218293
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.53216553
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][  140/  196]   Loss 0.096291   Top1 96.791295   Top5 99.986049   BatchTime 0.327288   LR 0.000023
0.53214514
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
0.53213084
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
0.53211647
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
0.53210056
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
0.53208989
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
0.53207171
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.53205889
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
0.53204143
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
0.53202677
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
0.53201962
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
0.53201342
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
0.53200400
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
0.53199112
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
0.53197873
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
0.53197640
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
0.53196102
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.53194541
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.53194505
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][  160/  196]   Loss 0.095979   Top1 96.840820   Top5 99.985352   BatchTime 0.328270   LR 0.000023
0.53194261
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
0.53193820
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
0.53192967
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.53191501
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
0.53190458
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
0.53189391
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
0.53188556
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
0.53188378
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
0.53188229
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
0.53188616
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
0.53188050
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
0.53187436
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.53187090
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
0.53186405
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
0.53186196
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
0.53185910
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
0.53185320
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
0.53184372
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
0.53183681
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
0.53182346
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
0.53182113
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [58][  180/  196]   Loss 0.096435   Top1 96.812066   Top5 99.982639   BatchTime 0.320915   LR 0.000022
0.53182316
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
0.53180832
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
0.53180343
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
0.53180385
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
0.53179878
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
0.53178269
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.53177154
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
0.53176153
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.53175366
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.53175378
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
0.53175509
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
0.53174871
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.53174776
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.53175038
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
0.53174645
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
0.53174090
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.814    Top5: 99.982    Loss: 0.097
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.53173590
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.53173459
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
0.53173119
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.53173101
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [58][   20/   40]   Loss 0.414868   Top1 88.750000   Top5 99.609375   BatchTime 0.130613
INFO - Validation [58][   40/   40]   Loss 0.404516   Top1 88.800000   Top5 99.680000   BatchTime 0.094771
INFO - ==> Top1: 88.800    Top5: 99.680    Loss: 0.405
INFO - ==> Sparsity : 0.656
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 89.050   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3160)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0592)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0907)
features.2.conv.0 tensor(0.1189)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5909)
features.3.conv.0 tensor(0.0758)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1155)
features.4.conv.0 tensor(0.0869)
features.4.conv.3 tensor(0.2998)
features.4.conv.6 tensor(0.3898)
features.5.conv.0 tensor(0.4030)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.5197)
features.6.conv.0 tensor(0.0506)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.1931)
features.7.conv.3 tensor(0.4569)
features.7.conv.6 tensor(0.5288)
features.8.conv.0 tensor(0.6201)
features.8.conv.3 tensor(0.5434)
features.8.conv.6 tensor(0.6538)
features.9.conv.0 tensor(0.5798)
features.9.conv.3 tensor(0.5561)
features.9.conv.6 tensor(0.6820)
features.10.conv.0 tensor(0.0568)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0974)
features.11.conv.0 tensor(0.7709)
features.11.conv.3 tensor(0.6345)
features.11.conv.6 tensor(0.8254)
features.12.conv.0 tensor(0.7683)
features.12.conv.3 tensor(0.6703)
features.12.conv.6 tensor(0.8551)
features.13.conv.0 tensor(0.2916)
features.13.conv.3 tensor(0.4836)
features.13.conv.6 tensor(0.5235)
features.14.conv.0 tensor(0.9243)
features.14.conv.3 tensor(0.8291)
features.14.conv.6 tensor(0.9628)
features.15.conv.0 tensor(0.9050)
features.15.conv.3 tensor(0.8368)
features.15.conv.6 tensor(0.9669)
features.16.conv.0 tensor(0.7042)
features.16.conv.3 tensor(0.8027)
features.16.conv.6 tensor(0.9123)
conv.0 tensor(0.2192)
tensor(1435066.) 2188896.0
0.53173405
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.53173459
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.53173470
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
0.53173381
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
0.53173774
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.53174686
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.53175193
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.53175277
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
0.53175694
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
0.53176242
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
0.53176409
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
0.53176665
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
0.53176105
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
0.53176808
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.53177220
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
0.53177094
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][   20/  196]   Loss 0.091294   Top1 96.855469   Top5 99.980469   BatchTime 0.341258   LR 0.000022
0.53176206
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
0.53175986
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
0.53175348
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.53174937
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
0.53174698
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
0.53174335
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
0.53173560
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
0.53173119
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.53172630
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
0.53172374
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
0.53171027
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
0.53169698
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
0.53168064
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.53167123
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
0.53166050
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
0.53164530
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
0.53163922
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
0.53163362
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
0.53162646
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][   40/  196]   Loss 0.093664   Top1 96.748047   Top5 99.980469   BatchTime 0.325405   LR 0.000021
0.53162009
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
0.53161395
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
0.53160793
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
0.53159165
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
0.53157932
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.53157574
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
0.53156430
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
0.53155822
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
0.53154838
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
0.53153837
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
0.53152686
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
0.53151345
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
0.53150815
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
0.53150356
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
0.53149992
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
0.53149134
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
0.53147870
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
0.53147632
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
0.53146797
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
0.53147244
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
0.53146350
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
0.53146696
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][   60/  196]   Loss 0.094030   Top1 96.705729   Top5 99.980469   BatchTime 0.307661   LR 0.000021
0.53146231
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
0.53145856
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
0.53145307
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
0.53144401
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
0.53143901
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
0.53142989
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
0.53142172
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.53140759
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
0.53139800
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
0.53138214
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
0.53136301
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.53134656
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.53133875
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
0.53132814
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
0.53131688
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
0.53130358
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
0.53129894
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.53128660
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
0.53127944
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
0.53127813
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
0.53127015
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][   80/  196]   Loss 0.094656   Top1 96.689453   Top5 99.975586   BatchTime 0.303047   LR 0.000020
0.53126574
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
0.53126001
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.53125358
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.53124452
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
0.53124094
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
0.53123355
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
0.53123814
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
0.53123337
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
0.53122681
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
0.53121847
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
0.53121144
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
0.53121108
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
0.53121138
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.53121006
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
0.53120410
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
0.53119636
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
0.53119284
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
0.53117979
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][  100/  196]   Loss 0.093055   Top1 96.792969   Top5 99.972656   BatchTime 0.310460   LR 0.000020
0.53118336
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
0.53117990
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
0.53117776
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
0.53116912
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
0.53116250
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.53116369
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.53115737
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
0.53115374
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
0.53114754
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
0.53112972
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
0.53113395
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
0.53113174
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
0.53113478
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
0.53113854
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
0.53113991
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
0.53114474
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
0.53114814
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.53114337
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
0.53113323
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.53112781
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
0.53112143
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
0.53112346
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.53112727
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
0.53112698
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
0.53112215
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][  120/  196]   Loss 0.094700   Top1 96.767578   Top5 99.973958   BatchTime 0.311083   LR 0.000020
0.53112829
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.53112769
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
0.53112841
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
0.53112233
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
0.53111905
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
0.53111374
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.53111434
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.53110772
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
0.53109604
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
0.53109074
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
0.53107899
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
0.53107494
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
0.53106761
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][  140/  196]   Loss 0.094149   Top1 96.824777   Top5 99.974888   BatchTime 0.307820   LR 0.000019
0.53106630
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
0.53106427
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
0.53105748
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
0.53104365
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
0.53103215
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
0.53102255
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
0.53101909
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
0.53101826
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.53100353
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
0.53098953
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
0.53098202
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
0.53096563
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
0.53095675
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
0.53094971
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
0.53092241
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
0.53090870
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
0.53089517
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
0.53088963
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
0.53087997
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
0.53087366
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
0.53086483
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][  160/  196]   Loss 0.092270   Top1 96.928711   Top5 99.978027   BatchTime 0.307140   LR 0.000019
0.53085607
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.53085619
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
0.53085822
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
0.53086245
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
0.53086144
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
0.53085303
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
0.53084421
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
0.53083479
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.53082973
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
0.53083295
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
0.53082782
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
0.53081906
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.53081602
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
0.53081375
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.53081149
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
0.53080213
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.53079796
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
0.53080046
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.53079009
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
0.53078210
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
0.53077394
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
0.53076535
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.53075814
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
0.53075314
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
0.53074276
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
0.53074354
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [59][  180/  196]   Loss 0.092051   Top1 96.950955   Top5 99.980469   BatchTime 0.307359   LR 0.000019
0.53073359
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
0.53072387
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
0.53071761
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
0.53070855
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
0.53070462
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
0.53069949
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
0.53069764
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
0.53068960
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
0.53068072
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
0.53067690
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.53066933
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
0.53065473
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
0.53065300
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.53066140
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.53065360
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.974    Top5: 99.982    Loss: 0.092
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [59][   20/   40]   Loss 0.409502   Top1 88.808594   Top5 99.589844   BatchTime 0.129766
INFO - Validation [59][   40/   40]   Loss 0.397003   Top1 88.940000   Top5 99.680000   BatchTime 0.092194
INFO - ==> Top1: 88.940    Top5: 99.680    Loss: 0.397
INFO - ==> Sparsity : 0.657
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 89.050   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 88.960   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3194)
features.0.conv.3 tensor(0.3438)
features.1.conv.0 tensor(0.0605)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0942)
features.2.conv.0 tensor(0.1215)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.5900)
features.3.conv.0 tensor(0.0761)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1163)
features.4.conv.0 tensor(0.0902)
features.4.conv.3 tensor(0.2986)
features.4.conv.6 tensor(0.3901)
features.5.conv.0 tensor(0.4032)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.5208)
features.6.conv.0 tensor(0.0509)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.1912)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.5330)
features.8.conv.0 tensor(0.6237)
features.8.conv.3 tensor(0.5446)
features.8.conv.6 tensor(0.6568)
features.9.conv.0 tensor(0.5817)
features.9.conv.3 tensor(0.5558)
features.9.conv.6 tensor(0.6833)
features.10.conv.0 tensor(0.0570)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0968)
features.11.conv.0 tensor(0.7714)
features.11.conv.3 tensor(0.6339)
features.11.conv.6 tensor(0.8252)
features.12.conv.0 tensor(0.7720)
features.12.conv.3 tensor(0.6701)
features.12.conv.6 tensor(0.8562)
features.13.conv.0 tensor(0.2975)
features.13.conv.3 tensor(0.4828)
features.13.conv.6 tensor(0.5289)
features.14.conv.0 tensor(0.9241)
features.14.conv.3 tensor(0.8295)
features.14.conv.6 tensor(0.9626)
features.15.conv.0 tensor(0.9050)
features.15.conv.3 tensor(0.8369)
features.15.conv.6 tensor(0.9668)
features.16.conv.0 tensor(0.7063)
features.16.conv.3 tensor(0.8024)
features.16.conv.6 tensor(0.9131)
conv.0 tensor(0.2240)
tensor(1438935.) 2188896.0
0.53064716
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
0.53063864
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
0.53062999
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
0.53062183
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
0.53061742
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
0.53061318
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
0.53061342
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
0.53060776
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
0.53059816
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
0.53060597
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.53059405
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
0.53058213
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
0.53056926
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
0.53056383
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
0.53056389
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
0.53055733
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.53054863
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
0.53054225
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][   20/  196]   Loss 0.083232   Top1 96.953125   Top5 99.960938   BatchTime 0.359385   LR 0.000018
0.53053606
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.53052384
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
0.53052974
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
0.53052264
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
0.53051758
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
0.53051150
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.53049332
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
0.53048760
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.53048795
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
0.53048509
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.53047925
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.53047734
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
0.53046679
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
0.53046781
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
0.53046536
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
0.53045899
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
0.53045630
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
0.53045601
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][   40/  196]   Loss 0.086396   Top1 97.099609   Top5 99.980469   BatchTime 0.340820   LR 0.000018
0.53044927
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
0.53044283
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
0.53043813
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.53043687
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.53043604
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.53043592
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
0.53043795
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
0.53043264
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
0.53042412
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
0.53042626
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
0.53042561
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
0.53042603
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
0.53042388
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.53041571
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.53041393
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
0.53040707
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
0.53039652
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
0.53040063
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
0.53040463
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
0.53040028
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.53039861
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.53039145
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
0.53039032
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][   60/  196]   Loss 0.087129   Top1 97.148438   Top5 99.980469   BatchTime 0.310553   LR 0.000017
0.53038746
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
0.53038722
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.53038484
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
0.53037947
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
0.53037399
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
0.53036815
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
0.53036249
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
0.53035456
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
0.53034663
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
0.53033775
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
0.53033108
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
0.53032243
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
0.53031158
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
0.53030920
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.53030038
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.53029037
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][   80/  196]   Loss 0.089701   Top1 97.045898   Top5 99.985352   BatchTime 0.300748   LR 0.000017
0.53027725
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
0.53027880
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
0.53028274
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
0.53027833
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
0.53027421
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
0.53027374
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
0.53027147
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
0.53027916
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
0.53027457
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
0.53026754
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
0.53026146
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
0.53025955
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
0.53025067
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
0.53024530
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
0.53024614
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
0.53023684
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
0.53023291
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
0.53022814
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
0.53022230
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
0.53022039
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
0.53021801
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.53021234
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
0.53020561
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
0.53020215
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
0.53019637
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
0.53019643
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][  100/  196]   Loss 0.089748   Top1 97.011719   Top5 99.988281   BatchTime 0.303303   LR 0.000017
0.53019255
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
0.53019196
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
0.53018224
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
0.53018087
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
0.53017437
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
0.53017372
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.53016049
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
0.53015822
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.53014922
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
0.53014439
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
0.53013867
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
0.53013450
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.53013384
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
0.53013700
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.53012568
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
0.53011662
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
0.53011167
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][  120/  196]   Loss 0.088767   Top1 97.067057   Top5 99.990234   BatchTime 0.309960   LR 0.000016
0.53010416
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
0.53009993
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
0.53009117
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
0.53008324
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
0.53007662
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
0.53007150
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
0.53006792
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
0.53006196
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.53005189
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
0.53004295
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
0.53003442
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
0.53003299
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.53002596
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.53002203
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
0.53001410
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
0.53001517
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
0.53001362
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
0.53002310
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
0.53002471
INFO - Training [60][  140/  196]   Loss 0.090194   Top1 97.011719   Top5 99.988839   BatchTime 0.310791   LR 0.000016
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.53002614
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.53001398
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
0.53000712
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
0.53000218
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
0.52999574
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.52999049
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
0.52999282
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
0.52998424
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
0.52998531
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
0.52998632
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
0.52999067
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
0.52998835
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
0.52998871
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
0.52998197
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
0.52997994
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
0.52997702
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
0.52997214
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52997005
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.52997434
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
0.52997184
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
0.52996641
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][  160/  196]   Loss 0.091425   Top1 96.953125   Top5 99.987793   BatchTime 0.308234   LR 0.000016
0.52996111
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.52995008
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
0.52994394
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
0.52993292
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
0.52992654
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.52992058
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
0.52991104
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
0.52989882
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
0.52989310
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
0.52989149
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
0.52988762
tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)
0.52987808
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
0.52987301
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52986199
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
0.52985275
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
0.52984977
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
0.52983928
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
0.52982968
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [60][  180/  196]   Loss 0.090480   Top1 96.981337   Top5 99.989149   BatchTime 0.311001   LR 0.000015
0.52982002
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52980477
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.52979076
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
0.52978683
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.52978361
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
0.52977628
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.52977800
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
0.52977210
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
0.52976221
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.52975935
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
0.52974796
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.52974397
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
0.52974427
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
0.52973050
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.52973062
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.52971977
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
0.52971894
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 96.966    Top5: 99.988    Loss: 0.091
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.52971548
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
0.52970845
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
0.52970123
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [60][   20/   40]   Loss 0.408763   Top1 88.906250   Top5 99.511719   BatchTime 0.129990
INFO - Validation [60][   40/   40]   Loss 0.399420   Top1 89.000000   Top5 99.640000   BatchTime 0.090982
INFO - ==> Top1: 89.000    Top5: 99.640    Loss: 0.399
INFO - ==> Sparsity : 0.660
INFO - Scoreboard best 1 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 89.050   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [60][Top1: 89.000   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3229)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0916)
features.2.conv.0 tensor(0.1238)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5903)
features.3.conv.0 tensor(0.0767)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1159)
features.4.conv.0 tensor(0.0926)
features.4.conv.3 tensor(0.2980)
features.4.conv.6 tensor(0.3914)
features.5.conv.0 tensor(0.4071)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.5225)
features.6.conv.0 tensor(0.0501)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0843)
features.7.conv.0 tensor(0.1921)
features.7.conv.3 tensor(0.4586)
features.7.conv.6 tensor(0.5347)
features.8.conv.0 tensor(0.6242)
features.8.conv.3 tensor(0.5448)
features.8.conv.6 tensor(0.6598)
features.9.conv.0 tensor(0.5824)
features.9.conv.3 tensor(0.5558)
features.9.conv.6 tensor(0.6842)
features.10.conv.0 tensor(0.0562)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0965)
features.11.conv.0 tensor(0.7713)
features.11.conv.3 tensor(0.6337)
features.11.conv.6 tensor(0.8262)
features.12.conv.0 tensor(0.7730)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.8567)
features.13.conv.0 tensor(0.3056)
features.13.conv.3 tensor(0.4824)
features.13.conv.6 tensor(0.5334)
features.14.conv.0 tensor(0.9243)
features.14.conv.3 tensor(0.8292)
features.14.conv.6 tensor(0.9626)
features.15.conv.0 tensor(0.9054)
features.15.conv.3 tensor(0.8369)
features.15.conv.6 tensor(0.9669)
features.16.conv.0 tensor(0.7091)
features.16.conv.3 tensor(0.8024)
features.16.conv.6 tensor(0.9138)
conv.0 tensor(0.2307)
tensor(1443626.) 2188896.0
0.52970725
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52970189
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
0.52969551
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
0.52968621
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.52967668
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
0.52966577
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
0.52966261
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
0.52965856
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.52965283
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
0.52964050
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
0.52962089
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
0.52961498
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52960497
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.52959639
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
0.52958387
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.52957994
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
0.52957499
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
0.52956265
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
0.52954912
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][   20/  196]   Loss 0.076218   Top1 97.441406   Top5 99.980469   BatchTime 0.405533   LR 0.000015
0.52953869
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
0.52953315
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.52952576
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.52951813
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
0.52951580
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
0.52951086
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.52950704
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
0.52950543
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
0.52949774
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.52949458
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
0.52949488
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.52949488
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
0.52949047
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
0.52948922
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.52948838
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.52948380
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
0.52946687
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][   40/  196]   Loss 0.079036   Top1 97.373047   Top5 99.990234   BatchTime 0.374234   LR 0.000014
0.52945417
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
0.52945375
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.52944666
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
0.52943736
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
0.52942616
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.52942234
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
0.52941191
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
0.52941006
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
0.52939934
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
0.52939188
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.52938539
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
0.52937889
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.52937901
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
0.52937406
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
0.52936137
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
0.52935606
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
0.52934963
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
0.52933323
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
0.52932423
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
0.52931839
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
0.52931809
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
0.52931428
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.52931213
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52930665
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][   60/  196]   Loss 0.084382   Top1 97.226562   Top5 99.980469   BatchTime 0.362723   LR 0.000014
0.52930164
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
0.52929705
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
0.52929157
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
0.52927846
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
0.52926332
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
0.52925783
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
0.52925259
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
0.52924401
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
0.52923584
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
0.52923000
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.52923250
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
0.52922124
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
0.52922302
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.52921981
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
0.52921575
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
0.52920878
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
0.52920467
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
0.52920312
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
0.52919310
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][   80/  196]   Loss 0.084243   Top1 97.202148   Top5 99.970703   BatchTime 0.349578   LR 0.000014
0.52919543
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
0.52919388
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.52919042
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
0.52918863
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
0.52917969
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
0.52918398
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
0.52917266
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
0.52917153
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
0.52917010
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
0.52918005
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
0.52918565
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
0.52918392
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.52918589
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
0.52917582
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
0.52917790
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
0.52918124
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
0.52917820
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
0.52917325
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][  100/  196]   Loss 0.083584   Top1 97.246094   Top5 99.976562   BatchTime 0.345124   LR 0.000013
0.52916992
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
0.52916580
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
0.52916169
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
0.52915943
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
0.52915329
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.52914935
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.52914745
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
0.52914226
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
0.52914125
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52913922
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
0.52913755
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
0.52913529
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
0.52912658
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
0.52911597
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
0.52910280
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.52910209
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
0.52909726
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
0.52909315
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52908498
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
0.52908045
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.52907139
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
0.52906936
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52906829
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
0.52906376
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][  120/  196]   Loss 0.083870   Top1 97.268880   Top5 99.980469   BatchTime 0.344858   LR 0.000013
0.52904946
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.52904713
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
0.52903074
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.52902925
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.52902943
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
0.52902085
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.52901447
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
0.52901304
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
0.52900594
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
0.52899516
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52899206
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
0.52898365
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
0.52897578
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.52896786
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
0.52896172
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
0.52894843
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
0.52895159
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
0.52894831
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
0.52894348
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][  140/  196]   Loss 0.084481   Top1 97.243304   Top5 99.980469   BatchTime 0.340640   LR 0.000013
0.52894258
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52894098
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
0.52893883
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
0.52893180
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
0.52893299
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.52892673
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
0.52892238
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
0.52891982
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
0.52891690
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
0.52891451
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
0.52891332
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
0.52891278
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
0.52891195
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
0.52890879
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
0.52890915
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
0.52889907
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
0.52889055
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][  160/  196]   Loss 0.085539   Top1 97.224121   Top5 99.980469   BatchTime 0.341266   LR 0.000012
0.52888793
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.52888525
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
0.52888912
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52888197
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.52886975
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.52886760
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
0.52886099
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.52885896
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
0.52884620
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
0.52884203
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52883440
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.52883393
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
0.52883238
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
0.52882969
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
0.52882588
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
0.52883202
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.52883118
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.52883321
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
0.52883393
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
0.52883375
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
0.52883035
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
0.52882797
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.52882224
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
0.52881742
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
0.52881533
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [61][  180/  196]   Loss 0.086755   Top1 97.189670   Top5 99.978299   BatchTime 0.340175   LR 0.000012
0.52880949
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
0.52880937
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
0.52880645
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
0.52880234
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
0.52879602
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
0.52879500
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
0.52879065
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
0.52878082
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
0.52877700
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
0.52877057
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
0.52876800
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
0.52876359
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
0.52876043
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 97.158    Top5: 99.980    Loss: 0.088
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.52876115
tensor(0.2111, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [61][   20/   40]   Loss 0.412433   Top1 88.984375   Top5 99.550781   BatchTime 0.126286
INFO - Validation [61][   40/   40]   Loss 0.401227   Top1 89.080000   Top5 99.710000   BatchTime 0.088810
INFO - ==> Top1: 89.080    Top5: 99.710    Loss: 0.401
INFO - ==> Sparsity : 0.662
INFO - Scoreboard best 1 ==> Epoch [61][Top1: 89.080   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [55][Top1: 89.050   Top5: 99.670]
features.0.conv.0 tensor(0.3194)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0658)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1264)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5909)
features.3.conv.0 tensor(0.0764)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1174)
features.4.conv.0 tensor(0.0921)
features.4.conv.3 tensor(0.2986)
features.4.conv.6 tensor(0.3924)
features.5.conv.0 tensor(0.4103)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.5246)
features.6.conv.0 tensor(0.0490)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0843)
features.7.conv.0 tensor(0.1939)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.5356)
features.8.conv.0 tensor(0.6269)
features.8.conv.3 tensor(0.5440)
features.8.conv.6 tensor(0.6607)
features.9.conv.0 tensor(0.5814)
features.9.conv.3 tensor(0.5564)
features.9.conv.6 tensor(0.6847)
features.10.conv.0 tensor(0.0576)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0947)
features.11.conv.0 tensor(0.7718)
features.11.conv.3 tensor(0.6341)
features.11.conv.6 tensor(0.8262)
features.12.conv.0 tensor(0.7747)
features.12.conv.3 tensor(0.6701)
features.12.conv.6 tensor(0.8575)
features.13.conv.0 tensor(0.3085)
features.13.conv.3 tensor(0.4834)
features.13.conv.6 tensor(0.5398)
features.14.conv.0 tensor(0.9244)
features.14.conv.3 tensor(0.8287)
features.14.conv.6 tensor(0.9627)
features.15.conv.0 tensor(0.9057)
features.15.conv.3 tensor(0.8369)
features.15.conv.6 tensor(0.9669)
features.16.conv.0 tensor(0.7119)
features.16.conv.3 tensor(0.8021)
features.16.conv.6 tensor(0.9145)
conv.0 tensor(0.2372)
tensor(1448092.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
0.52875924
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
0.52875936
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
0.52876252
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
0.52875751
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
0.52875221
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
0.52874643
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.52874571
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.52874058
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
0.52874207
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
0.52874333
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
0.52874017
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
0.52874172
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
0.52874261
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
0.52873427
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
0.52873492
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
0.52873760
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
0.52873391
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
0.52873397
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.52872896
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
0.52872199
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][   20/  196]   Loss 0.089939   Top1 97.128906   Top5 99.980469   BatchTime 0.334564   LR 0.000012
0.52872628
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
0.52872664
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
0.52872837
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
0.52872211
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
0.52871871
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
0.52871299
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
0.52871096
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
0.52871120
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.52871269
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
0.52870101
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
0.52869540
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
0.52868837
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.52868080
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.52868325
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
0.52867967
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
0.52867866
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][   40/  196]   Loss 0.086008   Top1 97.177734   Top5 99.990234   BatchTime 0.292269   LR 0.000011
0.52867419
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
0.52866453
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
0.52865595
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
0.52865124
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
0.52865070
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
0.52865463
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
0.52864808
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
0.52864259
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.52863836
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
0.52863461
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
0.52862757
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
0.52861899
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
0.52860731
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
0.52859217
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.52858210
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.52857381
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
0.52856761
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
0.52856320
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
0.52855247
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.52854604
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
0.52853698
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
0.52854276
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
0.52853703
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][   60/  196]   Loss 0.085748   Top1 97.239583   Top5 99.993490   BatchTime 0.284348   LR 0.000011
0.52853739
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
0.52852523
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
0.52851981
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
0.52851093
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
0.52850586
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
0.52850789
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
0.52850711
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
0.52850360
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.52850389
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
0.52850026
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
0.52850300
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
0.52849913
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
0.52850318
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.52849793
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
0.52849275
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
0.52849346
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
0.52848500
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
0.52847606
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
0.52846885
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
0.52846467
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][   80/  196]   Loss 0.084861   Top1 97.290039   Top5 99.980469   BatchTime 0.288033   LR 0.000011
0.52846402
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
0.52846009
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
0.52845502
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
0.52844977
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
0.52843761
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.52842736
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
0.52841961
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
0.52841312
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
0.52840561
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
0.52839762
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
0.52839422
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
0.52838373
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
0.52838451
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
0.52837038
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
0.52836901
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
0.52836192
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
0.52836126
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
0.52835172
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
0.52834272
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][  100/  196]   Loss 0.087013   Top1 97.257812   Top5 99.976562   BatchTime 0.292907   LR 0.000011
0.52833843
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
0.52833593
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
0.52833009
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
0.52833164
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
0.52832848
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
0.52832049
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.52831638
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
0.52831805
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
0.52830899
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.52830774
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
0.52830470
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
0.52829504
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
0.52829272
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
0.52828264
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
0.52827352
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
0.52826720
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
0.52826405
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.52825433
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.52825129
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.52824777
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
0.52823550
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][  120/  196]   Loss 0.085587   Top1 97.262370   Top5 99.980469   BatchTime 0.291823   LR 0.000010
0.52821332
tensor(0.1498, device='cuda:0', grad_fn=<AddBackward0>)
0.52821326
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
0.52821398
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
0.52820635
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
0.52820468
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
0.52819884
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
0.52819616
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.52819216
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
0.52819371
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
0.52819288
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
0.52819115
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.52818984
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
0.52818131
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
0.52817953
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
0.52817410
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
0.52817422
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
0.52817070
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.52815807
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
0.52815425
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
0.52814686
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
0.52813756
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][  140/  196]   Loss 0.084794   Top1 97.290737   Top5 99.980469   BatchTime 0.291068   LR 0.000010
0.52813357
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
0.52812678
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
0.52812314
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
0.52812082
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.52811569
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
0.52811748
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
0.52812248
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
0.52811718
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.52812243
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
0.52811980
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
0.52812213
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
0.52811801
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
0.52811211
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
0.52810580
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52810222
tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)
0.52809614
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
0.52809668
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
0.52809864
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
0.52810371
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
0.52809674
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][  160/  196]   Loss 0.084162   Top1 97.290039   Top5 99.980469   BatchTime 0.292229   LR 0.000010
0.52809125
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.52808619
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
0.52808607
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
0.52807975
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
0.52807492
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
0.52807623
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
0.52807200
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
0.52806866
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
0.52806294
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
0.52805823
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
0.52806240
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.52806431
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
0.52806157
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
0.52805865
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
0.52806115
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.52806109
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
0.52806091
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
0.52806181
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [62][  180/  196]   Loss 0.084351   Top1 97.263455   Top5 99.980469   BatchTime 0.296174   LR 0.000009
0.52805972
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
0.52806157
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
0.52806205
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
0.52806473
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
0.52806288
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.52806240
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52805668
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
0.52805018
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
0.52804750
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
0.52804065
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
0.52803868
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.52803433
tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)
0.52802151
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
0.52802062
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
0.52801901
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
0.52802002
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
0.52801907
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
0.52802080
tensor(0.1827, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.242    Top5: 99.980    Loss: 0.085
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [62][   20/   40]   Loss 0.406944   Top1 88.984375   Top5 99.531250   BatchTime 0.119005
INFO - Validation [62][   40/   40]   Loss 0.396032   Top1 89.160000   Top5 99.700000   BatchTime 0.086331
INFO - ==> Top1: 89.160    Top5: 99.700    Loss: 0.396
INFO - ==> Sparsity : 0.663
INFO - Scoreboard best 1 ==> Epoch [62][Top1: 89.160   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 89.080   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0658)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0890)
features.2.conv.0 tensor(0.1264)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.5920)
features.3.conv.0 tensor(0.0767)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1181)
features.4.conv.0 tensor(0.0913)
features.4.conv.3 tensor(0.2975)
features.4.conv.6 tensor(0.3936)
features.5.conv.0 tensor(0.4116)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.5249)
features.6.conv.0 tensor(0.0488)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0845)
features.7.conv.0 tensor(0.1959)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.5383)
features.8.conv.0 tensor(0.6257)
features.8.conv.3 tensor(0.5448)
features.8.conv.6 tensor(0.6632)
features.9.conv.0 tensor(0.5826)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6855)
features.10.conv.0 tensor(0.0584)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0964)
features.11.conv.0 tensor(0.7717)
features.11.conv.3 tensor(0.6331)
features.11.conv.6 tensor(0.8270)
features.12.conv.0 tensor(0.7745)
features.12.conv.3 tensor(0.6698)
features.12.conv.6 tensor(0.8581)
features.13.conv.0 tensor(0.3156)
features.13.conv.3 tensor(0.4832)
features.13.conv.6 tensor(0.5472)
features.14.conv.0 tensor(0.9244)
features.14.conv.3 tensor(0.8289)
features.14.conv.6 tensor(0.9628)
features.15.conv.0 tensor(0.9059)
features.15.conv.3 tensor(0.8372)
features.15.conv.6 tensor(0.9669)
features.16.conv.0 tensor(0.7138)
features.16.conv.3 tensor(0.8020)
features.16.conv.6 tensor(0.9149)
conv.0 tensor(0.2413)
tensor(1451672.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
0.52802289
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
0.52801794
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
0.52800810
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
0.52800000
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
0.52799761
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
0.52799350
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.52798873
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
0.52797848
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
0.52797312
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
0.52796936
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
0.52796793
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
0.52796322
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
0.52795935
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
0.52795291
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
0.52794635
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
0.52794218
tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)
0.52793270
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][   20/  196]   Loss 0.077427   Top1 97.539062   Top5 99.980469   BatchTime 0.345160   LR 0.000009
0.52792668
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
0.52792311
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
0.52791572
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
0.52790004
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
0.52789444
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
0.52789092
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
0.52788746
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
0.52787822
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
0.52786976
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
0.52786589
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52785140
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
0.52783811
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
0.52783126
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.52782714
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
0.52782720
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
0.52782243
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
0.52781242
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
0.52780300
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
0.52779537
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
0.52778202
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][   40/  196]   Loss 0.079033   Top1 97.480469   Top5 99.990234   BatchTime 0.324345   LR 0.000009
0.52778119
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
0.52776736
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
0.52775979
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
0.52775198
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
0.52773798
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
0.52772397
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
0.52771544
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
0.52771050
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.52770668
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
0.52770615
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
0.52769560
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
0.52769369
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
0.52769798
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
0.52768546
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
0.52767640
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
0.52769130
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
0.52768010
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
0.52766877
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.52767187
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
0.52767450
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
0.52767670
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
0.52767456
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
0.52766764
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
0.52766645
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][   60/  196]   Loss 0.079271   Top1 97.467448   Top5 99.993490   BatchTime 0.325522   LR 0.000008
0.52765614
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52765322
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
0.52764231
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
0.52763903
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
0.52763546
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.52762896
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
0.52762580
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
0.52763176
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
0.52762848
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
0.52762663
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
0.52762079
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.52760965
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
0.52759451
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
0.52759892
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
0.52759683
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
0.52759141
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
0.52758813
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
0.52758116
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][   80/  196]   Loss 0.079459   Top1 97.470703   Top5 99.990234   BatchTime 0.329383   LR 0.000008
0.52757502
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
0.52757746
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
0.52758199
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.52758062
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
0.52758497
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
0.52758700
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
0.52759051
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
0.52758843
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
0.52759159
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
0.52758688
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.52758640
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
0.52758831
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.52759534
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
0.52759182
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
0.52759510
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
0.52759498
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
0.52759737
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
0.52759528
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
0.52759683
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
0.52758813
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][  100/  196]   Loss 0.079330   Top1 97.445312   Top5 99.992188   BatchTime 0.323840   LR 0.000008
0.52758479
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.52758163
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
0.52757174
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
0.52756208
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.52756768
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
0.52755791
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
0.52755398
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
0.52754617
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
0.52754676
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
0.52753705
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
0.52752966
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
0.52752721
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
0.52752280
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
0.52752024
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
0.52751619
tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)
0.52750486
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.52750063
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
0.52749771
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
0.52748924
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
0.52748388
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][  120/  196]   Loss 0.079886   Top1 97.428385   Top5 99.990234   BatchTime 0.321522   LR 0.000008
0.52747512
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
0.52747327
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
0.52747017
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
0.52747083
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
0.52746522
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
0.52746183
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.52745813
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
0.52745533
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
0.52745157
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
0.52745372
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
0.52745634
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
0.52745605
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
0.52745503
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
0.52745014
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
0.52744597
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
0.52744216
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
0.52744889
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
0.52745539
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
0.52745104
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
0.52745259
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
0.52745730
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
0.52744687
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
0.52744526
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][  140/  196]   Loss 0.080366   Top1 97.424665   Top5 99.988839   BatchTime 0.324151   LR 0.000007
0.52744371
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.52744073
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
0.52744311
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
0.52744359
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
0.52744162
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
0.52743900
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
0.52743524
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.52743566
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
0.52743614
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
0.52743405
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
0.52743435
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
0.52743262
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
0.52743530
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
0.52743202
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
0.52742791
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52743202
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
0.52743107
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
0.52743196
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
0.52742946
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][  160/  196]   Loss 0.081276   Top1 97.421875   Top5 99.982910   BatchTime 0.324561   LR 0.000007
0.52742386
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.52741688
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
0.52741766
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
0.52742094
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
0.52741677
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
0.52741188
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
0.52740425
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
0.52740014
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
0.52739853
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.52740359
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
0.52739763
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
0.52739453
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
0.52739173
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
0.52738404
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
0.52738386
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
0.52738535
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52738369
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
0.52737784
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [63][  180/  196]   Loss 0.082265   Top1 97.374132   Top5 99.980469   BatchTime 0.324216   LR 0.000007
0.52737927
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
0.52737641
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
0.52737820
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
0.52737278
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
0.52737451
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
0.52737832
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
0.52736825
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
0.52735883
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
0.52735889
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
0.52735609
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
0.52736074
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
0.52735752
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
0.52735972
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
0.52734995
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
0.52734703
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
0.52734905
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
0.52734685
tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.382    Top5: 99.980    Loss: 0.082
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [63][   20/   40]   Loss 0.408110   Top1 89.140625   Top5 99.550781   BatchTime 0.123258
INFO - Validation [63][   40/   40]   Loss 0.397835   Top1 89.000000   Top5 99.680000   BatchTime 0.089278
INFO - ==> Top1: 89.000    Top5: 99.680    Loss: 0.398
INFO - ==> Sparsity : 0.664
INFO - Scoreboard best 1 ==> Epoch [62][Top1: 89.160   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 89.080   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [51][Top1: 89.070   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0632)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0890)
features.2.conv.0 tensor(0.1288)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.5920)
features.3.conv.0 tensor(0.0767)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1187)
features.4.conv.0 tensor(0.0929)
features.4.conv.3 tensor(0.2969)
features.4.conv.6 tensor(0.3950)
features.5.conv.0 tensor(0.4124)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.5251)
features.6.conv.0 tensor(0.0487)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.1959)
features.7.conv.3 tensor(0.4569)
features.7.conv.6 tensor(0.5392)
features.8.conv.0 tensor(0.6262)
features.8.conv.3 tensor(0.5451)
features.8.conv.6 tensor(0.6646)
features.9.conv.0 tensor(0.5841)
features.9.conv.3 tensor(0.5553)
features.9.conv.6 tensor(0.6858)
features.10.conv.0 tensor(0.0576)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0958)
features.11.conv.0 tensor(0.7718)
features.11.conv.3 tensor(0.6331)
features.11.conv.6 tensor(0.8270)
features.12.conv.0 tensor(0.7755)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.8589)
features.13.conv.0 tensor(0.3182)
features.13.conv.3 tensor(0.4834)
features.13.conv.6 tensor(0.5530)
features.14.conv.0 tensor(0.9246)
features.14.conv.3 tensor(0.8288)
features.14.conv.6 tensor(0.9629)
features.15.conv.0 tensor(0.9060)
features.15.conv.3 tensor(0.8370)
features.15.conv.6 tensor(0.9670)
features.16.conv.0 tensor(0.7153)
features.16.conv.3 tensor(0.8019)
features.16.conv.6 tensor(0.9154)
conv.0 tensor(0.2438)
tensor(1454037.) 2188896.0
0.52734339
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
0.52734184
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
0.52734011
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
0.52734363
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.52733916
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
0.52732956
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
0.52732950
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
0.52732450
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
0.52732432
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.52732861
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
0.52732283
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
0.52732390
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
0.52731580
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
0.52731413
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
0.52730638
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
0.52730137
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.52729911
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
0.52729392
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
0.52728879
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
0.52728939
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
0.52728373
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][   20/  196]   Loss 0.090591   Top1 97.050781   Top5 99.980469   BatchTime 0.343600   LR 0.000007
0.52727759
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
0.52727407
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
0.52727282
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
0.52727038
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
0.52727073
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
0.52726382
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
0.52726620
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
0.52725846
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
0.52725285
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
0.52724499
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.52724266
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
0.52723712
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
0.52722269
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
0.52722490
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
0.52722234
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.52721190
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
0.52720946
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
0.52720326
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
0.52720577
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][   40/  196]   Loss 0.087049   Top1 97.138672   Top5 99.980469   BatchTime 0.329824   LR 0.000006
0.52720261
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
0.52720535
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
0.52720243
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
0.52720106
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
0.52719700
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
0.52718943
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
0.52719104
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
0.52717519
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.52717131
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
0.52717435
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
0.52717471
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
0.52716953
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52716243
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.52716875
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
0.52716309
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
0.52716070
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
0.52715582
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
0.52715594
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
0.52714932
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
0.52714294
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
0.52713841
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][   60/  196]   Loss 0.083171   Top1 97.298177   Top5 99.986979   BatchTime 0.313185   LR 0.000006
0.52713966
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
0.52713507
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
0.52713525
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
0.52713543
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
0.52712899
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
0.52712381
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52712303
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52712262
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
0.52712625
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.52712679
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
0.52712506
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.52711791
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
0.52711487
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
0.52711421
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
0.52711332
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
0.52711052
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
0.52711266
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
0.52710652
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52711171
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
0.52711082
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][   80/  196]   Loss 0.083549   Top1 97.329102   Top5 99.975586   BatchTime 0.307735   LR 0.000006
0.52711004
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
0.52710646
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
0.52710319
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
0.52710420
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
0.52710909
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
0.52710396
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
0.52709991
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.52709401
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
0.52709651
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
0.52709180
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
0.52708817
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.52708596
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
0.52707809
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
0.52708101
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.52707851
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
0.52707785
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
0.52707392
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
0.52706820
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][  100/  196]   Loss 0.083658   Top1 97.289062   Top5 99.980469   BatchTime 0.312756   LR 0.000006
0.52706844
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
0.52706909
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
0.52706236
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.52705741
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
0.52705622
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.52705151
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
0.52705348
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52705306
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
0.52704328
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
0.52703923
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
0.52703559
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
0.52703542
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
0.52703130
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
0.52703029
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
0.52703434
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
0.52703524
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
0.52703714
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
0.52703363
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][  120/  196]   Loss 0.082777   Top1 97.333984   Top5 99.980469   BatchTime 0.316684   LR 0.000006
0.52703351
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
0.52703434
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
0.52703553
tensor(0.1526, device='cuda:0', grad_fn=<AddBackward0>)
0.52703583
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
0.52703530
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
0.52703458
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.52702898
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
0.52702808
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.52702975
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
0.52703232
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
0.52703869
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
0.52703154
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
0.52702790
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
0.52702165
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
0.52701759
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
0.52701914
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
0.52701324
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
0.52700895
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52700508
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
0.52700335
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52700472
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
0.52700436
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
0.52699840
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
0.52699900
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
0.52699465
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][  140/  196]   Loss 0.082443   Top1 97.340960   Top5 99.983259   BatchTime 0.316647   LR 0.000005
0.52700096
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
0.52699536
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
0.52699542
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
0.52698660
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52698946
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
0.52698523
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
0.52698278
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
0.52698499
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
0.52698362
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
0.52698481
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
0.52698553
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
0.52698684
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.52698350
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
0.52698362
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
0.52698225
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
0.52697742
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
0.52697259
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
0.52697235
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.52697521
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][  160/  196]   Loss 0.082165   Top1 97.343750   Top5 99.982910   BatchTime 0.316785   LR 0.000005
0.52697045
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
0.52696383
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.52696306
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
0.52696276
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
0.52695704
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.52695394
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.52695096
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
0.52695262
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
0.52694863
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
0.52694696
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
0.52694327
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
0.52694422
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
0.52694076
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
0.52693963
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
0.52693707
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
0.52693456
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
0.52692771
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
0.52693379
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [64][  180/  196]   Loss 0.082264   Top1 97.343750   Top5 99.982639   BatchTime 0.319478   LR 0.000005
0.52692884
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
0.52692437
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
0.52691841
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
0.52691668
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
0.52692145
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
0.52691442
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
0.52691549
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.52691090
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52691019
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
0.52690071
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52689797
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
0.52689713
tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)
0.52689379
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.52689189
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.52688611
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
0.52688485
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
0.52689004
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.386    Top5: 99.984    Loss: 0.082
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [64][   20/   40]   Loss 0.412081   Top1 89.199219   Top5 99.492188   BatchTime 0.119948
INFO - Validation [64][   40/   40]   Loss 0.399872   Top1 89.270000   Top5 99.640000   BatchTime 0.087549
INFO - ==> Top1: 89.270    Top5: 99.640    Loss: 0.400
INFO - ==> Sparsity : 0.665
INFO - Scoreboard best 1 ==> Epoch [64][Top1: 89.270   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 89.160   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 89.080   Top5: 99.710]
features.0.conv.0 tensor(0.3125)
features.0.conv.3 tensor(0.3438)
features.1.conv.0 tensor(0.0638)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.5917)
features.3.conv.0 tensor(0.0775)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1181)
features.4.conv.0 tensor(0.0934)
features.4.conv.3 tensor(0.2969)
features.4.conv.6 tensor(0.3953)
features.5.conv.0 tensor(0.4144)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.5257)
features.6.conv.0 tensor(0.0485)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0843)
features.7.conv.0 tensor(0.1968)
features.7.conv.3 tensor(0.4566)
features.7.conv.6 tensor(0.5403)
features.8.conv.0 tensor(0.6267)
features.8.conv.3 tensor(0.5446)
features.8.conv.6 tensor(0.6652)
features.9.conv.0 tensor(0.5854)
features.9.conv.3 tensor(0.5550)
features.9.conv.6 tensor(0.6864)
features.10.conv.0 tensor(0.0581)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0971)
features.11.conv.0 tensor(0.7719)
features.11.conv.3 tensor(0.6331)
features.11.conv.6 tensor(0.8273)
features.12.conv.0 tensor(0.7763)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.8597)
features.13.conv.0 tensor(0.3182)
features.13.conv.3 tensor(0.4826)
features.13.conv.6 tensor(0.5559)
features.14.conv.0 tensor(0.9247)
features.14.conv.3 tensor(0.8288)
features.14.conv.6 tensor(0.9630)
features.15.conv.0 tensor(0.9061)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.7166)
features.16.conv.3 tensor(0.8017)
features.16.conv.6 tensor(0.9158)
conv.0 tensor(0.2461)
tensor(1455937.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
0.52690154
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52690208
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
0.52689183
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.52688879
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
0.52688444
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
0.52688229
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
0.52688468
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
0.52688521
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
0.52688646
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.52688009
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
0.52688164
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.52687329
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
0.52686864
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
0.52686793
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
0.52686441
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
0.52686477
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52687150
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
0.52687365
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
0.52686918
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][   20/  196]   Loss 0.081299   Top1 97.265625   Top5 99.980469   BatchTime 0.316570   LR 0.000005
0.52685851
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
0.52685505
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
0.52684635
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
0.52683908
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
0.52683920
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
0.52683717
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
0.52683550
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
0.52682865
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
0.52682579
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
0.52682602
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
0.52681953
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
0.52681404
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
0.52681237
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
0.52681094
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
0.52681315
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
0.52682000
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
0.52680457
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
0.52679861
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
0.52679497
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
0.52679276
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
0.52678025
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][   40/  196]   Loss 0.080061   Top1 97.470703   Top5 99.980469   BatchTime 0.303492   LR 0.000004
0.52677298
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.52676660
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
0.52676922
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
0.52677083
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
0.52677274
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
0.52677399
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
0.52678299
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
0.52677727
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
0.52677113
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
0.52677953
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
0.52677035
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
0.52677172
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
0.52676797
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
0.52676672
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
0.52677023
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
0.52677041
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
0.52678114
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
0.52676922
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][   60/  196]   Loss 0.078100   Top1 97.584635   Top5 99.986979   BatchTime 0.310038   LR 0.000004
0.52675933
tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>)
0.52675915
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.52676433
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
0.52675819
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
0.52675426
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.52675164
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.52675045
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
0.52674466
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
0.52673930
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
0.52673799
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.52674675
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
0.52674693
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
0.52673894
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
0.52674335
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
0.52673811
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
0.52673990
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
0.52673531
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
0.52673596
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
0.52672833
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
0.52672940
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
0.52672875
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][   80/  196]   Loss 0.079245   Top1 97.529297   Top5 99.980469   BatchTime 0.305036   LR 0.000004
0.52673548
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
0.52672631
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
0.52672756
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
0.52673745
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52673525
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
0.52674067
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.52673286
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
0.52673274
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
0.52673137
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
0.52673072
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
0.52672726
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
0.52672678
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
0.52672136
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
0.52671838
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
0.52671695
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
0.52671832
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
0.52671868
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
0.52672249
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.52670932
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][  100/  196]   Loss 0.077992   Top1 97.566406   Top5 99.984375   BatchTime 0.306907   LR 0.000004
0.52670109
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
0.52671039
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
0.52670926
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52670974
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
0.52670795
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
0.52670741
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.52670628
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.52669686
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
0.52669209
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
0.52668899
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
0.52668977
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.52668709
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
0.52668047
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
0.52667868
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
0.52668077
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
0.52668190
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
0.52668256
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
0.52668244
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
0.52667975
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
0.52667630
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][  120/  196]   Loss 0.077396   Top1 97.555339   Top5 99.983724   BatchTime 0.307800   LR 0.000004
0.52667099
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
0.52666754
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.52666146
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
0.52666473
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
0.52666473
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
0.52666312
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
0.52665699
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
0.52665764
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
0.52665156
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
0.52665627
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
0.52665341
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
0.52664924
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
0.52663797
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
0.52664071
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
0.52664262
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
0.52663755
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
0.52664524
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
0.52664608
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
0.52664399
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][  140/  196]   Loss 0.077751   Top1 97.539062   Top5 99.983259   BatchTime 0.308953   LR 0.000004
0.52663511
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
0.52663964
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
0.52663946
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
0.52664590
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
0.52663851
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.52664286
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
0.52664149
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.52663904
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
0.52663410
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
0.52663678
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
0.52662587
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
0.52661967
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
0.52661830
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
0.52661979
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
0.52661526
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
0.52661747
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
0.52661663
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
0.52662188
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.52662760
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.52661741
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
0.52661842
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
0.52661502
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
0.52661377
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
0.52660954
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][  160/  196]   Loss 0.079092   Top1 97.487793   Top5 99.982910   BatchTime 0.312222   LR 0.000003
0.52660507
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52660280
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
0.52660042
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
0.52660656
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
0.52660465
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
0.52660877
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
0.52660537
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
0.52659744
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
0.52659649
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
0.52659589
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.52659440
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
0.52658522
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
0.52659178
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
0.52658880
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
0.52658850
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
0.52658439
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [65][  180/  196]   Loss 0.079139   Top1 97.478299   Top5 99.982639   BatchTime 0.305978   LR 0.000003
0.52658731
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
0.52658665
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
0.52658457
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
0.52658147
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
0.52658421
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
0.52658719
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
0.52657664
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
0.52657425
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
0.52657300
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.52656388
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
0.52656150
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
0.52656180
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
0.52655566
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
0.52655137
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
0.52654833
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
0.52654940
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 97.464    Top5: 99.984    Loss: 0.079
0.52654755
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
0.52654403
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
0.52654266
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [65][   20/   40]   Loss 0.411782   Top1 88.964844   Top5 99.550781   BatchTime 0.123968
features.0.conv.0 tensor(0.3160)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0658)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0916)
features.2.conv.0 tensor(0.1305)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5923)
features.3.conv.0 tensor(0.0781)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1185)
features.4.conv.0 tensor(0.0938)
features.4.conv.3 tensor(0.2969)
features.4.conv.6 tensor(0.3958)
features.5.conv.0 tensor(0.4142)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.5257)
features.6.conv.0 tensor(0.0485)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0840)
features.7.conv.0 tensor(0.1971)
features.7.conv.3 tensor(0.4557)
features.7.conv.6 tensor(0.5404)
features.8.conv.0 tensor(0.6270)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.6663)
features.9.conv.0 tensor(0.5857)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6868)
features.10.conv.0 tensor(0.0579)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0988)
features.11.conv.0 tensor(0.7724)
features.11.conv.3 tensor(0.6331)
features.11.conv.6 tensor(0.8273)
features.12.conv.0 tensor(0.7769)
features.12.conv.3 tensor(0.6696)
features.12.conv.6 tensor(0.8598)
features.13.conv.0 tensor(0.3204)
features.13.conv.3 tensor(0.4823)
features.13.conv.6 tensor(0.5581)
features.14.conv.0 tensor(0.9248)
features.14.conv.3 tensor(0.8288)
features.14.conv.6 tensor(0.9631)
features.15.conv.0 tensor(0.9062)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.7175)
features.16.conv.3 tensor(0.8017)
features.16.conv.6 tensor(0.9160)
conv.0 tensor(0.2477)
tensor(1457328.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.395995   Top1 89.190000   Top5 99.660000   BatchTime 0.088388
INFO - ==> Top1: 89.190    Top5: 99.660    Loss: 0.396
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [64][Top1: 89.270   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [65][Top1: 89.190   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [62][Top1: 89.160   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
0.52654201
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
0.52654785
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
0.52654868
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
0.52655274
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
0.52655637
tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>)
0.52655685
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
0.52655250
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.52655119
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.52655238
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
0.52654636
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
0.52654928
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
0.52655292
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
0.52655089
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
0.52654797
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
0.52654934
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
0.52654749
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
0.52655143
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
0.52654415
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
0.52654225
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
0.52654457
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][   20/  196]   Loss 0.083133   Top1 97.246094   Top5 99.960938   BatchTime 0.399451   LR 0.000003
0.52655029
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
0.52654523
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
0.52654433
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
0.52654058
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
0.52653533
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
0.52652973
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.52652884
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
0.52652359
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
0.52652711
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
0.52652729
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
0.52652091
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
0.52651978
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
0.52651513
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.52651864
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
0.52652103
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
0.52651876
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
0.52651811
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
0.52651429
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][   40/  196]   Loss 0.084162   Top1 97.285156   Top5 99.970703   BatchTime 0.365698   LR 0.000003
0.52651364
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
0.52650821
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
0.52650899
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
0.52650410
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
0.52650756
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
0.52650797
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52650440
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
0.52650088
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
0.52650034
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
0.52650440
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.52649844
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
0.52649319
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
0.52649534
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
0.52649963
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
0.52649856
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
0.52649468
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
0.52649438
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
0.52648902
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
0.52649355
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.52649373
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
0.52649534
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
0.52649742
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
0.52649635
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.52649254
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][   60/  196]   Loss 0.080106   Top1 97.428385   Top5 99.973958   BatchTime 0.354815   LR 0.000003
0.52649480
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.52649480
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
0.52649057
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
0.52648014
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
0.52649117
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
0.52648807
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
0.52647740
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
0.52648342
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
0.52648163
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
0.52647728
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
0.52647400
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
0.52648121
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
0.52648431
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
0.52648354
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
0.52648085
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
0.52647549
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.52647197
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
0.52646691
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][   80/  196]   Loss 0.080751   Top1 97.480469   Top5 99.980469   BatchTime 0.352750   LR 0.000002
0.52646053
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
0.52646053
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
0.52645350
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
0.52645034
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
0.52644426
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.52644688
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
0.52644157
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
0.52644211
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
0.52644110
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
0.52644086
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
0.52643460
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
0.52643609
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
0.52643722
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
0.52643973
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
0.52643698
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
0.52643365
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
0.52643418
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
0.52643090
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
0.52642846
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
0.52642411
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
0.52642393
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][  100/  196]   Loss 0.080062   Top1 97.488281   Top5 99.980469   BatchTime 0.339371   LR 0.000002
0.52642065
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
0.52642435
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
0.52642643
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
0.52642024
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
0.52641946
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
0.52641433
tensor(0.1914, device='cuda:0', grad_fn=<AddBackward0>)
0.52641201
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.52641064
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
0.52640653
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.52640969
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
0.52641183
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
0.52641362
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
0.52642095
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
0.52642196
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
0.52642387
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
0.52642292
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
0.52641565
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.52641153
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][  120/  196]   Loss 0.080867   Top1 97.447917   Top5 99.983724   BatchTime 0.337269   LR 0.000002
0.52641380
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
0.52642637
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.52642536
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
0.52642351
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
0.52642357
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
0.52642828
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
0.52642304
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
0.52642167
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
0.52641922
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
0.52641398
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
0.52641332
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
0.52641624
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.52641886
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
0.52641308
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.52640969
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
0.52641147
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
0.52640826
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
0.52640468
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][  140/  196]   Loss 0.080644   Top1 97.463728   Top5 99.980469   BatchTime 0.336537   LR 0.000002
0.52640623
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
0.52640265
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.52639914
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
0.52639574
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52638602
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
0.52639467
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
0.52639198
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
0.52639329
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
0.52639389
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
0.52639270
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
0.52639598
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52639431
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
0.52639097
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
0.52638894
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
0.52638489
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
0.52637917
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
0.52637637
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
0.52637190
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
0.52636772
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
0.52637428
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
0.52637798
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
0.52637523
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
0.52637082
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
0.52637160
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][  160/  196]   Loss 0.079266   Top1 97.519531   Top5 99.980469   BatchTime 0.336215   LR 0.000002
0.52636582
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
0.52636147
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
0.52636552
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
0.52636975
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
0.52637398
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
0.52637416
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
0.52637631
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
0.52636677
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
0.52636224
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
0.52636594
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.52636063
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
0.52636421
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
0.52635622
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
0.52635258
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.52635908
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
0.52635676
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
0.52635181
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
0.52635562
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
0.52634269
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [66][  180/  196]   Loss 0.078882   Top1 97.523872   Top5 99.982639   BatchTime 0.334460   LR 0.000002
0.52634442
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
0.52634913
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
0.52634251
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
0.52634293
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
0.52634317
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
0.52634436
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
0.52633721
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
0.52633899
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52633697
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
0.52634102
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
0.52634507
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
0.52634126
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
0.52633035
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
0.52633131
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
0.52633017
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
0.52633005
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.520    Top5: 99.984    Loss: 0.079
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [66][   20/   40]   Loss 0.415881   Top1 89.140625   Top5 99.570312   BatchTime 0.132037
INFO - Validation [66][   40/   40]   Loss 0.400383   Top1 89.280000   Top5 99.690000   BatchTime 0.093366
INFO - ==> Top1: 89.280    Top5: 99.690    Loss: 0.400
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [66][Top1: 89.280   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [64][Top1: 89.270   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [65][Top1: 89.190   Top5: 99.660]
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0664)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1317)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5920)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1185)
features.4.conv.0 tensor(0.0942)
features.4.conv.3 tensor(0.2969)
features.4.conv.6 tensor(0.3960)
features.5.conv.0 tensor(0.4147)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.5262)
features.6.conv.0 tensor(0.0482)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0845)
features.7.conv.0 tensor(0.1973)
features.7.conv.3 tensor(0.4557)
features.7.conv.6 tensor(0.5406)
features.8.conv.0 tensor(0.6274)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.6667)
features.9.conv.0 tensor(0.5865)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6868)
features.10.conv.0 tensor(0.0583)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0994)
features.11.conv.0 tensor(0.7726)
features.11.conv.3 tensor(0.6329)
features.11.conv.6 tensor(0.8273)
features.12.conv.0 tensor(0.7770)
features.12.conv.3 tensor(0.6696)
features.12.conv.6 tensor(0.8598)
features.13.conv.0 tensor(0.3221)
features.13.conv.3 tensor(0.4824)
features.13.conv.6 tensor(0.5594)
features.14.conv.0 tensor(0.9248)
features.14.conv.3 tensor(0.8288)
features.14.conv.6 tensor(0.9631)
features.15.conv.0 tensor(0.9062)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.7181)
features.16.conv.3 tensor(0.8019)
features.16.conv.6 tensor(0.9162)
conv.0 tensor(0.2489)
tensor(1458324.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
0.52632207
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
0.52631986
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
0.52631772
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
0.52631778
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
0.52631682
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
0.52631879
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.52631599
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.52632856
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
0.52632606
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.52632272
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
0.52632910
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
0.52632672
tensor(0.0270, device='cuda:0', grad_fn=<AddBackward0>)
0.52632916
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
0.52632499
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
0.52632576
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
0.52632165
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
0.52631879
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
0.52632123
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][   20/  196]   Loss 0.072928   Top1 97.656250   Top5 100.000000   BatchTime 0.329857   LR 0.000002
0.52631748
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
0.52631575
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
0.52631640
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
0.52631193
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
0.52630961
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
0.52631176
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
0.52630919
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
0.52630997
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
0.52630043
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52630514
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
0.52630359
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
0.52630270
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
0.52629912
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
0.52630287
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
0.52629906
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
0.52629262
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
0.52629048
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
0.52629483
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
0.52629149
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
0.52629143
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][   40/  196]   Loss 0.076564   Top1 97.597656   Top5 99.990234   BatchTime 0.316913   LR 0.000002
0.52629077
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
0.52628565
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
0.52629119
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
0.52628917
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
0.52629155
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
0.52628732
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
0.52628595
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
0.52628011
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.52628189
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.52627993
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
0.52628440
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
0.52628595
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
0.52628326
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
0.52628666
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
0.52628285
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
0.52628112
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
0.52628529
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
0.52628285
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.52628380
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
0.52628261
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.52627939
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][   60/  196]   Loss 0.076398   Top1 97.591146   Top5 99.993490   BatchTime 0.306946   LR 0.000001
0.52628136
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
0.52627867
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
0.52628237
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
0.52627617
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
0.52627957
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
0.52627647
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
0.52628011
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
0.52628189
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
0.52628028
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
0.52628237
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
0.52627903
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
0.52628529
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
0.52628374
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52628201
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.52627921
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
0.52628571
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
0.52627897
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
0.52628171
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
0.52628380
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
0.52628303
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][   80/  196]   Loss 0.075171   Top1 97.607422   Top5 99.990234   BatchTime 0.304174   LR 0.000001
0.52628183
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
0.52628237
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
0.52628326
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
0.52628320
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
0.52628547
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
0.52628481
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
0.52628684
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
0.52628654
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.52628052
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
0.52627742
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
0.52628225
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
0.52628380
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.52628690
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
0.52629226
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52629149
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.52628237
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52628565
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
0.52628869
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
0.52628797
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
0.52628607
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][  100/  196]   Loss 0.077093   Top1 97.527344   Top5 99.984375   BatchTime 0.303267   LR 0.000001
0.52628589
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
0.52628762
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52629280
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
0.52628922
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
0.52628928
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
0.52629036
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
0.52628112
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.52628231
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
0.52627903
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
0.52627140
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
0.52627617
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
0.52627575
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.52627510
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.52626884
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
0.52625942
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
0.52625591
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
0.52626061
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
0.52625376
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
0.52625543
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
0.52624738
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][  120/  196]   Loss 0.079011   Top1 97.480469   Top5 99.986979   BatchTime 0.304498   LR 0.000001
0.52624232
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
0.52623868
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
0.52624309
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
0.52624977
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.52625757
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
0.52626026
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
0.52625251
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
0.52624905
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.52624530
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
0.52623981
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
0.52623951
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
0.52623707
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
0.52624679
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
0.52625620
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
0.52626103
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.52626127
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
0.52626121
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.52626395
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
0.52625698
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
0.52625293
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
0.52625370
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.52625829
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
0.52625656
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
0.52625310
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.52625495
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][  140/  196]   Loss 0.078335   Top1 97.539062   Top5 99.986049   BatchTime 0.306925   LR 0.000001
0.52625424
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
0.52625835
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
0.52626145
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
0.52626121
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.52625901
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
0.52625060
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52625322
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
0.52625495
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
0.52625096
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
0.52625233
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
0.52624720
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
0.52624589
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
0.52624607
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
0.52624649
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
0.52625197
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
0.52624679
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
0.52625525
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][  160/  196]   Loss 0.076928   Top1 97.597656   Top5 99.987793   BatchTime 0.310577   LR 0.000001
0.52626359
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
0.52626276
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
0.52625138
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
0.52624321
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
0.52623928
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.52624017
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
0.52624792
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
0.52625114
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
0.52625477
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
0.52625442
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
0.52625275
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
0.52625477
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
0.52625060
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.52625448
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
0.52625006
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
0.52625132
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
0.52624309
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
0.52623963
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
0.52625060
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
0.52624804
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [67][  180/  196]   Loss 0.077849   Top1 97.567274   Top5 99.986979   BatchTime 0.311058   LR 0.000001
0.52624786
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
0.52625287
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
0.52624834
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
0.52623707
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
0.52623856
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
0.52623445
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
0.52623904
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.52623755
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
0.52624130
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
0.52623260
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
0.52622819
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
0.52622652
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
0.52623034
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
0.52622789
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
0.52623922
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.552    Top5: 99.988    Loss: 0.078
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [67][   20/   40]   Loss 0.407602   Top1 89.453125   Top5 99.550781   BatchTime 0.123527
INFO - Validation [67][   40/   40]   Loss 0.399629   Top1 89.240000   Top5 99.670000   BatchTime 0.089149
features.0.conv.0 tensor(0.3125)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0664)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1319)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5923)
features.3.conv.0 tensor(0.0775)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1183)
features.4.conv.0 tensor(0.0944)
features.4.conv.3 tensor(0.2963)
features.4.conv.6 tensor(0.3960)
features.5.conv.0 tensor(0.4149)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.5262)
features.6.conv.0 tensor(0.0488)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.1974)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.5407)
features.8.conv.0 tensor(0.6276)
features.8.conv.3 tensor(0.5448)
features.8.conv.6 tensor(0.6670)
features.9.conv.0 tensor(0.5864)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6870)
features.10.conv.0 tensor(0.0580)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0996)
features.11.conv.0 tensor(0.7726)
features.11.conv.3 tensor(0.6329)
features.11.conv.6 tensor(0.8273)
features.12.conv.0 tensor(0.7772)
features.12.conv.3 tensor(0.6698)
features.12.conv.6 tensor(0.8600)
features.13.conv.0 tensor(0.3228)
features.13.conv.3 tensor(0.4824)
features.13.conv.6 tensor(0.5597)
features.14.conv.0 tensor(0.9248)
features.14.conv.3 tensor(0.8288)
features.14.conv.6 tensor(0.9631)
features.15.conv.0 tensor(0.9062)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.7182)
features.16.conv.3 tensor(0.8019)
features.16.conv.6 tensor(0.9163)
conv.0 tensor(0.2493)
tensor(1458615.) 2188896.0
INFO - ==> Top1: 89.240    Top5: 99.670    Loss: 0.400
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [66][Top1: 89.280   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [64][Top1: 89.270   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [67][Top1: 89.240   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
0.52624410
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
0.52624047
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
0.52624208
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
0.52623624
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
0.52623814
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
0.52623266
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
0.52622712
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52623254
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
0.52623415
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
0.52623445
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
0.52623546
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
0.52623343
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
0.52623338
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
0.52623504
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
0.52623802
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
0.52624214
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
0.52625084
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
0.52625251
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][   20/  196]   Loss 0.086511   Top1 97.304688   Top5 100.000000   BatchTime 0.380081   LR 0.000001
0.52623874
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
0.52623522
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
0.52622944
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
0.52622944
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
0.52622825
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.52622449
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
0.52622706
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
0.52622890
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
0.52622151
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
0.52621680
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
0.52621359
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
0.52621388
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
0.52621055
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
0.52621776
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
0.52621710
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
0.52621156
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
0.52621371
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
0.52621317
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
0.52621192
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
0.52620965
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][   40/  196]   Loss 0.080740   Top1 97.402344   Top5 100.000000   BatchTime 0.338225   LR 0.000001
0.52621955
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
0.52621543
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
0.52621073
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
0.52620113
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
0.52621406
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
0.52621019
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
0.52621084
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.52621323
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
0.52621257
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
0.52621973
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
0.52621704
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
0.52622479
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
0.52621990
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
0.52622092
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
0.52621615
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
0.52620751
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
0.52620590
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
0.52620721
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
0.52620196
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
0.52620292
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
0.52620631
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
0.52619916
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
0.52621078
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][   60/  196]   Loss 0.078470   Top1 97.467448   Top5 99.986979   BatchTime 0.340714   LR 0.000001
0.52621269
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
0.52620900
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.52621800
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
0.52622461
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.52622807
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
0.52622819
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
0.52621990
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
0.52622187
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
0.52621698
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
0.52622169
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.52621979
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
0.52622169
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
0.52622110
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
0.52622414
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
0.52622092
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
0.52621782
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
0.52621669
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
0.52621740
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
0.52622259
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][   80/  196]   Loss 0.079097   Top1 97.485352   Top5 99.990234   BatchTime 0.335969   LR 0.000000
0.52621078
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
0.52622360
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
0.52622288
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
0.52621806
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
0.52622896
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
0.52622420
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
0.52621877
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
0.52621377
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
0.52621067
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
0.52621084
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
0.52620882
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
0.52621299
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
0.52621120
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
0.52620810
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
0.52620935
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
0.52620888
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
0.52621078
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
0.52621514
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
0.52621335
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][  100/  196]   Loss 0.078347   Top1 97.515625   Top5 99.984375   BatchTime 0.331430   LR 0.000000
0.52621341
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
0.52621573
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
0.52621055
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
0.52620840
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
0.52620173
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
0.52619421
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
0.52619863
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
0.52618980
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
0.52618265
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
0.52618414
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
0.52618444
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
0.52618164
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
0.52618325
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
0.52619028
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
0.52618921
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
0.52619064
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
0.52618271
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
0.52618116
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
0.52617604
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
0.52617615
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
0.52618003
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
0.52618498
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][  120/  196]   Loss 0.078280   Top1 97.503255   Top5 99.983724   BatchTime 0.323028   LR 0.000000
0.52618551
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
0.52618992
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
0.52618468
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
0.52618724
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
0.52618206
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
0.52618426
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
0.52618128
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
0.52618581
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
0.52618521
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
0.52619129
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
0.52619290
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
0.52618796
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
0.52619129
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
0.52618676
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.52619439
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
0.52619493
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
0.52618933
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
0.52619028
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][  140/  196]   Loss 0.077361   Top1 97.544643   Top5 99.986049   BatchTime 0.323125   LR 0.000000
0.52619159
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
0.52618921
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
0.52618802
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
0.52618623
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
0.52619034
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
0.52619070
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
0.52619117
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
0.52619034
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
0.52618885
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
0.52619088
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
0.52619451
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
0.52619511
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
0.52619261
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
0.52619070
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
0.52619451
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
0.52620059
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
0.52619892
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
0.52619308
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
0.52619779
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
0.52620065
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
0.52620387
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][  160/  196]   Loss 0.077118   Top1 97.561035   Top5 99.985352   BatchTime 0.318892   LR 0.000000
0.52620232
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
0.52620494
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
0.52620614
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
0.52619368
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
0.52619123
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
0.52619082
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
0.52618498
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
0.52618706
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
0.52618843
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
0.52618986
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
0.52618647
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
0.52618545
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
0.52618557
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.52618092
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
0.52617842
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
0.52618182
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
0.52618200
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.52618343
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
0.52618396
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
0.52618670
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
0.52618587
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [68][  180/  196]   Loss 0.077841   Top1 97.508681   Top5 99.986979   BatchTime 0.314755   LR 0.000000
0.52618688
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.52619129
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
0.52619410
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
0.52619702
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.52619338
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
0.52618843
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.52618653
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
0.52618939
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.52618259
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
0.52617896
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52618277
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
0.52618462
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
0.52618819
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
0.52618808
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
0.52619058
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
INFO - ==> Top1: 97.500    Top5: 99.988    Loss: 0.078
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [68][   20/   40]   Loss 0.413959   Top1 89.238281   Top5 99.453125   BatchTime 0.135274
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0671)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1319)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5929)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1183)
features.4.conv.0 tensor(0.0942)
features.4.conv.3 tensor(0.2963)
features.4.conv.6 tensor(0.3960)
features.5.conv.0 tensor(0.4150)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.5264)
features.6.conv.0 tensor(0.0488)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0844)
features.7.conv.0 tensor(0.1973)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.5409)
features.8.conv.0 tensor(0.6276)
features.8.conv.3 tensor(0.5448)
features.8.conv.6 tensor(0.6670)
features.9.conv.0 tensor(0.5869)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6871)
features.10.conv.0 tensor(0.0581)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0997)
features.11.conv.0 tensor(0.7726)
features.11.conv.3 tensor(0.6329)
features.11.conv.6 tensor(0.8273)
features.12.conv.0 tensor(0.7773)
features.12.conv.3 tensor(0.6698)
features.12.conv.6 tensor(0.8600)
features.13.conv.0 tensor(0.3231)
features.13.conv.3 tensor(0.4824)
features.13.conv.6 tensor(0.5598)
features.14.conv.0 tensor(0.9248)
features.14.conv.3 tensor(0.8288)
features.14.conv.6 tensor(0.9631)
features.15.conv.0 tensor(0.9062)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.7183)
features.16.conv.3 tensor(0.8019)
features.16.conv.6 tensor(0.9163)
conv.0 tensor(0.2494)
tensor(1458749.) 2188896.0
INFO - Validation [68][   40/   40]   Loss 0.400405   Top1 89.110000   Top5 99.610000   BatchTime 0.095535
INFO - ==> Top1: 89.110    Top5: 99.610    Loss: 0.400
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [66][Top1: 89.280   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [64][Top1: 89.270   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [67][Top1: 89.240   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
0.52619094
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
0.52618909
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
0.52619117
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
0.52619565
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
0.52619517
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
0.52619880
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
0.52620292
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
0.52619988
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
0.52619869
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
0.52619565
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
0.52619445
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
0.52619755
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
0.52619898
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
0.52619880
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
0.52619779
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
0.52619576
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
0.52620453
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
0.52620685
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
0.52620780
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][   20/  196]   Loss 0.077520   Top1 97.656250   Top5 100.000000   BatchTime 0.336232   LR 0.000000
0.52620864
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.52620721
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.52620256
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
0.52620065
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
0.52619040
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
0.52618575
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
0.52618986
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
0.52618653
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
0.52618814
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
0.52618539
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
0.52618581
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
0.52618295
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
0.52618343
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
0.52618510
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
0.52617782
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
0.52618009
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
0.52618134
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
0.52617460
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
0.52617610
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
0.52617544
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
0.52617812
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
0.52617413
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][   40/  196]   Loss 0.077071   Top1 97.558594   Top5 99.990234   BatchTime 0.300000   LR 0.000000
0.52617466
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
0.52617878
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.52618623
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
0.52618778
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
0.52618271
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
0.52618366
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
0.52618265
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
0.52617925
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
0.52618402
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
0.52618432
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
0.52618170
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
0.52618736
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
0.52618295
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
0.52618200
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
0.52618843
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
0.52618170
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
0.52617687
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
0.52617830
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
0.52617830
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
0.52617317
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
0.52617389
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
0.52617007
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
0.52617079
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][   60/  196]   Loss 0.077642   Top1 97.513021   Top5 99.986979   BatchTime 0.287817   LR 0.000000
0.52617210
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
0.52616996
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
0.52617723
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
0.52618206
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
0.52618444
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
0.52618080
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
0.52619004
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.52619225
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
0.52618909
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
0.52618486
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
0.52618891
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
0.52619362
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
0.52618563
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
0.52618963
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
0.52618355
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.52618098
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][   80/  196]   Loss 0.077969   Top1 97.490234   Top5 99.980469   BatchTime 0.281084   LR 0.000000
0.52618712
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
0.52618748
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
0.52618825
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
0.52618384
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
0.52618676
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
0.52618819
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
0.52619553
tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)
0.52619535
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.52619845
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
0.52619886
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
0.52619308
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
0.52619117
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
0.52619201
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
0.52619666
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
0.52620018
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
0.52620137
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
0.52619827
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
0.52618831
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
0.52618879
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
0.52619237
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
0.52619624
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
0.52619225
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][  100/  196]   Loss 0.076557   Top1 97.554688   Top5 99.980469   BatchTime 0.278449   LR 0.000000
0.52619618
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
0.52619565
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
0.52619106
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
0.52618718
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
0.52618676
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
0.52619165
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
0.52619088
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
0.52619028
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
0.52618968
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
0.52618611
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
0.52618808
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
0.52619272
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
0.52619338
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
0.52619487
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
0.52619922
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
0.52618742
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
0.52619147
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
0.52619135
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
0.52619022
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
0.52619344
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
0.52618366
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][  120/  196]   Loss 0.075527   Top1 97.594401   Top5 99.983724   BatchTime 0.280464   LR 0.000000
0.52618843
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
0.52619183
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52619159
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
0.52619183
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
0.52618897
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
0.52618223
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
0.52618587
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
0.52618659
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
0.52618074
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
0.52617753
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
0.52618206
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
0.52618629
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
0.52619004
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
0.52618796
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
0.52618867
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
0.52618915
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
0.52618897
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
0.52618992
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
0.52619135
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
0.52618039
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
0.52618676
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][  140/  196]   Loss 0.074895   Top1 97.628348   Top5 99.983259   BatchTime 0.280342   LR 0.000000
0.52618545
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
0.52618688
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
0.52618915
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
0.52618665
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
0.52618825
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
0.52618605
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
0.52618665
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
0.52619082
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
0.52619237
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
0.52618980
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
0.52618843
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
0.52618623
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
0.52619249
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
0.52618945
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
0.52619427
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
0.52619582
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][  160/  196]   Loss 0.076029   Top1 97.580566   Top5 99.980469   BatchTime 0.278061   LR 0.000000
0.52619922
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
0.52619332
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
0.52619731
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
0.52619207
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
0.52619171
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
0.52618939
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
0.52619630
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
0.52619797
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
0.52619469
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
0.52619213
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
0.52618623
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52618557
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
0.52618945
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
0.52619117
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
0.52618867
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
0.52618879
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
0.52619368
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
0.52619338
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
0.52619022
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
0.52618694
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
0.52618355
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
0.52618182
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
0.52617526
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
0.52617967
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
INFO - Training [69][  180/  196]   Loss 0.076737   Top1 97.556424   Top5 99.982639   BatchTime 0.275032   LR 0.000000
0.52619117
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
0.52618098
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
0.52618074
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
0.52618432
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
0.52618676
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
0.52618229
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
0.52618390
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
0.52618581
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
0.52619159
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
0.52618653
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
0.52618152
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
0.52619010
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.568    Top5: 99.984    Loss: 0.077
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [69][   20/   40]   Loss 0.411167   Top1 89.199219   Top5 99.589844   BatchTime 0.127557
features.0.conv.0 tensor(0.3090)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0664)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1322)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.5929)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1181)
features.4.conv.0 tensor(0.0938)
features.4.conv.3 tensor(0.2963)
features.4.conv.6 tensor(0.3962)
features.5.conv.0 tensor(0.4150)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.5264)
features.6.conv.0 tensor(0.0488)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.1973)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.5409)
features.8.conv.0 tensor(0.6277)
features.8.conv.3 tensor(0.5448)
features.8.conv.6 tensor(0.6670)
features.9.conv.0 tensor(0.5869)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.6871)
features.10.conv.0 tensor(0.0581)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0994)
features.11.conv.0 tensor(0.7726)
features.11.conv.3 tensor(0.6329)
features.11.conv.6 tensor(0.8274)
features.12.conv.0 tensor(0.7772)
features.12.conv.3 tensor(0.6698)
features.12.conv.6 tensor(0.8600)
features.13.conv.0 tensor(0.3232)
features.13.conv.3 tensor(0.4824)
features.13.conv.6 tensor(0.5597)
features.14.conv.0 tensor(0.9248)
features.14.conv.3 tensor(0.8288)
features.14.conv.6 tensor(0.9631)
features.15.conv.0 tensor(0.9062)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.7183)
features.16.conv.3 tensor(0.8019)
features.16.conv.6 tensor(0.9164)
conv.0 tensor(0.2495)
tensor(1458768.) 2188896.0
INFO - Validation [69][   40/   40]   Loss 0.398819   Top1 89.330000   Top5 99.720000   BatchTime 0.092210
INFO - ==> Top1: 89.330    Top5: 99.720    Loss: 0.399
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [69][Top1: 89.330   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [66][Top1: 89.280   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [64][Top1: 89.270   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [   20/   40]   Loss 0.411167   Top1 89.199219   Top5 99.589844   BatchTime 0.140618
*************hard_pruning_mode*******************
INFO - Validation [   40/   40]   Loss 0.398819   Top1 89.330000   Top5 99.720000   BatchTime 0.098612
INFO - ==> Top1: 89.330    Top5: 99.720    Loss: 0.399
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3239, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][   20/  196]   Loss 0.341326   Top1 88.828125   Top5 99.882812   BatchTime 0.324728   LR 0.000500
tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][   40/  196]   Loss 0.348891   Top1 88.476562   Top5 99.765625   BatchTime 0.282690   LR 0.000500
tensor(0.3526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][   60/  196]   Loss 0.347311   Top1 88.287760   Top5 99.759115   BatchTime 0.268197   LR 0.000499
tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][   80/  196]   Loss 0.352465   Top1 88.100586   Top5 99.721680   BatchTime 0.258627   LR 0.000498
tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][  100/  196]   Loss 0.347876   Top1 88.265625   Top5 99.722656   BatchTime 0.255571   LR 0.000497
tensor(0.3246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][  120/  196]   Loss 0.344859   Top1 88.414714   Top5 99.729818   BatchTime 0.253359   LR 0.000495
tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][  140/  196]   Loss 0.340913   Top1 88.482143   Top5 99.734933   BatchTime 0.251254   LR 0.000494
tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2909, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][  160/  196]   Loss 0.338354   Top1 88.520508   Top5 99.738770   BatchTime 0.249352   LR 0.000492
tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [0][  180/  196]   Loss 0.336451   Top1 88.608941   Top5 99.739583   BatchTime 0.247764   LR 0.000490
tensor(0.3145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3046, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4488, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 88.536    Top5: 99.742    Loss: 0.337
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.586822   Top1 81.992188   Top5 99.062500   BatchTime 0.136607
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.3594)
features.1.conv.0 tensor(0.0495)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0838)
features.2.conv.0 tensor(0.1276)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6123)
features.3.conv.0 tensor(0.0825)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1532)
features.4.conv.0 tensor(0.0916)
features.4.conv.3 tensor(0.2998)
features.4.conv.6 tensor(0.4354)
features.5.conv.0 tensor(0.4417)
features.5.conv.3 tensor(0.4207)
features.5.conv.6 tensor(0.5659)
features.6.conv.0 tensor(0.0483)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0841)
features.7.conv.0 tensor(0.2002)
features.7.conv.3 tensor(0.4537)
features.7.conv.6 tensor(0.5728)
features.8.conv.0 tensor(0.6486)
features.8.conv.3 tensor(0.5437)
features.8.conv.6 tensor(0.6921)
features.9.conv.0 tensor(0.6094)
features.9.conv.3 tensor(0.5530)
features.9.conv.6 tensor(0.7124)
features.10.conv.0 tensor(0.0603)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.1231)
features.11.conv.0 tensor(0.7830)
features.11.conv.3 tensor(0.6377)
features.11.conv.6 tensor(0.8409)
features.12.conv.0 tensor(0.7883)
features.12.conv.3 tensor(0.6723)
features.12.conv.6 tensor(0.8732)
features.13.conv.0 tensor(0.3589)
features.13.conv.3 tensor(0.4878)
features.13.conv.6 tensor(0.6104)
features.14.conv.0 tensor(0.9276)
features.14.conv.3 tensor(0.8266)
features.14.conv.6 tensor(0.9670)
features.15.conv.0 tensor(0.9112)
features.15.conv.3 tensor(0.8370)
features.15.conv.6 tensor(0.9718)
features.16.conv.0 tensor(0.7292)
features.16.conv.3 tensor(0.8057)
features.16.conv.6 tensor(0.9251)
conv.0 tensor(0.3444)
tensor(1518828.) 2188896.0
INFO - Validation [0][   40/   40]   Loss 0.582602   Top1 82.210000   Top5 99.240000   BatchTime 0.097394
INFO - ==> Top1: 82.210    Top5: 99.240    Loss: 0.583
INFO - ==> Sparsity : 0.694
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.210   Top5: 99.240]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][   20/  196]   Loss 0.324093   Top1 88.710938   Top5 99.785156   BatchTime 0.329319   LR 0.000485
tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][   40/  196]   Loss 0.318520   Top1 89.042969   Top5 99.765625   BatchTime 0.282584   LR 0.000482
tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3151, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][   60/  196]   Loss 0.313314   Top1 89.127604   Top5 99.746094   BatchTime 0.267940   LR 0.000479
tensor(0.3184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3109, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][   80/  196]   Loss 0.315838   Top1 89.106445   Top5 99.741211   BatchTime 0.259380   LR 0.000476
tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][  100/  196]   Loss 0.316474   Top1 89.062500   Top5 99.746094   BatchTime 0.254159   LR 0.000473
tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][  120/  196]   Loss 0.321023   Top1 88.925781   Top5 99.739583   BatchTime 0.249051   LR 0.000469
tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][  140/  196]   Loss 0.320841   Top1 88.967634   Top5 99.740513   BatchTime 0.247419   LR 0.000465
tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3143, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][  160/  196]   Loss 0.318983   Top1 89.052734   Top5 99.743652   BatchTime 0.244256   LR 0.000460
tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3501, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [1][  180/  196]   Loss 0.318651   Top1 89.001736   Top5 99.748264   BatchTime 0.244264   LR 0.000456
tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 88.930    Top5: 99.736    Loss: 0.320
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.521790   Top1 83.769531   Top5 99.062500   BatchTime 0.133783
features.0.conv.0 tensor(0.3194)
features.0.conv.3 tensor(0.3711)
features.1.conv.0 tensor(0.0566)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0885)
features.2.conv.0 tensor(0.1322)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6204)
features.3.conv.0 tensor(0.0822)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1745)
features.4.conv.0 tensor(0.0957)
features.4.conv.3 tensor(0.2963)
features.4.conv.6 tensor(0.4518)
features.5.conv.0 tensor(0.4539)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.5806)
features.6.conv.0 tensor(0.0438)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0822)
features.7.conv.0 tensor(0.2009)
features.7.conv.3 tensor(0.4543)
features.7.conv.6 tensor(0.5956)
features.8.conv.0 tensor(0.6579)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7049)
features.9.conv.0 tensor(0.6213)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.7239)
features.10.conv.0 tensor(0.0583)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.1377)
features.11.conv.0 tensor(0.7875)
features.11.conv.3 tensor(0.6375)
features.11.conv.6 tensor(0.8476)
features.12.conv.0 tensor(0.7936)
features.12.conv.3 tensor(0.6725)
features.12.conv.6 tensor(0.8807)
features.13.conv.0 tensor(0.3779)
features.13.conv.3 tensor(0.4840)
features.13.conv.6 tensor(0.6294)
features.14.conv.0 tensor(0.9288)
features.14.conv.3 tensor(0.8265)
features.14.conv.6 tensor(0.9689)
features.15.conv.0 tensor(0.9131)
features.15.conv.3 tensor(0.8360)
features.15.conv.6 tensor(0.9727)
features.16.conv.0 tensor(0.7319)
features.16.conv.3 tensor(0.8050)
features.16.conv.6 tensor(0.9284)
conv.0 tensor(0.3774)
tensor(1541378.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 0.510624   Top1 83.730000   Top5 99.260000   BatchTime 0.095051
INFO - ==> Top1: 83.730    Top5: 99.260    Loss: 0.511
INFO - ==> Sparsity : 0.704
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 83.730   Top5: 99.260]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 82.210   Top5: 99.240]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.3305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][   20/  196]   Loss 0.305797   Top1 89.199219   Top5 99.804688   BatchTime 0.331263   LR 0.000448
tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][   40/  196]   Loss 0.291052   Top1 89.638672   Top5 99.794922   BatchTime 0.284249   LR 0.000443
tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][   60/  196]   Loss 0.289586   Top1 89.791667   Top5 99.785156   BatchTime 0.269674   LR 0.000437
tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][   80/  196]   Loss 0.292300   Top1 89.853516   Top5 99.775391   BatchTime 0.262798   LR 0.000432
tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][  100/  196]   Loss 0.293981   Top1 89.859375   Top5 99.757812   BatchTime 0.260149   LR 0.000426
tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][  120/  196]   Loss 0.288743   Top1 90.019531   Top5 99.775391   BatchTime 0.259471   LR 0.000421
tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][  140/  196]   Loss 0.289067   Top1 89.997210   Top5 99.779576   BatchTime 0.258604   LR 0.000415
tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][  160/  196]   Loss 0.289710   Top1 89.948730   Top5 99.782715   BatchTime 0.257437   LR 0.000409
tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [2][  180/  196]   Loss 0.287128   Top1 90.073785   Top5 99.791667   BatchTime 0.255157   LR 0.000402
tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 90.000    Top5: 99.784    Loss: 0.289
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.462788   Top1 85.175781   Top5 99.277344   BatchTime 0.130328
INFO - Validation [2][   40/   40]   Loss 0.465568   Top1 85.000000   Top5 99.430000   BatchTime 0.094775
INFO - ==> Top1: 85.000    Top5: 99.430    Loss: 0.466
INFO - ==> Sparsity : 0.710
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 85.000   Top5: 99.430]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 83.730   Top5: 99.260]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 82.210   Top5: 99.240]
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.3867)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0842)
features.2.conv.0 tensor(0.1351)
features.2.conv.3 tensor(0.3403)
features.2.conv.6 tensor(0.6262)
features.3.conv.0 tensor(0.0813)
features.3.conv.3 tensor(0.0841)
features.3.conv.6 tensor(0.1858)
features.4.conv.0 tensor(0.0952)
features.4.conv.3 tensor(0.3056)
features.4.conv.6 tensor(0.4621)
features.5.conv.0 tensor(0.4634)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.5934)
features.6.conv.0 tensor(0.0443)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0815)
features.7.conv.0 tensor(0.2024)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.6075)
features.8.conv.0 tensor(0.6635)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7121)
features.9.conv.0 tensor(0.6287)
features.9.conv.3 tensor(0.5535)
features.9.conv.6 tensor(0.7322)
features.10.conv.0 tensor(0.0579)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.1518)
features.11.conv.0 tensor(0.7916)
features.11.conv.3 tensor(0.6383)
features.11.conv.6 tensor(0.8526)
features.12.conv.0 tensor(0.7974)
features.12.conv.3 tensor(0.6726)
features.12.conv.6 tensor(0.8859)
features.13.conv.0 tensor(0.3913)
features.13.conv.3 tensor(0.4826)
features.13.conv.6 tensor(0.6403)
features.14.conv.0 tensor(0.9296)
features.14.conv.3 tensor(0.8286)
features.14.conv.6 tensor(0.9693)
features.15.conv.0 tensor(0.9142)
features.15.conv.3 tensor(0.8359)
features.15.conv.6 tensor(0.9732)
features.16.conv.0 tensor(0.7337)
features.16.conv.3 tensor(0.8054)
features.16.conv.6 tensor(0.9302)
conv.0 tensor(0.3955)
tensor(1554623.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][   20/  196]   Loss 0.259549   Top1 90.878906   Top5 99.843750   BatchTime 0.340794   LR 0.000391
tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][   40/  196]   Loss 0.260631   Top1 90.869141   Top5 99.853516   BatchTime 0.290402   LR 0.000384
tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][   60/  196]   Loss 0.266836   Top1 90.540365   Top5 99.863281   BatchTime 0.275839   LR 0.000377
tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][   80/  196]   Loss 0.268740   Top1 90.532227   Top5 99.873047   BatchTime 0.268821   LR 0.000370
tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][  100/  196]   Loss 0.266598   Top1 90.597656   Top5 99.851562   BatchTime 0.268382   LR 0.000363
tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][  120/  196]   Loss 0.263632   Top1 90.651042   Top5 99.863281   BatchTime 0.261143   LR 0.000356
tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][  140/  196]   Loss 0.263335   Top1 90.711496   Top5 99.854911   BatchTime 0.256063   LR 0.000348
tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3021, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][  160/  196]   Loss 0.264199   Top1 90.664062   Top5 99.848633   BatchTime 0.253748   LR 0.000341
tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3009, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [3][  180/  196]   Loss 0.263342   Top1 90.679253   Top5 99.848090   BatchTime 0.251402   LR 0.000333
tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 90.650    Top5: 99.846    Loss: 0.264
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.3965)
features.1.conv.0 tensor(0.0573)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0833)
features.2.conv.0 tensor(0.1331)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.6305)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2010)
features.4.conv.0 tensor(0.0957)
features.4.conv.3 tensor(0.2969)
features.4.conv.6 tensor(0.4715)
features.5.conv.0 tensor(0.4694)
features.5.conv.3 tensor(0.4172)
features.5.conv.6 tensor(0.6004)
features.6.conv.0 tensor(0.0457)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0844)
features.7.conv.0 tensor(0.2024)
features.7.conv.3 tensor(0.4525)
features.7.conv.6 tensor(0.6154)
features.8.conv.0 tensor(0.6681)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7180)
features.9.conv.0 tensor(0.6355)
features.9.conv.3 tensor(0.5530)
features.9.conv.6 tensor(0.7386)
features.10.conv.0 tensor(0.0576)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.1635)
features.11.conv.0 tensor(0.7938)
features.11.conv.3 tensor(0.6360)
features.11.conv.6 tensor(0.8555)
features.12.conv.0 tensor(0.8000)
features.12.conv.3 tensor(0.6717)
features.12.conv.6 tensor(0.8892)
features.13.conv.0 tensor(0.3995)
features.13.conv.3 tensor(0.4823)
features.13.conv.6 tensor(0.6473)
features.14.conv.0 tensor(0.9301)
features.14.conv.3 tensor(0.8293)
features.14.conv.6 tensor(0.9695)
features.15.conv.0 tensor(0.9146)
features.15.conv.3 tensor(0.8366)
features.15.conv.6 tensor(0.9734)
features.16.conv.0 tensor(0.7347)
features.16.conv.3 tensor(0.8053)
features.16.conv.6 tensor(0.9313)
conv.0 tensor(0.4062)
tensor(1562847.) 2188896.0
INFO - Validation [3][   20/   40]   Loss 0.420512   Top1 87.363281   Top5 99.179688   BatchTime 0.139688
INFO - Validation [3][   40/   40]   Loss 0.411360   Top1 87.010000   Top5 99.400000   BatchTime 0.097556
INFO - ==> Top1: 87.010    Top5: 99.400    Loss: 0.411
INFO - ==> Sparsity : 0.714
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 87.010   Top5: 99.400]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 85.000   Top5: 99.430]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 83.730   Top5: 99.260]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][   20/  196]   Loss 0.221808   Top1 92.441406   Top5 99.863281   BatchTime 0.335397   LR 0.000320
tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][   40/  196]   Loss 0.225872   Top1 92.304688   Top5 99.843750   BatchTime 0.287707   LR 0.000312
tensor(0.2771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][   60/  196]   Loss 0.228345   Top1 92.207031   Top5 99.876302   BatchTime 0.275079   LR 0.000304
tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2862, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2708, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][   80/  196]   Loss 0.234044   Top1 92.070312   Top5 99.873047   BatchTime 0.270779   LR 0.000296
tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][  100/  196]   Loss 0.235722   Top1 91.964844   Top5 99.875000   BatchTime 0.264309   LR 0.000289
tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][  120/  196]   Loss 0.233684   Top1 92.037760   Top5 99.889323   BatchTime 0.260527   LR 0.000281
tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][  140/  196]   Loss 0.231619   Top1 92.117746   Top5 99.896763   BatchTime 0.257278   LR 0.000273
tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][  160/  196]   Loss 0.233019   Top1 92.001953   Top5 99.892578   BatchTime 0.256313   LR 0.000265
tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [4][  180/  196]   Loss 0.230101   Top1 92.078993   Top5 99.895833   BatchTime 0.254407   LR 0.000257
tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 92.052    Top5: 99.902    Loss: 0.231
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.3945)
features.1.conv.0 tensor(0.0592)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0842)
features.2.conv.0 tensor(0.1319)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6340)
features.3.conv.0 tensor(0.0807)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.2055)
features.4.conv.0 tensor(0.0965)
features.4.conv.3 tensor(0.3015)
features.4.conv.6 tensor(0.4751)
features.5.conv.0 tensor(0.4731)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.6034)
features.6.conv.0 tensor(0.0405)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0800)
features.7.conv.0 tensor(0.2020)
features.7.conv.3 tensor(0.4569)
features.7.conv.6 tensor(0.6187)
features.8.conv.0 tensor(0.6700)
features.8.conv.3 tensor(0.5443)
features.8.conv.6 tensor(0.7217)
features.9.conv.0 tensor(0.6383)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7421)
features.10.conv.0 tensor(0.0533)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.1732)
features.11.conv.0 tensor(0.7954)
features.11.conv.3 tensor(0.6373)
features.11.conv.6 tensor(0.8578)
features.12.conv.0 tensor(0.8016)
features.12.conv.3 tensor(0.6717)
features.12.conv.6 tensor(0.8923)
features.13.conv.0 tensor(0.4045)
features.13.conv.3 tensor(0.4826)
features.13.conv.6 tensor(0.6507)
features.14.conv.0 tensor(0.9304)
features.14.conv.3 tensor(0.8292)
features.14.conv.6 tensor(0.9697)
features.15.conv.0 tensor(0.9152)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9735)
features.16.conv.0 tensor(0.7352)
features.16.conv.3 tensor(0.8043)
features.16.conv.6 tensor(0.9319)
conv.0 tensor(0.4112)
tensor(1567045.) 2188896.0
INFO - Validation [4][   20/   40]   Loss 0.394037   Top1 87.558594   Top5 99.414062   BatchTime 0.131198
INFO - Validation [4][   40/   40]   Loss 0.393233   Top1 87.490000   Top5 99.520000   BatchTime 0.095642
INFO - ==> Top1: 87.490    Top5: 99.520    Loss: 0.393
INFO - ==> Sparsity : 0.716
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 87.490   Top5: 99.520]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 87.010   Top5: 99.400]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 85.000   Top5: 99.430]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][   20/  196]   Loss 0.211270   Top1 92.636719   Top5 99.960938   BatchTime 0.336452   LR 0.000242
tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][   40/  196]   Loss 0.208673   Top1 92.783203   Top5 99.951172   BatchTime 0.286828   LR 0.000234
tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][   60/  196]   Loss 0.202565   Top1 92.864583   Top5 99.915365   BatchTime 0.266698   LR 0.000226
tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][   80/  196]   Loss 0.203828   Top1 92.822266   Top5 99.916992   BatchTime 0.258667   LR 0.000218
tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][  100/  196]   Loss 0.203001   Top1 92.839844   Top5 99.914062   BatchTime 0.252829   LR 0.000210
tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][  120/  196]   Loss 0.202101   Top1 92.845052   Top5 99.915365   BatchTime 0.248182   LR 0.000202
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3040, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][  140/  196]   Loss 0.204616   Top1 92.748326   Top5 99.913504   BatchTime 0.245128   LR 0.000195
tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][  160/  196]   Loss 0.202737   Top1 92.832031   Top5 99.919434   BatchTime 0.243462   LR 0.000187
tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [5][  180/  196]   Loss 0.199034   Top1 92.949219   Top5 99.921875   BatchTime 0.241774   LR 0.000179
tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 92.950    Top5: 99.924    Loss: 0.199
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.367166   Top1 88.164062   Top5 99.531250   BatchTime 0.141891
INFO - Validation [5][   40/   40]   Loss 0.362598   Top1 88.350000   Top5 99.580000   BatchTime 0.099491
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.3965)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0890)
features.2.conv.0 tensor(0.1348)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.6357)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.2096)
features.4.conv.0 tensor(0.0959)
features.4.conv.3 tensor(0.2980)
features.4.conv.6 tensor(0.4795)
features.5.conv.0 tensor(0.4743)
features.5.conv.3 tensor(0.4172)
features.5.conv.6 tensor(0.6069)
features.6.conv.0 tensor(0.0431)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0815)
features.7.conv.0 tensor(0.2004)
features.7.conv.3 tensor(0.4517)
features.7.conv.6 tensor(0.6206)
features.8.conv.0 tensor(0.6711)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7241)
features.9.conv.0 tensor(0.6401)
features.9.conv.3 tensor(0.5512)
features.9.conv.6 tensor(0.7438)
features.10.conv.0 tensor(0.0546)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.1819)
features.11.conv.0 tensor(0.7959)
features.11.conv.3 tensor(0.6370)
features.11.conv.6 tensor(0.8591)
features.12.conv.0 tensor(0.8026)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.8935)
features.13.conv.0 tensor(0.4078)
features.13.conv.3 tensor(0.4848)
features.13.conv.6 tensor(0.6524)
features.14.conv.0 tensor(0.9306)
features.14.conv.3 tensor(0.8303)
features.14.conv.6 tensor(0.9698)
features.15.conv.0 tensor(0.9154)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9735)
features.16.conv.0 tensor(0.7356)
features.16.conv.3 tensor(0.8043)
features.16.conv.6 tensor(0.9323)
conv.0 tensor(0.4136)
tensor(1569514.) 2188896.0
INFO - ==> Top1: 88.350    Top5: 99.580    Loss: 0.363
INFO - ==> Sparsity : 0.717
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.350   Top5: 99.580]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 87.490   Top5: 99.520]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 87.010   Top5: 99.400]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][   20/  196]   Loss 0.169732   Top1 93.886719   Top5 99.941406   BatchTime 0.299278   LR 0.000166
tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][   40/  196]   Loss 0.171077   Top1 94.082031   Top5 99.921875   BatchTime 0.277444   LR 0.000158
tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][   60/  196]   Loss 0.173223   Top1 94.010417   Top5 99.928385   BatchTime 0.256456   LR 0.000151
tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][   80/  196]   Loss 0.168347   Top1 94.111328   Top5 99.926758   BatchTime 0.247898   LR 0.000143
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][  100/  196]   Loss 0.169296   Top1 94.093750   Top5 99.917969   BatchTime 0.241732   LR 0.000136
tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][  120/  196]   Loss 0.167476   Top1 94.140625   Top5 99.925130   BatchTime 0.239015   LR 0.000129
tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][  140/  196]   Loss 0.166290   Top1 94.168527   Top5 99.930246   BatchTime 0.238793   LR 0.000122
tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][  160/  196]   Loss 0.167049   Top1 94.106445   Top5 99.929199   BatchTime 0.239016   LR 0.000115
tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [6][  180/  196]   Loss 0.164491   Top1 94.225260   Top5 99.928385   BatchTime 0.237051   LR 0.000108
tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 94.224    Top5: 99.926    Loss: 0.165
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [6][   20/   40]   Loss 0.384046   Top1 88.476562   Top5 99.648438   BatchTime 0.140572
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.3965)
features.1.conv.0 tensor(0.0579)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.1369)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6354)
features.3.conv.0 tensor(0.0810)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2114)
features.4.conv.0 tensor(0.0952)
features.4.conv.3 tensor(0.2963)
features.4.conv.6 tensor(0.4797)
features.5.conv.0 tensor(0.4754)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.6073)
features.6.conv.0 tensor(0.0431)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0820)
features.7.conv.0 tensor(0.2016)
features.7.conv.3 tensor(0.4525)
features.7.conv.6 tensor(0.6220)
features.8.conv.0 tensor(0.6719)
features.8.conv.3 tensor(0.5408)
features.8.conv.6 tensor(0.7259)
features.9.conv.0 tensor(0.6407)
features.9.conv.3 tensor(0.5518)
features.9.conv.6 tensor(0.7442)
features.10.conv.0 tensor(0.0550)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.1878)
features.11.conv.0 tensor(0.7960)
features.11.conv.3 tensor(0.6373)
features.11.conv.6 tensor(0.8595)
features.12.conv.0 tensor(0.8029)
features.12.conv.3 tensor(0.6725)
features.12.conv.6 tensor(0.8938)
features.13.conv.0 tensor(0.4089)
features.13.conv.3 tensor(0.4844)
features.13.conv.6 tensor(0.6532)
features.14.conv.0 tensor(0.9306)
features.14.conv.3 tensor(0.8293)
features.14.conv.6 tensor(0.9699)
features.15.conv.0 tensor(0.9153)
features.15.conv.3 tensor(0.8362)
features.15.conv.6 tensor(0.9735)
features.16.conv.0 tensor(0.7356)
features.16.conv.3 tensor(0.8045)
features.16.conv.6 tensor(0.9324)
conv.0 tensor(0.4146)
tensor(1570563.) 2188896.0
INFO - Validation [6][   40/   40]   Loss 0.369206   Top1 88.670000   Top5 99.700000   BatchTime 0.098271
INFO - ==> Top1: 88.670    Top5: 99.700    Loss: 0.369
INFO - ==> Sparsity : 0.718
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 88.670   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.350   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 87.490   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][   20/  196]   Loss 0.144881   Top1 94.882812   Top5 99.980469   BatchTime 0.312019   LR 0.000097
tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][   40/  196]   Loss 0.143939   Top1 94.990234   Top5 99.970703   BatchTime 0.276506   LR 0.000091
tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][   60/  196]   Loss 0.142202   Top1 95.071615   Top5 99.973958   BatchTime 0.263172   LR 0.000085
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][   80/  196]   Loss 0.142915   Top1 95.073242   Top5 99.956055   BatchTime 0.254519   LR 0.000079
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][  100/  196]   Loss 0.144387   Top1 94.976562   Top5 99.964844   BatchTime 0.248467   LR 0.000073
tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][  120/  196]   Loss 0.143433   Top1 95.000000   Top5 99.964193   BatchTime 0.246469   LR 0.000067
tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][  140/  196]   Loss 0.142681   Top1 95.011161   Top5 99.963728   BatchTime 0.242810   LR 0.000062
tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][  160/  196]   Loss 0.140957   Top1 95.063477   Top5 99.968262   BatchTime 0.239900   LR 0.000057
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [7][  180/  196]   Loss 0.139778   Top1 95.058594   Top5 99.967448   BatchTime 0.235906   LR 0.000052
tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 95.080    Top5: 99.968    Loss: 0.140
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 0.352463   Top1 89.140625   Top5 99.667969   BatchTime 0.141411
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.3984)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.1337)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6348)
features.3.conv.0 tensor(0.0802)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2116)
features.4.conv.0 tensor(0.0954)
features.4.conv.3 tensor(0.2980)
features.4.conv.6 tensor(0.4800)
features.5.conv.0 tensor(0.4761)
features.5.conv.3 tensor(0.4172)
features.5.conv.6 tensor(0.6071)
features.6.conv.0 tensor(0.0431)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0830)
features.7.conv.0 tensor(0.2017)
features.7.conv.3 tensor(0.4528)
features.7.conv.6 tensor(0.6221)
features.8.conv.0 tensor(0.6721)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.7261)
features.9.conv.0 tensor(0.6407)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7445)
features.10.conv.0 tensor(0.0557)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.1887)
features.11.conv.0 tensor(0.7961)
features.11.conv.3 tensor(0.6366)
features.11.conv.6 tensor(0.8596)
features.12.conv.0 tensor(0.8030)
features.12.conv.3 tensor(0.6730)
features.12.conv.6 tensor(0.8937)
features.13.conv.0 tensor(0.4092)
features.13.conv.3 tensor(0.4821)
features.13.conv.6 tensor(0.6534)
features.14.conv.0 tensor(0.9306)
features.14.conv.3 tensor(0.8294)
features.14.conv.6 tensor(0.9702)
features.15.conv.0 tensor(0.9153)
features.15.conv.3 tensor(0.8368)
features.15.conv.6 tensor(0.9735)
features.16.conv.0 tensor(0.7356)
features.16.conv.3 tensor(0.8043)
features.16.conv.6 tensor(0.9325)
conv.0 tensor(0.4149)
tensor(1570867.) 2188896.0
INFO - Validation [7][   40/   40]   Loss 0.342417   Top1 89.510000   Top5 99.730000   BatchTime 0.097631
INFO - ==> Top1: 89.510    Top5: 99.730    Loss: 0.342
INFO - ==> Sparsity : 0.718
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 88.670   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 88.350   Top5: 99.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][   20/  196]   Loss 0.123463   Top1 95.898438   Top5 99.902344   BatchTime 0.329143   LR 0.000043
tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][   40/  196]   Loss 0.119202   Top1 95.996094   Top5 99.931641   BatchTime 0.281277   LR 0.000039
tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][   60/  196]   Loss 0.123522   Top1 95.683594   Top5 99.954427   BatchTime 0.263525   LR 0.000035
tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][   80/  196]   Loss 0.126516   Top1 95.517578   Top5 99.956055   BatchTime 0.253610   LR 0.000031
tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][  100/  196]   Loss 0.126779   Top1 95.558594   Top5 99.949219   BatchTime 0.250363   LR 0.000027
tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][  120/  196]   Loss 0.124838   Top1 95.618490   Top5 99.954427   BatchTime 0.248934   LR 0.000023
tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][  140/  196]   Loss 0.123550   Top1 95.644531   Top5 99.960938   BatchTime 0.247884   LR 0.000020
tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][  160/  196]   Loss 0.123324   Top1 95.673828   Top5 99.963379   BatchTime 0.245280   LR 0.000017
tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [8][  180/  196]   Loss 0.122708   Top1 95.696615   Top5 99.960938   BatchTime 0.242429   LR 0.000014
tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 95.676    Top5: 99.962    Loss: 0.123
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.353448   Top1 89.414062   Top5 99.687500   BatchTime 0.147830
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4004)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.1354)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6351)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.2105)
features.4.conv.0 tensor(0.0952)
features.4.conv.3 tensor(0.2957)
features.4.conv.6 tensor(0.4795)
features.5.conv.0 tensor(0.4759)
features.5.conv.3 tensor(0.4167)
features.5.conv.6 tensor(0.6073)
features.6.conv.0 tensor(0.0426)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.2021)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6219)
features.8.conv.0 tensor(0.6721)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.7259)
features.9.conv.0 tensor(0.6406)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7446)
features.10.conv.0 tensor(0.0545)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.1892)
features.11.conv.0 tensor(0.7961)
features.11.conv.3 tensor(0.6375)
features.11.conv.6 tensor(0.8596)
features.12.conv.0 tensor(0.8030)
features.12.conv.3 tensor(0.6728)
features.12.conv.6 tensor(0.8937)
features.13.conv.0 tensor(0.4093)
features.13.conv.3 tensor(0.4821)
features.13.conv.6 tensor(0.6534)
features.14.conv.0 tensor(0.9307)
features.14.conv.3 tensor(0.8294)
features.14.conv.6 tensor(0.9702)
features.15.conv.0 tensor(0.9153)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9735)
features.16.conv.0 tensor(0.7356)
features.16.conv.3 tensor(0.8042)
features.16.conv.6 tensor(0.9325)
conv.0 tensor(0.4150)
tensor(1570899.) 2188896.0
INFO - Validation [8][   40/   40]   Loss 0.344514   Top1 89.620000   Top5 99.770000   BatchTime 0.099374
INFO - ==> Top1: 89.620    Top5: 99.770    Loss: 0.345
INFO - ==> Sparsity : 0.718
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 88.670   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][   20/  196]   Loss 0.114615   Top1 96.269531   Top5 99.960938   BatchTime 0.331649   LR 0.000010
tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][   40/  196]   Loss 0.118854   Top1 95.751953   Top5 99.951172   BatchTime 0.280616   LR 0.000008
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][   60/  196]   Loss 0.116383   Top1 95.852865   Top5 99.947917   BatchTime 0.267223   LR 0.000006
tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][   80/  196]   Loss 0.114408   Top1 95.957031   Top5 99.951172   BatchTime 0.258111   LR 0.000004
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][  100/  196]   Loss 0.112310   Top1 96.007812   Top5 99.957031   BatchTime 0.253638   LR 0.000003
tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][  120/  196]   Loss 0.111747   Top1 96.025391   Top5 99.964193   BatchTime 0.250978   LR 0.000002
tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][  140/  196]   Loss 0.111673   Top1 96.065848   Top5 99.966518   BatchTime 0.249101   LR 0.000001
tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][  160/  196]   Loss 0.111702   Top1 96.054688   Top5 99.968262   BatchTime 0.248858   LR 0.000000
tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [9][  180/  196]   Loss 0.111208   Top1 96.078559   Top5 99.967448   BatchTime 0.246709   LR 0.000000
tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.084    Top5: 99.968    Loss: 0.112
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [9][   20/   40]   Loss 0.353066   Top1 89.648438   Top5 99.628906   BatchTime 0.149408
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4004)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1354)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6351)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.2105)
features.4.conv.0 tensor(0.0947)
features.4.conv.3 tensor(0.2980)
features.4.conv.6 tensor(0.4797)
features.5.conv.0 tensor(0.4759)
features.5.conv.3 tensor(0.4167)
features.5.conv.6 tensor(0.6073)
features.6.conv.0 tensor(0.0436)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0841)
features.7.conv.0 tensor(0.2027)
features.7.conv.3 tensor(0.4520)
features.7.conv.6 tensor(0.6218)
features.8.conv.0 tensor(0.6721)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7260)
features.9.conv.0 tensor(0.6408)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7446)
features.10.conv.0 tensor(0.0547)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.1891)
features.11.conv.0 tensor(0.7961)
features.11.conv.3 tensor(0.6372)
features.11.conv.6 tensor(0.8596)
features.12.conv.0 tensor(0.8030)
features.12.conv.3 tensor(0.6728)
features.12.conv.6 tensor(0.8937)
features.13.conv.0 tensor(0.4092)
features.13.conv.3 tensor(0.4817)
features.13.conv.6 tensor(0.6533)
features.14.conv.0 tensor(0.9306)
features.14.conv.3 tensor(0.8294)
features.14.conv.6 tensor(0.9702)
features.15.conv.0 tensor(0.9153)
features.15.conv.3 tensor(0.8367)
features.15.conv.6 tensor(0.9735)
features.16.conv.0 tensor(0.7356)
features.16.conv.3 tensor(0.8042)
features.16.conv.6 tensor(0.9325)
conv.0 tensor(0.4150)
tensor(1570919.) 2188896.0
INFO - Validation [9][   40/   40]   Loss 0.343429   Top1 89.810000   Top5 99.690000   BatchTime 0.101855
INFO - ==> Top1: 89.810    Top5: 99.690    Loss: 0.343
INFO - ==> Sparsity : 0.718
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][   20/  196]   Loss 0.140573   Top1 95.078125   Top5 99.960938   BatchTime 0.337707   LR 0.000250
tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][   40/  196]   Loss 0.158661   Top1 94.306641   Top5 99.960938   BatchTime 0.281313   LR 0.000250
tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2919, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][   60/  196]   Loss 0.171373   Top1 93.893229   Top5 99.960938   BatchTime 0.263419   LR 0.000250
tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][   80/  196]   Loss 0.179332   Top1 93.691406   Top5 99.936523   BatchTime 0.257822   LR 0.000250
tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][  100/  196]   Loss 0.183104   Top1 93.574219   Top5 99.933594   BatchTime 0.253849   LR 0.000250
tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][  120/  196]   Loss 0.189454   Top1 93.382161   Top5 99.912109   BatchTime 0.250745   LR 0.000249
tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][  140/  196]   Loss 0.193396   Top1 93.203125   Top5 99.916295   BatchTime 0.248350   LR 0.000249
tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][  160/  196]   Loss 0.194880   Top1 93.151855   Top5 99.914551   BatchTime 0.246567   LR 0.000249
tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [10][  180/  196]   Loss 0.199046   Top1 93.012153   Top5 99.904514   BatchTime 0.243878   LR 0.000249
tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 92.986    Top5: 99.906    Loss: 0.199
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0527)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0803)
features.2.conv.0 tensor(0.1325)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.6374)
features.3.conv.0 tensor(0.0830)
features.3.conv.3 tensor(0.0810)
features.3.conv.6 tensor(0.2194)
features.4.conv.0 tensor(0.0951)
features.4.conv.3 tensor(0.3009)
features.4.conv.6 tensor(0.4821)
features.5.conv.0 tensor(0.4795)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.6110)
features.6.conv.0 tensor(0.0425)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0820)
features.7.conv.0 tensor(0.2020)
features.7.conv.3 tensor(0.4537)
features.7.conv.6 tensor(0.6261)
features.8.conv.0 tensor(0.6743)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7284)
features.9.conv.0 tensor(0.6432)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7483)
features.10.conv.0 tensor(0.0537)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.2025)
features.11.conv.0 tensor(0.7976)
features.11.conv.3 tensor(0.6372)
features.11.conv.6 tensor(0.8638)
features.12.conv.0 tensor(0.8043)
features.12.conv.3 tensor(0.6726)
features.12.conv.6 tensor(0.8950)
features.13.conv.0 tensor(0.4135)
features.13.conv.3 tensor(0.4838)
features.13.conv.6 tensor(0.6577)
features.14.conv.0 tensor(0.9310)
features.14.conv.3 tensor(0.8293)
features.14.conv.6 tensor(0.9704)
features.15.conv.0 tensor(0.9159)
features.15.conv.3 tensor(0.8376)
features.15.conv.6 tensor(0.9737)
features.16.conv.0 tensor(0.7365)
features.16.conv.3 tensor(0.8046)
features.16.conv.6 tensor(0.9329)
conv.0 tensor(0.4207)
tensor(1575761.) 2188896.0
INFO - Validation [10][   20/   40]   Loss 0.438099   Top1 87.011719   Top5 99.414062   BatchTime 0.147990
INFO - Validation [10][   40/   40]   Loss 0.420838   Top1 87.070000   Top5 99.540000   BatchTime 0.101398
INFO - ==> Top1: 87.070    Top5: 99.540    Loss: 0.421
INFO - ==> Sparsity : 0.720
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.4402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.5111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][   20/  196]   Loss 0.532516   Top1 82.753906   Top5 97.011719   BatchTime 0.330259   LR 0.000248
tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][   40/  196]   Loss 0.501437   Top1 83.691406   Top5 98.134766   BatchTime 0.280712   LR 0.000248
tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][   60/  196]   Loss 0.450473   Top1 85.039062   Top5 98.619792   BatchTime 0.266516   LR 0.000247
tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][   80/  196]   Loss 0.415775   Top1 86.015625   Top5 98.911133   BatchTime 0.256371   LR 0.000247
tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][  100/  196]   Loss 0.388828   Top1 86.855469   Top5 99.089844   BatchTime 0.252130   LR 0.000247
tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][  120/  196]   Loss 0.367543   Top1 87.565104   Top5 99.189453   BatchTime 0.249752   LR 0.000246
tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3040, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][  140/  196]   Loss 0.350479   Top1 88.111049   Top5 99.294085   BatchTime 0.247172   LR 0.000246
tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][  160/  196]   Loss 0.336257   Top1 88.554688   Top5 99.367676   BatchTime 0.246259   LR 0.000245
tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [11][  180/  196]   Loss 0.325845   Top1 88.869358   Top5 99.424913   BatchTime 0.243694   LR 0.000244
tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 89.100    Top5: 99.458    Loss: 0.319
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.427027   Top1 86.894531   Top5 99.511719   BatchTime 0.148799
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4082)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0894)
features.2.conv.0 tensor(0.1322)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.6424)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.2303)
features.4.conv.0 tensor(0.0947)
features.4.conv.3 tensor(0.2934)
features.4.conv.6 tensor(0.4917)
features.5.conv.0 tensor(0.4839)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.6177)
features.6.conv.0 tensor(0.0451)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0816)
features.7.conv.0 tensor(0.2014)
features.7.conv.3 tensor(0.4511)
features.7.conv.6 tensor(0.6330)
features.8.conv.0 tensor(0.6787)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7343)
features.9.conv.0 tensor(0.6485)
features.9.conv.3 tensor(0.5550)
features.9.conv.6 tensor(0.7537)
features.10.conv.0 tensor(0.0560)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.2168)
features.11.conv.0 tensor(0.8000)
features.11.conv.3 tensor(0.6381)
features.11.conv.6 tensor(0.8708)
features.12.conv.0 tensor(0.8073)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.8995)
features.13.conv.0 tensor(0.4247)
features.13.conv.3 tensor(0.4797)
features.13.conv.6 tensor(0.6721)
features.14.conv.0 tensor(0.9322)
features.14.conv.3 tensor(0.8295)
features.14.conv.6 tensor(0.9708)
features.15.conv.0 tensor(0.9174)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9745)
features.16.conv.0 tensor(0.7385)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9343)
conv.0 tensor(0.4331)
tensor(1586469.) 2188896.0
INFO - Validation [11][   40/   40]   Loss 0.419085   Top1 86.880000   Top5 99.630000   BatchTime 0.101430
INFO - ==> Top1: 86.880    Top5: 99.630    Loss: 0.419
INFO - ==> Sparsity : 0.725
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][   20/  196]   Loss 0.203832   Top1 92.968750   Top5 99.863281   BatchTime 0.314542   LR 0.000243
tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][   40/  196]   Loss 0.205626   Top1 93.037109   Top5 99.882812   BatchTime 0.272351   LR 0.000243
tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][   60/  196]   Loss 0.208144   Top1 92.851562   Top5 99.882812   BatchTime 0.270492   LR 0.000242
tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][   80/  196]   Loss 0.204322   Top1 92.963867   Top5 99.887695   BatchTime 0.264471   LR 0.000241
tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][  100/  196]   Loss 0.208407   Top1 92.812500   Top5 99.894531   BatchTime 0.257951   LR 0.000240
tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][  120/  196]   Loss 0.207845   Top1 92.802734   Top5 99.902344   BatchTime 0.253022   LR 0.000240
tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][  140/  196]   Loss 0.207584   Top1 92.809710   Top5 99.902344   BatchTime 0.249766   LR 0.000239
tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][  160/  196]   Loss 0.206640   Top1 92.790527   Top5 99.907227   BatchTime 0.248379   LR 0.000238
tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [12][  180/  196]   Loss 0.207507   Top1 92.814670   Top5 99.904514   BatchTime 0.246711   LR 0.000237
tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 92.758    Top5: 99.908    Loss: 0.208
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.403498   Top1 87.285156   Top5 99.453125   BatchTime 0.147623
INFO - Validation [12][   40/   40]   Loss 0.389719   Top1 87.600000   Top5 99.620000   BatchTime 0.101350
INFO - ==> Top1: 87.600    Top5: 99.620    Loss: 0.390
INFO - ==> Sparsity : 0.726
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4062)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0825)
features.2.conv.0 tensor(0.1325)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6450)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.2324)
features.4.conv.0 tensor(0.0975)
features.4.conv.3 tensor(0.2940)
features.4.conv.6 tensor(0.4943)
features.5.conv.0 tensor(0.4873)
features.5.conv.3 tensor(0.4172)
features.5.conv.6 tensor(0.6201)
features.6.conv.0 tensor(0.0479)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0780)
features.7.conv.0 tensor(0.2006)
features.7.conv.3 tensor(0.4523)
features.7.conv.6 tensor(0.6345)
features.8.conv.0 tensor(0.6797)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.7350)
features.9.conv.0 tensor(0.6498)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7551)
features.10.conv.0 tensor(0.0544)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.2239)
features.11.conv.0 tensor(0.8006)
features.11.conv.3 tensor(0.6377)
features.11.conv.6 tensor(0.8731)
features.12.conv.0 tensor(0.8080)
features.12.conv.3 tensor(0.6701)
features.12.conv.6 tensor(0.9003)
features.13.conv.0 tensor(0.4267)
features.13.conv.3 tensor(0.4786)
features.13.conv.6 tensor(0.6738)
features.14.conv.0 tensor(0.9325)
features.14.conv.3 tensor(0.8295)
features.14.conv.6 tensor(0.9712)
features.15.conv.0 tensor(0.9176)
features.15.conv.3 tensor(0.8372)
features.15.conv.6 tensor(0.9748)
features.16.conv.0 tensor(0.7390)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.9348)
conv.0 tensor(0.4366)
tensor(1589236.) 2188896.0
tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][   20/  196]   Loss 0.188649   Top1 93.359375   Top5 99.882812   BatchTime 0.318140   LR 0.000235
tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][   40/  196]   Loss 0.188553   Top1 93.281250   Top5 99.873047   BatchTime 0.271889   LR 0.000235
tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][   60/  196]   Loss 0.184123   Top1 93.541667   Top5 99.895833   BatchTime 0.256310   LR 0.000234
tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][   80/  196]   Loss 0.184452   Top1 93.559570   Top5 99.916992   BatchTime 0.249317   LR 0.000233
tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][  100/  196]   Loss 0.186061   Top1 93.496094   Top5 99.917969   BatchTime 0.245772   LR 0.000232
tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][  120/  196]   Loss 0.182262   Top1 93.623047   Top5 99.925130   BatchTime 0.244585   LR 0.000230
tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][  140/  196]   Loss 0.184353   Top1 93.557478   Top5 99.927455   BatchTime 0.241834   LR 0.000229
tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][  160/  196]   Loss 0.185657   Top1 93.491211   Top5 99.921875   BatchTime 0.241058   LR 0.000228
tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [13][  180/  196]   Loss 0.185746   Top1 93.470052   Top5 99.919705   BatchTime 0.238126   LR 0.000227
tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 93.502    Top5: 99.922    Loss: 0.185
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.417932   Top1 87.148438   Top5 99.550781   BatchTime 0.145906
INFO - Validation [13][   40/   40]   Loss 0.406117   Top1 87.530000   Top5 99.670000   BatchTime 0.100850
INFO - ==> Top1: 87.530    Top5: 99.670    Loss: 0.406
INFO - ==> Sparsity : 0.727
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4121)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.1308)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.6464)
features.3.conv.0 tensor(0.0816)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2396)
features.4.conv.0 tensor(0.0960)
features.4.conv.3 tensor(0.2969)
features.4.conv.6 tensor(0.4964)
features.5.conv.0 tensor(0.4893)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.6216)
features.6.conv.0 tensor(0.0404)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0789)
features.7.conv.0 tensor(0.2010)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6355)
features.8.conv.0 tensor(0.6809)
features.8.conv.3 tensor(0.5437)
features.8.conv.6 tensor(0.7375)
features.9.conv.0 tensor(0.6522)
features.9.conv.3 tensor(0.5535)
features.9.conv.6 tensor(0.7567)
features.10.conv.0 tensor(0.0562)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.2297)
features.11.conv.0 tensor(0.8011)
features.11.conv.3 tensor(0.6387)
features.11.conv.6 tensor(0.8738)
features.12.conv.0 tensor(0.8086)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.9006)
features.13.conv.0 tensor(0.4281)
features.13.conv.3 tensor(0.4824)
features.13.conv.6 tensor(0.6751)
features.14.conv.0 tensor(0.9328)
features.14.conv.3 tensor(0.8297)
features.14.conv.6 tensor(0.9715)
features.15.conv.0 tensor(0.9178)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9749)
features.16.conv.0 tensor(0.7395)
features.16.conv.3 tensor(0.8025)
features.16.conv.6 tensor(0.9353)
conv.0 tensor(0.4394)
tensor(1591560.) 2188896.0
tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][   20/  196]   Loss 0.174717   Top1 93.554688   Top5 99.921875   BatchTime 0.304870   LR 0.000225
tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][   40/  196]   Loss 0.172352   Top1 93.652344   Top5 99.921875   BatchTime 0.266253   LR 0.000224
tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][   60/  196]   Loss 0.174839   Top1 93.567708   Top5 99.921875   BatchTime 0.261126   LR 0.000223
tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][   80/  196]   Loss 0.179058   Top1 93.559570   Top5 99.926758   BatchTime 0.254466   LR 0.000221
tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][  100/  196]   Loss 0.177554   Top1 93.652344   Top5 99.933594   BatchTime 0.251939   LR 0.000220
tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][  120/  196]   Loss 0.176245   Top1 93.766276   Top5 99.934896   BatchTime 0.248715   LR 0.000219
tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][  140/  196]   Loss 0.175565   Top1 93.777902   Top5 99.935826   BatchTime 0.245837   LR 0.000217
tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][  160/  196]   Loss 0.175080   Top1 93.771973   Top5 99.931641   BatchTime 0.245377   LR 0.000216
tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [14][  180/  196]   Loss 0.175897   Top1 93.754340   Top5 99.924045   BatchTime 0.242481   LR 0.000215
tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 93.762    Top5: 99.926    Loss: 0.177
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 0.381406   Top1 88.750000   Top5 99.570312   BatchTime 0.152918
INFO - Validation [14][   40/   40]   Loss 0.377993   Top1 88.480000   Top5 99.640000   BatchTime 0.104430
INFO - ==> Top1: 88.480    Top5: 99.640    Loss: 0.378
INFO - ==> Sparsity : 0.728
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.4141)
features.1.conv.0 tensor(0.0612)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.1340)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.6487)
features.3.conv.0 tensor(0.0807)
features.3.conv.3 tensor(0.0694)
features.3.conv.6 tensor(0.2405)
features.4.conv.0 tensor(0.0970)
features.4.conv.3 tensor(0.2922)
features.4.conv.6 tensor(0.4972)
features.5.conv.0 tensor(0.4901)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6234)
features.6.conv.0 tensor(0.0409)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0757)
features.7.conv.0 tensor(0.2000)
features.7.conv.3 tensor(0.4517)
features.7.conv.6 tensor(0.6376)
features.8.conv.0 tensor(0.6821)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.7382)
features.9.conv.0 tensor(0.6528)
features.9.conv.3 tensor(0.5530)
features.9.conv.6 tensor(0.7582)
features.10.conv.0 tensor(0.0553)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.2348)
features.11.conv.0 tensor(0.8018)
features.11.conv.3 tensor(0.6379)
features.11.conv.6 tensor(0.8747)
features.12.conv.0 tensor(0.8090)
features.12.conv.3 tensor(0.6707)
features.12.conv.6 tensor(0.9011)
features.13.conv.0 tensor(0.4298)
features.13.conv.3 tensor(0.4796)
features.13.conv.6 tensor(0.6764)
features.14.conv.0 tensor(0.9329)
features.14.conv.3 tensor(0.8304)
features.14.conv.6 tensor(0.9715)
features.15.conv.0 tensor(0.9179)
features.15.conv.3 tensor(0.8377)
features.15.conv.6 tensor(0.9750)
features.16.conv.0 tensor(0.7399)
features.16.conv.3 tensor(0.8030)
features.16.conv.6 tensor(0.9356)
conv.0 tensor(0.4418)
tensor(1593350.) 2188896.0
tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][   20/  196]   Loss 0.171030   Top1 93.789062   Top5 99.980469   BatchTime 0.338033   LR 0.000212
tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][   40/  196]   Loss 0.163501   Top1 94.091797   Top5 99.951172   BatchTime 0.281431   LR 0.000211
tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][   60/  196]   Loss 0.164687   Top1 94.082031   Top5 99.947917   BatchTime 0.262604   LR 0.000209
tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][   80/  196]   Loss 0.162300   Top1 94.208984   Top5 99.951172   BatchTime 0.256092   LR 0.000208
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][  100/  196]   Loss 0.164465   Top1 94.183594   Top5 99.941406   BatchTime 0.251008   LR 0.000206
tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][  120/  196]   Loss 0.166159   Top1 94.108073   Top5 99.941406   BatchTime 0.248206   LR 0.000205
tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][  140/  196]   Loss 0.167778   Top1 94.076451   Top5 99.946987   BatchTime 0.244695   LR 0.000203
tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][  160/  196]   Loss 0.167894   Top1 94.077148   Top5 99.948730   BatchTime 0.242940   LR 0.000201
tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [15][  180/  196]   Loss 0.167345   Top1 94.095052   Top5 99.950087   BatchTime 0.242401   LR 0.000200
tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 94.078    Top5: 99.946    Loss: 0.167
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.393919   Top1 87.968750   Top5 99.472656   BatchTime 0.147980
INFO - Validation [15][   40/   40]   Loss 0.386579   Top1 88.120000   Top5 99.610000   BatchTime 0.100548
INFO - ==> Top1: 88.120    Top5: 99.610    Loss: 0.387
INFO - ==> Sparsity : 0.729
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4141)
features.1.conv.0 tensor(0.0579)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0842)
features.2.conv.0 tensor(0.1354)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6487)
features.3.conv.0 tensor(0.0822)
features.3.conv.3 tensor(0.0694)
features.3.conv.6 tensor(0.2437)
features.4.conv.0 tensor(0.0952)
features.4.conv.3 tensor(0.2946)
features.4.conv.6 tensor(0.5002)
features.5.conv.0 tensor(0.4909)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6260)
features.6.conv.0 tensor(0.0438)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0784)
features.7.conv.0 tensor(0.2011)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.6394)
features.8.conv.0 tensor(0.6833)
features.8.conv.3 tensor(0.5394)
features.8.conv.6 tensor(0.7409)
features.9.conv.0 tensor(0.6545)
features.9.conv.3 tensor(0.5535)
features.9.conv.6 tensor(0.7598)
features.10.conv.0 tensor(0.0544)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.2405)
features.11.conv.0 tensor(0.8021)
features.11.conv.3 tensor(0.6395)
features.11.conv.6 tensor(0.8750)
features.12.conv.0 tensor(0.8100)
features.12.conv.3 tensor(0.6711)
features.12.conv.6 tensor(0.9016)
features.13.conv.0 tensor(0.4317)
features.13.conv.3 tensor(0.4809)
features.13.conv.6 tensor(0.6774)
features.14.conv.0 tensor(0.9330)
features.14.conv.3 tensor(0.8296)
features.14.conv.6 tensor(0.9719)
features.15.conv.0 tensor(0.9182)
features.15.conv.3 tensor(0.8375)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7401)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.9359)
conv.0 tensor(0.4440)
tensor(1595388.) 2188896.0
tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][   20/  196]   Loss 0.155852   Top1 94.863281   Top5 99.921875   BatchTime 0.350131   LR 0.000197
tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][   40/  196]   Loss 0.154523   Top1 94.667969   Top5 99.931641   BatchTime 0.295325   LR 0.000195
tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][   60/  196]   Loss 0.158788   Top1 94.524740   Top5 99.928385   BatchTime 0.276586   LR 0.000194
tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][   80/  196]   Loss 0.160374   Top1 94.438477   Top5 99.931641   BatchTime 0.266921   LR 0.000192
tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][  100/  196]   Loss 0.163020   Top1 94.281250   Top5 99.933594   BatchTime 0.259235   LR 0.000190
tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][  120/  196]   Loss 0.161370   Top1 94.332682   Top5 99.938151   BatchTime 0.253370   LR 0.000188
tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][  140/  196]   Loss 0.160834   Top1 94.369420   Top5 99.924665   BatchTime 0.250875   LR 0.000187
tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][  160/  196]   Loss 0.162484   Top1 94.338379   Top5 99.926758   BatchTime 0.248911   LR 0.000185
tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [16][  180/  196]   Loss 0.162647   Top1 94.346788   Top5 99.924045   BatchTime 0.247324   LR 0.000183
tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 94.306    Top5: 99.924    Loss: 0.163
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.386109   Top1 88.085938   Top5 99.628906   BatchTime 0.150626
INFO - Validation [16][   40/   40]   Loss 0.388862   Top1 88.000000   Top5 99.690000   BatchTime 0.102186
INFO - ==> Top1: 88.000    Top5: 99.690    Loss: 0.389
INFO - ==> Sparsity : 0.730
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1325)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6510)
features.3.conv.0 tensor(0.0787)
features.3.conv.3 tensor(0.0702)
features.3.conv.6 tensor(0.2452)
features.4.conv.0 tensor(0.0967)
features.4.conv.3 tensor(0.2940)
features.4.conv.6 tensor(0.5005)
features.5.conv.0 tensor(0.4914)
features.5.conv.3 tensor(0.4132)
features.5.conv.6 tensor(0.6278)
features.6.conv.0 tensor(0.0417)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0791)
features.7.conv.0 tensor(0.2017)
features.7.conv.3 tensor(0.4525)
features.7.conv.6 tensor(0.6409)
features.8.conv.0 tensor(0.6842)
features.8.conv.3 tensor(0.5422)
features.8.conv.6 tensor(0.7425)
features.9.conv.0 tensor(0.6550)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7611)
features.10.conv.0 tensor(0.0543)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.2448)
features.11.conv.0 tensor(0.8028)
features.11.conv.3 tensor(0.6383)
features.11.conv.6 tensor(0.8756)
features.12.conv.0 tensor(0.8105)
features.12.conv.3 tensor(0.6686)
features.12.conv.6 tensor(0.9023)
features.13.conv.0 tensor(0.4328)
features.13.conv.3 tensor(0.4840)
features.13.conv.6 tensor(0.6786)
features.14.conv.0 tensor(0.9330)
features.14.conv.3 tensor(0.8303)
features.14.conv.6 tensor(0.9719)
features.15.conv.0 tensor(0.9181)
features.15.conv.3 tensor(0.8372)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7403)
features.16.conv.3 tensor(0.8037)
features.16.conv.6 tensor(0.9361)
conv.0 tensor(0.4459)
tensor(1596907.) 2188896.0
tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][   20/  196]   Loss 0.152507   Top1 94.453125   Top5 99.902344   BatchTime 0.326019   LR 0.000180
tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][   40/  196]   Loss 0.155805   Top1 94.482422   Top5 99.931641   BatchTime 0.275368   LR 0.000178
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][   60/  196]   Loss 0.154545   Top1 94.472656   Top5 99.954427   BatchTime 0.260116   LR 0.000176
tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][   80/  196]   Loss 0.152906   Top1 94.497070   Top5 99.956055   BatchTime 0.253277   LR 0.000175
tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][  100/  196]   Loss 0.151982   Top1 94.570312   Top5 99.953125   BatchTime 0.247715   LR 0.000173
tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][  120/  196]   Loss 0.152909   Top1 94.554036   Top5 99.954427   BatchTime 0.247308   LR 0.000171
tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][  140/  196]   Loss 0.153648   Top1 94.514509   Top5 99.960938   BatchTime 0.245407   LR 0.000169
tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][  160/  196]   Loss 0.152894   Top1 94.577637   Top5 99.951172   BatchTime 0.242598   LR 0.000167
tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [17][  180/  196]   Loss 0.153407   Top1 94.555122   Top5 99.952257   BatchTime 0.240306   LR 0.000165
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 94.604    Top5: 99.950    Loss: 0.153
tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [17][   20/   40]   Loss 0.420519   Top1 87.773438   Top5 99.453125   BatchTime 0.150915
INFO - Validation [17][   40/   40]   Loss 0.409527   Top1 87.720000   Top5 99.590000   BatchTime 0.104913
INFO - ==> Top1: 87.720    Top5: 99.590    Loss: 0.410
INFO - ==> Sparsity : 0.730
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.4199)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.6519)
features.3.conv.0 tensor(0.0799)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2467)
features.4.conv.0 tensor(0.0933)
features.4.conv.3 tensor(0.2922)
features.4.conv.6 tensor(0.5011)
features.5.conv.0 tensor(0.4925)
features.5.conv.3 tensor(0.4115)
features.5.conv.6 tensor(0.6287)
features.6.conv.0 tensor(0.0472)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.1998)
features.7.conv.3 tensor(0.4520)
features.7.conv.6 tensor(0.6428)
features.8.conv.0 tensor(0.6855)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.7439)
features.9.conv.0 tensor(0.6563)
features.9.conv.3 tensor(0.5535)
features.9.conv.6 tensor(0.7622)
features.10.conv.0 tensor(0.0529)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.2485)
features.11.conv.0 tensor(0.8031)
features.11.conv.3 tensor(0.6381)
features.11.conv.6 tensor(0.8766)
features.12.conv.0 tensor(0.8109)
features.12.conv.3 tensor(0.6690)
features.12.conv.6 tensor(0.9025)
features.13.conv.0 tensor(0.4341)
features.13.conv.3 tensor(0.4809)
features.13.conv.6 tensor(0.6793)
features.14.conv.0 tensor(0.9331)
features.14.conv.3 tensor(0.8302)
features.14.conv.6 tensor(0.9721)
features.15.conv.0 tensor(0.9182)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7406)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9362)
conv.0 tensor(0.4477)
tensor(1598298.) 2188896.0
tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][   20/  196]   Loss 0.138666   Top1 95.136719   Top5 99.960938   BatchTime 0.328840   LR 0.000162
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][   40/  196]   Loss 0.140917   Top1 95.205078   Top5 99.960938   BatchTime 0.283627   LR 0.000160
tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][   60/  196]   Loss 0.141009   Top1 95.084635   Top5 99.967448   BatchTime 0.269765   LR 0.000158
tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][   80/  196]   Loss 0.136893   Top1 95.219727   Top5 99.970703   BatchTime 0.261084   LR 0.000156
tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][  100/  196]   Loss 0.134554   Top1 95.265625   Top5 99.972656   BatchTime 0.255465   LR 0.000154
tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][  120/  196]   Loss 0.134626   Top1 95.218099   Top5 99.977214   BatchTime 0.252945   LR 0.000152
tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][  140/  196]   Loss 0.133795   Top1 95.203683   Top5 99.974888   BatchTime 0.248891   LR 0.000150
tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][  160/  196]   Loss 0.133920   Top1 95.200195   Top5 99.963379   BatchTime 0.246753   LR 0.000148
tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [18][  180/  196]   Loss 0.135527   Top1 95.106337   Top5 99.960938   BatchTime 0.244586   LR 0.000146
tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 95.070    Top5: 99.956    Loss: 0.137
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [18][   20/   40]   Loss 0.389279   Top1 88.671875   Top5 99.511719   BatchTime 0.144418
INFO - Validation [18][   40/   40]   Loss 0.376060   Top1 88.720000   Top5 99.620000   BatchTime 0.100685
INFO - ==> Top1: 88.720    Top5: 99.620    Loss: 0.376
INFO - ==> Sparsity : 0.731
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.4199)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0907)
features.2.conv.0 tensor(0.1296)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6536)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.2500)
features.4.conv.0 tensor(0.0920)
features.4.conv.3 tensor(0.2888)
features.4.conv.6 tensor(0.5034)
features.5.conv.0 tensor(0.4932)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.6291)
features.6.conv.0 tensor(0.0402)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0749)
features.7.conv.0 tensor(0.2011)
features.7.conv.3 tensor(0.4531)
features.7.conv.6 tensor(0.6442)
features.8.conv.0 tensor(0.6861)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7446)
features.9.conv.0 tensor(0.6567)
features.9.conv.3 tensor(0.5535)
features.9.conv.6 tensor(0.7633)
features.10.conv.0 tensor(0.0516)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.2516)
features.11.conv.0 tensor(0.8034)
features.11.conv.3 tensor(0.6397)
features.11.conv.6 tensor(0.8770)
features.12.conv.0 tensor(0.8114)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9027)
features.13.conv.0 tensor(0.4348)
features.13.conv.3 tensor(0.4819)
features.13.conv.6 tensor(0.6801)
features.14.conv.0 tensor(0.9331)
features.14.conv.3 tensor(0.8299)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9183)
features.15.conv.3 tensor(0.8372)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7408)
features.16.conv.3 tensor(0.8036)
features.16.conv.6 tensor(0.9363)
conv.0 tensor(0.4492)
tensor(1599383.) 2188896.0
tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][   20/  196]   Loss 0.146920   Top1 94.746094   Top5 99.941406   BatchTime 0.318012   LR 0.000143
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][   40/  196]   Loss 0.139861   Top1 95.029297   Top5 99.960938   BatchTime 0.277595   LR 0.000141
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][   60/  196]   Loss 0.133106   Top1 95.292969   Top5 99.967448   BatchTime 0.261042   LR 0.000139
tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][   80/  196]   Loss 0.131857   Top1 95.351562   Top5 99.965820   BatchTime 0.256518   LR 0.000137
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][  100/  196]   Loss 0.131166   Top1 95.367188   Top5 99.968750   BatchTime 0.253440   LR 0.000135
tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][  120/  196]   Loss 0.129855   Top1 95.419922   Top5 99.970703   BatchTime 0.250419   LR 0.000133
tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][  140/  196]   Loss 0.130627   Top1 95.410156   Top5 99.969308   BatchTime 0.246243   LR 0.000131
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][  160/  196]   Loss 0.130548   Top1 95.368652   Top5 99.965820   BatchTime 0.244678   LR 0.000129
tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [19][  180/  196]   Loss 0.131284   Top1 95.345052   Top5 99.958767   BatchTime 0.241700   LR 0.000127
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 95.372    Top5: 99.958    Loss: 0.131
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 0.388251   Top1 88.613281   Top5 99.687500   BatchTime 0.146646
INFO - Validation [19][   40/   40]   Loss 0.382450   Top1 88.690000   Top5 99.760000   BatchTime 0.101455
INFO - ==> Top1: 88.690    Top5: 99.760    Loss: 0.382
INFO - ==> Sparsity : 0.731
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
features.0.conv.0 tensor(0.2674)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0907)
features.2.conv.0 tensor(0.1314)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6545)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2513)
features.4.conv.0 tensor(0.0923)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5033)
features.5.conv.0 tensor(0.4935)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.6304)
features.6.conv.0 tensor(0.0409)
features.6.conv.3 tensor(0.0521)
features.6.conv.6 tensor(0.0784)
features.7.conv.0 tensor(0.2010)
features.7.conv.3 tensor(0.4549)
features.7.conv.6 tensor(0.6445)
features.8.conv.0 tensor(0.6863)
features.8.conv.3 tensor(0.5394)
features.8.conv.6 tensor(0.7452)
features.9.conv.0 tensor(0.6573)
features.9.conv.3 tensor(0.5541)
features.9.conv.6 tensor(0.7635)
features.10.conv.0 tensor(0.0535)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.2534)
features.11.conv.0 tensor(0.8039)
features.11.conv.3 tensor(0.6385)
features.11.conv.6 tensor(0.8771)
features.12.conv.0 tensor(0.8118)
features.12.conv.3 tensor(0.6696)
features.12.conv.6 tensor(0.9028)
features.13.conv.0 tensor(0.4358)
features.13.conv.3 tensor(0.4811)
features.13.conv.6 tensor(0.6806)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8304)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8370)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7410)
features.16.conv.3 tensor(0.8030)
features.16.conv.6 tensor(0.9365)
conv.0 tensor(0.4504)
tensor(1600369.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][   20/  196]   Loss 0.113341   Top1 96.074219   Top5 99.980469   BatchTime 0.332340   LR 0.000123
tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][   40/  196]   Loss 0.113636   Top1 96.044922   Top5 99.990234   BatchTime 0.284271   LR 0.000121
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][   60/  196]   Loss 0.117566   Top1 95.924479   Top5 99.986979   BatchTime 0.268334   LR 0.000119
tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][   80/  196]   Loss 0.116315   Top1 95.903320   Top5 99.985352   BatchTime 0.258715   LR 0.000117
tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][  100/  196]   Loss 0.115202   Top1 95.921875   Top5 99.980469   BatchTime 0.252526   LR 0.000115
tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][  120/  196]   Loss 0.116524   Top1 95.852865   Top5 99.977214   BatchTime 0.248671   LR 0.000113
tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][  140/  196]   Loss 0.117185   Top1 95.825893   Top5 99.977679   BatchTime 0.247031   LR 0.000111
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][  160/  196]   Loss 0.116352   Top1 95.871582   Top5 99.980469   BatchTime 0.244765   LR 0.000109
tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [20][  180/  196]   Loss 0.115432   Top1 95.894097   Top5 99.978299   BatchTime 0.245061   LR 0.000107
tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 95.890    Top5: 99.978    Loss: 0.115
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 0.378326   Top1 88.886719   Top5 99.550781   BatchTime 0.150236
INFO - Validation [20][   40/   40]   Loss 0.373973   Top1 88.960000   Top5 99.670000   BatchTime 0.104230
INFO - ==> Top1: 88.960    Top5: 99.670    Loss: 0.374
INFO - ==> Sparsity : 0.731
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.510   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.4199)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.1325)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6534)
features.3.conv.0 tensor(0.0770)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2520)
features.4.conv.0 tensor(0.0944)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5033)
features.5.conv.0 tensor(0.4937)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6309)
features.6.conv.0 tensor(0.0402)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0775)
features.7.conv.0 tensor(0.2008)
features.7.conv.3 tensor(0.4517)
features.7.conv.6 tensor(0.6449)
features.8.conv.0 tensor(0.6869)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7453)
features.9.conv.0 tensor(0.6575)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.7640)
features.10.conv.0 tensor(0.0532)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.2552)
features.11.conv.0 tensor(0.8039)
features.11.conv.3 tensor(0.6381)
features.11.conv.6 tensor(0.8773)
features.12.conv.0 tensor(0.8122)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.9028)
features.13.conv.0 tensor(0.4362)
features.13.conv.3 tensor(0.4792)
features.13.conv.6 tensor(0.6811)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8301)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7411)
features.16.conv.3 tensor(0.8035)
features.16.conv.6 tensor(0.9366)
conv.0 tensor(0.4512)
tensor(1600876.) 2188896.0
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][   20/  196]   Loss 0.103062   Top1 96.289062   Top5 99.980469   BatchTime 0.331061   LR 0.000104
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][   40/  196]   Loss 0.108581   Top1 96.201172   Top5 99.970703   BatchTime 0.281121   LR 0.000102
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][   60/  196]   Loss 0.107597   Top1 96.210938   Top5 99.960938   BatchTime 0.266917   LR 0.000100
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][   80/  196]   Loss 0.109568   Top1 96.147461   Top5 99.965820   BatchTime 0.260118   LR 0.000098
tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][  100/  196]   Loss 0.111737   Top1 96.078125   Top5 99.964844   BatchTime 0.254624   LR 0.000096
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][  120/  196]   Loss 0.109757   Top1 96.184896   Top5 99.970703   BatchTime 0.251197   LR 0.000094
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][  140/  196]   Loss 0.107617   Top1 96.250000   Top5 99.972098   BatchTime 0.247545   LR 0.000092
tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][  160/  196]   Loss 0.108235   Top1 96.223145   Top5 99.975586   BatchTime 0.246869   LR 0.000090
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [21][  180/  196]   Loss 0.107296   Top1 96.241319   Top5 99.978299   BatchTime 0.245419   LR 0.000088
tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.248    Top5: 99.978    Loss: 0.107
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [21][   20/   40]   Loss 0.366559   Top1 89.765625   Top5 99.667969   BatchTime 0.150095
INFO - Validation [21][   40/   40]   Loss 0.362767   Top1 89.620000   Top5 99.780000   BatchTime 0.106310
INFO - ==> Top1: 89.620    Top5: 99.780    Loss: 0.363
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 89.620   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 89.620   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1317)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6539)
features.3.conv.0 tensor(0.0799)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.2526)
features.4.conv.0 tensor(0.0926)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5050)
features.5.conv.0 tensor(0.4937)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6313)
features.6.conv.0 tensor(0.0407)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.2002)
features.7.conv.3 tensor(0.4525)
features.7.conv.6 tensor(0.6452)
features.8.conv.0 tensor(0.6872)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7459)
features.9.conv.0 tensor(0.6580)
features.9.conv.3 tensor(0.5532)
features.9.conv.6 tensor(0.7642)
features.10.conv.0 tensor(0.0534)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.2566)
features.11.conv.0 tensor(0.8042)
features.11.conv.3 tensor(0.6387)
features.11.conv.6 tensor(0.8775)
features.12.conv.0 tensor(0.8121)
features.12.conv.3 tensor(0.6699)
features.12.conv.6 tensor(0.9029)
features.13.conv.0 tensor(0.4361)
features.13.conv.3 tensor(0.4805)
features.13.conv.6 tensor(0.6813)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8303)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9185)
features.15.conv.3 tensor(0.8372)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7413)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9366)
conv.0 tensor(0.4517)
tensor(1601316.) 2188896.0
tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][   20/  196]   Loss 0.094933   Top1 96.523438   Top5 100.000000   BatchTime 0.308557   LR 0.000085
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][   40/  196]   Loss 0.094381   Top1 96.503906   Top5 100.000000   BatchTime 0.267170   LR 0.000083
tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][   60/  196]   Loss 0.096902   Top1 96.477865   Top5 99.980469   BatchTime 0.254001   LR 0.000081
tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][   80/  196]   Loss 0.097508   Top1 96.479492   Top5 99.985352   BatchTime 0.251918   LR 0.000079
tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][  100/  196]   Loss 0.097759   Top1 96.519531   Top5 99.988281   BatchTime 0.248031   LR 0.000077
tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][  120/  196]   Loss 0.098011   Top1 96.484375   Top5 99.990234   BatchTime 0.245581   LR 0.000075
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][  140/  196]   Loss 0.100197   Top1 96.417411   Top5 99.988839   BatchTime 0.245323   LR 0.000073
tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][  160/  196]   Loss 0.099681   Top1 96.411133   Top5 99.987793   BatchTime 0.246344   LR 0.000072
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [22][  180/  196]   Loss 0.097913   Top1 96.486545   Top5 99.989149   BatchTime 0.245526   LR 0.000070
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.490    Top5: 99.990    Loss: 0.098
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.370531   Top1 90.019531   Top5 99.648438   BatchTime 0.146884
INFO - Validation [22][   40/   40]   Loss 0.364527   Top1 89.980000   Top5 99.740000   BatchTime 0.100042
INFO - ==> Top1: 89.980    Top5: 99.740    Loss: 0.365
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 89.620   Top5: 99.780]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4238)
features.1.conv.0 tensor(0.0579)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.1322)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6548)
features.3.conv.0 tensor(0.0781)
features.3.conv.3 tensor(0.0710)
features.3.conv.6 tensor(0.2524)
features.4.conv.0 tensor(0.0910)
features.4.conv.3 tensor(0.2905)
features.4.conv.6 tensor(0.5055)
features.5.conv.0 tensor(0.4937)
features.5.conv.3 tensor(0.4132)
features.5.conv.6 tensor(0.6312)
features.6.conv.0 tensor(0.0399)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0754)
features.7.conv.0 tensor(0.2019)
features.7.conv.3 tensor(0.4520)
features.7.conv.6 tensor(0.6457)
features.8.conv.0 tensor(0.6874)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7460)
features.9.conv.0 tensor(0.6583)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7644)
features.10.conv.0 tensor(0.0523)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.2579)
features.11.conv.0 tensor(0.8043)
features.11.conv.3 tensor(0.6400)
features.11.conv.6 tensor(0.8777)
features.12.conv.0 tensor(0.8122)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.9029)
features.13.conv.0 tensor(0.4363)
features.13.conv.3 tensor(0.4805)
features.13.conv.6 tensor(0.6813)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8301)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9367)
conv.0 tensor(0.4520)
tensor(1601537.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][   20/  196]   Loss 0.086292   Top1 96.992188   Top5 100.000000   BatchTime 0.339116   LR 0.000067
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][   40/  196]   Loss 0.088975   Top1 96.894531   Top5 100.000000   BatchTime 0.295313   LR 0.000065
tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][   60/  196]   Loss 0.088712   Top1 96.907552   Top5 99.986979   BatchTime 0.275114   LR 0.000063
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][   80/  196]   Loss 0.090496   Top1 96.796875   Top5 99.980469   BatchTime 0.266374   LR 0.000061
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][  100/  196]   Loss 0.090912   Top1 96.750000   Top5 99.980469   BatchTime 0.261167   LR 0.000060
tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][  120/  196]   Loss 0.092289   Top1 96.699219   Top5 99.983724   BatchTime 0.257902   LR 0.000058
tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][  140/  196]   Loss 0.093010   Top1 96.674107   Top5 99.986049   BatchTime 0.253772   LR 0.000056
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][  160/  196]   Loss 0.093837   Top1 96.652832   Top5 99.985352   BatchTime 0.251718   LR 0.000055
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [23][  180/  196]   Loss 0.094302   Top1 96.649306   Top5 99.980469   BatchTime 0.249159   LR 0.000053
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.632    Top5: 99.980    Loss: 0.095
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [23][   20/   40]   Loss 0.380539   Top1 89.355469   Top5 99.589844   BatchTime 0.144716
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4258)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0842)
features.2.conv.0 tensor(0.1325)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.6554)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0718)
features.3.conv.6 tensor(0.2530)
features.4.conv.0 tensor(0.0941)
features.4.conv.3 tensor(0.2899)
features.4.conv.6 tensor(0.5054)
features.5.conv.0 tensor(0.4937)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.6310)
features.6.conv.0 tensor(0.0391)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0739)
features.7.conv.0 tensor(0.2010)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6455)
features.8.conv.0 tensor(0.6873)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.7461)
features.9.conv.0 tensor(0.6579)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7645)
features.10.conv.0 tensor(0.0533)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.2584)
features.11.conv.0 tensor(0.8043)
features.11.conv.3 tensor(0.6389)
features.11.conv.6 tensor(0.8777)
features.12.conv.0 tensor(0.8122)
features.12.conv.3 tensor(0.6692)
features.12.conv.6 tensor(0.9029)
features.13.conv.0 tensor(0.4365)
features.13.conv.3 tensor(0.4794)
features.13.conv.6 tensor(0.6815)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8301)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9367)
conv.0 tensor(0.4523)
tensor(1601660.) 2188896.0
INFO - Validation [23][   40/   40]   Loss 0.376844   Top1 89.410000   Top5 99.700000   BatchTime 0.097315
INFO - ==> Top1: 89.410    Top5: 99.700    Loss: 0.377
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 89.620   Top5: 99.780]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][   20/  196]   Loss 0.087380   Top1 96.699219   Top5 99.980469   BatchTime 0.370924   LR 0.000050
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][   40/  196]   Loss 0.087106   Top1 96.865234   Top5 99.960938   BatchTime 0.304846   LR 0.000048
tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][   60/  196]   Loss 0.085823   Top1 96.875000   Top5 99.973958   BatchTime 0.280219   LR 0.000047
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][   80/  196]   Loss 0.085856   Top1 96.923828   Top5 99.975586   BatchTime 0.267006   LR 0.000045
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][  100/  196]   Loss 0.087790   Top1 96.851562   Top5 99.980469   BatchTime 0.261626   LR 0.000044
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][  120/  196]   Loss 0.087478   Top1 96.927083   Top5 99.977214   BatchTime 0.257867   LR 0.000042
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][  140/  196]   Loss 0.087092   Top1 96.941964   Top5 99.980469   BatchTime 0.255311   LR 0.000041
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][  160/  196]   Loss 0.086895   Top1 96.953125   Top5 99.980469   BatchTime 0.252794   LR 0.000039
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [24][  180/  196]   Loss 0.086856   Top1 96.946615   Top5 99.980469   BatchTime 0.249912   LR 0.000038
tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.948    Top5: 99.982    Loss: 0.086
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 0.380365   Top1 89.765625   Top5 99.550781   BatchTime 0.149685
INFO - Validation [24][   40/   40]   Loss 0.372505   Top1 89.710000   Top5 99.700000   BatchTime 0.104177
INFO - ==> Top1: 89.710    Top5: 99.700    Loss: 0.373
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 89.710   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4219)
features.1.conv.0 tensor(0.0592)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.1331)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.6554)
features.3.conv.0 tensor(0.0796)
features.3.conv.3 tensor(0.0718)
features.3.conv.6 tensor(0.2539)
features.4.conv.0 tensor(0.0928)
features.4.conv.3 tensor(0.2905)
features.4.conv.6 tensor(0.5050)
features.5.conv.0 tensor(0.4935)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.6309)
features.6.conv.0 tensor(0.0397)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0744)
features.7.conv.0 tensor(0.2016)
features.7.conv.3 tensor(0.4534)
features.7.conv.6 tensor(0.6455)
features.8.conv.0 tensor(0.6874)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7460)
features.9.conv.0 tensor(0.6580)
features.9.conv.3 tensor(0.5518)
features.9.conv.6 tensor(0.7644)
features.10.conv.0 tensor(0.0523)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.2586)
features.11.conv.0 tensor(0.8043)
features.11.conv.3 tensor(0.6393)
features.11.conv.6 tensor(0.8779)
features.12.conv.0 tensor(0.8122)
features.12.conv.3 tensor(0.6692)
features.12.conv.6 tensor(0.9029)
features.13.conv.0 tensor(0.4366)
features.13.conv.3 tensor(0.4797)
features.13.conv.6 tensor(0.6813)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8303)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8376)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9368)
conv.0 tensor(0.4523)
tensor(1601711.) 2188896.0
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][   20/  196]   Loss 0.070680   Top1 97.343750   Top5 99.980469   BatchTime 0.344782   LR 0.000035
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][   40/  196]   Loss 0.076020   Top1 97.412109   Top5 99.980469   BatchTime 0.289456   LR 0.000034
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][   60/  196]   Loss 0.075486   Top1 97.402344   Top5 99.986979   BatchTime 0.272806   LR 0.000033
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][   80/  196]   Loss 0.077064   Top1 97.348633   Top5 99.990234   BatchTime 0.260117   LR 0.000031
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][  100/  196]   Loss 0.077617   Top1 97.332031   Top5 99.992188   BatchTime 0.254677   LR 0.000030
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][  120/  196]   Loss 0.078579   Top1 97.252604   Top5 99.990234   BatchTime 0.251623   LR 0.000029
tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][  140/  196]   Loss 0.079076   Top1 97.243304   Top5 99.986049   BatchTime 0.248477   LR 0.000027
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][  160/  196]   Loss 0.078058   Top1 97.280273   Top5 99.985352   BatchTime 0.247174   LR 0.000026
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [25][  180/  196]   Loss 0.077783   Top1 97.291667   Top5 99.986979   BatchTime 0.245812   LR 0.000025
tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 97.276    Top5: 99.986    Loss: 0.078
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [25][   20/   40]   Loss 0.370892   Top1 90.039062   Top5 99.531250   BatchTime 0.148080
INFO - Validation [25][   40/   40]   Loss 0.365479   Top1 89.940000   Top5 99.680000   BatchTime 0.101141
INFO - ==> Top1: 89.940    Top5: 99.680    Loss: 0.365
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 89.940   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 89.810   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4238)
features.1.conv.0 tensor(0.0605)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0838)
features.2.conv.0 tensor(0.1317)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.6557)
features.3.conv.0 tensor(0.0778)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.2535)
features.4.conv.0 tensor(0.0924)
features.4.conv.3 tensor(0.2905)
features.4.conv.6 tensor(0.5052)
features.5.conv.0 tensor(0.4935)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6310)
features.6.conv.0 tensor(0.0402)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0743)
features.7.conv.0 tensor(0.2008)
features.7.conv.3 tensor(0.4523)
features.7.conv.6 tensor(0.6456)
features.8.conv.0 tensor(0.6873)
features.8.conv.3 tensor(0.5408)
features.8.conv.6 tensor(0.7460)
features.9.conv.0 tensor(0.6579)
features.9.conv.3 tensor(0.5512)
features.9.conv.6 tensor(0.7646)
features.10.conv.0 tensor(0.0518)
features.10.conv.3 tensor(0.0929)
features.10.conv.6 tensor(0.2586)
features.11.conv.0 tensor(0.8043)
features.11.conv.3 tensor(0.6399)
features.11.conv.6 tensor(0.8779)
features.12.conv.0 tensor(0.8123)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.9030)
features.13.conv.0 tensor(0.4365)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6813)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8304)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9368)
conv.0 tensor(0.4524)
tensor(1601699.) 2188896.0
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][   20/  196]   Loss 0.069511   Top1 97.695312   Top5 100.000000   BatchTime 0.334784   LR 0.000023
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][   40/  196]   Loss 0.071911   Top1 97.402344   Top5 100.000000   BatchTime 0.287212   LR 0.000022
tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][   60/  196]   Loss 0.076028   Top1 97.291667   Top5 100.000000   BatchTime 0.266602   LR 0.000021
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][   80/  196]   Loss 0.076021   Top1 97.358398   Top5 100.000000   BatchTime 0.256661   LR 0.000019
tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][  100/  196]   Loss 0.073800   Top1 97.453125   Top5 100.000000   BatchTime 0.258404   LR 0.000018
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][  120/  196]   Loss 0.073435   Top1 97.454427   Top5 100.000000   BatchTime 0.256668   LR 0.000017
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][  140/  196]   Loss 0.073643   Top1 97.433036   Top5 99.997210   BatchTime 0.255670   LR 0.000016
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][  160/  196]   Loss 0.073183   Top1 97.446289   Top5 99.992676   BatchTime 0.256625   LR 0.000015
tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [26][  180/  196]   Loss 0.074495   Top1 97.417535   Top5 99.993490   BatchTime 0.254027   LR 0.000014
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.410    Top5: 99.990    Loss: 0.075
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.370496   Top1 90.117188   Top5 99.648438   BatchTime 0.147101
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4219)
features.1.conv.0 tensor(0.0599)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.1314)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6557)
features.3.conv.0 tensor(0.0799)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.2539)
features.4.conv.0 tensor(0.0916)
features.4.conv.3 tensor(0.2899)
features.4.conv.6 tensor(0.5050)
features.5.conv.0 tensor(0.4937)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6312)
features.6.conv.0 tensor(0.0415)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0746)
features.7.conv.0 tensor(0.2008)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6458)
features.8.conv.0 tensor(0.6873)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7461)
features.9.conv.0 tensor(0.6578)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7646)
features.10.conv.0 tensor(0.0523)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.2589)
features.11.conv.0 tensor(0.8043)
features.11.conv.3 tensor(0.6397)
features.11.conv.6 tensor(0.8779)
features.12.conv.0 tensor(0.8123)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.9030)
features.13.conv.0 tensor(0.4367)
features.13.conv.3 tensor(0.4797)
features.13.conv.6 tensor(0.6815)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8303)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9368)
conv.0 tensor(0.4525)
tensor(1601792.) 2188896.0
INFO - Validation [26][   40/   40]   Loss 0.368330   Top1 89.900000   Top5 99.760000   BatchTime 0.101656
INFO - ==> Top1: 89.900    Top5: 99.760    Loss: 0.368
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 89.940   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 89.900   Top5: 99.760]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][   20/  196]   Loss 0.063690   Top1 97.929688   Top5 100.000000   BatchTime 0.320121   LR 0.000013
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][   40/  196]   Loss 0.067421   Top1 97.822266   Top5 99.980469   BatchTime 0.272424   LR 0.000012
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][   60/  196]   Loss 0.069615   Top1 97.662760   Top5 99.980469   BatchTime 0.256726   LR 0.000011
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][   80/  196]   Loss 0.069387   Top1 97.631836   Top5 99.985352   BatchTime 0.253142   LR 0.000010
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][  100/  196]   Loss 0.070478   Top1 97.597656   Top5 99.984375   BatchTime 0.247081   LR 0.000009
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][  120/  196]   Loss 0.070746   Top1 97.574870   Top5 99.986979   BatchTime 0.244227   LR 0.000009
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][  140/  196]   Loss 0.071374   Top1 97.527902   Top5 99.988839   BatchTime 0.241706   LR 0.000008
tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][  160/  196]   Loss 0.071272   Top1 97.553711   Top5 99.987793   BatchTime 0.239897   LR 0.000007
tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [27][  180/  196]   Loss 0.071345   Top1 97.547743   Top5 99.984809   BatchTime 0.238796   LR 0.000007
tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 97.514    Top5: 99.984    Loss: 0.072
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [27][   20/   40]   Loss 0.367935   Top1 90.351562   Top5 99.648438   BatchTime 0.148633
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.4219)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0842)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.6557)
features.3.conv.0 tensor(0.0804)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.2537)
features.4.conv.0 tensor(0.0916)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5052)
features.5.conv.0 tensor(0.4938)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.6312)
features.6.conv.0 tensor(0.0405)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0739)
features.7.conv.0 tensor(0.2011)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6456)
features.8.conv.0 tensor(0.6873)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7463)
features.9.conv.0 tensor(0.6578)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7646)
features.10.conv.0 tensor(0.0516)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.2590)
features.11.conv.0 tensor(0.8042)
features.11.conv.3 tensor(0.6406)
features.11.conv.6 tensor(0.8778)
features.12.conv.0 tensor(0.8123)
features.12.conv.3 tensor(0.6696)
features.12.conv.6 tensor(0.9030)
features.13.conv.0 tensor(0.4367)
features.13.conv.3 tensor(0.4799)
features.13.conv.6 tensor(0.6815)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8303)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9368)
conv.0 tensor(0.4525)
tensor(1601793.) 2188896.0
INFO - Validation [27][   40/   40]   Loss 0.366921   Top1 90.010000   Top5 99.750000   BatchTime 0.102737
INFO - ==> Top1: 90.010    Top5: 99.750    Loss: 0.367
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.940   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][   20/  196]   Loss 0.073798   Top1 97.382812   Top5 99.980469   BatchTime 0.342642   LR 0.000006
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][   40/  196]   Loss 0.069947   Top1 97.509766   Top5 99.980469   BatchTime 0.294288   LR 0.000005
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][   60/  196]   Loss 0.069960   Top1 97.545573   Top5 99.980469   BatchTime 0.278824   LR 0.000004
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][   80/  196]   Loss 0.069725   Top1 97.529297   Top5 99.985352   BatchTime 0.268218   LR 0.000004
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][  100/  196]   Loss 0.069659   Top1 97.589844   Top5 99.988281   BatchTime 0.266551   LR 0.000003
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][  120/  196]   Loss 0.069999   Top1 97.584635   Top5 99.990234   BatchTime 0.262930   LR 0.000003
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][  140/  196]   Loss 0.071440   Top1 97.547433   Top5 99.988839   BatchTime 0.261458   LR 0.000003
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][  160/  196]   Loss 0.072049   Top1 97.519531   Top5 99.990234   BatchTime 0.257272   LR 0.000002
tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [28][  180/  196]   Loss 0.071532   Top1 97.539062   Top5 99.991319   BatchTime 0.253343   LR 0.000002
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 97.546    Top5: 99.992    Loss: 0.072
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [28][   20/   40]   Loss 0.375390   Top1 90.019531   Top5 99.667969   BatchTime 0.149290
INFO - Validation [28][   40/   40]   Loss 0.370741   Top1 89.830000   Top5 99.750000   BatchTime 0.101717
INFO - ==> Top1: 89.830    Top5: 99.750    Loss: 0.371
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.940   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4219)
features.1.conv.0 tensor(0.0592)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6557)
features.3.conv.0 tensor(0.0802)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.2537)
features.4.conv.0 tensor(0.0913)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5054)
features.5.conv.0 tensor(0.4937)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.6312)
features.6.conv.0 tensor(0.0405)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0738)
features.7.conv.0 tensor(0.2012)
features.7.conv.3 tensor(0.4520)
features.7.conv.6 tensor(0.6455)
features.8.conv.0 tensor(0.6873)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.7462)
features.9.conv.0 tensor(0.6578)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7645)
features.10.conv.0 tensor(0.0514)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.2590)
features.11.conv.0 tensor(0.8042)
features.11.conv.3 tensor(0.6400)
features.11.conv.6 tensor(0.8778)
features.12.conv.0 tensor(0.8122)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.9030)
features.13.conv.0 tensor(0.4367)
features.13.conv.3 tensor(0.4797)
features.13.conv.6 tensor(0.6815)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8301)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9368)
conv.0 tensor(0.4525)
tensor(1601795.) 2188896.0
tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][   20/  196]   Loss 0.070568   Top1 97.636719   Top5 100.000000   BatchTime 0.328136   LR 0.000001
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][   40/  196]   Loss 0.070770   Top1 97.597656   Top5 99.960938   BatchTime 0.278798   LR 0.000001
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][   60/  196]   Loss 0.070717   Top1 97.552083   Top5 99.967448   BatchTime 0.260870   LR 0.000001
tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][   80/  196]   Loss 0.070121   Top1 97.519531   Top5 99.970703   BatchTime 0.253652   LR 0.000001
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][  100/  196]   Loss 0.068401   Top1 97.578125   Top5 99.976562   BatchTime 0.248891   LR 0.000000
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][  120/  196]   Loss 0.067910   Top1 97.597656   Top5 99.977214   BatchTime 0.246454   LR 0.000000
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][  140/  196]   Loss 0.067259   Top1 97.619978   Top5 99.980469   BatchTime 0.243681   LR 0.000000
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][  160/  196]   Loss 0.068600   Top1 97.563477   Top5 99.982910   BatchTime 0.241032   LR 0.000000
tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [29][  180/  196]   Loss 0.069032   Top1 97.541233   Top5 99.982639   BatchTime 0.238524   LR 0.000000
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.580    Top5: 99.984    Loss: 0.069
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 0.372575   Top1 90.253906   Top5 99.687500   BatchTime 0.153956
INFO - Validation [29][   40/   40]   Loss 0.366807   Top1 89.950000   Top5 99.770000   BatchTime 0.107306
INFO - ==> Top1: 89.950    Top5: 99.770    Loss: 0.367
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.4219)
features.1.conv.0 tensor(0.0599)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6557)
features.3.conv.0 tensor(0.0802)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.2537)
features.4.conv.0 tensor(0.0913)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5052)
features.5.conv.0 tensor(0.4938)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.6312)
features.6.conv.0 tensor(0.0410)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0738)
features.7.conv.0 tensor(0.2012)
features.7.conv.3 tensor(0.4517)
features.7.conv.6 tensor(0.6456)
features.8.conv.0 tensor(0.6873)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.7463)
features.9.conv.0 tensor(0.6580)
features.9.conv.3 tensor(0.5512)
features.9.conv.6 tensor(0.7646)
features.10.conv.0 tensor(0.0514)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.2591)
features.11.conv.0 tensor(0.8042)
features.11.conv.3 tensor(0.6400)
features.11.conv.6 tensor(0.8778)
features.12.conv.0 tensor(0.8122)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.9030)
features.13.conv.0 tensor(0.4368)
features.13.conv.3 tensor(0.4797)
features.13.conv.6 tensor(0.6815)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8301)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7412)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9368)
conv.0 tensor(0.4524)
tensor(1601773.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][   20/  196]   Loss 0.083056   Top1 97.246094   Top5 100.000000   BatchTime 0.360583   LR 0.000125
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][   40/  196]   Loss 0.085109   Top1 97.099609   Top5 99.990234   BatchTime 0.304584   LR 0.000125
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][   60/  196]   Loss 0.089148   Top1 96.894531   Top5 99.986979   BatchTime 0.284756   LR 0.000125
tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][   80/  196]   Loss 0.098320   Top1 96.567383   Top5 99.985352   BatchTime 0.277238   LR 0.000125
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][  100/  196]   Loss 0.101192   Top1 96.425781   Top5 99.984375   BatchTime 0.271161   LR 0.000125
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][  120/  196]   Loss 0.103265   Top1 96.341146   Top5 99.986979   BatchTime 0.266249   LR 0.000125
tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][  140/  196]   Loss 0.105306   Top1 96.255580   Top5 99.986049   BatchTime 0.260279   LR 0.000125
tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][  160/  196]   Loss 0.106031   Top1 96.196289   Top5 99.982910   BatchTime 0.255686   LR 0.000125
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [30][  180/  196]   Loss 0.106020   Top1 96.210938   Top5 99.982639   BatchTime 0.252062   LR 0.000125
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.186    Top5: 99.984    Loss: 0.106
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.407783   Top1 88.730469   Top5 99.609375   BatchTime 0.148659
INFO - Validation [30][   40/   40]   Loss 0.397219   Top1 88.920000   Top5 99.680000   BatchTime 0.100899
INFO - ==> Top1: 88.920    Top5: 99.680    Loss: 0.397
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.4297)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.1299)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6560)
features.3.conv.0 tensor(0.0781)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.2556)
features.4.conv.0 tensor(0.0913)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5067)
features.5.conv.0 tensor(0.4948)
features.5.conv.3 tensor(0.4109)
features.5.conv.6 tensor(0.6325)
features.6.conv.0 tensor(0.0412)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0760)
features.7.conv.0 tensor(0.2023)
features.7.conv.3 tensor(0.4505)
features.7.conv.6 tensor(0.6472)
features.8.conv.0 tensor(0.6876)
features.8.conv.3 tensor(0.5405)
features.8.conv.6 tensor(0.7468)
features.9.conv.0 tensor(0.6588)
features.9.conv.3 tensor(0.5535)
features.9.conv.6 tensor(0.7650)
features.10.conv.0 tensor(0.0526)
features.10.conv.3 tensor(0.0932)
features.10.conv.6 tensor(0.2621)
features.11.conv.0 tensor(0.8046)
features.11.conv.3 tensor(0.6397)
features.11.conv.6 tensor(0.8782)
features.12.conv.0 tensor(0.8127)
features.12.conv.3 tensor(0.6690)
features.12.conv.6 tensor(0.9036)
features.13.conv.0 tensor(0.4382)
features.13.conv.3 tensor(0.4817)
features.13.conv.6 tensor(0.6822)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8306)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9184)
features.15.conv.3 tensor(0.8377)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7413)
features.16.conv.3 tensor(0.8028)
features.16.conv.6 tensor(0.9369)
conv.0 tensor(0.4543)
tensor(1603151.) 2188896.0
tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][   20/  196]   Loss 0.106480   Top1 96.171875   Top5 100.000000   BatchTime 0.345415   LR 0.000125
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][   40/  196]   Loss 0.104090   Top1 96.308594   Top5 100.000000   BatchTime 0.284361   LR 0.000125
tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][   60/  196]   Loss 0.105595   Top1 96.315104   Top5 99.980469   BatchTime 0.265531   LR 0.000125
tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][   80/  196]   Loss 0.107930   Top1 96.186523   Top5 99.975586   BatchTime 0.253470   LR 0.000125
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][  100/  196]   Loss 0.108794   Top1 96.140625   Top5 99.976562   BatchTime 0.245446   LR 0.000125
tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][  120/  196]   Loss 0.110252   Top1 96.083984   Top5 99.980469   BatchTime 0.242390   LR 0.000125
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][  140/  196]   Loss 0.111754   Top1 96.068638   Top5 99.977679   BatchTime 0.240914   LR 0.000124
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][  160/  196]   Loss 0.112659   Top1 96.030273   Top5 99.978027   BatchTime 0.241162   LR 0.000124
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [31][  180/  196]   Loss 0.110671   Top1 96.121962   Top5 99.976128   BatchTime 0.238145   LR 0.000124
tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.142    Top5: 99.976    Loss: 0.110
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [31][   20/   40]   Loss 0.412878   Top1 88.457031   Top5 99.492188   BatchTime 0.145632
INFO - Validation [31][   40/   40]   Loss 0.399196   Top1 88.740000   Top5 99.650000   BatchTime 0.098030
INFO - ==> Top1: 88.740    Top5: 99.650    Loss: 0.399
INFO - ==> Sparsity : 0.733
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3125)
features.0.conv.3 tensor(0.4258)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1317)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6571)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.2574)
features.4.conv.0 tensor(0.0928)
features.4.conv.3 tensor(0.2905)
features.4.conv.6 tensor(0.5081)
features.5.conv.0 tensor(0.4958)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.6340)
features.6.conv.0 tensor(0.0410)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0775)
features.7.conv.0 tensor(0.2010)
features.7.conv.3 tensor(0.4508)
features.7.conv.6 tensor(0.6472)
features.8.conv.0 tensor(0.6883)
features.8.conv.3 tensor(0.5391)
features.8.conv.6 tensor(0.7474)
features.9.conv.0 tensor(0.6593)
features.9.conv.3 tensor(0.5521)
features.9.conv.6 tensor(0.7657)
features.10.conv.0 tensor(0.0515)
features.10.conv.3 tensor(0.0900)
features.10.conv.6 tensor(0.2639)
features.11.conv.0 tensor(0.8049)
features.11.conv.3 tensor(0.6391)
features.11.conv.6 tensor(0.8786)
features.12.conv.0 tensor(0.8129)
features.12.conv.3 tensor(0.6690)
features.12.conv.6 tensor(0.9037)
features.13.conv.0 tensor(0.4388)
features.13.conv.3 tensor(0.4823)
features.13.conv.6 tensor(0.6831)
features.14.conv.0 tensor(0.9332)
features.14.conv.3 tensor(0.8309)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9186)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9751)
features.16.conv.0 tensor(0.7415)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9371)
conv.0 tensor(0.4560)
tensor(1604244.) 2188896.0
tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][   20/  196]   Loss 0.100148   Top1 96.464844   Top5 99.960938   BatchTime 0.320205   LR 0.000124
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][   40/  196]   Loss 0.103560   Top1 96.289062   Top5 99.980469   BatchTime 0.272676   LR 0.000124
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][   60/  196]   Loss 0.105157   Top1 96.197917   Top5 99.986979   BatchTime 0.257600   LR 0.000124
tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][   80/  196]   Loss 0.104401   Top1 96.220703   Top5 99.990234   BatchTime 0.257187   LR 0.000124
tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][  100/  196]   Loss 0.107146   Top1 96.187500   Top5 99.988281   BatchTime 0.257352   LR 0.000124
tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][  120/  196]   Loss 0.107001   Top1 96.188151   Top5 99.983724   BatchTime 0.254540   LR 0.000124
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][  140/  196]   Loss 0.106478   Top1 96.236049   Top5 99.983259   BatchTime 0.251677   LR 0.000124
tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][  160/  196]   Loss 0.107057   Top1 96.210938   Top5 99.982910   BatchTime 0.250165   LR 0.000123
tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [32][  180/  196]   Loss 0.107381   Top1 96.195747   Top5 99.984809   BatchTime 0.250373   LR 0.000123
tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.162    Top5: 99.986    Loss: 0.108
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.396470   Top1 89.218750   Top5 99.531250   BatchTime 0.154072
INFO - Validation [32][   40/   40]   Loss 0.389845   Top1 89.030000   Top5 99.670000   BatchTime 0.103587
INFO - ==> Top1: 89.030    Top5: 99.670    Loss: 0.390
INFO - ==> Sparsity : 0.733
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.4277)
features.1.conv.0 tensor(0.0618)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0846)
features.2.conv.0 tensor(0.1285)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6577)
features.3.conv.0 tensor(0.0781)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.2567)
features.4.conv.0 tensor(0.0905)
features.4.conv.3 tensor(0.2905)
features.4.conv.6 tensor(0.5096)
features.5.conv.0 tensor(0.4963)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.6343)
features.6.conv.0 tensor(0.0394)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0763)
features.7.conv.0 tensor(0.2015)
features.7.conv.3 tensor(0.4523)
features.7.conv.6 tensor(0.6482)
features.8.conv.0 tensor(0.6886)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7480)
features.9.conv.0 tensor(0.6594)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7665)
features.10.conv.0 tensor(0.0527)
features.10.conv.3 tensor(0.0897)
features.10.conv.6 tensor(0.2661)
features.11.conv.0 tensor(0.8051)
features.11.conv.3 tensor(0.6406)
features.11.conv.6 tensor(0.8790)
features.12.conv.0 tensor(0.8130)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.9040)
features.13.conv.0 tensor(0.4392)
features.13.conv.3 tensor(0.4815)
features.13.conv.6 tensor(0.6839)
features.14.conv.0 tensor(0.9333)
features.14.conv.3 tensor(0.8307)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9186)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.7416)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9372)
conv.0 tensor(0.4577)
tensor(1605350.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][   20/  196]   Loss 0.105607   Top1 96.093750   Top5 99.960938   BatchTime 0.328729   LR 0.000123
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][   40/  196]   Loss 0.106354   Top1 96.035156   Top5 99.960938   BatchTime 0.289818   LR 0.000123
tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][   60/  196]   Loss 0.103863   Top1 96.165365   Top5 99.973958   BatchTime 0.275336   LR 0.000123
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][   80/  196]   Loss 0.105946   Top1 96.176758   Top5 99.970703   BatchTime 0.272743   LR 0.000123
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][  100/  196]   Loss 0.106463   Top1 96.175781   Top5 99.972656   BatchTime 0.264953   LR 0.000123
tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][  120/  196]   Loss 0.107523   Top1 96.165365   Top5 99.973958   BatchTime 0.256994   LR 0.000123
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][  140/  196]   Loss 0.106949   Top1 96.180246   Top5 99.977679   BatchTime 0.250869   LR 0.000122
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][  160/  196]   Loss 0.107934   Top1 96.171875   Top5 99.975586   BatchTime 0.247509   LR 0.000122
tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [33][  180/  196]   Loss 0.107770   Top1 96.143663   Top5 99.978299   BatchTime 0.244658   LR 0.000122
tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.166    Top5: 99.980    Loss: 0.107
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 0.404821   Top1 88.964844   Top5 99.628906   BatchTime 0.149968
INFO - Validation [33][   40/   40]   Loss 0.402983   Top1 88.860000   Top5 99.700000   BatchTime 0.103182
INFO - ==> Top1: 88.860    Top5: 99.700    Loss: 0.403
INFO - ==> Sparsity : 0.734
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3229)
features.0.conv.3 tensor(0.4336)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1317)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.6577)
features.3.conv.0 tensor(0.0744)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2580)
features.4.conv.0 tensor(0.0913)
features.4.conv.3 tensor(0.2928)
features.4.conv.6 tensor(0.5106)
features.5.conv.0 tensor(0.4979)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6348)
features.6.conv.0 tensor(0.0426)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0780)
features.7.conv.0 tensor(0.2007)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.6484)
features.8.conv.0 tensor(0.6892)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7485)
features.9.conv.0 tensor(0.6604)
features.9.conv.3 tensor(0.5527)
features.9.conv.6 tensor(0.7670)
features.10.conv.0 tensor(0.0529)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.2684)
features.11.conv.0 tensor(0.8055)
features.11.conv.3 tensor(0.6393)
features.11.conv.6 tensor(0.8794)
features.12.conv.0 tensor(0.8134)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.9042)
features.13.conv.0 tensor(0.4398)
features.13.conv.3 tensor(0.4805)
features.13.conv.6 tensor(0.6845)
features.14.conv.0 tensor(0.9333)
features.14.conv.3 tensor(0.8315)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9188)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7420)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.9373)
conv.0 tensor(0.4592)
tensor(1606499.) 2188896.0
tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][   20/  196]   Loss 0.099942   Top1 96.464844   Top5 100.000000   BatchTime 0.340826   LR 0.000122
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][   40/  196]   Loss 0.099377   Top1 96.503906   Top5 100.000000   BatchTime 0.283487   LR 0.000122
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][   60/  196]   Loss 0.101629   Top1 96.380208   Top5 100.000000   BatchTime 0.263588   LR 0.000121
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][   80/  196]   Loss 0.101187   Top1 96.381836   Top5 99.995117   BatchTime 0.256344   LR 0.000121
tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][  100/  196]   Loss 0.100846   Top1 96.417969   Top5 99.988281   BatchTime 0.257725   LR 0.000121
tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][  120/  196]   Loss 0.099450   Top1 96.425781   Top5 99.986979   BatchTime 0.256268   LR 0.000121
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][  140/  196]   Loss 0.100420   Top1 96.403460   Top5 99.986049   BatchTime 0.255181   LR 0.000121
tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][  160/  196]   Loss 0.101625   Top1 96.340332   Top5 99.987793   BatchTime 0.253541   LR 0.000121
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [34][  180/  196]   Loss 0.101824   Top1 96.375868   Top5 99.984809   BatchTime 0.251431   LR 0.000120
tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.388    Top5: 99.986    Loss: 0.102
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [34][   20/   40]   Loss 0.422978   Top1 88.320312   Top5 99.453125   BatchTime 0.147083
INFO - Validation [34][   40/   40]   Loss 0.414774   Top1 88.530000   Top5 99.590000   BatchTime 0.099446
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.4336)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0829)
features.2.conv.0 tensor(0.1334)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.6589)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.2593)
features.4.conv.0 tensor(0.0911)
features.4.conv.3 tensor(0.2934)
features.4.conv.6 tensor(0.5107)
features.5.conv.0 tensor(0.4980)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6354)
features.6.conv.0 tensor(0.0415)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.2013)
features.7.conv.3 tensor(0.4505)
features.7.conv.6 tensor(0.6495)
features.8.conv.0 tensor(0.6899)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7486)
features.9.conv.0 tensor(0.6605)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7672)
features.10.conv.0 tensor(0.0497)
features.10.conv.3 tensor(0.0917)
features.10.conv.6 tensor(0.2711)
features.11.conv.0 tensor(0.8056)
features.11.conv.3 tensor(0.6395)
features.11.conv.6 tensor(0.8796)
features.12.conv.0 tensor(0.8137)
features.12.conv.3 tensor(0.6676)
features.12.conv.6 tensor(0.9044)
features.13.conv.0 tensor(0.4403)
features.13.conv.3 tensor(0.4815)
features.13.conv.6 tensor(0.6850)
features.14.conv.0 tensor(0.9333)
features.14.conv.3 tensor(0.8314)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9189)
features.15.conv.3 tensor(0.8376)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7420)
features.16.conv.3 tensor(0.8028)
features.16.conv.6 tensor(0.9375)
conv.0 tensor(0.4605)
tensor(1607314.) 2188896.0
INFO - ==> Top1: 88.530    Top5: 99.590    Loss: 0.415
INFO - ==> Sparsity : 0.734
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][   20/  196]   Loss 0.098179   Top1 96.269531   Top5 100.000000   BatchTime 0.316947   LR 0.000120
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][   40/  196]   Loss 0.097189   Top1 96.298828   Top5 99.980469   BatchTime 0.269560   LR 0.000120
tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][   60/  196]   Loss 0.103946   Top1 96.276042   Top5 99.980469   BatchTime 0.250864   LR 0.000120
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][   80/  196]   Loss 0.103748   Top1 96.303711   Top5 99.985352   BatchTime 0.241498   LR 0.000119
tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][  100/  196]   Loss 0.106235   Top1 96.285156   Top5 99.980469   BatchTime 0.237408   LR 0.000119
tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][  120/  196]   Loss 0.105312   Top1 96.321615   Top5 99.970703   BatchTime 0.234008   LR 0.000119
tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][  140/  196]   Loss 0.105247   Top1 96.303013   Top5 99.974888   BatchTime 0.238819   LR 0.000119
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][  160/  196]   Loss 0.105117   Top1 96.296387   Top5 99.978027   BatchTime 0.239637   LR 0.000119
tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [35][  180/  196]   Loss 0.104609   Top1 96.312934   Top5 99.980469   BatchTime 0.240892   LR 0.000118
tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.290    Top5: 99.978    Loss: 0.104
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [35][   20/   40]   Loss 0.407545   Top1 88.828125   Top5 99.570312   BatchTime 0.158173
INFO - Validation [35][   40/   40]   Loss 0.402585   Top1 89.020000   Top5 99.660000   BatchTime 0.108202
INFO - ==> Top1: 89.020    Top5: 99.660    Loss: 0.403
INFO - ==> Sparsity : 0.735
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.4316)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.1305)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.6594)
features.3.conv.0 tensor(0.0758)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2609)
features.4.conv.0 tensor(0.0892)
features.4.conv.3 tensor(0.2946)
features.4.conv.6 tensor(0.5119)
features.5.conv.0 tensor(0.4987)
features.5.conv.3 tensor(0.4161)
features.5.conv.6 tensor(0.6361)
features.6.conv.0 tensor(0.0415)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0789)
features.7.conv.0 tensor(0.2017)
features.7.conv.3 tensor(0.4502)
features.7.conv.6 tensor(0.6497)
features.8.conv.0 tensor(0.6903)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.7493)
features.9.conv.0 tensor(0.6607)
features.9.conv.3 tensor(0.5532)
features.9.conv.6 tensor(0.7679)
features.10.conv.0 tensor(0.0510)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.2724)
features.11.conv.0 tensor(0.8059)
features.11.conv.3 tensor(0.6414)
features.11.conv.6 tensor(0.8798)
features.12.conv.0 tensor(0.8140)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.9046)
features.13.conv.0 tensor(0.4412)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6855)
features.14.conv.0 tensor(0.9335)
features.14.conv.3 tensor(0.8313)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9190)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7423)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9376)
conv.0 tensor(0.4620)
tensor(1608354.) 2188896.0
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][   20/  196]   Loss 0.086592   Top1 96.894531   Top5 100.000000   BatchTime 0.332240   LR 0.000118
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][   40/  196]   Loss 0.092679   Top1 96.660156   Top5 100.000000   BatchTime 0.290510   LR 0.000118
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][   60/  196]   Loss 0.095782   Top1 96.542969   Top5 99.993490   BatchTime 0.273064   LR 0.000117
tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][   80/  196]   Loss 0.095220   Top1 96.533203   Top5 99.990234   BatchTime 0.266969   LR 0.000117
tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][  100/  196]   Loss 0.095440   Top1 96.507812   Top5 99.992188   BatchTime 0.261656   LR 0.000117
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][  120/  196]   Loss 0.095791   Top1 96.494141   Top5 99.990234   BatchTime 0.259402   LR 0.000117
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][  140/  196]   Loss 0.096126   Top1 96.540179   Top5 99.986049   BatchTime 0.260757   LR 0.000117
tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][  160/  196]   Loss 0.097690   Top1 96.469727   Top5 99.982910   BatchTime 0.260134   LR 0.000116
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [36][  180/  196]   Loss 0.098768   Top1 96.440972   Top5 99.984809   BatchTime 0.259425   LR 0.000116
tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.410    Top5: 99.986    Loss: 0.100
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 0.401853   Top1 88.984375   Top5 99.550781   BatchTime 0.153305
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4375)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.1322)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6597)
features.3.conv.0 tensor(0.0747)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.2602)
features.4.conv.0 tensor(0.0894)
features.4.conv.3 tensor(0.2940)
features.4.conv.6 tensor(0.5133)
features.5.conv.0 tensor(0.4993)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.6367)
features.6.conv.0 tensor(0.0399)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0790)
features.7.conv.0 tensor(0.2006)
features.7.conv.3 tensor(0.4523)
features.7.conv.6 tensor(0.6503)
features.8.conv.0 tensor(0.6904)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7502)
features.9.conv.0 tensor(0.6610)
features.9.conv.3 tensor(0.5518)
features.9.conv.6 tensor(0.7683)
features.10.conv.0 tensor(0.0482)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.2741)
features.11.conv.0 tensor(0.8061)
features.11.conv.3 tensor(0.6397)
features.11.conv.6 tensor(0.8799)
features.12.conv.0 tensor(0.8144)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9047)
features.13.conv.0 tensor(0.4423)
features.13.conv.3 tensor(0.4805)
features.13.conv.6 tensor(0.6862)
features.14.conv.0 tensor(0.9335)
features.14.conv.3 tensor(0.8315)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9190)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7425)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.9378)
conv.0 tensor(0.4632)
tensor(1609133.) 2188896.0
INFO - Validation [36][   40/   40]   Loss 0.394677   Top1 89.020000   Top5 99.670000   BatchTime 0.106936
INFO - ==> Top1: 89.020    Top5: 99.670    Loss: 0.395
INFO - ==> Sparsity : 0.735
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][   20/  196]   Loss 0.089699   Top1 96.757812   Top5 100.000000   BatchTime 0.332232   LR 0.000116
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][   40/  196]   Loss 0.089002   Top1 96.738281   Top5 100.000000   BatchTime 0.285730   LR 0.000115
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][   60/  196]   Loss 0.088954   Top1 96.783854   Top5 99.986979   BatchTime 0.270590   LR 0.000115
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][   80/  196]   Loss 0.089557   Top1 96.796875   Top5 99.990234   BatchTime 0.257338   LR 0.000115
tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][  100/  196]   Loss 0.090030   Top1 96.800781   Top5 99.988281   BatchTime 0.249096   LR 0.000114
tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][  120/  196]   Loss 0.091326   Top1 96.728516   Top5 99.990234   BatchTime 0.246193   LR 0.000114
tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][  140/  196]   Loss 0.091943   Top1 96.688058   Top5 99.988839   BatchTime 0.245722   LR 0.000114
tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][  160/  196]   Loss 0.092759   Top1 96.652832   Top5 99.982910   BatchTime 0.245154   LR 0.000114
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [37][  180/  196]   Loss 0.093859   Top1 96.608073   Top5 99.984809   BatchTime 0.243899   LR 0.000113
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.590    Top5: 99.984    Loss: 0.094
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.404363   Top1 89.121094   Top5 99.667969   BatchTime 0.155030
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4355)
features.1.conv.0 tensor(0.0586)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.1354)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.6603)
features.3.conv.0 tensor(0.0799)
features.3.conv.3 tensor(0.0718)
features.3.conv.6 tensor(0.2613)
features.4.conv.0 tensor(0.0895)
features.4.conv.3 tensor(0.2899)
features.4.conv.6 tensor(0.5135)
features.5.conv.0 tensor(0.4998)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6377)
features.6.conv.0 tensor(0.0396)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.2022)
features.7.conv.3 tensor(0.4517)
features.7.conv.6 tensor(0.6512)
features.8.conv.0 tensor(0.6908)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7502)
features.9.conv.0 tensor(0.6617)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7690)
features.10.conv.0 tensor(0.0490)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.2761)
features.11.conv.0 tensor(0.8062)
features.11.conv.3 tensor(0.6389)
features.11.conv.6 tensor(0.8801)
features.12.conv.0 tensor(0.8145)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.9050)
features.13.conv.0 tensor(0.4428)
features.13.conv.3 tensor(0.4790)
features.13.conv.6 tensor(0.6867)
features.14.conv.0 tensor(0.9336)
features.14.conv.3 tensor(0.8315)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9190)
features.15.conv.3 tensor(0.8375)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7424)
features.16.conv.3 tensor(0.8030)
features.16.conv.6 tensor(0.9379)
conv.0 tensor(0.4642)
tensor(1609871.) 2188896.0
INFO - Validation [37][   40/   40]   Loss 0.398395   Top1 89.050000   Top5 99.730000   BatchTime 0.105479
INFO - ==> Top1: 89.050    Top5: 99.730    Loss: 0.398
INFO - ==> Sparsity : 0.735
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][   20/  196]   Loss 0.081383   Top1 97.109375   Top5 100.000000   BatchTime 0.352530   LR 0.000113
tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][   40/  196]   Loss 0.095379   Top1 96.601562   Top5 100.000000   BatchTime 0.296595   LR 0.000112
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][   60/  196]   Loss 0.090465   Top1 96.783854   Top5 99.986979   BatchTime 0.273278   LR 0.000112
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][   80/  196]   Loss 0.090075   Top1 96.845703   Top5 99.985352   BatchTime 0.260327   LR 0.000112
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][  100/  196]   Loss 0.088552   Top1 96.878906   Top5 99.984375   BatchTime 0.250401   LR 0.000112
tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][  120/  196]   Loss 0.090230   Top1 96.826172   Top5 99.977214   BatchTime 0.246364   LR 0.000111
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][  140/  196]   Loss 0.090139   Top1 96.810826   Top5 99.980469   BatchTime 0.248147   LR 0.000111
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][  160/  196]   Loss 0.090963   Top1 96.728516   Top5 99.980469   BatchTime 0.246564   LR 0.000111
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [38][  180/  196]   Loss 0.090291   Top1 96.733941   Top5 99.982639   BatchTime 0.245008   LR 0.000110
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 96.718    Top5: 99.984    Loss: 0.090
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [38][   20/   40]   Loss 0.413595   Top1 88.847656   Top5 99.531250   BatchTime 0.161391
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.4375)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.1305)
features.2.conv.3 tensor(0.3410)
features.2.conv.6 tensor(0.6606)
features.3.conv.0 tensor(0.0764)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.2622)
features.4.conv.0 tensor(0.0884)
features.4.conv.3 tensor(0.2940)
features.4.conv.6 tensor(0.5133)
features.5.conv.0 tensor(0.5002)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.6380)
features.6.conv.0 tensor(0.0373)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0756)
features.7.conv.0 tensor(0.2014)
features.7.conv.3 tensor(0.4494)
features.7.conv.6 tensor(0.6517)
features.8.conv.0 tensor(0.6909)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7506)
features.9.conv.0 tensor(0.6622)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7695)
features.10.conv.0 tensor(0.0495)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2778)
features.11.conv.0 tensor(0.8066)
features.11.conv.3 tensor(0.6379)
features.11.conv.6 tensor(0.8804)
features.12.conv.0 tensor(0.8148)
features.12.conv.3 tensor(0.6694)
features.12.conv.6 tensor(0.9053)
features.13.conv.0 tensor(0.4434)
features.13.conv.3 tensor(0.4778)
features.13.conv.6 tensor(0.6871)
INFO - Validation [38][   40/   40]   Loss 0.400105   Top1 89.010000   Top5 99.620000   BatchTime 0.109630
INFO - ==> Top1: 89.010    Top5: 99.620    Loss: 0.400
INFO - ==> Sparsity : 0.736
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
features.14.conv.0 tensor(0.9336)
features.14.conv.3 tensor(0.8315)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9191)
features.15.conv.3 tensor(0.8374)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7424)
features.16.conv.3 tensor(0.8030)
features.16.conv.6 tensor(0.9380)
conv.0 tensor(0.4654)
tensor(1610607.) 2188896.0
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][   20/  196]   Loss 0.085924   Top1 96.757812   Top5 99.980469   BatchTime 0.376737   LR 0.000110
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][   40/  196]   Loss 0.086732   Top1 96.933594   Top5 99.980469   BatchTime 0.304741   LR 0.000109
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][   60/  196]   Loss 0.090190   Top1 96.751302   Top5 99.973958   BatchTime 0.285626   LR 0.000109
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][   80/  196]   Loss 0.090966   Top1 96.704102   Top5 99.970703   BatchTime 0.275269   LR 0.000109
tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][  100/  196]   Loss 0.091730   Top1 96.679688   Top5 99.964844   BatchTime 0.268012   LR 0.000108
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][  120/  196]   Loss 0.091302   Top1 96.738281   Top5 99.964193   BatchTime 0.262695   LR 0.000108
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][  140/  196]   Loss 0.090889   Top1 96.752232   Top5 99.963728   BatchTime 0.259792   LR 0.000108
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][  160/  196]   Loss 0.090732   Top1 96.777344   Top5 99.968262   BatchTime 0.256455   LR 0.000107
tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [39][  180/  196]   Loss 0.090606   Top1 96.753472   Top5 99.969618   BatchTime 0.254034   LR 0.000107
tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.724    Top5: 99.972    Loss: 0.091
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [39][   20/   40]   Loss 0.409268   Top1 89.257812   Top5 99.628906   BatchTime 0.155906
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4355)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1319)
features.2.conv.3 tensor(0.3403)
features.2.conv.6 tensor(0.6617)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.2624)
features.4.conv.0 tensor(0.0895)
features.4.conv.3 tensor(0.2911)
features.4.conv.6 tensor(0.5146)
features.5.conv.0 tensor(0.5002)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.6382)
features.6.conv.0 tensor(0.0365)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0784)
features.7.conv.0 tensor(0.2007)
features.7.conv.3 tensor(0.4479)
features.7.conv.6 tensor(0.6517)
features.8.conv.0 tensor(0.6912)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.7509)
features.9.conv.0 tensor(0.6627)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7701)
features.10.conv.0 tensor(0.0490)
features.10.conv.3 tensor(0.0917)
features.10.conv.6 tensor(0.2795)
features.11.conv.0 tensor(0.8068)
features.11.conv.3 tensor(0.6395)
features.11.conv.6 tensor(0.8806)
features.12.conv.0 tensor(0.8147)
features.12.conv.3 tensor(0.6688)
features.12.conv.6 tensor(0.9056)
features.13.conv.0 tensor(0.4441)
features.13.conv.3 tensor(0.4778)
features.13.conv.6 tensor(0.6877)
features.14.conv.0 tensor(0.9336)
features.14.conv.3 tensor(0.8316)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9191)
features.15.conv.3 tensor(0.8373)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7426)
features.16.conv.3 tensor(0.8030)
features.16.conv.6 tensor(0.9380)
conv.0 tensor(0.4666)
tensor(1611384.) 2188896.0
INFO - Validation [39][   40/   40]   Loss 0.404377   Top1 89.300000   Top5 99.680000   BatchTime 0.105947
INFO - ==> Top1: 89.300    Top5: 99.680    Loss: 0.404
INFO - ==> Sparsity : 0.736
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][   20/  196]   Loss 0.077911   Top1 97.265625   Top5 100.000000   BatchTime 0.337485   LR 0.000106
tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][   40/  196]   Loss 0.080786   Top1 97.109375   Top5 99.990234   BatchTime 0.287821   LR 0.000106
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][   60/  196]   Loss 0.081399   Top1 97.070312   Top5 99.993490   BatchTime 0.272355   LR 0.000106
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][   80/  196]   Loss 0.084331   Top1 96.997070   Top5 99.995117   BatchTime 0.268615   LR 0.000105
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][  100/  196]   Loss 0.083031   Top1 97.062500   Top5 99.996094   BatchTime 0.264807   LR 0.000105
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][  120/  196]   Loss 0.084164   Top1 97.021484   Top5 99.996745   BatchTime 0.260107   LR 0.000105
tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][  140/  196]   Loss 0.084836   Top1 96.969866   Top5 99.997210   BatchTime 0.256368   LR 0.000104
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][  160/  196]   Loss 0.085146   Top1 96.987305   Top5 99.997559   BatchTime 0.253440   LR 0.000104
tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [40][  180/  196]   Loss 0.085306   Top1 96.974826   Top5 99.995660   BatchTime 0.251121   LR 0.000103
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.918    Top5: 99.994    Loss: 0.087
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [40][   20/   40]   Loss 0.408006   Top1 89.433594   Top5 99.511719   BatchTime 0.155531
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4375)
features.1.conv.0 tensor(0.0495)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1319)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.6623)
features.3.conv.0 tensor(0.0744)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.2626)
features.4.conv.0 tensor(0.0907)
features.4.conv.3 tensor(0.2917)
features.4.conv.6 tensor(0.5161)
features.5.conv.0 tensor(0.5008)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.6390)
features.6.conv.0 tensor(0.0371)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0750)
features.7.conv.0 tensor(0.2008)
features.7.conv.3 tensor(0.4523)
features.7.conv.6 tensor(0.6523)
features.8.conv.0 tensor(0.6921)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7513)
features.9.conv.0 tensor(0.6632)
features.9.conv.3 tensor(0.5532)
features.9.conv.6 tensor(0.7704)
features.10.conv.0 tensor(0.0487)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2809)
features.11.conv.0 tensor(0.8071)
features.11.conv.3 tensor(0.6393)
features.11.conv.6 tensor(0.8808)
features.12.conv.0 tensor(0.8149)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.9058)
features.13.conv.0 tensor(0.4446)
features.13.conv.3 tensor(0.4786)
features.13.conv.6 tensor(0.6879)
features.14.conv.0 tensor(0.9336)
features.14.conv.3 tensor(0.8316)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9192)
features.15.conv.3 tensor(0.8377)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7426)
features.16.conv.3 tensor(0.8035)
features.16.conv.6 tensor(0.9381)
conv.0 tensor(0.4675)
tensor(1612061.) 2188896.0
INFO - Validation [40][   40/   40]   Loss 0.403846   Top1 89.370000   Top5 99.570000   BatchTime 0.107132
INFO - ==> Top1: 89.370    Top5: 99.570    Loss: 0.404
INFO - ==> Sparsity : 0.736
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][   20/  196]   Loss 0.084691   Top1 96.914062   Top5 99.980469   BatchTime 0.338944   LR 0.000103
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][   40/  196]   Loss 0.081468   Top1 97.001953   Top5 99.990234   BatchTime 0.288810   LR 0.000102
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][   60/  196]   Loss 0.083872   Top1 96.953125   Top5 99.993490   BatchTime 0.272881   LR 0.000102
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][   80/  196]   Loss 0.082565   Top1 97.006836   Top5 99.990234   BatchTime 0.265086   LR 0.000102
tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][  100/  196]   Loss 0.083698   Top1 96.968750   Top5 99.976562   BatchTime 0.259749   LR 0.000101
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][  120/  196]   Loss 0.085010   Top1 96.891276   Top5 99.977214   BatchTime 0.262763   LR 0.000101
tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][  140/  196]   Loss 0.086663   Top1 96.810826   Top5 99.980469   BatchTime 0.262590   LR 0.000100
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][  160/  196]   Loss 0.086941   Top1 96.796875   Top5 99.982910   BatchTime 0.260043   LR 0.000100
tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [41][  180/  196]   Loss 0.086831   Top1 96.807726   Top5 99.980469   BatchTime 0.257174   LR 0.000100
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 96.824    Top5: 99.982    Loss: 0.087
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [41][   20/   40]   Loss 0.429085   Top1 88.886719   Top5 99.589844   BatchTime 0.154570
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.4395)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.1317)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.6626)
features.3.conv.0 tensor(0.0781)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2628)
features.4.conv.0 tensor(0.0907)
features.4.conv.3 tensor(0.2957)
features.4.conv.6 tensor(0.5160)
features.5.conv.0 tensor(0.5011)
features.5.conv.3 tensor(0.4178)
features.5.conv.6 tensor(0.6388)
features.6.conv.0 tensor(0.0436)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0750)
features.7.conv.0 tensor(0.2015)
features.7.conv.3 tensor(0.4502)
features.7.conv.6 tensor(0.6532)
features.8.conv.0 tensor(0.6922)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7517)
features.9.conv.0 tensor(0.6635)
features.9.conv.3 tensor(0.5521)
features.9.conv.6 tensor(0.7708)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.2820)
features.11.conv.0 tensor(0.8075)
features.11.conv.3 tensor(0.6404)
features.11.conv.6 tensor(0.8812)
features.12.conv.0 tensor(0.8149)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.9059)
features.13.conv.0 tensor(0.4453)
features.13.conv.3 tensor(0.4797)
features.13.conv.6 tensor(0.6885)
features.14.conv.0 tensor(0.9336)
features.14.conv.3 tensor(0.8316)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9191)
features.15.conv.3 tensor(0.8376)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7426)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.9382)
conv.0 tensor(0.4685)
tensor(1612749.) 2188896.0
INFO - Validation [41][   40/   40]   Loss 0.411852   Top1 88.920000   Top5 99.640000   BatchTime 0.106409
INFO - ==> Top1: 88.920    Top5: 99.640    Loss: 0.412
INFO - ==> Sparsity : 0.737
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][   20/  196]   Loss 0.069624   Top1 97.656250   Top5 99.980469   BatchTime 0.332132   LR 0.000099
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][   40/  196]   Loss 0.075987   Top1 97.392578   Top5 99.990234   BatchTime 0.293099   LR 0.000098
tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][   60/  196]   Loss 0.080176   Top1 97.180990   Top5 99.986979   BatchTime 0.275293   LR 0.000098
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][   80/  196]   Loss 0.076999   Top1 97.343750   Top5 99.990234   BatchTime 0.266706   LR 0.000098
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][  100/  196]   Loss 0.076199   Top1 97.355469   Top5 99.992188   BatchTime 0.263942   LR 0.000097
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][  120/  196]   Loss 0.078784   Top1 97.265625   Top5 99.993490   BatchTime 0.258450   LR 0.000097
tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][  140/  196]   Loss 0.079080   Top1 97.287946   Top5 99.988839   BatchTime 0.255689   LR 0.000096
tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][  160/  196]   Loss 0.078545   Top1 97.309570   Top5 99.987793   BatchTime 0.258874   LR 0.000096
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [42][  180/  196]   Loss 0.079278   Top1 97.246094   Top5 99.989149   BatchTime 0.257995   LR 0.000096
tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.238    Top5: 99.990    Loss: 0.080
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [42][   20/   40]   Loss 0.437504   Top1 88.769531   Top5 99.648438   BatchTime 0.155215
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4375)
features.1.conv.0 tensor(0.0430)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0807)
features.2.conv.0 tensor(0.1317)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.6629)
features.3.conv.0 tensor(0.0790)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.2645)
features.4.conv.0 tensor(0.0887)
features.4.conv.3 tensor(0.2917)
features.4.conv.6 tensor(0.5164)
features.5.conv.0 tensor(0.5015)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.6390)
features.6.conv.0 tensor(0.0384)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0757)
features.7.conv.0 tensor(0.2016)
features.7.conv.3 tensor(0.4482)
features.7.conv.6 tensor(0.6534)
features.8.conv.0 tensor(0.6923)
features.8.conv.3 tensor(0.5408)
features.8.conv.6 tensor(0.7524)
features.9.conv.0 tensor(0.6635)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7710)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.2835)
features.11.conv.0 tensor(0.8075)
features.11.conv.3 tensor(0.6375)
features.11.conv.6 tensor(0.8814)
features.12.conv.0 tensor(0.8151)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9060)
features.13.conv.0 tensor(0.4458)
features.13.conv.3 tensor(0.4792)
features.13.conv.6 tensor(0.6888)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8315)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9192)
features.15.conv.3 tensor(0.8375)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7427)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9383)
conv.0 tensor(0.4692)
tensor(1613217.) 2188896.0
INFO - Validation [42][   40/   40]   Loss 0.425117   Top1 88.830000   Top5 99.730000   BatchTime 0.108170
INFO - ==> Top1: 88.830    Top5: 99.730    Loss: 0.425
INFO - ==> Sparsity : 0.737
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][   20/  196]   Loss 0.079308   Top1 97.187500   Top5 99.960938   BatchTime 0.349305   LR 0.000095
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][   40/  196]   Loss 0.074990   Top1 97.402344   Top5 99.980469   BatchTime 0.297832   LR 0.000094
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][   60/  196]   Loss 0.074242   Top1 97.402344   Top5 99.986979   BatchTime 0.281151   LR 0.000094
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][   80/  196]   Loss 0.074529   Top1 97.456055   Top5 99.990234   BatchTime 0.273888   LR 0.000093
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][  100/  196]   Loss 0.076894   Top1 97.328125   Top5 99.992188   BatchTime 0.269528   LR 0.000093
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][  120/  196]   Loss 0.076684   Top1 97.333984   Top5 99.990234   BatchTime 0.267487   LR 0.000093
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][  140/  196]   Loss 0.077046   Top1 97.304688   Top5 99.988839   BatchTime 0.265289   LR 0.000092
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][  160/  196]   Loss 0.076549   Top1 97.326660   Top5 99.987793   BatchTime 0.263403   LR 0.000092
tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [43][  180/  196]   Loss 0.076875   Top1 97.289497   Top5 99.989149   BatchTime 0.265586   LR 0.000091
tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.246    Top5: 99.990    Loss: 0.078
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [43][   20/   40]   Loss 0.407988   Top1 89.257812   Top5 99.648438   BatchTime 0.153642
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0449)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1334)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6632)
features.3.conv.0 tensor(0.0770)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.2658)
features.4.conv.0 tensor(0.0905)
features.4.conv.3 tensor(0.2928)
features.4.conv.6 tensor(0.5166)
features.5.conv.0 tensor(0.5008)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.6392)
features.6.conv.0 tensor(0.0355)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0763)
features.7.conv.0 tensor(0.2015)
features.7.conv.3 tensor(0.4494)
features.7.conv.6 tensor(0.6538)
features.8.conv.0 tensor(0.6924)
features.8.conv.3 tensor(0.5382)
features.8.conv.6 tensor(0.7524)
features.9.conv.0 tensor(0.6635)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7715)
features.10.conv.0 tensor(0.0487)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.2843)
features.11.conv.0 tensor(0.8075)
features.11.conv.3 tensor(0.6381)
features.11.conv.6 tensor(0.8816)
features.12.conv.0 tensor(0.8153)
features.12.conv.3 tensor(0.6688)
features.12.conv.6 tensor(0.9061)
features.13.conv.0 tensor(0.4458)
features.13.conv.3 tensor(0.4796)
features.13.conv.6 tensor(0.6893)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8316)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9192)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7428)
features.16.conv.3 tensor(0.8030)
features.16.conv.6 tensor(0.9383)
conv.0 tensor(0.4700)
tensor(1613782.) 2188896.0
INFO - Validation [43][   40/   40]   Loss 0.403473   Top1 89.120000   Top5 99.710000   BatchTime 0.105051
INFO - ==> Top1: 89.120    Top5: 99.710    Loss: 0.403
INFO - ==> Sparsity : 0.737
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][   20/  196]   Loss 0.072300   Top1 97.285156   Top5 100.000000   BatchTime 0.334113   LR 0.000090
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][   40/  196]   Loss 0.072567   Top1 97.255859   Top5 100.000000   BatchTime 0.288795   LR 0.000090
tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][   60/  196]   Loss 0.071808   Top1 97.278646   Top5 100.000000   BatchTime 0.275871   LR 0.000090
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][   80/  196]   Loss 0.072063   Top1 97.255859   Top5 99.995117   BatchTime 0.269256   LR 0.000089
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][  100/  196]   Loss 0.075096   Top1 97.214844   Top5 99.996094   BatchTime 0.264382   LR 0.000089
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][  120/  196]   Loss 0.076548   Top1 97.184245   Top5 99.996745   BatchTime 0.261485   LR 0.000088
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][  140/  196]   Loss 0.077056   Top1 97.198661   Top5 99.997210   BatchTime 0.259604   LR 0.000088
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][  160/  196]   Loss 0.077813   Top1 97.165527   Top5 99.997559   BatchTime 0.257699   LR 0.000087
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [44][  180/  196]   Loss 0.077308   Top1 97.194010   Top5 99.997830   BatchTime 0.254454   LR 0.000087
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.158    Top5: 99.996    Loss: 0.078
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [44][   20/   40]   Loss 0.412460   Top1 89.121094   Top5 99.648438   BatchTime 0.147580
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0423)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.1291)
features.2.conv.3 tensor(0.3410)
features.2.conv.6 tensor(0.6629)
features.3.conv.0 tensor(0.0732)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.2663)
features.4.conv.0 tensor(0.0905)
features.4.conv.3 tensor(0.2928)
features.4.conv.6 tensor(0.5163)
features.5.conv.0 tensor(0.5016)
features.5.conv.3 tensor(0.4109)
features.5.conv.6 tensor(0.6395)
features.6.conv.0 tensor(0.0353)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0772)
features.7.conv.0 tensor(0.2016)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.6544)
features.8.conv.0 tensor(0.6927)
features.8.conv.3 tensor(0.5385)
features.8.conv.6 tensor(0.7528)
features.9.conv.0 tensor(0.6642)
features.9.conv.3 tensor(0.5527)
features.9.conv.6 tensor(0.7716)
features.10.conv.0 tensor(0.0480)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.2853)
features.11.conv.0 tensor(0.8074)
features.11.conv.3 tensor(0.6370)
features.11.conv.6 tensor(0.8817)
features.12.conv.0 tensor(0.8153)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9062)
features.13.conv.0 tensor(0.4461)
features.13.conv.3 tensor(0.4813)
features.13.conv.6 tensor(0.6895)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8317)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9193)
features.15.conv.3 tensor(0.8381)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7429)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9384)
conv.0 tensor(0.4706)
tensor(1614123.) 2188896.0
INFO - Validation [44][   40/   40]   Loss 0.411202   Top1 89.060000   Top5 99.690000   BatchTime 0.104052
INFO - ==> Top1: 89.060    Top5: 99.690    Loss: 0.411
INFO - ==> Sparsity : 0.737
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][   20/  196]   Loss 0.070619   Top1 97.519531   Top5 99.960938   BatchTime 0.335135   LR 0.000086
tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][   40/  196]   Loss 0.071974   Top1 97.490234   Top5 99.970703   BatchTime 0.290325   LR 0.000086
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][   60/  196]   Loss 0.073389   Top1 97.454427   Top5 99.980469   BatchTime 0.273934   LR 0.000085
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][   80/  196]   Loss 0.073267   Top1 97.421875   Top5 99.980469   BatchTime 0.263835   LR 0.000085
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][  100/  196]   Loss 0.074520   Top1 97.371094   Top5 99.984375   BatchTime 0.258343   LR 0.000084
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][  120/  196]   Loss 0.073651   Top1 97.373047   Top5 99.986979   BatchTime 0.255936   LR 0.000084
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][  140/  196]   Loss 0.072773   Top1 97.393973   Top5 99.988839   BatchTime 0.254454   LR 0.000083
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][  160/  196]   Loss 0.072774   Top1 97.416992   Top5 99.987793   BatchTime 0.253504   LR 0.000083
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [45][  180/  196]   Loss 0.072838   Top1 97.421875   Top5 99.986979   BatchTime 0.252419   LR 0.000082
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.408    Top5: 99.988    Loss: 0.073
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.414483   Top1 89.628906   Top5 99.589844   BatchTime 0.150665
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.4414)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1319)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6635)
features.3.conv.0 tensor(0.0767)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.2667)
features.4.conv.0 tensor(0.0907)
features.4.conv.3 tensor(0.2917)
features.4.conv.6 tensor(0.5169)
features.5.conv.0 tensor(0.5018)
features.5.conv.3 tensor(0.4074)
features.5.conv.6 tensor(0.6395)
features.6.conv.0 tensor(0.0353)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0743)
features.7.conv.0 tensor(0.2009)
features.7.conv.3 tensor(0.4505)
features.7.conv.6 tensor(0.6548)
features.8.conv.0 tensor(0.6925)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.7530)
features.9.conv.0 tensor(0.6643)
features.9.conv.3 tensor(0.5518)
features.9.conv.6 tensor(0.7721)
features.10.conv.0 tensor(0.0484)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.2859)
features.11.conv.0 tensor(0.8077)
features.11.conv.3 tensor(0.6381)
features.11.conv.6 tensor(0.8819)
features.12.conv.0 tensor(0.8154)
features.12.conv.3 tensor(0.6676)
features.12.conv.6 tensor(0.9063)
features.13.conv.0 tensor(0.4464)
features.13.conv.3 tensor(0.4799)
features.13.conv.6 tensor(0.6898)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8322)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9193)
features.15.conv.3 tensor(0.8372)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7429)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9384)
conv.0 tensor(0.4712)
tensor(1614467.) 2188896.0
INFO - Validation [45][   40/   40]   Loss 0.407130   Top1 89.400000   Top5 99.680000   BatchTime 0.104606
INFO - ==> Top1: 89.400    Top5: 99.680    Loss: 0.407
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][   20/  196]   Loss 0.066088   Top1 97.460938   Top5 99.980469   BatchTime 0.370909   LR 0.000081
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][   40/  196]   Loss 0.067240   Top1 97.509766   Top5 99.980469   BatchTime 0.317287   LR 0.000081
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][   60/  196]   Loss 0.067263   Top1 97.565104   Top5 99.980469   BatchTime 0.295858   LR 0.000080
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][   80/  196]   Loss 0.067140   Top1 97.583008   Top5 99.985352   BatchTime 0.282285   LR 0.000080
tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][  100/  196]   Loss 0.069366   Top1 97.535156   Top5 99.988281   BatchTime 0.272015   LR 0.000079
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][  120/  196]   Loss 0.069275   Top1 97.587891   Top5 99.986979   BatchTime 0.262778   LR 0.000079
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][  140/  196]   Loss 0.069117   Top1 97.575335   Top5 99.986049   BatchTime 0.257810   LR 0.000078
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][  160/  196]   Loss 0.069049   Top1 97.561035   Top5 99.985352   BatchTime 0.256451   LR 0.000078
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [46][  180/  196]   Loss 0.069292   Top1 97.536892   Top5 99.986979   BatchTime 0.254321   LR 0.000077
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 97.530    Top5: 99.988    Loss: 0.069
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [46][   20/   40]   Loss 0.410328   Top1 89.492188   Top5 99.531250   BatchTime 0.149881
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4395)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0911)
features.2.conv.0 tensor(0.1296)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6638)
features.3.conv.0 tensor(0.0712)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2652)
features.4.conv.0 tensor(0.0894)
features.4.conv.3 tensor(0.2911)
features.4.conv.6 tensor(0.5169)
features.5.conv.0 tensor(0.5029)
features.5.conv.3 tensor(0.4091)
features.5.conv.6 tensor(0.6405)
features.6.conv.0 tensor(0.0368)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0759)
features.7.conv.0 tensor(0.2006)
features.7.conv.3 tensor(0.4502)
features.7.conv.6 tensor(0.6549)
features.8.conv.0 tensor(0.6929)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7537)
features.9.conv.0 tensor(0.6643)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7721)
features.10.conv.0 tensor(0.0481)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.2869)
features.11.conv.0 tensor(0.8078)
features.11.conv.3 tensor(0.6377)
features.11.conv.6 tensor(0.8820)
features.12.conv.0 tensor(0.8154)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.9063)
features.13.conv.0 tensor(0.4468)
features.13.conv.3 tensor(0.4807)
features.13.conv.6 tensor(0.6899)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8321)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9193)
features.15.conv.3 tensor(0.8382)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7429)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9385)
conv.0 tensor(0.4716)
tensor(1614831.) 2188896.0
INFO - Validation [46][   40/   40]   Loss 0.406674   Top1 89.610000   Top5 99.650000   BatchTime 0.104174
INFO - ==> Top1: 89.610    Top5: 99.650    Loss: 0.407
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][   20/  196]   Loss 0.071638   Top1 97.304688   Top5 99.980469   BatchTime 0.379794   LR 0.000077
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][   40/  196]   Loss 0.072798   Top1 97.236328   Top5 99.990234   BatchTime 0.317935   LR 0.000076
tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][   60/  196]   Loss 0.074079   Top1 97.285156   Top5 99.993490   BatchTime 0.300754   LR 0.000076
tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][   80/  196]   Loss 0.074551   Top1 97.294922   Top5 99.990234   BatchTime 0.285074   LR 0.000075
tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][  100/  196]   Loss 0.071490   Top1 97.449219   Top5 99.992188   BatchTime 0.275730   LR 0.000075
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][  120/  196]   Loss 0.069367   Top1 97.539062   Top5 99.990234   BatchTime 0.270367   LR 0.000074
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][  140/  196]   Loss 0.068747   Top1 97.564174   Top5 99.991629   BatchTime 0.266063   LR 0.000074
tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][  160/  196]   Loss 0.067191   Top1 97.607422   Top5 99.992676   BatchTime 0.262330   LR 0.000073
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [47][  180/  196]   Loss 0.067513   Top1 97.573785   Top5 99.993490   BatchTime 0.259405   LR 0.000073
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.590    Top5: 99.994    Loss: 0.067
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [47][   20/   40]   Loss 0.408437   Top1 89.746094   Top5 99.570312   BatchTime 0.154459
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.4414)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0718)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0915)
features.4.conv.3 tensor(0.2922)
features.4.conv.6 tensor(0.5176)
features.5.conv.0 tensor(0.5028)
features.5.conv.3 tensor(0.4091)
features.5.conv.6 tensor(0.6405)
features.6.conv.0 tensor(0.0365)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0765)
features.7.conv.0 tensor(0.2013)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.6548)
features.8.conv.0 tensor(0.6930)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.7538)
features.9.conv.0 tensor(0.6646)
features.9.conv.3 tensor(0.5518)
features.9.conv.6 tensor(0.7721)
features.10.conv.0 tensor(0.0489)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.2876)
features.11.conv.0 tensor(0.8078)
features.11.conv.3 tensor(0.6379)
features.11.conv.6 tensor(0.8822)
features.12.conv.0 tensor(0.8154)
features.12.conv.3 tensor(0.6672)
features.12.conv.6 tensor(0.9064)
features.13.conv.0 tensor(0.4471)
features.13.conv.3 tensor(0.4796)
features.13.conv.6 tensor(0.6902)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8324)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9193)
features.15.conv.3 tensor(0.8387)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7429)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9386)
conv.0 tensor(0.4722)
tensor(1615239.) 2188896.0
INFO - Validation [47][   40/   40]   Loss 0.407589   Top1 89.610000   Top5 99.640000   BatchTime 0.106440
INFO - ==> Top1: 89.610    Top5: 99.640    Loss: 0.408
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][   20/  196]   Loss 0.067874   Top1 97.480469   Top5 100.000000   BatchTime 0.319003   LR 0.000072
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][   40/  196]   Loss 0.063663   Top1 97.705078   Top5 100.000000   BatchTime 0.267661   LR 0.000071
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][   60/  196]   Loss 0.065501   Top1 97.708333   Top5 100.000000   BatchTime 0.252635   LR 0.000071
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][   80/  196]   Loss 0.066690   Top1 97.709961   Top5 100.000000   BatchTime 0.254945   LR 0.000070
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][  100/  196]   Loss 0.065347   Top1 97.773438   Top5 100.000000   BatchTime 0.250494   LR 0.000070
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][  120/  196]   Loss 0.064593   Top1 97.799479   Top5 99.996745   BatchTime 0.248557   LR 0.000069
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][  140/  196]   Loss 0.064128   Top1 97.790179   Top5 99.997210   BatchTime 0.249343   LR 0.000069
tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][  160/  196]   Loss 0.064976   Top1 97.758789   Top5 99.992676   BatchTime 0.250459   LR 0.000068
tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [48][  180/  196]   Loss 0.065619   Top1 97.727865   Top5 99.991319   BatchTime 0.249298   LR 0.000068
tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 97.726    Top5: 99.990    Loss: 0.066
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [48][   20/   40]   Loss 0.419997   Top1 89.589844   Top5 99.648438   BatchTime 0.157566
INFO - Validation [48][   40/   40]   Loss 0.404382   Top1 89.620000   Top5 99.730000   BatchTime 0.105059
INFO - ==> Top1: 89.620    Top5: 99.730    Loss: 0.404
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1314)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.6638)
features.3.conv.0 tensor(0.0718)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2678)
features.4.conv.0 tensor(0.0894)
features.4.conv.3 tensor(0.2911)
features.4.conv.6 tensor(0.5177)
features.5.conv.0 tensor(0.5034)
features.5.conv.3 tensor(0.4115)
features.5.conv.6 tensor(0.6410)
features.6.conv.0 tensor(0.0369)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0784)
features.7.conv.0 tensor(0.2008)
features.7.conv.3 tensor(0.4505)
features.7.conv.6 tensor(0.6549)
features.8.conv.0 tensor(0.6932)
features.8.conv.3 tensor(0.5422)
features.8.conv.6 tensor(0.7537)
features.9.conv.0 tensor(0.6649)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7725)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.2882)
features.11.conv.0 tensor(0.8079)
features.11.conv.3 tensor(0.6383)
features.11.conv.6 tensor(0.8822)
features.12.conv.0 tensor(0.8155)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.9065)
features.13.conv.0 tensor(0.4472)
features.13.conv.3 tensor(0.4803)
features.13.conv.6 tensor(0.6903)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8321)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9193)
features.15.conv.3 tensor(0.8381)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7429)
features.16.conv.3 tensor(0.8036)
features.16.conv.6 tensor(0.9386)
conv.0 tensor(0.4726)
tensor(1615514.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][   20/  196]   Loss 0.059426   Top1 97.851562   Top5 100.000000   BatchTime 0.333827   LR 0.000067
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][   40/  196]   Loss 0.059164   Top1 97.900391   Top5 99.990234   BatchTime 0.284686   LR 0.000066
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][   60/  196]   Loss 0.057155   Top1 97.936198   Top5 99.993490   BatchTime 0.266874   LR 0.000066
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][   80/  196]   Loss 0.059862   Top1 97.939453   Top5 99.995117   BatchTime 0.265720   LR 0.000065
tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][  100/  196]   Loss 0.060334   Top1 97.910156   Top5 99.996094   BatchTime 0.269208   LR 0.000065
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][  120/  196]   Loss 0.059640   Top1 97.926432   Top5 99.996745   BatchTime 0.282095   LR 0.000064
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][  140/  196]   Loss 0.060232   Top1 97.904576   Top5 99.994420   BatchTime 0.289233   LR 0.000064
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][  160/  196]   Loss 0.059837   Top1 97.907715   Top5 99.995117   BatchTime 0.295268   LR 0.000063
tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [49][  180/  196]   Loss 0.059965   Top1 97.890625   Top5 99.995660   BatchTime 0.298115   LR 0.000063
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.872    Top5: 99.996    Loss: 0.060
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4453)
features.1.conv.0 tensor(0.0521)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.1325)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0692)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.2674)
features.4.conv.0 tensor(0.0908)
features.4.conv.3 tensor(0.2928)
features.4.conv.6 tensor(0.5186)
features.5.conv.0 tensor(0.5039)
features.5.conv.3 tensor(0.4109)
features.5.conv.6 tensor(0.6410)
features.6.conv.0 tensor(0.0361)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0770)
features.7.conv.0 tensor(0.2012)
features.7.conv.3 tensor(0.4505)
features.7.conv.6 tensor(0.6553)
features.8.conv.0 tensor(0.6936)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7541)
features.9.conv.0 tensor(0.6650)
features.9.conv.3 tensor(0.5506)
features.9.conv.6 tensor(0.7725)
features.10.conv.0 tensor(0.0488)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.2887)
features.11.conv.0 tensor(0.8077)
features.11.conv.3 tensor(0.6373)
features.11.conv.6 tensor(0.8824)
features.12.conv.0 tensor(0.8157)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.9066)
features.13.conv.0 tensor(0.4476)
features.13.conv.3 tensor(0.4805)
features.13.conv.6 tensor(0.6905)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8381)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7431)
features.16.conv.3 tensor(0.8035)
features.16.conv.6 tensor(0.9387)
conv.0 tensor(0.4729)
tensor(1615824.) 2188896.0
INFO - Validation [49][   20/   40]   Loss 0.428666   Top1 89.335938   Top5 99.589844   BatchTime 0.153115
INFO - Validation [49][   40/   40]   Loss 0.417880   Top1 89.280000   Top5 99.670000   BatchTime 0.103022
INFO - ==> Top1: 89.280    Top5: 99.670    Loss: 0.418
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][   20/  196]   Loss 0.054511   Top1 97.988281   Top5 99.980469   BatchTime 0.428231   LR 0.000062
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][   40/  196]   Loss 0.053223   Top1 98.017578   Top5 99.990234   BatchTime 0.373635   LR 0.000062
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][   60/  196]   Loss 0.053665   Top1 98.001302   Top5 99.993490   BatchTime 0.359732   LR 0.000061
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][   80/  196]   Loss 0.053139   Top1 98.041992   Top5 99.995117   BatchTime 0.355637   LR 0.000061
tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][  100/  196]   Loss 0.052882   Top1 98.105469   Top5 99.996094   BatchTime 0.349634   LR 0.000060
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][  120/  196]   Loss 0.054895   Top1 98.053385   Top5 99.996745   BatchTime 0.340123   LR 0.000060
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][  140/  196]   Loss 0.055283   Top1 98.002232   Top5 99.997210   BatchTime 0.329907   LR 0.000059
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][  160/  196]   Loss 0.055820   Top1 97.956543   Top5 99.997559   BatchTime 0.323652   LR 0.000059
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [50][  180/  196]   Loss 0.055540   Top1 97.960069   Top5 99.997830   BatchTime 0.324911   LR 0.000058
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.934    Top5: 99.996    Loss: 0.056
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [50][   20/   40]   Loss 0.427682   Top1 89.433594   Top5 99.687500   BatchTime 0.147703
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1299)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6644)
features.3.conv.0 tensor(0.0686)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.2674)
features.4.conv.0 tensor(0.0923)
features.4.conv.3 tensor(0.2911)
features.4.conv.6 tensor(0.5184)
features.5.conv.0 tensor(0.5037)
features.5.conv.3 tensor(0.4155)
features.5.conv.6 tensor(0.6408)
features.6.conv.0 tensor(0.0369)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.2006)
features.7.conv.3 tensor(0.4505)
features.7.conv.6 tensor(0.6552)
features.8.conv.0 tensor(0.6935)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7540)
features.9.conv.0 tensor(0.6648)
features.9.conv.3 tensor(0.5506)
features.9.conv.6 tensor(0.7726)
features.10.conv.0 tensor(0.0490)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.2892)
features.11.conv.0 tensor(0.8079)
features.11.conv.3 tensor(0.6375)
features.11.conv.6 tensor(0.8824)
features.12.conv.0 tensor(0.8156)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.9066)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4807)
features.13.conv.6 tensor(0.6907)
features.14.conv.0 tensor(0.9337)
features.14.conv.3 tensor(0.8325)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8385)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7431)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9387)
conv.0 tensor(0.4732)
tensor(1616028.) 2188896.0
INFO - Validation [50][   40/   40]   Loss 0.418280   Top1 89.680000   Top5 99.720000   BatchTime 0.099383
INFO - ==> Top1: 89.680    Top5: 99.720    Loss: 0.418
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][   20/  196]   Loss 0.056023   Top1 98.125000   Top5 99.980469   BatchTime 0.417132   LR 0.000057
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][   40/  196]   Loss 0.055005   Top1 98.125000   Top5 99.990234   BatchTime 0.375899   LR 0.000057
tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][   60/  196]   Loss 0.053525   Top1 98.177083   Top5 99.986979   BatchTime 0.370067   LR 0.000056
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][   80/  196]   Loss 0.054123   Top1 98.149414   Top5 99.985352   BatchTime 0.363110   LR 0.000056
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][  100/  196]   Loss 0.054728   Top1 98.078125   Top5 99.988281   BatchTime 0.356893   LR 0.000055
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][  120/  196]   Loss 0.053819   Top1 98.095703   Top5 99.990234   BatchTime 0.353280   LR 0.000055
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][  140/  196]   Loss 0.054770   Top1 98.055246   Top5 99.991629   BatchTime 0.352917   LR 0.000054
tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][  160/  196]   Loss 0.055319   Top1 98.037109   Top5 99.992676   BatchTime 0.343880   LR 0.000054
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [51][  180/  196]   Loss 0.055548   Top1 98.042535   Top5 99.993490   BatchTime 0.334916   LR 0.000053
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.022    Top5: 99.994    Loss: 0.056
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4414)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1305)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6644)
features.3.conv.0 tensor(0.0686)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.2669)
features.4.conv.0 tensor(0.0911)
features.4.conv.3 tensor(0.2888)
features.4.conv.6 tensor(0.5181)
features.5.conv.0 tensor(0.5036)
features.5.conv.3 tensor(0.4132)
features.5.conv.6 tensor(0.6408)
features.6.conv.0 tensor(0.0373)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0779)
features.7.conv.0 tensor(0.1995)
features.7.conv.3 tensor(0.4491)
features.7.conv.6 tensor(0.6554)
features.8.conv.0 tensor(0.6937)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7542)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5524)
features.9.conv.6 tensor(0.7726)
features.10.conv.0 tensor(0.0481)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.2897)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6387)
features.11.conv.6 tensor(0.8824)
features.12.conv.0 tensor(0.8157)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.9067)
features.13.conv.0 tensor(0.4478)
features.13.conv.3 tensor(0.4813)
features.13.conv.6 tensor(0.6907)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8325)
features.14.conv.6 tensor(0.9722)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8035)
features.16.conv.6 tensor(0.9388)
conv.0 tensor(0.4735)
tensor(1616188.) 2188896.0
INFO - Validation [51][   20/   40]   Loss 0.429403   Top1 89.160156   Top5 99.765625   BatchTime 0.148297
INFO - Validation [51][   40/   40]   Loss 0.412021   Top1 89.610000   Top5 99.830000   BatchTime 0.101690
INFO - ==> Top1: 89.610    Top5: 99.830    Loss: 0.412
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][   20/  196]   Loss 0.055615   Top1 97.968750   Top5 100.000000   BatchTime 0.412302   LR 0.000052
tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][   40/  196]   Loss 0.055882   Top1 97.968750   Top5 99.990234   BatchTime 0.375499   LR 0.000052
tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][   60/  196]   Loss 0.057660   Top1 97.903646   Top5 99.986979   BatchTime 0.362508   LR 0.000051
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][   80/  196]   Loss 0.058568   Top1 97.895508   Top5 99.990234   BatchTime 0.355671   LR 0.000051
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][  100/  196]   Loss 0.058371   Top1 97.937500   Top5 99.992188   BatchTime 0.351248   LR 0.000050
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][  120/  196]   Loss 0.057734   Top1 97.952474   Top5 99.993490   BatchTime 0.348895   LR 0.000050
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][  140/  196]   Loss 0.056835   Top1 97.979911   Top5 99.994420   BatchTime 0.347810   LR 0.000049
tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][  160/  196]   Loss 0.056644   Top1 97.988281   Top5 99.992676   BatchTime 0.346087   LR 0.000049
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [52][  180/  196]   Loss 0.056372   Top1 97.996962   Top5 99.993490   BatchTime 0.345159   LR 0.000048
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 97.990    Top5: 99.994    Loss: 0.057
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4453)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.1314)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0720)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.2669)
features.4.conv.0 tensor(0.0905)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5186)
features.5.conv.0 tensor(0.5039)
features.5.conv.3 tensor(0.4149)
features.5.conv.6 tensor(0.6408)
features.6.conv.0 tensor(0.0381)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0781)
features.7.conv.0 tensor(0.2004)
features.7.conv.3 tensor(0.4494)
features.7.conv.6 tensor(0.6554)
features.8.conv.0 tensor(0.6938)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.7543)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5521)
features.9.conv.6 tensor(0.7729)
features.10.conv.0 tensor(0.0479)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.2898)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6381)
INFO - Validation [52][   20/   40]   Loss 0.427706   Top1 89.570312   Top5 99.726562   BatchTime 0.176832
INFO - Validation [52][   40/   40]   Loss 0.415425   Top1 89.570000   Top5 99.750000   BatchTime 0.116732
features.11.conv.6 tensor(0.8824)
features.12.conv.0 tensor(0.8156)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9067)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4803)
features.13.conv.6 tensor(0.6907)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8318)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8383)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7433)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9388)
conv.0 tensor(0.4738)
tensor(1616381.) 2188896.0
INFO - ==> Top1: 89.570    Top5: 99.750    Loss: 0.415
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.950   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][   20/  196]   Loss 0.049397   Top1 98.359375   Top5 100.000000   BatchTime 0.403872   LR 0.000047
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][   40/  196]   Loss 0.049204   Top1 98.320312   Top5 100.000000   BatchTime 0.372047   LR 0.000047
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][   60/  196]   Loss 0.048925   Top1 98.320312   Top5 100.000000   BatchTime 0.359525   LR 0.000046
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][   80/  196]   Loss 0.048897   Top1 98.305664   Top5 99.990234   BatchTime 0.351222   LR 0.000046
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][  100/  196]   Loss 0.049287   Top1 98.312500   Top5 99.992188   BatchTime 0.347317   LR 0.000046
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][  120/  196]   Loss 0.048735   Top1 98.330078   Top5 99.990234   BatchTime 0.345091   LR 0.000045
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][  140/  196]   Loss 0.048326   Top1 98.359375   Top5 99.988839   BatchTime 0.343371   LR 0.000045
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][  160/  196]   Loss 0.048673   Top1 98.344727   Top5 99.990234   BatchTime 0.343656   LR 0.000044
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [53][  180/  196]   Loss 0.048696   Top1 98.328993   Top5 99.989149   BatchTime 0.344406   LR 0.000044
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 98.292    Top5: 99.990    Loss: 0.049
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0833)
features.2.conv.0 tensor(0.1314)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0700)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.2665)
features.4.conv.0 tensor(0.0928)
features.4.conv.3 tensor(0.2888)
features.4.conv.6 tensor(0.5187)
features.5.conv.0 tensor(0.5039)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6411)
features.6.conv.0 tensor(0.0360)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0779)
features.7.conv.0 tensor(0.2000)
features.7.conv.3 tensor(0.4502)
features.7.conv.6
INFO - Validation [53][   20/   40]   Loss 0.415280   Top1 90.136719   Top5 99.667969   BatchTime 0.149802
features.7.conv.6 tensor(0.6554)
features.8.conv.0 tensor(0.6935)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7543)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5501)
features.9.conv.6 tensor(0.7729)
features.10.conv.0 tensor(0.0481)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.2902)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6379)
features.11.conv.6 tensor(0.8824)
features.12.conv.0 tensor(0.8157)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4481)
features.13.conv.3 tensor(0.4811)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8314)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8029)
features.16.conv.6 tensor(0.9388)
conv.0 tensor(0.4740)
tensor(1616510.) 2188896.0
INFO - ==> Top1: 90.010    Top5: 99.780    Loss: 0.407
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.010   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][   20/  196]   Loss 0.047980   Top1 98.359375   Top5 100.000000   BatchTime 0.420095   LR 0.000043
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][   40/  196]   Loss 0.052002   Top1 98.046875   Top5 100.000000   BatchTime 0.381796   LR 0.000042
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][   60/  196]   Loss 0.049324   Top1 98.216146   Top5 100.000000   BatchTime 0.365911   LR 0.000042
tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][   80/  196]   Loss 0.051696   Top1 98.120117   Top5 100.000000   BatchTime 0.359230   LR 0.000041
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][  100/  196]   Loss 0.051179   Top1 98.128906   Top5 100.000000   BatchTime 0.353318   LR 0.000041
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][  120/  196]   Loss 0.050831   Top1 98.147786   Top5 100.000000   BatchTime 0.351788   LR 0.000040
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][  140/  196]   Loss 0.050985   Top1 98.155692   Top5 100.000000   BatchTime 0.352475   LR 0.000040
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][  160/  196]   Loss 0.049937   Top1 98.193359   Top5 99.997559   BatchTime 0.354213   LR 0.000039
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [54][  180/  196]   Loss 0.049266   Top1 98.231337   Top5 99.997830   BatchTime 0.361787   LR 0.000039
tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.248    Top5: 99.998    Loss: 0.049
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [54][   20/   40]   Loss 0.425511   Top1 90.019531   Top5 99.707031   BatchTime 0.149650
INFO - Validation [54][   40/   40]   Loss 0.423324   Top1 89.820000   Top5 99.780000   BatchTime 0.101339
INFO - ==> Top1: 89.820    Top5: 99.780    Loss: 0.423
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.010   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 89.980   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1291)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0712)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.2676)
features.4.conv.0 tensor(0.0920)
features.4.conv.3 tensor(0.2888)
features.4.conv.6 tensor(0.5190)
features.5.conv.0 tensor(0.5042)
features.5.conv.3 tensor(0.4138)
features.5.conv.6 tensor(0.6411)
features.6.conv.0 tensor(0.0360)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0780)
features.7.conv.0 tensor(0.1999)
features.7.conv.3 tensor(0.4517)
features.7.conv.6 tensor(0.6557)
features.8.conv.0 tensor(0.6936)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.7542)
features.9.conv.0 tensor(0.6653)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7728)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.2904)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6377)
features.11.conv.6 tensor(0.8825)
features.12.conv.0 tensor(0.8158)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4480)
features.13.conv.3 tensor(0.4817)
features.13.conv.6 tensor(0.6909)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8319)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9388)
conv.0 tensor(0.4741)
tensor(1616607.) 2188896.0
tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][   20/  196]   Loss 0.055568   Top1 98.203125   Top5 99.980469   BatchTime 0.495946   LR 0.000038
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][   40/  196]   Loss 0.050972   Top1 98.281250   Top5 99.990234   BatchTime 0.426721   LR 0.000038
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][   60/  196]   Loss 0.050768   Top1 98.326823   Top5 99.993490   BatchTime 0.408750   LR 0.000037
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][   80/  196]   Loss 0.050791   Top1 98.295898   Top5 99.995117   BatchTime 0.410625   LR 0.000037
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][  100/  196]   Loss 0.050961   Top1 98.281250   Top5 99.996094   BatchTime 0.413675   LR 0.000036
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][  120/  196]   Loss 0.049126   Top1 98.339844   Top5 99.996745   BatchTime 0.414024   LR 0.000036
tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][  140/  196]   Loss 0.049621   Top1 98.300781   Top5 99.997210   BatchTime 0.414731   LR 0.000035
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][  160/  196]   Loss 0.048987   Top1 98.310547   Top5 99.997559   BatchTime 0.415228   LR 0.000035
tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [55][  180/  196]   Loss 0.048748   Top1 98.302951   Top5 99.997830   BatchTime 0.415359   LR 0.000034
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.304    Top5: 99.998    Loss: 0.049
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [55][   20/   40]   Loss 0.425614   Top1 90.292969   Top5 99.609375   BatchTime 0.152984
INFO - Validation [55][   40/   40]   Loss 0.415032   Top1 90.160000   Top5 99.690000   BatchTime 0.102285
INFO - ==> Top1: 90.160    Top5: 99.690    Loss: 0.415
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.010   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1282)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0749)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.2678)
features.4.conv.0 tensor(0.0913)
features.4.conv.3 tensor(0.2888)
features.4.conv.6 tensor(0.5187)
features.5.conv.0 tensor(0.5037)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.6414)
features.6.conv.0 tensor(0.0335)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.2000)
features.7.conv.3 tensor(0.4494)
features.7.conv.6 tensor(0.6558)
features.8.conv.0 tensor(0.6935)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.7543)
features.9.conv.0 tensor(0.6652)
features.9.conv.3 tensor(0.5527)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0478)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.2906)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6372)
features.11.conv.6 tensor(0.8825)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4481)
features.13.conv.3 tensor(0.4803)
features.13.conv.6 tensor(0.6909)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8317)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8030)
features.16.conv.6 tensor(0.9388)
conv.0 tensor(0.4742)
tensor(1616628.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][   20/  196]   Loss 0.051279   Top1 98.261719   Top5 100.000000   BatchTime 0.422571   LR 0.000034
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][   40/  196]   Loss 0.046709   Top1 98.466797   Top5 99.990234   BatchTime 0.359678   LR 0.000033
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][   60/  196]   Loss 0.044714   Top1 98.457031   Top5 99.993490   BatchTime 0.322765   LR 0.000033
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][   80/  196]   Loss 0.044938   Top1 98.486328   Top5 99.995117   BatchTime 0.317789   LR 0.000032
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][  100/  196]   Loss 0.045287   Top1 98.457031   Top5 99.992188   BatchTime 0.329795   LR 0.000032
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][  120/  196]   Loss 0.045397   Top1 98.440755   Top5 99.993490   BatchTime 0.342674   LR 0.000031
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][  140/  196]   Loss 0.045577   Top1 98.429129   Top5 99.994420   BatchTime 0.351602   LR 0.000031
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][  160/  196]   Loss 0.045969   Top1 98.417969   Top5 99.995117   BatchTime 0.358535   LR 0.000031
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [56][  180/  196]   Loss 0.045585   Top1 98.439670   Top5 99.995660   BatchTime 0.364108   LR 0.000030
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.416    Top5: 99.996    Loss: 0.046
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [56][   20/   40]   Loss 0.435555   Top1 89.472656   Top5 99.589844   BatchTime 0.167417
INFO - Validation [56][   40/   40]   Loss 0.422345   Top1 89.480000   Top5 99.700000   BatchTime 0.125409
INFO - ==> Top1: 89.480    Top5: 99.700    Loss: 0.422
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.010   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4453)
features.1.conv.0 tensor(0.0566)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1291)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.6644)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.2682)
features.4.conv.0 tensor(0.0908)
features.4.conv.3 tensor(0.2917)
features.4.conv.6 tensor(0.5187)
features.5.conv.0 tensor(0.5041)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.6411)
features.6.conv.0 tensor(0.0350)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.1999)
features.7.conv.3 tensor(0.4497)
features.7.conv.6 tensor(0.6558)
features.8.conv.0 tensor(0.6936)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5518)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.2906)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6373)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4480)
features.13.conv.3 tensor(0.4799)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8325)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9389)
conv.0 tensor(0.4744)
tensor(1616731.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][   20/  196]   Loss 0.041282   Top1 98.554688   Top5 100.000000   BatchTime 0.520384   LR 0.000029
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][   40/  196]   Loss 0.043542   Top1 98.417969   Top5 100.000000   BatchTime 0.468350   LR 0.000029
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][   60/  196]   Loss 0.043661   Top1 98.430990   Top5 99.993490   BatchTime 0.447679   LR 0.000029
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][   80/  196]   Loss 0.041921   Top1 98.505859   Top5 99.990234   BatchTime 0.424050   LR 0.000028
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][  100/  196]   Loss 0.043515   Top1 98.488281   Top5 99.992188   BatchTime 0.412203   LR 0.000028
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][  120/  196]   Loss 0.042987   Top1 98.505859   Top5 99.990234   BatchTime 0.413623   LR 0.000027
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][  140/  196]   Loss 0.043870   Top1 98.454241   Top5 99.988839   BatchTime 0.410045   LR 0.000027
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][  160/  196]   Loss 0.043835   Top1 98.449707   Top5 99.990234   BatchTime 0.403613   LR 0.000027
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [57][  180/  196]   Loss 0.043747   Top1 98.454861   Top5 99.991319   BatchTime 0.405217   LR 0.000026
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.458    Top5: 99.992    Loss: 0.044
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [57][   20/   40]   Loss 0.427649   Top1 89.960938   Top5 99.628906   BatchTime 0.160912
INFO - Validation [57][   40/   40]   Loss 0.418679   Top1 89.940000   Top5 99.720000   BatchTime 0.108649
INFO - ==> Top1: 89.940    Top5: 99.720    Loss: 0.419
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.010   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.010   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.4453)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1305)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0890)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5195)
features.5.conv.0 tensor(0.5039)
features.5.conv.3 tensor(0.4144)
features.5.conv.6 tensor(0.6411)
features.6.conv.0 tensor(0.0366)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0787)
features.7.conv.0 tensor(0.2000)
features.7.conv.3 tensor(0.4499)
features.7.conv.6 tensor(0.6558)
features.8.conv.0 tensor(0.6936)
features.8.conv.3 tensor(0.5394)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6652)
features.9.conv.3 tensor(0.5512)
features.9.conv.6 tensor(0.7730)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.2907)
features.11.conv.0 tensor(0.8080)
features.11.conv.3 tensor(0.6375)
features.11.conv.6 tensor(0.8825)
features.12.conv.0 tensor(0.8158)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4796)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8324)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9193)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9389)
conv.0 tensor(0.4746)
tensor(1616787.) 2188896.0
tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][   20/  196]   Loss 0.040635   Top1 98.574219   Top5 100.000000   BatchTime 0.541642   LR 0.000025
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][   40/  196]   Loss 0.043259   Top1 98.583984   Top5 100.000000   BatchTime 0.487809   LR 0.000025
tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][   60/  196]   Loss 0.043222   Top1 98.613281   Top5 100.000000   BatchTime 0.472807   LR 0.000025
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][   80/  196]   Loss 0.043448   Top1 98.569336   Top5 99.995117   BatchTime 0.459218   LR 0.000024
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][  100/  196]   Loss 0.043157   Top1 98.582031   Top5 99.996094   BatchTime 0.444787   LR 0.000024
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][  120/  196]   Loss 0.043222   Top1 98.551432   Top5 99.996745   BatchTime 0.432165   LR 0.000023
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][  140/  196]   Loss 0.042902   Top1 98.551897   Top5 99.997210   BatchTime 0.429944   LR 0.000023
tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][  160/  196]   Loss 0.042947   Top1 98.547363   Top5 99.997559   BatchTime 0.426927   LR 0.000023
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [58][  180/  196]   Loss 0.043288   Top1 98.550347   Top5 99.995660   BatchTime 0.418233   LR 0.000022
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.526    Top5: 99.996    Loss: 0.044
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [58][   20/   40]   Loss 0.417921   Top1 90.058594   Top5 99.628906   BatchTime 0.156929
INFO - Validation [58][   40/   40]   Loss 0.416352   Top1 90.110000   Top5 99.730000   BatchTime 0.110190
INFO - ==> Top1: 90.110    Top5: 99.730    Loss: 0.416
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [58][Top1: 90.110   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.010   Top5: 99.780]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4453)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.1305)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0726)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.2674)
features.4.conv.0 tensor(0.0898)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5192)
features.5.conv.0 tensor(0.5042)
features.5.conv.3 tensor(0.4109)
features.5.conv.6 tensor(0.6411)
features.6.conv.0 tensor(0.0340)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0780)
features.7.conv.0 tensor(0.1995)
features.7.conv.3 tensor(0.4505)
features.7.conv.6 tensor(0.6558)
features.8.conv.0 tensor(0.6936)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.7543)
features.9.conv.0 tensor(0.6652)
features.9.conv.3 tensor(0.5518)
features.9.conv.6 tensor(0.7732)
features.10.conv.0 tensor(0.0475)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.2908)
features.11.conv.0 tensor(0.8080)
features.11.conv.3 tensor(0.6377)
features.11.conv.6 tensor(0.8825)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.9069)
features.13.conv.0 tensor(0.4480)
features.13.conv.3 tensor(0.4796)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8326)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9195)
features.15.conv.3 tensor(0.8381)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8035)
features.16.conv.6 tensor(0.9389)
conv.0 tensor(0.4746)
tensor(1616823.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][   20/  196]   Loss 0.036807   Top1 98.769531   Top5 100.000000   BatchTime 0.539296   LR 0.000022
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][   40/  196]   Loss 0.040125   Top1 98.681641   Top5 99.990234   BatchTime 0.486962   LR 0.000021
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][   60/  196]   Loss 0.041306   Top1 98.626302   Top5 99.993490   BatchTime 0.468518   LR 0.000021
tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][   80/  196]   Loss 0.041627   Top1 98.549805   Top5 99.995117   BatchTime 0.462230   LR 0.000020
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][  100/  196]   Loss 0.041376   Top1 98.582031   Top5 99.996094   BatchTime 0.455617   LR 0.000020
tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][  120/  196]   Loss 0.041113   Top1 98.570964   Top5 99.996745   BatchTime 0.448987   LR 0.000020
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][  140/  196]   Loss 0.040896   Top1 98.554688   Top5 99.997210   BatchTime 0.437869   LR 0.000019
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][  160/  196]   Loss 0.040761   Top1 98.566895   Top5 99.997559   BatchTime 0.428597   LR 0.000019
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [59][  180/  196]   Loss 0.040366   Top1 98.587240   Top5 99.997830   BatchTime 0.424608   LR 0.000019
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.574    Top5: 99.998    Loss: 0.041
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [59][   20/   40]   Loss 0.422051   Top1 90.175781   Top5 99.726562   BatchTime 0.206613
features.0.conv.0 tensor(0.3056)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.6644)
features.3.conv.0 tensor(0.0715)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0905)
features.4.conv.3 tensor(0.2888)
features.4.conv.6 tensor(0.5199)
features.5.conv.0 tensor(0.5042)
features.5.conv.3 tensor(0.4132)
features.5.conv.6 tensor(0.6410)
features.6.conv.0 tensor(0.0340)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0777)
features.7.conv.0 tensor(0.1999)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6557)
features.8.conv.0 tensor(0.6939)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6649)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0478)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.2909)
features.11.conv.0 tensor(0.8080)
features.11.conv.3 tensor(0.6375)
features.11.conv.6 tensor(0.8825)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.9069)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4797)
features.13.conv.6 tensor(0.6911)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7431)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9389)
conv.0 tensor(0.4746)
tensor(1616829.) 2188896.0
INFO - Validation [59][   40/   40]   Loss 0.419193   Top1 90.070000   Top5 99.770000   BatchTime 0.130005
INFO - ==> Top1: 90.070    Top5: 99.770    Loss: 0.419
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [58][Top1: 90.110   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [59][Top1: 90.070   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][   20/  196]   Loss 0.042273   Top1 98.359375   Top5 100.000000   BatchTime 0.523225   LR 0.000018
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][   40/  196]   Loss 0.039999   Top1 98.583984   Top5 100.000000   BatchTime 0.472535   LR 0.000018
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][   60/  196]   Loss 0.040098   Top1 98.567708   Top5 100.000000   BatchTime 0.452466   LR 0.000017
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][   80/  196]   Loss 0.038647   Top1 98.657227   Top5 100.000000   BatchTime 0.442338   LR 0.000017
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][  100/  196]   Loss 0.036863   Top1 98.730469   Top5 100.000000   BatchTime 0.437576   LR 0.000017
tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][  120/  196]   Loss 0.037857   Top1 98.717448   Top5 100.000000   BatchTime 0.434845   LR 0.000016
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][  140/  196]   Loss 0.037706   Top1 98.730469   Top5 100.000000   BatchTime 0.437429   LR 0.000016
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][  160/  196]   Loss 0.037441   Top1 98.730469   Top5 100.000000   BatchTime 0.431886   LR 0.000016
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [60][  180/  196]   Loss 0.036680   Top1 98.760851   Top5 100.000000   BatchTime 0.429922   LR 0.000015
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.736    Top5: 100.000    Loss: 0.037
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [60][   20/   40]   Loss 0.430274   Top1 90.292969   Top5 99.726562   BatchTime 0.173773
INFO - Validation [60][   40/   40]   Loss 0.419054   Top1 90.170000   Top5 99.770000   BatchTime 0.116180
INFO - ==> Top1: 90.170    Top5: 99.770    Loss: 0.419
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 90.110   Top5: 99.730]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0720)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.2674)
features.4.conv.0 tensor(0.0908)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5194)
features.5.conv.0 tensor(0.5041)
features.5.conv.3 tensor(0.4126)
features.5.conv.6 tensor(0.6411)
features.6.conv.0 tensor(0.0355)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0783)
features.7.conv.0 tensor(0.1998)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6936)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.7545)
features.9.conv.0 tensor(0.6654)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0478)
features.10.conv.3 tensor(0.0932)
features.10.conv.6 tensor(0.2909)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6373)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6672)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4478)
features.13.conv.3 tensor(0.4803)
features.13.conv.6 tensor(0.6911)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8329)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9389)
conv.0 tensor(0.4747)
tensor(1616889.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][   20/  196]   Loss 0.037474   Top1 98.789062   Top5 100.000000   BatchTime 0.531185   LR 0.000015
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][   40/  196]   Loss 0.038696   Top1 98.720703   Top5 100.000000   BatchTime 0.481572   LR 0.000014
tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][   60/  196]   Loss 0.040593   Top1 98.678385   Top5 100.000000   BatchTime 0.470865   LR 0.000014
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][   80/  196]   Loss 0.039698   Top1 98.691406   Top5 100.000000   BatchTime 0.463998   LR 0.000014
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][  100/  196]   Loss 0.040041   Top1 98.671875   Top5 100.000000   BatchTime 0.457293   LR 0.000013
tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][  120/  196]   Loss 0.038983   Top1 98.707682   Top5 100.000000   BatchTime 0.455281   LR 0.000013
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][  140/  196]   Loss 0.039083   Top1 98.688616   Top5 100.000000   BatchTime 0.445794   LR 0.000013
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][  160/  196]   Loss 0.038747   Top1 98.698730   Top5 99.997559   BatchTime 0.443011   LR 0.000012
tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [61][  180/  196]   Loss 0.039412   Top1 98.680556   Top5 99.997830   BatchTime 0.442730   LR 0.000012
tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.642    Top5: 99.998    Loss: 0.040
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [61][   20/   40]   Loss 0.419631   Top1 90.292969   Top5 99.589844   BatchTime 0.162534
INFO - Validation [61][   40/   40]   Loss 0.416352   Top1 90.120000   Top5 99.680000   BatchTime 0.111967
INFO - ==> Top1: 90.120    Top5: 99.680    Loss: 0.416
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.120   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0885)
features.2.conv.0 tensor(0.1311)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0723)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.2676)
features.4.conv.0 tensor(0.0897)
features.4.conv.3 tensor(0.2899)
features.4.conv.6 tensor(0.5192)
features.5.conv.0 tensor(0.5037)
features.5.conv.3 tensor(0.4109)
features.5.conv.6 tensor(0.6413)
features.6.conv.0 tensor(0.0345)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0779)
features.7.conv.0 tensor(0.1999)
features.7.conv.3 tensor(0.4511)
features.7.conv.6 tensor(0.6557)
features.8.conv.0 tensor(0.6938)
features.8.conv.3 tensor(0.5405)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5501)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0478)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.2912)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6373)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8160)
features.12.conv.3 tensor(0.6676)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8325)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8034)
features.16.conv.6 tensor(0.9389)
conv.0 tensor(0.4747)
tensor(1616881.) 2188896.0
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][   20/  196]   Loss 0.041476   Top1 98.613281   Top5 100.000000   BatchTime 0.531324   LR 0.000012
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][   40/  196]   Loss 0.038886   Top1 98.691406   Top5 100.000000   BatchTime 0.487223   LR 0.000011
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][   60/  196]   Loss 0.038859   Top1 98.730469   Top5 100.000000   BatchTime 0.468345   LR 0.000011
tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][   80/  196]   Loss 0.037380   Top1 98.750000   Top5 100.000000   BatchTime 0.461407   LR 0.000011
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][  100/  196]   Loss 0.037461   Top1 98.714844   Top5 100.000000   BatchTime 0.456316   LR 0.000011
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][  120/  196]   Loss 0.036397   Top1 98.746745   Top5 100.000000   BatchTime 0.453725   LR 0.000010
tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][  140/  196]   Loss 0.036146   Top1 98.769531   Top5 100.000000   BatchTime 0.445011   LR 0.000010
tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][  160/  196]   Loss 0.035839   Top1 98.762207   Top5 100.000000   BatchTime 0.443053   LR 0.000010
tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [62][  180/  196]   Loss 0.036240   Top1 98.747830   Top5 100.000000   BatchTime 0.442826   LR 0.000009
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.732    Top5: 100.000    Loss: 0.037
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [62][   20/   40]   Loss 0.424755   Top1 90.214844   Top5 99.648438   BatchTime 0.159163
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1308)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0741)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2669)
features.4.conv.0 tensor(0.0898)
features.4.conv.3 tensor(0.2905)
features.4.conv.6 tensor(0.5195)
features.5.conv.0 tensor(0.5042)
features.5.conv.3 tensor(0.4109)
features.5.conv.6 tensor(0.6413)
features.6.conv.0 tensor(0.0356)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.2000)
features.7.conv.3 tensor(0.4508)
features.7.conv.6 tensor(0.6557)
features.8.conv.0 tensor(0.6939)
features.8.conv.3 tensor(0.5417)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6653)
features.9.conv.3 tensor(0.5503)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.2910)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6377)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8160)
features.12.conv.3 tensor(0.6678)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4478)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6911)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8325)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7431)
features.16.conv.3 tensor(0.8031)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4748)
tensor(1616937.) 2188896.0
INFO - Validation [62][   40/   40]   Loss 0.425023   Top1 90.040000   Top5 99.710000   BatchTime 0.115395
INFO - ==> Top1: 90.040    Top5: 99.710    Loss: 0.425
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.120   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][   20/  196]   Loss 0.033540   Top1 98.750000   Top5 100.000000   BatchTime 0.475229   LR 0.000009
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][   40/  196]   Loss 0.036513   Top1 98.671875   Top5 100.000000   BatchTime 0.455492   LR 0.000009
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][   60/  196]   Loss 0.034593   Top1 98.828125   Top5 100.000000   BatchTime 0.446519   LR 0.000008
tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][   80/  196]   Loss 0.035527   Top1 98.784180   Top5 99.995117   BatchTime 0.448384   LR 0.000008
tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][  100/  196]   Loss 0.036291   Top1 98.773438   Top5 99.996094   BatchTime 0.447019   LR 0.000008
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][  120/  196]   Loss 0.035990   Top1 98.789062   Top5 99.996745   BatchTime 0.444017   LR 0.000008
tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][  140/  196]   Loss 0.035794   Top1 98.783482   Top5 99.997210   BatchTime 0.438094   LR 0.000007
tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][  160/  196]   Loss 0.036178   Top1 98.779297   Top5 99.997559   BatchTime 0.432894   LR 0.000007
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [63][  180/  196]   Loss 0.035956   Top1 98.780382   Top5 99.997830   BatchTime 0.434285   LR 0.000007
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.792    Top5: 99.998    Loss: 0.036
tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [63][   20/   40]   Loss 0.424678   Top1 90.273438   Top5 99.707031   BatchTime 0.153426
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0566)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.1299)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0744)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2669)
features.4.conv.0 tensor(0.0911)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5199)
features.5.conv.0 tensor(0.5039)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6414)
features.6.conv.0 tensor(0.0350)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0778)
features.7.conv.0 tensor(0.2000)
features.7.conv.3 tensor(0.4511)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6937)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6653)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.2911)
features.11.conv.0 tensor(0.8080)
features.11.conv.3 tensor(0.6379)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6684)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6911)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4748)
tensor(1616960.) 2188896.0
INFO - Validation [63][   40/   40]   Loss 0.422959   Top1 90.100000   Top5 99.740000   BatchTime 0.104783
INFO - ==> Top1: 90.100    Top5: 99.740    Loss: 0.423
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.120   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][   20/  196]   Loss 0.027952   Top1 98.964844   Top5 100.000000   BatchTime 0.471426   LR 0.000007
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][   40/  196]   Loss 0.031690   Top1 98.857422   Top5 100.000000   BatchTime 0.418021   LR 0.000006
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][   60/  196]   Loss 0.030893   Top1 98.958333   Top5 100.000000   BatchTime 0.412575   LR 0.000006
tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][   80/  196]   Loss 0.033501   Top1 98.876953   Top5 100.000000   BatchTime 0.421001   LR 0.000006
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][  100/  196]   Loss 0.033605   Top1 98.851562   Top5 100.000000   BatchTime 0.424743   LR 0.000006
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][  120/  196]   Loss 0.033538   Top1 98.860677   Top5 100.000000   BatchTime 0.425183   LR 0.000006
tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][  140/  196]   Loss 0.033113   Top1 98.861607   Top5 100.000000   BatchTime 0.427243   LR 0.000005
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][  160/  196]   Loss 0.033684   Top1 98.845215   Top5 100.000000   BatchTime 0.419649   LR 0.000005
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [64][  180/  196]   Loss 0.033906   Top1 98.830295   Top5 100.000000   BatchTime 0.421162   LR 0.000005
tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.830    Top5: 100.000    Loss: 0.034
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [64][   20/   40]   Loss 0.422507   Top1 90.292969   Top5 99.687500   BatchTime 0.159766
INFO - Validation [64][   40/   40]   Loss 0.415789   Top1 89.930000   Top5 99.740000   BatchTime 0.106524
INFO - ==> Top1: 89.930    Top5: 99.740    Loss: 0.416
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.120   Top5: 99.680]
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0566)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0868)
features.2.conv.0 tensor(0.1291)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0723)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0905)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5197)
features.5.conv.0 tensor(0.5039)
features.5.conv.3 tensor(0.4115)
features.5.conv.6 tensor(0.6414)
features.6.conv.0 tensor(0.0352)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0781)
features.7.conv.0 tensor(0.2000)
features.7.conv.3 tensor(0.4508)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6941)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5515)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0478)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.2911)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6381)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8160)
features.12.conv.3 tensor(0.6682)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4478)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6911)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8380)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7431)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4748)
tensor(1616940.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][   20/  196]   Loss 0.028746   Top1 99.042969   Top5 99.980469   BatchTime 0.527979   LR 0.000005
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][   40/  196]   Loss 0.033150   Top1 98.955078   Top5 99.990234   BatchTime 0.467899   LR 0.000004
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][   60/  196]   Loss 0.034395   Top1 98.925781   Top5 99.993490   BatchTime 0.439420   LR 0.000004
tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][   80/  196]   Loss 0.035644   Top1 98.803711   Top5 99.995117   BatchTime 0.429084   LR 0.000004
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][  100/  196]   Loss 0.036820   Top1 98.734375   Top5 99.996094   BatchTime 0.430945   LR 0.000004
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][  120/  196]   Loss 0.037064   Top1 98.743490   Top5 99.996745   BatchTime 0.435690   LR 0.000004
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][  140/  196]   Loss 0.037014   Top1 98.766741   Top5 99.997210   BatchTime 0.436794   LR 0.000004
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][  160/  196]   Loss 0.036793   Top1 98.762207   Top5 99.997559   BatchTime 0.430259   LR 0.000003
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [65][  180/  196]   Loss 0.036442   Top1 98.767361   Top5 99.997830   BatchTime 0.428462   LR 0.000003
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - ==> Top1: 98.782    Top5: 99.998    Loss: 0.036
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [65][   20/   40]   Loss 0.420661   Top1 90.410156   Top5 99.707031   BatchTime 0.155997
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0573)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.1299)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0911)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5195)
features.5.conv.0 tensor(0.5039)
features.5.conv.3 tensor(0.4115)
features.5.conv.6 tensor(0.6414)
features.6.conv.0 tensor(0.0355)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0778)
features.7.conv.0 tensor(0.1999)
features.7.conv.3 tensor(0.4511)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6937)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6652)
features.9.conv.3 tensor(0.5509)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.2912)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6383)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4478)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4749)
tensor(1616961.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.417878   Top1 90.140000   Top5 99.750000   BatchTime 0.104337
INFO - ==> Top1: 90.140    Top5: 99.750    Loss: 0.418
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [65][Top1: 90.140   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][   20/  196]   Loss 0.029913   Top1 99.082031   Top5 100.000000   BatchTime 0.534421   LR 0.000003
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][   40/  196]   Loss 0.030100   Top1 99.052734   Top5 100.000000   BatchTime 0.496460   LR 0.000003
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][   60/  196]   Loss 0.034438   Top1 98.893229   Top5 100.000000   BatchTime 0.467265   LR 0.000003
tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][   80/  196]   Loss 0.035316   Top1 98.818359   Top5 100.000000   BatchTime 0.439544   LR 0.000002
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][  100/  196]   Loss 0.035533   Top1 98.777344   Top5 100.000000   BatchTime 0.434210   LR 0.000002
tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][  120/  196]   Loss 0.035259   Top1 98.779297   Top5 100.000000   BatchTime 0.433988   LR 0.000002
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][  140/  196]   Loss 0.035535   Top1 98.772321   Top5 99.997210   BatchTime 0.432726   LR 0.000002
tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][  160/  196]   Loss 0.035599   Top1 98.771973   Top5 99.997559   BatchTime 0.431117   LR 0.000002
tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [66][  180/  196]   Loss 0.034904   Top1 98.797743   Top5 99.997830   BatchTime 0.424202   LR 0.000002
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.808    Top5: 99.996    Loss: 0.035
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [66][   20/   40]   Loss 0.431201   Top1 90.117188   Top5 99.746094   BatchTime 0.150811
INFO - Validation [66][   40/   40]   Loss 0.423184   Top1 90.000000   Top5 99.760000   BatchTime 0.101821
INFO - ==> Top1: 90.000    Top5: 99.760    Loss: 0.423
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [65][Top1: 90.140   Top5: 99.750]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0855)
features.2.conv.0 tensor(0.1299)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0744)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0898)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5195)
features.5.conv.0 tensor(0.5041)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6413)
features.6.conv.0 tensor(0.0353)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0785)
features.7.conv.0 tensor(0.2002)
features.7.conv.3 tensor(0.4508)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6937)
features.8.conv.3 tensor(0.5414)
features.8.conv.6 tensor(0.7545)
features.9.conv.0 tensor(0.6650)
features.9.conv.3 tensor(0.5506)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0475)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.2912)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6381)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4478)
features.13.conv.3 tensor(0.4799)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4749)
tensor(1616991.) 2188896.0
tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][   20/  196]   Loss 0.036034   Top1 98.730469   Top5 100.000000   BatchTime 0.543602   LR 0.000002
tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][   40/  196]   Loss 0.034599   Top1 98.828125   Top5 100.000000   BatchTime 0.483892   LR 0.000002
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][   60/  196]   Loss 0.034749   Top1 98.776042   Top5 100.000000   BatchTime 0.457582   LR 0.000001
tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][   80/  196]   Loss 0.035909   Top1 98.745117   Top5 100.000000   BatchTime 0.440131   LR 0.000001
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][  100/  196]   Loss 0.035747   Top1 98.734375   Top5 99.996094   BatchTime 0.422691   LR 0.000001
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][  120/  196]   Loss 0.036190   Top1 98.701172   Top5 99.996745   BatchTime 0.420095   LR 0.000001
tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][  140/  196]   Loss 0.035968   Top1 98.724888   Top5 99.997210   BatchTime 0.425398   LR 0.000001
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][  160/  196]   Loss 0.036637   Top1 98.715820   Top5 99.997559   BatchTime 0.430349   LR 0.000001
tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [67][  180/  196]   Loss 0.036419   Top1 98.717448   Top5 99.997830   BatchTime 0.423652   LR 0.000001
tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.718    Top5: 99.998    Loss: 0.037
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [67][   20/   40]   Loss 0.422731   Top1 90.351562   Top5 99.687500   BatchTime 0.183031
INFO - Validation [67][   40/   40]   Loss 0.417403   Top1 90.220000   Top5 99.730000   BatchTime 0.118822
INFO - ==> Top1: 90.220    Top5: 99.730    Loss: 0.417
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [67][Top1: 90.220   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [55][Top1: 90.160   Top5: 99.690]
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1299)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0738)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0898)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5195)
features.5.conv.0 tensor(0.5037)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6413)
features.6.conv.0 tensor(0.0353)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0782)
features.7.conv.0 tensor(0.2002)
features.7.conv.3 tensor(0.4511)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6937)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7545)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5506)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0474)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.2912)
features.11.conv.0 tensor(0.8080)
features.11.conv.3 tensor(0.6383)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8158)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4478)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4749)
tensor(1616954.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][   20/  196]   Loss 0.032900   Top1 98.945312   Top5 100.000000   BatchTime 0.521527   LR 0.000001
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][   40/  196]   Loss 0.035345   Top1 98.808594   Top5 100.000000   BatchTime 0.483663   LR 0.000001
tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][   60/  196]   Loss 0.034153   Top1 98.847656   Top5 100.000000   BatchTime 0.465843   LR 0.000001
tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][   80/  196]   Loss 0.033558   Top1 98.862305   Top5 100.000000   BatchTime 0.454307   LR 0.000000
tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][  100/  196]   Loss 0.032933   Top1 98.859375   Top5 100.000000   BatchTime 0.435947   LR 0.000000
tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][  120/  196]   Loss 0.033986   Top1 98.844401   Top5 100.000000   BatchTime 0.436076   LR 0.000000
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][  140/  196]   Loss 0.034188   Top1 98.830915   Top5 100.000000   BatchTime 0.435166   LR 0.000000
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][  160/  196]   Loss 0.033594   Top1 98.850098   Top5 100.000000   BatchTime 0.435267   LR 0.000000
tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [68][  180/  196]   Loss 0.033136   Top1 98.873698   Top5 100.000000   BatchTime 0.436483   LR 0.000000
tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.858    Top5: 99.998    Loss: 0.034
tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward0>)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [68][   20/   40]   Loss 0.428957   Top1 90.390625   Top5 99.687500   BatchTime 0.206051
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1302)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0741)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0902)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5192)
features.5.conv.0 tensor(0.5037)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6414)
features.6.conv.0 tensor(0.0356)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0782)
features.7.conv.0 tensor(0.2002)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6938)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7544)
features.9.conv.0 tensor(0.6651)
features.9.conv.3 tensor(0.5506)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0473)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.2912)
features.11.conv.0 tensor(0.8080)
features.11.conv.3 tensor(0.6383)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7432)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4749)
tensor(1616973.) 2188896.0
INFO - Validation [68][   40/   40]   Loss 0.422713   Top1 90.170000   Top5 99.770000   BatchTime 0.128996
INFO - ==> Top1: 90.170    Top5: 99.770    Loss: 0.423
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [67][Top1: 90.220   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [68][Top1: 90.170   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [60][Top1: 90.170   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][   20/  196]   Loss 0.028231   Top1 99.023438   Top5 100.000000   BatchTime 0.544647   LR 0.000000
tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][   40/  196]   Loss 0.029986   Top1 98.974609   Top5 100.000000   BatchTime 0.487677   LR 0.000000
tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][   60/  196]   Loss 0.030728   Top1 98.945312   Top5 100.000000   BatchTime 0.468535   LR 0.000000
tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][   80/  196]   Loss 0.032659   Top1 98.886719   Top5 99.995117   BatchTime 0.452514   LR 0.000000
tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][  100/  196]   Loss 0.032726   Top1 98.859375   Top5 99.996094   BatchTime 0.437878   LR 0.000000
tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][  120/  196]   Loss 0.033476   Top1 98.824870   Top5 99.996745   BatchTime 0.437806   LR 0.000000
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][  140/  196]   Loss 0.033697   Top1 98.808594   Top5 99.997210   BatchTime 0.438652   LR 0.000000
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][  160/  196]   Loss 0.033822   Top1 98.813477   Top5 99.997559   BatchTime 0.439121   LR 0.000000
tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - Training [69][  180/  196]   Loss 0.033584   Top1 98.817274   Top5 99.997830   BatchTime 0.439857   LR 0.000000
tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward0>)
INFO - ==> Top1: 98.816    Top5: 99.998    Loss: 0.034
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [69][   20/   40]   Loss 0.425921   Top1 90.527344   Top5 99.765625   BatchTime 0.183276
INFO - Validation [69][   40/   40]   Loss 0.418487   Top1 90.270000   Top5 99.770000   BatchTime 0.122967
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1299)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6641)
features.3.conv.0 tensor(0.0744)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.2671)
features.4.conv.0 tensor(0.0908)
features.4.conv.3 tensor(0.2894)
features.4.conv.6 tensor(0.5194)
features.5.conv.0 tensor(0.5034)
features.5.conv.3 tensor(0.4120)
features.5.conv.6 tensor(0.6414)
features.6.conv.0 tensor(0.0355)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0782)
features.7.conv.0 tensor(0.2002)
features.7.conv.3 tensor(0.4514)
features.7.conv.6 tensor(0.6556)
features.8.conv.0 tensor(0.6937)
features.8.conv.3 tensor(0.5420)
features.8.conv.6 tensor(0.7545)
features.9.conv.0 tensor(0.6650)
features.9.conv.3 tensor(0.5506)
features.9.conv.6 tensor(0.7731)
features.10.conv.0 tensor(0.0476)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.2912)
features.11.conv.0 tensor(0.8081)
features.11.conv.3 tensor(0.6383)
features.11.conv.6 tensor(0.8826)
features.12.conv.0 tensor(0.8159)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.9068)
features.13.conv.0 tensor(0.4479)
features.13.conv.3 tensor(0.4801)
features.13.conv.6 tensor(0.6910)
features.14.conv.0 tensor(0.9338)
features.14.conv.3 tensor(0.8328)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.8378)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.7431)
features.16.conv.3 tensor(0.8032)
features.16.conv.6 tensor(0.9390)
conv.0 tensor(0.4748)
tensor(1616954.) 2188896.0
INFO - ==> Top1: 90.270    Top5: 99.770    Loss: 0.418
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [69][Top1: 90.270   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [67][Top1: 90.220   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.170   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-051836/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [   20/   40]   Loss 0.425921   Top1 90.527344   Top5 99.765625   BatchTime 0.153557
INFO - Validation [   40/   40]   Loss 0.418487   Top1 90.270000   Top5 99.770000   BatchTime 0.103776
INFO - ==> Top1: 90.270    Top5: 99.770    Loss: 0.418
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...