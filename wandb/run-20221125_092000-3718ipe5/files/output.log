Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
*************soft_pruning_mode*******************
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.95438832
0.95024335
0.93731868
0.92053413
INFO - Training [0][   20/  196]   Loss 1.582584   Top1 53.398438   Top5 89.121094   BatchTime 0.322177   LR 0.004999
0.90632999
0.89980632
0.89493030
0.88962132
0.88780773
0.88602167
0.87842321
0.87229538
0.86669505
0.86332613
0.86307001
0.86413670
0.86486614
0.86529690
0.86544913
0.86595154
0.86634660
0.86714143
0.86832368
0.86933118
0.87044972
0.87114066
INFO - Training [0][   40/  196]   Loss 1.502025   Top1 51.865234   Top5 89.726562   BatchTime 0.294190   LR 0.004995
0.87264591
0.87419254
0.87629521
0.87946814
0.88232160
0.88494402
0.88602203
0.88665384
0.88743681
0.88820016
0.89009607
0.89437300
0.89529687
0.89370108
0.88981611
0.88591069
0.88919312
0.89145011
0.89280510
0.89471334
0.89663976
0.89895785
INFO - Training [0][   60/  196]   Loss 1.400304   Top1 54.205729   Top5 90.852865   BatchTime 0.288668   LR 0.004989
0.89978588
0.90055293
0.90119267
0.90205586
0.90273720
0.90331000
0.90414286
0.90474188
0.90622073
0.90759671
0.90791756
0.90658516
0.90851516
0.90963775
0.90888911
0.90398645
0.89626211
0.90450621
0.90922761
0.91180205
0.91283411
0.91479504
INFO - Training [0][   80/  196]   Loss 1.328494   Top1 56.176758   Top5 91.674805   BatchTime 0.284964   LR 0.004980
0.91557395
0.91560501
0.91656303
0.91914976
0.92388278
0.92506903
0.92608368
0.92769623
0.92882174
0.92967993
0.93041945
0.93111151
0.93002737
0.93175060
0.93206078
0.93231440
INFO - Training [0][  100/  196]   Loss 1.269065   Top1 57.855469   Top5 92.316406   BatchTime 0.279486   LR 0.004968
0.93239766
0.93254757
0.93259597
0.93240750
0.93294322
0.93293619
0.93281031
0.93279648
0.93298453
0.93292594
0.93295044
0.93298084
0.93281317
0.93299460
0.93317389
0.93308729
0.93335801
0.93360108
0.93339652
0.93377328
0.93382293
0.93381345
0.93373859
0.93388104
0.93376249
0.93402582
INFO - Training [0][  120/  196]   Loss 1.220416   Top1 59.436849   Top5 92.848307   BatchTime 0.273619   LR 0.004954
0.93384343
0.93379337
0.93366027
0.93361604
0.93381619
0.93374223
0.93362784
0.93354744
0.93381113
0.93373835
0.93372190
0.93354034
0.93348372
0.93349129
0.93349361
0.93345171
INFO - Training [0][  140/  196]   Loss 1.188486   Top1 60.432478   Top5 93.169643   BatchTime 0.271221   LR 0.004938
0.93346500
0.93339628
0.93322676
0.93331999
0.93322045
0.93329722
0.93317282
0.93318212
0.93343467
0.93346059
0.93281567
0.93306732
0.93304116
0.93299454
0.93294030
0.93286091
0.93284529
0.93282282
0.93282026
0.93268639
0.93278176
0.93282551
0.93286079
INFO - Training [0][  160/  196]   Loss 1.171574   Top1 60.888672   Top5 93.151855   BatchTime 0.269644   LR 0.004919
0.93298870
0.93316722
0.93304539
0.93313193
0.93325514
0.93340290
0.93370807
0.93340427
0.93303829
0.93311095
0.93291026
0.93300748
0.93335783
0.93311781
0.93333083
0.93334115
0.93333340
INFO - Training [0][  180/  196]   Loss 1.147262   Top1 61.636285   Top5 93.331163   BatchTime 0.266800   LR 0.004897
0.93331730
0.93352741
0.93368059
0.93372303
0.93378001
0.93392313
0.93359876
0.93402320
0.93189770
0.92902136
0.93034273
0.93157411
0.93377042
0.93393105
0.93368983
INFO - ==> Top1: 62.212    Top5: 93.506    Loss: 1.129
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.93338770
0.93334883
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.867443   Top1 72.421875   Top5 97.382812   BatchTime 0.115253
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.2129)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0703)
features.2.conv.0 tensor(0.0365)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0891)
features.3.conv.0 tensor(0.0362)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0658)
features.4.conv.0 tensor(0.0495)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.1037)
features.5.conv.0 tensor(0.0628)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.1151)
features.6.conv.0 tensor(0.0415)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0978)
features.7.conv.0 tensor(0.0734)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.1245)
features.8.conv.0 tensor(0.0858)
features.8.conv.3 tensor(0.0952)
features.8.conv.6 tensor(0.1292)
features.9.conv.0 tensor(0.0882)
features.9.conv.3 tensor(0.1244)
features.9.conv.6 tensor(0.1246)
features.10.conv.0 tensor(0.0743)
features.10.conv.3 tensor(0.0865)
features.10.conv.6 tensor(0.1061)
features.11.conv.0 tensor(0.1064)
features.11.conv.3 tensor(0.0961)
features.11.conv.6 tensor(0.1653)
features.12.conv.0 tensor(0.0924)
features.12.conv.3 tensor(0.0907)
features.12.conv.6 tensor(0.1819)
features.13.conv.0 tensor(0.1760)
features.13.conv.3 tensor(0.1194)
features.13.conv.6 tensor(0.1273)
features.14.conv.0 tensor(0.0656)
features.14.conv.3 tensor(0.0816)
features.14.conv.6 tensor(0.3532)
features.15.conv.0 tensor(0.7629)
features.15.conv.3 tensor(0.0707)
features.15.conv.6 tensor(0.2744)
features.16.conv.0 tensor(0.0480)
features.16.conv.3 tensor(0.0807)
features.16.conv.6 tensor(0.1011)
conv.0 tensor(0.0604)
tensor(370075.) 2188896.0
INFO - Validation [0][   40/   40]   Loss 0.868524   Top1 72.720000   Top5 97.480000   BatchTime 0.088119
INFO - ==> Top1: 72.720    Top5: 97.480    Loss: 0.869
INFO - ==> Sparsity : 0.169
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 72.720   Top5: 97.480]
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.93373162
0.93382084
0.93382925
0.93381107
0.93370908
0.93385303
0.93380934
0.93366623
0.93343329
0.93345588
0.93351650
0.93349349
0.93335497
0.93354088
0.93345237
0.93292058
0.93299830
0.93292862
INFO - Training [1][   20/  196]   Loss 1.025278   Top1 64.746094   Top5 92.695312   BatchTime 0.350951   LR 0.004853
0.93287253
0.93289459
0.93282723
0.93266284
0.93285012
0.93292648
0.93270922
0.93298811
0.93303525
0.93286604
0.93287027
0.93258923
0.93244338
0.93253750
0.93256795
0.93275505
0.93245935
0.93255603
0.93275452
0.93276495
0.93271977
0.93286210
0.93282151
0.93287510
INFO - Training [1][   40/  196]   Loss 0.982130   Top1 66.201172   Top5 94.277344   BatchTime 0.297257   LR 0.004825
0.93300575
0.93288779
0.93090814
0.91458297
0.93130463
0.93016666
0.91582882
0.92960906
0.93256623
0.93245357
0.93260330
0.93265337
0.93184167
0.93064266
0.93139231
0.93264091
0.93244338
0.93222523
0.93219632
0.93225580
0.93230981
INFO - Training [1][   60/  196]   Loss 0.960520   Top1 67.115885   Top5 94.648438   BatchTime 0.292151   LR 0.004794
0.93234408
0.93228960
0.93216389
0.93246627
0.93280017
0.93262941
0.93282217
0.93269539
0.93253702
0.93249226
0.93260366
0.93196541
0.92884773
0.90557891
0.90463269
INFO - Training [1][   80/  196]   Loss 0.936937   Top1 68.046875   Top5 95.112305   BatchTime 0.286758   LR 0.004761
0.90485626
0.90471798
0.90468550
0.90469080
0.90589362
0.91565561
0.93273735
0.93257719
0.93264246
0.93227899
0.93241668
0.93238711
0.93227386
0.93241042
0.93295699
0.93289685
0.93276364
0.93285769
0.93282664
0.93270117
0.93260926
0.93263131
0.93253237
INFO - Training [1][  100/  196]   Loss 0.915451   Top1 68.679688   Top5 95.394531   BatchTime 0.281489   LR 0.004725
0.93040961
0.93231899
0.93228114
0.93236238
0.93242419
0.93251652
0.93249363
0.93257248
0.93273497
0.93244910
0.93253636
0.93249661
0.93272793
0.93266803
0.93239361
0.93232065
INFO - Training [1][  120/  196]   Loss 0.901961   Top1 69.192708   Top5 95.618490   BatchTime 0.276257   LR 0.004687
0.93221831
0.93204218
0.93206894
0.93224961
0.93238574
0.93221378
0.93216860
0.93228406
0.93215269
0.93202996
0.93214881
0.93229204
0.93228185
0.93232244
0.93229330
0.93219244
0.93220764
0.93236285
0.93243259
0.93249655
0.93255574
0.93256271
0.93248326
INFO - Training [1][  140/  196]   Loss 0.891590   Top1 69.584263   Top5 95.831473   BatchTime 0.273742   LR 0.004647
0.93239087
0.93224573
0.93243277
0.93235421
0.93232876
0.93220139
0.93208385
0.93223631
0.93234038
0.93220729
0.93222308
0.93235409
0.93266040
0.93234807
0.93213910
0.93204170
0.93197298
0.93206650
0.93195009
0.93194038
0.93210179
0.93200642
0.93205923
0.93199503
INFO - Training [1][  160/  196]   Loss 0.883526   Top1 69.819336   Top5 95.900879   BatchTime 0.271604   LR 0.004605
0.93206471
0.93193018
0.93190110
0.93194997
0.93228084
0.93231916
0.93241191
0.93252438
0.93259650
0.93250918
0.93216914
0.93225956
0.93225628
0.93223530
0.93230653
INFO - Training [1][  180/  196]   Loss 0.871145   Top1 70.223524   Top5 95.970052   BatchTime 0.269487   LR 0.004560
0.93259495
0.93235546
0.93232399
0.93232220
0.93227690
0.93232435
0.93246138
0.93212795
0.93237263
0.93255889
0.93246812
0.93219823
0.93217218
0.93211931
0.93204957
0.93182868
0.93152767
INFO - ==> Top1: 70.462    Top5: 96.040    Loss: 0.865
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.607904   Top1 79.726562   Top5 98.554688   BatchTime 0.111360
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1934)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0694)
features.2.conv.0 tensor(0.0420)
features.2.conv.3 tensor(0.0710)
features.2.conv.6 tensor(0.0903)
features.3.conv.0 tensor(0.0454)
features.3.conv.3 tensor(0.0625)
features.3.conv.6 tensor(0.0720)
features.4.conv.0 tensor(0.0452)
features.4.conv.3 tensor(0.1007)
features.4.conv.6 tensor(0.0926)
features.5.conv.0 tensor(0.0596)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.1136)
features.6.conv.0 tensor(0.0454)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0938)
features.7.conv.0 tensor(0.0748)
features.7.conv.3 tensor(0.1010)
features.7.conv.6 tensor(0.1219)
features.8.conv.0 tensor(0.0920)
features.8.conv.3 tensor(0.1088)
features.8.conv.6 tensor(0.1334)
features.9.conv.0 tensor(0.0979)
features.9.conv.3 tensor(0.1403)
features.9.conv.6 tensor(0.1340)
features.10.conv.0 tensor(0.0621)
features.10.conv.3 tensor(0.0874)
features.10.conv.6 tensor(0.1050)
features.11.conv.0 tensor(0.1055)
features.11.conv.3 tensor(0.1059)
features.11.conv.6 tensor(0.1676)
features.12.conv.0 tensor(0.1287)
features.12.conv.3 tensor(0.1036)
features.12.conv.6 tensor(0.2031)
features.13.conv.0 tensor(0.1557)
features.13.conv.3 tensor(0.1271)
features.13.conv.6 tensor(0.1196)
features.14.conv.0 tensor(0.0823)
features.14.conv.3 tensor(0.0806)
features.14.conv.6 tensor(0.3568)
features.15.conv.0 tensor(0.8087)
features.15.conv.3 tensor(0.0770)
features.15.conv.6 tensor(0.3151)
features.16.conv.0 tensor(0.0575)
features.16.conv.3 tensor(0.0859)
features.16.conv.6 tensor(0.1243)
conv.0 tensor(0.0657)
tensor(399277.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 0.613225   Top1 79.720000   Top5 98.610000   BatchTime 0.078553
INFO - ==> Top1: 79.720    Top5: 98.610    Loss: 0.613
INFO - ==> Sparsity : 0.182
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 79.720   Top5: 98.610]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 72.720   Top5: 97.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.93174684
0.93193436
0.93143421
0.93162686
0.93197113
0.93182188
0.93155122
0.93163580
0.93182701
0.93180710
0.93191159
0.93206710
0.93225950
0.93207979
0.93225455
0.93187338
0.93181896
INFO - Training [2][   20/  196]   Loss 0.833349   Top1 71.601562   Top5 95.683594   BatchTime 0.324769   LR 0.004477
0.93211621
0.93187344
0.93199903
0.93209624
0.93220240
0.93265432
0.93135548
0.92327678
0.90867281
0.92192638
0.93148220
0.93246144
0.93229854
0.93211621
0.93210286
0.93186879
0.93149632
0.93189573
0.93229574
0.93240982
0.93219227
0.93229616
0.93213469
INFO - Training [2][   40/  196]   Loss 0.818095   Top1 72.041016   Top5 95.976562   BatchTime 0.291138   LR 0.004426
0.93208563
0.93222630
0.93210083
0.93197060
0.93208236
0.93200439
0.93211687
0.93199295
0.93208516
0.93211615
0.93216592
0.93230683
0.93234837
0.93237740
0.93234849
0.93258154
0.93197018
0.93200171
0.93196923
0.93184519
0.93181282
0.93176222
0.93163377
0.93145335
INFO - Training [2][   60/  196]   Loss 0.803170   Top1 72.519531   Top5 96.276042   BatchTime 0.279103   LR 0.004374
0.93149346
0.93143070
0.93148261
0.93132895
0.93140960
0.93143231
0.93187743
0.93156964
0.93179005
0.93204021
0.93179089
0.93191087
0.93175209
0.93178290
0.93174565
0.93170249
INFO - Training [2][   80/  196]   Loss 0.787981   Top1 73.159180   Top5 96.425781   BatchTime 0.271005   LR 0.004320
0.93156087
0.93157458
0.93160588
0.93165445
0.93184936
0.93173945
0.93165106
0.93170917
0.93172747
0.93177921
0.93201494
0.93204743
0.93179631
0.93181854
0.93143350
0.93137401
0.93158031
0.93167442
INFO - Training [2][  100/  196]   Loss 0.776979   Top1 73.601562   Top5 96.449219   BatchTime 0.262211   LR 0.004264
0.93167955
0.93160522
0.93190539
0.93189561
0.93166369
0.93165892
0.93155837
0.93153811
0.93175465
0.93160748
0.93166476
0.93156141
0.93169552
0.93162823
0.93155563
0.93155098
0.93160063
0.93145543
0.93160218
0.93139482
0.93132013
0.93123078
0.93111151
0.93089014
INFO - Training [2][  120/  196]   Loss 0.767851   Top1 73.912760   Top5 96.598307   BatchTime 0.259530   LR 0.004206
0.93052763
0.93066674
0.93060631
0.93074620
0.93050069
0.93051785
0.93060088
0.93072492
0.93077010
0.93062538
0.93039358
0.93052351
0.92983419
0.92869598
0.92920434
0.92878103
INFO - Training [2][  140/  196]   Loss 0.768353   Top1 73.828125   Top5 96.674107   BatchTime 0.258627   LR 0.004146
0.92871165
0.92725325
0.92612493
0.92611247
0.92452729
0.92343229
0.92182106
0.92268270
0.92356843
0.92318571
0.92149699
0.92103982
0.92097437
0.92055756
0.92029202
0.91978961
0.91946399
0.91897446
0.91890937
0.91805953
0.91779774
0.91779071
0.91810513
0.91831172
INFO - Training [2][  160/  196]   Loss 0.777880   Top1 73.532715   Top5 96.618652   BatchTime 0.257810   LR 0.004085
0.91860020
0.91891861
0.91878843
0.91884708
0.91922253
0.91948086
0.91935247
0.91925412
0.91922998
0.91979462
0.92263567
0.92814463
0.93139338
0.93155354
0.93128771
0.93107742
INFO - Training [2][  180/  196]   Loss 0.777568   Top1 73.554688   Top5 96.564670   BatchTime 0.257286   LR 0.004022
0.93130636
0.93142945
0.93124008
0.93108463
0.93109959
0.93100613
0.93097925
0.93122560
0.93136334
0.93166554
0.93199944
0.93190038
0.93173349
0.93141210
0.93120056
0.92984790
0.92847908
0.92579013
********************pre-trained*****************
INFO - ==> Top1: 73.674    Top5: 96.556    Loss: 0.775
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.798347   Top1 74.667969   Top5 97.226562   BatchTime 0.110898
INFO - Validation [2][   40/   40]   Loss 0.795760   Top1 74.170000   Top5 97.480000   BatchTime 0.083529
INFO - ==> Top1: 74.170    Top5: 97.480    Loss: 0.796
INFO - ==> Sparsity : 0.216
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 79.720   Top5: 98.610]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 74.170   Top5: 97.480]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 72.720   Top5: 97.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0961)
features.1.conv.6 tensor(0.3498)
features.2.conv.0 tensor(0.0295)
features.2.conv.3 tensor(0.0733)
features.2.conv.6 tensor(0.0929)
features.3.conv.0 tensor(0.0304)
features.3.conv.3 tensor(0.0648)
features.3.conv.6 tensor(0.0684)
features.4.conv.0 tensor(0.0369)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.1032)
features.5.conv.0 tensor(0.0570)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.1105)
features.6.conv.0 tensor(0.0568)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0903)
features.7.conv.0 tensor(0.0840)
features.7.conv.3 tensor(0.1105)
features.7.conv.6 tensor(0.1209)
features.8.conv.0 tensor(0.0940)
features.8.conv.3 tensor(0.1117)
features.8.conv.6 tensor(0.1302)
features.9.conv.0 tensor(0.1001)
features.9.conv.3 tensor(0.1458)
features.9.conv.6 tensor(0.1232)
features.10.conv.0 tensor(0.0570)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.1010)
features.11.conv.0 tensor(0.1158)
features.11.conv.3 tensor(0.1127)
features.11.conv.6 tensor(0.1739)
features.12.conv.0 tensor(0.1174)
features.12.conv.3 tensor(0.1400)
features.12.conv.6 tensor(0.1970)
features.13.conv.0 tensor(0.1462)
features.13.conv.3 tensor(0.1329)
features.13.conv.6 tensor(0.1185)
features.14.conv.0 tensor(0.4010)
features.14.conv.3 tensor(0.0848)
features.14.conv.6 tensor(0.3882)
features.15.conv.0 tensor(0.8258)
features.15.conv.3 tensor(0.0751)
features.15.conv.6 tensor(0.3432)
features.16.conv.0 tensor(0.0804)
features.16.conv.3 tensor(0.0890)
features.16.conv.6 tensor(0.1073)
conv.0 tensor(0.1038)
tensor(473886.) 2188896.0
0.92409974
0.91031921
0.90212286
0.90232569
0.90276474
0.90287793
0.90332323
0.90324807
0.90359265
0.90706736
0.91490185
0.92369807
0.92877030
0.93125033
0.93179268
0.93193227
0.93160963
0.93152553
0.93197507
0.93202800
0.93207729
0.93202454
INFO - Training [3][   20/  196]   Loss 0.757312   Top1 73.496094   Top5 96.230469   BatchTime 0.362676   LR 0.003907
0.93207496
0.93208504
0.93218768
0.93210649
0.93200988
0.93204397
0.93215573
0.93188673
0.93193990
0.93205309
0.93200868
0.93208212
0.93214500
0.93220210
0.93193561
0.93176419
0.93209100
0.93206573
0.93197960
0.93196374
INFO - Training [3][   40/  196]   Loss 0.748534   Top1 73.916016   Top5 96.640625   BatchTime 0.329444   LR 0.003840
0.93178970
0.93168068
0.93159515
0.93144417
0.93132001
0.93121260
0.93127877
0.93147403
0.93159693
0.93156719
0.93180531
0.93199366
0.93214303
0.93234974
0.93204892
0.93189287
0.93179429
INFO - Training [3][   60/  196]   Loss 0.739211   Top1 74.511719   Top5 96.679688   BatchTime 0.299678   LR 0.003771
0.93159473
0.93148315
0.93160802
0.93161643
0.93149596
0.93147880
0.93157089
0.93141735
0.93151140
0.93145066
0.93122047
0.93111187
0.93136811
0.93094134
0.93067443
0.93055022
0.93039167
0.93025315
0.93011433
0.93004775
0.93006647
0.92969018
0.92914891
0.92847049
0.92775738
INFO - Training [3][   80/  196]   Loss 0.732953   Top1 74.843750   Top5 96.835938   BatchTime 0.283261   LR 0.003701
0.92610550
0.92448956
0.92209595
0.92010880
0.91699880
0.91556317
0.90915924
0.90462428
0.90337944
0.90325087
0.90289664
0.90278751
0.90279782
0.90280259
0.90269363
0.90267807
0.90244341
INFO - Training [3][  100/  196]   Loss 0.725544   Top1 75.210938   Top5 96.937500   BatchTime 0.274954   LR 0.003630
0.90234172
0.90219992
0.90207285
0.90201437
0.90168977
0.90139616
0.90120429
0.90120178
0.90024263
0.89936101
0.89803934
0.89558524
0.89329082
0.89103764
0.88916945
0.88713652
0.88480550
0.88334835
0.88221663
0.88181502
0.88180375
INFO - Training [3][  120/  196]   Loss 0.715027   Top1 75.625000   Top5 97.067057   BatchTime 0.275507   LR 0.003558
0.88149959
0.88140017
0.88141650
0.88217187
0.88348162
0.88437599
0.88520330
0.88683003
0.88831890
0.88979989
0.89168370
0.89400214
0.89611042
0.89859706
0.90083516
0.90227681
0.90325099
0.90388477
INFO - Training [3][  140/  196]   Loss 0.710105   Top1 75.792411   Top5 97.140067   BatchTime 0.282472   LR 0.003484
0.90423977
0.90415078
0.90408009
0.90421104
0.90451962
0.90450329
0.90445530
0.90469617
0.90435350
0.90415025
0.90412295
0.90424466
0.90412360
0.90421653
0.90428597
0.90420163
0.90439612
0.90417898
0.90438533
0.90435296
INFO - Training [3][  160/  196]   Loss 0.712065   Top1 75.808105   Top5 97.109375   BatchTime 0.286951   LR 0.003410
0.90453374
0.90465724
0.90441602
0.90437007
0.90427524
0.90420306
0.90420717
0.90405214
0.90422493
0.90466660
0.90466648
0.90474796
0.90462250
0.90429252
0.90436274
0.90436071
0.90450907
0.90458310
0.90473938
0.90484184
0.90472490
INFO - Training [3][  180/  196]   Loss 0.708835   Top1 75.926649   Top5 97.059462   BatchTime 0.287023   LR 0.003335
0.90478116
0.90451479
0.90448534
0.90442550
0.90443134
0.90433824
0.90457916
0.90485317
0.90488011
0.90497106
0.90466273
0.90448463
0.90465569
INFO - ==> Top1: 76.032    Top5: 97.072    Loss: 0.706
0.90445566
0.90417379
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.689202   Top1 77.070312   Top5 98.437500   BatchTime 0.123656
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.2227)
features.1.conv.0 tensor(0.0273)
features.1.conv.3 tensor(0.1007)
features.1.conv.6 tensor(0.0720)
features.2.conv.0 tensor(0.0353)
features.2.conv.3 tensor(0.0702)
features.2.conv.6 tensor(0.0984)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0633)
features.3.conv.6 tensor(0.0634)
features.4.conv.0 tensor(0.0389)
features.4.conv.3 tensor(0.1076)
features.4.conv.6 tensor(0.0924)
features.5.conv.0 tensor(0.0399)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.1165)
features.6.conv.0 tensor(0.0451)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0875)
features.7.conv.0 tensor(0.0762)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1162)
features.8.conv.0 tensor(0.0778)
features.8.conv.3 tensor(0.1137)
features.8.conv.6 tensor(0.1302)
features.9.conv.0 tensor(0.0875)
features.9.conv.3 tensor(0.1493)
features.9.conv.6 tensor(0.1181)
features.10.conv.0 tensor(0.0511)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0993)
features.11.conv.0 tensor(0.1302)
features.11.conv.3 tensor(0.1146)
features.11.conv.6 tensor(0.1834)
features.12.conv.0 tensor(0.1155)
features.12.conv.3 tensor(0.1470)
features.12.conv.6 tensor(0.2120)
features.13.conv.0 tensor(0.1259)
features.13.conv.3 tensor(0.1360)
features.13.conv.6 tensor(0.1317)
features.14.conv.0 tensor(0.0995)
features.14.conv.3 tensor(0.0917)
features.14.conv.6 tensor(0.4097)
features.15.conv.0 tensor(0.8460)
features.15.conv.3 tensor(0.0752)
features.15.conv.6 tensor(0.8526)
features.16.conv.0 tensor(0.0760)
features.16.conv.3 tensor(0.0926)
features.16.conv.6 tensor(0.1566)
conv.0 tensor(0.1321)
tensor(538378.) 2188896.0
INFO - Validation [3][   40/   40]   Loss 0.696200   Top1 76.630000   Top5 98.310000   BatchTime 0.089149
INFO - ==> Top1: 76.630    Top5: 98.310    Loss: 0.696
INFO - ==> Sparsity : 0.246
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 79.720   Top5: 98.610]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 76.630   Top5: 98.310]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 74.170   Top5: 97.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.90395987
0.90390354
0.90376461
0.90364701
0.90319979
0.90281957
0.90258634
0.90237421
0.90216208
0.90201849
0.90195209
0.90202534
0.90229774
0.90231490
0.90227914
0.90263420
0.90263945
0.90256286
0.90261233
0.90292251
INFO - Training [4][   20/  196]   Loss 0.709113   Top1 75.664062   Top5 96.953125   BatchTime 0.309073   LR 0.003200
0.90314090
0.90300250
0.89942843
0.89666235
0.88750577
0.87977284
0.87568146
0.87535024
0.87568659
0.87968802
0.89167553
0.90101868
0.90329081
0.90325207
0.90317082
INFO - Training [4][   40/  196]   Loss 0.693547   Top1 76.699219   Top5 97.119141   BatchTime 0.285586   LR 0.003122
0.90326846
0.90347779
0.90369856
0.90369350
0.90368783
0.90391856
0.90419823
0.90418476
0.90411240
0.90410942
0.90416944
0.90414184
0.90432405
0.90432346
0.90438819
0.90437436
0.90415531
0.90410221
0.90413135
0.90405697
0.90406775
0.90403986
0.90391916
INFO - Training [4][   60/  196]   Loss 0.677423   Top1 77.122396   Top5 97.311198   BatchTime 0.275229   LR 0.003044
0.90374964
0.90357804
0.90368509
0.90389347
0.90399069
0.90420902
0.90422326
0.90407848
0.90362483
0.90355021
0.90331143
0.90311807
0.90298134
0.90301627
0.90297943
0.90310597
0.90305769
0.90306973
0.90312809
0.90305018
INFO - Training [4][   80/  196]   Loss 0.671370   Top1 77.324219   Top5 97.465820   BatchTime 0.284950   LR 0.002965
0.90331829
0.90335864
0.90356034
0.90382439
0.90385562
0.90386701
0.90372866
0.90378147
0.90376019
0.90362823
0.90339738
0.90354294
0.90356481
0.90381563
0.90367836
0.90377539
0.90379310
0.90418702
0.90371120
0.90352881
0.90316784
0.90311348
0.90288168
0.90255326
INFO - Training [4][  100/  196]   Loss 0.661610   Top1 77.726562   Top5 97.539062   BatchTime 0.277804   LR 0.002886
0.90280670
0.90325028
0.90338778
0.90341890
0.90362883
0.90346766
0.90353143
0.90358341
0.90404010
0.90397543
0.90392202
0.90378749
0.90349156
0.90340406
0.90342277
0.90324938
0.90329617
0.90277469
0.90211630
0.90191001
INFO - Training [4][  120/  196]   Loss 0.650365   Top1 78.056641   Top5 97.639974   BatchTime 0.279572   LR 0.002806
0.90193903
0.90236908
0.90187925
0.90195698
0.90139163
0.90214592
0.90191424
0.90185040
0.90174371
0.90158701
0.90119296
0.90071881
0.90036756
INFO - Training [4][  140/  196]   Loss 0.645789   Top1 78.150112   Top5 97.714844   BatchTime 0.281101   LR 0.002726
0.89978325
0.89931774
0.89878339
0.89818466
0.89757288
0.89720058
0.89702016
0.89664763
0.89649409
0.89560014
0.89432126
0.89308238
0.89126724
0.88903445
0.88740885
0.88645673
0.88450611
0.88163382
0.88018477
0.88024372
0.88117301
0.88313395
INFO - Training [4][  160/  196]   Loss 0.646621   Top1 78.100586   Top5 97.678223   BatchTime 0.281502   LR 0.002646
0.88564831
0.88884735
0.89141631
0.89225143
0.89199054
0.89097959
0.88997936
0.88884008
0.88637716
0.88535690
0.88471091
0.88439155
0.88361913
0.88384402
0.88380849
0.88390434
0.88345206
0.88252771
0.88207793
0.88213485
0.88262516
0.88320947
0.88296062
0.88226515
INFO - Training [4][  180/  196]   Loss 0.642087   Top1 78.179253   Top5 97.636719   BatchTime 0.277842   LR 0.002566
0.88171691
0.88042557
0.87860638
0.87760210
0.87685221
0.87651592
0.87665027
0.87643421
0.87630230
0.87643981
0.87657803
0.87625891
0.87626034
0.87639880
0.87652737
INFO - ==> Top1: 78.262    Top5: 97.658    Loss: 0.638
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.480507   Top1 83.574219   Top5 99.257812   BatchTime 0.118256
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.2207)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0694)
features.2.conv.0 tensor(0.0312)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0958)
features.3.conv.0 tensor(0.0304)
features.3.conv.3 tensor(0.0664)
features.3.conv.6 tensor(0.0636)
features.4.conv.0 tensor(0.0430)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.0983)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.1113)
features.6.conv.0 tensor(0.0412)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0885)
features.7.conv.0 tensor(0.0753)
features.7.conv.3 tensor(0.1146)
features.7.conv.6 tensor(0.1123)
features.8.conv.0 tensor(0.0684)
features.8.conv.3 tensor(0.1088)
features.8.conv.6 tensor(0.1271)
features.9.conv.0 tensor(0.0934)
features.9.conv.3 tensor(0.1403)
features.9.conv.6 tensor(0.1225)
features.10.conv.0 tensor(0.0561)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.0953)
features.11.conv.0 tensor(0.1383)
features.11.conv.3 tensor(0.1165)
features.11.conv.6 tensor(0.1806)
features.12.conv.0 tensor(0.1427)
features.12.conv.3 tensor(0.1524)
features.12.conv.6 tensor(0.2075)
features.13.conv.0 tensor(0.1560)
features.13.conv.3 tensor(0.1304)
features.13.conv.6 tensor(0.1197)
features.14.conv.0 tensor(0.9799)
features.14.conv.3 tensor(0.0873)
features.14.conv.6 tensor(0.4123)
features.15.conv.0 tensor(0.8605)
features.15.conv.3 tensor(0.0764)
features.15.conv.6 tensor(0.9118)
features.16.conv.0 tensor(0.0517)
features.16.conv.3 tensor(0.0918)
features.16.conv.6 tensor(0.1547)
INFO - Validation [4][   40/   40]   Loss 0.471418   Top1 83.730000   Top5 99.340000   BatchTime 0.082877
INFO - ==> Top1: 83.730    Top5: 99.340    Loss: 0.471
INFO - ==> Sparsity : 0.314
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 83.730   Top5: 99.340]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 79.720   Top5: 98.610]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 76.630   Top5: 98.310]
conv.0 tensor(0.1410)
tensor(686514.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.87611538
0.87598491
0.87602121
0.87581789
0.87585044
0.87611306
0.87649965
0.87643909
0.87602395
0.87622249
0.87658602
0.87678510
0.87695009
0.87721157
INFO - Training [5][   20/  196]   Loss 0.616867   Top1 78.867188   Top5 97.207031   BatchTime 0.329616   LR 0.002424
0.87680876
0.87670249
0.87657189
0.87655216
0.87656552
0.87636191
0.87633133
0.87647599
0.87673086
0.87697548
0.87693232
0.87702537
0.87673575
0.87678045
0.87666595
0.87654191
0.87667745
0.87673634
0.87659973
0.87689847
0.87690032
0.87726295
0.87755638
INFO - Training [5][   40/  196]   Loss 0.617894   Top1 79.121094   Top5 97.382812   BatchTime 0.294829   LR 0.002343
0.87779903
0.87797511
0.87847233
0.87919021
0.87900525
0.87914729
0.87934446
0.87934482
0.87949157
0.87898457
0.87852973
0.87845188
0.87875110
0.87894493
0.87825710
0.87775677
0.87748426
0.87833786
0.87761611
0.87727678
0.87694889
0.87623531
0.87617421
0.87537408
INFO - Training [5][   60/  196]   Loss 0.607175   Top1 79.674479   Top5 97.532552   BatchTime 0.281766   LR 0.002263
0.87527120
0.87511432
0.87456685
0.87468213
0.87238848
0.87115878
0.87081128
0.87203604
0.87090695
0.86891854
0.86635876
0.86636633
0.86302263
0.86185110
0.86438864
INFO - Training [5][   80/  196]   Loss 0.604533   Top1 79.599609   Top5 97.670898   BatchTime 0.274127   LR 0.002183
0.86534488
0.86547393
0.86570436
0.86419302
0.86395907
0.86363918
0.86407429
0.86450350
0.86439538
0.86360204
0.86289078
0.86193788
0.86052221
0.85955191
0.85949403
0.85881102
INFO - Training [5][  100/  196]   Loss 0.597580   Top1 79.757812   Top5 97.781250   BatchTime 0.267960   LR 0.002104
0.85838419
0.85763824
0.85689467
0.85651451
0.85550672
0.85531890
0.85508281
0.85561961
0.85678035
0.85809422
0.85877168
0.85886413
0.85894459
0.85887218
0.85871482
0.85865438
0.85811055
0.85759127
0.85803545
0.85818100
0.85854608
0.85844558
0.85861933
0.85809880
0.85766792
0.85733372
INFO - Training [5][  120/  196]   Loss 0.592918   Top1 80.003255   Top5 97.802734   BatchTime 0.261905   LR 0.002024
0.85720068
0.85712737
0.85664600
0.85636878
0.85660803
0.85614038
0.85525119
0.85485005
0.85432100
0.85391349
0.85354483
0.85316819
0.85269040
0.85199314
0.85194802
0.85170740
0.85156095
INFO - Training [5][  140/  196]   Loss 0.589908   Top1 80.069754   Top5 97.840402   BatchTime 0.260390   LR 0.001946
0.85167533
0.85171044
0.85182542
0.85152334
0.85149938
0.85184628
0.85165042
0.85120171
0.85089773
0.85052794
0.85065496
0.85065424
0.85068297
0.85060567
0.85074592
0.85086918
0.85085052
0.85091835
0.85070664
0.85029912
0.85014069
0.85025084
0.85042000
0.85053456
INFO - Training [5][  160/  196]   Loss 0.592784   Top1 79.892578   Top5 97.854004   BatchTime 0.258183   LR 0.001868
0.85027373
0.85013521
0.85042596
0.85052764
0.85073978
0.85046983
0.85005331
0.84987247
0.84966207
0.84945911
0.84933269
0.84915769
0.84910703
0.84880030
0.84870934
0.84810239
0.84773010
INFO - Training [5][  180/  196]   Loss 0.592229   Top1 79.913194   Top5 97.792969   BatchTime 0.255690   LR 0.001790
0.84789288
0.84811449
0.84821343
0.84838414
0.84840268
0.84824330
0.84689116
0.84302008
0.84661216
0.84892005
0.85086989
0.85255235
0.85426545
0.85623890
0.85717493
0.85804546
INFO - ==> Top1: 79.954    Top5: 97.804    Loss: 0.590
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.85945064
0.86111176
0.86207330
0.86230415
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.492008   Top1 83.867188   Top5 99.062500   BatchTime 0.135669
INFO - Validation [5][   40/   40]   Loss 0.482352   Top1 83.790000   Top5 99.230000   BatchTime 0.095391
INFO - ==> Top1: 83.790    Top5: 99.230    Loss: 0.482
INFO - ==> Sparsity : 0.318
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 83.790   Top5: 99.230]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 83.730   Top5: 99.340]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 79.720   Top5: 98.610]
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0286)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0367)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0906)
features.3.conv.0 tensor(0.0278)
features.3.conv.3 tensor(0.0656)
features.3.conv.6 tensor(0.0627)
features.4.conv.0 tensor(0.0439)
features.4.conv.3 tensor(0.1036)
features.4.conv.6 tensor(0.1029)
features.5.conv.0 tensor(0.0342)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.1107)
features.6.conv.0 tensor(0.0295)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0869)
features.7.conv.0 tensor(0.0695)
features.7.conv.3 tensor(0.1146)
features.7.conv.6 tensor(0.1060)
features.8.conv.0 tensor(0.0603)
features.8.conv.3 tensor(0.1117)
features.8.conv.6 tensor(0.1243)
features.9.conv.0 tensor(0.0937)
features.9.conv.3 tensor(0.1418)
features.9.conv.6 tensor(0.1174)
features.10.conv.0 tensor(0.0517)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.0925)
features.11.conv.0 tensor(0.1532)
features.11.conv.3 tensor(0.1175)
features.11.conv.6 tensor(0.1856)
features.12.conv.0 tensor(0.1559)
features.12.conv.3 tensor(0.1535)
features.12.conv.6 tensor(0.2113)
features.13.conv.0 tensor(0.1379)
features.13.conv.3 tensor(0.1354)
features.13.conv.6 tensor(0.1183)
features.14.conv.0 tensor(0.7991)
features.14.conv.3 tensor(0.0973)
features.14.conv.6 tensor(0.6518)
features.15.conv.0 tensor(0.8693)
features.15.conv.3 tensor(0.0773)
features.15.conv.6 tensor(0.9097)
features.16.conv.0 tensor(0.0525)
features.16.conv.3 tensor(0.0920)
features.16.conv.6 tensor(0.1391)
conv.0 tensor(0.1524)
tensor(696598.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.86329585
0.86404371
0.86454493
0.86457521
0.86454660
0.86395520
0.86373371
0.86323059
0.86267358
0.86214322
0.86166590
0.86104381
0.86005157
0.85886180
0.85785991
0.85707223
INFO - Training [6][   20/  196]   Loss 0.585835   Top1 80.273438   Top5 97.324219   BatchTime 0.309581   LR 0.001655
0.85635382
0.85538304
0.85479188
0.85482252
0.85462558
0.85435438
0.85430914
0.85442853
0.85433209
0.85424387
0.85409677
0.85401368
0.85421437
0.85392302
0.85379511
0.85338247
0.85307539
0.85289502
0.85251063
0.85198730
0.85150218
0.85110337
0.85099846
0.85091442
INFO - Training [6][   40/  196]   Loss 0.571363   Top1 80.507812   Top5 97.666016   BatchTime 0.277174   LR 0.001580
0.85094011
0.85087138
0.85096085
0.85124379
0.85131508
0.85140800
0.85157281
0.85133618
0.85113013
0.85101038
0.84982651
0.84850389
0.85005975
0.85169083
0.85159618
0.85156769
INFO - Training [6][   60/  196]   Loss 0.572151   Top1 80.319010   Top5 97.766927   BatchTime 0.266862   LR 0.001506
0.85162902
0.85148913
0.85140723
0.85136533
0.85121095
0.85124052
0.85133797
0.85151774
0.85159701
0.85149306
0.85123038
0.85078341
0.85025460
0.85004145
0.84996265
0.85003728
0.85028571
0.85020584
0.85025841
0.85034543
0.85055637
0.85024869
0.84994411
0.84960210
0.84986269
INFO - Training [6][   80/  196]   Loss 0.567867   Top1 80.537109   Top5 97.812500   BatchTime 0.262184   LR 0.001432
0.85086709
0.85097271
0.85098916
0.85089391
0.85093760
0.85085493
0.85082924
0.85067087
0.85056740
0.85064471
0.85063386
0.85081071
0.85091168
0.85076803
INFO - Training [6][  100/  196]   Loss 0.558470   Top1 80.867188   Top5 97.824219   BatchTime 0.263803   LR 0.001360
0.85088140
0.85110062
0.85113424
0.85109097
0.85100323
0.85095525
0.85094750
0.85106122
0.85108519
0.85102904
0.85091043
0.85086888
0.85047662
0.85101497
0.85152370
0.85184723
0.85192990
0.85196590
0.85175943
0.85145062
0.85157269
0.85168582
0.85119104
INFO - Training [6][  120/  196]   Loss 0.552768   Top1 81.123047   Top5 97.913411   BatchTime 0.263989   LR 0.001289
0.85083002
0.85075021
0.85036832
0.84984499
0.84948170
0.84958613
0.84943628
0.84942245
0.84959191
0.84946418
0.84930724
0.84918463
0.84916502
0.84876794
0.84845692
0.84795868
0.84801048
INFO - Training [6][  140/  196]   Loss 0.550563   Top1 81.303013   Top5 97.965960   BatchTime 0.260873   LR 0.001220
0.84788108
0.84787995
0.84800708
0.84775305
0.84768760
0.84769773
0.84776103
0.84770507
0.84784114
0.84771013
0.84784466
0.84913737
0.84925145
0.84927505
0.84931350
0.84917092
0.84887481
0.84889352
0.84846461
0.84826666
0.84806418
0.84785843
0.84792149
INFO - Training [6][  160/  196]   Loss 0.550313   Top1 81.286621   Top5 97.949219   BatchTime 0.260771   LR 0.001151
0.84792429
0.84788239
0.84773254
0.84754562
0.84723634
0.84677714
0.84675092
0.84624916
0.84583288
0.84529430
0.84537113
0.84553528
0.84561020
0.84546667
0.84565532
0.84549069
INFO - Training [6][  180/  196]   Loss 0.547824   Top1 81.304253   Top5 97.927517   BatchTime 0.259733   LR 0.001084
0.84532052
0.84545273
0.84513843
0.84499365
0.84498459
0.84493202
0.84469110
0.84413248
0.84383625
0.84360862
0.84371114
0.84331495
0.84244937
0.84164774
0.84162498
0.84182984
0.84123677
0.84128875
0.84044415
0.83949310
0.83824652
0.83720320
INFO - ==> Top1: 81.350    Top5: 97.950    Loss: 0.546
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [6][   20/   40]   Loss 0.500521   Top1 83.144531   Top5 99.140625   BatchTime 0.118164
INFO - Validation [6][   40/   40]   Loss 0.491801   Top1 83.520000   Top5 99.130000   BatchTime 0.086096
INFO - ==> Top1: 83.520    Top5: 99.130    Loss: 0.492
INFO - ==> Sparsity : 0.349
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 83.790   Top5: 99.230]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 83.730   Top5: 99.340]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 83.520   Top5: 99.130]
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.2188)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0681)
features.2.conv.0 tensor(0.0341)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0932)
features.3.conv.0 tensor(0.0286)
features.3.conv.3 tensor(0.0594)
features.3.conv.6 tensor(0.0645)
features.4.conv.0 tensor(0.0412)
features.4.conv.3 tensor(0.1082)
features.4.conv.6 tensor(0.0986)
features.5.conv.0 tensor(0.0345)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1086)
features.6.conv.0 tensor(0.0286)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0868)
features.7.conv.0 tensor(0.0719)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.1061)
features.8.conv.0 tensor(0.0683)
features.8.conv.3 tensor(0.1097)
features.8.conv.6 tensor(0.1202)
features.9.conv.0 tensor(0.0983)
features.9.conv.3 tensor(0.1424)
features.9.conv.6 tensor(0.1140)
features.10.conv.0 tensor(0.0543)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.0944)
features.11.conv.0 tensor(0.1648)
features.11.conv.3 tensor(0.1171)
features.11.conv.6 tensor(0.1836)
features.12.conv.0 tensor(0.1510)
features.12.conv.3 tensor(0.1532)
features.12.conv.6 tensor(0.5211)
features.13.conv.0 tensor(0.1526)
features.13.conv.3 tensor(0.1337)
features.13.conv.6 tensor(0.1209)
features.14.conv.0 tensor(0.8270)
features.14.conv.3 tensor(0.0969)
features.14.conv.6 tensor(0.9040)
features.15.conv.0 tensor(0.8785)
features.15.conv.3 tensor(0.0764)
features.15.conv.6 tensor(0.9208)
features.16.conv.0 tensor(0.0550)
features.16.conv.3 tensor(0.0899)
features.16.conv.6 tensor(0.1421)
conv.0 tensor(0.1541)
tensor(763398.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.83795917
0.83863157
0.83904946
0.83967292
0.83999264
0.83999878
0.84018570
0.84089369
0.84105617
0.84094840
0.84097153
0.84125239
0.84131014
0.84178191
0.84199399
0.84238982
0.84286481
0.84314781
0.84359491
INFO - Training [7][   20/  196]   Loss 0.525264   Top1 81.875000   Top5 97.929688   BatchTime 0.323965   LR 0.000969
0.84402186
0.84456897
0.84644336
0.84822172
0.84762776
0.84735656
0.84688133
0.84671932
0.84610826
0.84600329
0.84584963
0.84593678
0.84613365
0.84766865
0.84718031
INFO - Training [7][   40/  196]   Loss 0.528318   Top1 81.835938   Top5 97.910156   BatchTime 0.290653   LR 0.000907
0.84723550
0.84677303
0.84635979
0.84581912
0.84585094
0.84621501
0.84681314
0.84690344
0.84717762
0.84765452
0.84806418
0.84924239
0.84966457
0.84926546
0.84936857
0.84946597
0.84920704
0.84871012
0.84865397
0.84870011
0.84869891
0.84865922
0.84878886
0.84886718
0.84880525
0.84881085
INFO - Training [7][   60/  196]   Loss 0.529241   Top1 81.855469   Top5 97.923177   BatchTime 0.270693   LR 0.000845
0.84894395
0.84891987
0.84877032
0.84859049
0.84877968
0.84883273
0.84885317
0.84864515
0.84838617
0.84839696
0.84845150
0.84847647
0.84845078
0.84847653
0.84852153
0.84828782
INFO - Training [7][   80/  196]   Loss 0.519690   Top1 82.211914   Top5 98.056641   BatchTime 0.262179   LR 0.000786
0.84846866
0.84827912
0.84808934
0.84778851
0.84767812
0.84757954
0.84758586
0.84759367
0.84754467
0.84740078
0.84740478
0.84733129
0.84735262
0.84737134
0.84710419
0.84717143
0.84739345
0.84734428
INFO - Training [7][  100/  196]   Loss 0.515944   Top1 82.320312   Top5 98.113281   BatchTime 0.257981   LR 0.000728
0.84716362
0.84707391
0.84711468
0.84697336
0.84690267
0.84705657
0.84703422
0.84694344
0.84675080
0.84678799
0.84697008
0.84676909
0.84666592
0.84672672
0.84688902
0.84701031
0.84715050
0.84714973
0.84700364
0.84688985
0.84693003
0.84689671
0.84719837
0.84670365
0.84679908
0.84687126
INFO - Training [7][  120/  196]   Loss 0.509447   Top1 82.500000   Top5 98.186849   BatchTime 0.269769   LR 0.000673
0.84671408
0.84665447
0.84646785
0.84632009
0.84649134
0.84651780
0.84651166
0.84660107
0.84660870
0.84629405
0.84628707
0.84625143
0.84615195
0.84606004
0.84592491
0.84599811
0.84599453
0.84586930
INFO - Training [7][  140/  196]   Loss 0.507897   Top1 82.502790   Top5 98.219866   BatchTime 0.278612   LR 0.000619
0.84565896
0.84564131
0.84575558
0.84585202
0.84573066
0.84556758
0.84531605
0.84495151
0.84472299
0.84445661
0.84477705
0.84515941
0.84505820
0.84483123
0.84474957
0.84441316
0.84433210
0.84450877
0.84458989
0.84441692
0.84424824
INFO - Training [7][  160/  196]   Loss 0.511793   Top1 82.353516   Top5 98.220215   BatchTime 0.279010   LR 0.000567
0.84425139
0.84425747
0.84415489
0.84387469
0.84364122
0.84359854
0.84312236
0.84240061
0.84184504
0.84145486
0.84092093
0.84067458
0.84022009
0.83988887
0.83939326
INFO - Training [7][  180/  196]   Loss 0.512673   Top1 82.343750   Top5 98.144531   BatchTime 0.276001   LR 0.000517
0.83925641
0.83893681
0.83855110
0.83849454
0.83832335
0.83827072
0.83838862
0.83838296
0.83795625
0.83790153
0.83739203
0.83716518
0.83712804
0.83736271
0.83760977
0.83828783
INFO - ==> Top1: 82.408    Top5: 98.140    Loss: 0.511
0.83917391
0.83926046
0.83888656
0.83851922
0.83836174
0.83813149
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.404024   Top1 86.367188   Top5 99.335938   BatchTime 0.119095
INFO - Validation [7][   40/   40]   Loss 0.393438   Top1 86.630000   Top5 99.490000   BatchTime 0.086216
INFO - ==> Top1: 86.630    Top5: 99.490    Loss: 0.393
INFO - ==> Sparsity : 0.349
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 83.790   Top5: 99.230]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 83.730   Top5: 99.340]
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0734)
features.2.conv.0 tensor(0.0376)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0943)
features.3.conv.0 tensor(0.0237)
features.3.conv.3 tensor(0.0602)
features.3.conv.6 tensor(0.0634)
features.4.conv.0 tensor(0.0399)
features.4.conv.3 tensor(0.1047)
features.4.conv.6 tensor(0.0972)
features.5.conv.0 tensor(0.0361)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.1139)
features.6.conv.0 tensor(0.0301)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0857)
features.7.conv.0 tensor(0.0730)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1070)
features.8.conv.0 tensor(0.0717)
features.8.conv.3 tensor(0.1100)
features.8.conv.6 tensor(0.1278)
features.9.conv.0 tensor(0.0999)
features.9.conv.3 tensor(0.1418)
features.9.conv.6 tensor(0.1120)
features.10.conv.0 tensor(0.0511)
features.10.conv.3 tensor(0.0917)
features.10.conv.6 tensor(0.0935)
features.11.conv.0 tensor(0.1764)
features.11.conv.3 tensor(0.1171)
features.11.conv.6 tensor(0.3711)
features.12.conv.0 tensor(0.1537)
features.12.conv.3 tensor(0.1534)
features.12.conv.6 tensor(0.2131)
features.13.conv.0 tensor(0.1557)
features.13.conv.3 tensor(0.1321)
features.13.conv.6 tensor(0.1248)
features.14.conv.0 tensor(0.8400)
features.14.conv.3 tensor(0.0951)
features.14.conv.6 tensor(0.9103)
features.15.conv.0 tensor(0.8828)
features.15.conv.3 tensor(0.0757)
features.15.conv.6 tensor(0.9282)
features.16.conv.0 tensor(0.0723)
features.16.conv.3 tensor(0.0914)
features.16.conv.6 tensor(0.1428)
conv.0 tensor(0.1504)
tensor(764358.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.83766568
0.83749038
0.83714211
0.83697826
0.83689570
0.83670503
0.83652061
0.83636421
0.83649182
0.83640593
0.83618271
0.83585382
0.83546537
0.83497387
0.83449918
0.83435857
0.83409286
0.83346832
0.83220845
0.83157903
INFO - Training [8][   20/  196]   Loss 0.499148   Top1 82.539062   Top5 97.460938   BatchTime 0.315325   LR 0.000434
0.83111966
0.83117861
0.83085579
0.83045113
0.83003461
0.82994944
0.82987326
0.83136994
0.83130121
0.83102095
0.83104742
0.83063585
0.83034790
0.83003348
0.83012617
0.83026093
0.83026505
0.83031619
0.83015084
INFO - Training [8][   40/  196]   Loss 0.513820   Top1 82.363281   Top5 97.578125   BatchTime 0.314822   LR 0.000389
0.83039391
0.83073759
0.83104110
0.83120781
0.83121765
0.83147645
0.83150572
0.83172572
0.83205694
0.83232927
0.83252859
0.83272767
0.83266497
0.83250159
INFO - Training [8][   60/  196]   Loss 0.509363   Top1 82.421875   Top5 97.773438   BatchTime 0.297831   LR 0.000347
0.83222997
0.83186752
0.83174562
0.83175492
0.83193052
0.83202648
0.83200961
0.83221680
0.83238292
0.83238465
0.83238858
0.83249700
0.83257228
0.83263510
0.83275491
0.83367229
0.83441120
0.83451140
0.83451408
0.83435380
0.83429676
0.83429086
0.83426589
0.83422321
0.83426499
INFO - Training [8][   80/  196]   Loss 0.508473   Top1 82.543945   Top5 97.871094   BatchTime 0.285569   LR 0.000308
0.83608592
0.83625245
0.83629048
0.83630240
0.83628666
0.83635575
0.83632183
0.83619088
0.83622617
0.83622640
0.83617437
0.83614039
0.83607727
0.83605248
0.83604729
INFO - Training [8][  100/  196]   Loss 0.501087   Top1 82.734375   Top5 97.964844   BatchTime 0.278359   LR 0.000270
0.83605504
0.83604336
0.83602011
0.83594537
0.83592600
0.83591038
0.83598733
0.83585930
0.83587980
0.83588612
0.83573478
0.83572972
0.83554482
0.83509129
0.83448368
0.83413178
0.83399111
0.83400685
0.83403271
0.83404368
0.83400321
0.83405846
0.83395231
0.83379382
0.83369327
INFO - Training [8][  120/  196]   Loss 0.493927   Top1 83.030599   Top5 98.079427   BatchTime 0.273055   LR 0.000235
0.83347183
0.83320403
0.83285141
0.83270985
0.83259642
0.83232105
0.83213282
0.83205688
0.83181918
0.83173949
0.83169329
0.83159232
0.83168018
0.83173919
0.83165908
0.83165294
0.83162880
0.83169478
0.83168954
0.83168405
0.83174628
0.83178031
0.83175594
INFO - Training [8][  140/  196]   Loss 0.491033   Top1 83.155692   Top5 98.166853   BatchTime 0.270781   LR 0.000202
0.83175439
0.83173370
0.83174187
0.83169508
0.83160567
0.83147305
0.83151531
0.83150256
0.83134013
0.83135200
0.83129442
0.83127201
0.83122337
0.83122051
0.83119184
0.83109307
INFO - Training [8][  160/  196]   Loss 0.491741   Top1 83.083496   Top5 98.173828   BatchTime 0.269704   LR 0.000172
0.83106780
0.83093953
0.83069170
0.83034676
0.83020478
0.83017045
0.82999086
0.82992065
0.82993603
0.82994235
0.82992631
0.82992560
0.83152604
0.83104181
0.83107942
0.83123982
0.83119762
0.83119452
0.83133703
0.83132219
0.83117563
0.83106232
0.83099097
INFO - Training [8][  180/  196]   Loss 0.489752   Top1 83.161892   Top5 98.153212   BatchTime 0.268526   LR 0.000143
0.83098751
0.83099174
0.83110720
0.83123910
0.83121490
0.83116680
0.83111620
0.83113277
0.83112293
0.83108747
0.83107656
0.83110291
0.83107907
0.83097494
0.83093083
0.83093208
INFO - ==> Top1: 83.284    Top5: 98.166    Loss: 0.486
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.396920   Top1 86.601562   Top5 99.316406   BatchTime 0.118122
INFO - Validation [8][   40/   40]   Loss 0.382463   Top1 86.780000   Top5 99.490000   BatchTime 0.087179
INFO - ==> Top1: 86.780    Top5: 99.490    Loss: 0.382
INFO - ==> Sparsity : 0.351
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 83.790   Top5: 99.230]
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0280)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0694)
features.2.conv.0 tensor(0.0350)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0935)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0629)
features.4.conv.0 tensor(0.0426)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.0983)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1151)
features.6.conv.0 tensor(0.0290)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0829)
features.7.conv.0 tensor(0.0757)
features.7.conv.3 tensor(0.1097)
features.7.conv.6 tensor(0.1061)
features.8.conv.0 tensor(0.0714)
features.8.conv.3 tensor(0.1100)
features.8.conv.6 tensor(0.1278)
features.9.conv.0 tensor(0.0989)
features.9.conv.3 tensor(0.1398)
features.9.conv.6 tensor(0.1106)
features.10.conv.0 tensor(0.0487)
features.10.conv.3 tensor(0.0911)
features.10.conv.6 tensor(0.1026)
features.11.conv.0 tensor(0.1823)
features.11.conv.3 tensor(0.1161)
features.11.conv.6 tensor(0.3501)
features.12.conv.0 tensor(0.1672)
features.12.conv.3 tensor(0.1528)
features.12.conv.6 tensor(0.2408)
features.13.conv.0 tensor(0.1562)
features.13.conv.3 tensor(0.1321)
features.13.conv.6 tensor(0.1235)
features.14.conv.0 tensor(0.8501)
features.14.conv.3 tensor(0.0959)
features.14.conv.6 tensor(0.9122)
features.15.conv.0 tensor(0.8851)
features.15.conv.3 tensor(0.0750)
features.15.conv.6 tensor(0.9308)
features.16.conv.0 tensor(0.0748)
features.16.conv.3 tensor(0.0904)
features.16.conv.6 tensor(0.1418)
conv.0 tensor(0.1490)
tensor(767988.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.83120382
0.83261287
0.83250415
0.83248192
0.83249283
0.83249241
0.83243370
0.83245015
0.83231252
0.83227348
0.83223289
0.83207792
0.83205485
0.83198911
0.83183426
INFO - Training [9][   20/  196]   Loss 0.484993   Top1 83.554688   Top5 97.695312   BatchTime 0.320296   LR 0.000100
0.83175236
0.83171636
0.83170402
0.83168626
0.83164752
0.83145458
0.83126193
0.83103079
0.83076149
0.83075321
0.83062255
0.83050203
0.83046228
0.83037353
0.83033973
0.83035648
0.83029598
0.83027601
0.83032060
0.83023632
0.83017337
0.83020085
0.83023000
0.83020073
INFO - Training [9][   40/  196]   Loss 0.486095   Top1 83.320312   Top5 97.939453   BatchTime 0.288209   LR 0.000079
0.83010632
0.83009636
0.83008128
0.83011603
0.83013189
0.83005941
0.83004946
0.83005881
0.83011025
0.83003318
0.83002186
0.83000767
0.82999814
0.83005345
0.83007395
0.83004338
INFO - Training [9][   60/  196]   Loss 0.481497   Top1 83.326823   Top5 98.111979   BatchTime 0.275509   LR 0.000060
0.83010352
0.83011097
0.83007437
0.83008987
0.83006805
0.83004624
0.82999474
0.82993752
0.82990712
0.82981378
0.82976466
0.82967049
0.82946944
0.82925975
0.82926923
0.82906628
0.82884115
0.82868433
0.82858700
0.82851356
0.82849503
0.82848686
0.82849658
INFO - Training [9][   80/  196]   Loss 0.481256   Top1 83.408203   Top5 98.154297   BatchTime 0.272579   LR 0.000044
0.82846659
0.82845092
0.82844239
0.82838553
0.82837653
0.82838547
0.82837409
0.82838517
0.82838690
0.82835650
0.82835913
0.82838404
0.82839024
0.82836604
0.82838166
0.82841146
0.82843369
0.82846946
0.82852161
0.82867295
0.82883912
0.82881445
0.82881379
INFO - Training [9][  100/  196]   Loss 0.472542   Top1 83.609375   Top5 98.265625   BatchTime 0.270236   LR 0.000030
0.82873291
0.82868534
0.82865602
0.82865143
0.82869983
0.82873595
0.82871115
0.82871765
0.82874829
0.82865286
0.82863945
0.82865572
0.82852197
0.82851416
0.82849658
0.82845461
INFO - Training [9][  120/  196]   Loss 0.469132   Top1 83.759766   Top5 98.310547   BatchTime 0.266858   LR 0.000019
0.82838082
0.82825261
0.82817322
0.82809216
0.82795566
0.82767600
0.82743633
0.82729745
0.82725680
0.82724309
0.82723206
0.82723057
0.82721657
0.82718045
0.82719839
0.82715464
INFO - Training [9][  140/  196]   Loss 0.469205   Top1 83.791853   Top5 98.373326   BatchTime 0.262670   LR 0.000010
0.82710850
0.82710439
0.82708853
0.82710117
0.82707453
0.82707745
0.82706821
0.82707018
0.82707840
0.82707262
0.82707179
0.82704544
0.82701695
0.82703120
0.82704955
0.82703358
0.82701796
0.82701504
0.82702309
0.82701248
0.82702857
0.82703078
0.82701546
0.82701606
INFO - Training [9][  160/  196]   Loss 0.470488   Top1 83.764648   Top5 98.354492   BatchTime 0.262232   LR 0.000004
0.82701904
0.82700503
0.82700336
0.82701242
0.82699913
0.82701546
0.82701641
0.82699776
0.82700855
0.82700932
0.82702613
0.82701641
0.82702267
0.82700688
0.82702112
0.82702684
0.82701200
0.82701468
0.82700819
0.82701200
0.82701695
0.82701647
0.82700562
0.82702929
INFO - Training [9][  180/  196]   Loss 0.471339   Top1 83.723958   Top5 98.292101   BatchTime 0.261245   LR 0.000001
0.82701409
0.82700396
0.82701302
0.82699263
0.82700378
0.82700789
0.82699925
0.82700342
0.82700729
0.82700008
0.82700151
0.82698703
0.82700729
0.82699102
0.82697922
INFO - ==> Top1: 83.730    Top5: 98.278    Loss: 0.471
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.415014   Top1 86.406250   Top5 99.472656   BatchTime 0.124063
INFO - Validation [9][   40/   40]   Loss 0.399142   Top1 86.990000   Top5 99.570000   BatchTime 0.088913
INFO - ==> Top1: 86.990    Top5: 99.570    Loss: 0.399
INFO - ==> Sparsity : 0.353
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0273)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0690)
features.2.conv.0 tensor(0.0370)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0940)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0627)
features.4.conv.0 tensor(0.0422)
features.4.conv.3 tensor(0.1047)
features.4.conv.6 tensor(0.0983)
features.5.conv.0 tensor(0.0360)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.1146)
features.6.conv.0 tensor(0.0282)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0829)
features.7.conv.0 tensor(0.0758)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1075)
features.8.conv.0 tensor(0.0713)
features.8.conv.3 tensor(0.1111)
features.8.conv.6 tensor(0.1324)
features.9.conv.0 tensor(0.0985)
features.9.conv.3 tensor(0.1392)
features.9.conv.6 tensor(0.1156)
features.10.conv.0 tensor(0.0487)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.1117)
features.11.conv.0 tensor(0.1824)
features.11.conv.3 tensor(0.1161)
features.11.conv.6 tensor(0.3559)
features.12.conv.0 tensor(0.1715)
features.12.conv.3 tensor(0.1520)
features.12.conv.6 tensor(0.2505)
features.13.conv.0 tensor(0.1563)
features.13.conv.3 tensor(0.1323)
features.13.conv.6 tensor(0.1247)
features.14.conv.0 tensor(0.8516)
features.14.conv.3 tensor(0.0954)
features.14.conv.6 tensor(0.9191)
features.15.conv.0 tensor(0.8858)
features.15.conv.3 tensor(0.0752)
features.15.conv.6 tensor(0.9327)
features.16.conv.0 tensor(0.0763)
features.16.conv.3 tensor(0.0903)
features.16.conv.6 tensor(0.1417)
conv.0 tensor(0.1492)
tensor(771758.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.82702070
0.83186030
0.83240587
0.83220059
0.83185548
0.83704478
0.83860171
0.83768797
0.83764285
0.83842188
0.83925301
0.83875161
0.83955020
0.84137654
0.84156746
0.84177142
0.84172332
INFO - Training [10][   20/  196]   Loss 0.560522   Top1 81.054688   Top5 97.421875   BatchTime 0.332371   LR 0.002500
0.84180790
0.84169990
0.84276158
0.84267962
0.84284234
0.84288269
0.84297901
0.84323096
0.84381104
0.84508640
0.84836864
0.84879088
0.84878683
0.84884834
0.84914082
0.84946525
0.85007477
0.85276955
0.85266387
0.85278982
0.85294127
0.85329866
0.85343325
INFO - Training [10][   40/  196]   Loss 0.558296   Top1 81.240234   Top5 97.646484   BatchTime 0.296818   LR 0.002499
0.85365844
0.85358351
0.85349882
0.85351509
0.85332006
0.85326356
0.85338527
0.85376662
0.85423160
0.85465115
0.85524523
0.85650235
0.85787928
0.85982949
0.86162388
0.86348873
0.86550164
0.86772084
0.86914688
0.87016535
INFO - Training [10][   60/  196]   Loss 0.562582   Top1 80.970052   Top5 97.753906   BatchTime 0.297132   LR 0.002499
0.87092417
0.87166828
0.87229252
0.87214583
0.87190288
0.87158316
0.87124884
0.87081623
0.87061667
0.87034553
0.87002587
0.86958528
0.86880070
0.86785328
0.86649549
INFO - Training [10][   80/  196]   Loss 0.567812   Top1 80.864258   Top5 97.846680   BatchTime 0.289490   LR 0.002497
0.86484289
0.86307794
0.86170906
0.85958248
0.85814184
0.85704356
0.85550457
0.85403115
0.85278177
0.85141498
0.84993386
0.84870285
0.84828329
0.84784085
0.84705794
0.84663367
0.84653795
0.84648663
0.84647810
0.84690177
0.84724927
0.84695089
0.84668970
INFO - Training [10][  100/  196]   Loss 0.559620   Top1 81.027344   Top5 97.902344   BatchTime 0.285670   LR 0.002496
0.84647191
0.84648210
0.84611779
0.84521353
0.84509522
0.84494019
0.84510869
0.84586549
0.84637487
0.84734583
0.84913778
0.84945196
0.84976321
0.84923941
0.84919041
0.84922069
0.84952128
0.84992987
0.85011536
0.85093367
0.85156661
0.85223705
0.85302496
INFO - Training [10][  120/  196]   Loss 0.556020   Top1 81.175130   Top5 98.017578   BatchTime 0.280112   LR 0.002494
0.85349357
0.85376692
0.85376334
0.85377967
0.85383976
0.85383213
0.85395890
0.85419047
0.85416543
0.85391158
0.85379392
0.85302591
0.85232222
0.85198319
0.85157454
0.85114342
0.85075355
INFO - Training [10][  140/  196]   Loss 0.555836   Top1 81.118862   Top5 98.038504   BatchTime 0.275236   LR 0.002492
0.84994102
0.84989411
0.84985977
0.84987754
0.84994394
0.85009414
0.84964299
0.84930319
0.84923208
0.84936702
0.84943527
0.84937286
0.84933561
0.84954518
0.84954315
0.84935087
0.84934247
0.84908992
0.84895176
0.84919071
0.84909779
0.84926271
0.84898418
INFO - Training [10][  160/  196]   Loss 0.560209   Top1 80.988770   Top5 97.978516   BatchTime 0.273797   LR 0.002490
0.84876466
0.84871280
0.84852391
0.84838563
0.84834296
0.84820777
0.84802788
0.84813410
0.84832507
0.84850842
0.84868646
0.84894127
0.84899557
0.84892285
0.84876388
INFO - Training [10][  180/  196]   Loss 0.561165   Top1 80.935330   Top5 97.929688   BatchTime 0.272909   LR 0.002487
0.84893107
0.84917283
0.84950531
0.84975600
0.85017347
0.85055119
0.85094720
0.85058588
0.85039997
0.85061353
0.85098487
0.85115546
0.85115176
0.85195380
0.85231221
0.85280538
0.85277826
0.85266501
0.85224509
0.85210937
********************pre-trained*****************
INFO - ==> Top1: 80.968    Top5: 97.918    Loss: 0.561
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.494653   Top1 82.832031   Top5 98.867188   BatchTime 0.223009
INFO - Validation [10][   40/   40]   Loss 0.494459   Top1 83.060000   Top5 98.920000   BatchTime 0.140331
INFO - ==> Top1: 83.060    Top5: 98.920    Loss: 0.494
INFO - ==> Sparsity : 0.333
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5382)
features.0.conv.3 tensor(0.2148)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0699)
features.2.conv.0 tensor(0.0367)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0926)
features.3.conv.0 tensor(0.0275)
features.3.conv.3 tensor(0.0594)
features.3.conv.6 tensor(0.0658)
features.4.conv.0 tensor(0.0444)
features.4.conv.3 tensor(0.1059)
features.4.conv.6 tensor(0.0951)
features.5.conv.0 tensor(0.0343)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1082)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0789)
features.7.conv.0 tensor(0.0992)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.1097)
features.8.conv.0 tensor(0.0573)
features.8.conv.3 tensor(0.1172)
features.8.conv.6 tensor(0.1278)
features.9.conv.0 tensor(0.0918)
features.9.conv.3 tensor(0.1467)
features.9.conv.6 tensor(0.1101)
features.10.conv.0 tensor(0.0393)
features.10.conv.3 tensor(0.0874)
features.10.conv.6 tensor(0.0878)
features.11.conv.0 tensor(0.1310)
features.11.conv.3 tensor(0.1165)
features.11.conv.6 tensor(0.1802)
features.12.conv.0 tensor(0.1241)
features.12.conv.3 tensor(0.1505)
features.12.conv.6 tensor(0.2076)
features.13.conv.0 tensor(0.0744)
features.13.conv.3 tensor(0.1304)
features.13.conv.6 tensor(0.1040)
features.14.conv.0 tensor(0.8915)
features.14.conv.3 tensor(0.0947)
features.14.conv.6 tensor(0.7835)
features.15.conv.0 tensor(0.8908)
features.15.conv.3 tensor(0.0756)
features.15.conv.6 tensor(0.8922)
features.16.conv.0 tensor(0.0636)
features.16.conv.3 tensor(0.0890)
features.16.conv.6 tensor(0.1708)
conv.0 tensor(0.1375)
tensor(728486.) 2188896.0
0.85211837
0.85178614
0.85128653
0.85104614
0.85077912
0.85023206
0.84986222
0.84943718
0.84904033
0.84881628
0.84864479
0.84856755
0.84853822
0.84808910
0.84768766
0.84757841
0.84736621
0.84690571
0.84635961
INFO - Training [11][   20/  196]   Loss 0.577077   Top1 80.058594   Top5 97.695312   BatchTime 0.342453   LR 0.002481
0.84614307
0.84599352
0.84576041
0.84577841
0.84584957
0.84573025
0.84577930
0.84582072
0.84602350
0.84598243
0.84568799
0.84573483
0.84587038
0.84567696
0.84575099
0.84584004
INFO - Training [11][   40/  196]   Loss 0.580554   Top1 80.302734   Top5 97.626953   BatchTime 0.293796   LR 0.002478
0.84549069
0.84551609
0.84530342
0.84518325
0.84520596
0.84476769
0.84471130
0.84460872
0.84452057
0.84478360
0.84454495
0.84442824
0.84436405
0.84426785
0.84411865
0.84344512
0.84308106
0.84302747
0.84248799
0.84225243
0.84193432
0.84162432
0.84112662
INFO - Training [11][   60/  196]   Loss 0.577353   Top1 80.338542   Top5 97.740885   BatchTime 0.283299   LR 0.002474
0.84071678
0.83967185
0.83841044
0.83796000
0.83791834
0.83639377
0.83512586
0.83584595
0.83672643
0.83664471
0.83639818
0.83773637
0.83663088
0.83604252
0.83680683
0.83697164
INFO - Training [11][   80/  196]   Loss 0.575965   Top1 80.498047   Top5 97.783203   BatchTime 0.275852   LR 0.002470
0.83743370
0.83750564
0.83823305
0.83728272
0.83731568
0.83804947
0.83787590
0.83783948
0.83807766
0.83922273
0.84033936
0.84149528
0.84270674
0.84323990
0.84352046
0.84376103
0.84451318
0.84519434
0.84555906
0.84570616
0.84665346
0.84849501
0.84841943
INFO - Training [11][  100/  196]   Loss 0.574983   Top1 80.570312   Top5 97.808594   BatchTime 0.273229   LR 0.002465
0.84856224
0.84879708
0.84918028
0.84898913
0.84852052
0.84846532
0.84952426
0.84947920
0.84917706
0.84880489
0.84875369
0.84861451
0.84840572
0.84824437
0.84802991
0.84792572
0.84770608
0.84750730
0.84738755
0.84751976
0.84749705
0.84714901
0.84709805
INFO - Training [11][  120/  196]   Loss 0.570792   Top1 80.781250   Top5 97.832031   BatchTime 0.270595   LR 0.002460
0.84701020
0.84704995
0.84699571
0.84670818
0.84609246
0.84538305
0.84495890
0.84424382
0.84330493
0.84277093
0.84241539
0.84226590
0.84257466
0.84370881
0.84202605
0.84074426
INFO - Training [11][  140/  196]   Loss 0.572926   Top1 80.616629   Top5 97.879464   BatchTime 0.268521   LR 0.002455
0.84275258
0.84200782
0.84373909
0.84465516
0.84577912
0.84635979
0.84800035
0.85017955
0.85122097
0.85088891
0.85031599
0.84993523
0.84962612
0.84924287
0.84909034
0.84888619
0.84856069
0.84846574
0.84819961
0.84776908
0.84754783
0.84699476
0.84699500
0.84708673
INFO - Training [11][  160/  196]   Loss 0.575739   Top1 80.583496   Top5 97.890625   BatchTime 0.265979   LR 0.002450
0.84700447
0.84715813
0.84763771
0.84770900
0.84771675
0.84798342
0.84814751
0.84811521
0.84762508
0.84747696
0.84765440
0.84771401
0.84778303
0.84780067
0.84798276
0.84801888
0.84792131
0.84803319
0.84823292
0.84848392
0.84885776
INFO - Training [11][  180/  196]   Loss 0.574000   Top1 80.551215   Top5 97.834201   BatchTime 0.268343   LR 0.002444
0.84925318
0.84938568
0.84939104
0.84954292
0.84963310
0.84971201
0.84767491
0.84017658
0.84750986
0.85099989
0.85159308
0.85138285
0.85121071
0.85119456
0.85120332
INFO - ==> Top1: 80.578    Top5: 97.836    Loss: 0.574
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.484955   Top1 83.613281   Top5 98.945312   BatchTime 0.120249
INFO - Validation [11][   40/   40]   Loss 0.469972   Top1 84.090000   Top5 99.120000   BatchTime 0.088412
INFO - ==> Top1: 84.090    Top5: 99.120    Loss: 0.470
INFO - ==> Sparsity : 0.339
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.2227)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0301)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0894)
features.3.conv.0 tensor(0.0220)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0586)
features.4.conv.0 tensor(0.0379)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0959)
features.5.conv.0 tensor(0.0368)
features.5.conv.3 tensor(0.0856)
features.5.conv.6 tensor(0.1102)
features.6.conv.0 tensor(0.0285)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0755)
features.7.conv.0 tensor(0.0787)
features.7.conv.3 tensor(0.1126)
features.7.conv.6 tensor(0.1098)
features.8.conv.0 tensor(0.0516)
features.8.conv.3 tensor(0.1149)
features.8.conv.6 tensor(0.1335)
features.9.conv.0 tensor(0.0733)
features.9.conv.3 tensor(0.1496)
features.9.conv.6 tensor(0.1101)
features.10.conv.0 tensor(0.0431)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0847)
features.11.conv.0 tensor(0.1390)
features.11.conv.3 tensor(0.1202)
features.11.conv.6 tensor(0.1728)
features.12.conv.0 tensor(0.1413)
features.12.conv.3 tensor(0.1557)
features.12.conv.6 tensor(0.2389)
features.13.conv.0 tensor(0.1272)
features.13.conv.3 tensor(0.1331)
features.13.conv.6 tensor(0.1051)
features.14.conv.0 tensor(0.8928)
features.14.conv.3 tensor(0.0972)
features.14.conv.6 tensor(0.8196)
features.15.conv.0 tensor(0.9019)
features.15.conv.3 tensor(0.0766)
features.15.conv.6 tensor(0.9112)
features.16.conv.0 tensor(0.0541)
features.16.conv.3 tensor(0.0938)
features.16.conv.6 tensor(0.1644)
conv.0 tensor(0.1400)
tensor(741242.) 2188896.0
0.85078084
0.85063058
0.85033977
0.85008579
0.84975547
0.84943116
0.84932584
0.84912544
0.84876400
0.84847045
0.84826034
0.84806746
0.84781671
0.84771574
0.84731787
0.84719443
0.84709173
0.84697598
0.84698445
0.84693152
INFO - Training [12][   20/  196]   Loss 0.573881   Top1 80.371094   Top5 97.558594   BatchTime 0.327469   LR 0.002433
0.84694815
0.84724790
0.84678954
0.84685689
0.84679276
0.84675699
0.84689802
0.84694111
0.84702110
0.84726149
0.84738916
0.84765267
0.84729993
0.84738374
0.84738350
INFO - Training [12][   40/  196]   Loss 0.579554   Top1 80.185547   Top5 97.773438   BatchTime 0.296391   LR 0.002426
0.84718299
0.84706962
0.84693271
0.84678769
0.84698439
0.84724194
0.84736508
0.84792238
0.84810090
0.84813654
0.84828806
0.84846652
0.84866244
0.84849191
0.84872490
0.84848678
0.84853148
0.84826756
0.84825695
0.84811223
0.84764111
0.84721720
0.84668076
0.84626514
0.84568900
INFO - Training [12][   60/  196]   Loss 0.567968   Top1 80.520833   Top5 97.825521   BatchTime 0.276699   LR 0.002419
0.84523952
0.84506857
0.84493488
0.84485650
0.84488070
0.84476125
0.84446400
0.84433419
0.84414005
0.84376484
0.84374702
0.84360892
0.84348851
0.84323382
0.84291148
0.84225243
0.84197742
0.84177762
0.84130073
0.84096134
INFO - Training [12][   80/  196]   Loss 0.563385   Top1 80.761719   Top5 97.915039   BatchTime 0.282565   LR 0.002412
0.84089619
0.84075010
0.84056729
0.84076095
0.84218788
0.84206289
0.84209991
0.84203053
0.84202868
0.84188586
0.84176379
0.84147632
0.84142125
0.84139931
0.84126139
INFO - Training [12][  100/  196]   Loss 0.557389   Top1 81.035156   Top5 97.925781   BatchTime 0.280984   LR 0.002404
0.84124362
0.84124440
0.84084731
0.84037685
0.83972150
0.84011871
0.83983654
0.83956403
0.83882290
0.83911449
0.83959919
0.84117866
0.84186345
0.84218597
0.84264749
0.84325844
0.84430319
0.84648860
0.84833854
0.84991682
0.85385621
0.85606396
0.85603029
INFO - Training [12][  120/  196]   Loss 0.547802   Top1 81.357422   Top5 97.998047   BatchTime 0.276513   LR 0.002396
0.85568827
0.85516018
0.85434252
0.85384077
0.85303694
0.85251522
0.85160249
0.85106868
0.85052073
0.84971595
0.84948546
0.84915882
0.84898818
0.84864604
0.84864783
0.84866941
0.84844792
0.84806478
0.84781349
0.84754866
0.84713322
INFO - Training [12][  140/  196]   Loss 0.546403   Top1 81.431362   Top5 98.044085   BatchTime 0.276805   LR 0.002388
0.84655428
0.84607321
0.84579039
0.84491503
0.84457970
0.84386498
0.84346700
0.84288108
0.84157097
0.83958000
0.83896881
0.84345663
0.84436738
0.84611332
0.84761554
0.84823877
0.84820360
INFO - Training [12][  160/  196]   Loss 0.550500   Top1 81.298828   Top5 98.034668   BatchTime 0.273472   LR 0.002380
0.84832013
0.84853667
0.84839714
0.84831667
0.84812528
0.84789467
0.84779429
0.84774494
0.84755588
0.84723359
0.84662932
0.84660506
0.84653854
0.84660059
0.84638441
0.84647971
0.84639889
0.84628451
0.84617245
0.84604043
0.84606367
0.84598643
0.84573299
INFO - Training [12][  180/  196]   Loss 0.550751   Top1 81.250000   Top5 97.988281   BatchTime 0.271522   LR 0.002371
0.84551454
0.84519452
0.84453714
0.84384179
0.84329540
0.84469622
0.84507501
0.84502500
0.84545648
0.84521443
0.84518939
0.84489858
0.84405613
0.84310859
0.84108222
INFO - ==> Top1: 81.258    Top5: 98.008    Loss: 0.550
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84042197
0.83982062
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [12][   20/   40]   Loss 0.418706   Top1 86.484375   Top5 99.316406   BatchTime 0.123503
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3320)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0995)
features.1.conv.6 tensor(0.0720)
features.2.conv.0 tensor(0.0333)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0804)
features.3.conv.0 tensor(0.0223)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0549)
features.4.conv.0 tensor(0.0368)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0975)
features.5.conv.0 tensor(0.0332)
features.5.conv.3 tensor(0.0856)
features.5.conv.6 tensor(0.1045)
features.6.conv.0 tensor(0.0329)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0758)
features.7.conv.0 tensor(0.0640)
features.7.conv.3 tensor(0.1123)
features.7.conv.6 tensor(0.1090)
features.8.conv.0 tensor(0.0603)
features.8.conv.3 tensor(0.1157)
features.8.conv.6 tensor(0.1336)
features.9.conv.0 tensor(0.0769)
features.9.conv.3 tensor(0.1484)
features.9.conv.6 tensor(0.1104)
features.10.conv.0 tensor(0.0419)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0819)
features.11.conv.0 tensor(0.1641)
features.11.conv.3 tensor(0.1240)
features.11.conv.6 tensor(0.1566)
features.12.conv.0 tensor(0.1502)
features.12.conv.3 tensor(0.1566)
features.12.conv.6 tensor(0.2138)
features.13.conv.0 tensor(0.0888)
features.13.conv.3 tensor(0.1331)
features.13.conv.6 tensor(0.1090)
features.14.conv.0 tensor(0.8999)
features.14.conv.3 tensor(0.0987)
features.14.conv.6 tensor(0.8603)
features.15.conv.0 tensor(0.9067)
features.15.conv.3 tensor(0.0751)
features.15.conv.6 tensor(0.9163)
features.16.conv.0 tensor(0.0513)
features.16.conv.3 tensor(0.0926)
features.16.conv.6 tensor(0.1584)
conv.0 tensor(0.1479)
tensor(748761.) 2188896.0
INFO - Validation [12][   40/   40]   Loss 0.416574   Top1 85.830000   Top5 99.440000   BatchTime 0.090245
INFO - ==> Top1: 85.830    Top5: 99.440    Loss: 0.417
INFO - ==> Sparsity : 0.342
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
0.84130895
0.84382403
0.84707600
0.84937942
0.84931487
0.84914780
0.84906197
0.84928131
0.84974307
0.84950906
0.84934419
0.84918171
0.84925002
0.84953582
0.84962910
0.84957850
INFO - Training [13][   20/  196]   Loss 0.551470   Top1 80.976562   Top5 97.753906   BatchTime 0.350553   LR 0.002355
0.84965348
0.84978461
0.85005987
0.85059053
0.85036618
0.85067028
0.85066396
0.85076362
0.85076720
0.85060972
0.85030341
0.84976614
0.84926146
0.84874439
0.84825188
0.84797019
0.84767580
0.84781605
0.84758133
0.84669048
0.84636533
INFO - Training [13][   40/  196]   Loss 0.553381   Top1 80.937500   Top5 97.851562   BatchTime 0.322338   LR 0.002345
0.84624410
0.84572512
0.84526128
0.84475011
0.84384543
0.84330231
0.84258813
0.84178424
0.84156978
0.84083253
0.84093302
0.84227711
0.84241229
0.84264284
0.84255373
0.84223175
0.84198391
0.84227157
0.84247577
0.84281200
0.84276915
0.84203511
0.84206814
INFO - Training [13][   60/  196]   Loss 0.551382   Top1 81.106771   Top5 97.962240   BatchTime 0.299494   LR 0.002336
0.84212148
0.84197134
0.84179616
0.84154040
0.84145695
0.84185749
0.84191948
0.84198809
0.84214801
0.84230918
0.84391582
0.84490699
0.84550244
0.84554529
0.84595895
INFO - Training [13][   80/  196]   Loss 0.541515   Top1 81.542969   Top5 98.095703   BatchTime 0.290780   LR 0.002325
0.84681886
0.84764099
0.84746510
0.84771514
0.84799713
0.84793133
0.84779412
0.84804457
0.84804446
0.84797633
0.84773445
0.84763807
0.84695131
0.84686077
0.84682000
0.84681678
0.84684855
0.84691012
0.84678864
0.84688836
0.84699899
0.84686172
0.84697211
INFO - Training [13][  100/  196]   Loss 0.535309   Top1 81.835938   Top5 98.113281   BatchTime 0.285199   LR 0.002315
0.84683865
0.84705174
0.84792602
0.84829551
0.84838754
0.84858423
0.84864867
0.84863049
0.84867722
0.84888083
0.84884119
0.84868801
0.84865159
0.84860718
0.84855926
0.84856087
0.84888113
0.84875065
0.84917855
0.84951490
0.84883028
0.84828043
0.84795743
INFO - Training [13][  120/  196]   Loss 0.527051   Top1 82.093099   Top5 98.157552   BatchTime 0.282492   LR 0.002304
0.84753823
0.84704554
0.84659779
0.84613901
0.84576201
0.84506339
0.84464890
0.84429324
0.84435302
0.84441859
0.84384525
0.84412742
0.84456533
0.84456217
0.84491134
0.84505337
INFO - Training [13][  140/  196]   Loss 0.526242   Top1 82.089844   Top5 98.253348   BatchTime 0.278335   LR 0.002293
0.84532726
0.84574652
0.84650636
0.84780413
0.84791964
0.84788078
0.84791023
0.84788692
0.84779394
0.84785628
0.84808677
0.84813261
0.84820068
0.84824383
0.84814101
0.84832919
0.84837145
0.84848326
0.84856606
0.84861451
INFO - Training [13][  160/  196]   Loss 0.526044   Top1 82.089844   Top5 98.239746   BatchTime 0.280899   LR 0.002282
0.84887022
0.84899336
0.84903485
0.84914905
0.84904468
0.84940958
0.84923542
0.84900016
0.84833622
0.84767950
0.84621823
0.84522259
0.84385902
0.84550315
0.84698397
0.84808046
0.84859902
0.84877795
0.84888864
0.84891409
0.84860575
0.84797114
INFO - Training [13][  180/  196]   Loss 0.527986   Top1 82.031250   Top5 98.166233   BatchTime 0.279461   LR 0.002271
0.84784573
0.84783322
0.84897679
0.84888154
0.84892839
0.84841102
0.84783638
0.84781021
0.84768218
0.84729779
0.84715122
0.84735000
0.84717572
0.84690720
0.84685946
INFO - ==> Top1: 81.974    Top5: 98.160    Loss: 0.529
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84662104
0.84651935
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.457855   Top1 84.375000   Top5 99.160156   BatchTime 0.119905
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0961)
features.1.conv.6 tensor(0.0694)
features.2.conv.0 tensor(0.0333)
features.2.conv.3 tensor(0.0687)
features.2.conv.6 tensor(0.0836)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0543)
features.4.conv.0 tensor(0.0340)
features.4.conv.3 tensor(0.1007)
features.4.conv.6 tensor(0.0923)
features.5.conv.0 tensor(0.0402)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0988)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0731)
features.7.conv.0 tensor(0.0705)
features.7.conv.3 tensor(0.1157)
features.7.conv.6 tensor(0.1075)
features.8.conv.0 tensor(0.0675)
features.8.conv.3 tensor(0.1178)
features.8.conv.6 tensor(0.1335)
features.9.conv.0 tensor(0.0701)
features.9.conv.3 tensor(0.1484)
features.9.conv.6 tensor(0.1145)
features.10.conv.0 tensor(0.0427)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.0811)
features.11.conv.0 tensor(0.1715)
features.11.conv.3 tensor(0.1273)
features.11.conv.6 tensor(0.1686)
features.12.conv.0 tensor(0.1642)
features.12.conv.3 tensor(0.1624)
features.12.conv.6 tensor(0.2175)
features.13.conv.0 tensor(0.0905)
features.13.conv.3 tensor(0.1331)
features.13.conv.6 tensor(0.1125)
features.14.conv.0 tensor(0.9044)
features.14.conv.3 tensor(0.1010)
features.14.conv.6 tensor(0.8733)
features.15.conv.0 tensor(0.9145)
features.15.conv.3 tensor(0.0755)
features.15.conv.6 tensor(0.9291)
features.16.conv.0 tensor(0.0564)
features.16.conv.3 tensor(0.1020)
features.16.conv.6 tensor(0.1679)
conv.0 tensor(0.1591)
tensor(765590.) 2188896.0
INFO - Validation [13][   40/   40]   Loss 0.436845   Top1 84.880000   Top5 99.390000   BatchTime 0.085244
INFO - ==> Top1: 84.880    Top5: 99.390    Loss: 0.437
INFO - ==> Sparsity : 0.350
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
0.84653139
0.84647089
0.84630579
0.84629637
0.84618616
0.84605342
0.84576797
0.84565121
0.84524208
0.84524202
0.84522897
0.84656864
0.84627181
0.84602052
0.84576607
0.84550387
0.84559488
INFO - Training [14][   20/  196]   Loss 0.543401   Top1 81.250000   Top5 97.304688   BatchTime 0.347556   LR 0.002250
0.84584421
0.84588069
0.84583759
0.84643382
0.84655696
0.84658384
0.84672302
0.84628773
0.84578371
0.84646773
0.84709930
0.84723419
0.84708875
0.84732985
0.84749132
0.84844792
0.84875011
0.84859419
0.84817743
0.84784245
INFO - Training [14][   40/  196]   Loss 0.542522   Top1 81.357422   Top5 97.607422   BatchTime 0.330057   LR 0.002238
0.84754330
0.84695458
0.84679753
0.84646624
0.84628642
0.84609628
0.84591609
0.84551787
0.84524500
0.84525466
0.84569389
0.84556139
0.84559393
0.84679592
0.84542042
0.84650636
0.84723669
0.84720087
0.84711403
INFO - Training [14][   60/  196]   Loss 0.538172   Top1 81.393229   Top5 97.838542   BatchTime 0.324673   LR 0.002225
0.84693235
0.84687138
0.84665346
0.84652025
0.84616905
0.84581524
0.84575194
0.84557420
0.84519655
0.84500682
0.84495795
0.84528774
0.84515679
0.84504676
0.84534925
0.84522897
0.84491599
0.84523457
INFO - Training [14][   80/  196]   Loss 0.527521   Top1 81.835938   Top5 97.968750   BatchTime 0.320677   LR 0.002213
0.84502697
0.84467047
0.84479821
0.84479207
0.84486246
0.84568566
0.84702975
0.84681082
0.84677964
0.84683967
0.84683090
0.84637922
0.84629589
0.84616810
0.84607774
0.84610438
0.84599191
0.84629494
0.84646457
0.84664500
0.84693956
0.84727132
0.84855932
INFO - Training [14][  100/  196]   Loss 0.523586   Top1 82.046875   Top5 97.988281   BatchTime 0.312246   LR 0.002200
0.84854281
0.84864271
0.84856540
0.84812754
0.84771132
0.84900856
0.84917980
0.84942102
0.84952587
0.84952873
0.84952748
0.84945154
0.84919596
0.84928685
0.84922493
0.84926862
0.84911197
0.84918863
0.84922701
0.84920353
0.84913963
INFO - Training [14][  120/  196]   Loss 0.519225   Top1 82.180990   Top5 98.092448   BatchTime 0.305515   LR 0.002186
0.84920579
0.84950757
0.84935081
0.84942132
0.84923792
0.84931600
0.84950513
0.84948152
0.84955746
0.84965706
0.84953713
0.84929365
0.84952563
0.84953779
0.84954578
0.84944332
INFO - Training [14][  140/  196]   Loss 0.517121   Top1 82.324219   Top5 98.119420   BatchTime 0.297584   LR 0.002173
0.84924895
0.84928930
0.84938449
0.84952390
0.84943783
0.84919554
0.84892350
0.84897208
0.84860849
0.84813762
0.84806019
0.84799147
0.84752005
0.84743464
0.84743172
0.84747326
0.84742242
0.84707606
0.84698600
0.84687191
0.84687543
0.84685045
0.84686333
0.84710252
INFO - Training [14][  160/  196]   Loss 0.520298   Top1 82.172852   Top5 98.120117   BatchTime 0.291830   LR 0.002159
0.84704763
0.84702480
0.84707600
0.84684724
0.84611702
0.84547687
0.84446996
0.84492350
0.84477448
0.84537244
0.84567040
0.84575015
0.84575438
0.84606415
0.84604031
0.84658247
INFO - Training [14][  180/  196]   Loss 0.519049   Top1 82.220052   Top5 98.090278   BatchTime 0.286513   LR 0.002145
0.84711784
0.84852308
0.84868658
0.84854650
0.84833914
0.84827095
0.84795302
0.84805828
0.84837025
0.84863335
0.84848845
0.84845191
0.84837800
0.84812075
0.84731370
0.84751678
0.84750724
0.84754360
0.84744757
0.84705049
0.84751385
0.84769547
********************pre-trained*****************
INFO - ==> Top1: 82.234    Top5: 98.104    Loss: 0.518
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 0.415804   Top1 86.699219   Top5 99.296875   BatchTime 0.125848
INFO - Validation [14][   40/   40]   Loss 0.409304   Top1 86.330000   Top5 99.410000   BatchTime 0.088837
INFO - ==> Top1: 86.330    Top5: 99.410    Loss: 0.409
INFO - ==> Sparsity : 0.359
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.2227)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0599)
features.2.conv.0 tensor(0.0286)
features.2.conv.3 tensor(0.0772)
features.2.conv.6 tensor(0.0816)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0516)
features.4.conv.0 tensor(0.0366)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.0894)
features.5.conv.0 tensor(0.0314)
features.5.conv.3 tensor(0.0885)
features.5.conv.6 tensor(0.0970)
features.6.conv.0 tensor(0.0343)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0742)
features.7.conv.0 tensor(0.0638)
features.7.conv.3 tensor(0.1131)
features.7.conv.6 tensor(0.1069)
features.8.conv.0 tensor(0.0558)
features.8.conv.3 tensor(0.1166)
features.8.conv.6 tensor(0.1313)
features.9.conv.0 tensor(0.0697)
features.9.conv.3 tensor(0.1551)
features.9.conv.6 tensor(0.1178)
features.10.conv.0 tensor(0.0454)
features.10.conv.3 tensor(0.0911)
features.10.conv.6 tensor(0.0792)
features.11.conv.0 tensor(0.1715)
features.11.conv.3 tensor(0.1271)
features.11.conv.6 tensor(0.1714)
features.12.conv.0 tensor(0.1719)
features.12.conv.3 tensor(0.1626)
features.12.conv.6 tensor(0.2182)
features.13.conv.0 tensor(0.0884)
features.13.conv.3 tensor(0.1356)
features.13.conv.6 tensor(0.1151)
features.14.conv.0 tensor(0.9059)
features.14.conv.3 tensor(0.1052)
features.14.conv.6 tensor(0.8729)
features.15.conv.0 tensor(0.9177)
features.15.conv.3 tensor(0.0735)
features.15.conv.6 tensor(0.9202)
features.16.conv.0 tensor(0.0586)
features.16.conv.3 tensor(0.1041)
features.16.conv.6 tensor(0.1530)
conv.0 tensor(0.2212)
tensor(786373.) 2188896.0
0.84918451
0.84911108
0.84893298
0.84877414
0.84861839
0.84864199
0.84855497
0.84843570
0.84829718
0.84816158
0.84786242
0.84783709
0.84792078
0.84755951
0.84749514
0.84752142
INFO - Training [15][   20/  196]   Loss 0.534725   Top1 81.660156   Top5 97.578125   BatchTime 0.403150   LR 0.002120
0.84745121
0.84765774
0.84773177
0.84770375
0.84774411
0.84750682
0.84702933
0.84695011
0.84825373
0.84777755
0.84741724
0.84711772
0.84684694
0.84671986
0.84665918
0.84664232
0.84665549
0.84668195
0.84685910
0.84786540
0.84935796
INFO - Training [15][   40/  196]   Loss 0.524952   Top1 81.914062   Top5 97.773438   BatchTime 0.332633   LR 0.002106
0.84915066
0.84832537
0.84820521
0.84832948
0.84800667
0.84778672
0.84765381
0.84744960
0.84696853
0.84646684
0.84622246
0.84631300
0.84592217
0.84595823
0.84569770
0.84591085
0.84619588
0.84619063
0.84618866
0.84628975
0.84653264
0.84629631
0.84610629
INFO - Training [15][   60/  196]   Loss 0.522054   Top1 81.875000   Top5 97.838542   BatchTime 0.309582   LR 0.002091
0.84618777
0.84606779
0.84580034
0.84606016
0.84582084
0.84586000
0.84590197
0.84573883
0.84578830
0.84578449
0.84569067
0.84567124
0.84583569
0.84598947
0.84568614
INFO - Training [15][   80/  196]   Loss 0.516725   Top1 82.099609   Top5 97.963867   BatchTime 0.297762   LR 0.002076
0.84535700
0.84526736
0.84552008
0.84556073
0.84524918
0.84520108
0.84508133
0.84500337
0.84507233
0.84467137
0.84439695
0.84368008
0.84313720
0.84278536
0.84256893
0.84228945
0.84185189
0.84176898
0.84187740
0.84196073
0.84219980
0.84243828
0.84265280
INFO - Training [15][  100/  196]   Loss 0.508556   Top1 82.417969   Top5 98.078125   BatchTime 0.290716   LR 0.002061
0.84275162
0.84309530
0.84298378
0.84222388
0.84372008
0.84475982
0.84623826
0.84659141
0.84647864
0.84662765
0.84603471
0.84563309
0.84583282
0.84698510
0.84703487
0.84702772
0.84700596
INFO - Training [15][  120/  196]   Loss 0.502110   Top1 82.688802   Top5 98.167318   BatchTime 0.282447   LR 0.002045
0.84704435
0.84654361
0.84631550
0.84628630
0.84605610
0.84586138
0.84589005
0.84603220
0.84711534
0.84767818
0.84759980
0.84738827
0.84747994
0.84720188
0.84718239
0.84704971
0.84692115
0.84705466
0.84696943
0.84672904
0.84661478
INFO - Training [15][  140/  196]   Loss 0.500530   Top1 82.834821   Top5 98.233817   BatchTime 0.283377   LR 0.002030
0.84643173
0.84636700
0.84615755
0.84625930
0.84641629
0.84674037
0.84653258
0.84686852
0.84678346
0.84667474
0.84664452
0.84676123
0.84680450
0.84684169
0.84706450
0.84683573
0.84679508
0.84673131
0.84674537
0.84682423
0.84681422
0.84684998
0.84691542
INFO - Training [15][  160/  196]   Loss 0.503386   Top1 82.736816   Top5 98.234863   BatchTime 0.280879   LR 0.002014
0.84690267
0.84670186
0.84678495
0.84685296
0.84679258
0.84682965
0.84687269
0.84699857
0.84717572
0.84742355
0.84738135
0.84744644
0.84730130
0.84723526
0.84726346
0.84702730
0.84667408
0.84637052
0.84664696
INFO - Training [15][  180/  196]   Loss 0.502294   Top1 82.758247   Top5 98.161892   BatchTime 0.284762   LR 0.001998
0.84635788
0.84624535
0.84623700
0.84634036
0.84614486
0.84592134
0.84578806
0.84569514
0.84573346
0.84414154
0.84576458
0.84573954
0.84572715
0.84580094
0.84578633
0.84582376
INFO - ==> Top1: 82.812    Top5: 98.160    Loss: 0.501
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84618002
0.84688646
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.403571   Top1 86.386719   Top5 99.277344   BatchTime 0.124098
INFO - Validation [15][   40/   40]   Loss 0.392730   Top1 86.440000   Top5 99.410000   BatchTime 0.089130
INFO - ==> Top1: 86.440    Top5: 99.410    Loss: 0.393
INFO - ==> Sparsity : 0.360
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.630   Top5: 99.490]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0267)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0660)
features.2.conv.0 tensor(0.0272)
features.2.conv.3 tensor(0.0710)
features.2.conv.6 tensor(0.0848)
features.3.conv.0 tensor(0.0223)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.0534)
features.4.conv.0 tensor(0.0306)
features.4.conv.3 tensor(0.1024)
features.4.conv.6 tensor(0.0908)
features.5.conv.0 tensor(0.0280)
features.5.conv.3 tensor(0.0845)
features.5.conv.6 tensor(0.0968)
features.6.conv.0 tensor(0.0278)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0741)
features.7.conv.0 tensor(0.0680)
features.7.conv.3 tensor(0.1146)
features.7.conv.6 tensor(0.1067)
features.8.conv.0 tensor(0.0684)
features.8.conv.3 tensor(0.1172)
features.8.conv.6 tensor(0.1211)
features.9.conv.0 tensor(0.0502)
features.9.conv.3 tensor(0.1461)
features.9.conv.6 tensor(0.1169)
features.10.conv.0 tensor(0.0361)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.0785)
features.11.conv.0 tensor(0.1707)
features.11.conv.3 tensor(0.1256)
features.11.conv.6 tensor(0.1692)
features.12.conv.0 tensor(0.1824)
features.12.conv.3 tensor(0.1696)
features.12.conv.6 tensor(0.2178)
features.13.conv.0 tensor(0.1028)
features.13.conv.3 tensor(0.1373)
features.13.conv.6 tensor(0.1069)
features.14.conv.0 tensor(0.9112)
features.14.conv.3 tensor(0.1036)
features.14.conv.6 tensor(0.9135)
features.15.conv.0 tensor(0.9225)
features.15.conv.3 tensor(0.0753)
features.15.conv.6 tensor(0.9305)
features.16.conv.0 tensor(0.0667)
features.16.conv.3 tensor(0.1035)
features.16.conv.6 tensor(0.1636)
conv.0 tensor(0.1906)
tensor(787450.) 2188896.0
0.84720629
0.84731895
0.84738499
0.84743154
0.84724301
0.84713185
0.84710467
0.84708387
0.84678209
0.84687603
0.84688705
0.84713018
0.84710163
0.84716660
0.84723800
INFO - Training [16][   20/  196]   Loss 0.499514   Top1 82.910156   Top5 97.656250   BatchTime 0.319277   LR 0.001969
0.84704059
0.84713411
0.84730291
0.84710377
0.84696364
0.84699780
0.84709990
0.84706891
0.84702724
0.84705073
0.84685296
0.84694815
0.84683418
0.84709692
0.84693915
0.84645134
0.84625882
0.84639692
0.84627664
0.84614998
0.84650737
0.84634614
0.84611309
INFO - Training [16][   40/  196]   Loss 0.501117   Top1 83.251953   Top5 98.007812   BatchTime 0.293268   LR 0.001953
0.84602857
0.84586442
0.84572273
0.84565192
0.84613329
0.84660757
0.84672880
0.84687793
0.84687155
0.84668046
0.84645319
0.84617054
0.84599835
0.84596068
0.84578311
0.84556973
0.84540933
0.84537274
0.84540790
0.84519160
INFO - Training [16][   60/  196]   Loss 0.496253   Top1 83.072917   Top5 98.170573   BatchTime 0.293168   LR 0.001936
0.84524745
0.84545541
0.84558946
0.84571326
0.84571964
0.84542495
0.84531605
0.84523773
0.84484172
0.84466839
0.84462762
0.84449911
0.84414089
0.84368849
0.84356761
0.84348303
0.84352696
0.84333187
0.84469157
0.84467345
0.84460348
0.84463668
INFO - Training [16][   80/  196]   Loss 0.492631   Top1 83.222656   Top5 98.271484   BatchTime 0.289305   LR 0.001919
0.84446573
0.84443527
0.84456444
0.84475893
0.84513402
0.84550011
0.84558952
0.84552485
0.84552801
0.84564173
0.84551406
0.84553200
0.84522349
0.84533310
0.84524661
0.84501725
0.84498793
0.84487897
0.84494591
0.84554756
0.84590894
INFO - Training [16][  100/  196]   Loss 0.486695   Top1 83.519531   Top5 98.285156   BatchTime 0.289443   LR 0.001902
0.84627736
0.84620500
0.84629887
0.84612262
0.84594768
0.84574932
0.84620351
0.84629780
0.84645391
0.84671634
0.84665716
0.84651130
0.84631461
0.84657693
0.84682274
0.84655493
0.84661955
0.84654135
0.84632146
0.84616584
INFO - Training [16][  120/  196]   Loss 0.484797   Top1 83.603516   Top5 98.330078   BatchTime 0.290116   LR 0.001885
0.84583580
0.84555149
0.84537464
0.84507918
0.84497255
0.84485716
0.84488249
0.84502137
0.84478259
0.84473008
0.84452450
0.84465152
0.84442002
0.84429997
0.84405041
INFO - Training [16][  140/  196]   Loss 0.482798   Top1 83.680246   Top5 98.351004   BatchTime 0.287121   LR 0.001867
0.84414023
0.84406698
0.84370923
0.84363443
0.84362561
0.84375620
0.84349024
0.84377158
0.84380442
0.84341526
0.84299386
0.84248769
0.84246135
0.84245098
0.84215695
0.84142166
0.84125572
0.84127259
0.84090573
0.84063810
0.84073412
0.84077531
INFO - Training [16][  160/  196]   Loss 0.483880   Top1 83.659668   Top5 98.310547   BatchTime 0.285100   LR 0.001850
0.84107292
0.84059930
0.84032893
0.84050018
0.84110481
0.84107411
0.84076130
0.84068012
0.84081775
0.84116220
0.84090662
0.84122455
0.84118092
0.84103686
0.84076285
0.83996570
0.83980501
0.83934534
0.83826858
0.83792835
0.83769888
0.83720684
0.83681613
0.83712953
INFO - Training [16][  180/  196]   Loss 0.485148   Top1 83.535156   Top5 98.263889   BatchTime 0.281869   LR 0.001832
0.83693391
0.83662528
0.83660924
0.83708227
0.83776158
0.83849281
0.83999699
0.84127283
0.84313381
INFO - ==> Top1: 83.516    Top5: 98.234    Loss: 0.485
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84694535
0.84718180
0.84725815
0.84714901
0.84694320
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.370312   Top1 87.656250   Top5 99.414062   BatchTime 0.125135
INFO - Validation [16][   40/   40]   Loss 0.361400   Top1 87.800000   Top5 99.520000   BatchTime 0.090428
INFO - ==> Top1: 87.800    Top5: 99.520    Loss: 0.361
INFO - ==> Sparsity : 0.361
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 87.800   Top5: 99.520]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 86.780   Top5: 99.490]
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.2148)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0699)
features.2.conv.0 tensor(0.0260)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0862)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0551)
features.4.conv.0 tensor(0.0332)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0890)
features.5.conv.0 tensor(0.0282)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0964)
features.6.conv.0 tensor(0.0267)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0713)
features.7.conv.0 tensor(0.0774)
features.7.conv.3 tensor(0.1131)
features.7.conv.6 tensor(0.1047)
features.8.conv.0 tensor(0.0682)
features.8.conv.3 tensor(0.1181)
features.8.conv.6 tensor(0.1195)
features.9.conv.0 tensor(0.0600)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.1133)
features.10.conv.0 tensor(0.0406)
features.10.conv.3 tensor(0.0877)
features.10.conv.6 tensor(0.0796)
features.11.conv.0 tensor(0.1635)
features.11.conv.3 tensor(0.1279)
features.11.conv.6 tensor(0.2100)
features.12.conv.0 tensor(0.1883)
features.12.conv.3 tensor(0.1684)
features.12.conv.6 tensor(0.2245)
features.13.conv.0 tensor(0.0917)
features.13.conv.3 tensor(0.1360)
features.13.conv.6 tensor(0.1035)
features.14.conv.0 tensor(0.9168)
features.14.conv.3 tensor(0.1054)
features.14.conv.6 tensor(0.9179)
features.15.conv.0 tensor(0.9284)
features.15.conv.3 tensor(0.0753)
features.15.conv.6 tensor(0.9335)
features.16.conv.0 tensor(0.0721)
features.16.conv.3 tensor(0.1013)
features.16.conv.6 tensor(0.1734)
conv.0 tensor(0.1761)
tensor(790263.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
0.84682089
0.84650379
0.84617305
0.84608531
0.84546906
0.84505123
0.84482878
0.84463125
0.84430587
0.84407502
0.84354311
0.84356916
0.84369618
0.84493333
0.84486061
0.84467494
0.84451252
0.84456974
0.84462601
0.84438002
0.84393591
INFO - Training [17][   20/  196]   Loss 0.527550   Top1 81.718750   Top5 97.500000   BatchTime 0.336238   LR 0.001800
0.84374416
0.84372842
0.84372050
0.84336245
0.84340662
0.84363186
0.84543538
0.84543151
0.84522754
0.84524482
0.84465373
0.84440976
0.84459364
0.84444046
0.84547603
0.84554672
0.84548563
0.84518379
0.84519893
0.84456325
0.84436363
INFO - Training [17][   40/  196]   Loss 0.509210   Top1 82.519531   Top5 97.861328   BatchTime 0.315589   LR 0.001782
0.84423482
0.84387600
0.84323210
0.84306252
0.84324735
0.84340489
0.84297842
0.84266889
0.84206647
0.84088951
0.83997256
0.83825934
0.83593100
0.83344644
0.83104193
INFO - Training [17][   60/  196]   Loss 0.501574   Top1 82.747396   Top5 97.975260   BatchTime 0.298866   LR 0.001764
0.83193821
0.83207363
0.83314145
0.83644968
0.83902419
0.83976632
0.84016520
0.84074974
0.84175444
INFO - Training [17][   80/  196]   Loss 0.500660   Top1 82.856445   Top5 98.090820   BatchTime 0.287189   LR 0.001746
0.84307945
0.84429014
0.84567583
0.84669876
0.84640306
0.84626186
0.84648287
0.84642625
0.84644699
0.84642166
0.84653229
0.84662336
0.84658092
0.84673733
0.84673446
0.84698272
0.84718221
0.84713376
0.84702271
0.84686410
0.84700114
0.84698445
0.84691936
0.84694266
0.84712851
0.84688264
0.84694916
0.84707165
0.84713322
0.84745097
0.84747887
0.84795135
0.84769571
0.84768599
0.84779888
0.84813017
INFO - Training [17][  100/  196]   Loss 0.491511   Top1 83.058594   Top5 98.074219   BatchTime 0.286079   LR 0.001727
0.84834206
0.84806240
0.84810609
0.84855741
0.84885424
0.84860140
0.84854853
0.84851086
0.84836221
0.84828573
0.84830499
0.84802586
0.84780431
0.84790874
0.84785783
INFO - Training [17][  120/  196]   Loss 0.485515   Top1 83.382161   Top5 98.173828   BatchTime 0.283260   LR 0.001708
0.84756786
0.84764397
0.84751511
0.84742093
0.84730178
0.84713030
0.84702593
0.84692407
0.84686625
0.84687412
0.84690559
0.84706801
0.84715217
0.84699237
0.84728754
0.84717596
0.84695870
0.84692562
0.84676677
0.84684020
0.84652513
0.84628451
0.84615934
INFO - Training [17][  140/  196]   Loss 0.481100   Top1 83.579799   Top5 98.231027   BatchTime 0.280299   LR 0.001690
0.84599650
0.84543759
0.84514195
0.84479702
0.84429526
0.84395391
0.84323919
0.84295130
0.84417862
0.84368664
0.84300190
0.84218627
0.84159291
0.84142041
0.84096283
0.84103864
0.84098482
0.84091330
INFO - Training [17][  160/  196]   Loss 0.482116   Top1 83.527832   Top5 98.232422   BatchTime 0.285690   LR 0.001671
0.84070075
0.84062070
0.84031069
0.84054917
0.84084815
0.84097087
0.84046721
0.84044391
0.84045899
0.84024769
0.84007901
0.84016734
0.83945805
0.83915806
0.83905035
0.83881962
0.83846307
0.83825606
0.83806151
0.83805895
0.83793122
0.83779824
0.83767778
0.83762866
INFO - Training [17][  180/  196]   Loss 0.480127   Top1 83.550347   Top5 98.194444   BatchTime 0.281802   LR 0.001652
0.83747721
0.83734149
0.83725780
0.83733231
0.83719575
0.83713794
0.83712864
0.83689708
0.83673775
0.83632392
0.83677876
0.83659267
0.83634055
0.83573753
********************pre-trained*****************
INFO - ==> Top1: 83.564    Top5: 98.208    Loss: 0.480
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [17][   20/   40]   Loss 0.376432   Top1 87.890625   Top5 99.453125   BatchTime 0.117986
INFO - Validation [17][   40/   40]   Loss 0.361805   Top1 87.960000   Top5 99.550000   BatchTime 0.085388
INFO - ==> Top1: 87.960    Top5: 99.550    Loss: 0.362
INFO - ==> Sparsity : 0.370
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 87.960   Top5: 99.550]
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 87.800   Top5: 99.520]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 86.990   Top5: 99.570]
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.2871)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0240)
features.2.conv.3 tensor(0.0687)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0508)
features.4.conv.0 tensor(0.0184)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.0859)
features.5.conv.0 tensor(0.0233)
features.5.conv.3 tensor(0.0839)
features.5.conv.6 tensor(0.0975)
features.6.conv.0 tensor(0.0308)
features.6.conv.3 tensor(0.0324)
features.6.conv.6 tensor(0.0719)
features.7.conv.0 tensor(0.0728)
features.7.conv.3 tensor(0.1111)
features.7.conv.6 tensor(0.1021)
features.8.conv.0 tensor(0.0618)
features.8.conv.3 tensor(0.1198)
features.8.conv.6 tensor(0.1216)
features.9.conv.0 tensor(0.0540)
features.9.conv.3 tensor(0.1496)
features.9.conv.6 tensor(0.1855)
features.10.conv.0 tensor(0.0438)
features.10.conv.3 tensor(0.0885)
features.10.conv.6 tensor(0.0780)
features.11.conv.0 tensor(0.1734)
features.11.conv.3 tensor(0.1267)
features.11.conv.6 tensor(0.3015)
features.12.conv.0 tensor(0.1931)
features.12.conv.3 tensor(0.1671)
features.12.conv.6 tensor(0.2257)
features.13.conv.0 tensor(0.0983)
features.13.conv.3 tensor(0.1354)
features.13.conv.6 tensor(0.1027)
features.14.conv.0 tensor(0.9195)
features.14.conv.3 tensor(0.1060)
features.14.conv.6 tensor(0.9607)
features.15.conv.0 tensor(0.9261)
features.15.conv.3 tensor(0.0743)
features.15.conv.6 tensor(0.9428)
features.16.conv.0 tensor(0.0682)
features.16.conv.3 tensor(0.1038)
features.16.conv.6 tensor(0.1830)
conv.0 tensor(0.1782)
tensor(809060.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.83486044
0.83317471
0.83146125
0.83220601
0.83177358
0.83130240
0.83711869
0.83723003
0.83704394
0.83775234
0.83869702
0.84033984
0.84079587
0.84164935
0.84301502
0.84314018
0.84320474
0.84361279
INFO - Training [18][   20/  196]   Loss 0.462212   Top1 83.808594   Top5 97.851562   BatchTime 0.330330   LR 0.001618
0.84399110
0.84391749
0.84388447
0.84383100
0.84387434
0.84375119
0.84367293
0.84368914
0.84392691
0.84420496
0.84432882
0.84425658
0.84449065
0.84505141
0.84418976
0.84544235
0.84532827
0.84533525
0.84536362
INFO - Training [18][   40/  196]   Loss 0.474683   Top1 83.525391   Top5 98.046875   BatchTime 0.326855   LR 0.001599
0.84520644
0.84519368
0.84500861
0.84484249
0.84507114
0.84506357
0.84480488
0.84488016
0.84485161
0.84466076
0.84439397
0.84444577
0.84474683
0.84497911
0.84506392
0.84481353
0.84465671
0.84464306
0.84445965
0.84413034
0.84443241
INFO - Training [18][   60/  196]   Loss 0.467750   Top1 83.834635   Top5 98.177083   BatchTime 0.310784   LR 0.001579
0.84459507
0.84456062
0.84440857
0.84454685
0.84437168
0.84421468
0.84396404
0.84383720
0.84377891
0.84368169
0.84371030
0.84376168
0.84346223
0.84325445
0.84304315
0.84274447
0.84278578
0.84278005
0.84281474
0.84262472
0.84242517
0.84227455
0.84191144
INFO - Training [18][   80/  196]   Loss 0.467215   Top1 83.779297   Top5 98.251953   BatchTime 0.298768   LR 0.001560
0.84140199
0.84066647
0.83999008
0.83921075
0.83807254
0.83670056
0.83486378
0.83358842
0.83209544
0.83194196
0.83080178
0.83109337
0.83142525
0.83191502
0.83345008
0.83488154
INFO - Training [18][  100/  196]   Loss 0.464758   Top1 83.890625   Top5 98.253906   BatchTime 0.291299   LR 0.001540
0.83718687
0.83879167
0.83981049
0.84029871
0.84083235
0.84161353
0.84242398
0.84343266
0.84495115
0.84491462
0.84485525
0.84480596
0.84463149
0.84425968
0.84376299
0.84311360
0.84205770
0.84136629
0.84064442
0.84043509
0.84058785
0.84069026
INFO - Training [18][  120/  196]   Loss 0.459166   Top1 84.078776   Top5 98.323568   BatchTime 0.287256   LR 0.001521
0.84074557
0.84091622
0.84125954
0.84166360
0.84148151
0.84135592
0.84081662
0.84028691
0.83913982
0.83858436
0.83877403
0.83893985
0.83905154
0.83885831
0.83861321
0.83801830
0.83731753
0.83577567
0.83340698
0.83097124
0.83243710
0.83265930
INFO - Training [18][  140/  196]   Loss 0.456038   Top1 84.257812   Top5 98.378906   BatchTime 0.284786   LR 0.001501
0.83250970
0.83156902
0.83141798
0.83036029
0.82824087
0.82483608
0.82654440
0.82535076
0.82496536
0.82522702
0.82392079
0.82269716
0.82225084
0.82198817
INFO - Training [18][  160/  196]   Loss 0.459614   Top1 84.182129   Top5 98.364258   BatchTime 0.285100   LR 0.001482
0.82109535
0.81985068
0.81836903
0.81532532
0.81021470
0.81890881
0.81877959
0.81929803
0.82093149
0.82388842
0.82850444
0.83302921
0.83670813
0.83963960
0.84288025
0.84389615
0.84374571
0.84328812
0.84322196
0.84322345
0.84304971
INFO - Training [18][  180/  196]   Loss 0.460518   Top1 84.199219   Top5 98.259549   BatchTime 0.285198   LR 0.001462
0.84272933
0.84244764
0.84223229
0.84220147
0.84196585
0.84219712
0.84230077
0.84225690
0.84268141
0.84256470
0.84224945
0.84211957
0.84210265
0.84163982
0.84134382
0.84110057
0.84072548
0.84048551
0.84045303
0.84083146
********************pre-trained*****************
INFO - ==> Top1: 84.234    Top5: 98.266    Loss: 0.460
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [18][   20/   40]   Loss 0.368012   Top1 87.851562   Top5 99.433594   BatchTime 0.140531
INFO - Validation [18][   40/   40]   Loss 0.360243   Top1 87.890000   Top5 99.520000   BatchTime 0.097568
INFO - ==> Top1: 87.890    Top5: 99.520    Loss: 0.360
INFO - ==> Sparsity : 0.367
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 87.960   Top5: 99.550]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 87.890   Top5: 99.520]
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 87.800   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.2324)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0226)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0482)
features.4.conv.0 tensor(0.0267)
features.4.conv.3 tensor(0.1076)
features.4.conv.6 tensor(0.0828)
features.5.conv.0 tensor(0.0182)
features.5.conv.3 tensor(0.0839)
features.5.conv.6 tensor(0.0926)
features.6.conv.0 tensor(0.0348)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0719)
features.7.conv.0 tensor(0.0636)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.0992)
features.8.conv.0 tensor(0.0706)
features.8.conv.3 tensor(0.1149)
features.8.conv.6 tensor(0.1279)
features.9.conv.0 tensor(0.0576)
features.9.conv.3 tensor(0.1505)
features.9.conv.6 tensor(0.1224)
features.10.conv.0 tensor(0.0405)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.0789)
features.11.conv.0 tensor(0.1822)
features.11.conv.3 tensor(0.1281)
features.11.conv.6 tensor(0.1984)
features.12.conv.0 tensor(0.2887)
features.12.conv.3 tensor(0.1649)
features.12.conv.6 tensor(0.2358)
features.13.conv.0 tensor(0.1003)
features.13.conv.3 tensor(0.1302)
features.13.conv.6 tensor(0.0954)
features.14.conv.0 tensor(0.9180)
features.14.conv.3 tensor(0.1051)
features.14.conv.6 tensor(0.8976)
features.15.conv.0 tensor(0.9307)
features.15.conv.3 tensor(0.0757)
features.15.conv.6 tensor(0.9382)
features.16.conv.0 tensor(0.0715)
features.16.conv.3 tensor(0.1037)
features.16.conv.6 tensor(0.2021)
conv.0 tensor(0.1759)
tensor(803047.) 2188896.0
0.84194613
0.84370708
0.84415573
0.84505862
0.84501123
0.84501421
0.84490609
0.84506792
0.84537178
0.84715146
0.84738100
0.84723318
0.84725428
0.84699821
0.84713554
INFO - Training [19][   20/  196]   Loss 0.468444   Top1 83.691406   Top5 97.617188   BatchTime 0.327873   LR 0.001427
0.84703237
0.84704077
0.84702349
0.84681791
0.84680885
0.84678292
0.84658229
0.84614366
0.84572798
0.84568417
0.84586185
0.84575403
0.84540313
0.84543061
0.84558064
0.84561849
0.84556502
0.84551483
0.84547704
0.84554738
0.84571332
0.84574306
0.84566092
0.84555191
INFO - Training [19][   40/  196]   Loss 0.464992   Top1 83.837891   Top5 97.851562   BatchTime 0.285534   LR 0.001407
0.84553301
0.84590530
0.84583718
0.84565902
0.84569710
0.84562308
0.84608102
0.84654784
0.84679806
0.84680420
0.84700161
0.84708965
0.84685689
0.84697127
0.84679353
0.84666973
INFO - Training [19][   60/  196]   Loss 0.463331   Top1 83.997396   Top5 98.007812   BatchTime 0.273950   LR 0.001387
0.84675545
0.84670299
0.84675413
0.84657651
0.84630811
0.84618747
0.84591240
0.84554064
0.84554303
0.84543449
0.84530085
0.84543252
0.84532332
0.84530151
0.84551221
0.84520322
0.84526306
0.84496641
0.84460270
0.84459382
0.84438759
0.84411770
0.84381980
0.84334934
INFO - Training [19][   80/  196]   Loss 0.456913   Top1 84.267578   Top5 98.208008   BatchTime 0.266856   LR 0.001367
0.84298283
0.84284323
0.84269917
0.84240711
0.84228313
0.84221894
0.84182370
0.84189934
0.84163344
0.84116352
0.84127235
0.84140033
0.84145206
0.84156507
0.84143430
0.84088403
0.84063476
0.84071475
0.84044957
0.84046721
0.84039372
INFO - Training [19][  100/  196]   Loss 0.449786   Top1 84.484375   Top5 98.242188   BatchTime 0.271765   LR 0.001347
0.84050035
0.84044975
0.84064597
0.84040654
0.83996618
0.83954251
0.83955669
0.83940351
0.83927178
0.83923596
0.83922529
0.83879584
0.83808047
0.83762717
0.83733034
0.83726215
0.83730721
0.83708423
0.83700377
0.83678639
0.83671999
INFO - Training [19][  120/  196]   Loss 0.445256   Top1 84.729818   Top5 98.352865   BatchTime 0.273559   LR 0.001327
0.83655077
0.83644158
0.83594167
0.83553922
0.83511329
0.83460605
0.83397257
0.83290207
0.83179682
0.83035064
0.82920521
0.82851869
0.82762748
0.82767600
0.82765508
0.82808942
0.82645541
INFO - Training [19][  140/  196]   Loss 0.443942   Top1 84.801897   Top5 98.406808   BatchTime 0.270538   LR 0.001307
0.82627714
0.82665879
0.82642531
0.82737947
0.82855999
0.82967818
0.83026046
0.83044076
0.83112979
0.83154160
0.83267796
0.83288181
0.83488137
0.83491015
0.83410025
0.83218151
0.83006048
0.83243662
0.83515340
0.83652812
0.83638877
0.83647275
0.83679116
INFO - Training [19][  160/  196]   Loss 0.447594   Top1 84.677734   Top5 98.369141   BatchTime 0.268354   LR 0.001287
0.83692008
0.83714497
0.83697850
0.83705944
0.83719128
0.83732796
0.83753520
0.83749127
0.83768982
0.83770305
0.83763981
0.83735025
0.83710545
0.83672470
0.83679670
0.83683097
INFO - Training [19][  180/  196]   Loss 0.445797   Top1 84.739583   Top5 98.305122   BatchTime 0.267612   LR 0.001266
0.83675402
0.83647591
0.83598357
0.83617371
0.83586842
0.83580214
0.83552426
0.83527577
0.83500093
0.83488667
0.83435416
0.83380437
0.83328009
0.83287597
0.83184332
0.83141685
INFO - ==> Top1: 84.822    Top5: 98.310    Loss: 0.444
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83096045
0.83125764
0.83129734
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 0.363741   Top1 88.222656   Top5 99.511719   BatchTime 0.124570
INFO - Validation [19][   40/   40]   Loss 0.350207   Top1 88.360000   Top5 99.600000   BatchTime 0.090137
INFO - ==> Top1: 88.360    Top5: 99.600    Loss: 0.350
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.360   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 87.960   Top5: 99.550]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 87.890   Top5: 99.520]
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0240)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0828)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0477)
features.4.conv.0 tensor(0.0208)
features.4.conv.3 tensor(0.1013)
features.4.conv.6 tensor(0.0840)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0856)
features.5.conv.6 tensor(0.0951)
features.6.conv.0 tensor(0.0303)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0685)
features.7.conv.0 tensor(0.0629)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.0981)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1114)
features.8.conv.6 tensor(0.1261)
features.9.conv.0 tensor(0.0583)
features.9.conv.3 tensor(0.1513)
features.9.conv.6 tensor(0.1158)
features.10.conv.0 tensor(0.0330)
features.10.conv.3 tensor(0.0932)
features.10.conv.6 tensor(0.0798)
features.11.conv.0 tensor(0.1803)
features.11.conv.3 tensor(0.1252)
features.11.conv.6 tensor(0.4702)
features.12.conv.0 tensor(0.1811)
features.12.conv.3 tensor(0.1645)
features.12.conv.6 tensor(0.3066)
features.13.conv.0 tensor(0.1016)
features.13.conv.3 tensor(0.1348)
features.13.conv.6 tensor(0.0924)
features.14.conv.0 tensor(0.9244)
features.14.conv.3 tensor(0.1056)
features.14.conv.6 tensor(0.9401)
features.15.conv.0 tensor(0.9296)
features.15.conv.3 tensor(0.0745)
features.15.conv.6 tensor(0.9400)
features.16.conv.0 tensor(0.0753)
features.16.conv.3 tensor(0.1003)
features.16.conv.6 tensor(0.1998)
conv.0 tensor(0.1762)
tensor(822780.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.83156049
0.83093745
0.82974041
0.82807857
0.82606047
0.82425189
0.82220507
0.81959873
0.81953913
0.82094151
0.82245773
0.82498205
0.82702303
0.82926840
0.83142692
0.83310968
0.83448446
0.83554661
INFO - Training [20][   20/  196]   Loss 0.453565   Top1 83.671875   Top5 97.929688   BatchTime 0.339775   LR 0.001231
0.83689815
0.83796155
0.83951968
0.83969617
0.83957279
0.83909470
0.83890676
0.83879781
0.83861887
0.83843076
0.83826053
0.83810961
0.83779401
0.83797055
0.83785760
0.83675599
0.83608603
0.83549196
0.83475202
0.83450657
0.83460009
INFO - Training [20][   40/  196]   Loss 0.444603   Top1 84.218750   Top5 98.144531   BatchTime 0.315954   LR 0.001211
0.83451396
0.83386189
0.83388752
0.83393443
0.83382577
0.83410764
0.83551002
0.83583045
0.83690315
0.83673614
0.83683163
0.83678043
0.83679157
0.83632904
0.83588982
0.83557492
0.83514917
0.83470124
0.83463615
0.83510959
0.83429384
0.83254737
INFO - Training [20][   60/  196]   Loss 0.452395   Top1 84.042969   Top5 98.125000   BatchTime 0.301402   LR 0.001191
0.83212239
0.83214098
0.83182329
0.82984060
0.82897389
0.82801324
0.82643491
0.82439667
0.82185090
0.82227194
0.82155365
0.82226014
0.82298356
0.82328498
0.82313842
INFO - Training [20][   80/  196]   Loss 0.446931   Top1 84.272461   Top5 98.242188   BatchTime 0.293431   LR 0.001171
0.82307309
0.82273793
0.82420307
0.82516503
0.82640642
0.82777429
0.82826632
0.82911122
0.82995683
0.83020347
0.83057755
0.83138651
0.83298111
0.83293140
0.83309132
0.83335221
0.83343583
0.83350807
0.83344907
0.83330965
0.83322424
0.83317846
0.83325028
0.83333868
0.83353513
0.83379614
INFO - Training [20][  100/  196]   Loss 0.442072   Top1 84.554688   Top5 98.289062   BatchTime 0.279754   LR 0.001151
0.83375996
0.83393389
0.83423007
0.83425122
0.83432990
0.83431858
0.83438748
0.83459747
0.83468157
0.83487880
0.83502132
0.83511174
0.83515269
0.83537138
0.83560109
0.83540767
0.83507001
INFO - Training [20][  120/  196]   Loss 0.435349   Top1 84.889323   Top5 98.362630   BatchTime 0.271998   LR 0.001131
0.83493000
0.83520544
0.83526284
0.83533245
0.83536977
0.83555323
0.83537179
0.83519632
0.83518654
0.83517998
0.83507586
0.83484459
0.83459622
0.83477253
0.83462697
0.83462733
0.83456874
0.83481002
0.83480489
0.83514524
INFO - Training [20][  140/  196]   Loss 0.434033   Top1 84.966518   Top5 98.404018   BatchTime 0.276591   LR 0.001111
0.83525443
0.83515435
0.83538395
0.83560383
0.83546120
0.83532113
0.83544308
0.83558780
0.83547544
0.83538532
0.83539212
0.83547926
0.83521944
0.83529645
0.83547604
0.83532667
0.83524930
0.83520985
INFO - Training [20][  160/  196]   Loss 0.433929   Top1 84.990234   Top5 98.444824   BatchTime 0.282710   LR 0.001091
0.83537239
0.83547109
0.83528346
0.83529979
0.83537078
0.83539855
0.83539426
0.83530539
0.83525687
0.83519328
0.83520877
0.83526498
0.83519793
0.83520103
0.83527511
0.83556378
0.83530825
0.83546364
0.83547384
0.83559263
0.83536261
0.83551443
INFO - Training [20][  180/  196]   Loss 0.435319   Top1 84.889323   Top5 98.346354   BatchTime 0.281246   LR 0.001071
0.83561432
0.83539993
0.83530748
0.83523870
0.83539140
0.83526868
0.83507508
0.83493590
0.83481103
0.83491254
0.83462280
0.83432263
0.83434063
0.83424544
0.83421773
INFO - ==> Top1: 84.978    Top5: 98.368    Loss: 0.433
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83408141
0.83391345
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [20][   20/   40]   Loss 0.363246   Top1 87.968750   Top5 99.531250   BatchTime 0.125141
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.2324)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0793)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0497)
features.4.conv.0 tensor(0.0251)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.0835)
features.5.conv.0 tensor(0.0270)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0934)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0689)
features.7.conv.0 tensor(0.0544)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.1674)
features.8.conv.0 tensor(0.0603)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.1222)
features.9.conv.0 tensor(0.0627)
features.9.conv.3 tensor(0.1461)
features.9.conv.6 tensor(0.1130)
features.10.conv.0 tensor(0.0344)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.0782)
features.11.conv.0 tensor(0.1842)
features.11.conv.3 tensor(0.1269)
features.11.conv.6 tensor(0.1844)
features.12.conv.0 tensor(0.1802)
features.12.conv.3 tensor(0.1645)
features.12.conv.6 tensor(0.4026)
features.13.conv.0 tensor(0.0916)
features.13.conv.3 tensor(0.1321)
features.13.conv.6 tensor(0.0909)
features.14.conv.0 tensor(0.9233)
features.14.conv.3 tensor(0.1051)
features.14.conv.6 tensor(0.9434)
features.15.conv.0 tensor(0.9304)
features.15.conv.3 tensor(0.0719)
features.15.conv.6 tensor(0.9461)
features.16.conv.0 tensor(0.0818)
features.16.conv.3 tensor(0.1024)
features.16.conv.6 tensor(0.2196)
conv.0 tensor(0.1963)
tensor(829730.) 2188896.0
INFO - Validation [20][   40/   40]   Loss 0.346449   Top1 88.290000   Top5 99.620000   BatchTime 0.088649
INFO - ==> Top1: 88.290    Top5: 99.620    Loss: 0.346
INFO - ==> Sparsity : 0.379
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.360   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 88.290   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 87.960   Top5: 99.550]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
0.83398753
0.83375889
0.83301198
0.83285993
0.83271217
0.83279049
0.83283424
0.83274704
0.83258867
0.83252722
0.83201712
0.83144784
0.83155090
0.83271432
0.83371377
0.83430308
0.83421189
INFO - Training [21][   20/  196]   Loss 0.427945   Top1 85.234375   Top5 97.929688   BatchTime 0.342750   LR 0.001036
0.83415592
0.83413225
0.83405733
0.83372933
0.83340609
0.83352494
0.83362263
0.83367360
0.83347565
0.83345187
0.83317947
0.83295476
0.83277345
0.83256316
0.83239406
0.83207011
0.83201224
0.83199829
0.83200300
0.83207911
0.83197361
0.83182335
INFO - Training [21][   40/  196]   Loss 0.432629   Top1 84.990234   Top5 98.056641   BatchTime 0.304200   LR 0.001016
0.83170485
0.83181405
0.83211392
0.83243102
0.83219910
0.83191121
0.83174932
0.83157825
0.83088708
0.83057809
0.83059257
0.83067316
0.83050454
0.83048940
0.83038521
0.83019239
0.82971853
0.82949024
0.82940394
0.82923162
0.82898384
0.82879114
INFO - Training [21][   60/  196]   Loss 0.422283   Top1 85.358073   Top5 98.164062   BatchTime 0.295479   LR 0.000996
0.82879770
0.82894534
0.82960117
0.83112109
0.83106244
0.83120561
0.83125073
0.83124977
0.83157629
0.83251679
0.83257264
0.83244962
0.83227831
0.83211613
INFO - Training [21][   80/  196]   Loss 0.422100   Top1 85.336914   Top5 98.276367   BatchTime 0.291126   LR 0.000976
0.83185786
0.83190721
0.83167493
0.83155465
0.83152539
0.83153927
0.83176255
0.83264977
0.83254361
0.83254635
0.83242565
0.83236545
0.83228046
0.83253443
0.83241296
0.83225483
0.83213216
0.83202487
0.83186972
0.83171451
0.83138192
0.83074009
0.83017808
0.83035976
INFO - Training [21][  100/  196]   Loss 0.417180   Top1 85.554688   Top5 98.320312   BatchTime 0.283950   LR 0.000957
0.83056265
0.83137757
0.83128625
0.83101863
0.83097225
0.83076245
0.83100867
0.83082670
0.83043557
0.83023959
0.83009583
0.82989001
0.82963449
0.82935292
0.82931924
0.82899755
0.82873088
0.82863563
0.82884610
0.82905686
0.82894802
0.82865715
INFO - Training [21][  120/  196]   Loss 0.410259   Top1 85.846354   Top5 98.453776   BatchTime 0.281302   LR 0.000937
0.82863683
0.82890981
0.82880622
0.82913613
0.82949793
0.82916605
0.82850850
0.82872415
0.82874513
0.82834816
0.82813287
0.82796031
0.82790095
0.82674408
INFO - Training [21][  140/  196]   Loss 0.408927   Top1 85.943080   Top5 98.535156   BatchTime 0.281416   LR 0.000918
0.82720923
0.82547903
0.82659775
0.82615817
0.82616556
0.82616782
0.82699025
0.82717609
0.82783049
0.82843471
0.82905412
0.82905591
0.82901818
0.82916766
0.82962495
0.82934946
0.82942599
0.82931525
0.82916504
0.82892948
0.82885319
0.82870305
0.82850319
INFO - Training [21][  160/  196]   Loss 0.414338   Top1 85.793457   Top5 98.518066   BatchTime 0.278754   LR 0.000899
0.82841212
0.82862693
0.82873970
0.82884127
0.82872593
0.82843709
0.82828426
0.82775551
0.82740068
0.82707673
0.82682079
0.82636583
0.82576627
0.82543612
0.82487029
0.82487839
0.82476449
0.82466471
0.82439321
0.82412606
0.82396662
0.82364184
0.82331860
INFO - Training [21][  180/  196]   Loss 0.412083   Top1 85.881076   Top5 98.470052   BatchTime 0.276007   LR 0.000879
0.82276273
0.82219100
0.82177502
0.82177353
0.82106006
0.82029116
0.81840098
0.81570667
0.81459224
0.81616706
0.81789714
0.81916505
0.82040936
0.82141024
0.82288563
INFO - ==> Top1: 85.894    Top5: 98.458    Loss: 0.411
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 0.336222   Top1 88.964844   Top5 99.589844   BatchTime 0.142575
INFO - Validation [21][   40/   40]   Loss 0.328579   Top1 89.150000   Top5 99.690000   BatchTime 0.097026
INFO - ==> Top1: 89.150    Top5: 99.690    Loss: 0.329
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 89.150   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 88.360   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 88.290   Top5: 99.620]
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0729)
features.2.conv.0 tensor(0.0223)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0758)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0473)
features.4.conv.0 tensor(0.0254)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.0856)
features.5.conv.0 tensor(0.0225)
features.5.conv.3 tensor(0.0828)
features.5.conv.6 tensor(0.0947)
features.6.conv.0 tensor(0.0311)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0677)
features.7.conv.0 tensor(0.0612)
features.7.conv.3 tensor(0.1105)
features.7.conv.6 tensor(0.1523)
features.8.conv.0 tensor(0.0585)
features.8.conv.3 tensor(0.1126)
features.8.conv.6 tensor(0.1265)
features.9.conv.0 tensor(0.0679)
features.9.conv.3 tensor(0.1493)
features.9.conv.6 tensor(0.1161)
features.10.conv.0 tensor(0.0331)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.0776)
features.11.conv.0 tensor(0.1583)
features.11.conv.3 tensor(0.1246)
features.11.conv.6 tensor(0.2378)
features.12.conv.0 tensor(0.2176)
features.12.conv.3 tensor(0.1651)
features.12.conv.6 tensor(0.5410)
features.13.conv.0 tensor(0.0896)
features.13.conv.3 tensor(0.1308)
features.13.conv.6 tensor(0.1147)
features.14.conv.0 tensor(0.9279)
features.14.conv.3 tensor(0.1059)
features.14.conv.6 tensor(0.9213)
features.15.conv.0 tensor(0.9003)
features.15.conv.3 tensor(0.0715)
features.15.conv.6 tensor(0.9502)
features.16.conv.0 tensor(0.0797)
features.16.conv.3 tensor(0.1031)
features.16.conv.6 tensor(0.2072)
conv.0 tensor(0.1757)
tensor(823761.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.82453907
0.82411057
0.82376766
0.82379806
0.82389545
0.82336718
0.82267952
0.82213718
0.82212836
0.82197076
0.82133937
0.82093072
0.82148081
0.82218170
0.82281107
0.82434666
INFO - Training [22][   20/  196]   Loss 0.423080   Top1 85.703125   Top5 97.734375   BatchTime 0.351281   LR 0.000846
0.82381928
0.82412624
0.82446253
0.82482541
0.82534939
0.82561934
0.82558757
0.82558310
0.82497001
0.82398540
0.82317597
0.82344908
0.82372350
0.82405138
0.82425457
0.82447404
0.82475418
0.82551867
0.82833678
0.83457041
0.83502007
INFO - Training [22][   40/  196]   Loss 0.424270   Top1 85.488281   Top5 98.037109   BatchTime 0.314677   LR 0.000827
0.83544260
0.83532751
0.83527094
0.83507609
0.83497339
0.83473080
0.83468854
0.83490044
0.83487141
0.83459443
0.83445346
0.83434588
0.83435601
0.83423001
0.83409363
0.83387041
0.83344412
0.83377570
0.83395725
0.83432251
0.83498579
0.83543551
0.83575690
INFO - Training [22][   60/  196]   Loss 0.422711   Top1 85.533854   Top5 98.209635   BatchTime 0.297256   LR 0.000808
0.83601797
0.83653218
0.83690441
0.83725470
0.83745867
0.83790833
0.83906078
0.83895171
0.83916318
0.83901715
0.83894277
0.83926785
0.83929580
0.83907878
0.83907485
0.83936977
0.83932930
0.83925498
0.83913219
0.83911985
0.83907914
0.83913732
INFO - Training [22][   80/  196]   Loss 0.416980   Top1 85.625000   Top5 98.286133   BatchTime 0.290330   LR 0.000789
0.83932334
0.83944243
0.83926982
0.83940643
0.83947003
0.83931619
0.83922505
0.83938819
0.83961153
0.83979464
0.83963263
0.83947235
0.83980995
0.83981055
0.83985478
INFO - Training [22][  100/  196]   Loss 0.411517   Top1 85.875000   Top5 98.359375   BatchTime 0.285297   LR 0.000770
0.83967847
0.83978230
0.83971673
0.83980995
0.83973461
0.83966714
0.83984059
0.83977640
0.83967072
0.83957529
0.83964729
0.83987588
0.83977234
0.83972460
0.83933437
0.83946979
0.83942777
0.83944792
0.83960152
0.83978754
0.83951753
0.83966309
0.83982897
INFO - Training [22][  120/  196]   Loss 0.406480   Top1 86.077474   Top5 98.453776   BatchTime 0.280873   LR 0.000752
0.84007293
0.84000450
0.83996600
0.83994728
0.83987582
0.83988750
0.83987802
0.83967245
0.83971852
0.83964556
0.83943498
0.83914572
0.83877140
0.83872807
0.83877420
0.83868104
INFO - Training [22][  140/  196]   Loss 0.403993   Top1 86.146763   Top5 98.523996   BatchTime 0.278366   LR 0.000734
0.83890933
0.83882880
0.83864951
0.83877140
0.83861798
0.83864683
0.83863431
0.83866209
0.83852673
0.83850265
0.83827883
0.83826625
0.83825713
0.83808428
0.83808774
0.83790833
0.83763194
0.83748066
0.83759415
0.83778411
0.83777022
INFO - Training [22][  160/  196]   Loss 0.406255   Top1 86.064453   Top5 98.515625   BatchTime 0.279121   LR 0.000715
0.83765531
0.83757997
0.83747745
0.83733964
0.83723354
0.83688205
0.83645076
0.83633697
0.83630037
0.83610779
0.83579797
0.83578438
0.83547592
0.83555859
0.83537728
0.83530653
0.83519858
0.83516717
0.83518046
0.83536047
0.83525592
0.83579493
INFO - Training [22][  180/  196]   Loss 0.406770   Top1 86.054688   Top5 98.470052   BatchTime 0.279046   LR 0.000697
0.83721215
0.83710116
0.83718884
0.83796656
0.83776593
0.83744049
0.83708328
0.83661360
0.83627981
0.83572531
0.83547705
0.83523214
0.83503127
0.83474320
INFO - ==> Top1: 86.066    Top5: 98.470    Loss: 0.406
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83435315
0.83423251
0.83394289
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.342236   Top1 88.574219   Top5 99.472656   BatchTime 0.120916
INFO - Validation [22][   40/   40]   Loss 0.336802   Top1 88.620000   Top5 99.590000   BatchTime 0.095986
INFO - ==> Top1: 88.620    Top5: 99.590    Loss: 0.337
INFO - ==> Sparsity : 0.372
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 89.150   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.620   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 88.360   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.2207)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0767)
features.3.conv.0 tensor(0.0136)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0438)
features.4.conv.0 tensor(0.0295)
features.4.conv.3 tensor(0.0990)
features.4.conv.6 tensor(0.0853)
features.5.conv.0 tensor(0.0275)
features.5.conv.3 tensor(0.0828)
features.5.conv.6 tensor(0.0929)
features.6.conv.0 tensor(0.0260)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0680)
features.7.conv.0 tensor(0.0567)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1555)
features.8.conv.0 tensor(0.0603)
features.8.conv.3 tensor(0.1198)
features.8.conv.6 tensor(0.1206)
features.9.conv.0 tensor(0.0684)
features.9.conv.3 tensor(0.1458)
features.9.conv.6 tensor(0.1820)
features.10.conv.0 tensor(0.0310)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0812)
features.11.conv.0 tensor(0.1607)
features.11.conv.3 tensor(0.1283)
features.11.conv.6 tensor(0.2652)
features.12.conv.0 tensor(0.2333)
features.12.conv.3 tensor(0.1651)
features.12.conv.6 tensor(0.2936)
features.13.conv.0 tensor(0.0866)
features.13.conv.3 tensor(0.1304)
features.13.conv.6 tensor(0.1060)
features.14.conv.0 tensor(0.9285)
features.14.conv.3 tensor(0.1057)
features.14.conv.6 tensor(0.9261)
features.15.conv.0 tensor(0.8724)
features.15.conv.3 tensor(0.0691)
features.15.conv.6 tensor(0.9508)
features.16.conv.0 tensor(0.0794)
features.16.conv.3 tensor(0.1012)
features.16.conv.6 tensor(0.2179)
conv.0 tensor(0.1771)
tensor(813657.) 2188896.0
0.83378625
0.83342910
0.83297443
0.83235073
0.83185726
0.83134794
0.83124501
0.83071756
0.83033806
0.83020234
0.82936341
0.82866341
0.82834411
0.82832187
0.82747191
0.82793111
0.82851499
0.82961494
INFO - Training [23][   20/  196]   Loss 0.413587   Top1 85.898438   Top5 97.832031   BatchTime 0.385578   LR 0.000666
0.83100331
0.83128184
0.83143842
0.83120400
0.83114678
0.83105379
0.83081877
0.83031094
0.83012879
0.83015174
0.83000433
0.82984477
0.82964998
0.82931745
0.82863492
0.82816637
0.82774711
0.82731193
INFO - Training [23][   40/  196]   Loss 0.423737   Top1 85.468750   Top5 98.105469   BatchTime 0.305017   LR 0.000648
0.82716054
0.82684910
0.82652944
0.82628810
0.82604104
0.82614022
0.82654691
0.82920295
0.83011585
0.83023328
0.82980686
0.82966310
0.82955414
0.82937294
0.82899606
0.82872570
0.82837874
0.82830292
0.82824421
0.82826042
0.82799768
0.82801408
0.82809520
0.82797247
0.82813984
0.82821709
INFO - Training [23][   60/  196]   Loss 0.409521   Top1 85.963542   Top5 98.242188   BatchTime 0.277967   LR 0.000630
0.82816774
0.82815689
0.82852334
0.82859790
0.82884443
0.82880008
0.82862055
0.82846391
0.82837451
0.82839125
0.82832313
0.82950312
0.83027869
0.82982039
INFO - Training [23][   80/  196]   Loss 0.404820   Top1 86.201172   Top5 98.383789   BatchTime 0.279870   LR 0.000613
0.83042246
0.83047229
0.83033782
0.83033597
0.83025622
0.82984799
0.82953197
0.82913381
0.82876045
0.82851946
0.82833236
0.82812142
0.82782793
0.82743198
0.82720828
0.82683712
0.82656366
0.82606959
0.82576227
0.82549775
0.82524627
0.82497168
0.82480359
INFO - Training [23][  100/  196]   Loss 0.398412   Top1 86.386719   Top5 98.417969   BatchTime 0.275686   LR 0.000596
0.82467043
0.82455611
0.82436353
0.82421464
0.82403207
0.82382452
0.82368505
0.82360470
0.82353419
0.82333463
0.82315785
0.82304651
0.82295215
0.82287091
0.82278109
0.82278943
INFO - Training [23][  120/  196]   Loss 0.391153   Top1 86.624349   Top5 98.505859   BatchTime 0.271408   LR 0.000579
0.82275325
0.82279789
0.82303834
0.82330900
0.82384717
0.82413143
0.82397753
0.82390070
0.82379228
0.82387137
0.82386732
0.82369101
0.82357836
0.82349712
0.82343596
0.82340765
0.82344031
0.82335407
0.82326543
0.82323408
0.82332939
0.82321370
0.82318968
INFO - Training [23][  140/  196]   Loss 0.390659   Top1 86.665737   Top5 98.529576   BatchTime 0.269663   LR 0.000562
0.82316929
0.82313186
0.82306522
0.82306105
0.82311779
0.82462120
0.82462430
0.82455736
0.82421535
0.82404315
0.82391024
0.82344460
0.82293093
0.82258219
0.82213610
0.82189250
0.82132888
0.82071930
0.82044041
0.81911218
0.81769216
0.81484181
0.80881113
INFO - Training [23][  160/  196]   Loss 0.391206   Top1 86.635742   Top5 98.535156   BatchTime 0.269450   LR 0.000545
0.80646312
0.80111384
0.80244154
0.80449253
0.80482620
0.80737823
0.80673546
0.80898511
0.80641532
0.80884409
0.81161845
0.81296498
0.81372923
0.81541991
0.81733245
0.81917381
INFO - Training [23][  180/  196]   Loss 0.390656   Top1 86.655816   Top5 98.483073   BatchTime 0.267044   LR 0.000529
0.82064283
0.82185894
0.82341623
0.82471299
0.82561612
0.82610947
0.82670075
0.82717216
0.82744288
0.82748532
0.82753807
0.82736272
0.82722765
0.82758886
INFO - ==> Top1: 86.670    Top5: 98.510    Loss: 0.390
0.82801414
0.82831848
0.82826275
0.82840884
0.82847744
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.326029   Top1 89.726562   Top5 99.687500   BatchTime 0.127063
INFO - Validation [23][   40/   40]   Loss 0.312902   Top1 89.820000   Top5 99.760000   BatchTime 0.091197
INFO - ==> Top1: 89.820    Top5: 99.760    Loss: 0.313
INFO - ==> Sparsity : 0.366
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 89.820   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 89.150   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.620   Top5: 99.590]
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.2227)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0629)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0747)
features.3.conv.0 tensor(0.0142)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0434)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0858)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0845)
features.5.conv.6 tensor(0.0936)
features.6.conv.0 tensor(0.0286)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0679)
features.7.conv.0 tensor(0.0584)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.1595)
features.8.conv.0 tensor(0.0588)
features.8.conv.3 tensor(0.1143)
features.8.conv.6 tensor(0.1213)
features.9.conv.0 tensor(0.0592)
features.9.conv.3 tensor(0.1464)
features.9.conv.6 tensor(0.1426)
features.10.conv.0 tensor(0.0345)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0862)
features.11.conv.0 tensor(0.1712)
features.11.conv.3 tensor(0.1267)
features.11.conv.6 tensor(0.3005)
features.12.conv.0 tensor(0.2125)
features.12.conv.3 tensor(0.1665)
features.12.conv.6 tensor(0.4075)
features.13.conv.0 tensor(0.0961)
features.13.conv.3 tensor(0.1289)
features.13.conv.6 tensor(0.0958)
features.14.conv.0 tensor(0.9297)
features.14.conv.3 tensor(0.1071)
features.14.conv.6 tensor(0.9292)
features.15.conv.0 tensor(0.7674)
features.15.conv.3 tensor(0.0612)
features.15.conv.6 tensor(0.9472)
features.16.conv.0 tensor(0.0795)
features.16.conv.3 tensor(0.1017)
features.16.conv.6 tensor(0.2092)
conv.0 tensor(0.1763)
tensor(801055.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.82842022
0.82807785
0.82750601
0.82689035
0.82635182
0.82582444
0.82554895
0.82640594
0.82609713
0.82528514
0.82474685
0.82413447
0.82395375
0.82386631
0.82334584
0.82338005
0.82313973
0.82312256
0.82300299
0.82261503
0.82255381
INFO - Training [24][   20/  196]   Loss 0.407303   Top1 85.761719   Top5 98.066406   BatchTime 0.367002   LR 0.000500
0.82231891
0.82210153
0.82205129
0.82182997
0.82216215
0.82250053
0.82220191
0.82212102
0.82231218
0.82223368
0.82205248
0.82249588
0.82243103
0.82254863
0.82200760
0.82150608
0.82102066
0.82053059
0.81960416
0.81809467
0.81706834
INFO - Training [24][   40/  196]   Loss 0.407494   Top1 86.035156   Top5 98.300781   BatchTime 0.326375   LR 0.000484
0.81621224
0.81392741
0.81204110
0.81334090
0.81525338
0.81727654
0.81849360
0.81938744
0.82004929
0.82043278
0.82078362
0.82105625
0.82099062
0.82096165
0.82043672
0.81949371
0.81960863
0.81925637
0.81934422
INFO - Training [24][   60/  196]   Loss 0.402674   Top1 86.256510   Top5 98.450521   BatchTime 0.321941   LR 0.000468
0.81965399
0.81944692
0.81931901
0.81915152
0.81903893
0.81896484
0.81896752
0.81900609
0.81907153
0.81906861
0.81891543
0.81885666
0.81879377
0.81853116
0.81815577
0.81784958
0.81777179
0.81755257
0.81733590
0.81711656
0.81687558
INFO - Training [24][   80/  196]   Loss 0.399668   Top1 86.440430   Top5 98.530273   BatchTime 0.313773   LR 0.000453
0.81682819
0.81679231
0.81677186
0.81680590
0.81659621
0.81629866
0.81602573
0.81574762
0.81558836
0.81538457
0.81535488
0.81530750
0.81512403
0.81504029
0.81480527
0.81444603
INFO - Training [24][  100/  196]   Loss 0.389274   Top1 86.816406   Top5 98.605469   BatchTime 0.301931   LR 0.000437
0.81429279
0.81585157
0.81564969
0.81542206
0.81500405
0.81458461
0.81419539
0.81384188
0.81331462
0.81291085
0.81273627
0.81247759
0.81218827
0.81199861
0.81199151
0.81202745
0.81199592
0.81193066
0.81179762
0.81168294
0.81166244
0.81176823
0.81198871
INFO - Training [24][  120/  196]   Loss 0.379405   Top1 87.167969   Top5 98.701172   BatchTime 0.295493   LR 0.000422
0.81214255
0.81229514
0.81433123
0.81378925
0.81405109
0.81409907
0.81418532
0.81439567
0.81457996
0.81477809
0.81535524
0.81763142
0.81992024
0.82042974
0.82268250
INFO - Training [24][  140/  196]   Loss 0.375993   Top1 87.234933   Top5 98.761161   BatchTime 0.290947   LR 0.000407
0.82258022
0.82248712
0.82243741
0.82235044
0.82222271
0.82198828
0.82178229
0.82158148
0.82134241
0.82107121
0.82092208
0.82086629
0.82078898
0.82080513
0.82072043
0.82057709
0.82053083
0.82056993
0.82057309
0.82052845
0.82070118
0.82072604
INFO - Training [24][  160/  196]   Loss 0.378885   Top1 87.109375   Top5 98.735352   BatchTime 0.288695   LR 0.000392
0.82067937
0.82063276
0.82062685
0.82049018
0.82045329
0.82041025
0.82042873
0.82051212
0.82055020
0.82060754
0.82057828
0.82068151
0.82079655
0.82071316
0.82064664
0.82069856
0.82082415
0.82097983
0.82105231
0.82117587
0.82097501
0.82105231
INFO - Training [24][  180/  196]   Loss 0.378094   Top1 87.141927   Top5 98.661024   BatchTime 0.287357   LR 0.000378
0.82093167
0.82089877
0.82077748
0.82059717
0.82047659
0.82051200
0.82041234
0.82055765
0.82059133
0.82041174
0.82019335
0.82004887
0.81985140
0.81965613
INFO - ==> Top1: 87.118    Top5: 98.664    Loss: 0.378
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.81950808
0.81941390
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 0.312466   Top1 90.214844   Top5 99.746094   BatchTime 0.128536
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0612)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.0718)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0434)
features.4.conv.0 tensor(0.0269)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0848)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0942)
features.6.conv.0 tensor(0.0278)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0663)
features.7.conv.0 tensor(0.0592)
features.7.conv.3 tensor(0.1097)
features.7.conv.6 tensor(0.1603)
features.8.conv.0 tensor(0.0643)
features.8.conv.3 tensor(0.1120)
features.8.conv.6 tensor(0.1223)
features.9.conv.0 tensor(0.0629)
features.9.conv.3 tensor(0.1429)
features.9.conv.6 tensor(0.2728)
features.10.conv.0 tensor(0.0355)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.0974)
features.11.conv.0 tensor(0.1810)
features.11.conv.3 tensor(0.1294)
features.11.conv.6 tensor(0.2923)
features.12.conv.0 tensor(0.2302)
features.12.conv.3 tensor(0.1645)
features.12.conv.6 tensor(0.4266)
features.13.conv.0 tensor(0.0962)
features.13.conv.3 tensor(0.1287)
features.13.conv.6 tensor(0.1156)
features.14.conv.0 tensor(0.9282)
features.14.conv.3 tensor(0.1052)
features.14.conv.6 tensor(0.9359)
features.15.conv.0 tensor(0.8324)
features.15.conv.3 tensor(0.0659)
features.15.conv.6 tensor(0.9494)
features.16.conv.0 tensor(0.0768)
features.16.conv.3 tensor(0.0995)
features.16.conv.6 tensor(0.2211)
conv.0 tensor(0.1717)
tensor(821355.) 2188896.0
INFO - Validation [24][   40/   40]   Loss 0.301069   Top1 90.270000   Top5 99.790000   BatchTime 0.092797
INFO - ==> Top1: 90.270    Top5: 99.790    Loss: 0.301
INFO - ==> Sparsity : 0.375
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 89.820   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 89.150   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.81948280
0.81965142
0.81953561
0.81938010
0.81936961
0.81916225
0.81913590
0.81880760
0.81863827
0.81850064
0.81836849
0.81828076
0.81797719
0.81792259
0.81794727
0.81786138
0.81781238
0.81789470
INFO - Training [25][   20/  196]   Loss 0.390985   Top1 86.269531   Top5 98.300781   BatchTime 0.345889   LR 0.000353
0.81817037
0.81986129
0.81975758
0.81965613
0.81943309
0.81938958
0.81934863
0.81924880
0.81925058
0.81910568
0.81908458
0.81896728
0.81898928
0.81912112
0.81914598
0.81917143
0.81935287
0.81945121
0.81943268
0.81933880
0.81924564
0.81913513
0.81914914
INFO - Training [25][   40/  196]   Loss 0.384270   Top1 86.708984   Top5 98.447266   BatchTime 0.303847   LR 0.000339
0.81883860
0.81872845
0.81860811
0.81848842
0.81831545
0.81816304
0.81797165
0.81782061
0.81766474
0.81749648
0.81741971
0.81739682
0.81709284
0.81688231
0.81656927
0.81608236
0.81562972
INFO - Training [25][   60/  196]   Loss 0.380029   Top1 86.992188   Top5 98.587240   BatchTime 0.281664   LR 0.000325
0.81529516
0.81506300
0.81484908
0.81464982
0.81445670
0.81426960
0.81397462
0.81372768
0.81344008
0.81293237
0.81269175
0.81240880
0.81212300
0.81179106
0.81139308
0.81119514
0.81089234
0.81050134
0.81017560
0.80979258
0.80951542
0.80924988
INFO - Training [25][   80/  196]   Loss 0.380224   Top1 86.918945   Top5 98.637695   BatchTime 0.279023   LR 0.000312
0.80903858
0.80894613
0.80897218
0.80893761
0.80907542
0.80909085
0.80904251
0.80913073
0.80936062
0.81181067
0.81202698
0.81219357
0.81219625
0.81208152
0.81200737
0.81206840
0.81211340
0.81190908
0.81180340
0.81177342
0.81169361
0.81169122
INFO - Training [25][  100/  196]   Loss 0.377583   Top1 87.007812   Top5 98.625000   BatchTime 0.280305   LR 0.000299
0.81153351
0.81133187
0.81133932
0.81114537
0.81111944
0.81090420
0.81089449
0.81074214
0.81073928
0.81064010
0.81067926
0.81072271
0.81087500
0.81098872
0.81067973
INFO - Training [25][  120/  196]   Loss 0.371706   Top1 87.236328   Top5 98.678385   BatchTime 0.277840   LR 0.000286
0.81041843
0.81029123
0.81018907
0.81009728
0.81001800
0.80991369
0.80997270
0.80996186
0.80985117
0.80967104
0.80949795
0.80919129
0.80874521
0.80870920
0.80854452
0.80850840
0.80842942
0.80827254
0.80806571
INFO - Training [25][  140/  196]   Loss 0.368367   Top1 87.279576   Top5 98.752790   BatchTime 0.280284   LR 0.000273
0.80777985
0.80745542
0.80736214
0.80736530
0.80716795
0.80710131
0.80704248
0.80707687
0.80699086
0.80686033
0.80664855
0.80655324
0.80648929
0.80644453
0.80827677
0.80819994
0.80812883
0.80802929
0.80784249
0.80786169
0.80788255
0.80781430
0.80756617
0.80719894
0.80701131
INFO - Training [25][  160/  196]   Loss 0.368929   Top1 87.287598   Top5 98.718262   BatchTime 0.276643   LR 0.000261
0.80691808
0.80681038
0.80668062
0.80661613
0.80653834
0.80647200
0.80634594
0.80627519
0.80631846
0.80637407
0.80640560
0.80643129
0.80643362
0.80643278
0.80642110
INFO - Training [25][  180/  196]   Loss 0.367829   Top1 87.330729   Top5 98.652344   BatchTime 0.276350   LR 0.000248
0.80639958
0.80642080
0.80649906
0.80652684
0.80653894
0.80646664
0.80640697
0.80637038
0.80620903
0.80610687
0.80590051
0.80563176
0.80538982
0.80518860
0.80510569
0.80509830
0.80506772
0.80520529
0.80534983
0.80554414
INFO - ==> Top1: 87.362    Top5: 98.684    Loss: 0.367
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [25][   20/   40]   Loss 0.305875   Top1 89.882812   Top5 99.667969   BatchTime 0.131400
INFO - Validation [25][   40/   40]   Loss 0.292780   Top1 90.030000   Top5 99.720000   BatchTime 0.091943
INFO - ==> Top1: 90.030    Top5: 99.720    Loss: 0.293
INFO - ==> Sparsity : 0.388
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 90.030   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 89.820   Top5: 99.760]
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.2402)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0629)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0720)
features.3.conv.0 tensor(0.0148)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0456)
features.4.conv.0 tensor(0.0295)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0861)
features.5.conv.0 tensor(0.0319)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0939)
features.6.conv.0 tensor(0.0312)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0658)
features.7.conv.0 tensor(0.0548)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.1607)
features.8.conv.0 tensor(0.0675)
features.8.conv.3 tensor(0.1128)
features.8.conv.6 tensor(0.1315)
features.9.conv.0 tensor(0.0673)
features.9.conv.3 tensor(0.1447)
features.9.conv.6 tensor(0.2439)
features.10.conv.0 tensor(0.0350)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.0922)
features.11.conv.0 tensor(0.1944)
features.11.conv.3 tensor(0.1281)
features.11.conv.6 tensor(0.3834)
features.12.conv.0 tensor(0.3385)
features.12.conv.3 tensor(0.1626)
features.12.conv.6 tensor(0.4268)
features.13.conv.0 tensor(0.0971)
features.13.conv.3 tensor(0.1289)
features.13.conv.6 tensor(0.2723)
features.14.conv.0 tensor(0.9291)
features.14.conv.3 tensor(0.1046)
features.14.conv.6 tensor(0.9405)
features.15.conv.0 tensor(0.8350)
features.15.conv.3 tensor(0.0655)
features.15.conv.6 tensor(0.9547)
features.16.conv.0 tensor(0.0755)
features.16.conv.3 tensor(0.0995)
features.16.conv.6 tensor(0.2216)
conv.0 tensor(0.1717)
tensor(849108.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
0.80571848
0.80572182
0.80559123
0.80560380
0.80570489
0.80578840
0.80754930
0.80792338
0.80757576
0.80738860
0.80734533
0.80733430
0.80734146
0.80731678
0.80742478
0.80755186
0.80768764
0.80786836
INFO - Training [26][   20/  196]   Loss 0.394857   Top1 86.933594   Top5 97.988281   BatchTime 0.365177   LR 0.000228
0.80799204
0.80788404
0.80774105
0.80770248
0.80767858
0.80765975
0.80759138
0.80750179
0.80751950
0.80762160
0.80757213
0.80737442
0.80738789
0.80738455
0.80729651
0.80694610
0.80684268
0.80678391
0.80674160
0.80664110
0.80656189
0.80672646
0.80683351
0.80665976
0.80669111
INFO - Training [26][   40/  196]   Loss 0.378433   Top1 87.207031   Top5 98.437500   BatchTime 0.301298   LR 0.000216
0.80678946
0.80689865
0.80687219
0.80671805
0.80655158
0.80641794
0.80655098
0.80645138
0.80645573
0.80640054
0.80620056
0.80589128
0.80596089
0.80603838
0.80580449
INFO - Training [26][   60/  196]   Loss 0.373399   Top1 87.317708   Top5 98.515625   BatchTime 0.285093   LR 0.000205
0.80597562
0.80598438
0.80598742
0.80608332
0.80616766
0.80591798
0.80572641
0.80564934
0.80530113
0.80490160
0.80484056
0.80478042
0.80468589
0.80442673
0.80423224
0.80405086
0.80403239
0.80393231
0.80384320
0.80372250
0.80363947
0.80373496
0.80382550
0.80352217
INFO - Training [26][   80/  196]   Loss 0.371193   Top1 87.290039   Top5 98.681641   BatchTime 0.277612   LR 0.000194
0.80337006
0.80367887
0.80388880
0.80391514
0.80364448
0.80358517
0.80358052
0.80355579
0.80354029
0.80351597
0.80340213
0.80322731
0.80325764
0.80328518
0.80327243
0.80322337
0.80317199
0.80311328
0.80322695
INFO - Training [26][  100/  196]   Loss 0.365666   Top1 87.449219   Top5 98.675781   BatchTime 0.283615   LR 0.000183
0.80329323
0.80300826
0.80284405
0.80275828
0.80268019
0.80263096
0.80258846
0.80267590
0.80267835
0.80259854
0.80256408
0.80253416
0.80243224
0.80234253
0.80216640
0.80190438
0.80161196
0.80108124
0.80080479
0.80079609
0.80089599
INFO - Training [26][  120/  196]   Loss 0.367971   Top1 87.337240   Top5 98.746745   BatchTime 0.283900   LR 0.000173
0.80088371
0.80105054
0.80258244
0.80259496
0.80252284
0.80236191
0.80223626
0.80217367
0.80210954
0.80208153
0.80203301
0.80203331
0.80204582
0.80207330
INFO - Training [26][  140/  196]   Loss 0.369001   Top1 87.307478   Top5 98.780692   BatchTime 0.284217   LR 0.000163
0.80200452
0.80194223
0.80196470
0.80198020
0.80197531
0.80195832
0.80187941
0.80182558
0.80182225
0.80172104
0.80168116
0.80155182
0.80150187
0.80142254
0.80142629
0.80155325
0.80154890
0.80163115
0.80155689
0.80159217
0.80160379
0.80165756
0.80174279
0.80188686
0.80203658
0.80221409
INFO - Training [26][  160/  196]   Loss 0.368994   Top1 87.290039   Top5 98.774414   BatchTime 0.287447   LR 0.000153
0.80233639
0.80235469
0.80233729
0.80230480
0.80222499
0.80218607
0.80223876
0.80226797
0.80237430
0.80235273
0.80238646
0.80247509
0.80244201
0.80241019
0.80239344
INFO - Training [26][  180/  196]   Loss 0.367833   Top1 87.352431   Top5 98.745660   BatchTime 0.285465   LR 0.000144
0.80234975
0.80231106
0.80229568
0.80231607
0.80230016
0.80221653
0.80225307
0.80241370
0.80232513
0.80230010
0.80241710
0.80234313
0.80231065
0.80235678
0.80235320
0.80240095
0.80241942
0.80237925
0.80229610
********************pre-trained*****************
INFO - ==> Top1: 87.438    Top5: 98.736    Loss: 0.366
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.302421   Top1 90.058594   Top5 99.648438   BatchTime 0.129369
INFO - Validation [26][   40/   40]   Loss 0.290327   Top1 90.300000   Top5 99.730000   BatchTime 0.090443
INFO - ==> Top1: 90.300    Top5: 99.730    Loss: 0.290
INFO - ==> Sparsity : 0.396
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.030   Top5: 99.720]
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.2383)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0738)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0451)
features.4.conv.0 tensor(0.0295)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0856)
features.5.conv.0 tensor(0.0306)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0929)
features.6.conv.0 tensor(0.0311)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0661)
features.7.conv.0 tensor(0.0539)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.1645)
features.8.conv.0 tensor(0.0651)
features.8.conv.3 tensor(0.1126)
features.8.conv.6 tensor(0.1321)
features.9.conv.0 tensor(0.0691)
features.9.conv.3 tensor(0.1421)
features.9.conv.6 tensor(0.2066)
features.10.conv.0 tensor(0.0365)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.0945)
features.11.conv.0 tensor(0.2007)
features.11.conv.3 tensor(0.1273)
features.11.conv.6 tensor(0.3527)
features.12.conv.0 tensor(0.3843)
features.12.conv.3 tensor(0.1636)
features.12.conv.6 tensor(0.4448)
features.13.conv.0 tensor(0.0958)
features.13.conv.3 tensor(0.1289)
features.13.conv.6 tensor(0.2141)
features.14.conv.0 tensor(0.9296)
features.14.conv.3 tensor(0.1047)
features.14.conv.6 tensor(0.9399)
features.15.conv.0 tensor(0.8370)
features.15.conv.3 tensor(0.0647)
features.15.conv.6 tensor(0.9552)
features.16.conv.0 tensor(0.0773)
features.16.conv.3 tensor(0.1005)
features.16.conv.6 tensor(0.2746)
conv.0 tensor(0.1819)
tensor(866208.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
0.80215341
0.80192930
0.80183786
0.80179155
0.80166775
0.80154186
0.80144787
0.80128211
0.80117822
0.80097497
0.80073291
0.80046588
0.80019736
0.79990220
0.79974848
0.79967386
0.79957896
0.79946858
0.79940218
0.79934728
0.79932630
0.79932934
INFO - Training [27][   20/  196]   Loss 0.374154   Top1 87.246094   Top5 97.890625   BatchTime 0.334345   LR 0.000128
0.79932564
0.79926503
0.79908407
0.79891461
0.79873347
0.79852533
0.79834872
0.79804903
0.79779190
0.79763758
0.79746276
0.79728115
0.79721147
0.79705656
0.79692900
0.79669827
INFO - Training [27][   40/  196]   Loss 0.379231   Top1 87.080078   Top5 98.212891   BatchTime 0.295548   LR 0.000119
0.79648179
0.79621983
0.79594034
0.79550320
0.79509419
0.79485422
0.79446560
0.79413056
0.79393506
0.79379284
0.79357666
0.79321605
0.79294455
0.79243916
0.79173017
0.79104984
0.79025203
0.78936708
0.78886759
0.78916669
0.78947526
0.78979480
INFO - Training [27][   60/  196]   Loss 0.369958   Top1 87.441406   Top5 98.385417   BatchTime 0.287467   LR 0.000111
0.78992522
0.78990608
0.79028398
0.79073167
0.79113632
0.79111362
0.79127479
0.79139256
0.79158425
0.79184967
0.79205781
0.79208535
0.79201609
0.79189879
0.79179382
0.79147619
INFO - Training [27][   80/  196]   Loss 0.367926   Top1 87.412109   Top5 98.476562   BatchTime 0.279664   LR 0.000102
0.79114604
0.79076159
0.79050416
0.79017133
0.78996867
0.78970313
0.78953207
0.78948176
0.78948349
0.78964764
0.78974670
0.78943592
0.78924614
0.78911537
0.78886455
0.78876740
0.78873032
0.78882313
0.78867614
0.78847784
0.78807044
0.78771597
INFO - Training [27][  100/  196]   Loss 0.363435   Top1 87.527344   Top5 98.570312   BatchTime 0.277496   LR 0.000095
0.78741908
0.78705019
0.78680331
0.78685105
0.78677940
0.78667110
0.78650510
0.78660470
0.78655201
0.78654063
0.78648621
0.78665155
0.78671354
0.78640532
0.78619319
0.78601730
0.78563237
0.78527826
0.78489405
0.78472888
0.78437603
0.78406006
INFO - Training [27][  120/  196]   Loss 0.356313   Top1 87.776693   Top5 98.619792   BatchTime 0.275998   LR 0.000087
0.78367335
0.78366363
0.78350985
0.78356320
0.78352177
0.78339803
0.78344351
0.78364295
0.78396231
0.78409261
0.78412527
0.78443491
0.78467619
0.78514171
0.78535283
INFO - Training [27][  140/  196]   Loss 0.354112   Top1 87.876674   Top5 98.702567   BatchTime 0.274173   LR 0.000080
0.78569835
0.78590935
0.78619266
0.78644997
0.78631300
0.78618276
0.78601402
0.78588992
0.78592479
0.78588533
0.78594321
0.78604913
0.78596675
0.78575355
0.78559238
0.78540164
0.78532535
0.78531188
0.78525561
0.78517032
0.78519750
0.78511924
0.78530574
0.78542560
INFO - Training [27][  160/  196]   Loss 0.359252   Top1 87.651367   Top5 98.669434   BatchTime 0.271381   LR 0.000073
0.78557462
0.78555638
0.78541881
0.78526092
0.78526634
0.78530973
0.78521878
0.78505361
0.78489822
0.78480226
0.78469646
0.78459984
0.78458965
0.78461885
0.78450578
0.78436249
0.78416556
INFO - Training [27][  180/  196]   Loss 0.360137   Top1 87.645399   Top5 98.602431   BatchTime 0.267814   LR 0.000066
0.78404498
0.78384829
0.78359884
0.78333992
0.78315365
0.78311986
0.78308642
0.78297067
0.78303468
0.78325063
0.78345311
0.78358942
0.78365004
0.78377795
0.78373802
0.78376406
0.78383487
0.78383267
0.78371209
0.78347969
********************pre-trained*****************
INFO - ==> Top1: 87.762    Top5: 98.622    Loss: 0.357
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [27][   20/   40]   Loss 0.355331   Top1 88.535156   Top5 99.472656   BatchTime 0.184143
INFO - Validation [27][   40/   40]   Loss 0.345953   Top1 88.720000   Top5 99.570000   BatchTime 0.121616
INFO - ==> Top1: 88.720    Top5: 99.570    Loss: 0.346
INFO - ==> Sparsity : 0.443
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.030   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0744)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0458)
features.4.conv.0 tensor(0.0301)
features.4.conv.3 tensor(0.0990)
features.4.conv.6 tensor(0.0845)
features.5.conv.0 tensor(0.0304)
features.5.conv.3 tensor(0.0839)
features.5.conv.6 tensor(0.1003)
features.6.conv.0 tensor(0.0311)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0662)
features.7.conv.0 tensor(0.0544)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.1651)
features.8.conv.0 tensor(0.0666)
features.8.conv.3 tensor(0.1131)
features.8.conv.6 tensor(0.1432)
features.9.conv.0 tensor(0.0700)
features.9.conv.3 tensor(0.1415)
features.9.conv.6 tensor(0.2088)
features.10.conv.0 tensor(0.0365)
features.10.conv.3 tensor(0.0929)
features.10.conv.6 tensor(0.0944)
features.11.conv.0 tensor(0.2016)
features.11.conv.3 tensor(0.1273)
features.11.conv.6 tensor(0.3622)
features.12.conv.0 tensor(0.4095)
features.12.conv.3 tensor(0.1615)
features.12.conv.6 tensor(0.4553)
features.13.conv.0 tensor(0.0966)
features.13.conv.3 tensor(0.1283)
features.13.conv.6 tensor(0.2676)
features.14.conv.0 tensor(0.9300)
features.14.conv.3 tensor(0.1046)
features.14.conv.6 tensor(0.9391)
features.15.conv.0 tensor(0.8380)
features.15.conv.3 tensor(0.0637)
features.15.conv.6 tensor(0.9575)
features.16.conv.0 tensor(0.0784)
features.16.conv.3 tensor(0.1015)
features.16.conv.6 tensor(0.5697)
conv.0 tensor(0.1918)
tensor(969470.) 2188896.0
0.78352296
0.78359425
0.78349620
0.78361374
0.78362155
0.78361917
0.78363264
0.78351820
0.78340012
0.78330964
0.78331667
0.78320223
0.78325385
0.78333610
0.78340369
0.78339845
0.78331441
0.78324050
0.78321433
0.78325039
INFO - Training [28][   20/  196]   Loss 0.371711   Top1 87.207031   Top5 98.242188   BatchTime 0.348287   LR 0.000055
0.78321797
0.78322011
0.78320104
0.78312355
0.78309107
0.78305250
0.78298849
0.78291458
0.78280002
0.78273839
0.78266007
0.78254998
0.78240854
0.78235269
0.78229129
0.78220773
0.78209496
0.78194755
0.78177530
0.78163904
0.78152341
INFO - Training [28][   40/  196]   Loss 0.367309   Top1 87.343750   Top5 98.476562   BatchTime 0.315638   LR 0.000050
0.78150922
0.78152341
0.78155428
0.78155708
0.78157049
0.78164852
0.78171104
0.78171837
0.78166682
0.78164327
0.78156561
0.78150040
0.78140318
0.78133696
0.78124720
0.78113908
INFO - Training [28][   60/  196]   Loss 0.355294   Top1 87.597656   Top5 98.593750   BatchTime 0.295448   LR 0.000044
0.78104317
0.78089231
0.78084856
0.78081942
0.78081930
0.78075135
0.78072596
0.78064907
0.78053981
0.78045356
0.78034222
0.78027016
0.78021193
0.78012246
0.78012151
0.78012031
0.78014421
0.78016573
0.78020787
0.78024000
0.78027374
0.78030384
0.78045112
0.78053069
INFO - Training [28][   80/  196]   Loss 0.354364   Top1 87.680664   Top5 98.676758   BatchTime 0.284002   LR 0.000039
0.78062844
0.78070694
0.78069228
0.78070265
0.78064501
0.78066736
0.78058857
0.78051788
0.78050339
0.78047556
0.78043383
0.78042358
0.78040463
0.78035021
0.78034180
INFO - Training [28][  100/  196]   Loss 0.349950   Top1 87.839844   Top5 98.718750   BatchTime 0.279654   LR 0.000034
0.78039306
0.78042883
0.78045768
0.78051782
0.78055263
0.78062111
0.78067946
0.78073794
0.78075552
0.78077573
0.78081053
0.78085774
0.78085858
0.78089386
0.78089935
0.78089678
0.78091496
0.78088063
0.78088361
0.78093427
0.78093874
0.78097123
0.78103024
INFO - Training [28][  120/  196]   Loss 0.345348   Top1 88.053385   Top5 98.740234   BatchTime 0.276402   LR 0.000030
0.78108758
0.78113657
0.78116405
0.78120404
0.78124762
0.78123546
0.78120714
0.78116816
0.78120512
0.78125054
0.78126538
0.78129554
0.78129417
0.78126317
0.78123647
0.78119659
INFO - Training [28][  140/  196]   Loss 0.346306   Top1 88.077567   Top5 98.794643   BatchTime 0.272902   LR 0.000026
0.78116393
0.78117597
0.78116965
0.78118998
0.78122878
0.78128183
0.78133255
0.78134781
0.78136081
0.78138036
0.78140295
0.78144127
0.78146708
0.78149956
0.78150994
0.78151119
0.78155994
0.78158885
0.78165728
0.78171211
0.78174818
0.78180540
0.78186083
0.78193051
INFO - Training [28][  160/  196]   Loss 0.351854   Top1 87.897949   Top5 98.786621   BatchTime 0.271112   LR 0.000022
0.78202838
0.78210384
0.78218532
0.78223443
0.78229582
0.78233397
0.78234291
0.78234953
0.78237146
0.78236485
0.78235775
0.78234643
0.78233743
0.78235394
0.78237057
0.78233546
0.78230041
0.78223842
0.78221291
0.78217095
0.78212881
0.78209770
0.78209984
INFO - Training [28][  180/  196]   Loss 0.351856   Top1 87.881944   Top5 98.708767   BatchTime 0.269441   LR 0.000018
0.78209025
0.78206897
0.78206748
0.78202820
0.78200293
0.78196728
0.78191072
0.78185028
0.78176916
0.78169495
0.78165913
0.78163040
0.78162503
0.78159827
INFO - ==> Top1: 87.944    Top5: 98.702    Loss: 0.351
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.320547   Top1 89.785156   Top5 99.609375   BatchTime 0.129634
INFO - Validation [28][   40/   40]   Loss 0.305945   Top1 90.090000   Top5 99.710000   BatchTime 0.090307
INFO - ==> Top1: 90.090    Top5: 99.710    Loss: 0.306
INFO - ==> Sparsity : 0.446
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 90.090   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.2383)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0738)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0464)
features.4.conv.0 tensor(0.0301)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0848)
features.5.conv.0 tensor(0.0304)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0994)
features.6.conv.0 tensor(0.0319)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0666)
features.7.conv.0 tensor(0.0544)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.1662)
features.8.conv.0 tensor(0.0673)
features.8.conv.3 tensor(0.1120)
features.8.conv.6 tensor(0.1496)
features.9.conv.0 tensor(0.0709)
features.9.conv.3 tensor(0.1415)
features.9.conv.6 tensor(0.2081)
features.10.conv.0 tensor(0.0358)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.1036)
features.11.conv.0 tensor(0.2033)
features.11.conv.3 tensor(0.1273)
features.11.conv.6 tensor(0.3749)
features.12.conv.0 tensor(0.4091)
features.12.conv.3 tensor(0.1613)
features.12.conv.6 tensor(0.4537)
features.13.conv.0 tensor(0.0962)
features.13.conv.3 tensor(0.1273)
features.13.conv.6 tensor(0.2783)
features.14.conv.0 tensor(0.9299)
features.14.conv.3 tensor(0.1043)
features.14.conv.6 tensor(0.9415)
features.15.conv.0 tensor(0.8379)
features.15.conv.3 tensor(0.0633)
features.15.conv.6 tensor(0.9592)
features.16.conv.0 tensor(0.0789)
features.16.conv.3 tensor(0.1007)
features.16.conv.6 tensor(0.5774)
conv.0 tensor(0.1959)
tensor(976415.) 2188896.0
0.78160512
0.78156936
0.78155416
0.78155744
0.78153962
0.78154576
0.78154129
0.78152746
0.78154004
0.78152138
0.78153855
0.78154886
0.78155476
0.78156781
0.78160059
0.78160435
0.78161687
0.78161442
0.78160381
0.78159487
0.78158033
INFO - Training [29][   20/  196]   Loss 0.367222   Top1 87.539062   Top5 97.988281   BatchTime 0.327122   LR 0.000013
0.78156441
0.78157026
0.78157097
0.78160566
0.78162241
0.78165776
0.78168231
0.78168362
0.78166556
0.78165245
0.78164542
0.78164983
0.78165054
0.78161979
0.78160632
0.78159416
INFO - Training [29][   40/  196]   Loss 0.372798   Top1 87.236328   Top5 98.085938   BatchTime 0.288885   LR 0.000010
0.78158098
0.78156358
0.78154677
0.78155863
0.78157037
0.78158194
0.78160626
0.78161502
0.78163719
0.78167790
0.78167659
0.78167331
0.78165764
0.78165042
0.78165638
0.78164816
0.78162956
0.78159976
0.78157526
0.78155649
0.78152990
0.78150213
INFO - Training [29][   60/  196]   Loss 0.362257   Top1 87.558594   Top5 98.294271   BatchTime 0.286699   LR 0.000008
0.78146273
0.78141403
0.78139508
0.78136450
0.78135824
0.78134167
0.78132921
0.78134203
0.78135592
0.78136951
0.78138506
0.78137797
0.78138930
0.78138614
0.78140306
0.78142244
0.78143823
0.78144419
0.78143901
0.78145951
0.78145969
INFO - Training [29][   80/  196]   Loss 0.355846   Top1 87.807617   Top5 98.457031   BatchTime 0.286629   LR 0.000005
0.78146285
0.78142905
0.78140146
0.78136796
0.78134447
0.78132617
0.78131378
0.78131932
0.78132486
0.78131694
0.78131390
0.78129327
0.78129631
0.78128546
0.78127998
INFO - Training [29][  100/  196]   Loss 0.349902   Top1 87.957031   Top5 98.519531   BatchTime 0.281721   LR 0.000004
0.78129274
0.78129351
0.78130376
0.78128338
0.78129834
0.78129059
0.78128976
0.78130633
0.78133154
0.78132194
0.78132015
0.78134388
0.78133434
0.78133905
0.78133249
0.78133178
0.78134537
0.78134340
0.78132999
0.78133321
0.78132576
0.78131920
INFO - Training [29][  120/  196]   Loss 0.345232   Top1 88.056641   Top5 98.583984   BatchTime 0.281776   LR 0.000002
0.78131568
0.78130364
0.78129941
0.78129351
0.78131920
0.78132528
0.78132528
0.78132004
0.78133470
0.78134435
0.78134656
0.78135031
0.78135133
0.78134930
0.78136623
0.78136951
0.78136039
0.78136438
0.78134495
0.78135061
0.78133905
0.78134394
0.78135401
INFO - Training [29][  140/  196]   Loss 0.343256   Top1 88.144531   Top5 98.655134   BatchTime 0.278173   LR 0.000001
0.78135121
0.78135693
0.78136683
0.78138214
0.78138250
0.78141427
0.78139037
0.78136480
0.78137475
0.78138953
0.78138763
0.78140032
0.78139073
0.78140938
0.78139985
0.78142887
INFO - Training [29][  160/  196]   Loss 0.347923   Top1 87.968750   Top5 98.654785   BatchTime 0.275274   LR 0.000001
0.78144056
0.78143090
0.78142816
0.78142649
0.78140777
0.78141797
0.78140169
0.78139883
0.78140354
0.78139138
0.78140795
0.78140217
0.78141862
0.78139549
0.78139710
0.78137499
0.78137356
0.78137404
0.78137755
0.78138542
0.78139544
0.78138107
0.78138006
INFO - Training [29][  180/  196]   Loss 0.348082   Top1 87.921007   Top5 98.611111   BatchTime 0.274513   LR 0.000000
0.78136736
0.78134608
0.78135759
0.78134257
0.78134686
0.78134102
0.78133953
0.78132248
0.78133649
0.78133565
0.78133953
0.78135264
0.78135872
0.78134876
INFO - ==> Top1: 87.916    Top5: 98.634    Loss: 0.348
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.78135192
0.78134972
0.78135920
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 0.313206   Top1 89.707031   Top5 99.628906   BatchTime 0.137075
INFO - Validation [29][   40/   40]   Loss 0.297191   Top1 90.150000   Top5 99.730000   BatchTime 0.096136
INFO - ==> Top1: 90.150    Top5: 99.730    Loss: 0.297
INFO - ==> Sparsity : 0.446
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.2383)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0738)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0464)
features.4.conv.0 tensor(0.0298)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0846)
features.5.conv.0 tensor(0.0304)
features.5.conv.3 tensor(0.0828)
features.5.conv.6 tensor(0.0991)
features.6.conv.0 tensor(0.0319)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0665)
features.7.conv.0 tensor(0.0542)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.1661)
features.8.conv.0 tensor(0.0670)
features.8.conv.3 tensor(0.1117)
features.8.conv.6 tensor(0.1502)
features.9.conv.0 tensor(0.0706)
features.9.conv.3 tensor(0.1424)
features.9.conv.6 tensor(0.2080)
features.10.conv.0 tensor(0.0358)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.1037)
features.11.conv.0 tensor(0.2040)
features.11.conv.3 tensor(0.1271)
features.11.conv.6 tensor(0.3767)
features.12.conv.0 tensor(0.4119)
features.12.conv.3 tensor(0.1624)
features.12.conv.6 tensor(0.4560)
features.13.conv.0 tensor(0.0964)
features.13.conv.3 tensor(0.1279)
features.13.conv.6 tensor(0.2768)
features.14.conv.0 tensor(0.9299)
features.14.conv.3 tensor(0.1049)
features.14.conv.6 tensor(0.9418)
features.15.conv.0 tensor(0.8376)
features.15.conv.3 tensor(0.0632)
features.15.conv.6 tensor(0.9589)
features.16.conv.0 tensor(0.0791)
features.16.conv.3 tensor(0.1002)
features.16.conv.6 tensor(0.5753)
conv.0 tensor(0.1974)
tensor(976663.) 2188896.0
0.78136557
0.78271681
0.78264517
0.78588861
0.78713107
0.78886449
0.79070818
0.79354918
0.79615390
0.79686797
0.79743057
0.79808772
0.79877168
0.79889196
0.79853863
0.79882336
0.79967755
0.80028230
0.80111212
0.80204976
0.80307388
INFO - Training [30][   20/  196]   Loss 0.408596   Top1 85.820312   Top5 98.085938   BatchTime 0.356121   LR 0.001250
0.80407232
0.80538923
0.80654603
0.81252205
0.81378502
0.81401777
0.81422335
0.81390446
0.81617630
0.81860888
0.81870002
0.81876087
0.81867808
0.81883037
0.81942642
0.82238072
INFO - Training [30][   40/  196]   Loss 0.409467   Top1 85.859375   Top5 98.251953   BatchTime 0.304064   LR 0.001250
0.82737261
0.82727438
0.82716268
0.82721519
0.82735902
0.82762372
0.82779330
0.82807934
0.82858539
0.82900494
0.82925570
0.82949626
0.82977742
0.83000714
0.83058095
0.83058763
0.83077556
0.83102041
0.83109170
0.83119607
0.83125126
0.83140445
0.83153343
INFO - Training [30][   60/  196]   Loss 0.410792   Top1 85.833333   Top5 98.326823   BatchTime 0.290040   LR 0.001250
0.83165103
0.83169377
0.83149385
0.83147848
0.83165163
0.83097434
0.83056498
0.83031708
0.83025807
0.83008534
0.83006406
0.83011520
0.82994592
0.82960880
0.82932115
0.82916027
INFO - Training [30][   80/  196]   Loss 0.407867   Top1 85.932617   Top5 98.427734   BatchTime 0.280045   LR 0.001250
0.82880419
0.82857519
0.82853109
0.82841778
0.82838202
0.82819772
0.82808924
0.82797229
0.82790965
0.82776815
0.82784748
0.82777840
0.82769912
0.82782781
0.82797438
0.82826567
0.82843971
0.82836723
0.82839990
0.82846922
0.82840538
0.82836682
0.82832980
INFO - Training [30][  100/  196]   Loss 0.402535   Top1 86.078125   Top5 98.457031   BatchTime 0.277428   LR 0.001250
0.82815921
0.82801610
0.82787335
0.82798266
0.82834548
0.82845366
0.82863855
0.82885182
0.82922643
0.82936043
0.82905984
0.82866305
0.82862693
0.82871789
0.82897145
0.82926583
0.82935244
0.82943213
0.82954681
0.82982701
0.82955396
INFO - Training [30][  120/  196]   Loss 0.397733   Top1 86.321615   Top5 98.535156   BatchTime 0.278208   LR 0.001249
0.83007735
0.83528376
0.83529526
0.83512652
0.83492941
0.83479702
0.83474505
0.83467251
0.83479071
0.83493251
0.83500391
0.83474594
0.83465153
0.83467382
0.83453345
0.83440399
0.83423573
0.83409369
0.83378220
0.83384717
0.83383447
0.83377528
INFO - Training [30][  140/  196]   Loss 0.397683   Top1 86.311384   Top5 98.590960   BatchTime 0.276819   LR 0.001249
0.83369207
0.83388174
0.83414745
0.83384460
0.83368999
0.83356470
0.83323020
0.83347964
0.83378917
0.83388478
0.83362705
0.83357584
0.83380508
0.83388364
0.83377838
0.83326894
INFO - Training [30][  160/  196]   Loss 0.404583   Top1 86.030273   Top5 98.564453   BatchTime 0.274303   LR 0.001249
0.83317578
0.83312088
0.83307469
0.83296734
0.83243150
0.83184797
0.83164877
0.83047271
0.82885730
0.82685024
0.82406557
0.82287133
0.82412857
0.82582003
0.82705855
0.82774734
0.82899177
0.83015257
0.83136326
0.83274752
0.83305126
0.83335757
0.83439511
0.83477253
INFO - Training [30][  180/  196]   Loss 0.407475   Top1 85.930990   Top5 98.522135   BatchTime 0.271338   LR 0.001248
0.83499616
0.83536595
0.83546519
0.83503836
0.83535862
0.83561957
0.83764911
0.83706284
0.83624834
0.83533323
0.83540398
0.83482617
0.83410084
0.83366716
INFO - ==> Top1: 85.988    Top5: 98.516    Loss: 0.407
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.350090   Top1 88.261719   Top5 99.511719   BatchTime 0.152769
INFO - Validation [30][   40/   40]   Loss 0.333548   Top1 88.620000   Top5 99.630000   BatchTime 0.103700
INFO - ==> Top1: 88.620    Top5: 99.630    Loss: 0.334
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.2441)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0590)
features.2.conv.0 tensor(0.0223)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0773)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.0462)
features.4.conv.0 tensor(0.0348)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0853)
features.5.conv.0 tensor(0.0243)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.0920)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0667)
features.7.conv.0 tensor(0.0481)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.0939)
features.8.conv.0 tensor(0.0678)
features.8.conv.3 tensor(0.1166)
features.8.conv.6 tensor(0.1334)
features.9.conv.0 tensor(0.0718)
features.9.conv.3 tensor(0.1427)
features.9.conv.6 tensor(0.1246)
features.10.conv.0 tensor(0.0392)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.0820)
features.11.conv.0 tensor(0.1617)
features.11.conv.3 tensor(0.1277)
features.11.conv.6 tensor(0.1506)
features.12.conv.0 tensor(0.1903)
features.12.conv.3 tensor(0.1617)
features.12.conv.6 tensor(0.4210)
features.13.conv.0 tensor(0.0873)
features.13.conv.3 tensor(0.1285)
features.13.conv.6 tensor(0.0975)
features.14.conv.0 tensor(0.9336)
features.14.conv.3 tensor(0.1024)
features.14.conv.6 tensor(0.9447)
features.15.conv.0 tensor(0.8384)
features.15.conv.3 tensor(0.0606)
features.15.conv.6 tensor(0.9366)
features.16.conv.0 tensor(0.0670)
features.16.conv.3 tensor(0.1087)
features.16.conv.6 tensor(0.2497)
conv.0 tensor(0.1862)
tensor(816699.) 2188896.0
0.83347028
0.83322531
0.83302778
0.83243561
0.83160740
0.83077323
0.83031815
0.82996452
0.82970697
0.83145815
0.83046788
0.82915622
0.82826960
0.82735020
0.82637125
INFO - Training [31][   20/  196]   Loss 0.413112   Top1 85.664062   Top5 98.144531   BatchTime 0.331863   LR 0.001248
0.82593256
0.82482553
0.82173091
0.82004553
0.81619638
0.81452191
0.81245303
0.80888546
0.80715817
0.80542177
0.80718106
0.80869901
0.81251794
0.81534487
0.81932217
0.82126927
0.82176554
0.82320487
0.82286799
0.82193381
0.82146722
0.82091600
0.82053196
INFO - Training [31][   40/  196]   Loss 0.427388   Top1 85.429688   Top5 98.242188   BatchTime 0.298897   LR 0.001247
0.81930238
0.82021165
0.82096821
0.82214433
0.82370532
0.82591778
0.82719749
0.82867181
0.82931620
0.82982898
0.83079964
0.83178419
0.83215767
0.83231169
0.83250856
0.83233762
0.83233875
0.83227277
0.83385998
0.83403242
0.83402383
0.83404970
0.83406496
INFO - Training [31][   60/  196]   Loss 0.418896   Top1 85.533854   Top5 98.281250   BatchTime 0.289055   LR 0.001247
0.83418679
0.83433110
0.83421326
0.83414346
0.83428317
0.83435690
0.83456421
0.83485013
0.83527935
0.83545256
0.83578497
0.83623683
0.83635193
0.83632207
0.83609015
0.83600861
0.83598554
0.83591765
0.83557236
0.83547020
0.83547133
0.83544731
INFO - Training [31][   80/  196]   Loss 0.413842   Top1 85.791016   Top5 98.393555   BatchTime 0.285322   LR 0.001246
0.83534735
0.83534807
0.83554339
0.83524066
0.83507991
0.83518016
0.83543193
0.83578730
0.83620137
0.83625090
0.83638722
0.83615631
0.83610100
0.83567274
0.83547837
INFO - Training [31][  100/  196]   Loss 0.406488   Top1 86.000000   Top5 98.496094   BatchTime 0.280280   LR 0.001246
0.83534676
0.83509648
0.83485961
0.83455861
0.83444792
0.83412683
0.83377391
0.83360231
0.83355749
0.83331716
0.83331347
0.83273149
0.83279902
0.83262253
0.83224779
0.83173037
0.83150381
0.83133572
0.83109558
0.83070666
0.83050090
0.83056033
0.83030468
INFO - Training [31][  120/  196]   Loss 0.400243   Top1 86.243490   Top5 98.538411   BatchTime 0.278355   LR 0.001245
0.82982564
0.82910180
0.82804984
0.82722622
0.82666165
0.82647312
0.82641256
0.82612556
0.82577187
0.82558066
0.82551461
0.82568365
0.82592666
0.82544267
0.82546169
0.82567865
INFO - Training [31][  140/  196]   Loss 0.398097   Top1 86.275112   Top5 98.610491   BatchTime 0.274888   LR 0.001244
0.82605958
0.82581240
0.82576597
0.82571125
0.82569152
0.82570660
0.82542485
0.82533604
0.82501656
0.82498485
0.82502359
0.82522434
0.82533020
0.82529163
0.82560319
0.82688040
0.82758856
0.82822031
0.82817405
0.82812041
0.82809931
0.82795954
0.82779133
INFO - Training [31][  160/  196]   Loss 0.400196   Top1 86.247559   Top5 98.623047   BatchTime 0.273044   LR 0.001244
0.82779491
0.82782799
0.82813698
0.82931149
0.83283252
0.83315974
0.83317322
0.83326697
0.83364069
0.83385086
0.83639419
0.83623755
0.83611727
0.83605373
0.83584428
0.83571965
0.83565176
0.83555055
0.83549362
INFO - Training [31][  180/  196]   Loss 0.402551   Top1 86.132812   Top5 98.565538   BatchTime 0.276592   LR 0.001243
0.83546931
0.83571488
0.83513600
0.83516473
0.83487684
0.83422643
0.83411670
0.83390504
0.83336157
0.83315188
0.83289653
0.83271909
0.83246458
0.83228415
INFO - ==> Top1: 86.072    Top5: 98.556    Loss: 0.403
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83219618
0.83217746
0.83189571
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [31][   20/   40]   Loss 0.392020   Top1 87.167969   Top5 99.472656   BatchTime 0.134706
INFO - Validation [31][   40/   40]   Loss 0.382319   Top1 87.240000   Top5 99.500000   BatchTime 0.095066
INFO - ==> Top1: 87.240    Top5: 99.500    Loss: 0.382
INFO - ==> Sparsity : 0.371
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.2305)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0599)
features.2.conv.0 tensor(0.0237)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0752)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0447)
features.4.conv.0 tensor(0.0260)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0830)
features.5.conv.0 tensor(0.0275)
features.5.conv.3 tensor(0.0856)
features.5.conv.6 tensor(0.0952)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0644)
features.7.conv.0 tensor(0.0446)
features.7.conv.3 tensor(0.1137)
features.7.conv.6 tensor(0.0935)
features.8.conv.0 tensor(0.0695)
features.8.conv.3 tensor(0.1134)
features.8.conv.6 tensor(0.1393)
features.9.conv.0 tensor(0.0756)
features.9.conv.3 tensor(0.1496)
features.9.conv.6 tensor(0.1252)
features.10.conv.0 tensor(0.0360)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0789)
features.11.conv.0 tensor(0.1586)
features.11.conv.3 tensor(0.1267)
features.11.conv.6 tensor(0.1607)
features.12.conv.0 tensor(0.2277)
features.12.conv.3 tensor(0.1599)
features.12.conv.6 tensor(0.4301)
features.13.conv.0 tensor(0.0888)
features.13.conv.3 tensor(0.1285)
features.13.conv.6 tensor(0.1174)
features.14.conv.0 tensor(0.9352)
features.14.conv.3 tensor(0.1063)
features.14.conv.6 tensor(0.9390)
features.15.conv.0 tensor(0.8514)
features.15.conv.3 tensor(0.0620)
features.15.conv.6 tensor(0.9595)
features.16.conv.0 tensor(0.0739)
features.16.conv.3 tensor(0.1073)
features.16.conv.6 tensor(0.2620)
conv.0 tensor(0.1394)
tensor(812159.) 2188896.0
0.83176953
0.83170968
0.83151907
0.83157945
0.83141512
0.83139151
0.83109492
0.83079207
0.83052981
0.83026618
0.82996213
0.82969844
0.82942468
0.82911164
0.82888234
0.82870418
0.82861000
0.82851005
0.82853478
0.82809299
0.82825595
0.82802844
INFO - Training [32][   20/  196]   Loss 0.423265   Top1 84.902344   Top5 98.046875   BatchTime 0.337332   LR 0.001242
0.82769328
0.82708257
0.82694632
0.82726187
0.82840693
0.82843441
0.82812274
0.82771164
0.82708317
0.82676762
0.82648742
0.82602704
0.82548594
0.82479078
0.82404047
0.82334882
0.82157248
0.82142508
0.82150066
0.82310563
INFO - Training [32][   40/  196]   Loss 0.417797   Top1 85.322266   Top5 98.310547   BatchTime 0.319779   LR 0.001241
0.82371825
0.82439256
0.82490760
0.82550251
0.82608235
0.82675129
0.82743555
0.82791889
0.82809168
0.82847834
0.82914633
0.82998496
0.83124691
0.83128375
0.83121401
0.83122855
INFO - Training [32][   60/  196]   Loss 0.411697   Top1 85.670573   Top5 98.411458   BatchTime 0.302438   LR 0.001240
0.83282393
0.84296745
0.84335989
0.84364051
0.84415871
0.84435380
0.84582239
0.84574199
0.84518850
0.84475797
0.84432197
0.84388238
0.84351975
0.84345233
0.84334713
0.84321940
0.84333771
0.84342855
0.84330344
0.84319448
INFO - Training [32][   80/  196]   Loss 0.413956   Top1 85.673828   Top5 98.510742   BatchTime 0.299989   LR 0.001239
0.84320676
0.84323174
0.84303588
0.84271431
0.84267187
0.84261304
0.84278053
0.84283268
0.84262609
0.84283024
0.84279454
0.84259993
0.84253603
0.84252989
0.84244120
0.84262419
0.84283757
0.84323406
0.84354240
0.84361815
0.84352279
0.84368110
0.84364837
INFO - Training [32][  100/  196]   Loss 0.410350   Top1 85.839844   Top5 98.511719   BatchTime 0.293744   LR 0.001238
0.84430414
0.84616011
0.84612793
0.84599203
0.84607643
0.84631538
0.84639746
0.84640777
0.84659231
0.84657377
0.84647137
0.84668016
0.84655458
0.84644562
0.84645134
INFO - Training [32][  120/  196]   Loss 0.404932   Top1 85.986328   Top5 98.577474   BatchTime 0.288373   LR 0.001237
0.84664583
0.84676844
0.84656841
0.84640771
0.84729761
0.84784257
0.84768724
0.84756571
0.84738201
0.84745234
0.84749395
0.84725124
0.84706438
0.84673142
0.84641671
0.84611231
0.84591162
0.84569538
0.84562987
0.84551609
0.84531814
0.84466499
0.84434420
0.84443355
INFO - Training [32][  140/  196]   Loss 0.405648   Top1 85.998884   Top5 98.652344   BatchTime 0.282891   LR 0.001236
0.84435081
0.84446406
0.84430206
0.84381956
0.84384483
0.84351593
0.84319133
0.84268928
0.84237862
0.84183401
0.84172541
0.84169811
0.84145802
0.84135973
0.84118545
0.84132618
0.84123486
INFO - Training [32][  160/  196]   Loss 0.410601   Top1 85.839844   Top5 98.613281   BatchTime 0.276453   LR 0.001235
0.84111220
0.84112716
0.84133756
0.84217101
0.84196854
0.84164059
0.84139466
0.84118509
0.84077317
0.84032559
0.84034204
0.84166718
0.84351295
0.84362465
0.84352452
0.84358954
0.84363991
0.84381574
INFO - Training [32][  180/  196]   Loss 0.409911   Top1 85.900608   Top5 98.550347   BatchTime 0.270241   LR 0.001234
0.84402901
0.84412867
0.84398812
0.84394860
0.84389943
0.84392864
0.84411341
0.84410757
0.84400761
0.84406316
0.84424239
0.84417713
0.84407264
0.84398097
0.84340978
0.84268326
0.84254122
0.84244031
INFO - ==> Top1: 85.938    Top5: 98.534    Loss: 0.409
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84227437
0.84209311
0.84177977
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.387821   Top1 87.304688   Top5 99.355469   BatchTime 0.133552
INFO - Validation [32][   40/   40]   Loss 0.382957   Top1 87.410000   Top5 99.430000   BatchTime 0.094617
INFO - ==> Top1: 87.410    Top5: 99.430    Loss: 0.383
INFO - ==> Sparsity : 0.370
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.2305)
features.1.conv.0 tensor(0.0111)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0590)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0752)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0540)
features.3.conv.6 tensor(0.0443)
features.4.conv.0 tensor(0.0304)
features.4.conv.3 tensor(0.1076)
features.4.conv.6 tensor(0.0804)
features.5.conv.0 tensor(0.0256)
features.5.conv.3 tensor(0.0839)
features.5.conv.6 tensor(0.1016)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0624)
features.7.conv.0 tensor(0.0469)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.0968)
features.8.conv.0 tensor(0.0612)
features.8.conv.3 tensor(0.1105)
features.8.conv.6 tensor(0.1288)
features.9.conv.0 tensor(0.0806)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.1967)
features.10.conv.0 tensor(0.0396)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.0763)
features.11.conv.0 tensor(0.1695)
features.11.conv.3 tensor(0.1221)
features.11.conv.6 tensor(0.1489)
features.12.conv.0 tensor(0.1771)
features.12.conv.3 tensor(0.1644)
features.12.conv.6 tensor(0.2283)
features.13.conv.0 tensor(0.0898)
features.13.conv.3 tensor(0.1271)
features.13.conv.6 tensor(0.0822)
features.14.conv.0 tensor(0.9386)
features.14.conv.3 tensor(0.1057)
features.14.conv.6 tensor(0.9184)
features.15.conv.0 tensor(0.8550)
features.15.conv.3 tensor(0.0604)
features.15.conv.6 tensor(0.9531)
features.16.conv.0 tensor(0.0835)
features.16.conv.3 tensor(0.1061)
features.16.conv.6 tensor(0.2913)
conv.0 tensor(0.1564)
tensor(810851.) 2188896.0
0.84151053
0.84139407
0.84162617
0.84155387
0.84183341
0.84137177
0.84140033
0.84155631
0.84206527
0.84308290
0.84308308
0.84300619
0.84323919
0.84300548
0.84314597
0.84321952
0.84305179
0.84270144
0.84241039
INFO - Training [33][   20/  196]   Loss 0.427832   Top1 85.546875   Top5 97.968750   BatchTime 0.379315   LR 0.001232
0.84254813
0.84188676
0.84489018
0.84411228
0.84359640
0.84346503
0.84331650
0.84243941
0.84156007
0.84134561
0.84101176
0.84073526
0.83984357
0.83867657
0.83838004
0.84032851
0.84261256
0.84453750
0.84445393
0.84425312
0.84436405
0.84414357
INFO - Training [33][   40/  196]   Loss 0.419823   Top1 85.634766   Top5 98.212891   BatchTime 0.326880   LR 0.001230
0.84389025
0.84367681
0.84347916
0.84313476
0.84300828
0.84273952
0.84273285
0.84233135
0.84207392
0.84192240
0.84187865
0.84136814
0.84090066
0.84060246
0.84022254
0.83953911
0.83874524
0.83778280
0.83678013
0.83615988
0.83475834
INFO - Training [33][   60/  196]   Loss 0.417278   Top1 85.891927   Top5 98.287760   BatchTime 0.313610   LR 0.001229
0.83358818
0.83367759
0.83503860
0.83562875
0.83594811
0.83575898
0.83582836
0.83553654
0.83506185
0.83462578
0.83415186
0.83379394
0.83349228
0.83318758
0.83332610
0.83302879
INFO - Training [33][   80/  196]   Loss 0.417032   Top1 85.795898   Top5 98.408203   BatchTime 0.299710   LR 0.001228
0.83293575
0.83316725
0.83325583
0.83368886
0.83368683
0.83390176
0.83371490
0.83417851
0.83476597
0.83574319
0.83608299
0.83586752
0.83556902
0.83526492
0.83490139
0.83472389
0.83474743
0.83585525
0.83569247
0.83530211
0.83461225
0.83419853
INFO - Training [33][  100/  196]   Loss 0.412440   Top1 85.957031   Top5 98.500000   BatchTime 0.295857   LR 0.001226
0.83395052
0.83390576
0.83351821
0.83319408
0.83298635
0.83294332
0.83302766
0.83311677
0.83315557
0.83352977
0.83413863
0.83502960
0.83513546
0.83497733
0.83473575
INFO - Training [33][  120/  196]   Loss 0.403389   Top1 86.217448   Top5 98.570964   BatchTime 0.287987   LR 0.001225
0.83433992
0.83415562
0.83370167
0.83353096
0.83345449
0.83319974
0.83261573
0.83210039
0.83200544
0.83204597
0.83223069
0.83234298
0.83201206
0.83156639
0.83112252
0.83098447
0.83082873
0.83060110
0.83036530
0.83004862
0.82977259
0.83001399
0.83001250
0.82956296
INFO - Training [33][  140/  196]   Loss 0.402485   Top1 86.294643   Top5 98.610491   BatchTime 0.284263   LR 0.001224
0.82918668
0.82890379
0.82854801
0.82875657
0.82819408
0.82766366
0.82739997
0.82718015
0.82696700
0.82718706
0.82735151
0.82771820
0.82842064
0.82915151
0.82888347
0.82848221
0.82817471
0.82770771
0.82591337
0.82535672
0.82502228
INFO - Training [33][  160/  196]   Loss 0.406409   Top1 86.152344   Top5 98.571777   BatchTime 0.284634   LR 0.001222
0.82478672
0.82474291
0.82455724
0.82375556
0.82263297
0.82212502
0.82146913
0.82117206
0.82201385
0.82463956
0.82527149
0.82484317
0.82489032
0.82453167
0.82431996
0.82427090
0.82427776
0.82403505
0.82401240
0.82375628
0.82391685
INFO - Training [33][  180/  196]   Loss 0.405910   Top1 86.152344   Top5 98.496094   BatchTime 0.284677   LR 0.001221
0.82376200
0.82349670
0.82341856
0.82370949
0.82293916
0.82111347
0.82033688
0.82022625
0.82075512
0.82103789
0.82031655
0.81831002
0.81565326
0.81164360
0.80794406
INFO - ==> Top1: 86.230    Top5: 98.494    Loss: 0.405
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [33][   20/   40]   Loss 0.343189   Top1 88.906250   Top5 99.453125   BatchTime 0.126851
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0603)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0723)
features.3.conv.0 tensor(0.0145)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0419)
features.4.conv.0 tensor(0.0326)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.0835)
features.5.conv.0 tensor(0.0233)
features.5.conv.3 tensor(0.0845)
features.5.conv.6 tensor(0.0946)
features.6.conv.0 tensor(0.0272)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0616)
features.7.conv.0 tensor(0.0433)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.0997)
features.8.conv.0 tensor(0.0600)
features.8.conv.3 tensor(0.1128)
features.8.conv.6 tensor(0.1597)
features.9.conv.0 tensor(0.0858)
features.9.conv.3 tensor(0.1508)
features.9.conv.6 tensor(0.1025)
features.10.conv.0 tensor(0.0402)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0761)
features.11.conv.0 tensor(0.1820)
features.11.conv.3 tensor(0.1264)
features.11.conv.6 tensor(0.3256)
features.12.conv.0 tensor(0.1858)
features.12.conv.3 tensor(0.1676)
features.12.conv.6 tensor(0.4463)
features.13.conv.0 tensor(0.0944)
features.13.conv.3 tensor(0.1256)
features.13.conv.6 tensor(0.0808)
features.14.conv.0 tensor(0.9395)
features.14.conv.3 tensor(0.1089)
features.14.conv.6 tensor(0.9200)
features.15.conv.0 tensor(0.8505)
features.15.conv.3 tensor(0.0589)
features.15.conv.6 tensor(0.9625)
features.16.conv.0 tensor(0.0748)
features.16.conv.3 tensor(0.1068)
features.16.conv.6 tensor(0.3109)
conv.0 tensor(0.7118)
tensor(1065813.) 2188896.0
INFO - Validation [33][   40/   40]   Loss 0.339538   Top1 88.960000   Top5 99.570000   BatchTime 0.089825
INFO - ==> Top1: 88.960    Top5: 99.570    Loss: 0.340
INFO - ==> Sparsity : 0.487
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
0.80764097
0.81186181
0.81508964
0.81565911
0.81675309
0.81677490
0.81610072
0.81531805
0.81175196
0.80975157
0.81019074
0.81023926
0.81005257
0.80971658
0.80764055
0.80444735
0.80396068
0.80388224
INFO - Training [34][   20/  196]   Loss 0.424324   Top1 85.410156   Top5 98.125000   BatchTime 0.333070   LR 0.001218
0.80434853
0.80397558
0.80400151
0.80429888
0.80473685
0.80465561
0.80572182
0.80683690
0.80770302
0.80895537
0.80916911
0.80951524
0.81030709
0.81193596
0.81405342
0.81629080
0.81810367
0.82019639
0.82269669
0.82483894
0.82557946
0.82619131
INFO - Training [34][   40/  196]   Loss 0.414906   Top1 85.683594   Top5 98.300781   BatchTime 0.302656   LR 0.001216
0.82656908
0.82693887
0.82681561
0.82615596
0.82540077
0.82472318
0.82444257
0.82414681
0.82343358
0.82263881
0.82186323
0.82194889
0.82154119
0.82158381
0.82165283
0.82184166
0.82215577
0.82206970
0.82223773
0.82265621
0.82312065
0.82353884
INFO - Training [34][   60/  196]   Loss 0.406931   Top1 86.009115   Top5 98.404948   BatchTime 0.290415   LR 0.001215
0.82344031
0.82351470
0.82411623
0.82456928
0.82491821
0.82489026
0.82514244
0.82625252
0.83114159
0.83133954
0.83161247
0.83198982
0.83215320
0.83218348
0.83255398
0.83409232
0.83895177
0.84030277
0.84020370
0.83981067
0.83942860
INFO - Training [34][   80/  196]   Loss 0.405578   Top1 86.137695   Top5 98.535156   BatchTime 0.291493   LR 0.001213
0.83897144
0.84016871
0.84015566
0.83979189
0.84001416
0.84048617
0.84085011
0.84111899
0.84270453
0.84289867
0.84431237
0.84380263
0.84415293
0.84429365
0.84404141
INFO - Training [34][  100/  196]   Loss 0.399830   Top1 86.367188   Top5 98.558594   BatchTime 0.287379   LR 0.001211
0.84378487
0.84428191
0.84652257
0.84673172
0.84670043
0.84636390
0.84647948
0.84592193
0.84500974
0.84461677
0.84450132
0.84437060
0.84451443
0.84428734
0.84412360
0.84414083
0.84427458
0.84443247
0.84460503
0.84451699
0.84463567
0.84445065
0.84481829
0.84545541
INFO - Training [34][  120/  196]   Loss 0.394184   Top1 86.516927   Top5 98.668620   BatchTime 0.281697   LR 0.001209
0.84575456
0.84571278
0.84561175
0.84564006
0.84520018
0.84495813
0.84495318
0.84503496
0.84520751
0.84492213
0.84497935
0.84496552
0.84667665
0.84670931
0.84668618
0.84664035
INFO - Training [34][  140/  196]   Loss 0.393419   Top1 86.520647   Top5 98.708147   BatchTime 0.276337   LR 0.001208
0.84657425
0.84657955
0.84606630
0.84584773
0.84613138
0.84630167
0.84623444
0.84621936
0.84658051
0.84654117
0.84640020
0.84632134
0.84650028
0.84661466
0.84659284
0.84634286
0.84621620
0.84594703
0.84592086
0.84592777
0.84581143
0.84569323
0.84559864
0.84581453
INFO - Training [34][  160/  196]   Loss 0.398906   Top1 86.391602   Top5 98.645020   BatchTime 0.273591   LR 0.001206
0.84579515
0.84580457
0.84583431
0.84593803
0.84607911
0.84601289
0.84595525
0.84576571
0.84579593
0.84613639
0.84632403
0.84600312
0.84625268
0.84606868
0.84589344
INFO - Training [34][  180/  196]   Loss 0.399330   Top1 86.302083   Top5 98.587240   BatchTime 0.272834   LR 0.001204
0.84609717
0.84598953
0.84554416
0.84544921
0.84522700
0.84500682
0.84451431
0.84416944
0.84413689
0.84381461
0.84338623
0.84318119
0.84290087
0.84252602
0.84239846
0.84231699
INFO - ==> Top1: 86.290    Top5: 98.588    Loss: 0.399
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84223872
0.84241802
0.84194791
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [34][   20/   40]   Loss 0.350197   Top1 88.730469   Top5 99.433594   BatchTime 0.135487
INFO - Validation [34][   40/   40]   Loss 0.339409   Top1 88.780000   Top5 99.550000   BatchTime 0.097051
INFO - ==> Top1: 88.780    Top5: 99.550    Loss: 0.339
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.2324)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0577)
features.2.conv.0 tensor(0.0191)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0718)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0404)
features.4.conv.0 tensor(0.0329)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0814)
features.5.conv.0 tensor(0.0257)
features.5.conv.3 tensor(0.0828)
features.5.conv.6 tensor(0.0941)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0640)
features.7.conv.0 tensor(0.0462)
features.7.conv.3 tensor(0.1123)
features.7.conv.6 tensor(0.0949)
features.8.conv.0 tensor(0.0658)
features.8.conv.3 tensor(0.1149)
features.8.conv.6 tensor(0.1426)
features.9.conv.0 tensor(0.0739)
features.9.conv.3 tensor(0.1453)
features.9.conv.6 tensor(0.1047)
features.10.conv.0 tensor(0.0352)
features.10.conv.3 tensor(0.0932)
features.10.conv.6 tensor(0.0721)
features.11.conv.0 tensor(0.1797)
features.11.conv.3 tensor(0.1294)
features.11.conv.6 tensor(0.1850)
features.12.conv.0 tensor(0.3146)
features.12.conv.3 tensor(0.1698)
features.12.conv.6 tensor(0.2214)
features.13.conv.0 tensor(0.0844)
features.13.conv.3 tensor(0.1240)
features.13.conv.6 tensor(0.0868)
features.14.conv.0 tensor(0.9213)
features.14.conv.3 tensor(0.1064)
features.14.conv.6 tensor(0.9165)
features.15.conv.0 tensor(0.8530)
features.15.conv.3 tensor(0.0572)
features.15.conv.6 tensor(0.9524)
features.16.conv.0 tensor(0.0842)
features.16.conv.3 tensor(0.1081)
features.16.conv.6 tensor(0.3379)
conv.0 tensor(0.1389)
tensor(822247.) 2188896.0
0.84192210
0.84177595
0.84167594
0.84142643
0.84130043
0.84111798
0.84062237
0.84011215
0.83955765
0.83927685
0.83890635
0.83873135
0.83894247
0.83940977
0.83981782
0.84012669
0.84033716
0.84127873
INFO - Training [35][   20/  196]   Loss 0.415991   Top1 85.664062   Top5 98.281250   BatchTime 0.375575   LR 0.001201
0.84240216
0.84248501
0.84253985
0.84252107
0.84273052
0.84311479
0.84324795
0.84333110
0.84307754
0.84317070
0.84256512
0.84213448
0.84148163
0.84072000
0.83977342
0.83943707
0.83912367
0.83895665
0.83867985
0.83865279
0.83834404
0.83850622
0.83885217
INFO - Training [35][   40/  196]   Loss 0.417911   Top1 85.527344   Top5 98.457031   BatchTime 0.316833   LR 0.001199
0.84048384
0.84051615
0.84062374
0.84359175
0.84327137
0.84303737
0.84298337
0.84245199
0.84237105
0.84236449
0.84186542
0.84167248
0.84158182
0.84151340
0.84119934
0.84085184
0.84071136
0.84048569
0.83972734
0.83934021
0.83904684
0.83894497
INFO - Training [35][   60/  196]   Loss 0.407195   Top1 85.924479   Top5 98.489583   BatchTime 0.306664   LR 0.001197
0.83865696
0.83850586
0.83832282
0.83816147
0.83792847
0.83781397
0.83786571
0.83720392
0.83736056
0.83705097
0.83671898
0.83613920
0.83587736
0.83505315
0.83453995
INFO - Training [35][   80/  196]   Loss 0.405304   Top1 86.103516   Top5 98.535156   BatchTime 0.294883   LR 0.001195
0.83403909
0.83379430
0.83337939
0.83302706
0.83281910
0.83254153
0.83237720
0.83201176
0.83156103
0.83167368
0.83159977
0.83157754
0.83156091
0.83121371
0.83096093
0.83061343
0.83033365
0.83026505
0.83024132
0.83031362
0.83038139
0.83050305
0.83188146
INFO - Training [35][  100/  196]   Loss 0.393223   Top1 86.488281   Top5 98.566406   BatchTime 0.288989   LR 0.001192
0.83150226
0.83145362
0.83131766
0.83186471
0.83277369
0.83305770
0.83336216
0.83815438
0.83812851
0.83799982
0.83787763
0.83808863
0.83810735
0.83808959
0.83844483
INFO - Training [35][  120/  196]   Loss 0.388221   Top1 86.718750   Top5 98.619792   BatchTime 0.283651   LR 0.001190
0.83839947
0.83830887
0.83833462
0.83858562
0.84031743
0.84082943
0.84067428
0.84063327
0.84051019
0.84017432
0.84026331
0.84015441
0.84035283
0.84044802
0.84008771
0.83994424
0.83991575
0.83930796
0.83879811
0.83799851
0.83740711
INFO - Training [35][  140/  196]   Loss 0.389023   Top1 86.674107   Top5 98.671875   BatchTime 0.286318   LR 0.001188
0.83660918
0.83536005
0.83418894
0.83269954
0.83111304
0.83130038
0.83083510
0.83063751
0.82964212
0.82891136
0.82840925
0.82747853
0.82697880
0.82653058
0.82640821
0.82623690
0.82582361
0.82598311
0.82637888
0.82635486
0.82633209
0.82625937
0.82630497
INFO - Training [35][  160/  196]   Loss 0.392412   Top1 86.584473   Top5 98.662109   BatchTime 0.281899   LR 0.001186
0.82640696
0.82657760
0.82681292
0.82730430
0.82734931
0.82698399
0.82690543
0.82739931
0.82773745
0.82807350
0.82840109
0.82895762
0.83019054
0.83041865
0.83032525
0.83027220
0.83010250
0.82996750
0.82988811
0.82976985
0.82960570
0.82954586
0.82974201
INFO - Training [35][  180/  196]   Loss 0.392115   Top1 86.566840   Top5 98.585069   BatchTime 0.279695   LR 0.001184
0.83152616
0.83146340
0.83160377
0.83144993
0.83134317
0.83098668
0.83071297
0.83055341
0.83041137
0.83033681
0.83014762
0.82997584
0.83004725
********************pre-trained*****************
INFO - ==> Top1: 86.598    Top5: 98.612    Loss: 0.391
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.334154   Top1 89.179688   Top5 99.550781   BatchTime 0.123375
INFO - Validation [35][   40/   40]   Loss 0.318018   Top1 89.380000   Top5 99.620000   BatchTime 0.088598
INFO - ==> Top1: 89.380    Top5: 99.620    Loss: 0.318
INFO - ==> Sparsity : 0.387
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.2207)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0525)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0671)
features.2.conv.6 tensor(0.0744)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0408)
features.4.conv.0 tensor(0.0299)
features.4.conv.3 tensor(0.0984)
features.4.conv.6 tensor(0.0801)
features.5.conv.0 tensor(0.0275)
features.5.conv.3 tensor(0.0845)
features.5.conv.6 tensor(0.0889)
features.6.conv.0 tensor(0.0239)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0592)
features.7.conv.0 tensor(0.0441)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.0990)
features.8.conv.0 tensor(0.0548)
features.8.conv.3 tensor(0.1155)
features.8.conv.6 tensor(0.1358)
features.9.conv.0 tensor(0.0721)
features.9.conv.3 tensor(0.1461)
features.9.conv.6 tensor(0.1027)
features.10.conv.0 tensor(0.0334)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0715)
features.11.conv.0 tensor(0.1878)
features.11.conv.3 tensor(0.1271)
features.11.conv.6 tensor(0.1776)
features.12.conv.0 tensor(0.2613)
features.12.conv.3 tensor(0.1698)
features.12.conv.6 tensor(0.4674)
features.13.conv.0 tensor(0.0744)
features.13.conv.3 tensor(0.1265)
features.13.conv.6 tensor(0.0858)
features.14.conv.0 tensor(0.9234)
features.14.conv.3 tensor(0.1047)
features.14.conv.6 tensor(0.9219)
features.15.conv.0 tensor(0.8637)
features.15.conv.3 tensor(0.0600)
features.15.conv.6 tensor(0.9576)
features.16.conv.0 tensor(0.0834)
features.16.conv.3 tensor(0.1081)
features.16.conv.6 tensor(0.3822)
conv.0 tensor(0.1329)
tensor(846387.) 2188896.0
0.82985431
0.82937741
0.82896340
0.82861573
0.82845801
0.82842267
0.82838285
0.82847935
0.82852352
0.82862717
0.82887965
0.82934642
0.82926899
0.82974952
0.83032185
0.83029336
0.83037239
0.83046925
0.83059591
0.83084232
0.83106607
0.83114201
0.83082449
INFO - Training [36][   20/  196]   Loss 0.401110   Top1 86.582031   Top5 97.988281   BatchTime 0.326241   LR 0.001180
0.83081192
0.83207726
0.83207649
0.83228779
0.83224756
0.83192986
0.83222175
0.83292371
0.83430046
0.83360577
0.83319312
0.83301091
0.83310705
0.83308780
0.83245808
INFO - Training [36][   40/  196]   Loss 0.394726   Top1 86.513672   Top5 98.173828   BatchTime 0.293560   LR 0.001177
0.83232397
0.83193272
0.83152747
0.83140236
0.83146912
0.83174515
0.83143526
0.83242124
0.83183229
0.83159995
0.83110279
0.83069891
0.83031696
0.82980114
0.82905352
0.82868129
0.82802349
0.82752454
0.82723659
0.82670361
0.82604331
0.82532918
INFO - Training [36][   60/  196]   Loss 0.391456   Top1 86.614583   Top5 98.339844   BatchTime 0.286392   LR 0.001175
0.82454658
0.82402647
0.82354057
0.82316417
0.82266563
0.82241261
0.82198876
0.82179314
0.82168347
0.82165116
0.82153547
0.82170737
0.82251793
0.82380623
0.82400215
0.82403386
0.82411659
0.82423580
INFO - Training [36][   80/  196]   Loss 0.388103   Top1 86.738281   Top5 98.461914   BatchTime 0.296509   LR 0.001173
0.82391483
0.82360601
0.82346207
0.82298279
0.82265258
0.82265162
0.82264918
0.82262456
0.82239974
0.82201564
0.82220304
0.82247061
0.82293540
0.82348520
0.82348472
0.82390648
0.82419169
0.82439411
0.82451016
0.82476300
0.82472712
0.82513100
0.82528293
INFO - Training [36][  100/  196]   Loss 0.387657   Top1 86.777344   Top5 98.468750   BatchTime 0.290636   LR 0.001170
0.82561874
0.82601124
0.82652199
0.82701892
0.82772917
0.82897079
0.83083242
0.83171391
0.83218014
0.83373040
0.83580595
0.83566672
0.83552998
0.83509213
0.83470058
0.83465433
INFO - Training [36][  120/  196]   Loss 0.383869   Top1 86.868490   Top5 98.531901   BatchTime 0.285146   LR 0.001168
0.83449793
0.83415067
0.83378273
0.83379173
0.83374989
0.83501232
0.83535165
0.83572769
0.84171909
0.84119451
0.84049529
0.83991683
0.83936828
0.83905357
0.83890527
0.83848816
0.83823091
0.83791184
0.83753109
0.83706349
0.83675116
0.83638620
0.83602530
INFO - Training [36][  140/  196]   Loss 0.384798   Top1 86.866629   Top5 98.585379   BatchTime 0.280559   LR 0.001165
0.83577198
0.83543080
0.83513474
0.83509183
0.83483481
0.83423716
0.83407491
0.83370471
0.83346063
0.83296132
0.83214426
0.83200210
0.83209753
0.83198786
0.83187985
0.83167446
0.83176076
0.83167559
0.83185893
0.83188641
0.83217156
0.83224678
INFO - Training [36][  160/  196]   Loss 0.388357   Top1 86.674805   Top5 98.549805   BatchTime 0.278952   LR 0.001163
0.83223099
0.83202600
0.83161509
0.83114934
0.83054227
0.83009285
0.83135754
0.83107048
0.83101720
0.83153802
0.83329600
0.83370441
0.83319235
0.83280623
0.83239681
0.83190328
0.83187318
0.83209270
0.83365780
0.83357894
INFO - Training [36][  180/  196]   Loss 0.389068   Top1 86.655816   Top5 98.478733   BatchTime 0.280873   LR 0.001160
0.83364600
0.83408314
0.83395654
0.83394367
0.83405942
0.83388573
0.83378845
0.83361143
0.83405954
0.83578867
0.83515835
0.83510590
0.83493906
0.83437425
INFO - ==> Top1: 86.660    Top5: 98.494    Loss: 0.389
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 0.340489   Top1 88.769531   Top5 99.570312   BatchTime 0.132865
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.2227)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0521)
features.2.conv.0 tensor(0.0191)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0712)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0339)
features.4.conv.0 tensor(0.0304)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.0806)
features.5.conv.0 tensor(0.0243)
features.5.conv.3 tensor(0.0845)
features.5.conv.6 tensor(0.0876)
features.6.conv.0 tensor(0.0280)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0597)
features.7.conv.0 tensor(0.0400)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.1055)
features.8.conv.0 tensor(0.0591)
features.8.conv.3 tensor(0.1143)
features.8.conv.6 tensor(0.1477)
features.9.conv.0 tensor(0.0632)
features.9.conv.3 tensor(0.1479)
features.9.conv.6 tensor(0.1147)
features.10.conv.0 tensor(0.0329)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.0728)
features.11.conv.0 tensor(0.1908)
features.11.conv.3 tensor(0.1281)
features.11.conv.6 tensor(0.1729)
features.12.conv.0 tensor(0.1813)
features.12.conv.3 tensor(0.1713)
features.12.conv.6 tensor(0.4260)
features.13.conv.0 tensor(0.0818)
features.13.conv.3 tensor(0.1264)
features.13.conv.6 tensor(0.0920)
features.14.conv.0 tensor(0.8901)
features.14.conv.3 tensor(0.0981)
features.14.conv.6 tensor(0.9285)
features.15.conv.0 tensor(0.8740)
features.15.conv.3 tensor(0.0573)
features.15.conv.6 tensor(0.9593)
features.16.conv.0 tensor(0.0903)
features.16.conv.3 tensor(0.1072)
features.16.conv.6 tensor(0.3153)
conv.0 tensor(0.1407)
tensor(822396.) 2188896.0
INFO - Validation [36][   40/   40]   Loss 0.330192   Top1 88.940000   Top5 99.680000   BatchTime 0.094518
INFO - ==> Top1: 88.940    Top5: 99.680    Loss: 0.330
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
0.83384401
0.83338410
0.83314657
0.83252907
0.83181864
0.83154619
0.83201689
0.83459908
0.83457655
0.83435875
0.83428413
0.83513469
0.83532292
0.83532250
0.83523667
0.83529878
0.83517212
0.83497053
0.83489436
0.83451873
0.83455002
0.83471817
0.83449501
0.83430111
INFO - Training [37][   20/  196]   Loss 0.407867   Top1 85.957031   Top5 98.222656   BatchTime 0.367614   LR 0.001155
0.83413035
0.83407569
0.83401197
0.83384234
0.83375823
0.83364749
0.83385295
0.83330792
0.83288014
0.83290392
0.83304733
0.83274710
0.83256823
INFO - Training [37][   40/  196]   Loss 0.401065   Top1 86.298828   Top5 98.339844   BatchTime 0.328301   LR 0.001153
0.83232337
0.83205181
0.83183551
0.83161902
0.83129072
0.83112323
0.83097988
0.83063149
0.83045286
0.83039623
0.83033341
0.83042657
0.83028811
0.83033061
0.83055204
0.83057070
0.83046353
0.83054286
0.83075309
0.83085716
0.83062428
INFO - Training [37][   60/  196]   Loss 0.396821   Top1 86.321615   Top5 98.411458   BatchTime 0.313905   LR 0.001150
0.83074534
0.83114547
0.83218020
0.83377540
0.83336079
0.83290237
0.83254868
0.83211845
0.83219188
0.83222967
0.83209175
0.83188003
0.83173388
0.83129460
0.83134896
0.83123702
0.83139759
0.83156353
0.83179832
0.83177501
0.83166462
0.83128172
INFO - Training [37][   80/  196]   Loss 0.390255   Top1 86.528320   Top5 98.496094   BatchTime 0.301712   LR 0.001147
0.83103067
0.83064204
0.83009297
0.82990223
0.82922441
0.82849729
0.82866585
0.82849187
0.82798731
0.82774794
0.82730442
0.82724094
0.82686287
0.82649624
0.82573086
0.82514960
0.82428509
0.82332999
0.82280761
0.82246172
0.82217389
0.82156634
0.82112890
INFO - Training [37][  100/  196]   Loss 0.381781   Top1 86.824219   Top5 98.488281   BatchTime 0.295857   LR 0.001144
0.82056475
0.82023549
0.81970966
0.81952715
0.81932145
0.81906128
0.81845272
0.81865931
0.81816524
0.81734920
0.81624615
0.81563658
0.81469601
0.81359726
0.81307548
0.81443393
INFO - Training [37][  120/  196]   Loss 0.379929   Top1 86.927083   Top5 98.570964   BatchTime 0.288260   LR 0.001142
0.81573635
0.81565386
0.81551123
0.81533074
0.81568193
0.81607211
0.81625140
0.81600004
0.81601435
0.81558216
0.81546378
0.81578588
0.81600219
0.81837732
0.81940180
0.81946713
0.82042509
0.82142448
0.82387412
0.82362527
0.82371497
INFO - Training [37][  140/  196]   Loss 0.379384   Top1 86.997768   Top5 98.638393   BatchTime 0.286768   LR 0.001139
0.82341701
0.82320601
0.82314557
0.82309490
0.82308084
0.82302523
0.82315660
0.82323557
0.82364142
0.82383078
0.82396650
0.82438272
0.82455939
0.82439649
0.82462466
0.82478523
0.82476133
0.82619131
0.82572830
0.82567251
0.82556045
0.82531077
INFO - Training [37][  160/  196]   Loss 0.380422   Top1 86.975098   Top5 98.627930   BatchTime 0.286851   LR 0.001136
0.82515895
0.82516986
0.82537609
0.82542890
0.82539809
0.82518297
0.82544518
0.82522547
0.82506508
0.82530528
0.82519960
0.82512993
0.82486671
0.82471806
0.82454264
0.82422900
0.82354587
0.82287627
0.82250631
0.82841808
INFO - Training [37][  180/  196]   Loss 0.381446   Top1 86.911892   Top5 98.574219   BatchTime 0.287134   LR 0.001133
0.82762009
0.82730871
0.82730281
0.82719690
0.82695806
0.82622099
0.82523370
0.82323593
0.81897634
0.81556982
0.81513554
0.81739974
0.81709188
0.81921279
INFO - ==> Top1: 86.874    Top5: 98.578    Loss: 0.382
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.350318   Top1 88.535156   Top5 99.453125   BatchTime 0.129822
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.2148)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0569)
features.2.conv.0 tensor(0.0211)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0694)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0352)
features.4.conv.3 tensor(0.1076)
features.4.conv.6 tensor(0.0812)
features.5.conv.0 tensor(0.0295)
features.5.conv.3 tensor(0.0804)
features.5.conv.6 tensor(0.0881)
features.6.conv.0 tensor(0.0280)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0594)
features.7.conv.0 tensor(0.0424)
features.7.conv.3 tensor(0.1128)
features.7.conv.6 tensor(0.1028)
features.8.conv.0 tensor(0.0423)
features.8.conv.3 tensor(0.1137)
features.8.conv.6 tensor(0.1388)
features.9.conv.0 tensor(0.0672)
features.9.conv.3 tensor(0.1536)
features.9.conv.6 tensor(0.1189)
features.10.conv.0 tensor(0.0352)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0701)
features.11.conv.0 tensor(0.1818)
features.11.conv.3 tensor(0.1308)
features.11.conv.6 tensor(0.2120)
features.12.conv.0 tensor(0.5602)
features.12.conv.3 tensor(0.1705)
features.12.conv.6 tensor(0.4621)
features.13.conv.0 tensor(0.0834)
features.13.conv.3 tensor(0.1262)
features.13.conv.6 tensor(0.0781)
features.14.conv.0 tensor(0.9157)
features.14.conv.3 tensor(0.0965)
features.14.conv.6 tensor(0.9591)
features.15.conv.0 tensor(0.8794)
features.15.conv.3 tensor(0.0556)
features.15.conv.6 tensor(0.9613)
features.16.conv.0 tensor(0.0991)
features.16.conv.3 tensor(0.1116)
features.16.conv.6 tensor(0.3278)
conv.0 tensor(0.1491)
tensor(863837.) 2188896.0
INFO - Validation [37][   40/   40]   Loss 0.331411   Top1 88.880000   Top5 99.550000   BatchTime 0.092283
INFO - ==> Top1: 88.880    Top5: 99.550    Loss: 0.331
INFO - ==> Sparsity : 0.395
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
0.82312065
0.82590306
0.82810795
0.82988060
0.83060235
0.83160847
0.83215392
0.83255744
0.83345234
0.83299828
0.83248037
0.83223110
0.83206606
0.83189356
0.83192146
0.83160186
0.83120894
0.83065861
0.83042562
0.82960302
0.82954371
0.82883304
INFO - Training [38][   20/  196]   Loss 0.405319   Top1 86.308594   Top5 98.105469   BatchTime 0.373382   LR 0.001128
0.82837284
0.82784742
0.82819408
0.82855207
0.82851589
0.83017832
0.83335000
0.83548868
0.83546793
0.83540523
0.83537149
0.83576846
0.83562428
0.83552998
0.83569980
INFO - Training [38][   40/  196]   Loss 0.391672   Top1 86.640625   Top5 98.300781   BatchTime 0.321780   LR 0.001125
0.83562797
0.83548141
0.83512664
0.83495057
0.83480930
0.83469015
0.83389688
0.83312774
0.83302397
0.83283234
0.83276874
0.83235967
0.83184856
0.83163261
0.83127809
0.83088964
0.83098060
0.83084220
0.83063608
0.83025652
0.83031899
0.83003438
0.82990533
INFO - Training [38][   60/  196]   Loss 0.385341   Top1 86.803385   Top5 98.398438   BatchTime 0.301629   LR 0.001122
0.82971579
0.82963878
0.82961917
0.82944006
0.82953316
0.82912213
0.82887381
0.83024836
0.82988292
0.82945591
0.82896477
0.82851309
0.82802254
0.82735628
0.82695526
0.82657599
0.82629114
0.82623059
0.82620728
0.82620704
0.82572424
0.82482600
INFO - Training [38][   80/  196]   Loss 0.387772   Top1 86.665039   Top5 98.525391   BatchTime 0.292462   LR 0.001119
0.82342947
0.82175833
0.82022333
0.81892866
0.81791931
0.81761992
0.81640291
0.81526524
0.81445330
0.81478637
0.81448603
0.81470293
0.81470239
0.81525820
0.81592518
0.81693131
0.81817222
0.81868619
0.81959015
0.81949693
0.81910414
0.81877321
INFO - Training [38][  100/  196]   Loss 0.380739   Top1 86.882812   Top5 98.585938   BatchTime 0.291995   LR 0.001116
0.81934756
0.82113254
0.82304573
0.82367200
0.82430267
0.82478535
0.82538438
0.82749653
0.82731128
0.82721263
0.82713991
0.82680577
0.82657623
0.82648379
0.83067852
INFO - Training [38][  120/  196]   Loss 0.374677   Top1 87.125651   Top5 98.662109   BatchTime 0.287106   LR 0.001112
0.83040094
0.83017373
0.83006859
0.82988536
0.82972413
0.82949460
0.82925409
0.82922882
0.82903636
0.82882410
0.82866645
0.82832092
0.82835782
0.82842523
0.82847673
0.82835716
0.82805759
0.82791495
0.82755756
0.82654327
0.82614285
0.82826626
0.82697874
0.82664305
INFO - Training [38][  140/  196]   Loss 0.373588   Top1 87.190290   Top5 98.719308   BatchTime 0.281776   LR 0.001109
0.82642847
0.82640755
0.82689333
0.82850701
0.82959169
0.82941329
0.82919818
0.82884651
0.82845539
0.82778984
0.82676429
0.82556570
0.82423931
0.82071555
0.81402642
0.80752480
INFO - Training [38][  160/  196]   Loss 0.378406   Top1 87.033691   Top5 98.686523   BatchTime 0.278460   LR 0.001106
0.81102300
0.81220204
0.81740803
0.82269323
0.82599574
0.82761890
0.82892364
0.82975644
0.82988530
0.82992971
0.83004051
0.82989907
0.83003908
0.83005220
0.83020467
0.83046025
0.83073020
0.83111596
0.83642983
0.83602589
0.83605415
0.83602470
0.83572197
INFO - Training [38][  180/  196]   Loss 0.380844   Top1 86.918403   Top5 98.613281   BatchTime 0.276723   LR 0.001103
0.83585966
0.83578539
0.83575690
0.83579701
0.83568627
0.83558196
0.83515155
0.83469009
0.83429861
0.83397442
0.83382326
0.83311325
0.83244234
0.83213288
INFO - ==> Top1: 86.980    Top5: 98.610    Loss: 0.380
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [38][   20/   40]   Loss 0.340137   Top1 88.886719   Top5 99.531250   BatchTime 0.130596
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0556)
features.2.conv.0 tensor(0.0272)
features.2.conv.3 tensor(0.0664)
features.2.conv.6 tensor(0.0715)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0345)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.1019)
features.4.conv.6 tensor(0.0806)
features.5.conv.0 tensor(0.0314)
features.5.conv.3 tensor(0.0787)
features.5.conv.6 tensor(0.0859)
features.6.conv.0 tensor(0.0326)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0582)
features.7.conv.0 tensor(0.0471)
features.7.conv.3 tensor(0.1123)
features.7.conv.6 tensor(0.1057)
features.8.conv.0 tensor(0.0670)
features.8.conv.3 tensor(0.1111)
features.8.conv.6 tensor(0.1316)
features.9.conv.0 tensor(0.0737)
features.9.conv.3 tensor(0.1508)
features.9.conv.6 tensor(0.1125)
features.10.conv.0 tensor(0.0332)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0754)
features.11.conv.0 tensor(0.1888)
features.11.conv.3 tensor(0.1262)
features.11.conv.6 tensor(0.1771)
features.12.conv.0 tensor(0.1594)
features.12.conv.3 tensor(0.1771)
features.12.conv.6 tensor(0.5095)
features.13.conv.0 tensor(0.0842)
features.13.conv.3 tensor(0.1252)
features.13.conv.6 tensor(0.0840)
features.14.conv.0 tensor(0.8992)
features.14.conv.3 tensor(0.0961)
features.14.conv.6 tensor(0.9431)
features.15.conv.0 tensor(0.8852)
features.15.conv.3 tensor(0.0557)
features.15.conv.6 tensor(0.9629)
features.16.conv.0 tensor(0.0749)
features.16.conv.3 tensor(0.1104)
features.16.conv.6 tensor(0.3241)
conv.0 tensor(0.1349)
tensor(829613.) 2188896.0
INFO - Validation [38][   40/   40]   Loss 0.324368   Top1 89.210000   Top5 99.590000   BatchTime 0.092647
INFO - ==> Top1: 89.210    Top5: 99.590    Loss: 0.324
INFO - ==> Sparsity : 0.379
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
0.83203131
0.83203423
0.83215332
0.83216006
0.83273304
0.83335841
0.83312094
0.83277929
0.83281261
0.83266133
0.83270317
0.83273810
0.83255517
0.83237231
0.83244067
0.83265853
0.83274078
0.83268851
0.83282143
0.83261544
0.83241713
0.83224005
INFO - Training [39][   20/  196]   Loss 0.395829   Top1 86.699219   Top5 98.105469   BatchTime 0.357241   LR 0.001097
0.83199561
0.83158410
0.83152920
0.83168131
0.83191156
0.83181280
0.83206785
0.83189416
0.83170331
0.83142900
0.83118403
0.83096963
0.83054918
0.82998723
0.82971239
0.82912540
0.82845491
INFO - Training [39][   40/  196]   Loss 0.392345   Top1 86.816406   Top5 98.369141   BatchTime 0.305845   LR 0.001094
0.82798249
0.82786822
0.82777625
0.82787055
0.82793266
0.82787853
0.82799608
0.82796532
0.82766205
0.82721692
0.82695365
0.82680523
0.82693535
0.82660842
0.82649189
0.82649958
0.82641119
0.82628942
0.82632977
0.82639849
0.82669598
0.82688957
0.82820380
0.82826179
0.82810193
INFO - Training [39][   60/  196]   Loss 0.390548   Top1 86.790365   Top5 98.444010   BatchTime 0.282672   LR 0.001090
0.82784116
0.82738775
0.82661909
0.82573408
0.82518798
0.82576227
0.82489914
0.82444358
0.82419819
0.82427990
0.82434613
0.82453996
0.82418913
0.82370239
0.82351267
0.82315391
0.82288677
0.82314318
0.82378566
0.82436967
INFO - Training [39][   80/  196]   Loss 0.391001   Top1 86.831055   Top5 98.505859   BatchTime 0.288142   LR 0.001087
0.82400066
0.82459617
0.82527888
0.82522440
0.82511032
0.82534301
0.82557338
0.82602280
0.82777560
0.82763118
0.82753563
0.82713282
0.82647097
INFO - Training [39][  100/  196]   Loss 0.384640   Top1 86.906250   Top5 98.542969   BatchTime 0.286014   LR 0.001084
0.82625395
0.82630926
0.82672215
0.82669455
0.82663810
0.82680690
0.82689393
0.82692486
0.82678545
0.82683903
0.82709831
0.82742560
0.82753032
0.82754540
0.82769549
0.82769740
0.82838851
0.83000857
0.82940912
0.82907385
0.82877594
0.82882780
0.82851475
INFO - Training [39][  120/  196]   Loss 0.377362   Top1 87.073568   Top5 98.629557   BatchTime 0.283184   LR 0.001080
0.82820851
0.82782418
0.82768667
0.82750392
0.82739598
0.82727343
0.82716167
0.82677335
0.82635599
0.82615572
0.82592136
0.82587177
0.82563454
0.82490969
0.82434589
0.82366478
0.82306904
0.82229894
0.82176155
0.82159477
0.82148916
0.82154322
INFO - Training [39][  140/  196]   Loss 0.375661   Top1 87.173549   Top5 98.702567   BatchTime 0.280688   LR 0.001077
0.82119179
0.82074738
0.82035428
0.81937742
0.81839073
0.81781960
0.81788307
0.81746387
0.81779474
0.81868213
0.82163149
0.82138866
0.82119387
0.82101887
0.82065195
0.82035595
INFO - Training [39][  160/  196]   Loss 0.378012   Top1 87.016602   Top5 98.706055   BatchTime 0.278515   LR 0.001073
0.82008988
0.81997317
0.82129622
0.82387829
0.82368261
0.82373685
0.82403159
0.82529986
0.82484978
0.82467043
0.82378483
0.82351089
0.82348162
0.82365340
0.82364953
0.82353818
0.82388216
0.82381701
0.82397777
0.82376277
0.82363379
0.82360703
INFO - Training [39][  180/  196]   Loss 0.377646   Top1 86.985677   Top5 98.641493   BatchTime 0.276951   LR 0.001070
0.82337517
0.82322377
0.82324833
0.82307243
0.82336217
0.82367671
0.82394469
0.82408029
0.82431906
0.82424897
0.82412291
0.82410270
0.82420307
0.82419026
0.82413000
INFO - ==> Top1: 86.982    Top5: 98.636    Loss: 0.377
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82402968
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [39][   20/   40]   Loss 0.332696   Top1 89.667969   Top5 99.648438   BatchTime 0.138012
INFO - Validation [39][   40/   40]   Loss 0.323647   Top1 89.750000   Top5 99.750000   BatchTime 0.098053
INFO - ==> Top1: 89.750    Top5: 99.750    Loss: 0.324
INFO - ==> Sparsity : 0.394
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.2188)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0525)
features.2.conv.0 tensor(0.0231)
features.2.conv.3 tensor(0.0664)
features.2.conv.6 tensor(0.0709)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0371)
features.4.conv.0 tensor(0.0340)
features.4.conv.3 tensor(0.1036)
features.4.conv.6 tensor(0.0807)
features.5.conv.0 tensor(0.0337)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0918)
features.6.conv.0 tensor(0.0316)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0591)
features.7.conv.0 tensor(0.0471)
features.7.conv.3 tensor(0.1071)
features.7.conv.6 tensor(0.1025)
features.8.conv.0 tensor(0.0585)
features.8.conv.3 tensor(0.1079)
features.8.conv.6 tensor(0.1932)
features.9.conv.0 tensor(0.0723)
features.9.conv.3 tensor(0.1479)
features.9.conv.6 tensor(0.1274)
features.10.conv.0 tensor(0.0368)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0726)
features.11.conv.0 tensor(0.1916)
features.11.conv.3 tensor(0.1269)
features.11.conv.6 tensor(0.1777)
features.12.conv.0 tensor(0.2563)
features.12.conv.3 tensor(0.1819)
features.12.conv.6 tensor(0.4723)
features.13.conv.0 tensor(0.0794)
features.13.conv.3 tensor(0.1283)
features.13.conv.6 tensor(0.0883)
features.14.conv.0 tensor(0.8911)
features.14.conv.3 tensor(0.0951)
features.14.conv.6 tensor(0.9417)
features.15.conv.0 tensor(0.8946)
features.15.conv.3 tensor(0.0582)
features.15.conv.6 tensor(0.9561)
features.16.conv.0 tensor(0.0789)
features.16.conv.3 tensor(0.1113)
features.16.conv.6 tensor(0.3217)
conv.0 tensor(0.2038)
tensor(861924.) 2188896.0
0.82414728
0.82401747
0.82393312
0.82420594
0.82506371
0.82424986
0.82397652
0.82418895
0.82421988
0.82434541
0.82452476
0.82628679
0.82754147
0.82724237
0.82696861
0.82662600
0.82631117
0.82620573
0.82599068
0.82607973
INFO - Training [40][   20/  196]   Loss 0.401096   Top1 86.660156   Top5 98.203125   BatchTime 0.358277   LR 0.001064
0.82667583
0.82909459
0.82904482
0.82906014
0.82895333
0.82897073
0.82896167
0.82895434
0.82877308
0.82862276
0.82825905
0.82769805
0.82734627
0.82735938
0.82730764
0.82708895
0.82678223
0.82666206
0.82638252
0.82614934
0.82597691
0.82608438
INFO - Training [40][   40/  196]   Loss 0.398550   Top1 86.484375   Top5 98.291016   BatchTime 0.321449   LR 0.001060
0.82582200
0.82540727
0.82499707
0.82438546
0.82412398
0.82384616
0.82361388
0.82344091
0.82289094
0.82311338
0.82339042
0.82313132
0.82284278
0.82267040
0.82199776
0.82232857
INFO - Training [40][   60/  196]   Loss 0.389841   Top1 86.686198   Top5 98.457031   BatchTime 0.301795   LR 0.001056
0.82251239
0.82208496
0.82206565
0.82161433
0.82088858
0.81979167
0.81863695
0.81837040
0.81738210
0.81544411
0.81353253
0.81137079
0.80988407
0.81055039
0.81020701
0.81276870
0.81105220
0.81069022
0.81277913
0.81434470
0.81558633
0.81654137
INFO - Training [40][   80/  196]   Loss 0.386772   Top1 86.796875   Top5 98.598633   BatchTime 0.294762   LR 0.001053
0.81752747
0.81855047
0.81891310
0.81949264
0.81998307
0.82059723
0.82123107
0.82338357
0.82603729
0.82569540
0.82547361
0.82542890
0.82534105
0.82516074
0.82492840
0.82443017
0.82398832
0.82339710
0.82266921
0.82229614
INFO - Training [40][  100/  196]   Loss 0.380735   Top1 86.996094   Top5 98.648438   BatchTime 0.295073   LR 0.001049
0.82179898
0.82121497
0.82039768
0.81959790
0.81929916
0.81860912
0.81790632
0.81713039
0.81630701
0.81545597
0.81478494
0.81486869
0.81438351
0.81415862
0.81436908
0.81506395
0.81599134
0.81657898
0.81672090
0.81700885
0.81737953
0.81789285
INFO - Training [40][  120/  196]   Loss 0.375122   Top1 87.141927   Top5 98.730469   BatchTime 0.291392   LR 0.001045
0.81847709
0.81919283
0.82036710
0.82102942
0.82304436
0.82329530
0.82337987
0.82308161
0.82256854
0.82229710
0.82235157
0.82236910
0.82213044
0.82198882
INFO - Training [40][  140/  196]   Loss 0.373167   Top1 87.218192   Top5 98.783482   BatchTime 0.289444   LR 0.001042
0.82181478
0.82138050
0.82103521
0.82001603
0.81986940
0.81987351
0.81967390
0.81907821
0.81892264
0.81849736
0.81881690
0.82265633
0.82267535
0.82199234
0.82174730
0.82156104
0.82162756
0.82164878
0.82191986
0.82205164
0.82244927
0.82246643
0.82242954
INFO - Training [40][  160/  196]   Loss 0.374616   Top1 87.197266   Top5 98.759766   BatchTime 0.286060   LR 0.001038
0.82255894
0.82368457
0.82339424
0.82245189
0.82168305
0.82115340
0.82041180
0.82014555
0.81942230
0.81854218
0.81738520
0.81647235
0.81548727
0.81444180
0.81386018
0.81365424
0.81361419
0.81385911
0.81475383
0.81490809
0.81538838
0.81523925
0.81480062
0.81493795
0.81522131
INFO - Training [40][  180/  196]   Loss 0.375196   Top1 87.154948   Top5 98.697917   BatchTime 0.281581   LR 0.001034
0.81564528
0.81604880
0.81579643
0.81562477
0.81636184
0.81721348
0.81695414
0.81689352
0.81700701
0.81708103
0.81719643
0.81717402
********************pre-trained*****************
INFO - ==> Top1: 87.154    Top5: 98.724    Loss: 0.375
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.362038   Top1 88.808594   Top5 99.648438   BatchTime 0.135008
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.2324)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0543)
features.2.conv.0 tensor(0.0188)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0715)
features.3.conv.0 tensor(0.0220)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0336)
features.4.conv.0 tensor(0.0410)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0799)
features.5.conv.0 tensor(0.0358)
features.5.conv.3 tensor(0.0787)
features.5.conv.6 tensor(0.0863)
features.6.conv.0 tensor(0.0306)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0612)
features.7.conv.0 tensor(0.0603)
features.7.conv.3 tensor(0.1377)
features.7.conv.6 tensor(0.1025)
features.8.conv.0 tensor(0.0588)
features.8.conv.3 tensor(0.1082)
features.8.conv.6 tensor(0.2247)
features.9.conv.0 tensor(0.0676)
features.9.conv.3 tensor(0.1496)
features.9.conv.6 tensor(0.1419)
features.10.conv.0 tensor(0.0363)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0697)
features.11.conv.0 tensor(0.1952)
features.11.conv.3 tensor(0.1292)
features.11.conv.6 tensor(0.3418)
features.12.conv.0 tensor(0.2791)
features.12.conv.3 tensor(0.1821)
features.12.conv.6 tensor(0.4826)
features.13.conv.0 tensor(0.0816)
features.13.conv.3 tensor(0.1254)
features.13.conv.6 tensor(0.1131)
features.14.conv.0 tensor(0.8700)
features.14.conv.3 tensor(0.0920)
features.14.conv.6 tensor(0.9234)
features.15.conv.0 tensor(0.8914)
features.15.conv.3 tensor(0.0565)
features.15.conv.6 tensor(0.9628)
features.16.conv.0 tensor(0.0975)
features.16.conv.3 tensor(0.1104)
features.16.conv.6 tensor(0.3363)
conv.0 tensor(0.1335)
tensor(849716.) 2188896.0
INFO - Validation [40][   40/   40]   Loss 0.350134   Top1 88.890000   Top5 99.700000   BatchTime 0.095927
INFO - ==> Top1: 88.890    Top5: 99.700    Loss: 0.350
INFO - ==> Sparsity : 0.388
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
0.81729037
0.81718856
0.81769288
0.81792313
0.81808954
0.81853414
0.81896198
0.81979728
0.82156754
0.82633883
0.82599062
0.82610321
0.82621342
0.82626772
0.82663691
0.82681400
0.82688659
0.82697880
0.82708126
INFO - Training [41][   20/  196]   Loss 0.396366   Top1 86.210938   Top5 97.968750   BatchTime 0.335846   LR 0.001027
0.82758200
0.82822865
0.82888144
0.83191347
0.83178979
0.83148086
0.83140749
0.83120656
0.83096582
0.83075076
0.83053327
0.83041775
0.83025718
0.83000892
0.82991809
0.82974201
0.82959557
0.82936275
0.82917750
0.82935357
0.82929981
INFO - Training [41][   40/  196]   Loss 0.401241   Top1 86.064453   Top5 98.261719   BatchTime 0.314552   LR 0.001023
0.82946968
0.82936060
0.82955819
0.82941055
0.82944816
0.82950413
0.82934755
0.82940590
0.82942593
0.82935101
0.82947314
0.82898575
0.82877225
0.82840329
0.82809740
0.82772928
0.82738310
0.82709432
0.82683724
0.82588387
0.82485056
0.82357460
INFO - Training [41][   60/  196]   Loss 0.384600   Top1 86.731771   Top5 98.430990   BatchTime 0.302373   LR 0.001020
0.82184082
0.81970471
0.81746459
0.81541085
0.81280017
0.81133443
0.81204879
0.81300396
0.81397301
0.81704468
0.81781322
0.81852794
0.81951195
0.82135600
0.82129347
INFO - Training [41][   80/  196]   Loss 0.380866   Top1 86.699219   Top5 98.554688   BatchTime 0.290088   LR 0.001016
0.82129925
0.82207263
0.82222021
0.82239074
0.82289982
0.82287979
0.82309967
0.82439387
0.82689911
0.82902521
0.82947499
0.82964081
0.83105844
0.83073294
0.83020264
0.82962686
0.82891327
0.82838249
0.82780504
0.82755762
0.82744402
0.82720184
0.82702792
0.82682610
0.82693368
INFO - Training [41][  100/  196]   Loss 0.372752   Top1 87.035156   Top5 98.558594   BatchTime 0.282136   LR 0.001012
0.82617557
0.82605612
0.82610184
0.82623076
0.82616115
0.82602757
0.82583594
0.82575887
0.82579434
0.82585692
0.82567567
0.82552797
0.82540298
0.82543641
0.82541746
0.82533354
INFO - Training [41][  120/  196]   Loss 0.367220   Top1 87.333984   Top5 98.642578   BatchTime 0.277137   LR 0.001008
0.82517487
0.82508910
0.82494503
0.82465297
0.82446259
0.82393074
0.82394856
0.82390147
0.82399541
0.82411933
0.82404262
0.82380557
0.82376719
0.82383060
0.82362384
0.82364899
0.82344198
0.82331520
0.82337052
0.82348120
0.82391256
0.82490760
INFO - Training [41][  140/  196]   Loss 0.365793   Top1 87.385603   Top5 98.691406   BatchTime 0.276848   LR 0.001004
0.82473171
0.82457614
0.82444388
0.82425910
0.82400179
0.82374740
0.82355893
0.82348591
0.82342845
0.82290965
0.82267517
0.82219934
0.82194316
0.82162160
0.82141364
0.82135499
0.82118630
0.82110417
0.82098150
0.82121897
0.82112712
0.82103616
INFO - Training [41][  160/  196]   Loss 0.371113   Top1 87.216797   Top5 98.674316   BatchTime 0.276029   LR 0.001000
0.82095903
0.82108790
0.82135010
0.82157606
0.82154953
0.82144213
0.82116270
0.82097775
0.82076442
0.82049471
0.82049882
0.82046777
0.82036859
0.82031763
0.81959212
0.81905389
INFO - Training [41][  180/  196]   Loss 0.371390   Top1 87.154948   Top5 98.639323   BatchTime 0.273068   LR 0.000996
0.81852365
0.81802863
0.81756991
0.81726933
0.81693226
0.81663096
0.81637824
0.81602794
0.81586725
0.81585360
0.81580168
0.81543618
0.81523532
0.81450105
0.81372684
0.81253552
0.80954480
0.80570191
********************pre-trained*****************
INFO - ==> Top1: 87.178    Top5: 98.632    Loss: 0.371
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [41][   20/   40]   Loss 0.356293   Top1 88.320312   Top5 99.355469   BatchTime 0.135162
INFO - Validation [41][   40/   40]   Loss 0.334943   Top1 89.030000   Top5 99.540000   BatchTime 0.095736
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0111)
features.1.conv.3 tensor(0.0995)
features.1.conv.6 tensor(0.0586)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0706)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0362)
features.4.conv.0 tensor(0.0363)
features.4.conv.3 tensor(0.0990)
features.4.conv.6 tensor(0.0786)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0839)
features.5.conv.6 tensor(0.0900)
features.6.conv.0 tensor(0.0308)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0605)
features.7.conv.0 tensor(0.0605)
features.7.conv.3 tensor(0.1047)
features.7.conv.6 tensor(0.1007)
features.8.conv.0 tensor(0.0616)
features.8.conv.3 tensor(0.1036)
features.8.conv.6 tensor(0.1178)
features.9.conv.0 tensor(0.4716)
features.9.conv.3 tensor(0.1516)
features.9.conv.6 tensor(0.2275)
features.10.conv.0 tensor(0.0337)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0722)
features.11.conv.0 tensor(0.1827)
features.11.conv.3 tensor(0.1333)
features.11.conv.6 tensor(0.2191)
features.12.conv.0 tensor(0.2857)
features.12.conv.3 tensor(0.1767)
features.12.conv.6 tensor(0.4933)
features.13.conv.0 tensor(0.0788)
features.13.conv.3 tensor(0.1271)
features.13.conv.6 tensor(0.1852)
features.14.conv.0 tensor(0.8906)
features.14.conv.3 tensor(0.0958)
features.14.conv.6 tensor(0.9470)
features.15.conv.0 tensor(0.8882)
features.15.conv.3 tensor(0.0568)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.1030)
features.16.conv.3 tensor(0.1159)
features.16.conv.6 tensor(0.4089)
conv.0 tensor(0.1610)
tensor(900276.) 2188896.0
INFO - ==> Top1: 89.030    Top5: 99.540    Loss: 0.335
INFO - ==> Sparsity : 0.411
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
0.80213761
0.80145347
0.80155689
0.80097121
0.80075938
0.80104190
0.80168271
0.80257046
0.80421150
0.80549687
0.80879205
0.81087989
0.81197661
0.81245106
0.81275654
0.81286764
0.81302822
0.81316525
0.81316596
0.81326371
INFO - Training [42][   20/  196]   Loss 0.392585   Top1 86.464844   Top5 98.515625   BatchTime 0.347430   LR 0.000988
0.81360614
0.81534582
0.81992060
0.82003295
0.82013905
0.82123882
0.82373673
0.82372499
0.82350379
0.82320255
0.82309902
0.82397372
0.82446623
0.82427037
0.82429242
0.82432520
0.82439435
0.82437462
0.82414144
0.82361650
0.82341766
0.82305580
0.82262266
INFO - Training [42][   40/  196]   Loss 0.394756   Top1 86.445312   Top5 98.437500   BatchTime 0.300315   LR 0.000984
0.82226425
0.82180965
0.82101083
0.82029909
0.81992954
0.81948525
0.81912345
0.81891769
0.81857407
0.81817335
0.81813371
0.81797296
0.81781518
0.81794697
0.81764871
0.81729126
INFO - Training [42][   60/  196]   Loss 0.388519   Top1 86.692708   Top5 98.535156   BatchTime 0.287076   LR 0.000980
0.81707609
0.81680918
0.81744206
0.81764883
0.81776232
0.81779963
0.81779128
0.81757915
0.81765795
0.81772524
0.81772119
0.81771094
0.81761700
0.81789309
0.81870598
0.81898087
0.81904703
0.82020426
0.82066160
0.82080972
0.82054502
0.82024050
INFO - Training [42][   80/  196]   Loss 0.382074   Top1 86.914062   Top5 98.593750   BatchTime 0.282967   LR 0.000976
0.82013685
0.81981403
0.81962985
0.81998420
0.81996202
0.81985915
0.81971556
0.81955135
0.81936550
0.81940347
0.81905770
0.81867248
0.81817812
0.81799060
0.81804287
0.81803375
0.81835884
0.81819886
0.81824499
0.81813413
0.81761032
0.81754780
INFO - Training [42][  100/  196]   Loss 0.368261   Top1 87.398438   Top5 98.640625   BatchTime 0.278866   LR 0.000972
0.81746596
0.81733143
0.81721395
0.81718135
0.81717885
0.81650102
0.81673938
0.81687123
0.81717092
0.81766206
0.81824064
0.81858039
0.81885356
0.81883138
0.81915224
0.81990904
INFO - Training [42][  120/  196]   Loss 0.363356   Top1 87.620443   Top5 98.710938   BatchTime 0.275708   LR 0.000968
0.82639515
0.82615572
0.82594019
0.82569784
0.82507539
0.82461303
0.82427675
0.82398236
0.82386631
0.82377601
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [42][  140/  196]   Loss nan   Top1 82.098214   Top5 95.231585   BatchTime 0.274233   LR 0.000964
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [42][  160/  196]   Loss nan   Top1 73.054199   Top5 89.611816   BatchTime 0.271013   LR 0.000959
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [42][  180/  196]   Loss nan   Top1 66.024306   Top5 85.188802   BatchTime 0.269744   LR 0.000955
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
********************pre-trained*****************
INFO - ==> Top1: 61.644    Top5: 82.338    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [42][   20/   40]   Loss 73.750423   Top1 9.863281   Top5 50.214844   BatchTime 0.132706
INFO - Validation [42][   40/   40]   Loss 74.013327   Top1 10.000000   Top5 50.000000   BatchTime 0.093662
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.0422)
features.15.conv.6 tensor(0.8318)
features.16.conv.0 tensor(0.0198)
features.16.conv.3 tensor(0.0324)
features.16.conv.6 tensor(0.2765)
conv.0 tensor(0.1430)
tensor(274973.) 2188896.0
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 74.013
INFO - ==> Sparsity : 0.126
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][   20/  196]   Loss nan   Top1 9.804688   Top5 50.019531   BatchTime 0.354381   LR 0.000947
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][   40/  196]   Loss nan   Top1 9.912109   Top5 50.068359   BatchTime 0.331603   LR 0.000943
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][   60/  196]   Loss nan   Top1 9.947917   Top5 50.084635   BatchTime 0.322272   LR 0.000939
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][   80/  196]   Loss nan   Top1 9.946289   Top5 49.926758   BatchTime 0.306904   LR 0.000934
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][  100/  196]   Loss nan   Top1 10.097656   Top5 50.179688   BatchTime 0.296617   LR 0.000930
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][  120/  196]   Loss nan   Top1 10.084635   Top5 50.110677   BatchTime 0.292301   LR 0.000926
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][  140/  196]   Loss nan   Top1 10.086496   Top5 50.058594   BatchTime 0.289197   LR 0.000921
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][  160/  196]   Loss nan   Top1 10.114746   Top5 50.205078   BatchTime 0.284167   LR 0.000917
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [43][  180/  196]   Loss nan   Top1 10.023872   Top5 50.043403   BatchTime 0.279974   LR 0.000912
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - ==> Top1: 10.048    Top5: 49.984    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [43][   20/   40]   Loss 65.138189   Top1 9.863281   Top5 50.039062   BatchTime 0.138983
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0276)
features.16.conv.3 tensor(0.0308)
features.16.conv.6 tensor(0.3625)
conv.0 tensor(0.1573)
tensor(180306.) 2188896.0
INFO - Validation [43][   40/   40]   Loss 65.351309   Top1 10.000000   Top5 50.000000   BatchTime 0.097189
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 65.351
INFO - ==> Sparsity : 0.082
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][   20/  196]   Loss nan   Top1 10.078125   Top5 49.531250   BatchTime 0.338319   LR 0.000904
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][   40/  196]   Loss nan   Top1 10.175781   Top5 49.658203   BatchTime 0.298234   LR 0.000900
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][   60/  196]   Loss nan   Top1 10.305990   Top5 49.414062   BatchTime 0.281597   LR 0.000895
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][   80/  196]   Loss nan   Top1 10.356445   Top5 49.667969   BatchTime 0.273560   LR 0.000891
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][  100/  196]   Loss nan   Top1 10.238281   Top5 49.785156   BatchTime 0.268343   LR 0.000886
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][  120/  196]   Loss nan   Top1 10.283203   Top5 49.781901   BatchTime 0.272618   LR 0.000882
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][  140/  196]   Loss nan   Top1 10.242746   Top5 49.916295   BatchTime 0.273338   LR 0.000877
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][  160/  196]   Loss nan   Top1 10.273438   Top5 49.951172   BatchTime 0.271804   LR 0.000873
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [44][  180/  196]   Loss nan   Top1 10.262587   Top5 50.075955   BatchTime 0.270367   LR 0.000868
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - ==> Top1: 10.258    Top5: 50.176    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
nan
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [44][   20/   40]   Loss 55.528798   Top1 9.863281   Top5 50.214844   BatchTime 0.136563
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0
INFO - Validation [44][   40/   40]   Loss 55.722855   Top1 10.000000   Top5 50.000000   BatchTime 0.096436
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 55.723
INFO - ==> Sparsity : 0.072
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0348)
features.16.conv.3 tensor(0.0302)
features.16.conv.6 tensor(0.2852)
conv.0 tensor(0.1556)
tensor(156943.) 2188896.0
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][   20/  196]   Loss nan   Top1 10.078125   Top5 50.644531   BatchTime 0.347800   LR 0.000860
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][   40/  196]   Loss nan   Top1 9.755859   Top5 50.263672   BatchTime 0.304270   LR 0.000855
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][   60/  196]   Loss nan   Top1 9.674479   Top5 50.169271   BatchTime 0.291370   LR 0.000850
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][   80/  196]   Loss nan   Top1 9.570312   Top5 49.624023   BatchTime 0.283736   LR 0.000846
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][  100/  196]   Loss nan   Top1 9.664062   Top5 49.582031   BatchTime 0.279096   LR 0.000841
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][  120/  196]   Loss nan   Top1 9.749349   Top5 49.661458   BatchTime 0.274927   LR 0.000836
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][  140/  196]   Loss nan   Top1 9.849330   Top5 49.835379   BatchTime 0.271762   LR 0.000832
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][  160/  196]   Loss nan   Top1 9.853516   Top5 49.763184   BatchTime 0.268469   LR 0.000827
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [45][  180/  196]   Loss nan   Top1 9.878472   Top5 49.685330   BatchTime 0.265487   LR 0.000822
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - ==> Top1: 9.848    Top5: 49.576    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [45][   20/   40]   Loss 55.574403   Top1 9.863281   Top5 50.039062   BatchTime 0.152907
INFO - Validation [45][   40/   40]   Loss 55.736403   Top1 10.000000   Top5 50.000000   BatchTime 0.103682
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 55.736
INFO - ==> Sparsity : 0.075
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0403)
features.16.conv.3 tensor(0.0316)
features.16.conv.6 tensor(0.2974)
conv.0 tensor(0.1598)
tensor(163267.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][   20/  196]   Loss nan   Top1 10.292969   Top5 51.250000   BatchTime 0.325541   LR 0.000814
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][   40/  196]   Loss nan   Top1 10.117188   Top5 50.468750   BatchTime 0.291877   LR 0.000809
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][   60/  196]   Loss nan   Top1 9.915365   Top5 50.136719   BatchTime 0.278039   LR 0.000804
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][   80/  196]   Loss nan   Top1 9.863281   Top5 50.078125   BatchTime 0.276582   LR 0.000799
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][  100/  196]   Loss nan   Top1 9.882812   Top5 49.781250   BatchTime 0.274373   LR 0.000794
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][  120/  196]   Loss nan   Top1 9.960938   Top5 49.837240   BatchTime 0.271558   LR 0.000789
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][  140/  196]   Loss nan   Top1 9.916295   Top5 49.849330   BatchTime 0.269807   LR 0.000785
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][  160/  196]   Loss nan   Top1 9.951172   Top5 49.875488   BatchTime 0.269934   LR 0.000780
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [46][  180/  196]   Loss nan   Top1 9.947917   Top5 49.891493   BatchTime 0.268337   LR 0.000775
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
********************pre-trained*****************
INFO - ==> Top1: 10.046    Top5: 49.958    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [46][   20/   40]   Loss 59.133834   Top1 9.863281   Top5 50.039062   BatchTime 0.131820
INFO - Validation [46][   40/   40]   Loss 59.335754   Top1 10.000000   Top5 50.000000   BatchTime 0.090423
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 59.336
INFO - ==> Sparsity : 0.161
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0494)
features.16.conv.3 tensor(0.0314)
features.16.conv.6 tensor(0.8984)
conv.0 tensor(0.1658)
tensor(351763.) 2188896.0
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][   20/  196]   Loss nan   Top1 9.648438   Top5 49.160156   BatchTime 0.326027   LR 0.000766
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][   40/  196]   Loss nan   Top1 9.697266   Top5 49.853516   BatchTime 0.293334   LR 0.000761
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][   60/  196]   Loss nan   Top1 10.078125   Top5 50.026042   BatchTime 0.300766   LR 0.000756
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][   80/  196]   Loss nan   Top1 9.990234   Top5 50.175781   BatchTime 0.292104   LR 0.000752
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][  100/  196]   Loss nan   Top1 10.070312   Top5 50.257812   BatchTime 0.284846   LR 0.000747
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][  120/  196]   Loss nan   Top1 9.915365   Top5 50.227865   BatchTime 0.282101   LR 0.000742
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][  140/  196]   Loss nan   Top1 9.935826   Top5 50.212054   BatchTime 0.279447   LR 0.000737
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][  160/  196]   Loss nan   Top1 9.997559   Top5 50.195312   BatchTime 0.276464   LR 0.000732
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [47][  180/  196]   Loss nan   Top1 10.041233   Top5 50.288628   BatchTime 0.274706   LR 0.000727
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - ==> Top1: 9.970    Top5: 50.250    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [47][   20/   40]   Loss 53.430234   Top1 10.078125   Top5 50.039062   BatchTime 0.134684
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0531)
features.16.conv.3 tensor(0.0311)
features.16.conv.6 tensor(0.8566)
conv.0 tensor(0.1723)
tensor(342127.) 2188896.0
INFO - Validation [47][   40/   40]   Loss 53.595900   Top1 10.000000   Top5 50.000000   BatchTime 0.094976
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 53.596
INFO - ==> Sparsity : 0.156
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][   20/  196]   Loss nan   Top1 10.468750   Top5 50.058594   BatchTime 0.341160   LR 0.000718
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][   40/  196]   Loss nan   Top1 10.068359   Top5 49.482422   BatchTime 0.303046   LR 0.000713
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][   60/  196]   Loss nan   Top1 9.635417   Top5 49.511719   BatchTime 0.287952   LR 0.000708
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][   80/  196]   Loss nan   Top1 9.780273   Top5 49.331055   BatchTime 0.283230   LR 0.000703
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][  100/  196]   Loss nan   Top1 9.816406   Top5 49.406250   BatchTime 0.282396   LR 0.000698
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][  120/  196]   Loss nan   Top1 9.843750   Top5 49.417318   BatchTime 0.280871   LR 0.000693
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][  140/  196]   Loss nan   Top1 9.829799   Top5 49.514509   BatchTime 0.282292   LR 0.000688
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][  160/  196]   Loss nan   Top1 9.877930   Top5 49.660645   BatchTime 0.278760   LR 0.000683
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [48][  180/  196]   Loss nan   Top1 9.969618   Top5 49.711372   BatchTime 0.275561   LR 0.000678
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
********************pre-trained*****************
INFO - ==> Top1: 10.010    Top5: 49.700    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [48][   20/   40]   Loss 54.676475   Top1 9.863281   Top5 50.039062   BatchTime 0.129621
INFO - Validation [48][   40/   40]   Loss 54.829307   Top1 10.000000   Top5 50.000000   BatchTime 0.090902
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 54.829
INFO - ==> Sparsity : 0.155
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0550)
features.16.conv.3 tensor(0.0307)
features.16.conv.6 tensor(0.8512)
conv.0 tensor(0.1661)
tensor(338220.) 2188896.0
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][   20/  196]   Loss nan   Top1 9.648438   Top5 49.453125   BatchTime 0.337450   LR 0.000669
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][   40/  196]   Loss nan   Top1 9.384766   Top5 49.023438   BatchTime 0.297837   LR 0.000664
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][   60/  196]   Loss nan   Top1 9.329427   Top5 49.401042   BatchTime 0.286681   LR 0.000659
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][   80/  196]   Loss nan   Top1 9.438477   Top5 49.218750   BatchTime 0.281041   LR 0.000654
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][  100/  196]   Loss nan   Top1 9.582031   Top5 49.453125   BatchTime 0.276937   LR 0.000649
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][  120/  196]   Loss nan   Top1 9.739583   Top5 49.856771   BatchTime 0.274507   LR 0.000644
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][  140/  196]   Loss nan   Top1 9.776786   Top5 49.790737   BatchTime 0.272559   LR 0.000639
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][  160/  196]   Loss nan   Top1 9.677734   Top5 49.653320   BatchTime 0.271204   LR 0.000634
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [49][  180/  196]   Loss nan   Top1 9.674479   Top5 49.752604   BatchTime 0.270516   LR 0.000629
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - ==> Top1: 9.696    Top5: 49.732    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [49][   20/   40]   Loss 53.237608   Top1 9.863281   Top5 50.039062   BatchTime 0.173683
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0577)
features.16.conv.3 tensor(0.0311)
INFO - Validation [49][   40/   40]   Loss 53.375948   Top1 10.000000   Top5 50.000000   BatchTime 0.113044
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 53.376
INFO - ==> Sparsity : 0.155
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 90.300   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.270   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 90.150   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-091958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
features.16.conv.6 tensor(0.8498)
conv.0 tensor(0.1696)
tensor(339656.) 2188896.0
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [50][   20/  196]   Loss nan   Top1 9.960938   Top5 50.332031   BatchTime 0.367306   LR 0.000620
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [50][   40/  196]   Loss nan   Top1 10.058594   Top5 50.341797   BatchTime 0.323303   LR 0.000615
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 77, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader,qat_model, teacher_model,criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 186, in train_one_epoch_slsq
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt