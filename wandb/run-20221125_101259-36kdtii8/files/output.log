Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
*************soft_pruning_mode*******************
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.95438832
0.95024323
0.92089206
0.90054411
0.89509124
0.89861453
0.90330869
0.90149170
0.90214515
INFO - Training [0][   20/  196]   Loss 1.579268   Top1 53.320312   Top5 89.101562   BatchTime 0.462311   LR 0.004999
0.89787596
0.88850850
0.87829107
0.87446558
0.88652313
0.89632010
0.90408021
0.90893525
0.91306359
0.91591311
0.91789484
0.91850126
0.91860545
0.91861773
0.91854435
0.91820168
0.91861421
0.91868412
0.91901898
0.91894430
INFO - Training [0][   40/  196]   Loss 1.494304   Top1 52.597656   Top5 89.667969   BatchTime 0.434832   LR 0.004995
0.91931134
0.91937721
0.91953886
0.92014730
0.91969669
0.92000389
0.92029393
0.92081439
0.92094141
0.92120492
0.92169356
0.92180991
0.92157930
0.92133915
0.92195755
0.92199928
INFO - Training [0][   60/  196]   Loss 1.388046   Top1 55.169271   Top5 90.748698   BatchTime 0.408817   LR 0.004989
0.92220461
0.92217356
0.92192358
0.92095363
0.91910416
0.91979766
0.92243612
0.92262042
0.92289573
0.92313248
0.92300230
0.92302692
0.92334950
0.92373723
0.92378306
0.92402762
0.92405939
0.92414600
0.92441028
0.92078167
0.90280908
0.90714115
0.91749030
INFO - Training [0][   80/  196]   Loss 1.322441   Top1 56.772461   Top5 91.625977   BatchTime 0.395343   LR 0.004980
0.91635454
0.91454554
0.91263950
0.91107589
0.91183352
0.91172469
0.91214168
0.91267568
0.91234082
0.91226709
0.91159183
0.91053981
0.91076928
0.91057020
0.90945321
0.90482271
0.90909946
0.91127831
0.91270691
INFO - Training [0][  100/  196]   Loss 1.262185   Top1 58.429688   Top5 92.265625   BatchTime 0.400880   LR 0.004968
0.91332853
0.91371071
0.91427100
0.91442847
0.91670465
0.91711688
0.91674888
0.91701591
0.91746789
0.91759491
0.91793853
0.91855532
0.91919512
0.91920060
0.91911131
0.91925573
0.91921246
0.91948390
0.91972238
0.92004108
0.92111409
INFO - Training [0][  120/  196]   Loss 1.212714   Top1 59.918620   Top5 92.815755   BatchTime 0.402868   LR 0.004954
0.92185044
0.92402381
0.92490083
0.92760265
0.93473577
0.93944901
0.94176877
0.94441861
0.94712687
0.94903588
0.95048457
0.95127523
0.95163918
0.95198071
0.95235980
0.95270967
0.95333225
0.95421028
0.95411348
INFO - Training [0][  140/  196]   Loss 1.181057   Top1 60.929129   Top5 93.200335   BatchTime 0.404147   LR 0.004938
0.95416617
0.95399016
0.95413005
0.95408022
0.95425081
0.95432663
0.95423192
0.95423084
0.95408058
0.95410693
0.95434165
0.95440066
0.95445228
0.95439935
0.95428997
0.95413971
0.95420927
0.95447767
0.95440620
0.95446867
0.95440495
0.95453197
INFO - Training [0][  160/  196]   Loss 1.159270   Top1 61.601562   Top5 93.386230   BatchTime 0.410809   LR 0.004919
0.95455700
0.95433855
0.95422721
0.95395243
0.95394236
0.95392543
0.95392567
0.95407307
0.95427883
0.95447779
0.95421559
0.95431721
0.95425022
0.95414126
0.95404816
0.95399249
0.95399523
0.95413065
0.95426559
INFO - Training [0][  180/  196]   Loss 1.136386   Top1 62.272135   Top5 93.561198   BatchTime 0.413428   LR 0.004897
0.95419276
0.95415026
0.95438588
0.95464581
0.95465064
0.95435512
0.95436531
0.95436722
0.94758493
0.93129492
0.92682302
0.93025070
0.94437766
0.95417404
0.95411974
0.95402259
0.95399451
********************pre-trained*****************
INFO - ==> Top1: 62.836    Top5: 93.720    Loss: 1.118
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.892575   Top1 71.582031   Top5 97.343750   BatchTime 0.106148
INFO - Validation [0][   40/   40]   Loss 0.878241   Top1 71.870000   Top5 97.430000   BatchTime 0.080092
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0509)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0839)
features.3.conv.0 tensor(0.0417)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0692)
features.4.conv.0 tensor(0.0596)
features.4.conv.3 tensor(0.1019)
features.4.conv.6 tensor(0.0929)
features.5.conv.0 tensor(0.0549)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1081)
features.6.conv.0 tensor(0.0591)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0798)
features.7.conv.0 tensor(0.0641)
features.7.conv.3 tensor(0.1004)
features.7.conv.6 tensor(0.1171)
features.8.conv.0 tensor(0.1016)
features.8.conv.3 tensor(0.1016)
features.8.conv.6 tensor(0.1260)
features.9.conv.0 tensor(0.0931)
features.9.conv.3 tensor(0.1198)
features.9.conv.6 tensor(0.1275)
features.10.conv.0 tensor(0.0573)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.1055)
features.11.conv.0 tensor(0.3003)
features.11.conv.3 tensor(0.0899)
features.11.conv.6 tensor(0.1741)
features.12.conv.0 tensor(0.1236)
features.12.conv.3 tensor(0.0855)
features.12.conv.6 tensor(0.1695)
features.13.conv.0 tensor(0.1141)
features.13.conv.3 tensor(0.1227)
features.13.conv.6 tensor(0.1127)
features.14.conv.0 tensor(0.0667)
features.14.conv.3 tensor(0.0804)
features.14.conv.6 tensor(0.3117)
features.15.conv.0 tensor(0.0401)
features.15.conv.3 tensor(0.0677)
features.15.conv.6 tensor(0.1905)
features.16.conv.0 tensor(0.0625)
features.16.conv.3 tensor(0.0795)
features.16.conv.6 tensor(0.1116)
conv.0 tensor(0.0743)
tensor(257998.) 2188896.0
INFO - ==> Top1: 71.870    Top5: 97.430    Loss: 0.878
INFO - ==> Sparsity : 0.118
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 71.870   Top5: 97.430]
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.95408779
0.95451802
0.95461237
0.95475745
0.95467222
0.95472068
0.95475441
0.95472872
0.95466626
0.95462316
0.95438451
0.95473951
0.95458066
0.95440900
0.95439708
0.95430577
0.95442706
0.95438612
0.95435804
INFO - Training [1][   20/  196]   Loss 0.937208   Top1 68.652344   Top5 95.468750   BatchTime 0.406841   LR 0.004853
0.95452601
0.95469499
0.95478618
0.95448810
0.95461518
0.95473367
0.95465988
0.95429355
0.95344275
0.95324844
0.95405567
0.95424944
0.95428443
0.95437878
0.94812912
0.95433742
0.95423776
0.95431185
0.95414621
INFO - Training [1][   40/  196]   Loss 0.919020   Top1 69.150391   Top5 95.761719   BatchTime 0.358569   LR 0.004825
0.95310450
0.95289660
0.95476264
0.95489258
0.95510632
0.95522934
0.95505571
0.95503390
0.95492184
0.95475453
0.95451838
0.95466459
0.95478976
0.95479447
0.95480281
0.95475298
0.95471179
0.95482457
0.95499849
0.95504987
0.95494294
0.95482212
INFO - Training [1][   60/  196]   Loss 0.907187   Top1 69.225260   Top5 95.781250   BatchTime 0.364320   LR 0.004794
0.95487237
0.95480394
0.95500273
0.95484543
0.95470846
0.95477766
0.95481747
0.95504493
0.95555550
0.95553946
0.95584244
0.95541292
0.95507687
0.95492601
0.95499229
0.95520377
0.95520198
0.95535046
0.95518076
0.95544165
INFO - Training [1][   80/  196]   Loss 0.898102   Top1 69.677734   Top5 95.917969   BatchTime 0.369999   LR 0.004761
0.95551157
0.95531261
0.95531321
0.95577890
0.95567548
0.95565224
0.95557368
0.95521402
0.95500958
0.95505905
0.95482022
0.95464796
0.95442516
0.95471710
0.95461184
0.95446420
0.95453137
0.95447159
0.95416117
0.95332146
0.95472807
0.95463651
0.95467043
INFO - Training [1][  100/  196]   Loss 0.880755   Top1 70.195312   Top5 96.050781   BatchTime 0.367995   LR 0.004725
0.95443398
0.95452374
0.95465809
0.95479786
0.95482099
0.95488626
0.95492429
0.95504647
0.95530176
0.95543838
0.95539039
0.95509195
0.95547050
0.95538574
0.95515978
0.95508015
INFO - Training [1][  120/  196]   Loss 0.875264   Top1 70.358073   Top5 96.155599   BatchTime 0.367689   LR 0.004687
0.95371830
0.94232011
0.93011296
0.93947905
0.94093227
0.93569803
0.93204004
0.93963927
0.94825518
0.95257998
0.95436543
0.95449001
0.95489568
0.95497310
0.95518786
0.95527941
0.95521152
0.95505971
0.95489502
0.95453262
0.95355111
0.95191246
INFO - Training [1][  140/  196]   Loss 0.866921   Top1 70.597098   Top5 96.255580   BatchTime 0.367371   LR 0.004647
0.95232928
0.95312476
0.95500427
0.95519078
0.95522344
0.95517260
0.95507962
0.95518225
0.95504808
0.95501417
0.95489478
0.95490760
0.95498413
0.95493072
0.95511091
0.95537794
INFO - Training [1][  160/  196]   Loss 0.861160   Top1 70.771484   Top5 96.274414   BatchTime 0.369538   LR 0.004605
0.95550561
0.95547360
0.95539540
0.95553344
0.95527774
0.95512503
0.95507008
0.95494807
0.95514154
0.95521635
0.95535254
0.95543605
0.95500994
0.95501888
0.95503122
0.95533055
0.95532876
0.95542550
0.95556515
0.95567638
0.95535165
INFO - Training [1][  180/  196]   Loss 0.850831   Top1 71.117622   Top5 96.341146   BatchTime 0.369255   LR 0.004560
0.95535254
0.95532036
0.95529467
0.95553362
0.95548427
0.95541954
0.95533013
0.95542383
0.95566493
0.95583445
0.95585454
0.95591098
0.95582819
0.95576376
0.95587373
0.95550942
INFO - ==> Top1: 71.256    Top5: 96.362    Loss: 0.847
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95558649
0.95527315
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [1][   20/   40]   Loss 0.636989   Top1 79.238281   Top5 97.949219   BatchTime 0.113604
INFO - Validation [1][   40/   40]   Loss 0.624728   Top1 79.540000   Top5 98.200000   BatchTime 0.084830
INFO - ==> Top1: 79.540    Top5: 98.200    Loss: 0.625
INFO - ==> Sparsity : 0.141
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 79.540   Top5: 98.200]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 71.870   Top5: 97.430]
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0404)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0807)
features.2.conv.0 tensor(0.0437)
features.2.conv.3 tensor(0.0694)
features.2.conv.6 tensor(0.0917)
features.3.conv.0 tensor(0.0417)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0697)
features.4.conv.0 tensor(0.0566)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.1021)
features.5.conv.0 tensor(0.0439)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.1040)
features.6.conv.0 tensor(0.0480)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0701)
features.7.conv.0 tensor(0.0793)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.1152)
features.8.conv.0 tensor(0.0788)
features.8.conv.3 tensor(0.1143)
features.8.conv.6 tensor(0.1275)
features.9.conv.0 tensor(0.0946)
features.9.conv.3 tensor(0.1282)
features.9.conv.6 tensor(0.1363)
features.10.conv.0 tensor(0.0444)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.1010)
features.11.conv.0 tensor(0.2659)
features.11.conv.3 tensor(0.1046)
features.11.conv.6 tensor(0.1764)
features.12.conv.0 tensor(0.1346)
features.12.conv.3 tensor(0.0968)
features.12.conv.6 tensor(0.2064)
features.13.conv.0 tensor(0.1031)
features.13.conv.3 tensor(0.1372)
features.13.conv.6 tensor(0.1111)
features.14.conv.0 tensor(0.0818)
features.14.conv.3 tensor(0.0806)
features.14.conv.6 tensor(0.3989)
features.15.conv.0 tensor(0.0627)
features.15.conv.3 tensor(0.0692)
features.15.conv.6 tensor(0.2533)
features.16.conv.0 tensor(0.0488)
features.16.conv.3 tensor(0.0904)
features.16.conv.6 tensor(0.1465)
conv.0 tensor(0.1068)
tensor(308689.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.95533890
0.95535547
0.95536590
0.95537883
0.95553547
0.95536089
0.95542973
0.95541674
0.95566797
0.95562428
0.95561355
0.95553166
0.95564282
0.95541888
0.95541811
0.95551747
0.95551795
0.95555633
0.95578116
0.95568961
0.95565325
INFO - Training [2][   20/  196]   Loss 0.823446   Top1 71.660156   Top5 95.937500   BatchTime 0.385613   LR 0.004477
0.95565987
0.95560592
0.95560187
0.95560431
0.95539498
0.95562375
0.95587307
0.95585400
0.95548820
0.95564651
0.95561635
0.95540959
0.95552266
0.95534480
0.95513290
0.95523900
INFO - Training [2][   40/  196]   Loss 0.807194   Top1 72.578125   Top5 96.162109   BatchTime 0.383011   LR 0.004426
0.95510250
0.95500129
0.95490557
0.95482683
0.95466930
0.95488280
0.95466691
0.95434493
0.95429277
0.95414746
0.95411861
0.95451599
0.95490313
0.95559490
0.95565045
0.95575231
0.95591098
0.95602643
0.95610631
0.95570958
0.95559335
0.95567590
INFO - Training [2][   60/  196]   Loss 0.796474   Top1 72.845052   Top5 96.315104   BatchTime 0.376248   LR 0.004374
0.95580697
0.95587754
0.95592266
0.95589817
0.95572507
0.95571947
0.95594573
0.95582706
0.95573562
0.95568132
0.95566940
0.95549101
0.95541608
0.95564133
0.95508629
0.95394760
0.95325333
0.95352036
0.94796360
0.93554926
0.92990077
INFO - Training [2][   80/  196]   Loss 0.781955   Top1 73.305664   Top5 96.513672   BatchTime 0.376713   LR 0.004320
0.93329716
0.93672591
0.93772298
0.93750727
0.93641651
0.93509287
0.93588692
0.93791336
0.94085526
0.94402593
0.94690841
0.94885457
0.95036441
0.95162034
0.95410019
0.95569474
0.95560634
INFO - Training [2][  100/  196]   Loss 0.772080   Top1 73.718750   Top5 96.585938   BatchTime 0.371894   LR 0.004264
0.95510459
0.95494741
0.95435065
0.95397252
0.95300138
0.95200413
0.95099729
0.94971240
0.94779879
0.94495177
0.94073653
0.93600792
0.93395931
0.93330532
0.93178350
0.93280995
0.93618119
0.93704003
0.93687582
0.93607098
0.93447655
INFO - Training [2][  120/  196]   Loss 0.763170   Top1 74.075521   Top5 96.722005   BatchTime 0.373186   LR 0.004206
0.93256474
0.93037015
0.92948252
0.92845291
0.92810845
0.92785794
0.92670918
0.92811018
0.92811739
0.92826867
0.92829764
0.92830068
0.92849314
0.92835480
0.92842120
0.92837268
0.92849886
0.92852265
0.92834353
0.92819250
0.92826575
INFO - Training [2][  140/  196]   Loss 0.761162   Top1 74.171317   Top5 96.799665   BatchTime 0.376594   LR 0.004146
0.92833042
0.92847544
0.92832613
0.92819333
0.92807060
0.92802590
0.92812717
0.92804408
0.92812502
0.92810196
0.92821360
0.92823231
0.92829818
0.92837101
0.92822903
0.92842191
0.92883939
0.92903286
0.92932343
0.92955840
0.92960888
0.92935753
INFO - Training [2][  160/  196]   Loss 0.763969   Top1 74.050293   Top5 96.760254   BatchTime 0.374210   LR 0.004085
0.92938846
0.92940396
0.92952091
0.92961079
0.92992169
0.92985678
0.92945045
0.92983246
0.93179154
0.93179095
0.93210638
0.93206006
0.93194866
0.93190855
0.93189293
0.93179637
0.93167675
INFO - Training [2][  180/  196]   Loss 0.762042   Top1 74.082031   Top5 96.742622   BatchTime 0.371986   LR 0.004022
0.93169266
0.93198723
0.93204737
0.93212700
0.93182766
0.93190968
0.93202525
0.93184733
0.93183821
0.93174887
0.93162841
0.93162906
0.93189633
0.93135023
0.93126041
0.93110329
INFO - ==> Top1: 74.282    Top5: 96.776    Loss: 0.758
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.93103725
0.93107021
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.554052   Top1 81.640625   Top5 99.042969   BatchTime 0.110671
features.0.conv.0 tensor(0.5972)
features.0.conv.3 tensor(0.2012)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0790)
features.2.conv.0 tensor(0.0469)
INFO - Validation [2][   40/   40]   Loss 0.542837   Top1 81.820000   Top5 99.150000   BatchTime 0.082136
INFO - ==> Top1: 81.820    Top5: 99.150    Loss: 0.543
INFO - ==> Sparsity : 0.211
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 81.820   Top5: 99.150]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 79.540   Top5: 98.200]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 71.870   Top5: 97.430]
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0885)
features.3.conv.0 tensor(0.0379)
features.3.conv.3 tensor(0.0540)
features.3.conv.6 tensor(0.0660)
features.4.conv.0 tensor(0.0505)
features.4.conv.3 tensor(0.1082)
features.4.conv.6 tensor(0.1074)
features.5.conv.0 tensor(0.0439)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.1051)
features.6.conv.0 tensor(0.0417)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0719)
features.7.conv.0 tensor(0.0771)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.1202)
features.8.conv.0 tensor(0.0619)
features.8.conv.3 tensor(0.1291)
features.8.conv.6 tensor(0.1259)
features.9.conv.0 tensor(0.0878)
features.9.conv.3 tensor(0.1386)
features.9.conv.6 tensor(0.1440)
features.10.conv.0 tensor(0.0462)
features.10.conv.3 tensor(0.0865)
features.10.conv.6 tensor(0.0926)
features.11.conv.0 tensor(0.2858)
features.11.conv.3 tensor(0.1051)
features.11.conv.6 tensor(0.2123)
features.12.conv.0 tensor(0.1478)
features.12.conv.3 tensor(0.0959)
features.12.conv.6 tensor(0.1995)
features.13.conv.0 tensor(0.0802)
features.13.conv.3 tensor(0.1323)
features.13.conv.6 tensor(0.1071)
features.14.conv.0 tensor(0.1078)
features.14.conv.3 tensor(0.0824)
features.14.conv.6 tensor(0.4999)
features.15.conv.0 tensor(0.7937)
features.15.conv.3 tensor(0.0716)
features.15.conv.6 tensor(0.2144)
features.16.conv.0 tensor(0.0636)
features.16.conv.3 tensor(0.0958)
features.16.conv.6 tensor(0.1913)
conv.0 tensor(0.1334)
tensor(462657.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.93046337
0.93015873
0.93018234
0.93004465
0.93025202
0.93048280
0.93001771
0.93001240
0.92970550
0.92946261
0.92938936
0.92924780
0.92941666
0.92937273
0.92941600
0.92936563
0.92928302
0.92937714
0.92898005
INFO - Training [3][   20/  196]   Loss 0.714229   Top1 75.839844   Top5 96.640625   BatchTime 0.454007   LR 0.003907
0.92849171
0.93153960
0.93135625
0.93046099
0.93344969
0.93429744
0.93483853
0.93492275
0.93493688
0.93519717
0.93534058
0.93586260
0.93629318
0.93654740
0.93669850
0.93703324
0.94044304
0.94000351
0.94115853
0.94211233
0.94200146
INFO - Training [3][   40/  196]   Loss 0.717261   Top1 75.458984   Top5 96.777344   BatchTime 0.420672   LR 0.003840
0.94178253
0.94122517
0.94052011
0.93949622
0.93842673
0.93756062
0.93696290
0.93586898
0.93520987
0.93447042
0.93364257
0.93336809
0.93281376
0.93240410
0.93206543
0.93170446
INFO - Training [3][   60/  196]   Loss 0.715055   Top1 75.579427   Top5 97.050781   BatchTime 0.399826   LR 0.003771
0.93142968
0.93024826
0.92886978
0.92875850
0.92757028
0.92145163
0.90964174
0.91628122
0.91460633
0.91731220
0.91904980
0.91940743
0.92221630
0.92565525
0.92870659
0.92879170
0.92838734
0.92815572
0.92811912
0.92612648
0.92752546
INFO - Training [3][   80/  196]   Loss 0.704145   Top1 75.908203   Top5 97.138672   BatchTime 0.396730   LR 0.003701
0.92782950
0.92804813
0.92795551
0.92774445
0.92750043
0.92731333
0.92681938
0.92606384
0.92522609
0.92436272
0.92332566
0.92154694
0.91889709
0.91530114
0.91232735
0.90935016
0.90471566
0.90290177
0.90150523
0.90091407
0.90075582
0.90083891
INFO - Training [3][  100/  196]   Loss 0.695994   Top1 76.316406   Top5 97.191406   BatchTime 0.390413   LR 0.003630
0.90102857
0.90163022
0.90329075
0.90807962
0.91750169
0.92498112
0.92834383
0.92897415
0.92933166
0.92936605
0.92930561
0.92928785
0.92921233
0.92911452
0.92924368
0.92890394
INFO - Training [3][  120/  196]   Loss 0.691094   Top1 76.669922   Top5 97.265625   BatchTime 0.388590   LR 0.003558
0.92900175
0.92910272
0.92783266
0.92705798
0.92719638
0.92635673
0.92320442
0.92492014
0.92024469
0.91361988
0.90720266
0.90333021
0.90173304
0.90076756
0.90201741
0.90374047
0.90722328
0.91179997
0.91613495
0.92015511
0.92293346
0.92494047
INFO - Training [3][  140/  196]   Loss 0.688122   Top1 76.746652   Top5 97.321429   BatchTime 0.385023   LR 0.003484
0.92738491
0.92770845
0.92756951
0.92736000
0.92684740
0.92675692
0.92684078
0.92684948
0.92676526
0.92688704
0.92658782
0.92643082
0.92649561
0.92645699
0.92627889
0.92614257
0.92594171
0.92586106
0.92567772
0.92574024
0.92574489
0.92556977
INFO - Training [3][  160/  196]   Loss 0.689697   Top1 76.667480   Top5 97.329102   BatchTime 0.382686   LR 0.003410
0.92524809
0.92495662
0.92453909
0.92410159
0.92358381
0.92317367
0.92290848
0.92254841
0.92220330
0.92162061
0.92079341
0.91968793
0.91893184
0.91763377
0.91614205
0.91493797
INFO - Training [3][  180/  196]   Loss 0.687954   Top1 76.660156   Top5 97.300347   BatchTime 0.380191   LR 0.003335
0.91332585
0.91164744
0.91084737
0.90867573
0.90704739
0.90625483
0.90541112
0.90454429
0.90354574
0.90189886
0.90053427
0.89956582
0.89817202
0.89656734
0.89579988
0.89537811
0.89499873
INFO - ==> Top1: 76.694    Top5: 97.302    Loss: 0.686
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.89443713
0.89360857
0.89203101
0.89100915
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [3][   20/   40]   Loss 0.609571   Top1 80.234375   Top5 98.398438   BatchTime 0.143490
INFO - Validation [3][   40/   40]   Loss 0.615419   Top1 79.570000   Top5 98.520000   BatchTime 0.103047
INFO - ==> Top1: 79.570    Top5: 98.520    Loss: 0.615
INFO - ==> Sparsity : 0.283
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 81.820   Top5: 99.150]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 79.570   Top5: 98.520]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 79.540   Top5: 98.200]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5868)
features.0.conv.3 tensor(0.2617)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0751)
features.2.conv.0 tensor(0.0434)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0830)
features.3.conv.0 tensor(0.0275)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0616)
features.4.conv.0 tensor(0.0423)
features.4.conv.3 tensor(0.1047)
features.4.conv.6 tensor(0.1058)
features.5.conv.0 tensor(0.0340)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.1011)
features.6.conv.0 tensor(0.0332)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0703)
features.7.conv.0 tensor(0.0654)
features.7.conv.3 tensor(0.1039)
features.7.conv.6 tensor(0.1137)
features.8.conv.0 tensor(0.0565)
features.8.conv.3 tensor(0.1236)
features.8.conv.6 tensor(0.1254)
features.9.conv.0 tensor(0.0997)
features.9.conv.3 tensor(0.1473)
features.9.conv.6 tensor(0.1407)
features.10.conv.0 tensor(0.0581)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.0911)
features.11.conv.0 tensor(0.2925)
features.11.conv.3 tensor(0.1078)
features.11.conv.6 tensor(0.2025)
features.12.conv.0 tensor(0.1539)
features.12.conv.3 tensor(0.0986)
features.12.conv.6 tensor(0.1758)
features.13.conv.0 tensor(0.0798)
features.13.conv.3 tensor(0.1418)
features.13.conv.6 tensor(0.1103)
features.14.conv.0 tensor(0.9415)
features.14.conv.3 tensor(0.0791)
features.14.conv.6 tensor(0.5689)
features.15.conv.0 tensor(0.8874)
features.15.conv.3 tensor(0.0816)
features.15.conv.6 tensor(0.2314)
features.16.conv.0 tensor(0.0775)
features.16.conv.3 tensor(0.0935)
features.16.conv.6 tensor(0.1849)
conv.0 tensor(0.1385)
tensor(619397.) 2188896.0
0.88915586
0.88639784
0.87644440
0.87651527
0.87702173
0.87738216
0.87977326
0.88305819
0.88669020
0.88951850
0.89175570
0.89306158
0.89346182
0.89297247
0.89205986
0.88900387
0.88821477
0.88872337
INFO - Training [4][   20/  196]   Loss 0.647899   Top1 77.832031   Top5 97.109375   BatchTime 0.454951   LR 0.003200
0.88857818
0.88791710
0.88813835
0.88816351
0.88598782
0.88570523
0.88492930
0.88061631
0.87705064
0.87497759
0.87475753
0.87411553
0.87425572
0.87484747
0.87382030
0.87372780
0.87393367
0.87351567
0.87358671
0.87387133
INFO - Training [4][   40/  196]   Loss 0.672451   Top1 76.923828   Top5 97.128906   BatchTime 0.428481   LR 0.003122
0.87393326
0.87424397
0.87411207
0.87426633
0.87423772
0.87399614
0.87387520
0.87368637
0.87380916
0.87394458
0.87413609
0.87679011
0.88026249
0.88559848
0.89177787
0.89754826
0.90078264
0.90099132
0.90113312
0.90118647
INFO - Training [4][   60/  196]   Loss 0.677710   Top1 76.894531   Top5 97.161458   BatchTime 0.422732   LR 0.003044
0.90106618
0.90104628
0.90133184
0.90104443
0.90087664
0.90094364
0.90105850
0.90117544
0.90121132
0.90106356
0.90119588
0.90078461
0.90070951
0.90083718
0.90086603
0.90138692
0.90088904
0.90088743
0.90066010
0.90065354
INFO - Training [4][   80/  196]   Loss 0.671365   Top1 77.163086   Top5 97.285156   BatchTime 0.417182   LR 0.002965
0.90049273
0.90046161
0.89986157
0.90095818
0.90105665
0.90121418
0.90103734
0.90084773
0.90110552
0.90125430
0.90124601
0.90092254
0.90120512
0.90118361
0.90109569
0.90120697
0.90131205
0.90128183
0.90167236
0.90238714
0.90279740
INFO - Training [4][  100/  196]   Loss 0.667589   Top1 77.257812   Top5 97.335938   BatchTime 0.407112   LR 0.002886
0.90263516
0.90266240
0.90267807
0.90269381
0.90307963
0.90278369
0.90292460
0.90296751
0.90287936
0.90249836
0.90176469
0.90196645
0.90301907
0.90286052
0.90272796
0.90272439
0.90258729
INFO - Training [4][  120/  196]   Loss 0.664694   Top1 77.425130   Top5 97.434896   BatchTime 0.399618   LR 0.002806
0.90235329
0.90222830
0.90216142
0.90209246
0.90192688
0.90170819
0.90174806
0.90178233
0.90155554
0.90152478
0.90127045
0.90107417
0.90076500
0.90097582
0.90079653
0.90032464
0.90018511
0.89965588
0.89926809
0.89907354
0.89880663
0.89831012
INFO - Training [4][  140/  196]   Loss 0.660377   Top1 77.606027   Top5 97.488839   BatchTime 0.393571   LR 0.002726
0.89772087
0.89718562
0.89679050
0.89581728
0.89489853
0.89386743
0.89215243
0.89036524
0.88743323
0.88414901
0.88248116
0.88062125
0.87884396
0.87749809
0.87758976
0.87679315
0.87675762
INFO - Training [4][  160/  196]   Loss 0.659640   Top1 77.658691   Top5 97.475586   BatchTime 0.389482   LR 0.002646
0.87743676
0.87740475
0.87735760
0.87723351
0.87742972
0.87733412
0.87707675
0.87669587
0.87640947
0.87608051
0.87605745
0.87627858
0.87637293
0.87649149
0.87656510
0.87677914
0.87738377
0.87734181
0.87767977
0.87764883
0.87803954
INFO - Training [4][  180/  196]   Loss 0.653332   Top1 77.853733   Top5 97.471788   BatchTime 0.389904   LR 0.002566
0.87838954
0.87852228
0.87793386
0.87792027
0.87763113
0.87725985
0.87676656
0.87660247
0.87598938
0.87484992
0.87458456
0.87373596
0.87414318
0.87596494
0.87595570
0.87564647
0.87561756
INFO - ==> Top1: 78.010    Top5: 97.458    Loss: 0.648
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87570179
0.87570751
0.87586486
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.466498   Top1 84.414062   Top5 99.160156   BatchTime 0.146012
INFO - Validation [4][   40/   40]   Loss 0.451677   Top1 84.620000   Top5 99.300000   BatchTime 0.109558
INFO - ==> Top1: 84.620    Top5: 99.300    Loss: 0.452
INFO - ==> Sparsity : 0.314
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 84.620   Top5: 99.300]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 81.820   Top5: 99.150]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 79.570   Top5: 98.520]
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.2051)
features.1.conv.0 tensor(0.0410)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0729)
features.2.conv.0 tensor(0.0388)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0911)
features.3.conv.0 tensor(0.0321)
features.3.conv.3 tensor(0.0633)
features.3.conv.6 tensor(0.0603)
features.4.conv.0 tensor(0.0505)
features.4.conv.3 tensor(0.1059)
features.4.conv.6 tensor(0.1016)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.1060)
features.6.conv.0 tensor(0.0280)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0711)
features.7.conv.0 tensor(0.0726)
features.7.conv.3 tensor(0.1013)
features.7.conv.6 tensor(0.1095)
features.8.conv.0 tensor(0.0564)
features.8.conv.3 tensor(0.1270)
features.8.conv.6 tensor(0.1246)
features.9.conv.0 tensor(0.0977)
features.9.conv.3 tensor(0.1528)
features.9.conv.6 tensor(0.1437)
features.10.conv.0 tensor(0.0513)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.0884)
features.11.conv.0 tensor(0.3202)
features.11.conv.3 tensor(0.1057)
features.11.conv.6 tensor(0.2071)
features.12.conv.0 tensor(0.1758)
features.12.conv.3 tensor(0.1024)
features.12.conv.6 tensor(0.1821)
features.13.conv.0 tensor(0.0841)
features.13.conv.3 tensor(0.1447)
features.13.conv.6 tensor(0.1024)
features.14.conv.0 tensor(0.8666)
features.14.conv.3 tensor(0.0884)
features.14.conv.6 tensor(0.4083)
features.15.conv.0 tensor(0.8937)
features.15.conv.3 tensor(0.0844)
features.15.conv.6 tensor(0.8708)
features.16.conv.0 tensor(0.0591)
features.16.conv.3 tensor(0.0935)
features.16.conv.6 tensor(0.1879)
conv.0 tensor(0.1477)
tensor(687098.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.87623429
0.87648839
0.87665826
0.87709272
0.87759358
0.87795866
0.87835234
0.87886238
0.87946981
0.88013387
0.87994587
0.87959743
0.87905627
0.87893951
0.87867039
0.87848788
0.87811583
0.87767053
0.87722880
0.87692428
INFO - Training [5][   20/  196]   Loss 0.639509   Top1 77.890625   Top5 97.148438   BatchTime 0.419903   LR 0.002424
0.87711471
0.87714231
0.87712872
0.87684977
0.87699276
0.87691265
0.87697333
0.87688321
0.87653148
0.87637013
0.87603825
0.87585783
0.87549466
0.87537044
0.87521046
0.87516218
0.87512755
INFO - Training [5][   40/  196]   Loss 0.642313   Top1 77.968750   Top5 97.304688   BatchTime 0.395221   LR 0.002343
0.87473905
0.87440658
0.87437612
0.87429726
0.87417704
0.87414110
0.87426758
0.87428606
0.87393892
0.87386960
0.87401986
0.87416118
0.87436718
0.87462240
0.87534434
0.87617528
0.87685657
0.87764788
0.87843996
0.87895525
INFO - Training [5][   60/  196]   Loss 0.628315   Top1 78.404948   Top5 97.428385   BatchTime 0.390744   LR 0.002263
0.87979937
0.88061672
0.88133216
0.88200301
0.88206762
0.88231879
0.88267195
0.88221365
0.88158494
0.88163334
0.88162082
0.88080645
0.88003308
0.87866986
0.87780863
0.87733513
0.87697339
0.87717217
0.87688601
0.87728614
0.87706906
0.87706286
0.87719965
INFO - Training [5][   80/  196]   Loss 0.625769   Top1 78.437500   Top5 97.539062   BatchTime 0.383218   LR 0.002183
0.87740350
0.87780023
0.87763894
0.87756687
0.87747967
0.87728244
0.87683612
0.87660789
0.87636352
0.87612849
0.87575978
0.87539661
0.87531257
0.87534791
0.87535089
0.87548041
INFO - Training [5][  100/  196]   Loss 0.615493   Top1 78.843750   Top5 97.628906   BatchTime 0.377436   LR 0.002104
0.87553257
0.87568271
0.87600875
0.87600356
0.87595683
0.87581789
0.87569404
0.87577838
0.87601316
0.87606633
0.87582994
0.87603617
0.87627923
0.87645262
0.87645161
0.87618405
0.87597573
0.87583596
0.87609273
0.87608665
0.87620890
INFO - Training [5][  120/  196]   Loss 0.603161   Top1 79.277344   Top5 97.692057   BatchTime 0.379357   LR 0.002024
0.87560970
0.87381935
0.87097251
0.86261451
0.85429120
0.86392111
0.87322450
0.87557083
0.87484312
0.87474871
0.87507963
0.87482339
0.87424839
0.87353462
0.87317818
0.87407225
0.87379146
0.87453306
0.87460947
0.87476337
INFO - Training [5][  140/  196]   Loss 0.594750   Top1 79.531250   Top5 97.770647   BatchTime 0.383681   LR 0.001946
0.87475652
0.87451321
0.87439758
0.87437606
0.87474710
0.87462240
0.87462908
0.87457836
0.87442839
0.87439793
0.87428147
0.87421709
0.87417942
0.87425637
0.87417829
0.87381905
0.87382597
0.87358338
0.87304682
0.87280482
0.87272269
0.87225139
0.87209010
INFO - Training [5][  160/  196]   Loss 0.595384   Top1 79.509277   Top5 97.778320   BatchTime 0.378753   LR 0.001868
0.87148654
0.87048215
0.86984277
0.86887765
0.86756068
0.86656779
0.86562937
0.86461008
0.86400366
0.86214733
0.86117321
0.86060828
0.86032069
0.85918349
0.85788536
0.85606670
0.85445797
0.85346687
0.85342592
0.85369623
INFO - Training [5][  180/  196]   Loss 0.594966   Top1 79.513889   Top5 97.717014   BatchTime 0.370600   LR 0.001790
0.85410148
0.85357863
0.85325754
0.85223812
0.85180706
0.85221696
0.85191321
0.85196060
0.85121924
0.85084581
0.85001981
0.84938276
INFO - ==> Top1: 79.764    Top5: 97.726    Loss: 0.590
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84867728
0.84752709
0.84675628
0.84639931
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.461544   Top1 84.511719   Top5 99.257812   BatchTime 0.110047
INFO - Validation [5][   40/   40]   Loss 0.451852   Top1 84.640000   Top5 99.350000   BatchTime 0.081279
INFO - ==> Top1: 84.640    Top5: 99.350    Loss: 0.452
INFO - ==> Sparsity : 0.352
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 84.640   Top5: 99.350]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 84.620   Top5: 99.300]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 81.820   Top5: 99.150]
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.2012)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0747)
features.2.conv.0 tensor(0.0437)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.0856)
features.3.conv.0 tensor(0.0278)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0521)
features.4.conv.0 tensor(0.0420)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.1019)
features.5.conv.0 tensor(0.0270)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.1037)
features.6.conv.0 tensor(0.0304)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0681)
features.7.conv.0 tensor(0.0811)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.1093)
features.8.conv.0 tensor(0.0570)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.1227)
features.9.conv.0 tensor(0.0912)
features.9.conv.3 tensor(0.1496)
features.9.conv.6 tensor(0.1339)
features.10.conv.0 tensor(0.0446)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.0869)
features.11.conv.0 tensor(0.3463)
features.11.conv.3 tensor(0.1036)
features.11.conv.6 tensor(0.2019)
features.12.conv.0 tensor(0.1993)
features.12.conv.3 tensor(0.1038)
features.12.conv.6 tensor(0.1987)
features.13.conv.0 tensor(0.0831)
features.13.conv.3 tensor(0.1497)
features.13.conv.6 tensor(0.1008)
features.14.conv.0 tensor(0.8732)
features.14.conv.3 tensor(0.0869)
features.14.conv.6 tensor(0.8785)
features.15.conv.0 tensor(0.8958)
features.15.conv.3 tensor(0.0837)
features.15.conv.6 tensor(0.9287)
features.16.conv.0 tensor(0.0729)
features.16.conv.3 tensor(0.0944)
features.16.conv.6 tensor(0.1772)
conv.0 tensor(0.1474)
tensor(770847.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.84643608
0.84617966
0.84608418
0.84608686
0.84574366
0.84591830
0.84583318
0.84557933
0.84559298
0.84594876
0.84571505
0.84597725
0.84657884
0.84735757
0.84815365
0.84961277
0.85075623
0.85185498
0.85289270
0.85427612
INFO - Training [6][   20/  196]   Loss 0.570005   Top1 80.429688   Top5 97.304688   BatchTime 0.416858   LR 0.001655
0.85557657
0.85610527
0.85664040
0.85704684
0.85695606
0.85795456
0.85678798
0.85578984
0.85396773
0.85258251
0.85119170
0.85005707
0.84951961
0.84920686
0.84841084
0.84791678
INFO - Training [6][   40/  196]   Loss 0.567880   Top1 80.585938   Top5 97.509766   BatchTime 0.396832   LR 0.001580
0.84779495
0.84782845
0.84768635
0.84761941
0.84779906
0.84725338
0.84661108
0.84660631
0.84699416
0.84716380
0.84705979
0.84730756
0.84781641
0.84995884
0.84928948
0.84852409
0.84811109
0.84767962
0.84734219
0.84711164
0.84740919
INFO - Training [6][   60/  196]   Loss 0.565973   Top1 80.651042   Top5 97.675781   BatchTime 0.392039   LR 0.001506
0.84756994
0.84805334
0.84817469
0.84859037
0.84904724
0.84960562
0.84992892
0.85008752
0.85078943
0.85179949
0.85251433
0.85249835
0.85314959
0.85342574
0.85358483
0.85300976
0.85256296
0.85229504
0.85269254
0.85345584
INFO - Training [6][   80/  196]   Loss 0.558496   Top1 80.859375   Top5 97.827148   BatchTime 0.393628   LR 0.001432
0.85296971
0.85227448
0.85134369
0.85058427
0.84976441
0.84949124
0.84957498
0.84961283
0.85056663
0.85149431
0.85157299
0.85181093
0.85199749
0.85175049
0.85124433
0.85074908
0.85090101
0.85052085
0.85025644
0.85037279
0.85041863
INFO - Training [6][  100/  196]   Loss 0.551012   Top1 81.085938   Top5 97.894531   BatchTime 0.390432   LR 0.001360
0.85069716
0.85079437
0.85121816
0.85147333
0.85218012
0.85253125
0.85302830
0.85319358
0.85361606
0.85357273
0.85314167
0.85292745
0.85239738
0.85141903
0.85056973
0.84993225
0.84943998
0.84949905
0.84936202
0.84939760
0.84973067
INFO - Training [6][  120/  196]   Loss 0.542059   Top1 81.461589   Top5 98.001302   BatchTime 0.387784   LR 0.001289
0.84982437
0.85003853
0.85012549
0.84991753
0.84976184
0.85009348
0.85037649
0.85002160
0.84987760
0.84921265
0.84873199
0.84823787
0.84765166
0.84725088
0.84636116
0.84621811
INFO - Training [6][  140/  196]   Loss 0.539656   Top1 81.548549   Top5 98.044085   BatchTime 0.385397   LR 0.001220
0.84629250
0.84673375
0.84653103
0.84649521
0.84646201
0.84657478
0.84660828
0.84668499
0.84644938
0.84623283
0.84576398
0.84567720
0.84547412
0.84529001
0.84532851
0.84528846
0.84536892
0.84557557
0.84559113
0.84585458
0.84603244
0.84595901
0.84626055
INFO - Training [6][  160/  196]   Loss 0.543576   Top1 81.462402   Top5 98.041992   BatchTime 0.380842   LR 0.001151
0.84600157
0.84567118
0.84550434
0.84552217
0.84536082
0.84551364
0.84548253
0.84484428
0.84493256
0.84500921
0.84509987
0.84481579
0.84447277
0.84423321
0.84417331
0.84448683
0.84460896
0.84449494
INFO - Training [6][  180/  196]   Loss 0.542873   Top1 81.456163   Top5 98.001302   BatchTime 0.374474   LR 0.001084
0.84425575
0.84422547
0.84409004
0.84386009
0.84375268
0.84360248
0.84347308
0.84342068
0.84321022
0.84307379
0.84305638
0.84298021
0.84300572
0.84306681
0.84339267
0.84345073
0.84352458
0.84334886
INFO - ==> Top1: 81.552    Top5: 98.018    Loss: 0.541
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84319621
0.84316319
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.417515   Top1 86.484375   Top5 99.375000   BatchTime 0.108118
INFO - Validation [6][   40/   40]   Loss 0.414269   Top1 86.380000   Top5 99.500000   BatchTime 0.080910
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.2129)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0768)
features.2.conv.0 tensor(0.0451)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0888)
features.3.conv.0 tensor(0.0260)
features.3.conv.3 tensor(0.0602)
features.3.conv.6 tensor(0.0506)
features.4.conv.0 tensor(0.0363)
features.4.conv.3 tensor(0.1047)
features.4.conv.6 tensor(0.1038)
features.5.conv.0 tensor(0.0295)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.1074)
features.6.conv.0 tensor(0.0295)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0642)
features.7.conv.0 tensor(0.0664)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.1066)
features.8.conv.0 tensor(0.0529)
features.8.conv.3 tensor(0.1233)
features.8.conv.6 tensor(0.1232)
features.9.conv.0 tensor(0.0850)
features.9.conv.3 tensor(0.1473)
features.9.conv.6 tensor(0.1316)
features.10.conv.0 tensor(0.0448)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.0853)
features.11.conv.0 tensor(0.3595)
features.11.conv.3 tensor(0.1040)
features.11.conv.6 tensor(0.2109)
features.12.conv.0 tensor(0.2069)
features.12.conv.3 tensor(0.1051)
features.12.conv.6 tensor(0.1833)
features.13.conv.0 tensor(0.0869)
features.13.conv.3 tensor(0.1456)
features.13.conv.6 tensor(0.1082)
features.14.conv.0 tensor(0.8868)
features.14.conv.3 tensor(0.0854)
features.14.conv.6 tensor(0.9109)
features.15.conv.0
INFO - ==> Top1: 86.380    Top5: 99.500    Loss: 0.414
INFO - ==> Sparsity : 0.353
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 86.380   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 84.640   Top5: 99.350]
features.15.conv.0 tensor(0.8964)
features.15.conv.3 tensor(0.0830)
features.15.conv.6 tensor(0.8930)
features.16.conv.0 tensor(0.0633)
features.16.conv.3 tensor(0.0953)
features.16.conv.6 tensor(0.1921)
conv.0 tensor(0.1379)
tensor(772603.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.84330028
0.84366578
0.84380990
0.84381813
0.84373903
0.84367645
0.84374422
0.84390795
0.84425759
0.84457803
0.84527850
0.84558284
0.84587264
0.84617573
0.84637958
0.84623057
0.84603161
INFO - Training [7][   20/  196]   Loss 0.546695   Top1 80.957031   Top5 97.480469   BatchTime 0.436747   LR 0.000969
0.84586716
0.84548146
0.84555846
0.84554660
0.84528911
0.84494340
0.84470779
0.84445572
0.84395045
0.84326458
0.84325927
0.84337181
0.84323919
0.84355915
0.84312415
0.84268051
0.84241956
0.84240568
0.84217858
0.84207678
0.84220082
0.84202915
0.84182698
INFO - Training [7][   40/  196]   Loss 0.531691   Top1 81.582031   Top5 97.705078   BatchTime 0.395409   LR 0.000907
0.84175324
0.84169406
0.84160060
0.84159940
0.84177023
0.84197199
0.84224749
0.84285545
0.84385329
0.84390455
0.84381145
0.84359968
0.84380162
0.84399301
0.84376103
0.84362483
INFO - Training [7][   60/  196]   Loss 0.526382   Top1 81.796875   Top5 97.832031   BatchTime 0.384992   LR 0.000845
0.84358537
0.84348375
0.84332591
0.84324527
0.84340560
0.84340799
0.84338480
0.84325290
0.84290779
0.84260845
0.84221691
0.84237826
0.84262234
0.84259045
0.84273899
0.84268355
0.84269691
0.84359449
0.84357548
0.84357947
0.84338820
INFO - Training [7][   80/  196]   Loss 0.524256   Top1 81.879883   Top5 97.944336   BatchTime 0.386705   LR 0.000786
0.84330845
0.84315598
0.84283239
0.84243965
0.84253854
0.84289551
0.84324968
0.84347868
0.84345692
0.84344232
0.84314311
0.84297192
0.84290653
0.84302980
0.84311104
0.84375626
0.84518075
0.84517545
0.84512043
0.84511137
INFO - Training [7][  100/  196]   Loss 0.519292   Top1 82.054688   Top5 98.007812   BatchTime 0.387294   LR 0.000728
0.84489274
0.84477156
0.84483743
0.84499508
0.84546095
0.84553534
0.84577519
0.84587288
0.84594297
0.84598666
0.84583491
0.84599125
0.84619069
0.84597832
0.84559697
0.84550846
0.84548020
0.84533983
0.84520632
0.84486926
0.84489417
0.84433246
0.84394485
INFO - Training [7][  120/  196]   Loss 0.514903   Top1 82.268880   Top5 98.102214   BatchTime 0.380703   LR 0.000673
0.84424067
0.84380984
0.84342980
0.84339601
0.84328741
0.84294295
0.84271479
0.84264022
0.84259892
0.84226650
0.84235871
0.84260064
0.84363359
0.84429848
0.84426367
0.84444350
0.84450889
INFO - Training [7][  140/  196]   Loss 0.511229   Top1 82.396763   Top5 98.191964   BatchTime 0.376221   LR 0.000619
0.84455580
0.84460264
0.84473705
0.84475660
0.84466004
0.84452879
0.84466857
0.84462601
0.84453392
0.84441543
0.84436852
0.84435773
0.84439504
0.84426534
0.84435683
0.84460002
0.84463120
0.84472400
0.84462237
0.84460455
0.84453636
INFO - Training [7][  160/  196]   Loss 0.513242   Top1 82.365723   Top5 98.178711   BatchTime 0.367160   LR 0.000567
0.84438568
0.84427845
0.84434634
0.84428239
0.84426230
0.84408528
0.84403950
0.84415925
0.84400535
0.84384817
0.84344721
0.84305990
0.84279454
0.84258777
0.84245074
0.84204251
0.84212327
INFO - Training [7][  180/  196]   Loss 0.512749   Top1 82.348090   Top5 98.118490   BatchTime 0.363953   LR 0.000517
0.84225953
0.84221750
0.84197551
0.84186465
0.84177268
0.84185600
0.84187078
0.84192806
0.84212041
0.84192687
0.84164852
0.84178710
0.84270018
0.84251046
0.84229529
0.84213090
0.84144539
INFO - ==> Top1: 82.442    Top5: 98.148    Loss: 0.509
0.84093654
0.84057194
0.84043962
0.84045357
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.371097   Top1 87.480469   Top5 99.453125   BatchTime 0.112314
INFO - Validation [7][   40/   40]   Loss 0.353159   Top1 88.000000   Top5 99.590000   BatchTime 0.083060
INFO - ==> Top1: 88.000    Top5: 99.590    Loss: 0.353
INFO - ==> Sparsity : 0.352
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 86.380   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 84.640   Top5: 99.350]
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0747)
features.2.conv.0 tensor(0.0460)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0880)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0499)
features.4.conv.0 tensor(0.0353)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.1016)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.1040)
features.6.conv.0 tensor(0.0311)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0646)
features.7.conv.0 tensor(0.0771)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.1069)
features.8.conv.0 tensor(0.0552)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.1296)
features.9.conv.0 tensor(0.0872)
features.9.conv.3 tensor(0.1493)
features.9.conv.6 tensor(0.1289)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0840)
features.11.conv.0 tensor(0.3724)
features.11.conv.3 tensor(0.1047)
features.11.conv.6 tensor(0.2118)
features.12.conv.0 tensor(0.2162)
features.12.conv.3 tensor(0.1061)
features.12.conv.6 tensor(0.1938)
features.13.conv.0 tensor(0.0923)
features.13.conv.3 tensor(0.1470)
features.13.conv.6 tensor(0.1132)
features.14.conv.0 tensor(0.8905)
features.14.conv.3 tensor(0.0852)
features.14.conv.6 tensor(0.8705)
features.15.conv.0 tensor(0.8968)
features.15.conv.3 tensor(0.0845)
features.15.conv.6 tensor(0.9263)
features.16.conv.0 tensor(0.0655)
features.16.conv.3 tensor(0.0932)
features.16.conv.6 tensor(0.1809)
conv.0 tensor(0.1351)
tensor(770728.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.84034991
0.84019798
0.84014332
0.84019995
0.84021199
0.84012449
0.83990431
0.84001446
0.84013331
0.83995134
0.83990806
0.84033269
0.84088618
0.84097934
0.84083265
0.84090775
0.84063655
0.84050065
0.84048826
INFO - Training [8][   20/  196]   Loss 0.504450   Top1 82.421875   Top5 97.851562   BatchTime 0.419875   LR 0.000434
0.84012592
0.83994508
0.83972913
0.83975065
0.83983076
0.83993596
0.84003413
0.84023643
0.84023058
0.84027290
0.84012097
0.84002352
0.84000176
0.83976209
0.83944511
0.83943558
0.83938324
INFO - Training [8][   40/  196]   Loss 0.497376   Top1 82.763672   Top5 98.066406   BatchTime 0.390914   LR 0.000389
0.83930349
0.83922774
0.83911908
0.83891177
0.83894783
0.83883059
0.83864254
0.83868635
0.83874506
0.83867288
0.83847862
0.83842820
0.83828598
0.83830750
0.83811885
0.83809114
0.83814359
0.83823687
0.83819938
0.83809876
0.83807689
0.83824384
INFO - Training [8][   60/  196]   Loss 0.498193   Top1 82.897135   Top5 98.059896   BatchTime 0.379313   LR 0.000347
0.83830410
0.83815950
0.83800703
0.83746028
0.83704746
0.83667028
0.83670241
0.83663595
0.83644938
0.83645684
0.83614254
0.83588845
0.83601242
0.83601314
0.83593959
0.83603895
0.83594167
0.83588934
0.83560044
0.83583194
INFO - Training [8][   80/  196]   Loss 0.494772   Top1 83.149414   Top5 98.198242   BatchTime 0.386326   LR 0.000308
0.83586794
0.83594918
0.83602279
0.83577693
0.83568180
0.83573610
0.83589363
0.83572984
0.83564776
0.83574313
0.83574659
0.83575600
0.83580232
0.83567482
0.83554667
0.83536077
0.83518410
0.83486813
0.83429956
INFO - Training [8][  100/  196]   Loss 0.488887   Top1 83.421875   Top5 98.250000   BatchTime 0.390722   LR 0.000270
0.83405471
0.83384562
0.83365780
0.83341926
0.83325905
0.83307803
0.83288103
0.83255589
0.83260649
0.83246142
0.83258313
0.83231872
0.83201939
0.83199567
0.83188987
0.83176005
0.83170694
0.83167690
0.83158672
0.83155912
0.83160615
INFO - Training [8][  120/  196]   Loss 0.482046   Top1 83.541667   Top5 98.362630   BatchTime 0.391484   LR 0.000235
0.83156085
0.83152056
0.83141232
0.83137536
0.83132285
0.83131063
0.83172202
0.83160633
0.83150941
0.83135951
0.83099848
0.83139789
0.83154690
0.83125341
0.83104444
0.83089566
0.83081204
INFO - Training [8][  140/  196]   Loss 0.475368   Top1 83.808594   Top5 98.429129   BatchTime 0.381233   LR 0.000202
0.83076692
0.83068573
0.83061373
0.83054584
0.83045274
0.83040404
0.83185548
0.83219594
0.83217949
0.83206254
0.83196586
0.83187503
0.83178246
0.83160973
0.83153248
0.83156645
0.83153307
0.83160549
0.83169132
0.83263147
INFO - Training [8][  160/  196]   Loss 0.479642   Top1 83.645020   Top5 98.403320   BatchTime 0.373985   LR 0.000172
0.83291101
0.83284593
0.83269238
0.83264041
0.83257645
0.83256084
0.83247524
0.83242780
0.83233440
0.83227867
0.83222562
0.83220506
0.83216923
0.83213019
0.83213562
INFO - Training [8][  180/  196]   Loss 0.477268   Top1 83.687066   Top5 98.305122   BatchTime 0.372298   LR 0.000143
0.83214951
0.83220035
0.83218360
0.83208424
0.83200330
0.83198911
0.83195072
0.83197528
0.83197886
0.83202493
0.83205390
0.83209175
0.83204627
0.83204037
0.83200961
0.83200324
0.83194870
0.83187646
0.83185959
0.83195460
0.83198547
0.83196044
0.83193445
0.83181596
0.83175379
INFO - ==> Top1: 83.784    Top5: 98.304    Loss: 0.476
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83174318
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.564727   Top1 81.308594   Top5 98.710938   BatchTime 0.112655
INFO - Validation [8][   40/   40]   Loss 0.553518   Top1 81.240000   Top5 98.940000   BatchTime 0.080497
INFO - ==> Top1: 81.240    Top5: 98.940    Loss: 0.554
INFO - ==> Sparsity : 0.356
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 86.380   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 84.640   Top5: 99.350]
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.2246)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0738)
features.2.conv.0 tensor(0.0463)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0883)
features.3.conv.0 tensor(0.0243)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0484)
features.4.conv.0 tensor(0.0360)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.1022)
features.5.conv.0 tensor(0.0257)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.1064)
features.6.conv.0 tensor(0.0335)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0625)
features.7.conv.0 tensor(0.0761)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.1565)
features.8.conv.0 tensor(0.0532)
features.8.conv.3 tensor(0.1230)
features.8.conv.6 tensor(0.1219)
features.9.conv.0 tensor(0.0878)
features.9.conv.3 tensor(0.1487)
features.9.conv.6 tensor(0.1299)
features.10.conv.0 tensor(0.0397)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0843)
features.11.conv.0 tensor(0.3843)
features.11.conv.3 tensor(0.1047)
features.11.conv.6 tensor(0.2137)
features.12.conv.0 tensor(0.2639)
features.12.conv.3 tensor(0.1051)
features.12.conv.6 tensor(0.2253)
features.13.conv.0 tensor(0.0920)
features.13.conv.3 tensor(0.1464)
features.13.conv.6 tensor(0.1141)
features.14.conv.0 tensor(0.8930)
features.14.conv.3 tensor(0.0847)
features.14.conv.6 tensor(0.8924)
features.15.conv.0 tensor(0.8968)
features.15.conv.3 tensor(0.0840)
features.15.conv.6 tensor(0.9232)
features.16.conv.0 tensor(0.0680)
features.16.conv.3 tensor(0.0938)
features.16.conv.6 tensor(0.1767)
conv.0 tensor(0.1349)
tensor(779245.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.83171362
0.83161700
0.83159900
0.83152533
0.83153468
0.83152479
0.83133924
0.83100265
0.83074278
0.83057058
0.83049852
0.83039415
0.83033955
0.83031994
0.83027804
0.83024716
0.83024663
0.83027422
INFO - Training [9][   20/  196]   Loss 0.498787   Top1 82.988281   Top5 97.597656   BatchTime 0.445685   LR 0.000100
0.83022583
0.83015186
0.83017796
0.83014119
0.83010232
0.83010691
0.83008242
0.83009976
0.83009374
0.83010519
0.83010483
0.83012044
0.83005774
0.83005536
0.83004707
0.83000237
0.83002216
0.83005631
0.83014029
0.83024061
0.83000284
0.83006287
INFO - Training [9][   40/  196]   Loss 0.493173   Top1 82.978516   Top5 97.910156   BatchTime 0.401200   LR 0.000079
0.83002889
0.82998532
0.82997489
0.83002472
0.82991803
0.82988423
0.82983696
0.82972974
0.82959664
0.82936579
0.82919163
0.82924360
0.82909727
0.82892185
0.82876241
0.82871413
INFO - Training [9][   60/  196]   Loss 0.483271   Top1 83.424479   Top5 98.085938   BatchTime 0.393457   LR 0.000060
0.82868266
0.82871968
0.82873774
0.82876176
0.82877743
0.82877630
0.82875991
0.82876879
0.82862359
0.82858533
0.82856536
0.82856530
0.82853085
0.82853305
0.82851809
0.82843167
0.82840735
0.82840741
0.82834256
0.82830960
0.82827580
0.82813591
INFO - Training [9][   80/  196]   Loss 0.481871   Top1 83.466797   Top5 98.242188   BatchTime 0.383876   LR 0.000044
0.82798600
0.82762945
0.82753038
0.82757556
0.82758600
0.82751507
0.82744688
0.82736892
0.82725573
0.82724339
0.82721859
0.82721239
0.82719135
0.82718360
0.82713991
0.82714260
0.82712275
0.82713443
INFO - Training [9][  100/  196]   Loss 0.469819   Top1 83.917969   Top5 98.289062   BatchTime 0.377176   LR 0.000030
0.82712823
0.82709706
0.82707965
0.82706791
0.82707971
0.82705361
0.82706106
0.82706195
0.82705510
0.82703131
0.82704836
0.82705015
0.82704920
0.82706213
0.82705921
0.82704937
0.82703769
0.82701832
0.82700020
0.82698238
0.82699126
0.82696116
INFO - Training [9][  120/  196]   Loss 0.463522   Top1 84.156901   Top5 98.385417   BatchTime 0.375219   LR 0.000019
0.82693940
0.82696289
0.82694268
0.82693279
0.82693934
0.82693046
0.82693094
0.82692468
0.82693630
0.82694793
0.82694477
0.82692367
0.82693052
0.82692981
0.82694381
0.82692897
0.82693851
0.82694209
0.82693076
0.82692695
0.82693791
0.82696724
INFO - Training [9][  140/  196]   Loss 0.463309   Top1 84.070871   Top5 98.423549   BatchTime 0.372922   LR 0.000010
0.82696152
0.82695484
0.82694584
0.82697332
0.82696748
0.82695973
0.82694322
0.82694882
0.82695937
0.82697284
0.82700634
0.82698429
0.82701576
0.82702553
0.82701558
0.82701105
0.82704091
0.82703739
0.82700306
INFO - Training [9][  160/  196]   Loss 0.465311   Top1 83.974609   Top5 98.437500   BatchTime 0.366387   LR 0.000004
0.82701159
0.82698017
0.82700211
0.82701492
0.82701659
0.82700628
0.82700139
0.82697350
0.82699001
0.82697755
0.82697070
0.82696658
0.82695949
0.82697111
0.82697320
0.82696688
0.82698137
INFO - Training [9][  180/  196]   Loss 0.465727   Top1 83.973524   Top5 98.378906   BatchTime 0.364254   LR 0.000001
0.82699662
0.82697511
0.82699156
0.82698894
0.82700014
0.82700044
0.82698536
0.82696229
0.82697737
0.82698292
0.82701737
0.82701069
0.82703388
0.82704669
0.82702011
0.82700020
0.82698745
INFO - ==> Top1: 83.958    Top5: 98.364    Loss: 0.467
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82699233
0.82699949
0.82699537
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.362255   Top1 88.203125   Top5 99.570312   BatchTime 0.115150
INFO - Validation [9][   40/   40]   Loss 0.349438   Top1 88.270000   Top5 99.610000   BatchTime 0.084989
INFO - ==> Top1: 88.270    Top5: 99.610    Loss: 0.349
INFO - ==> Sparsity : 0.357
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 86.380   Top5: 99.500]
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.2168)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0738)
features.2.conv.0 tensor(0.0463)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0888)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0480)
features.4.conv.0 tensor(0.0373)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.1029)
features.5.conv.0 tensor(0.0244)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.1071)
features.6.conv.0 tensor(0.0329)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0631)
features.7.conv.0 tensor(0.0757)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.1333)
features.8.conv.0 tensor(0.0542)
features.8.conv.3 tensor(0.1221)
features.8.conv.6 tensor(0.1256)
features.9.conv.0 tensor(0.0882)
features.9.conv.3 tensor(0.1473)
features.9.conv.6 tensor(0.1324)
features.10.conv.0 tensor(0.0402)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0942)
features.11.conv.0 tensor(0.3888)
features.11.conv.3 tensor(0.1040)
features.11.conv.6 tensor(0.2142)
features.12.conv.0 tensor(0.2650)
features.12.conv.3 tensor(0.1040)
features.12.conv.6 tensor(0.2255)
features.13.conv.0 tensor(0.0917)
features.13.conv.3 tensor(0.1466)
features.13.conv.6 tensor(0.1156)
features.14.conv.0 tensor(0.8942)
features.14.conv.3 tensor(0.0851)
features.14.conv.6 tensor(0.8961)
features.15.conv.0 tensor(0.8971)
features.15.conv.3 tensor(0.0843)
features.15.conv.6 tensor(0.9271)
features.16.conv.0 tensor(0.0682)
features.16.conv.3 tensor(0.0947)
features.16.conv.6 tensor(0.1766)
conv.0 tensor(0.1360)
tensor(781535.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.82697541
0.83329976
0.83354944
0.83415335
0.83765316
0.83824551
0.83821082
0.84223390
0.84314847
0.84389371
0.84410822
0.84401375
0.84392869
0.84542555
0.84466350
INFO - Training [10][   20/  196]   Loss 0.539394   Top1 81.621094   Top5 97.695312   BatchTime 0.438103   LR 0.002500
0.84236526
0.83992398
0.83807909
0.83666420
0.83578622
0.83632058
0.83460885
0.83175522
0.82889324
0.82529527
0.82304358
0.82563567
0.82671857
0.82570523
0.82461613
0.82571137
0.82746542
0.82945603
0.83182079
0.83368868
0.83469540
0.83560687
INFO - Training [10][   40/  196]   Loss 0.545847   Top1 81.464844   Top5 97.861328   BatchTime 0.403160   LR 0.002499
0.83645535
0.83753538
0.83840448
0.83864886
0.83877659
0.83905369
0.83926255
0.83969986
0.84040767
0.84111220
0.84173483
0.84375024
0.84406978
0.84407133
0.84422141
0.84460950
0.84442294
0.84460777
0.84497416
0.84547448
0.84637344
0.84738463
INFO - Training [10][   60/  196]   Loss 0.553761   Top1 81.243490   Top5 97.923177   BatchTime 0.388189   LR 0.002499
0.84830993
0.84874743
0.84923118
0.84914184
0.84943831
0.84975481
0.85011798
0.85107905
0.85146540
0.85252964
0.85316813
0.85359788
0.85417622
0.85471970
0.85518670
0.85465556
0.85367811
INFO - Training [10][   80/  196]   Loss 0.554602   Top1 81.225586   Top5 97.993164   BatchTime 0.379804   LR 0.002497
0.85306209
0.85221368
0.85166574
0.85057026
0.84998626
0.84935927
0.84896415
0.84864414
0.84843224
0.84887940
0.84906238
0.84947014
0.84939754
0.84970301
0.84976906
0.84937876
0.84949249
0.84926879
0.84849793
0.84834909
0.84739405
0.84672016
INFO - Training [10][  100/  196]   Loss 0.550202   Top1 81.410156   Top5 98.046875   BatchTime 0.376379   LR 0.002496
0.84600824
0.84511131
0.84509975
0.84460223
0.84469360
0.84502953
0.84542638
0.84528297
0.84631449
0.84708500
0.84801275
0.84825820
0.84830809
0.84846145
0.84790981
0.84755552
0.84678894
0.84617084
0.84542543
0.84441924
0.84373575
0.84336239
0.84282655
INFO - Training [10][  120/  196]   Loss 0.550717   Top1 81.442057   Top5 98.037109   BatchTime 0.374462   LR 0.002494
0.84229791
0.84116298
0.84015143
0.83905208
0.83814889
0.83618402
0.83244306
0.82945341
0.82888055
0.83349979
0.83832699
0.84284145
0.84251976
0.84256929
0.84265405
0.84276438
0.84280664
0.84322155
0.84343755
INFO - Training [10][  140/  196]   Loss 0.550214   Top1 81.462054   Top5 98.077567   BatchTime 0.365052   LR 0.002492
0.84381229
0.84428334
0.84461749
0.84482294
0.84514207
0.84549105
0.84558451
0.84586942
0.84724748
0.84815079
0.84861386
0.84906703
0.84946424
0.84989524
0.85014915
0.85049576
0.85079509
0.85102731
INFO - Training [10][  160/  196]   Loss 0.596299   Top1 80.385742   Top5 97.770996   BatchTime 0.361669   LR 0.002490
0.85121912
0.85115951
0.85093969
0.85076779
0.85078275
0.85049397
0.85017896
0.85005188
0.85007185
0.85023987
0.85006452
0.84967721
0.84963429
0.84931386
0.84881026
0.84839439
0.84838128
0.84834605
0.84833932
0.84811085
0.84769297
INFO - Training [10][  180/  196]   Loss 0.606742   Top1 80.017361   Top5 97.632378   BatchTime 0.364548   LR 0.002487
0.84740037
0.84746492
0.84700185
0.84623927
0.83915108
0.82346010
0.82219011
0.84629971
0.85003579
0.85018909
0.85073316
0.85132825
0.85145235
0.85084474
0.84952825
INFO - ==> Top1: 79.752    Top5: 97.562    Loss: 0.612
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84785551
0.84384102
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.575634   Top1 81.855469   Top5 98.847656   BatchTime 0.120158
INFO - Validation [10][   40/   40]   Loss 0.566897   Top1 82.130000   Top5 99.010000   BatchTime 0.089081
INFO - ==> Top1: 82.130    Top5: 99.010    Loss: 0.567
INFO - ==> Sparsity : 0.446
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 86.380   Top5: 99.500]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.2051)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0677)
features.2.conv.0 tensor(0.0336)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0848)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0471)
features.4.conv.0 tensor(0.0417)
features.4.conv.3 tensor(0.1059)
features.4.conv.6 tensor(0.0973)
features.5.conv.0 tensor(0.0342)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0973)
features.6.conv.0 tensor(0.0316)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0605)
features.7.conv.0 tensor(0.0624)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.1121)
features.8.conv.0 tensor(0.0562)
features.8.conv.3 tensor(0.1262)
features.8.conv.6 tensor(0.1211)
features.9.conv.0 tensor(0.0955)
features.9.conv.3 tensor(0.1484)
features.9.conv.6 tensor(0.1329)
features.10.conv.0 tensor(0.0471)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0812)
features.11.conv.0 tensor(0.3795)
features.11.conv.3 tensor(0.1142)
features.11.conv.6 tensor(0.2011)
features.12.conv.0 tensor(0.1802)
features.12.conv.3 tensor(0.1101)
features.12.conv.6 tensor(0.2180)
features.13.conv.0 tensor(0.0722)
features.13.conv.3 tensor(0.1451)
features.13.conv.6 tensor(0.0953)
features.14.conv.0 tensor(0.8953)
features.14.conv.3 tensor(0.0881)
features.14.conv.6 tensor(0.8386)
features.15.conv.0 tensor(0.8976)
features.15.conv.3 tensor(0.0822)
features.15.conv.6 tensor(0.7857)
features.16.conv.0 tensor(0.0652)
features.16.conv.3 tensor(0.0969)
features.16.conv.6 tensor(0.1771)
conv.0 tensor(0.7131)
tensor(976939.) 2188896.0
0.83143812
0.82303292
0.82852364
0.84465510
0.84964871
0.84948045
0.84958625
0.84950966
0.84921968
0.84907961
0.84863991
0.84866601
0.84870708
0.84908783
0.84932286
0.84945184
INFO - Training [11][   20/  196]   Loss 0.657036   Top1 77.832031   Top5 96.640625   BatchTime 0.438466   LR 0.002481
0.84936017
0.84952581
0.84891981
0.84829736
0.84787214
0.84760356
0.84756333
0.84719843
0.84757233
0.84770924
0.84800071
0.84769469
0.84786946
0.84842521
0.84916240
0.85044324
0.85295910
0.85677069
0.85940522
0.86127907
0.86230224
INFO - Training [11][   40/  196]   Loss 0.637489   Top1 78.222656   Top5 97.060547   BatchTime 0.403734   LR 0.002478
0.86266100
0.86239564
0.86138594
0.85991758
0.85812670
0.85604531
0.85309166
0.85051715
0.84870565
0.84793091
0.84713513
0.84677428
0.84642333
0.84640282
0.84634948
0.84622681
0.84616417
0.84595239
0.84592563
0.84570426
0.84572273
0.84596479
INFO - Training [11][   60/  196]   Loss 0.630961   Top1 78.554688   Top5 97.272135   BatchTime 0.388691   LR 0.002474
0.84622318
0.84658206
0.84665829
0.84716409
0.84788656
0.84955364
0.85343462
0.85885876
0.86438698
0.86893523
0.87207413
0.87400395
0.87530899
0.87537724
0.87522638
0.87505454
0.87507498
0.87535173
0.87557054
0.87559348
0.87569886
0.87594563
INFO - Training [11][   80/  196]   Loss 0.628542   Top1 78.618164   Top5 97.470703   BatchTime 0.384997   LR 0.002470
0.87570071
0.87567264
0.87553078
0.87547517
0.87514901
0.87516600
0.87490702
0.87511426
0.87502342
0.87449145
0.87414998
0.87394899
0.87408018
0.87405384
0.87368470
0.87344342
INFO - Training [11][  100/  196]   Loss 0.618390   Top1 78.867188   Top5 97.566406   BatchTime 0.381084   LR 0.002465
0.87319070
0.87287426
0.87263936
0.87224263
0.87191576
0.87170750
0.87175816
0.87173802
0.87170035
0.87172788
0.87137657
0.87122369
0.87114108
0.87058175
0.86984158
0.86941826
0.86909306
0.86889976
0.86802471
0.86675781
0.86599326
0.86458105
INFO - Training [11][  120/  196]   Loss 0.610255   Top1 79.153646   Top5 97.692057   BatchTime 0.381544   LR 0.002460
0.86217070
0.85864651
0.85518342
0.85914677
0.86342609
0.86617213
0.86870217
0.87166107
0.87163246
0.87142551
0.87156445
0.87117463
0.87112105
0.87098473
0.87216032
0.87194264
INFO - Training [11][  140/  196]   Loss 0.607745   Top1 79.238281   Top5 97.745536   BatchTime 0.374766   LR 0.002455
0.87213933
0.87257713
0.87252402
0.87252170
0.87254381
0.87270904
0.87272340
0.87277520
0.87294823
0.87298924
0.87308991
0.87296909
0.87309474
0.87293118
0.87299204
0.87289792
0.87289727
0.87309593
0.87321609
0.87332904
0.87316877
0.87298882
0.87301213
0.87289888
INFO - Training [11][  160/  196]   Loss 0.609638   Top1 79.177246   Top5 97.783203   BatchTime 0.370958   LR 0.002450
0.87288249
0.87289017
0.87282115
0.87282664
0.87276226
0.87263960
0.87261134
0.87280631
0.87293279
0.87301850
0.87292981
0.87289959
0.87271768
0.87276006
0.87277591
0.87281662
0.87275189
0.87196964
0.87190920
0.87207448
0.87230492
0.87252635
INFO - Training [11][  180/  196]   Loss 0.604610   Top1 79.281684   Top5 97.749566   BatchTime 0.360610   LR 0.002444
0.87244046
0.87233353
0.87247193
0.87256789
0.87257969
0.87262112
0.87245107
0.87215191
0.87199205
0.87187427
0.87165880
0.87144953
0.87134701
0.87159157
0.87148243
INFO - ==> Top1: 79.356    Top5: 97.766    Loss: 0.604
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [11][   20/   40]   Loss 0.458464   Top1 84.941406   Top5 99.355469   BatchTime 0.119416
INFO - Validation [11][   40/   40]   Loss 0.442060   Top1 85.350000   Top5 99.440000   BatchTime 0.085697
INFO - ==> Top1: 85.350    Top5: 99.440    Loss: 0.442
INFO - ==> Sparsity : 0.319
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 86.380   Top5: 99.500]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5382)
features.0.conv.3 tensor(0.2129)
features.1.conv.0 tensor(0.0417)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0681)
features.2.conv.0 tensor(0.0289)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0781)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0617)
features.3.conv.6 tensor(0.0519)
features.4.conv.0 tensor(0.0352)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0970)
features.5.conv.0 tensor(0.0386)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.0955)
features.6.conv.0 tensor(0.0269)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0610)
features.7.conv.0 tensor(0.0480)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1140)
features.8.conv.0 tensor(0.0573)
features.8.conv.3 tensor(0.1311)
features.8.conv.6 tensor(0.1203)
features.9.conv.0 tensor(0.0872)
features.9.conv.3 tensor(0.1510)
features.9.conv.6 tensor(0.1279)
features.10.conv.0 tensor(0.0498)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0807)
features.11.conv.0 tensor(0.4020)
features.11.conv.3 tensor(0.1132)
features.11.conv.6 tensor(0.1849)
features.12.conv.0 tensor(0.2079)
features.12.conv.3 tensor(0.1128)
features.12.conv.6 tensor(0.2137)
features.13.conv.0 tensor(0.0807)
features.13.conv.3 tensor(0.1539)
features.13.conv.6 tensor(0.0932)
features.14.conv.0 tensor(0.8976)
features.14.conv.3 tensor(0.0863)
features.14.conv.6 tensor(0.9040)
features.15.conv.0 tensor(0.9018)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.3144)
features.16.conv.0 tensor(0.0796)
features.16.conv.3 tensor(0.0977)
features.16.conv.6 tensor(0.2226)
conv.0 tensor(0.1418)
tensor(699215.) 2188896.0
0.87138695
0.87129593
0.87132341
0.87130022
0.87131804
0.87113369
0.87109274
0.87107301
0.87106949
0.87109298
0.87107241
0.87106717
0.87109417
0.87123424
0.87130970
0.87122113
0.87102258
0.87107974
0.87090665
0.87083834
INFO - Training [12][   20/  196]   Loss 0.568999   Top1 80.546875   Top5 97.226562   BatchTime 0.430530   LR 0.002433
0.87101537
0.87113404
0.87091494
0.87087846
0.87080520
0.87063587
0.87067389
0.87029743
0.86994463
0.86941570
0.86904931
0.86840266
0.86740965
0.86559778
0.86487788
0.86520779
0.86576897
0.86635971
0.86654413
0.86602747
INFO - Training [12][   40/  196]   Loss 0.574690   Top1 80.390625   Top5 97.626953   BatchTime 0.412007   LR 0.002426
0.86483741
0.86246032
0.85918087
0.85502255
0.85075581
0.84698844
0.84441817
0.84345412
0.84307188
0.84274852
0.84259987
0.84232014
0.84187043
0.84179133
0.84168392
0.84172666
INFO - Training [12][   60/  196]   Loss 0.574969   Top1 80.240885   Top5 97.682292   BatchTime 0.398099   LR 0.002419
0.84163797
0.84205371
0.84278286
0.84341049
0.84413010
0.84476244
0.84483302
0.84425980
0.84301341
0.84152055
0.84060043
0.84006035
0.83976048
0.83997357
0.84034258
0.84040016
0.84028232
0.84009010
0.84008336
0.84013528
0.84045231
0.84190321
0.84225595
INFO - Training [12][   80/  196]   Loss 0.569177   Top1 80.541992   Top5 97.763672   BatchTime 0.388392   LR 0.002412
0.84280890
0.84307450
0.84356743
0.84395301
0.84416205
0.84393007
0.84388590
0.84360200
0.84347141
0.84336239
0.84334886
0.84359783
0.84337783
0.84344530
0.84304368
0.84270781
0.84273130
0.84317797
0.84338087
0.84377384
0.84527534
0.84487343
INFO - Training [12][  100/  196]   Loss 0.560287   Top1 80.828125   Top5 97.867188   BatchTime 0.384578   LR 0.002404
0.84444481
0.84419411
0.84412819
0.84412813
0.84381908
0.84373891
0.84327263
0.84303880
0.84280193
0.84275472
0.84280163
0.84305239
0.84316951
0.84317577
0.84342599
0.84326822
INFO - Training [12][  120/  196]   Loss 0.556474   Top1 80.983073   Top5 97.975260   BatchTime 0.380573   LR 0.002396
0.84297633
0.84284306
0.84297639
0.84299612
0.84308726
0.84280276
0.84269446
0.84267181
0.84288681
0.84301299
0.84301186
0.84290850
0.84274274
0.84266502
0.84248406
0.84199911
0.84171969
0.84145141
0.84125435
0.84125912
0.84114331
0.84110445
0.84103078
INFO - Training [12][  140/  196]   Loss 0.555945   Top1 81.010045   Top5 97.996652   BatchTime 0.376625   LR 0.002388
0.84082484
0.84086210
0.84050941
0.84028506
0.84028447
0.84023660
0.83990455
0.84022504
0.84041315
0.84018975
0.84016436
0.84047616
0.84099525
0.84203327
0.84324211
0.84297353
0.84297282
INFO - Training [12][  160/  196]   Loss 0.557578   Top1 80.981445   Top5 98.005371   BatchTime 0.376645   LR 0.002380
0.84312642
0.84319407
0.84328526
0.84341365
0.84368384
0.84369981
0.84353024
0.84341580
0.84321004
0.84312016
0.84319162
0.84286809
0.84266251
0.84245759
0.84196335
0.84179789
0.84158450
0.84130907
0.84099245
0.84059918
0.84055477
0.84034944
0.84011841
INFO - Training [12][  180/  196]   Loss 0.559054   Top1 80.872396   Top5 97.942708   BatchTime 0.371489   LR 0.002371
0.84011179
0.84028465
0.84034991
0.84032869
0.84039748
0.84070832
0.84229505
0.84235108
0.84248263
0.84241730
0.84237814
0.84230524
INFO - ==> Top1: 80.962    Top5: 97.970    Loss: 0.557
0.84233052
0.84253699
0.84254116
0.84245110
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.419991   Top1 86.113281   Top5 99.394531   BatchTime 0.120925
INFO - Validation [12][   40/   40]   Loss 0.409304   Top1 86.390000   Top5 99.470000   BatchTime 0.085606
INFO - ==> Top1: 86.390    Top5: 99.470    Loss: 0.409
INFO - ==> Sparsity : 0.362
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 86.390   Top5: 99.470]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0269)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0784)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0671)
features.3.conv.6 tensor(0.0543)
features.4.conv.0 tensor(0.0348)
features.4.conv.3 tensor(0.1088)
features.4.conv.6 tensor(0.0993)
features.5.conv.0 tensor(0.0363)
features.5.conv.3 tensor(0.0793)
features.5.conv.6 tensor(0.0967)
features.6.conv.0 tensor(0.0202)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0574)
features.7.conv.0 tensor(0.0528)
features.7.conv.3 tensor(0.1155)
features.7.conv.6 tensor(0.1088)
features.8.conv.0 tensor(0.0618)
features.8.conv.3 tensor(0.1337)
features.8.conv.6 tensor(0.1230)
features.9.conv.0 tensor(0.0736)
features.9.conv.3 tensor(0.1499)
features.9.conv.6 tensor(0.1248)
features.10.conv.0 tensor(0.0457)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0824)
features.11.conv.0 tensor(0.4013)
features.11.conv.3 tensor(0.1148)
features.11.conv.6 tensor(0.2493)
features.12.conv.0 tensor(0.1693)
features.12.conv.3 tensor(0.1217)
features.12.conv.6 tensor(0.2069)
features.13.conv.0 tensor(0.0750)
features.13.conv.3 tensor(0.1518)
features.13.conv.6 tensor(0.0908)
features.14.conv.0 tensor(0.9098)
features.14.conv.3 tensor(0.0866)
features.14.conv.6 tensor(0.9084)
features.15.conv.0 tensor(0.9040)
features.15.conv.3 tensor(0.0833)
features.15.conv.6 tensor(0.8487)
features.16.conv.0 tensor(0.0791)
features.16.conv.3 tensor(0.0976)
features.16.conv.6 tensor(0.2392)
conv.0 tensor(0.1492)
tensor(792371.) 2188896.0
0.84260255
0.84247220
0.84263575
0.84250170
0.84247959
0.84241605
0.84249091
0.84221178
0.84219950
0.84228951
0.84220213
0.84246653
0.84233755
0.84213591
0.84184229
0.84152877
INFO - Training [13][   20/  196]   Loss 0.548364   Top1 81.367188   Top5 97.363281   BatchTime 0.424990   LR 0.002355
0.84132242
0.84082329
0.84183609
0.84201348
0.84217733
0.84212500
0.84137940
0.84019524
0.84006137
0.83984500
0.84015137
0.83969498
0.83941334
0.83942449
0.83919632
0.83897263
0.83898079
0.83905858
0.83823687
0.83829033
0.83869457
INFO - Training [13][   40/  196]   Loss 0.553069   Top1 81.171875   Top5 97.666016   BatchTime 0.410133   LR 0.002345
0.83994037
0.83973652
0.83964491
0.83970577
0.83982825
0.83985245
0.83969569
0.83970517
0.83992130
0.84000832
0.84002352
0.84035301
0.84067839
0.84060609
0.84045160
0.84052193
0.84030139
0.84048969
0.84077317
0.84084076
INFO - Training [13][   60/  196]   Loss 0.548566   Top1 81.399740   Top5 97.845052   BatchTime 0.402455   LR 0.002336
0.84083939
0.84093207
0.84118056
0.84109825
0.84124422
0.84105384
0.84080976
0.84071672
0.84065616
0.84069788
0.84021813
0.84025025
0.84028232
0.84051627
0.84081602
0.84080708
0.84078676
0.84057564
0.84069979
0.84070897
0.84033185
INFO - Training [13][   80/  196]   Loss 0.554791   Top1 81.064453   Top5 97.871094   BatchTime 0.398537   LR 0.002325
0.84026527
0.84024101
0.83997661
0.84015715
0.84055376
0.84022152
0.84010011
0.84030569
0.84058493
0.84073699
0.84079438
0.84089792
0.84001917
0.83974504
0.83966726
0.83951557
0.83911347
0.83883733
0.83850306
0.83821833
0.83811808
0.83803231
0.83808988
INFO - Training [13][  100/  196]   Loss 0.550429   Top1 81.222656   Top5 97.902344   BatchTime 0.389792   LR 0.002315
0.83811891
0.83827269
0.83829343
0.83820891
0.83827639
0.83820558
0.83834612
0.83865643
0.83896559
0.83900440
0.83936769
0.83979797
0.83979899
0.83939004
0.83929205
0.83952469
INFO - Training [13][  120/  196]   Loss 0.546026   Top1 81.370443   Top5 97.945964   BatchTime 0.386494   LR 0.002304
0.83933443
0.83953971
0.83937746
0.83907557
0.83883685
0.83894026
0.83900917
0.83879638
0.83865112
0.83861870
0.83883905
0.83876175
0.83875829
0.83887476
0.83890396
0.83861554
0.83836812
0.83829308
0.83823663
0.83780932
0.83796871
0.83842647
INFO - Training [13][  140/  196]   Loss 0.545649   Top1 81.431362   Top5 98.049665   BatchTime 0.381535   LR 0.002293
0.83868426
0.83860451
0.83848232
0.83830833
0.83803159
0.83786446
0.83776146
0.83800524
0.83809751
0.83803046
0.83793068
0.83782703
0.83776909
0.83754957
0.83731180
0.83725864
0.83715653
INFO - Training [13][  160/  196]   Loss 0.545332   Top1 81.445312   Top5 98.051758   BatchTime 0.378669   LR 0.002282
0.83719498
0.83707404
0.83700323
0.83682871
0.83675784
0.83675611
0.83667427
0.83663017
0.83644724
0.83620179
0.83599877
0.83580422
0.83559412
0.83533007
0.83524334
0.83527362
0.83543336
0.83553123
0.83539194
0.83506888
0.83505905
0.83527219
0.83534157
0.83540314
INFO - Training [13][  180/  196]   Loss 0.545774   Top1 81.399740   Top5 98.009983   BatchTime 0.373552   LR 0.002271
0.83566546
0.83615988
0.83830595
0.83839279
0.83848029
0.83857018
0.83856988
0.83868045
0.83850658
0.83849049
0.83856887
0.83848995
0.83858478
INFO - ==> Top1: 81.452    Top5: 98.020    Loss: 0.544
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83864963
0.83861923
0.83850825
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.430571   Top1 85.488281   Top5 99.433594   BatchTime 0.118422
INFO - Validation [13][   40/   40]   Loss 0.420024   Top1 85.830000   Top5 99.510000   BatchTime 0.084724
INFO - ==> Top1: 85.830    Top5: 99.510    Loss: 0.420
INFO - ==> Sparsity : 0.380
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 86.390   Top5: 99.470]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5312)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0365)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0621)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0839)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0633)
features.3.conv.6 tensor(0.0506)
features.4.conv.0 tensor(0.0399)
features.4.conv.3 tensor(0.1082)
features.4.conv.6 tensor(0.0972)
features.5.conv.0 tensor(0.0379)
features.5.conv.3 tensor(0.0787)
features.5.conv.6 tensor(0.0964)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0570)
features.7.conv.0 tensor(0.0538)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1151)
features.8.conv.0 tensor(0.0660)
features.8.conv.3 tensor(0.1357)
features.8.conv.6 tensor(0.1214)
features.9.conv.0 tensor(0.0787)
features.9.conv.3 tensor(0.1421)
features.9.conv.6 tensor(0.1276)
features.10.conv.0 tensor(0.0416)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.0774)
features.11.conv.0 tensor(0.4238)
features.11.conv.3 tensor(0.1127)
features.11.conv.6 tensor(0.2600)
features.12.conv.0 tensor(0.1685)
features.12.conv.3 tensor(0.1248)
features.12.conv.6 tensor(0.2150)
features.13.conv.0 tensor(0.0817)
features.13.conv.3 tensor(0.1510)
features.13.conv.6 tensor(0.0808)
features.14.conv.0 tensor(0.9095)
features.14.conv.3 tensor(0.0852)
features.14.conv.6 tensor(0.9808)
features.15.conv.0 tensor(0.9057)
features.15.conv.3 tensor(0.0841)
features.15.conv.6 tensor(0.9317)
features.16.conv.0 tensor(0.0857)
features.16.conv.3 tensor(0.0977)
features.16.conv.6 tensor(0.2578)
conv.0 tensor(0.1634)
tensor(830805.) 2188896.0
0.83832890
0.83818990
0.83853948
0.83843505
0.83818179
0.83815634
0.83825582
0.83821869
0.83817345
0.83801842
0.83831614
0.83805108
0.83723956
0.83768308
0.83797604
0.83807778
INFO - Training [14][   20/  196]   Loss 0.522823   Top1 81.816406   Top5 97.363281   BatchTime 0.438169   LR 0.002250
0.83803052
0.83818233
0.83808279
0.83833981
0.83863884
0.83881259
0.83921355
0.83976227
0.84192282
0.84708309
0.85501689
0.86198449
0.86559993
0.86602390
0.86605370
0.86589170
0.86571461
0.86609817
0.86612040
0.86599338
0.86587447
0.86576611
INFO - Training [14][   40/  196]   Loss 0.544802   Top1 81.162109   Top5 97.626953   BatchTime 0.401795   LR 0.002238
0.86580187
0.86539716
0.86551654
0.86626971
0.86596143
0.86568034
0.86520845
0.86496067
0.86477095
0.86455953
0.86429554
0.86399084
0.86365044
0.86357391
0.86346704
0.86333311
0.86317688
0.86324614
0.86442596
0.86466819
INFO - Training [14][   60/  196]   Loss 0.539670   Top1 81.458333   Top5 97.832031   BatchTime 0.402475   LR 0.002225
0.86481816
0.86507267
0.86517161
0.86497843
0.86488110
0.86474591
0.86460948
0.86422706
0.86417985
0.86458051
0.86472261
0.86468869
0.86452049
0.86430407
0.86427987
0.86418921
0.86423862
0.86389416
0.86322886
0.86292076
INFO - Training [14][   80/  196]   Loss 0.536652   Top1 81.591797   Top5 97.924805   BatchTime 0.400502   LR 0.002213
0.86315459
0.86308926
0.86305088
0.86313820
0.86347669
0.86495489
0.86472672
0.86433554
0.86456925
0.86496454
0.86493158
0.86501515
0.86522639
0.86542279
0.86516905
0.86514616
0.86477751
0.86470699
0.86486942
0.86498654
0.86456865
INFO - Training [14][  100/  196]   Loss 0.530201   Top1 81.769531   Top5 98.000000   BatchTime 0.396707   LR 0.002200
0.86453044
0.86457437
0.86473858
0.86488312
0.86487401
0.86488938
0.86491901
0.86487812
0.86471981
0.86472028
0.86492515
0.86505324
0.86517018
0.86513454
0.86517006
0.86525398
0.86530471
0.86525881
0.86522603
0.86529964
0.86501777
INFO - Training [14][  120/  196]   Loss 0.525814   Top1 81.949870   Top5 98.095703   BatchTime 0.394596   LR 0.002186
0.86482650
0.86474305
0.86477923
0.86482120
0.86493021
0.86451608
0.86448646
0.86455959
0.86425817
0.86425614
0.86406893
0.86393839
0.86350489
0.86333591
0.86299896
0.86305666
INFO - Training [14][  140/  196]   Loss 0.526938   Top1 81.961496   Top5 98.119420   BatchTime 0.392507   LR 0.002173
0.86334187
0.86374390
0.86356246
0.86333984
0.86331612
0.86329675
0.86330986
0.86309320
0.86323047
0.86343831
0.86344457
0.86336756
0.86351973
0.86364752
0.86337227
0.86314148
0.86302453
0.86298662
0.86268562
0.86257309
INFO - Training [14][  160/  196]   Loss 0.528180   Top1 81.962891   Top5 98.085938   BatchTime 0.392498   LR 0.002159
0.86258364
0.86254460
0.86246926
0.86247134
0.86268479
0.86288774
0.86302620
0.86320567
0.86321884
0.86319017
0.86326909
0.86345905
0.86389118
0.86470795
0.86475170
0.86475229
0.86449629
0.86380988
0.86426568
0.86486661
0.86502939
0.86531222
0.86554080
0.86549866
INFO - Training [14][  180/  196]   Loss 0.528071   Top1 81.987847   Top5 98.031684   BatchTime 0.386573   LR 0.002145
0.86549193
0.86548054
0.86589295
0.86614263
0.86614537
0.86603487
0.86589992
0.86580163
0.86585617
0.86576313
0.86557806
0.86539418
0.86515653
0.86503083
0.86501515
0.86498946
********************pre-trained*****************
INFO - ==> Top1: 82.086    Top5: 98.034    Loss: 0.525
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 0.398386   Top1 86.601562   Top5 99.511719   BatchTime 0.116915
INFO - Validation [14][   40/   40]   Loss 0.393351   Top1 86.600000   Top5 99.540000   BatchTime 0.084816
INFO - ==> Top1: 86.600    Top5: 99.540    Loss: 0.393
INFO - ==> Sparsity : 0.346
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 86.600   Top5: 99.540]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.2051)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0629)
features.2.conv.0 tensor(0.0240)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0851)
features.3.conv.0 tensor(0.0246)
features.3.conv.3 tensor(0.0602)
features.3.conv.6 tensor(0.0503)
features.4.conv.0 tensor(0.0405)
features.4.conv.3 tensor(0.1111)
features.4.conv.6 tensor(0.0978)
features.5.conv.0 tensor(0.0335)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0934)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0549)
features.7.conv.0 tensor(0.0522)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1186)
features.8.conv.0 tensor(0.0567)
features.8.conv.3 tensor(0.1314)
features.8.conv.6 tensor(0.1198)
features.9.conv.0 tensor(0.0775)
features.9.conv.3 tensor(0.1464)
features.9.conv.6 tensor(0.1270)
features.10.conv.0 tensor(0.0475)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.0776)
features.11.conv.0 tensor(0.4444)
features.11.conv.3 tensor(0.1123)
features.11.conv.6 tensor(0.2664)
features.12.conv.0 tensor(0.1978)
features.12.conv.3 tensor(0.1258)
features.12.conv.6 tensor(0.1936)
features.13.conv.0 tensor(0.0790)
features.13.conv.3 tensor(0.1578)
features.13.conv.6 tensor(0.0821)
features.14.conv.0 tensor(0.9174)
features.14.conv.3 tensor(0.0894)
features.14.conv.6 tensor(0.5195)
features.15.conv.0 tensor(0.9067)
features.15.conv.3 tensor(0.0831)
features.15.conv.6 tensor(0.9173)
features.16.conv.0 tensor(0.0866)
features.16.conv.3 tensor(0.0959)
features.16.conv.6 tensor(0.2715)
conv.0 tensor(0.1448)
tensor(757604.) 2188896.0
0.86513186
0.86529553
0.86526012
0.86568213
0.86596024
0.86600989
0.86652386
0.86645836
0.86653447
0.86667758
0.86668772
0.86650598
0.86661035
0.86677277
0.86655962
0.86657435
0.86648202
0.86620468
0.86605293
0.86637944
INFO - Training [15][   20/  196]   Loss 0.532092   Top1 81.914062   Top5 97.539062   BatchTime 0.437945   LR 0.002120
0.86630160
0.86622924
0.86588246
0.86557263
0.86536300
0.86508012
0.86484802
0.86461884
0.86450315
0.86436588
0.86426073
0.86393636
0.86360204
0.86337143
0.86301637
0.86263746
INFO - Training [15][   40/  196]   Loss 0.526541   Top1 81.875000   Top5 97.802734   BatchTime 0.402067   LR 0.002106
0.86284959
0.86278379
0.86258405
0.86243522
0.86276478
0.86245888
0.86227977
0.86230874
0.86190373
0.86186826
0.86163861
0.86136192
0.86128730
0.86142415
0.86158586
0.86199242
0.86153507
0.86104727
0.86108702
0.86058760
0.86052430
0.86054069
INFO - Training [15][   60/  196]   Loss 0.529649   Top1 81.940104   Top5 97.942708   BatchTime 0.395551   LR 0.002091
0.86068040
0.86106634
0.86125982
0.86129630
0.86164123
0.86191577
0.86187911
0.86313826
0.86322933
0.86349612
0.86368227
0.86361611
0.86371535
0.86387116
0.86410433
0.86444074
0.86505502
0.86636186
0.86974728
0.87392354
0.87827003
INFO - Training [15][   80/  196]   Loss 0.527176   Top1 82.104492   Top5 98.046875   BatchTime 0.387943   LR 0.002076
0.88224393
0.88523811
0.88739747
0.88884944
0.88970333
0.89059567
0.89041972
0.89059281
0.89069176
0.89024580
0.89015126
0.88955939
0.88888925
0.88847750
0.88862264
0.88835001
0.88816297
0.88785583
0.88800579
0.88764429
0.88754380
INFO - Training [15][  100/  196]   Loss 0.517524   Top1 82.378906   Top5 98.082031   BatchTime 0.389324   LR 0.002061
0.88721222
0.88709217
0.88716811
0.88688606
0.88643414
0.88602507
0.88604623
0.88577533
0.88541269
0.88505852
0.88447726
0.88397533
0.88327283
0.88275045
0.88321167
0.88229960
0.88151014
0.88069987
0.87906784
0.87728608
INFO - Training [15][  120/  196]   Loss 0.514315   Top1 82.454427   Top5 98.196615   BatchTime 0.390528   LR 0.002045
0.87591594
0.87484223
0.87332422
0.87160122
0.86983711
0.86879486
0.86805475
0.86665773
0.86576605
0.86512178
0.86441839
0.86375540
0.86346954
0.86356068
0.86309862
0.86255354
0.86235023
0.86221367
0.86201733
0.86192870
INFO - Training [15][  140/  196]   Loss 0.513964   Top1 82.536272   Top5 98.233817   BatchTime 0.391567   LR 0.002030
0.86189121
0.86174905
0.86154062
0.86170983
0.86186755
0.86184257
0.86201894
0.86213076
0.86218780
0.86203516
0.86196393
0.86206746
0.86193556
0.86192751
0.86201978
0.86183232
INFO - Training [15][  160/  196]   Loss 0.516706   Top1 82.470703   Top5 98.212891   BatchTime 0.388625   LR 0.002014
0.86163384
0.86156088
0.86136371
0.86155659
0.86104351
0.86073798
0.86040068
0.86016983
0.85978812
0.85994571
0.85980546
0.85998559
0.86003453
0.85989434
0.85991955
0.86013967
0.86016816
0.85980558
0.85926431
0.85874915
0.85842842
0.85824305
0.85779381
0.85750419
INFO - Training [15][  180/  196]   Loss 0.515532   Top1 82.497830   Top5 98.138021   BatchTime 0.383445   LR 0.001998
0.85682613
0.85637969
0.85615706
0.85511929
0.85355175
0.85395890
0.85584462
0.85577768
0.85547286
0.85487443
0.85397929
0.85248375
0.85056067
0.84804219
0.84349936
0.83904654
********************pre-trained*****************
INFO - ==> Top1: 82.542    Top5: 98.146    Loss: 0.514
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.415889   Top1 86.230469   Top5 99.296875   BatchTime 0.140492
INFO - Validation [15][   40/   40]   Loss 0.401737   Top1 86.500000   Top5 99.450000   BatchTime 0.100801
INFO - ==> Top1: 86.500    Top5: 99.450    Loss: 0.402
INFO - ==> Sparsity : 0.369
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 86.600   Top5: 99.540]
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.2090)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0778)
features.3.conv.0 tensor(0.0249)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0467)
features.4.conv.0 tensor(0.0371)
features.4.conv.3 tensor(0.1157)
features.4.conv.6 tensor(0.0957)
features.5.conv.0 tensor(0.0288)
features.5.conv.3 tensor(0.0810)
features.5.conv.6 tensor(0.0926)
features.6.conv.0 tensor(0.0256)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0564)
features.7.conv.0 tensor(0.0563)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.1208)
features.8.conv.0 tensor(0.0609)
features.8.conv.3 tensor(0.1253)
features.8.conv.6 tensor(0.1296)
features.9.conv.0 tensor(0.0720)
features.9.conv.3 tensor(0.1496)
features.9.conv.6 tensor(0.1233)
features.10.conv.0 tensor(0.0330)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0740)
features.11.conv.0 tensor(0.4560)
features.11.conv.3 tensor(0.1100)
features.11.conv.6 tensor(0.2817)
features.12.conv.0 tensor(0.1869)
features.12.conv.3 tensor(0.1275)
features.12.conv.6 tensor(0.2781)
features.13.conv.0 tensor(0.0821)
features.13.conv.3 tensor(0.1582)
features.13.conv.6 tensor(0.0852)
features.14.conv.0 tensor(0.9242)
features.14.conv.3 tensor(0.0916)
features.14.conv.6 tensor(0.8343)
features.15.conv.0 tensor(0.9099)
features.15.conv.3 tensor(0.0836)
features.15.conv.6 tensor(0.8417)
features.16.conv.0 tensor(0.0938)
features.16.conv.3 tensor(0.1017)
features.16.conv.6 tensor(0.2664)
conv.0 tensor(0.1607)
tensor(807752.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
0.83576059
0.83456165
0.83391637
0.83373165
0.83327395
0.83305246
0.83402354
0.83622468
0.83688343
0.83819479
0.83772016
0.83706063
0.83675969
0.83684403
0.83663183
0.83631265
0.83629048
0.83611745
INFO - Training [16][   20/  196]   Loss 0.524995   Top1 81.777344   Top5 97.968750   BatchTime 0.458273   LR 0.001969
0.83546323
0.83529627
0.83534729
0.83520275
0.83507031
0.83579367
0.83655345
0.83756799
0.83892572
0.83942473
0.84041721
0.84010977
0.83960038
0.83939892
0.83915967
0.83922917
0.83940214
0.83944941
0.83985031
0.83991361
0.83996379
INFO - Training [16][   40/  196]   Loss 0.519332   Top1 82.060547   Top5 97.968750   BatchTime 0.417950   LR 0.001953
0.83984387
0.83953083
0.83949548
0.83931249
0.83921969
0.83926708
0.83895135
0.83900422
0.83867532
0.83859944
0.83841020
0.83813781
0.83800834
0.83773476
0.83720756
0.83733827
0.83788288
0.83809608
0.83853877
0.83863473
0.83863884
0.83857661
INFO - Training [16][   60/  196]   Loss 0.511255   Top1 82.382812   Top5 98.098958   BatchTime 0.400757   LR 0.001936
0.83916730
0.84013242
0.84168190
0.84340096
0.84550309
0.84754187
0.84922510
0.84907216
0.84877270
0.84713060
0.84525567
0.84301239
0.84084284
0.83895367
0.83792090
0.83717805
INFO - Training [16][   80/  196]   Loss 0.514366   Top1 82.392578   Top5 98.144531   BatchTime 0.395133   LR 0.001919
0.83716214
0.83584243
0.83520037
0.83483583
0.83473593
0.83451420
0.83443779
0.83427262
0.83408242
0.83410233
0.83392102
0.83358920
0.83367020
0.83369499
0.83334208
0.83309197
0.83202654
0.82972407
0.82828814
0.82664871
0.82449138
INFO - Training [16][  100/  196]   Loss 0.505307   Top1 82.632812   Top5 98.171875   BatchTime 0.391333   LR 0.001902
0.82415628
0.82449138
0.82388252
0.82333809
0.82325357
0.82301521
0.82271081
0.82282650
0.82204401
0.82151604
0.82144809
0.82166409
0.82190377
0.82181054
0.82220966
0.82290339
0.82393032
0.82560360
0.82833785
0.83099157
0.83393615
0.83368009
0.83304542
INFO - Training [16][  120/  196]   Loss 0.499652   Top1 82.835286   Top5 98.232422   BatchTime 0.384751   LR 0.001885
0.83292997
0.83252877
0.83238393
0.83221298
0.83191323
0.83145267
0.83108300
0.83048177
0.82955515
0.82923889
0.82864094
0.82852322
0.82809460
0.82754159
0.82800025
0.82883316
0.83088762
INFO - Training [16][  140/  196]   Loss 0.497265   Top1 83.005022   Top5 98.281250   BatchTime 0.379314   LR 0.001867
0.83220702
0.83227652
0.83232635
0.83234298
0.83233118
0.83246934
0.83277816
0.83314788
0.83328795
0.83332139
0.83316833
0.83364344
0.83381873
0.83394128
0.83430171
0.83435154
0.83395398
0.83379519
INFO - Training [16][  160/  196]   Loss 0.498374   Top1 83.051758   Top5 98.283691   BatchTime 0.373811   LR 0.001850
0.83386093
0.83410388
0.83442622
0.83443898
0.83472300
0.83483905
0.83468491
0.83459157
0.83415437
0.83414358
0.83427489
0.83415443
0.83431202
0.83439749
0.83427900
0.83431298
0.83455223
0.83462512
0.83479846
0.83474487
0.83474773
0.83486938
INFO - Training [16][  180/  196]   Loss 0.497797   Top1 83.040365   Top5 98.216146   BatchTime 0.372357   LR 0.001832
0.83498752
0.83471119
0.83456457
0.83451909
0.83449149
0.83445352
0.83460921
0.83477211
0.83499992
0.83481848
0.83496678
0.83491099
0.83509696
0.83528274
0.83529276
0.83513725
0.83481270
INFO - ==> Top1: 83.036    Top5: 98.208    Loss: 0.498
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83483911
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.380699   Top1 86.679688   Top5 99.531250   BatchTime 0.127138
INFO - Validation [16][   40/   40]   Loss 0.373687   Top1 87.060000   Top5 99.610000   BatchTime 0.096634
INFO - ==> Top1: 87.060    Top5: 99.610    Loss: 0.374
INFO - ==> Sparsity : 0.382
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 87.060   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.2891)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0208)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0540)
features.3.conv.6 tensor(0.0460)
features.4.conv.0 tensor(0.0360)
features.4.conv.3 tensor(0.1157)
features.4.conv.6 tensor(0.0955)
features.5.conv.0 tensor(0.0182)
features.5.conv.3 tensor(0.0793)
features.5.conv.6 tensor(0.0964)
features.6.conv.0 tensor(0.0254)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0566)
features.7.conv.0 tensor(0.0485)
features.7.conv.3 tensor(0.1126)
features.7.conv.6 tensor(0.1325)
features.8.conv.0 tensor(0.0526)
features.8.conv.3 tensor(0.1256)
features.8.conv.6 tensor(0.1189)
features.9.conv.0 tensor(0.0776)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.1212)
features.10.conv.0 tensor(0.0413)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0720)
features.11.conv.0 tensor(0.4433)
features.11.conv.3 tensor(0.1169)
features.11.conv.6 tensor(0.2989)
features.12.conv.0 tensor(0.1892)
features.12.conv.3 tensor(0.1256)
features.12.conv.6 tensor(0.2220)
features.13.conv.0 tensor(0.0902)
features.13.conv.3 tensor(0.1526)
features.13.conv.6 tensor(0.0881)
features.14.conv.0 tensor(0.9265)
features.14.conv.3 tensor(0.0913)
features.14.conv.6 tensor(0.8922)
features.15.conv.0 tensor(0.9106)
features.15.conv.3 tensor(0.0840)
features.15.conv.6 tensor(0.9236)
features.16.conv.0 tensor(0.0919)
features.16.conv.3 tensor(0.1008)
features.16.conv.6 tensor(0.2814)
conv.0 tensor(0.1721)
tensor(836427.) 2188896.0
0.83509469
0.83522964
0.83481365
0.83489531
0.83491242
0.83460814
0.83449227
0.83406347
0.83405381
0.83433110
0.83430558
0.83432871
0.83418667
0.83401847
0.83393615
0.83427048
0.83386683
0.83377212
0.83397776
0.83384120
0.83398783
INFO - Training [17][   20/  196]   Loss 0.516236   Top1 81.367188   Top5 97.949219   BatchTime 0.433249   LR 0.001800
0.83403742
0.83385926
0.83362794
0.83370078
0.83403915
0.83435714
0.83408850
0.83422482
0.83427477
0.83401918
0.83381003
0.83391160
0.83394516
0.83389246
0.83392709
0.83382148
0.83377701
0.83382899
0.83381724
0.83383948
INFO - Training [17][   40/  196]   Loss 0.503590   Top1 82.353516   Top5 97.929688   BatchTime 0.417270   LR 0.001782
0.83389103
0.83367705
0.83341575
0.83345062
0.83363330
0.83364224
0.83349276
0.83336216
0.83348292
0.83379579
0.83390778
0.83388174
0.83380121
0.83370638
0.83368045
0.83360732
0.83351642
0.83345425
0.83323193
INFO - Training [17][   60/  196]   Loss 0.494300   Top1 82.851562   Top5 98.072917   BatchTime 0.413454   LR 0.001764
0.83304268
0.83288050
0.83271259
0.83276302
0.83268130
0.83228290
0.83229512
0.83237398
0.83245176
0.83236861
0.83260971
0.83282775
0.83290821
0.83265847
0.83230120
0.83203250
INFO - Training [17][   80/  196]   Loss 0.494634   Top1 82.949219   Top5 98.144531   BatchTime 0.404085   LR 0.001746
0.83178800
0.83144873
0.83106625
0.83080775
0.83053625
0.83040261
0.83013934
0.82989031
0.82961845
0.82961708
0.82958490
0.82931000
0.82885808
0.82878429
0.82819211
0.82709944
0.82630461
0.82562548
0.82489860
0.82412487
0.82401234
0.82464558
INFO - Training [17][  100/  196]   Loss 0.489416   Top1 83.113281   Top5 98.160156   BatchTime 0.396549   LR 0.001727
0.82446021
0.82407606
0.82340074
0.82356888
0.82384068
0.82415116
0.82389474
0.82362694
0.82374614
0.82394797
0.82379591
0.82393694
0.82430351
0.82508850
0.82588410
0.82610559
0.82659954
0.82738125
0.82762241
0.82759601
0.82729483
INFO - Training [17][  120/  196]   Loss 0.486597   Top1 83.251953   Top5 98.255208   BatchTime 0.394254   LR 0.001708
0.82666194
0.82623482
0.82571447
0.82528907
0.82485390
0.82416588
0.82378650
0.82336658
0.82254744
0.82162815
0.82063085
0.81933075
0.81969368
0.81985909
0.82098234
0.82036769
0.81958181
0.81873751
0.81724042
0.81839263
0.81766605
0.81694287
INFO - Training [17][  140/  196]   Loss 0.484255   Top1 83.395647   Top5 98.317522   BatchTime 0.390311   LR 0.001690
0.81505728
0.81544685
0.81614304
0.81674683
0.81867987
0.81999016
0.82175672
0.82261115
0.82311243
0.82354301
0.82432491
0.82581192
0.83038557
0.83029652
0.83059728
0.83045280
INFO - Training [17][  160/  196]   Loss 0.486402   Top1 83.405762   Top5 98.291016   BatchTime 0.389170   LR 0.001671
0.83052766
0.83085090
0.83103675
0.83073223
0.83041859
0.83021504
0.83094460
0.83109832
0.83117861
0.83097285
0.83060259
0.83024096
0.82991618
0.82980353
0.82957089
0.82908010
0.82897663
0.82910937
0.82894945
0.82862455
0.82825255
INFO - Training [17][  180/  196]   Loss 0.485110   Top1 83.413628   Top5 98.248698   BatchTime 0.387467   LR 0.001652
0.82789379
0.82766354
0.82737708
0.82731837
0.82688814
0.82658869
0.82630450
0.82621843
0.82691437
0.82715595
0.82739002
0.82730937
0.82733119
0.82645732
0.82628638
INFO - ==> Top1: 83.456    Top5: 98.254    Loss: 0.484
0.82605714
0.82585347
0.82580274
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [17][   20/   40]   Loss 0.367132   Top1 87.890625   Top5 99.453125   BatchTime 0.130715
INFO - Validation [17][   40/   40]   Loss 0.361847   Top1 87.870000   Top5 99.590000   BatchTime 0.094727
INFO - ==> Top1: 87.870    Top5: 99.590    Loss: 0.362
INFO - ==> Sparsity : 0.381
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 88.000   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 87.870   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.2949)
features.1.conv.0 tensor(0.0365)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0608)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.0761)
features.3.conv.0 tensor(0.0205)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0436)
features.4.conv.0 tensor(0.0332)
features.4.conv.3 tensor(0.1175)
features.4.conv.6 tensor(0.1499)
features.5.conv.0 tensor(0.0409)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0944)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0552)
features.7.conv.0 tensor(0.0497)
features.7.conv.3 tensor(0.1082)
features.7.conv.6 tensor(0.1151)
features.8.conv.0 tensor(0.0561)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.1175)
features.9.conv.0 tensor(0.0749)
features.9.conv.3 tensor(0.1461)
features.9.conv.6 tensor(0.1182)
features.10.conv.0 tensor(0.0465)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0692)
features.11.conv.0 tensor(0.5233)
features.11.conv.3 tensor(0.1155)
features.11.conv.6 tensor(0.3037)
features.12.conv.0 tensor(0.1604)
features.12.conv.3 tensor(0.1302)
features.12.conv.6 tensor(0.2185)
features.13.conv.0 tensor(0.0864)
features.13.conv.3 tensor(0.1530)
features.13.conv.6 tensor(0.0874)
features.14.conv.0 tensor(0.9308)
features.14.conv.3 tensor(0.0882)
features.14.conv.6 tensor(0.8952)
features.15.conv.0 tensor(0.9110)
features.15.conv.3 tensor(0.0841)
features.15.conv.6 tensor(0.9457)
features.16.conv.0 tensor(0.0898)
features.16.conv.3 tensor(0.1032)
features.16.conv.6 tensor(0.2624)
conv.0 tensor(0.1655)
tensor(834764.) 2188896.0
0.82589835
0.82604611
0.82625079
0.82673508
0.82722819
0.82764751
0.82884204
0.82893169
0.82872862
0.82874894
0.82881981
0.82885295
0.82895082
0.82891500
0.82902580
0.82878757
0.82871640
0.82898539
0.82951725
INFO - Training [18][   20/  196]   Loss 0.486026   Top1 83.125000   Top5 97.734375   BatchTime 0.449157   LR 0.001618
0.82966477
0.82975531
0.82970005
0.82962406
0.82938713
0.82938445
0.82915109
0.82923633
0.82929790
0.82954556
0.82952166
0.82954764
0.83017588
0.83055663
0.83095628
0.83046293
0.82987338
0.82993865
0.82976961
0.83016455
INFO - Training [18][   40/  196]   Loss 0.489438   Top1 83.271484   Top5 97.851562   BatchTime 0.423546   LR 0.001599
0.82998234
0.82967293
0.82973462
0.82942873
0.82863480
0.82836264
0.82863063
0.82826298
0.82765591
0.82766306
0.82733208
0.82707912
0.82734591
0.82724303
0.82736552
0.82762176
0.82759690
0.82762063
0.82743227
0.82711458
INFO - Training [18][   60/  196]   Loss 0.484721   Top1 83.509115   Top5 98.001302   BatchTime 0.417202   LR 0.001579
0.82702804
0.82699883
0.82687265
0.82687962
0.82685518
0.82687473
0.82675034
0.82661861
0.82631487
0.82606852
0.82588583
0.82559401
0.82520217
0.82503909
0.82475448
0.82463986
0.82440042
0.82450479
0.82413208
0.82388979
INFO - Training [18][   80/  196]   Loss 0.482393   Top1 83.623047   Top5 98.139648   BatchTime 0.411483   LR 0.001560
0.82372701
0.82318515
0.82261753
0.82269120
0.82272249
0.82234877
0.82231981
0.82251364
0.82227135
0.82202429
0.82176399
0.82164037
0.82110441
0.82083094
0.82059342
0.82041848
0.82017887
INFO - Training [18][  100/  196]   Loss 0.474193   Top1 83.847656   Top5 98.222656   BatchTime 0.401815   LR 0.001540
0.81989545
0.81968445
0.81982648
0.81975526
0.81937623
0.81851918
0.81912220
0.81964552
0.81994247
0.81976163
0.81945497
0.81913972
0.81901032
0.81889999
0.81863230
0.81781709
0.81749713
0.81767744
0.81818962
0.81789488
0.81781602
0.81783104
INFO - Training [18][  120/  196]   Loss 0.464019   Top1 84.166667   Top5 98.313802   BatchTime 0.393252   LR 0.001521
0.81768560
0.81763268
0.81753474
0.81764299
0.81778204
0.81770808
0.81785023
0.81768620
0.81761235
0.81741196
0.81722969
0.81717211
0.81707138
0.81695706
0.81720990
0.81855762
0.81824124
0.81805557
0.81792772
0.81743705
0.81728446
0.81727654
0.81717646
INFO - Training [18][  140/  196]   Loss 0.463726   Top1 84.196429   Top5 98.353795   BatchTime 0.389457   LR 0.001501
0.81724739
0.81742275
0.81758839
0.81716275
0.81690109
0.81660408
0.81607181
0.81546229
0.81494576
0.81419665
0.81334335
0.81285793
0.81233215
0.81145859
0.81107175
0.81100613
INFO - Training [18][  160/  196]   Loss 0.465925   Top1 84.140625   Top5 98.364258   BatchTime 0.387057   LR 0.001482
0.81124741
0.81154597
0.81196707
0.81231505
0.81289655
0.81310427
0.81342804
0.81382191
0.81449008
0.81488913
0.81528842
0.81544179
0.81564319
0.81623614
0.81759518
0.82123649
0.82088035
0.82068199
0.82055241
0.81974244
0.81837773
INFO - Training [18][  180/  196]   Loss 0.466332   Top1 84.127604   Top5 98.311632   BatchTime 0.385137   LR 0.001462
0.81881851
0.81838590
0.81825405
0.81854528
0.81847978
0.81814182
0.81815374
0.81867456
0.81947130
0.82026917
0.82149023
0.82119524
0.82123971
0.82131046
0.82097584
0.82088417
0.82080293
INFO - ==> Top1: 84.228    Top5: 98.322    Loss: 0.464
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82050151
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 0.359240   Top1 88.300781   Top5 99.609375   BatchTime 0.117937
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.3867)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0638)
features.2.conv.0 tensor(0.0205)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0697)
features.3.conv.0 tensor(0.0263)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0445)
features.4.conv.0
INFO - Validation [18][   40/   40]   Loss 0.346826   Top1 88.510000   Top5 99.630000   BatchTime 0.097585
INFO - ==> Top1: 88.510    Top5: 99.630    Loss: 0.347
INFO - ==> Sparsity : 0.378
INFO - Scoreboard best 1 ==> Epoch [18][Top1: 88.510   Top5: 99.630]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
features.4.conv.0 tensor(0.0249)
features.4.conv.3 tensor(0.1123)
features.4.conv.6 tensor(0.0936)
features.5.conv.0 tensor(0.0194)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.0996)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0537)
features.7.conv.0 tensor(0.0469)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.1222)
features.8.conv.0 tensor(0.0660)
features.8.conv.3 tensor(0.1285)
features.8.conv.6 tensor(0.1138)
features.9.conv.0 tensor(0.0852)
features.9.conv.3 tensor(0.1464)
features.9.conv.6 tensor(0.1267)
features.10.conv.0 tensor(0.0339)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0677)
features.11.conv.0 tensor(0.4710)
features.11.conv.3 tensor(0.1155)
features.11.conv.6 tensor(0.3290)
features.12.conv.0 tensor(0.1541)
features.12.conv.3 tensor(0.1269)
features.12.conv.6 tensor(0.2121)
features.13.conv.0 tensor(0.0896)
features.13.conv.3 tensor(0.1532)
features.13.conv.6 tensor(0.0868)
features.14.conv.0 tensor(0.9357)
features.14.conv.3 tensor(0.0896)
features.14.conv.6 tensor(0.8907)
features.15.conv.0 tensor(0.9123)
features.15.conv.3 tensor(0.0817)
features.15.conv.6 tensor(0.9487)
features.16.conv.0 tensor(0.0945)
features.16.conv.3 tensor(0.1034)
features.16.conv.6 tensor(0.2504)
conv.0 tensor(0.1605)
tensor(828254.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
0.82019651
0.82005703
0.81983417
0.81987238
0.81971729
0.81913644
0.81918031
0.81878352
0.81837380
0.81872290
0.81881803
0.81871969
0.81895733
0.81835514
0.81810153
0.81819445
0.81861931
0.81893694
0.81924784
0.81936795
INFO - Training [19][   20/  196]   Loss 0.473064   Top1 83.281250   Top5 97.832031   BatchTime 0.436194   LR 0.001427
0.81965697
0.81985414
0.82142675
0.82133067
0.82135868
0.82122827
0.82119930
0.82126218
0.82136494
0.82151079
0.82144892
0.82108486
0.82100320
0.82101959
0.82097399
0.82059169
0.82030612
INFO - Training [19][   40/  196]   Loss 0.464147   Top1 83.505859   Top5 98.056641   BatchTime 0.397825   LR 0.001407
0.82015681
0.82030070
0.82019860
0.82022208
0.82018387
0.81998646
0.81980950
0.82688856
0.83243328
0.83182645
0.83151352
0.83155990
0.83135170
0.83107525
0.83116812
0.83100939
0.83088499
0.83059877
0.83051091
0.83004373
0.83001292
0.82990485
INFO - Training [19][   60/  196]   Loss 0.463007   Top1 83.691406   Top5 98.131510   BatchTime 0.386029   LR 0.001387
0.83005577
0.82982266
0.82969731
0.82965428
0.82932925
0.82930601
0.82922947
0.82912499
0.82922548
0.82899046
0.82886404
0.82935095
0.83032382
0.83025223
0.83046788
0.83040768
0.83008552
0.83001471
0.83054286
0.83116150
0.83137870
0.83144552
INFO - Training [19][   80/  196]   Loss 0.461476   Top1 83.916016   Top5 98.193359   BatchTime 0.380679   LR 0.001367
0.83173239
0.83201224
0.83151972
0.83145690
0.83127558
0.83127922
0.83125073
0.83158922
0.83179718
0.83180708
0.83181417
0.83179820
0.83174771
0.83198482
0.83184481
0.83158594
INFO - Training [19][  100/  196]   Loss 0.457170   Top1 84.042969   Top5 98.214844   BatchTime 0.379234   LR 0.001347
0.83116877
0.83034676
0.83056509
0.83002210
0.82979423
0.82990360
0.82988191
0.82998902
0.82998180
0.83028740
0.83087194
0.83089375
0.83090311
0.83102518
0.83127654
0.83118212
0.83118075
0.83108735
0.83159411
0.83182573
0.83179450
INFO - Training [19][  120/  196]   Loss 0.449768   Top1 84.329427   Top5 98.277995   BatchTime 0.377714   LR 0.001327
0.83190697
0.83191746
0.83202720
0.83198208
0.83192366
0.83190042
0.83175617
0.83161587
0.83155841
0.83133876
0.83120292
0.83136886
0.83138663
0.83106840
0.83106434
0.83083206
0.83073205
0.83061963
0.83053374
0.83019090
INFO - Training [19][  140/  196]   Loss 0.448656   Top1 84.408482   Top5 98.342634   BatchTime 0.382091   LR 0.001307
0.82988811
0.83017069
0.83023876
0.83004326
0.83013254
0.83022863
0.83040786
0.83035713
0.83010441
0.83018851
0.82998782
0.82986850
0.83002096
0.83083940
0.83064091
0.83043838
0.83024877
0.83015317
0.83006692
0.83004302
0.82986563
INFO - Training [19][  160/  196]   Loss 0.452708   Top1 84.318848   Top5 98.322754   BatchTime 0.382805   LR 0.001287
0.82965189
0.82956594
0.82911754
0.82875896
0.82894284
0.82896674
0.82845062
0.82818645
0.82803673
0.82753283
0.82699430
0.82650530
0.82599121
0.82582968
0.82558447
0.82496500
0.82473058
0.82466608
0.82453346
0.82438493
0.82414007
0.82392812
INFO - Training [19][  180/  196]   Loss 0.453015   Top1 84.316406   Top5 98.279080   BatchTime 0.380697   LR 0.001266
0.82381952
0.82399315
0.82407963
0.82417279
0.82422364
0.82502657
0.82506418
0.82540715
0.82511765
0.82355458
0.82370222
0.82221919
INFO - ==> Top1: 84.408    Top5: 98.294    Loss: 0.451
0.81926996
0.82156754
0.82234484
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 0.357969   Top1 88.359375   Top5 99.492188   BatchTime 0.248993
INFO - Validation [19][   40/   40]   Loss 0.343824   Top1 88.670000   Top5 99.640000   BatchTime 0.159153
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.2773)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0616)
features.2.conv.0 tensor(0.0205)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0720)
features.3.conv.0 tensor(0.0220)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0421)
features.4.conv.0 tensor(0.0324)
features.4.conv.3 tensor(0.1152)
features.4.conv.6 tensor(0.0960)
features.5.conv.0 tensor(0.0216)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.0931)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0552)
features.7.conv.0 tensor(0.0459)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1378)
features.8.conv.0 tensor(0.0660)
features.8.conv.3 tensor(0.1314)
features.8.conv.6 tensor(0.1163)
features.9.conv.0 tensor(0.3088)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.1249)
features.10.conv.0 tensor(0.0374)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0681)
features.11.conv.0 tensor(0.4925)
features.11.conv.3 tensor(0.1211)
features.11.conv.6 tensor(0.3437)
features.12.conv.0 tensor(0.1846)
features.12.conv.3 tensor(0.1275)
features.12.conv.6 tensor(0.3476)
features.13.conv.0 tensor(0.0927)
features.13.conv.3 tensor(0.1524)
features.13.conv.6 tensor(0.0842)
features.14.conv.0 tensor(0.9340)
features.14.conv.3 tensor(0.0880)
features.14.conv.6 tensor(0.9270)
features.15.conv.0 tensor(0.9142)
features.15.conv.3 tensor(0.0829)
features.15.conv.6 tensor(0.9544)
features.16.conv.0 tensor(0.0943)
features.16.conv.3 tensor(0.1047)
features.16.conv.6 tensor(0.2586)
conv.0 tensor(0.1711)
tensor(858674.) 2188896.0
INFO - ==> Top1: 88.670    Top5: 99.640    Loss: 0.344
INFO - ==> Sparsity : 0.392
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.670   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 88.510   Top5: 99.630]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 88.270   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.82279694
0.82313585
0.82301778
0.82307583
0.82284445
0.82314324
0.82359749
0.82333589
0.82333630
0.82313472
0.82295948
0.82274783
0.82269478
0.82261646
0.82215297
0.82165986
0.82164687
0.82145780
0.82104713
INFO - Training [20][   20/  196]   Loss 0.469215   Top1 83.398438   Top5 97.851562   BatchTime 0.482916   LR 0.001231
0.82045341
0.82060772
0.82032806
0.82007676
0.81987292
0.81972152
0.81933212
0.81883466
0.81870228
0.81726682
0.81572670
0.81474507
0.81505954
0.81405848
0.81201398
0.80905741
0.80621362
0.80424386
0.80288506
0.80207181
0.80197537
INFO - Training [20][   40/  196]   Loss 0.457996   Top1 84.003906   Top5 98.183594   BatchTime 0.431387   LR 0.001211
0.80252856
0.80340862
0.80424803
0.80544150
0.80639356
0.80696297
0.80803061
0.80874771
0.80977023
0.81089205
0.81189489
0.81275296
0.81349045
0.81403035
0.81437659
0.81484181
0.81513894
INFO - Training [20][   60/  196]   Loss 0.459241   Top1 83.984375   Top5 98.183594   BatchTime 0.405720   LR 0.001191
0.81540567
0.81599462
0.81636590
0.81672955
0.81716627
0.81747675
0.81782228
0.81808627
0.81828225
0.81856257
0.81861717
0.81843364
0.81860769
0.81855500
0.81837165
0.81836385
0.81879318
0.81880015
0.81856304
0.81854922
0.81876451
INFO - Training [20][   80/  196]   Loss 0.451535   Top1 84.282227   Top5 98.305664   BatchTime 0.398251   LR 0.001171
0.81829411
0.81792396
0.81811768
0.81826895
0.81809360
0.81797808
0.81778157
0.81756872
0.81760240
0.81770498
0.81719851
0.81680715
0.81643552
0.81641501
0.81632066
0.81606370
0.81608158
0.81613755
0.81604928
0.81612688
0.81642818
INFO - Training [20][  100/  196]   Loss 0.445706   Top1 84.496094   Top5 98.371094   BatchTime 0.393923   LR 0.001151
0.81649154
0.81612206
0.81594664
0.81588966
0.81544739
0.81533682
0.81521064
0.81511241
0.81503135
0.81510979
0.81524098
0.81547600
0.81561506
0.81560886
0.81563634
0.81576538
0.81606448
0.81617874
0.81652927
0.81655008
INFO - Training [20][  120/  196]   Loss 0.440909   Top1 84.720052   Top5 98.427734   BatchTime 0.396275   LR 0.001131
0.81631064
0.81624234
0.81618512
0.81613469
0.81636310
0.81747425
0.81731427
0.81713200
0.81702131
0.81716990
0.81693518
0.81721920
0.81694496
0.81687009
0.81695265
0.81672972
0.81672430
0.81661576
0.81663263
0.81645441
INFO - Training [20][  140/  196]   Loss 0.439724   Top1 84.824219   Top5 98.493304   BatchTime 0.397353   LR 0.001111
0.81619900
0.81568950
0.81575865
0.81596351
0.81593090
0.81594467
0.81597292
0.81632555
0.81655109
0.81672049
0.81667978
0.81666207
0.81684929
0.81690776
0.81715775
0.81707084
0.81690091
0.81720144
0.81714922
INFO - Training [20][  160/  196]   Loss 0.439338   Top1 84.858398   Top5 98.496094   BatchTime 0.385968   LR 0.001091
0.81705439
0.81685996
0.81665784
0.81648219
0.81646019
0.81680411
0.81646055
0.81657857
0.81659073
0.81671244
0.81661361
0.81627250
0.81636572
0.81631577
0.81634939
0.81629473
0.81643558
0.81653935
0.81737304
0.81848788
0.81823492
0.81828934
INFO - Training [20][  180/  196]   Loss 0.440308   Top1 84.806858   Top5 98.450521   BatchTime 0.384751   LR 0.001071
0.81824726
0.81815362
0.81760216
0.81715548
0.81714284
0.81742579
0.81826919
0.81807166
0.81800979
0.81801188
0.81812263
0.81811935
0.81768858
0.81759185
0.81753224
INFO - ==> Top1: 84.880    Top5: 98.462    Loss: 0.438
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.81700480
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 0.367513   Top1 87.851562   Top5 99.492188   BatchTime 0.116547
INFO - Validation [20][   40/   40]   Loss 0.351042   Top1 88.330000   Top5 99.590000   BatchTime 0.082398
INFO - ==> Top1: 88.330    Top5: 99.590    Loss: 0.351
INFO - ==> Sparsity : 0.392
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 88.670   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 88.510   Top5: 99.630]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 88.330   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.2812)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0603)
features.2.conv.0 tensor(0.0188)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0689)
features.3.conv.0 tensor(0.0249)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0432)
features.4.conv.0 tensor(0.0382)
features.4.conv.3 tensor(0.1082)
features.4.conv.6 tensor(0.0949)
features.5.conv.0 tensor(0.0267)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0947)
features.6.conv.0 tensor(0.0260)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0520)
features.7.conv.0 tensor(0.0535)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1369)
features.8.conv.0 tensor(0.0511)
features.8.conv.3 tensor(0.1334)
features.8.conv.6 tensor(0.1131)
features.9.conv.0 tensor(0.0782)
features.9.conv.3 tensor(0.1415)
features.9.conv.6 tensor(0.1260)
features.10.conv.0 tensor(0.0432)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0682)
features.11.conv.0 tensor(0.4825)
features.11.conv.3 tensor(0.1225)
features.11.conv.6 tensor(0.3612)
features.12.conv.0 tensor(0.1909)
features.12.conv.3 tensor(0.1323)
features.12.conv.6 tensor(0.4411)
features.13.conv.0 tensor(0.0862)
features.13.conv.3 tensor(0.1534)
features.13.conv.6 tensor(0.0922)
features.14.conv.0 tensor(0.9399)
features.14.conv.3 tensor(0.0877)
features.14.conv.6 tensor(0.9506)
features.15.conv.0 tensor(0.9155)
features.15.conv.3 tensor(0.0822)
features.15.conv.6 tensor(0.9391)
features.16.conv.0 tensor(0.1005)
features.16.conv.3 tensor(0.1051)
features.16.conv.6 tensor(0.2520)
conv.0 tensor(0.1666)
tensor(858758.) 2188896.0
0.81704926
0.81667864
0.81660032
0.81631041
0.81613320
0.81579906
0.81564236
0.81586444
0.81552386
0.81558764
0.81589472
0.81585062
0.81600887
0.81609023
0.81601268
0.81610018
0.81580561
0.81561691
INFO - Training [21][   20/  196]   Loss 0.445382   Top1 84.589844   Top5 97.929688   BatchTime 0.422705   LR 0.001036
0.81564879
0.81561559
0.81683290
0.81667644
0.81662852
0.81686860
0.81650805
0.81647354
0.81631875
0.81608123
0.81598568
0.81599450
0.81613821
0.81600565
0.81570637
0.81573159
0.81569153
0.81599468
0.81596988
0.81588346
0.81602532
0.81612980
INFO - Training [21][   40/  196]   Loss 0.447071   Top1 84.335938   Top5 98.251953   BatchTime 0.394349   LR 0.001016
0.81557089
0.81528145
0.81514883
0.81650263
0.81628412
0.81624323
0.81629795
0.81612068
0.81588715
0.81588304
0.81556958
0.81587213
0.81594354
0.81658959
0.81658953
0.81647927
0.81633610
0.81769454
0.81823069
0.81763214
0.81768900
INFO - Training [21][   60/  196]   Loss 0.433985   Top1 84.986979   Top5 98.320312   BatchTime 0.388022   LR 0.000996
0.81796545
0.81771839
0.81768048
0.81772572
0.81776655
0.81781512
0.81746823
0.81823409
0.81804359
0.81827062
0.81835157
0.81816655
0.81799352
0.81820506
0.81838554
0.81858695
0.81933159
0.81925648
0.81926554
INFO - Training [21][   80/  196]   Loss 0.435243   Top1 85.053711   Top5 98.403320   BatchTime 0.395509   LR 0.000976
0.81911045
0.81891161
0.81880391
0.81876940
0.81885058
0.81894159
0.81901687
0.81901842
0.81884140
0.81859452
0.81848133
0.81828159
0.81802136
0.81777382
0.81776577
0.81767458
0.81778508
0.81758219
0.81761193
0.81726789
0.81705582
INFO - Training [21][  100/  196]   Loss 0.428257   Top1 85.316406   Top5 98.449219   BatchTime 0.391977   LR 0.000957
0.81677133
0.81646562
0.81651503
0.81642061
0.81639534
0.81617010
0.81606990
0.81594008
0.81604123
0.81583261
0.81579232
0.81590658
0.81588960
0.81598872
0.81646591
0.81724292
INFO - Training [21][  120/  196]   Loss 0.422254   Top1 85.530599   Top5 98.551432   BatchTime 0.387923   LR 0.000937
0.81725919
0.81700951
0.81684470
0.81688863
0.81666917
0.81654435
0.81637943
0.81631130
0.81651187
0.81620777
0.81607789
0.81633884
0.81616300
0.81607157
0.81632614
0.81629992
0.81626207
0.81619263
0.81625092
0.81604373
0.81534505
0.81514597
0.81550080
INFO - Training [21][  140/  196]   Loss 0.420220   Top1 85.577567   Top5 98.593750   BatchTime 0.384094   LR 0.000918
0.81517357
0.81561524
0.81548637
0.81571895
0.81572318
0.81569064
0.81549501
0.81537044
0.81520420
0.81505328
0.81489217
0.81630003
0.81594396
0.81558269
0.81530201
0.81510693
0.81464386
0.81384051
INFO - Training [21][  160/  196]   Loss 0.425690   Top1 85.339355   Top5 98.554688   BatchTime 0.377089   LR 0.000899
0.81365180
0.81336355
0.81314355
0.81308609
0.81323701
0.81325442
0.81321335
0.81309491
0.81269038
0.81225348
0.81200939
0.81182081
0.81160921
0.81153983
0.81145722
0.81126577
0.81097829
0.81068856
0.81044436
INFO - Training [21][  180/  196]   Loss 0.423650   Top1 85.397135   Top5 98.489583   BatchTime 0.371927   LR 0.000879
0.81025875
0.80991834
0.80973339
0.80938858
0.80935568
0.81051689
0.81069899
0.81068730
0.81058729
0.81027246
0.80976701
0.80951649
0.80950183
0.80945253
0.80934590
0.80922276
0.80910110
0.80888176
0.80866116
INFO - ==> Top1: 85.432    Top5: 98.478    Loss: 0.423
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [21][   20/   40]   Loss 0.325849   Top1 89.296875   Top5 99.667969   BatchTime 0.122077
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0293)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0534)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0668)
features.3.conv.0 tensor(0.0275)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0443)
features.4.conv.0 tensor(0.0306)
features.4.conv.3 tensor(0.1117)
features.4.conv.6 tensor(0.1204)
features.5.conv.0 tensor(0.0282)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.1001)
features.6.conv.0 tensor(0.0282)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0532)
features.7.conv.0 tensor(0.0619)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.1184)
features.8.conv.0 tensor(0.0590)
features.8.conv.3 tensor(0.1308)
features.8.conv.6 tensor(0.1094)
features.9.conv.0 tensor(0.0774)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.1227)
features.10.conv.0 tensor(0.0401)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0676)
features.11.conv.0 tensor(0.4881)
features.11.conv.3 tensor(0.1196)
features.11.conv.6 tensor(0.3815)
features.12.conv.0 tensor(0.1946)
features.12.conv.3 tensor(0.1300)
features.12.conv.6 tensor(0.4829)
features.13.conv.0 tensor(0.0877)
features.13.conv.3 tensor(0.1508)
features.13.conv.6 tensor(0.0910)
features.14.conv.0 tensor(0.9434)
features.14.conv.3 tensor(0.0874)
features.14.conv.6 tensor(0.9505)
features.15.conv.0 tensor(0.9170)
features.15.conv.3 tensor(0.0814)
features.15.conv.6 tensor(0.9625)
features.16.conv.0 tensor(0.0978)
features.16.conv.3 tensor(0.1023)
features.16.conv.6 tensor(0.2449)
conv.0 tensor(0.1806)
tensor(869911.) 2188896.0
INFO - Validation [21][   40/   40]   Loss 0.319558   Top1 89.200000   Top5 99.690000   BatchTime 0.089085
INFO - ==> Top1: 89.200    Top5: 99.690    Loss: 0.320
INFO - ==> Sparsity : 0.397
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 89.200   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 88.670   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 88.510   Top5: 99.630]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.80871922
0.80888247
0.80880719
0.80888087
0.80883181
0.80876386
0.80888039
0.80929583
0.81059247
0.81050760
0.81013805
0.80979955
0.80990803
0.80939609
0.80888379
0.80838317
0.80797607
0.80756772
0.80731571
0.80687982
0.80648971
INFO - Training [22][   20/  196]   Loss 0.420040   Top1 85.507812   Top5 97.910156   BatchTime 0.436945   LR 0.000846
0.80606341
0.80516678
0.80485040
0.80497664
0.80503809
0.80479622
0.80487972
0.80501109
0.80522501
0.80547303
0.80610466
0.80631483
0.80636597
0.80649811
0.80641335
0.80656916
INFO - Training [22][   40/  196]   Loss 0.423975   Top1 85.224609   Top5 98.173828   BatchTime 0.402220   LR 0.000827
0.80661482
0.80679941
0.80694634
0.80809504
0.80872649
0.80900306
0.80903459
0.80901819
0.80897498
0.80887592
0.80872947
0.80898798
0.80909628
0.80927062
0.80930543
0.80959105
0.80947000
0.80951339
0.80953383
0.80939430
0.80928469
0.80910021
INFO - Training [22][   60/  196]   Loss 0.423357   Top1 85.221354   Top5 98.313802   BatchTime 0.390390   LR 0.000808
0.80867094
0.80835229
0.80851781
0.80826187
0.80802011
0.80777067
0.80757266
0.80693781
0.80721295
0.80729532
0.80726445
0.80845237
0.80828160
0.80835092
0.80812508
0.80821484
0.80838394
0.80848145
0.80849189
0.80858850
0.80851710
0.80846113
INFO - Training [22][   80/  196]   Loss 0.422206   Top1 85.322266   Top5 98.354492   BatchTime 0.384737   LR 0.000789
0.80837744
0.80816513
0.80806559
0.80806351
0.80808026
0.80800819
0.80781609
0.80752403
0.80732983
0.80708176
0.80698770
0.80682439
0.80682373
0.80680776
0.80688936
0.80682778
0.80661124
0.80654240
0.80635417
0.80628556
INFO - Training [22][  100/  196]   Loss 0.418612   Top1 85.449219   Top5 98.410156   BatchTime 0.384953   LR 0.000770
0.80614716
0.80610633
0.80612868
0.80627149
0.80637920
0.80663663
0.80771428
0.80755639
0.80733210
0.80732393
0.80720586
0.80702639
0.80685592
0.80685437
0.80678612
0.80647987
0.80615628
INFO - Training [22][  120/  196]   Loss 0.411987   Top1 85.755208   Top5 98.535156   BatchTime 0.382660   LR 0.000752
0.80644190
0.80788773
0.80785811
0.80781287
0.80762690
0.80758578
0.80767506
0.80756426
0.80764747
0.80770248
0.80736935
0.80716300
0.80719352
0.80715752
0.80676985
0.80670696
0.80652857
0.80608648
0.80594409
0.80587918
0.80560356
0.80561215
0.80568343
0.80570352
INFO - Training [22][  140/  196]   Loss 0.406062   Top1 85.917969   Top5 98.607701   BatchTime 0.374553   LR 0.000734
0.80568302
0.80561638
0.80547780
0.80545950
0.80538750
0.80525744
0.80516291
0.80512482
0.80512720
0.80493510
0.80504572
0.80520093
0.80535704
0.80559534
0.80569893
0.80572158
0.80569720
0.80660528
INFO - Training [22][  160/  196]   Loss 0.410509   Top1 85.834961   Top5 98.571777   BatchTime 0.369623   LR 0.000715
0.80646515
0.80638516
0.80649430
0.80660254
0.80659163
0.80641454
0.80636805
0.80612904
0.80604601
0.80610478
0.80616826
0.80608946
0.80615157
0.80600172
0.80570543
0.80554932
0.80545288
0.80513608
0.80495399
INFO - Training [22][  180/  196]   Loss 0.410319   Top1 85.863715   Top5 98.513455   BatchTime 0.374035   LR 0.000697
0.80471849
0.80446583
0.80416077
0.80370098
0.80367434
0.80342221
0.80311304
0.80296624
0.80287594
0.80268991
0.80241692
0.80224472
0.80231649
0.80214334
INFO - ==> Top1: 85.892    Top5: 98.512    Loss: 0.409
0.80197448
0.80205625
0.80369163
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.357326   Top1 88.183594   Top5 99.472656   BatchTime 0.119099
INFO - Validation [22][   40/   40]   Loss 0.343381   Top1 88.720000   Top5 99.580000   BatchTime 0.084924
INFO - ==> Top1: 88.720    Top5: 99.580    Loss: 0.343
INFO - ==> Sparsity : 0.396
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 89.200   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.720   Top5: 99.580]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 88.670   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0599)
features.2.conv.0 tensor(0.0208)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0686)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0447)
features.4.conv.0 tensor(0.0360)
features.4.conv.3 tensor(0.1140)
features.4.conv.6 tensor(0.0911)
features.5.conv.0 tensor(0.0278)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.1012)
features.6.conv.0 tensor(0.0270)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0515)
features.7.conv.0 tensor(0.0544)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.1125)
features.8.conv.0 tensor(0.0522)
features.8.conv.3 tensor(0.1328)
features.8.conv.6 tensor(0.1104)
features.9.conv.0 tensor(0.0750)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.1227)
features.10.conv.0 tensor(0.0446)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0993)
features.11.conv.0 tensor(0.5071)
features.11.conv.3 tensor(0.1208)
features.11.conv.6 tensor(0.3747)
features.12.conv.0 tensor(0.2133)
features.12.conv.3 tensor(0.1273)
features.12.conv.6 tensor(0.4856)
features.13.conv.0 tensor(0.0864)
features.13.conv.3 tensor(0.1522)
features.13.conv.6 tensor(0.0912)
features.14.conv.0 tensor(0.9445)
features.14.conv.3 tensor(0.0888)
features.14.conv.6 tensor(0.9359)
features.15.conv.0 tensor(0.9177)
features.15.conv.3 tensor(0.0812)
features.15.conv.6 tensor(0.9505)
features.16.conv.0 tensor(0.1194)
features.16.conv.3 tensor(0.1003)
features.16.conv.6 tensor(0.2483)
conv.0 tensor(0.1684)
tensor(867890.) 2188896.0
0.80321527
0.80252802
0.80234963
0.80244762
0.80192196
0.80162323
0.80128264
0.80096596
0.80082297
0.80058295
0.80090773
0.80108398
0.80123591
0.80131429
0.80136377
0.80292040
0.80292994
0.80304712
0.80313718
0.80431950
INFO - Training [23][   20/  196]   Loss 0.411218   Top1 85.585938   Top5 97.929688   BatchTime 0.462555   LR 0.000666
0.80413848
0.80409199
0.80428857
0.80464125
0.80434668
0.80439073
0.80411905
0.80385369
0.80371332
0.80368537
0.80341327
0.80313540
0.80283535
0.80236757
0.80187082
0.80085808
0.79931504
0.79810959
0.79759222
0.79897678
INFO - Training [23][   40/  196]   Loss 0.425322   Top1 85.146484   Top5 98.154297   BatchTime 0.428770   LR 0.000648
0.79966044
0.79950041
0.79956847
0.80025786
0.80078667
0.80045152
0.80007744
0.79978496
0.79989976
0.79947019
0.79887241
0.79800451
0.79727340
0.79521728
0.79459703
0.79323477
0.79112840
0.78807062
0.78941292
0.79000294
0.78916442
INFO - Training [23][   60/  196]   Loss 0.413648   Top1 85.605469   Top5 98.307292   BatchTime 0.409868   LR 0.000630
0.78979689
0.78961223
0.79046959
0.79190177
0.79276317
0.79279786
0.79344785
0.79383183
0.79441690
0.79486424
0.79501241
0.79522073
0.79520035
0.79554051
0.79534215
0.79533756
INFO - Training [23][   80/  196]   Loss 0.411300   Top1 85.732422   Top5 98.432617   BatchTime 0.401347   LR 0.000613
0.79542667
0.79545218
0.79585660
0.79640663
0.79667562
0.79688895
0.79699874
0.79734695
0.79754269
0.79764241
0.79771107
0.79766595
0.79772413
0.79749376
0.79750943
0.79752135
0.79753721
0.79761112
0.79744238
0.79727721
0.79725140
0.79681522
INFO - Training [23][  100/  196]   Loss 0.406310   Top1 85.976562   Top5 98.523438   BatchTime 0.394088   LR 0.000596
0.79651076
0.79630631
0.79612815
0.79586929
0.79538512
0.79524434
0.79521996
0.79493099
0.79465002
0.79425192
0.79460496
0.79478151
0.79467899
0.79444635
0.79431689
0.79422778
0.79406029
0.79385895
0.79358953
0.79331434
0.79302061
0.79281288
INFO - Training [23][  120/  196]   Loss 0.397564   Top1 86.253255   Top5 98.629557   BatchTime 0.390699   LR 0.000579
0.79248548
0.79189932
0.79160076
0.79146051
0.79137015
0.79285216
0.79324907
0.79350078
0.79379195
0.79442358
0.79484463
0.79517138
0.79562354
0.79720622
0.79772812
0.79761696
0.79752636
0.79732758
0.79767185
INFO - Training [23][  140/  196]   Loss 0.397335   Top1 86.367188   Top5 98.660714   BatchTime 0.381160   LR 0.000562
0.79776162
0.79786468
0.79801649
0.79795617
0.79763943
0.79755348
0.79751903
0.79735941
0.79736352
0.79726177
0.79725242
0.79709190
0.79702586
0.79703635
0.79710168
0.79699063
0.79691488
INFO - Training [23][  160/  196]   Loss 0.398754   Top1 86.315918   Top5 98.630371   BatchTime 0.378466   LR 0.000545
0.79689646
0.79686391
0.79690945
0.79694706
0.79695588
0.79700041
0.79845047
0.79827052
0.79824072
0.79794079
0.79774058
0.79780859
0.79792809
0.79768622
0.79751611
0.79743594
0.79741198
0.79746282
0.79744089
0.79750407
0.79756314
0.79750699
INFO - Training [23][  180/  196]   Loss 0.398482   Top1 86.336806   Top5 98.563368   BatchTime 0.376218   LR 0.000529
0.79751575
0.79753327
0.79768980
0.79784852
0.79779404
0.79794377
0.79821700
0.79843837
0.79843813
0.79832965
0.79822159
0.79831022
0.79828417
0.79818887
0.79798532
0.79787952
INFO - ==> Top1: 86.386    Top5: 98.576    Loss: 0.397
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80016834
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.338124   Top1 88.828125   Top5 99.648438   BatchTime 0.126439
INFO - Validation [23][   40/   40]   Loss 0.325468   Top1 89.320000   Top5 99.720000   BatchTime 0.091562
INFO - ==> Top1: 89.320    Top5: 99.720    Loss: 0.325
INFO - ==> Sparsity : 0.401
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 89.320   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 89.200   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.720   Top5: 99.580]
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.3301)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0564)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0709)
features.3.conv.0 tensor(0.0252)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0449)
features.4.conv.0 tensor(0.0428)
features.4.conv.3 tensor(0.1134)
features.4.conv.6 tensor(0.0924)
features.5.conv.0 tensor(0.0293)
features.5.conv.3 tensor(0.0799)
features.5.conv.6 tensor(0.0981)
features.6.conv.0 tensor(0.0241)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0514)
features.7.conv.0 tensor(0.0564)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.1205)
features.8.conv.0 tensor(0.0601)
features.8.conv.3 tensor(0.1334)
features.8.conv.6 tensor(0.1098)
features.9.conv.0 tensor(0.0762)
features.9.conv.3 tensor(0.1441)
features.9.conv.6 tensor(0.1325)
features.10.conv.0 tensor(0.0444)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.0681)
features.11.conv.0 tensor(0.5027)
features.11.conv.3 tensor(0.1190)
features.11.conv.6 tensor(0.3766)
features.12.conv.0 tensor(0.2519)
features.12.conv.3 tensor(0.1262)
features.12.conv.6 tensor(0.5203)
features.13.conv.0 tensor(0.0954)
features.13.conv.3 tensor(0.1508)
features.13.conv.6 tensor(0.1252)
features.14.conv.0 tensor(0.9451)
features.14.conv.3 tensor(0.0905)
features.14.conv.6 tensor(0.9469)
features.15.conv.0 tensor(0.9188)
features.15.conv.3 tensor(0.0797)
features.15.conv.6 tensor(0.9600)
features.16.conv.0 tensor(0.0976)
features.16.conv.3 tensor(0.1002)
features.16.conv.6 tensor(0.2494)
conv.0 tensor(0.1761)
tensor(878482.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.80021250
0.80021763
0.80031353
0.80028123
0.80019873
0.80035371
0.80014801
0.80010325
0.80012691
0.80014360
0.80004567
0.80008674
0.79999107
0.79999155
0.79980171
0.79974419
0.79960531
0.79948425
0.79930216
0.79879993
INFO - Training [24][   20/  196]   Loss 0.401203   Top1 86.093750   Top5 98.203125   BatchTime 0.472949   LR 0.000500
0.79873222
0.79888022
0.79876941
0.79860562
0.79859215
0.79845834
0.79836613
0.79828715
0.79834032
0.79823595
0.79838806
0.79833591
0.79823750
0.79827589
0.79827154
0.79830998
0.79827559
0.79822844
0.79824984
0.79836506
INFO - Training [24][   40/  196]   Loss 0.410441   Top1 85.810547   Top5 98.251953   BatchTime 0.428037   LR 0.000484
0.79824120
0.79816085
0.79815614
0.79820269
0.79820913
0.79817253
0.79809356
0.79814279
0.79823929
0.79837614
0.79823834
0.79796410
0.79898423
0.79879719
0.79873019
0.79856068
0.79848343
INFO - Training [24][   60/  196]   Loss 0.402985   Top1 86.113281   Top5 98.391927   BatchTime 0.406560   LR 0.000468
0.79843569
0.79835004
0.79828256
0.79822552
0.79814529
0.79793245
0.79894650
0.79945010
0.79942489
0.79926860
0.79936296
0.79915708
0.79903656
0.79902101
0.79886132
0.79868555
0.79865861
0.79858214
0.79854304
0.79842490
0.79833150
0.79831564
INFO - Training [24][   80/  196]   Loss 0.398649   Top1 86.406250   Top5 98.500977   BatchTime 0.394756   LR 0.000453
0.79827839
0.79823089
0.79805696
0.79806018
0.79799205
0.79797125
0.79802400
0.79791236
0.79773343
0.79763019
0.79752469
0.79733658
0.79725707
0.79717159
0.79707873
0.79718357
0.79726130
0.79714465
0.79706109
0.79705340
0.79696190
0.79686260
0.79669917
INFO - Training [24][  100/  196]   Loss 0.390720   Top1 86.671875   Top5 98.593750   BatchTime 0.387186   LR 0.000437
0.79663140
0.79662037
0.79791504
0.79876614
0.79849285
0.79826367
0.79811883
0.79798621
0.79819715
0.79818523
0.79802340
0.79800677
0.79814214
0.79816157
0.79808134
0.79808301
0.79967028
0.79913288
0.79914862
INFO - Training [24][  120/  196]   Loss 0.382020   Top1 86.894531   Top5 98.694661   BatchTime 0.376198   LR 0.000422
0.79894787
0.79866511
0.79864758
0.79860610
0.79868734
0.79859883
0.79840058
0.79824078
0.79800659
0.79784697
0.79772925
0.79756963
0.79745680
0.79746723
0.79755646
0.79768664
0.79762787
0.79768044
INFO - Training [24][  140/  196]   Loss 0.380605   Top1 86.983817   Top5 98.722098   BatchTime 0.370024   LR 0.000407
0.79767859
0.79760903
0.79761720
0.79768711
0.79770583
0.79795647
0.79815346
0.79848719
0.79845363
0.79846191
0.79857111
0.79840618
0.79838127
0.79842389
0.79826158
0.79835093
0.79841441
0.79823542
INFO - Training [24][  160/  196]   Loss 0.382767   Top1 86.904297   Top5 98.696289   BatchTime 0.365785   LR 0.000392
0.79829496
0.79836309
0.79845989
0.79865444
0.79849625
0.79855347
0.79851681
0.79842210
0.79825640
0.79826200
0.79819489
0.79801887
0.79792631
0.79790944
0.79789114
0.79774261
0.79755598
0.79753393
0.79734558
0.79713947
INFO - Training [24][  180/  196]   Loss 0.384076   Top1 86.870660   Top5 98.611111   BatchTime 0.368933   LR 0.000378
0.79689282
0.79671526
0.79665864
0.79645574
0.79648310
0.79627168
0.79610240
0.79596853
0.79589009
0.79566628
0.79528648
0.79508626
0.79514468
0.79511315
0.79505110
0.79499191
0.79484969
0.79468685
0.79456311
********************pre-trained*****************
INFO - ==> Top1: 86.878    Top5: 98.618    Loss: 0.385
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 0.313710   Top1 89.667969   Top5 99.648438   BatchTime 0.129665
INFO - Validation [24][   40/   40]   Loss 0.302890   Top1 90.050000   Top5 99.690000   BatchTime 0.090979
features.0.conv.0 tensor(0.5556)
features.0.conv.3 tensor(0.3320)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0586)
features.2.conv.0 tensor(0.0165)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0697)
features.3.conv.0 tensor(0.0237)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0443)
features.4.conv.0 tensor(0.0384)
features.4.conv.3 tensor(0.1100)
features.4.conv.6 tensor(0.0934)
features.5.conv.0 tensor(0.0304)
features.5.conv.3 tensor(0.0793)
features.5.conv.6 tensor(0.0996)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0507)
features.7.conv.0 tensor(0.0567)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.1194)
features.8.conv.0 tensor(0.0568)
features.8.conv.3 tensor(0.1343)
features.8.conv.6 tensor(0.1086)
features.9.conv.0 tensor(0.0779)
features.9.conv.3 tensor(0.1447)
features.9.conv.6 tensor(0.1414)
features.10.conv.0 tensor(0.0395)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0752)
features.11.conv.0 tensor(0.5237)
features.11.conv.3 tensor(0.1202)
features.11.conv.6 tensor(0.4091)
features.12.conv.0 tensor(0.2522)
features.12.conv.3 tensor(0.1260)
features.12.conv.6 tensor(0.5348)
features.13.conv.0 tensor(0.1041)
features.13.conv.3 tensor(0.1507)
features.13.conv.6 tensor(0.1016)
features.14.conv.0 tensor(0.9418)
features.14.conv.3 tensor(0.0910)
features.14.conv.6 tensor(0.9309)
features.15.conv.0 tensor(0.9196)
features.15.conv.3 tensor(0.0803)
features.15.conv.6 tensor(0.9544)
features.16.conv.0 tensor(0.1058)
features.16.conv.3 tensor(0.1006)
features.16.conv.6 tensor(0.2506)
conv.0 tensor(0.1708)
tensor(876651.) 2188896.0
INFO - ==> Top1: 90.050    Top5: 99.690    Loss: 0.303
INFO - ==> Sparsity : 0.400
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 89.320   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 89.200   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.79451734
0.79452324
0.79450268
0.79441351
0.79431760
0.79395831
0.79378456
0.79354119
0.79334849
0.79308641
0.79287004
0.79274315
0.79261178
0.79240137
0.79220164
0.79220921
0.79219389
0.79192275
0.79168957
0.79150146
0.79144567
INFO - Training [25][   20/  196]   Loss 0.408135   Top1 85.781250   Top5 98.144531   BatchTime 0.423155   LR 0.000353
0.79135019
0.79135770
0.79146868
0.79155016
0.79140562
0.79139608
0.79136902
0.79142517
0.79135907
0.79136491
0.79217184
0.79313564
0.79310459
0.79294103
0.79285926
0.79284364
INFO - Training [25][   40/  196]   Loss 0.402913   Top1 86.250000   Top5 98.222656   BatchTime 0.397294   LR 0.000339
0.79280812
0.79283082
0.79269350
0.79267979
0.79257065
0.79235482
0.79216754
0.79189277
0.79177403
0.79152119
0.79127282
0.79108149
0.79086024
0.79065186
0.79044050
0.79003298
0.78976178
0.78970039
0.78976148
0.78973407
0.78960955
0.78954864
INFO - Training [25][   60/  196]   Loss 0.394685   Top1 86.490885   Top5 98.365885   BatchTime 0.386115   LR 0.000325
0.78952491
0.78955710
0.78955907
0.78953946
0.78955841
0.78958762
0.78963178
0.78972179
0.78974879
0.78976399
0.78979683
0.78977174
0.78969449
0.78960258
0.78952622
0.78942865
0.78933048
0.78923506
0.78914917
0.78910673
0.78910029
0.78939784
INFO - Training [25][   80/  196]   Loss 0.392868   Top1 86.459961   Top5 98.413086   BatchTime 0.380617   LR 0.000312
0.79139000
0.79117769
0.79090136
0.79088479
0.79080355
0.79074860
0.79067725
0.79069293
0.79074192
0.79085124
0.79061943
0.79071158
0.79067385
0.79058319
0.79049963
0.79028940
INFO - Training [25][  100/  196]   Loss 0.389861   Top1 86.503906   Top5 98.460938   BatchTime 0.377071   LR 0.000299
0.79007894
0.78999484
0.78995299
0.78982890
0.78967381
0.78969544
0.78964943
0.78944647
0.78947765
0.78933042
0.78921133
0.78915781
0.78913945
0.78911173
0.78901750
0.78902817
0.78897846
0.78886753
0.78870648
0.78855109
0.78842086
0.78829092
0.78825182
INFO - Training [25][  120/  196]   Loss 0.382625   Top1 86.813151   Top5 98.531901   BatchTime 0.372387   LR 0.000286
0.78825122
0.78821713
0.78821200
0.78816730
0.78808075
0.78802347
0.78789979
0.78773272
0.78745133
0.78728932
0.78724533
0.78713882
0.78708130
0.78702980
0.78698641
0.78696722
0.78694332
0.78687054
0.78683180
0.78680950
INFO - Training [25][  140/  196]   Loss 0.380297   Top1 86.958705   Top5 98.643973   BatchTime 0.366085   LR 0.000273
0.78679931
0.78693444
0.78706741
0.78720695
0.78745246
0.78761965
0.78790635
0.78798062
0.78802615
0.78977805
0.78994346
0.78993905
0.78991443
0.78984439
0.78986198
0.78985953
0.78988779
0.79014516
0.79229516
0.79233843
INFO - Training [25][  160/  196]   Loss 0.382739   Top1 86.850586   Top5 98.620605   BatchTime 0.366878   LR 0.000261
0.79198307
0.79179883
0.79170436
0.79164505
0.79155058
0.79133487
0.79107475
0.79074794
0.79065806
0.79064524
0.79062092
0.79052800
0.79044151
0.79029542
0.79026276
0.79011494
0.78985506
0.78951150
0.78929543
0.78911018
0.78890800
INFO - Training [25][  180/  196]   Loss 0.381504   Top1 86.896701   Top5 98.565538   BatchTime 0.368503   LR 0.000248
0.78877121
0.78864402
0.78850484
0.78840315
0.78829342
0.78818947
0.78804636
0.78791660
0.78782558
0.78773510
0.78768158
0.78752542
0.78748143
0.78747714
0.78742474
INFO - ==> Top1: 86.968    Top5: 98.594    Loss: 0.379
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [25][   20/   40]   Loss 0.304888   Top1 89.824219   Top5 99.707031   BatchTime 0.121792
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.3457)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0569)
features.2.conv.0 tensor(0.0220)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0694)
features.3.conv.0 tensor(0.0258)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0438)
features.4.conv.0 tensor(0.0386)
features.4.conv.3 tensor(0.1088)
features.4.conv.6 tensor(0.0907)
features.5.conv.0 tensor(0.0283)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1019)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0509)
features.7.conv.0 tensor(0.0558)
features.7.conv.3 tensor(0.1102)
INFO - Validation [25][   40/   40]   Loss 0.289028   Top1 90.260000   Top5 99.770000   BatchTime 0.086382
INFO - ==> Top1: 90.260    Top5: 99.770    Loss: 0.289
INFO - ==> Sparsity : 0.406
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 89.320   Top5: 99.720]
features.7.conv.6 tensor(0.1183)
features.8.conv.0 tensor(0.0607)
features.8.conv.3 tensor(0.1325)
features.8.conv.6 tensor(0.1126)
features.9.conv.0 tensor(0.0803)
features.9.conv.3 tensor(0.1450)
features.9.conv.6 tensor(0.1979)
features.10.conv.0 tensor(0.0399)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0730)
features.11.conv.0 tensor(0.5416)
features.11.conv.3 tensor(0.1186)
features.11.conv.6 tensor(0.4172)
features.12.conv.0 tensor(0.3202)
features.12.conv.3 tensor(0.1260)
features.12.conv.6 tensor(0.5435)
features.13.conv.0 tensor(0.1053)
features.13.conv.3 tensor(0.1499)
features.13.conv.6 tensor(0.1047)
features.14.conv.0 tensor(0.9444)
features.14.conv.3 tensor(0.0894)
features.14.conv.6 tensor(0.9452)
features.15.conv.0 tensor(0.9204)
features.15.conv.3 tensor(0.0802)
features.15.conv.6 tensor(0.9647)
features.16.conv.0 tensor(0.1028)
features.16.conv.3 tensor(0.1012)
features.16.conv.6 tensor(0.2515)
conv.0 tensor(0.1745)
tensor(889782.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
0.78737783
0.78742105
0.78743821
0.78741795
0.78735578
0.78736222
0.78722936
0.78717715
0.78714907
0.78710222
0.78708267
0.78706092
0.78708619
0.78711742
0.78697866
0.78696901
0.78691047
0.78684431
0.78683662
0.78678304
INFO - Training [26][   20/  196]   Loss 0.421328   Top1 85.175781   Top5 97.968750   BatchTime 0.467757   LR 0.000228
0.78682476
0.78689927
0.78691977
0.78701103
0.78701854
0.78705239
0.78679287
0.78680509
0.78677410
0.78673607
0.78676683
0.78679132
0.78674138
0.78686565
0.78699195
0.78697890
0.78701538
INFO - Training [26][   40/  196]   Loss 0.406565   Top1 85.664062   Top5 98.144531   BatchTime 0.420587   LR 0.000216
0.78714114
0.78764111
0.78881848
0.79069912
0.79061919
0.79046661
0.79032862
0.79015136
0.79006851
0.79002124
0.78992987
0.78979146
0.78964865
0.78958952
0.78942454
0.78927863
0.78905857
0.78889507
0.78882557
0.78878647
0.78880924
INFO - Training [26][   60/  196]   Loss 0.399281   Top1 86.087240   Top5 98.229167   BatchTime 0.403587   LR 0.000205
0.78868115
0.78871685
0.78871399
0.78879255
0.78891182
0.78883296
0.78882325
0.78894591
0.78908074
0.78914803
0.78924179
0.78926021
0.78919703
0.78911471
0.78909409
0.78909838
0.78902072
0.78878862
0.78839630
0.78814137
0.78808379
0.78814095
INFO - Training [26][   80/  196]   Loss 0.396187   Top1 86.191406   Top5 98.393555   BatchTime 0.394645   LR 0.000194
0.78814226
0.78826106
0.78848696
0.78859866
0.78853071
0.78844064
0.78838366
0.78836739
0.78835678
0.78834510
0.78839642
0.78828293
0.78817135
0.78807884
0.78801423
0.78790748
0.78784293
0.78782094
INFO - Training [26][  100/  196]   Loss 0.388397   Top1 86.433594   Top5 98.480469   BatchTime 0.381880   LR 0.000183
0.78780138
0.78769249
0.78776026
0.78775966
0.78761345
0.78748184
0.78727561
0.78706354
0.78684098
0.78642195
0.78609091
0.78597540
0.78587896
0.78581703
0.78562999
0.78552550
0.78543180
0.78532177
0.78527808
0.78525007
0.78516442
0.78514159
0.78510761
0.78495967
INFO - Training [26][  120/  196]   Loss 0.379053   Top1 86.835938   Top5 98.593750   BatchTime 0.374292   LR 0.000173
0.78489572
0.78485525
0.78483635
0.78471905
0.78469813
0.78470129
0.78469682
0.78471202
0.78470397
0.78474945
0.78482968
0.78479868
0.78479010
0.78473634
0.78470927
0.78463894
INFO - Training [26][  140/  196]   Loss 0.377647   Top1 86.905692   Top5 98.674665   BatchTime 0.374052   LR 0.000163
0.78458583
0.78451526
0.78445750
0.78441495
0.78425944
0.78417671
0.78416091
0.78416950
0.78414500
0.78408456
0.78410870
0.78409654
0.78404301
0.78398097
0.78389412
0.78380388
0.78367996
0.78359497
0.78350186
0.78347975
0.78342724
0.78338635
INFO - Training [26][  160/  196]   Loss 0.377500   Top1 86.857910   Top5 98.684082   BatchTime 0.373628   LR 0.000153
0.78334236
0.78322142
0.78316498
0.78313565
0.78309715
0.78307962
0.78299671
0.78295326
0.78295350
0.78294307
0.78281939
0.78275198
0.78265649
0.78261811
0.78249300
0.78236938
0.78228283
0.78223783
0.78209978
0.78197867
0.78179759
INFO - Training [26][  180/  196]   Loss 0.378077   Top1 86.879340   Top5 98.634983   BatchTime 0.372683   LR 0.000144
0.78167951
0.78157008
0.78146929
0.78139496
0.78129274
0.78121448
0.78091073
0.78077543
0.78069919
0.78063887
0.78061843
0.78059042
0.78056180
0.78049523
0.78042388
INFO - ==> Top1: 86.992    Top5: 98.646    Loss: 0.374
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.311913   Top1 89.882812   Top5 99.667969   BatchTime 0.119835
INFO - Validation [26][   40/   40]   Loss 0.300653   Top1 90.010000   Top5 99.700000   BatchTime 0.085701
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.3457)
features.1.conv.0 tensor(0.0293)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0569)
features.2.conv.0 tensor(0.0226)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0686)
features.3.conv.0 tensor(0.0252)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0447)
features.4.conv.0 tensor(0.0347)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.1252)
features.5.conv.0 tensor(0.0269)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.1063)
features.6.conv.0 tensor(0.0221)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0504)
features.7.conv.0 tensor(0.0554)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.1223)
features.8.conv.0 tensor(0.0590)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.1209)
features.9.conv.0 tensor(0.0804)
features.9.conv.3 tensor(0.1450)
features.9.conv.6 tensor(0.1588)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0741)
features.11.conv.0 tensor(0.5667)
features.11.conv.3 tensor(0.1177)
features.11.conv.6 tensor(0.4500)
features.12.conv.0 tensor(0.3810)
features.12.conv.3 tensor(0.1256)
features.12.conv.6 tensor(0.5326)
features.13.conv.0 tensor(0.1043)
features.13.conv.3 tensor(0.1495)
features.13.conv.6 tensor(0.1636)
features.14.conv.0 tensor(0.9463)
features.14.conv.3 tensor(0.0876)
features.14.conv.6 tensor(0.9466)
features.15.conv.0 tensor(0.9210)
features.15.conv.3 tensor(0.0795)
features.15.conv.6 tensor(0.9649)
features.16.conv.0 tensor(0.1055)
features.16.conv.3 tensor(0.0995)
features.16.conv.6 tensor(0.2514)
conv.0 tensor(0.1853)
tensor(906017.) 2188896.0
INFO - ==> Top1: 90.010    Top5: 99.700    Loss: 0.301
INFO - ==> Sparsity : 0.414
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.010   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
0.78039891
0.78041768
0.78040761
0.78034234
0.78026539
0.78022569
0.78017586
0.78014171
0.78014904
0.78014004
0.78014529
0.78016371
0.78016126
0.78015995
0.78016484
0.78012848
0.78011757
0.78009039
0.78006667
0.78002059
0.77997416
0.77996391
INFO - Training [27][   20/  196]   Loss 0.385675   Top1 86.132812   Top5 98.007812   BatchTime 0.490513   LR 0.000128
0.77996373
0.77994090
0.77987117
0.77983499
0.77975845
0.77970177
0.77963841
0.77961504
0.77956861
0.77950412
0.77949524
0.77946532
0.77939487
0.77933919
0.77928209
0.77921110
0.77911866
0.77907044
0.77901959
0.77897149
INFO - Training [27][   40/  196]   Loss 0.394264   Top1 86.132812   Top5 98.232422   BatchTime 0.443121   LR 0.000119
0.77896416
0.77891064
0.77888018
0.77888906
0.77890474
0.77896190
0.77901602
0.77910334
0.77915257
0.77919501
0.77922451
0.77927721
0.77930820
0.77931458
0.77930796
0.77931261
0.77931404
0.77930570
0.77933300
INFO - Training [27][   60/  196]   Loss 0.386860   Top1 86.627604   Top5 98.294271   BatchTime 0.431733   LR 0.000111
0.77935678
0.77935511
0.77933633
0.77933466
0.77931070
0.77928406
0.77927935
0.77927387
0.77923661
0.77916765
0.77912617
0.77908677
0.77904177
0.77901602
0.77898872
0.77896374
INFO - Training [27][   80/  196]   Loss 0.381161   Top1 86.811523   Top5 98.457031   BatchTime 0.420024   LR 0.000102
0.77894741
0.77888918
0.77884936
0.77877754
0.77871364
0.77865589
0.77862483
0.77855986
0.77846432
0.77839226
0.77829945
0.77821511
0.77813584
0.77805847
0.77800089
0.77792364
0.77783722
0.77775055
0.77765548
0.77758974
0.77750826
0.77740759
0.77729690
INFO - Training [27][  100/  196]   Loss 0.378157   Top1 86.933594   Top5 98.515625   BatchTime 0.403314   LR 0.000095
0.77723932
0.77722025
0.77719563
0.77714527
0.77707708
0.77702105
0.77695501
0.77690274
0.77683640
0.77680665
0.77674150
0.77668762
0.77661407
0.77655435
0.77650499
0.77640826
0.77631849
INFO - Training [27][  120/  196]   Loss 0.370409   Top1 87.164714   Top5 98.606771   BatchTime 0.396454   LR 0.000087
0.77626479
0.77624309
0.77622050
0.77618372
0.77614343
0.77612507
0.77610463
0.77610320
0.77607173
0.77601546
0.77595001
0.77589083
0.77584368
0.77578932
0.77573800
0.77572507
0.77569997
0.77567416
0.77565604
0.77562636
0.77557027
0.77552968
INFO - Training [27][  140/  196]   Loss 0.367715   Top1 87.148438   Top5 98.691406   BatchTime 0.391793   LR 0.000080
0.77546960
0.77542150
0.77538645
0.77535278
0.77533478
0.77530122
0.77526194
0.77521908
0.77515864
0.77508181
0.77501816
0.77498311
0.77493453
0.77488416
0.77483553
0.77475917
0.77462578
0.77451009
0.77437514
0.77419937
0.77412814
0.77408528
INFO - Training [27][  160/  196]   Loss 0.371430   Top1 87.075195   Top5 98.664551   BatchTime 0.388080   LR 0.000073
0.77403170
0.77394527
0.77386558
0.77379519
0.77373552
0.77371103
0.77365649
0.77357036
0.77353644
0.77344769
0.77341235
0.77334249
0.77330768
0.77332419
0.77332073
0.77330625
0.77323520
0.77315164
0.77304769
0.77298599
0.77293855
INFO - Training [27][  180/  196]   Loss 0.371349   Top1 87.109375   Top5 98.613281   BatchTime 0.387723   LR 0.000066
0.77296549
0.77301031
0.77303278
0.77307892
0.77313101
0.77313370
0.77313435
0.77309102
0.77300149
0.77285558
0.77272493
0.77263206
0.77252251
0.77240080
********************pre-trained*****************
INFO - ==> Top1: 87.256    Top5: 98.616    Loss: 0.368
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 0.309267   Top1 89.804688   Top5 99.687500   BatchTime 0.126644
INFO - Validation [27][   40/   40]   Loss 0.297128   Top1 90.020000   Top5 99.770000   BatchTime 0.088564
INFO - ==> Top1: 90.020    Top5: 99.770    Loss: 0.297
INFO - ==> Sparsity : 0.437
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3555)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0586)
features.2.conv.0 tensor(0.0229)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0674)
features.3.conv.0 tensor(0.0249)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0445)
features.4.conv.0 tensor(0.0340)
features.4.conv.3 tensor(0.1071)
features.4.conv.6 tensor(0.1178)
features.5.conv.0 tensor(0.0267)
features.5.conv.3 tensor(0.0793)
features.5.conv.6 tensor(0.1120)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0501)
features.7.conv.0 tensor(0.0563)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.1237)
features.8.conv.0 tensor(0.0585)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.1232)
features.9.conv.0 tensor(0.0803)
features.9.conv.3 tensor(0.1450)
features.9.conv.6 tensor(0.1712)
features.10.conv.0 tensor(0.0408)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0792)
features.11.conv.0 tensor(0.5800)
features.11.conv.3 tensor(0.1173)
features.11.conv.6 tensor(0.4654)
features.12.conv.0 tensor(0.3734)
features.12.conv.3 tensor(0.1256)
features.12.conv.6 tensor(0.5244)
features.13.conv.0 tensor(0.1063)
features.13.conv.3 tensor(0.1495)
features.13.conv.6 tensor(0.1541)
features.14.conv.0 tensor(0.9473)
features.14.conv.3 tensor(0.0880)
features.14.conv.6 tensor(0.9448)
features.15.conv.0 tensor(0.9213)
features.15.conv.3 tensor(0.0782)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.1085)
features.16.conv.3 tensor(0.1000)
features.16.conv.6 tensor(0.3979)
conv.0 tensor(0.1948)
tensor(955906.) 2188896.0
0.77251941
0.77262551
0.77271879
0.77289736
0.77306426
0.77323407
0.77333695
0.77340245
0.77343673
0.77346539
0.77346218
0.77345097
0.77344650
0.77346557
0.77352065
0.77360332
0.77364135
0.77365440
0.77368706
0.77370441
0.77366674
INFO - Training [28][   20/  196]   Loss 0.384133   Top1 86.093750   Top5 98.066406   BatchTime 0.443196   LR 0.000055
0.77363074
0.77358043
0.77353925
0.77349639
0.77346665
0.77341807
0.77337420
0.77335215
0.77332437
0.77336061
0.77340609
0.77346671
0.77352214
0.77355486
0.77354676
0.77354497
0.77351242
INFO - Training [28][   40/  196]   Loss 0.384726   Top1 86.123047   Top5 98.349609   BatchTime 0.400341   LR 0.000050
0.77346355
0.77343136
0.77339399
0.77337176
0.77336931
0.77339834
0.77338094
0.77336556
0.77337164
0.77335614
0.77332747
0.77332115
0.77331680
0.77331388
0.77329838
0.77332956
0.77334774
0.77335888
0.77337807
0.77337337
0.77339286
0.77344054
INFO - Training [28][   60/  196]   Loss 0.373034   Top1 86.946615   Top5 98.463542   BatchTime 0.389802   LR 0.000044
0.77347249
0.77347779
0.77348739
0.77350599
0.77352864
0.77352345
0.77353585
0.77354968
0.77353972
0.77350754
0.77346402
0.77342772
0.77338970
0.77336800
0.77334386
0.77331841
0.77328974
0.77329320
INFO - Training [28][   80/  196]   Loss 0.369822   Top1 87.099609   Top5 98.535156   BatchTime 0.377129   LR 0.000039
0.77330965
0.77331901
0.77332073
0.77333051
0.77333754
0.77333182
0.77331799
0.77331346
0.77330828
0.77328402
0.77329201
0.77331084
0.77330279
0.77329022
0.77326423
0.77321482
0.77318841
0.77316910
INFO - Training [28][  100/  196]   Loss 0.364378   Top1 87.246094   Top5 98.585938   BatchTime 0.366653   LR 0.000034
0.77314073
0.77309573
0.77307045
0.77304769
0.77304417
0.77302355
0.77301937
0.77303785
0.77306753
0.77312738
0.77315980
0.77318728
0.77320826
0.77325380
0.77328676
0.77331012
0.77330106
0.77330714
0.77329350
0.77328169
0.77326018
0.77326989
0.77327174
INFO - Training [28][  120/  196]   Loss 0.359158   Top1 87.389323   Top5 98.649089   BatchTime 0.365003   LR 0.000030
0.77326804
0.77325290
0.77325606
0.77327031
0.77327067
0.77325743
0.77324945
0.77321768
0.77319425
0.77317631
0.77317113
0.77317125
0.77315944
0.77315283
0.77314740
0.77314335
0.77313495
0.77313197
0.77314121
0.77312517
0.77312726
0.77313304
INFO - Training [28][  140/  196]   Loss 0.358665   Top1 87.477679   Top5 98.705357   BatchTime 0.364351   LR 0.000026
0.77312756
0.77311563
0.77311492
0.77309799
0.77308518
0.77306551
0.77304333
0.77303976
0.77303571
0.77304894
0.77304250
0.77302414
0.77301848
0.77301103
0.77298570
0.77297705
0.77297437
0.77297676
0.77296835
0.77294445
0.77293897
INFO - Training [28][  160/  196]   Loss 0.364284   Top1 87.290039   Top5 98.723145   BatchTime 0.367510   LR 0.000022
0.77293903
0.77292567
0.77292132
0.77290791
0.77286792
0.77284461
0.77282274
0.77280462
0.77277154
0.77275431
0.77274352
0.77273417
0.77270323
0.77268726
0.77266568
0.77264518
INFO - Training [28][  180/  196]   Loss 0.363907   Top1 87.341580   Top5 98.674045   BatchTime 0.366853   LR 0.000018
0.77263457
0.77263629
0.77261972
0.77261722
0.77258080
0.77255005
0.77254289
0.77254349
0.77253759
0.77252746
0.77250248
0.77248979
0.77250987
0.77250201
0.77251154
0.77251941
0.77254230
INFO - ==> Top1: 87.388    Top5: 98.676    Loss: 0.363
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77254641
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.307897   Top1 89.746094   Top5 99.726562   BatchTime 0.133073
INFO - Validation [28][   40/   40]   Loss 0.306066   Top1 89.660000   Top5 99.760000   BatchTime 0.098287
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3555)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0573)
features.2.conv.0 tensor(0.0223)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0677)
features.3.conv.0 tensor(0.0258)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0434)
features.4.conv.0 tensor(0.0348)
features.4.conv.3 tensor(0.1082)
features.4.conv.6 tensor(0.1084)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0793)
features.5.conv.6 tensor(0.1146)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0347)
features.6.conv.6 tensor(0.0504)
features.7.conv.0 tensor(0.0567)
features.7.conv.3 tensor(0.1065)
features.7.conv.6 tensor(0.1257)
features.8.conv.0 tensor(0.0587)
features.8.conv.3 tensor(0.1299)
features.8.conv.6 tensor(0.1246)
features.9.conv.0 tensor(0.0803)
features.9.conv.3 tensor(0.1450)
features.9.conv.6 tensor(0.1799)
features.10.conv.0 tensor(0.0411)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0837)
features.11.conv.0 tensor(0.5804)
features.11.conv.3 tensor(0.1181)
features.11.conv.6 tensor(0.4637)
features.12.conv.0 tensor(0.3867)
features.12.conv.3 tensor(0.1248)
features.12.conv.6 tensor(0.5297)
features.13.conv.0 tensor(0.1071)
features.13.conv.3 tensor(0.1470)
features.13.conv.6 tensor(0.1685)
features.14.conv.0 tensor(0.9478)
features.14.conv.3 tensor(0.0882)
features.14.conv.6 tensor(0.9455)
features.15.conv.0 tensor(0.9213)
features.15.conv.3 tensor(0.0784)
features.15.conv.6 tensor(0.9672)
features.16.conv.0 tensor(0.1066)
features.16.conv.3 tensor(0.0990)
features.16.conv.6 tensor(0.3489)
conv.0 tensor(0.2059)
tensor(948306.) 2188896.0
INFO - ==> Top1: 89.660    Top5: 99.760    Loss: 0.306
INFO - ==> Sparsity : 0.433
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
0.77254206
0.77254522
0.77253574
0.77254206
0.77254063
0.77254450
0.77254796
0.77255851
0.77256441
0.77256143
0.77258074
0.77259284
0.77260786
0.77260959
0.77261883
0.77263451
0.77265990
0.77267540
INFO - Training [29][   20/  196]   Loss 0.373492   Top1 86.718750   Top5 98.222656   BatchTime 0.456080   LR 0.000013
0.77267587
0.77267838
0.77268142
0.77268368
0.77269047
0.77269012
0.77268171
0.77268010
0.77268386
0.77269888
0.77269238
0.77268434
0.77267826
0.77269137
0.77267045
0.77267939
0.77267736
0.77268177
0.77267259
0.77266848
0.77267814
INFO - Training [29][   40/  196]   Loss 0.381787   Top1 86.699219   Top5 98.457031   BatchTime 0.409493   LR 0.000010
0.77267057
0.77266884
0.77268374
0.77268332
0.77267474
0.77267885
0.77266872
0.77266651
0.77266186
0.77265340
0.77265024
0.77265793
0.77266234
0.77265602
0.77263999
0.77262712
0.77261555
0.77260637
0.77260870
0.77261180
0.77261376
INFO - Training [29][   60/  196]   Loss 0.372230   Top1 87.005208   Top5 98.567708   BatchTime 0.403873   LR 0.000008
0.77259785
0.77258557
0.77259243
0.77257711
0.77256083
0.77257842
0.77258539
0.77258974
0.77260435
0.77259749
0.77260166
0.77261055
0.77261156
0.77261496
0.77262384
0.77262127
0.77262115
0.77262694
INFO - Training [29][   80/  196]   Loss 0.364924   Top1 87.275391   Top5 98.662109   BatchTime 0.385330   LR 0.000005
0.77262837
0.77263409
0.77263445
0.77263910
0.77264673
0.77264076
0.77264178
0.77263850
0.77264172
0.77263945
0.77265233
0.77265614
0.77266258
0.77265459
0.77264744
0.77263623
0.77265704
0.77266085
0.77266049
INFO - Training [29][  100/  196]   Loss 0.359243   Top1 87.460938   Top5 98.707031   BatchTime 0.373320   LR 0.000004
0.77267134
0.77266783
0.77267599
0.77267122
0.77266860
0.77268177
0.77267623
0.77267170
0.77266657
0.77266705
0.77266651
0.77265781
0.77265871
0.77265894
0.77266842
0.77266759
0.77265078
0.77265888
0.77265471
0.77265972
0.77265579
0.77265346
0.77264535
INFO - Training [29][  120/  196]   Loss 0.355658   Top1 87.620443   Top5 98.772786   BatchTime 0.369447   LR 0.000002
0.77263921
0.77264458
0.77264869
0.77265406
0.77264172
0.77263772
0.77262533
0.77261847
0.77262455
0.77262992
0.77261990
0.77263296
0.77263153
0.77263486
0.77264190
0.77263933
0.77263963
0.77264786
0.77263206
0.77264023
0.77264476
0.77264500
INFO - Training [29][  140/  196]   Loss 0.355127   Top1 87.589286   Top5 98.811384   BatchTime 0.368840   LR 0.000001
0.77265131
0.77265817
0.77265471
0.77266538
0.77265763
0.77264553
0.77263808
0.77264577
0.77264369
0.77264351
0.77263385
0.77262640
0.77262002
0.77262205
0.77262545
0.77261972
0.77261108
0.77261513
INFO - Training [29][  160/  196]   Loss 0.359397   Top1 87.456055   Top5 98.798828   BatchTime 0.364909   LR 0.000001
0.77261353
0.77261746
0.77262068
0.77262443
0.77261603
0.77260840
0.77261478
0.77261585
0.77262002
0.77261108
0.77261096
0.77259326
0.77259862
0.77260888
0.77259690
0.77258915
0.77260488
INFO - Training [29][  180/  196]   Loss 0.359717   Top1 87.508681   Top5 98.743490   BatchTime 0.362445   LR 0.000000
0.77261013
0.77262270
0.77261722
0.77261841
0.77261966
0.77261984
0.77262300
0.77261317
0.77261430
0.77262580
0.77263755
0.77262831
0.77262723
0.77262270
0.77262068
0.77262479
0.77261835
INFO - ==> Top1: 87.540    Top5: 98.758    Loss: 0.358
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77261692
0.77261901
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 0.314034   Top1 89.707031   Top5 99.648438   BatchTime 0.118387
INFO - Validation [29][   40/   40]   Loss 0.303550   Top1 89.890000   Top5 99.730000   BatchTime 0.083859
INFO - ==> Top1: 89.890    Top5: 99.730    Loss: 0.304
INFO - ==> Sparsity : 0.433
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3555)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0569)
features.2.conv.0 tensor(0.0223)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0677)
features.3.conv.0 tensor(0.0258)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0436)
features.4.conv.0 tensor(0.0348)
features.4.conv.3 tensor(0.1065)
features.4.conv.6 tensor(0.1084)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0804)
features.5.conv.6 tensor(0.1144)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0502)
features.7.conv.0 tensor(0.0566)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.1257)
features.8.conv.0 tensor(0.0590)
features.8.conv.3 tensor(0.1302)
features.8.conv.6 tensor(0.1243)
features.9.conv.0 tensor(0.0804)
features.9.conv.3 tensor(0.1455)
features.9.conv.6 tensor(0.1815)
features.10.conv.0 tensor(0.0411)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0835)
features.11.conv.0 tensor(0.5816)
features.11.conv.3 tensor(0.1177)
features.11.conv.6 tensor(0.4638)
features.12.conv.0 tensor(0.3835)
features.12.conv.3 tensor(0.1248)
features.12.conv.6 tensor(0.5298)
features.13.conv.0 tensor(0.1070)
features.13.conv.3 tensor(0.1483)
features.13.conv.6 tensor(0.1723)
features.14.conv.0 tensor(0.9479)
features.14.conv.3 tensor(0.0883)
features.14.conv.6 tensor(0.9458)
features.15.conv.0 tensor(0.9212)
features.15.conv.3 tensor(0.0784)
features.15.conv.6 tensor(0.9673)
features.16.conv.0 tensor(0.1068)
features.16.conv.3 tensor(0.0997)
features.16.conv.6 tensor(0.3429)
conv.0 tensor(0.2057)
tensor(946766.) 2188896.0
0.77261543
0.77220458
0.77160412
0.77344358
0.77453107
0.77661777
0.77774686
0.77810889
0.77822191
0.77775723
0.77724016
0.77669793
0.77646130
0.77785319
0.77671695
0.77725178
0.77820015
INFO - Training [30][   20/  196]   Loss 0.420817   Top1 85.605469   Top5 98.281250   BatchTime 0.436908   LR 0.001250
0.77944779
0.78096539
0.78196633
0.78259915
0.78317279
0.78370422
0.78447860
0.78513300
0.78575909
0.78964478
0.78974247
0.78951049
0.78950399
0.78970033
0.78989720
0.79010850
0.79020143
0.79075098
0.79252839
0.79260117
INFO - Training [30][   40/  196]   Loss 0.420412   Top1 85.478516   Top5 98.310547   BatchTime 0.418046   LR 0.001250
0.79259706
0.79263306
0.79266721
0.79272616
0.79287195
0.79306304
0.79310286
0.79308784
0.79507637
0.79520184
0.79530001
0.79549778
0.79544312
0.79531759
0.79540813
0.79518676
0.79510933
0.79509205
0.79497296
0.79496562
0.79496533
0.79483253
INFO - Training [30][   60/  196]   Loss 0.419787   Top1 85.677083   Top5 98.372396   BatchTime 0.398321   LR 0.001250
0.79472667
0.79467219
0.79459918
0.79458076
0.79478848
0.79494327
0.79489517
0.79487789
0.79477799
0.79470187
0.79450774
0.79437774
0.79444122
0.79846722
0.79790407
0.79824334
0.79804957
0.79796815
0.79772377
0.79766119
0.79751205
0.79757339
INFO - Training [30][   80/  196]   Loss 0.419258   Top1 85.625000   Top5 98.476562   BatchTime 0.390986   LR 0.001250
0.79765308
0.79764503
0.79778284
0.79787213
0.79805821
0.79822254
0.79844999
0.79828602
0.79834622
0.79825145
0.79793346
0.79834461
0.80351502
0.81007940
0.81036502
0.81057489
0.81065923
0.81052107
INFO - Training [30][  100/  196]   Loss 0.415226   Top1 85.777344   Top5 98.476562   BatchTime 0.378717   LR 0.001250
0.81054986
0.81050879
0.81034875
0.81034869
0.81064743
0.81060940
0.81052613
0.81083935
0.81095541
0.81128746
0.81159931
0.81206006
0.81201535
0.81261456
0.81269926
0.81286389
0.81304818
0.81356245
0.81338030
INFO - Training [30][  120/  196]   Loss 0.410471   Top1 85.914714   Top5 98.554688   BatchTime 0.370538   LR 0.001249
0.81332350
0.81348109
0.81393409
0.81420636
0.81409889
0.81411743
0.81447625
0.81461269
0.81469822
0.81461722
0.81467056
0.81461370
0.81459069
0.81442004
0.81440401
0.81424636
0.81414175
0.81449944
0.81466174
0.81492478
0.81634831
0.81721652
INFO - Training [30][  140/  196]   Loss 0.410056   Top1 85.943080   Top5 98.607701   BatchTime 0.370693   LR 0.001249
0.81733841
0.81719726
0.81724524
0.81728196
0.81710547
0.81706846
0.81730968
0.81732863
0.81713444
0.81705105
0.81707203
0.81710678
0.81716591
0.81695092
0.81656289
0.81654865
0.81634569
0.81603557
0.81587994
0.81574738
0.81566185
0.81537801
INFO - Training [30][  160/  196]   Loss 0.414223   Top1 85.769043   Top5 98.623047   BatchTime 0.368811   LR 0.001249
0.81555188
0.81555367
0.81540668
0.81545246
0.81515908
0.81491315
0.81495804
0.81481111
0.81460154
0.81409544
0.81431556
0.81443489
0.81414682
0.81385237
0.81371993
0.81368130
INFO - Training [30][  180/  196]   Loss 0.416908   Top1 85.705295   Top5 98.608941   BatchTime 0.368907   LR 0.001248
0.81358254
0.81342983
0.81310898
0.81309086
0.81268901
0.81212157
0.81248152
0.81242090
0.81207693
0.81165850
0.81119019
0.81077474
0.81061524
0.81018311
0.80892789
0.80903625
INFO - ==> Top1: 85.758    Top5: 98.596    Loss: 0.416
0.80993801
0.80990434
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.345083   Top1 88.261719   Top5 99.648438   BatchTime 0.124398
INFO - Validation [30][   40/   40]   Loss 0.330353   Top1 88.630000   Top5 99.750000   BatchTime 0.086824
INFO - ==> Top1: 88.630    Top5: 99.750    Loss: 0.330
INFO - ==> Sparsity : 0.393
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5556)
features.0.conv.3 tensor(0.2988)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0556)
features.2.conv.0 tensor(0.0237)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0683)
features.3.conv.0 tensor(0.0246)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0414)
features.4.conv.0 tensor(0.0322)
features.4.conv.3 tensor(0.1111)
features.4.conv.6 tensor(0.1444)
features.5.conv.0 tensor(0.0249)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0446)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0489)
features.7.conv.0 tensor(0.0555)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.0990)
features.8.conv.0 tensor(0.0554)
features.8.conv.3 tensor(0.1264)
features.8.conv.6 tensor(0.1087)
features.9.conv.0 tensor(0.0745)
features.9.conv.3 tensor(0.1447)
features.9.conv.6 tensor(0.1176)
features.10.conv.0 tensor(0.0402)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0675)
features.11.conv.0 tensor(0.5048)
features.11.conv.3 tensor(0.1146)
features.11.conv.6 tensor(0.4410)
features.12.conv.0 tensor(0.1770)
features.12.conv.3 tensor(0.1254)
features.12.conv.6 tensor(0.4997)
features.13.conv.0 tensor(0.0693)
features.13.conv.3 tensor(0.1507)
features.13.conv.6 tensor(0.1197)
features.14.conv.0 tensor(0.9466)
features.14.conv.3 tensor(0.0931)
features.14.conv.6 tensor(0.9186)
features.15.conv.0 tensor(0.9229)
features.15.conv.3 tensor(0.0758)
features.15.conv.6 tensor(0.9518)
features.16.conv.0 tensor(0.0921)
features.16.conv.3 tensor(0.1027)
features.16.conv.6 tensor(0.2223)
conv.0 tensor(0.1776)
tensor(860188.) 2188896.0
0.80981457
0.80981052
0.80956417
0.80922860
0.80898565
0.80849218
0.80761564
0.80694854
0.80584675
0.80447823
0.80342275
0.80208594
0.80149335
0.80040759
0.79965186
0.79911238
0.79787976
0.79494536
0.79185355
INFO - Training [31][   20/  196]   Loss 0.426160   Top1 85.234375   Top5 98.085938   BatchTime 0.465926   LR 0.001248
0.78679627
0.78799665
0.78610903
0.78479290
0.78830910
0.79744738
0.80258840
0.80483681
0.80682653
0.80779839
0.80817515
0.80845845
0.80868816
0.80875546
0.80931336
0.81004238
0.81037396
0.81061357
0.81068265
0.81059194
0.81047696
0.81034505
INFO - Training [31][   40/  196]   Loss 0.430597   Top1 85.273438   Top5 98.222656   BatchTime 0.416118   LR 0.001247
0.81007308
0.81785357
0.81843197
0.81832963
0.81802040
0.81778681
0.81786102
0.81778347
0.81739002
0.81650537
0.81576246
0.81543654
0.81498301
0.81483597
0.81434846
0.81413078
0.81372917
0.81340611
0.81388462
0.81434590
0.81454122
INFO - Training [31][   60/  196]   Loss 0.427908   Top1 85.449219   Top5 98.326823   BatchTime 0.402394   LR 0.001247
0.81471348
0.81480396
0.81528980
0.81570470
0.81600922
0.81620562
0.81628948
0.81638414
0.81636322
0.81636149
0.81626695
0.81641084
0.81649220
0.81658524
0.81654292
0.81651735
0.81658959
0.81674564
0.81705177
0.81745088
INFO - Training [31][   80/  196]   Loss 0.424464   Top1 85.522461   Top5 98.496094   BatchTime 0.401995   LR 0.001246
0.81751263
0.81753653
0.81723338
0.81778365
0.81844985
0.81906217
0.81978083
0.82064635
0.82339627
0.82435513
0.83015120
0.83021814
0.82996750
0.82995099
0.83019304
0.83042592
0.83023238
INFO - Training [31][  100/  196]   Loss 0.418345   Top1 85.644531   Top5 98.535156   BatchTime 0.393955   LR 0.001246
0.83041298
0.83045095
0.83062035
0.83084065
0.83124459
0.83281124
0.83271813
0.83272719
0.83269978
0.83273381
0.83278924
0.83270985
0.83281189
0.83287048
0.83283561
0.83310241
0.83366406
0.83491820
0.83478314
0.83461988
0.83472139
0.83457631
0.83436638
INFO - Training [31][  120/  196]   Loss 0.413312   Top1 85.807292   Top5 98.597005   BatchTime 0.385232   LR 0.001245
0.83421385
0.83422691
0.83318114
0.83022296
0.82513732
0.82428962
0.82012731
0.82169318
0.81945854
0.81857353
0.81822681
0.81934792
0.82007933
0.81922430
0.82016093
0.82067370
INFO - Training [31][  140/  196]   Loss 0.411020   Top1 85.851004   Top5 98.657924   BatchTime 0.383423   LR 0.001244
0.82274592
0.82517797
0.82715839
0.82854348
0.82883197
0.82944053
0.83012635
0.83063543
0.83080280
0.83100885
0.83109272
0.83102745
0.83109242
0.83120596
0.83124006
0.83118325
0.83101940
0.83058280
0.83034408
0.83032769
0.83022594
0.82981139
0.82970273
INFO - Training [31][  160/  196]   Loss 0.414513   Top1 85.764160   Top5 98.645020   BatchTime 0.379550   LR 0.001244
0.82959473
0.82976788
0.82970870
0.82939851
0.82910156
0.82896888
0.82873225
0.82855874
0.82839400
0.82805455
0.82800472
0.82778853
0.82758981
0.82739604
0.82721734
0.82753146
INFO - Training [31][  180/  196]   Loss 0.417128   Top1 85.674913   Top5 98.585069   BatchTime 0.377507   LR 0.001243
0.82754236
0.82744104
0.82766467
0.82769442
0.82766801
0.82777542
0.82768911
0.82761395
0.82753938
0.82746786
0.82741100
0.82741827
0.82745361
0.82749051
0.82760620
0.82762092
0.82767099
INFO - ==> Top1: 85.606    Top5: 98.570    Loss: 0.418
0.82848799
0.82920974
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [31][   20/   40]   Loss 0.359478   Top1 88.339844   Top5 99.433594   BatchTime 0.116669
INFO - Validation [31][   40/   40]   Loss 0.353686   Top1 87.960000   Top5 99.570000   BatchTime 0.081468
INFO - ==> Top1: 87.960    Top5: 99.570    Loss: 0.354
INFO - ==> Sparsity : 0.372
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.3105)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0573)
features.2.conv.0 tensor(0.0220)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0723)
features.3.conv.0 tensor(0.0246)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0401)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.1140)
features.4.conv.6 tensor(0.1230)
features.5.conv.0 tensor(0.0234)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0431)
features.6.conv.0 tensor(0.0164)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0452)
features.7.conv.0 tensor(0.0516)
features.7.conv.3 tensor(0.1108)
features.7.conv.6 tensor(0.0959)
features.8.conv.0 tensor(0.0654)
features.8.conv.3 tensor(0.1360)
features.8.conv.6 tensor(0.1149)
features.9.conv.0 tensor(0.0903)
features.9.conv.3 tensor(0.1470)
features.9.conv.6 tensor(0.1143)
features.10.conv.0 tensor(0.0380)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.0692)
features.11.conv.0 tensor(0.1736)
features.11.conv.3 tensor(0.1161)
features.11.conv.6 tensor(0.2006)
features.12.conv.0 tensor(0.1432)
features.12.conv.3 tensor(0.1256)
features.12.conv.6 tensor(0.5014)
features.13.conv.0 tensor(0.0738)
features.13.conv.3 tensor(0.1485)
features.13.conv.6 tensor(0.0825)
features.14.conv.0 tensor(0.9424)
features.14.conv.3 tensor(0.0932)
features.14.conv.6 tensor(0.9434)
features.15.conv.0 tensor(0.9243)
features.15.conv.3 tensor(0.0785)
features.15.conv.6 tensor(0.9592)
features.16.conv.0 tensor(0.1099)
features.16.conv.3 tensor(0.1038)
features.16.conv.6 tensor(0.2237)
conv.0 tensor(0.1332)
tensor(813520.) 2188896.0
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
0.82939398
0.82938683
0.82933080
0.82925683
0.82911724
0.82907230
0.82903850
0.83097368
0.83078104
0.83064801
0.83042634
0.83013970
0.82974046
0.82930970
0.82862610
0.82790262
0.82700974
0.82651287
0.82534844
INFO - Training [32][   20/  196]   Loss 0.422652   Top1 85.175781   Top5 97.968750   BatchTime 0.467415   LR 0.001242
0.82427472
0.82085884
0.82267267
0.82223511
0.82377881
0.82564574
0.82693338
0.82780254
0.82783604
0.82783979
0.82764030
0.82767540
0.82760954
0.82757807
0.82752264
0.82744914
0.82735443
0.82736796
0.82742006
0.82738769
INFO - Training [32][   40/  196]   Loss 0.421598   Top1 85.380859   Top5 98.125000   BatchTime 0.429793   LR 0.001241
0.82737142
0.82723373
0.82703900
0.82876050
0.82892531
0.82886469
0.82859886
0.82851875
0.82837254
0.82829142
0.82829171
0.82815921
0.82830930
0.82828963
0.82828897
0.82804930
0.82788527
0.82775086
0.82788414
0.82785058
0.82785571
INFO - Training [32][   60/  196]   Loss 0.422120   Top1 85.397135   Top5 98.242188   BatchTime 0.411934   LR 0.001240
0.82782215
0.82762212
0.82752562
0.82756943
0.82722080
0.82693362
0.82665402
0.82635432
0.82633626
0.82628715
0.82606465
0.82587779
0.82555890
0.82496744
0.82450902
0.82417655
0.82377988
INFO - Training [32][   80/  196]   Loss 0.421528   Top1 85.463867   Top5 98.383789   BatchTime 0.399899   LR 0.001239
0.82337719
0.82295501
0.82250470
0.82229799
0.82209736
0.82202095
0.82192332
0.82155120
0.82120651
0.82110989
0.82081276
0.82029253
0.81977260
0.81939918
0.81849939
0.81824583
0.81803280
0.81765479
0.81732684
0.81703550
0.81689864
0.81683785
0.81676799
0.81663758
0.81679595
0.81673521
INFO - Training [32][  100/  196]   Loss 0.414576   Top1 85.687500   Top5 98.437500   BatchTime 0.381275   LR 0.001238
0.81648117
0.81626165
0.81584716
0.81568056
0.81559485
0.81546390
0.81526971
0.81521088
0.81548023
0.81544298
0.81550014
0.81545508
0.81533879
0.81548089
0.81559193
0.81577289
0.81623560
0.81641835
0.81630844
INFO - Training [32][  120/  196]   Loss 0.408157   Top1 85.901693   Top5 98.525391   BatchTime 0.367856   LR 0.001237
0.81625748
0.81608438
0.81598938
0.81561637
0.81544375
0.81552464
0.81558561
0.81577379
0.81728250
0.81755751
0.81760800
0.81754863
0.81727898
0.81757903
0.81734025
0.81688654
0.81666809
INFO - Training [32][  140/  196]   Loss 0.408563   Top1 85.917969   Top5 98.630022   BatchTime 0.367398   LR 0.001236
0.81644535
0.81629777
0.81571144
0.81498587
0.81488365
0.81438380
0.81414318
0.81386203
0.81365281
0.81298792
0.81222254
0.81162125
0.81218129
0.81202835
0.81170160
0.81189597
0.81212616
0.81217414
0.81242782
0.81283545
0.81264186
0.81252521
INFO - Training [32][  160/  196]   Loss 0.413989   Top1 85.727539   Top5 98.593750   BatchTime 0.366733   LR 0.001235
0.81265110
0.81278926
0.81283122
0.81267589
0.81261992
0.81272215
0.81245935
0.81190497
0.81165642
0.81168485
0.81152076
0.81088144
0.80981368
0.80902576
0.80868083
0.80822384
INFO - Training [32][  180/  196]   Loss 0.414668   Top1 85.733507   Top5 98.537326   BatchTime 0.366837   LR 0.001234
0.80814922
0.80820906
0.80883223
0.80992568
0.80918390
0.80802286
0.80695635
0.80653322
0.80663556
0.80582321
0.80438548
0.80335158
0.80231655
0.80180585
0.80069822
0.80169064
0.80342418
INFO - ==> Top1: 85.744    Top5: 98.542    Loss: 0.414
0.80482215
0.80561906
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.332951   Top1 89.492188   Top5 99.648438   BatchTime 0.120714
INFO - Validation [32][   40/   40]   Loss 0.315379   Top1 89.540000   Top5 99.720000   BatchTime 0.086245
INFO - ==> Top1: 89.540    Top5: 99.720    Loss: 0.315
INFO - ==> Sparsity : 0.377
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.3457)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0573)
features.2.conv.0 tensor(0.0234)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0723)
features.3.conv.0 tensor(0.0249)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0371)
features.4.conv.0 tensor(0.0267)
features.4.conv.3 tensor(0.1076)
features.4.conv.6 tensor(0.0882)
features.5.conv.0 tensor(0.0394)
features.5.conv.3 tensor(0.0787)
features.5.conv.6 tensor(0.0480)
features.6.conv.0 tensor(0.0265)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0458)
features.7.conv.0 tensor(0.0461)
features.7.conv.3 tensor(0.1166)
features.7.conv.6 tensor(0.0850)
features.8.conv.0 tensor(0.0697)
features.8.conv.3 tensor(0.1288)
features.8.conv.6 tensor(0.2538)
features.9.conv.0 tensor(0.0802)
features.9.conv.3 tensor(0.1496)
features.9.conv.6 tensor(0.1105)
features.10.conv.0 tensor(0.0388)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0805)
features.11.conv.0 tensor(0.4108)
features.11.conv.3 tensor(0.1206)
features.11.conv.6 tensor(0.4260)
features.12.conv.0 tensor(0.1408)
features.12.conv.3 tensor(0.1250)
features.12.conv.6 tensor(0.4657)
features.13.conv.0 tensor(0.0793)
features.13.conv.3 tensor(0.1481)
features.13.conv.6 tensor(0.0813)
features.14.conv.0 tensor(0.9432)
features.14.conv.3 tensor(0.0978)
features.14.conv.6 tensor(0.9261)
features.15.conv.0 tensor(0.9254)
features.15.conv.3 tensor(0.0767)
features.15.conv.6 tensor(0.9459)
features.16.conv.0 tensor(0.0795)
features.16.conv.3 tensor(0.1027)
features.16.conv.6 tensor(0.2043)
conv.0 tensor(0.1320)
tensor(824906.) 2188896.0
0.80564928
0.80565989
0.80561525
0.80557466
0.80661774
0.80786806
0.80826724
0.81467384
0.81486166
0.81470448
0.81461871
0.81481695
0.81498563
0.81552762
0.81586432
0.81634134
0.81702513
0.81724221
0.81780881
0.81845731
0.82124543
0.82131785
INFO - Training [33][   20/  196]   Loss 0.428061   Top1 85.839844   Top5 98.164062   BatchTime 0.445030   LR 0.001232
0.82138306
0.82157189
0.82132614
0.82107663
0.82090306
0.82098353
0.82098478
0.82111108
0.82110173
0.82110262
0.82115948
0.82150793
0.82185310
0.82226521
0.82274491
0.82293034
INFO - Training [33][   40/  196]   Loss 0.418307   Top1 85.830078   Top5 98.330078   BatchTime 0.407841   LR 0.001230
0.82335764
0.82369131
0.82420921
0.82465827
0.82601196
0.82608235
0.82601362
0.82599503
0.82595378
0.82579237
0.82563400
0.82548660
0.82540232
0.82525980
0.82508832
0.82485467
0.82575542
0.82710093
0.82835436
0.82922888
INFO - Training [33][   60/  196]   Loss 0.417574   Top1 85.690104   Top5 98.404948   BatchTime 0.409005   LR 0.001229
0.83126134
0.83596689
0.83705264
0.83716428
0.83717883
0.83671707
0.83617336
0.83580613
0.83547550
0.83519393
0.83479887
0.83458722
0.83455873
0.83447802
0.83468461
0.83470684
0.83503819
0.83645028
0.83635962
0.83641028
0.83612233
INFO - Training [33][   80/  196]   Loss 0.417663   Top1 85.673828   Top5 98.520508   BatchTime 0.399667   LR 0.001228
0.83608031
0.83592188
0.83578587
0.83601618
0.83616668
0.83745676
0.83732188
0.83724821
0.83722097
0.83696669
0.83656567
0.83643496
0.83624107
0.83607769
0.83576709
0.83564925
0.83560550
0.83551317
0.83507782
0.83484876
0.83455223
0.83454496
0.83443117
INFO - Training [33][  100/  196]   Loss 0.415690   Top1 85.703125   Top5 98.578125   BatchTime 0.389190   LR 0.001226
0.83433342
0.83446079
0.83441961
0.83425522
0.83407831
0.83387947
0.83371872
0.83334351
0.83299595
0.83302164
0.83288842
0.83264673
0.83228201
0.83194262
0.83184624
0.83164573
0.83130175
0.83062363
0.83009577
INFO - Training [33][  120/  196]   Loss 0.406649   Top1 86.025391   Top5 98.649089   BatchTime 0.378716   LR 0.001225
0.82982308
0.82956666
0.82950091
0.82969004
0.82966381
0.82955116
0.82966816
0.82980317
0.82983303
0.82973653
0.82934630
0.82912904
0.82927322
0.82922673
0.82883841
0.82896733
0.82866049
0.82851559
0.82859218
0.82841718
0.82993174
INFO - Training [33][  140/  196]   Loss 0.405535   Top1 86.079799   Top5 98.705357   BatchTime 0.378851   LR 0.001224
0.82990038
0.83034593
0.83144516
0.83129716
0.83112293
0.83104396
0.83108842
0.83069432
0.83038086
0.83006936
0.83010030
0.82990032
0.82936466
0.82902062
0.82870597
0.82711029
INFO - Training [33][  160/  196]   Loss 0.409083   Top1 85.939941   Top5 98.684082   BatchTime 0.376638   LR 0.001222
0.82243055
0.81010115
0.80742604
0.81512153
0.82141346
0.82202387
0.81574708
0.82364035
0.82749039
0.83003283
0.83024776
0.83006823
0.83016932
0.83016330
0.83018237
0.83031493
0.83042765
0.83064592
0.83080578
0.83109373
0.83085340
0.83071971
INFO - Training [33][  180/  196]   Loss 0.409883   Top1 85.935330   Top5 98.598090   BatchTime 0.375292   LR 0.001221
0.83075589
0.83063465
0.83061302
0.83033907
0.83006781
0.82995921
0.82946408
0.82902628
0.82850987
0.82847309
0.82838392
0.82807511
0.82777911
0.82743037
0.82717389
0.82676125
INFO - ==> Top1: 85.932    Top5: 98.592    Loss: 0.410
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [33][   20/   40]   Loss 0.352649   Top1 88.300781   Top5 99.589844   BatchTime 0.157136
INFO - Validation [33][   40/   40]   Loss 0.348396   Top1 88.220000   Top5 99.650000   BatchTime 0.107755
INFO - ==> Top1: 88.220    Top5: 99.650    Loss: 0.348
INFO - ==> Sparsity : 0.382
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3066)
features.1.conv.0 tensor(0.0286)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0582)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0738)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0401)
features.4.conv.0 tensor(0.0309)
features.4.conv.3 tensor(0.1059)
features.4.conv.6 tensor(0.0892)
features.5.conv.0 tensor(0.0251)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.0571)
features.6.conv.0 tensor(0.0269)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0442)
features.7.conv.0 tensor(0.0518)
features.7.conv.3 tensor(0.1149)
features.7.conv.6 tensor(0.0901)
features.8.conv.0 tensor(0.0659)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.1260)
features.9.conv.0 tensor(0.0832)
features.9.conv.3 tensor(0.1536)
features.9.conv.6 tensor(0.1139)
features.10.conv.0 tensor(0.0362)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0695)
features.11.conv.0 tensor(0.1681)
features.11.conv.3 tensor(0.1217)
features.11.conv.6 tensor(0.2426)
features.12.conv.0 tensor(0.1459)
features.12.conv.3 tensor(0.1233)
features.12.conv.6 tensor(0.5065)
features.13.conv.0 tensor(0.0692)
features.13.conv.3 tensor(0.1489)
features.13.conv.6 tensor(0.1325)
features.14.conv.0 tensor(0.9472)
features.14.conv.3 tensor(0.0957)
features.14.conv.6 tensor(0.9209)
features.15.conv.0 tensor(0.9275)
features.15.conv.3 tensor(0.0766)
features.15.conv.6 tensor(0.9488)
features.16.conv.0 tensor(0.0820)
features.16.conv.3 tensor(0.1076)
features.16.conv.6 tensor(0.2162)
conv.0 tensor(0.1975)
tensor(836193.) 2188896.0
0.82628930
0.82539231
0.82438660
0.82356602
0.82265997
0.82156014
0.82099164
0.82065135
0.82048965
0.81901616
0.81886822
0.82023454
0.82186067
0.82218415
0.82205158
0.82201105
0.82214081
0.82223147
0.82192957
0.82181162
INFO - Training [34][   20/  196]   Loss 0.429380   Top1 85.410156   Top5 98.222656   BatchTime 0.458470   LR 0.001218
0.82150614
0.82143432
0.82109159
0.82085705
0.82056528
0.82018691
0.81994790
0.81972319
0.81950557
0.81909573
0.81868690
0.81850153
0.81836438
0.81984687
0.81882995
0.81839663
0.81773907
0.81791842
0.81803769
0.81832951
INFO - Training [34][   40/  196]   Loss 0.420962   Top1 85.761719   Top5 98.281250   BatchTime 0.428351   LR 0.001216
0.81872380
0.81907958
0.81939942
0.81965387
0.81987834
0.82054871
0.82200694
0.82202137
0.82216454
0.82211864
0.82218558
0.82225102
0.82216436
0.82239252
0.82227159
0.82217455
0.82220495
0.82226712
0.82242572
0.82273632
0.82303959
INFO - Training [34][   60/  196]   Loss 0.415825   Top1 86.002604   Top5 98.411458   BatchTime 0.411866   LR 0.001215
0.82298177
0.82282615
0.82241297
0.82190675
0.82162154
0.82171035
0.82129109
0.82114184
0.82064110
0.82021064
0.82013738
0.81984866
0.81961989
0.81940705
0.81892139
0.81862384
0.81866378
INFO - Training [34][   80/  196]   Loss 0.411614   Top1 86.083984   Top5 98.569336   BatchTime 0.398314   LR 0.001213
0.81853098
0.81829315
0.81844449
0.81888443
0.81854898
0.81857640
0.81844431
0.81926686
0.81995785
0.81987339
0.81973547
0.81986052
0.81979758
0.81958789
0.81939006
0.81913823
0.81904650
0.81858063
0.81829047
0.81813043
0.81808990
0.81894809
0.81878048
INFO - Training [34][  100/  196]   Loss 0.402466   Top1 86.207031   Top5 98.613281   BatchTime 0.388364   LR 0.001211
0.81822431
0.81812274
0.81800741
0.81785381
0.81809926
0.81775379
0.81760269
0.81753188
0.81703556
0.81692499
0.81685781
0.81662828
0.81626368
0.81597757
0.81562436
0.81501776
0.81422508
0.81379920
0.81334662
0.81261444
0.81162858
0.81052434
INFO - Training [34][  120/  196]   Loss 0.396428   Top1 86.402995   Top5 98.675130   BatchTime 0.371069   LR 0.001209
0.81013566
0.80979931
0.80944014
0.80971158
0.80998141
0.81047744
0.81073582
0.81077147
0.81003743
0.80962193
0.80946845
0.80994260
0.81002533
0.81014562
0.81028289
0.81002146
INFO - Training [34][  140/  196]   Loss 0.396271   Top1 86.386719   Top5 98.730469   BatchTime 0.366782   LR 0.001208
0.81007934
0.80997664
0.80992609
0.80987680
0.80983436
0.80946457
0.80905187
0.80836457
0.80732042
0.80755067
0.80708265
0.80634630
0.80723065
0.80882633
0.81078535
0.81259912
0.81505108
0.81485140
0.81467587
0.81441665
INFO - Training [34][  160/  196]   Loss 0.403360   Top1 86.191406   Top5 98.703613   BatchTime 0.372384   LR 0.001206
0.81426293
0.81442887
0.81400698
0.81348252
0.81334186
0.81321985
0.81317675
0.81295073
0.81275827
0.81248903
0.81238121
0.81202394
0.81140429
0.81098390
0.81059241
0.81044650
0.81055146
0.81106836
0.81197155
0.81284535
0.81343591
INFO - Training [34][  180/  196]   Loss 0.404504   Top1 86.180556   Top5 98.630642   BatchTime 0.372800   LR 0.001204
0.81374514
0.81395507
0.81428194
0.81443393
0.81441879
0.81446975
0.81478971
0.81487757
0.81493962
0.81500834
0.81510860
0.81486219
0.81599665
0.81643707
0.81639576
0.81627536
INFO - ==> Top1: 86.182    Top5: 98.624    Loss: 0.405
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.343786   Top1 88.945312   Top5 99.589844   BatchTime 0.132761
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.3164)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0577)
features.2.conv.0 tensor(0.0237)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0671)
features.3.conv.0 tensor(0.0237)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0404)
features.4.conv.0 tensor(0.0301)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0895)
INFO - Validation [34][   40/   40]   Loss 0.333341   Top1 88.990000   Top5 99.620000   BatchTime 0.097390
INFO - ==> Top1: 88.990    Top5: 99.620    Loss: 0.333
INFO - ==> Sparsity : 0.372
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
features.5.conv.0 tensor(0.0267)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0690)
features.6.conv.0 tensor(0.0212)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0435)
features.7.conv.0 tensor(0.0470)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.0953)
features.8.conv.0 tensor(0.0653)
features.8.conv.3 tensor(0.1270)
features.8.conv.6 tensor(0.1190)
features.9.conv.0 tensor(0.0802)
features.9.conv.3 tensor(0.1513)
features.9.conv.6 tensor(0.1626)
features.10.conv.0 tensor(0.0350)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0675)
features.11.conv.0 tensor(0.1840)
features.11.conv.3 tensor(0.1215)
features.11.conv.6 tensor(0.3786)
features.12.conv.0 tensor(0.1469)
features.12.conv.3 tensor(0.1271)
features.12.conv.6 tensor(0.5213)
features.13.conv.0 tensor(0.0790)
features.13.conv.3 tensor(0.1491)
features.13.conv.6 tensor(0.0891)
features.14.conv.0 tensor(0.9482)
features.14.conv.3 tensor(0.0970)
features.14.conv.6 tensor(0.9320)
features.15.conv.0 tensor(0.9295)
features.15.conv.3 tensor(0.0764)
features.15.conv.6 tensor(0.9627)
features.16.conv.0 tensor(0.0785)
features.16.conv.3 tensor(0.1072)
features.16.conv.6 tensor(0.2150)
conv.0 tensor(0.1198)
tensor(814443.) 2188896.0
0.81621587
0.81617594
0.81601220
0.81565863
0.81568551
0.81567907
0.81583840
0.81578434
0.81595045
0.81595212
0.81595242
0.81595904
0.81596315
0.81644273
0.81740338
0.81696403
0.81662893
0.81639963
0.81575251
0.81488186
0.81448781
INFO - Training [35][   20/  196]   Loss 0.403765   Top1 85.820312   Top5 98.164062   BatchTime 0.447602   LR 0.001201
0.81433827
0.81399512
0.81371248
0.81337851
0.81312180
0.81264216
0.81216502
0.81167799
0.81111550
0.81066376
0.80946475
0.80629468
0.80682129
0.80793077
0.80946678
0.81028938
0.81148261
0.81217217
0.81266099
0.81231093
INFO - Training [35][   40/  196]   Loss 0.413949   Top1 85.693359   Top5 98.330078   BatchTime 0.429662   LR 0.001199
0.81202328
0.81214052
0.81256557
0.81325966
0.81364489
0.81538558
0.81589860
0.81612545
0.81676507
0.81680101
0.81685787
0.81700099
0.81692392
0.81671864
0.81665170
0.81664389
0.81742191
0.82251132
0.82778156
0.83081520
0.83082378
INFO - Training [35][   60/  196]   Loss 0.406640   Top1 85.800781   Top5 98.365885   BatchTime 0.412961   LR 0.001197
0.83114648
0.83174199
0.83452290
0.83467841
0.83465809
0.83448803
0.83421290
0.83392298
0.83382630
0.83376998
0.83392763
0.83406168
0.83420616
0.83414453
0.83409554
0.83385098
INFO - Training [35][   80/  196]   Loss 0.407169   Top1 85.883789   Top5 98.535156   BatchTime 0.405117   LR 0.001195
0.83383799
0.83365476
0.83355004
0.83359271
0.83329457
0.83286554
0.83299100
0.83262604
0.83254021
0.83239830
0.83232331
0.83201617
0.83208442
0.83209610
0.83177364
0.83178520
0.83162552
0.83131200
0.83117712
0.83100569
0.83085442
INFO - Training [35][  100/  196]   Loss 0.397872   Top1 86.218750   Top5 98.613281   BatchTime 0.397217   LR 0.001192
0.83064264
0.83048564
0.83035737
0.83000457
0.82977796
0.82963866
0.82945174
0.82936025
0.82937086
0.82950234
0.82943970
0.82970005
0.82999778
0.83028358
0.83052921
0.83205909
0.83324748
0.83334970
0.83308959
INFO - Training [35][  120/  196]   Loss 0.391729   Top1 86.422526   Top5 98.668620   BatchTime 0.385869   LR 0.001190
0.83298761
0.83274090
0.83271140
0.83258170
0.83249813
0.83247381
0.83250552
0.83246547
0.83218014
0.83173710
0.83124387
0.83090627
0.83046967
0.83054328
0.83054620
0.83049560
0.83050054
0.83045357
0.83040744
0.83046216
0.83031702
0.83031380
0.83042866
INFO - Training [35][  140/  196]   Loss 0.392976   Top1 86.411830   Top5 98.691406   BatchTime 0.381142   LR 0.001188
0.83039653
0.83027649
0.83021963
0.83024573
0.83013052
0.83006465
0.82992935
0.82982129
0.82996136
0.82990533
0.82961643
0.82921433
0.82926679
0.82911527
0.82930058
0.82961220
0.83008748
0.83124703
0.83129305
0.83138770
0.83231086
INFO - Training [35][  160/  196]   Loss 0.398960   Top1 86.230469   Top5 98.681641   BatchTime 0.379580   LR 0.001186
0.83346897
0.83457232
0.83503115
0.83516628
0.83527529
0.83525968
0.83518982
0.83515716
0.83499384
0.83486962
0.83465445
0.83457369
0.83453423
0.83453572
0.83441138
0.83445174
0.83422190
INFO - Training [35][  180/  196]   Loss 0.398416   Top1 86.158854   Top5 98.630642   BatchTime 0.378519   LR 0.001184
0.83417219
0.83403486
0.83358359
0.83319050
0.83302993
0.83316165
0.83337885
0.83349395
0.83457172
0.83473027
0.83405423
0.83402276
0.83340800
0.83314669
0.83281690
INFO - ==> Top1: 86.184    Top5: 98.658    Loss: 0.398
0.83189279
0.83116508
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.326096   Top1 89.257812   Top5 99.648438   BatchTime 0.166403
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3086)
features.1.conv.0 tensor(0.0280)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0538)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0726)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0406)
features.4.conv.0 tensor(0.0326)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0898)
features.5.conv.0 tensor(0.0309)
features.5.conv.3 tensor(0.0810)
features.5.conv.6 tensor(0.0814)
features.6.conv.0 tensor(0.0239)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0435)
features.7.conv.0 tensor(0.0459)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.0897)
features.8.conv.0 tensor(0.0617)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.1193)
features.9.conv.0 tensor(0.0811)
features.9.conv.3 tensor(0.1499)
features.9.conv.6 tensor(0.1213)
features.10.conv.0 tensor(0.0402)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0711)
features.11.conv.0 tensor(0.1773)
features.11.conv.3 tensor(0.1235)
features.11.conv.6 tensor(0.4135)
features.12.conv.0 tensor(0.1474)
features.12.conv.3 tensor(0.1296)
features.12.conv.6 tensor(0.1889)
features.13.conv.0 tensor(0.0721)
features.13.conv.3 tensor(0.1534)
features.13.conv.6 tensor(0.0824)
features.14.conv.0 tensor(0.9492)
features.14.conv.3 tensor(0.1015)
features.14.conv.6 tensor(0.9353)
features.15.conv.0 tensor(0.9298)
features.15.conv.3 tensor(0.0734)
features.15.conv.6 tensor(0.9527)
features.16.conv.0 tensor(0.2825)
features.16.conv.3 tensor(0.1131)
features.16.conv.6 tensor(0.2095)
conv.0 tensor(0.1114)
tensor(821300.) 2188896.0
INFO - Validation [35][   40/   40]   Loss 0.310085   Top1 89.580000   Top5 99.740000   BatchTime 0.115119
INFO - ==> Top1: 89.580    Top5: 99.740    Loss: 0.310
INFO - ==> Sparsity : 0.375
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
0.83244234
0.83299041
0.83331388
0.83332616
0.83359873
0.83408803
0.83442456
0.83439362
0.83452642
0.83433092
0.83430648
0.83404499
0.83402973
0.83411944
0.83425152
0.83451867
0.83637798
0.83600795
0.83561248
INFO - Training [36][   20/  196]   Loss 0.408726   Top1 85.703125   Top5 98.085938   BatchTime 0.444928   LR 0.001180
0.83564264
0.83569658
0.83585000
0.83584869
0.83600843
0.83618218
0.83654720
0.83726507
0.83735114
0.83771431
0.83811414
0.83825296
0.83825952
0.83796567
0.83738822
0.83664948
0.83639771
0.83603245
0.83528519
0.83500636
0.83478636
INFO - Training [36][   40/  196]   Loss 0.402633   Top1 85.849609   Top5 98.281250   BatchTime 0.413327   LR 0.001177
0.83425450
0.83392245
0.83369792
0.83347672
0.83322400
0.83290279
0.83259672
0.83253658
0.83233923
0.83237344
0.83181405
0.83124024
0.83059478
0.82991147
0.82890975
0.82790989
0.82690644
0.82610613
0.82537776
0.82510936
INFO - Training [36][   60/  196]   Loss 0.403472   Top1 85.865885   Top5 98.457031   BatchTime 0.407153   LR 0.001175
0.82440901
0.82362360
0.82271761
0.82020569
0.81737667
0.81319100
0.80809951
0.80902708
0.80799180
0.80541050
0.80229133
0.80024660
0.80072325
0.80197042
0.80317581
0.80582970
0.80883944
0.80994630
0.81034291
0.80993998
0.80997616
0.81217831
INFO - Training [36][   80/  196]   Loss 0.404418   Top1 86.030273   Top5 98.554688   BatchTime 0.395339   LR 0.001173
0.81531972
0.81583929
0.81578434
0.81741208
0.81710291
0.81668752
0.81658185
0.81612229
0.81543481
0.81539202
0.81432343
0.81329650
0.81220645
0.81109899
0.81050968
0.80982059
0.80892473
0.80820888
0.80831426
0.80821377
INFO - Training [36][  100/  196]   Loss 0.401529   Top1 86.238281   Top5 98.562500   BatchTime 0.379978   LR 0.001170
0.80763674
0.80722499
0.80763245
0.80800617
0.80774289
0.80821407
0.80871689
0.80917293
0.80931413
0.80886358
0.80939710
0.81190455
0.81389636
0.81539208
0.81670505
0.81777459
0.81921691
0.82058388
0.82206428
0.82353407
INFO - Training [36][  120/  196]   Loss 0.396080   Top1 86.422526   Top5 98.619792   BatchTime 0.367184   LR 0.001168
0.82681251
0.83170086
0.83341360
0.83449847
0.83457714
0.83412248
0.83381474
0.83344102
0.83298326
0.83257765
0.83226150
0.83234125
0.83236527
0.83223152
0.83199000
0.83184522
0.83192950
0.83186352
0.83171588
0.83174926
INFO - Training [36][  140/  196]   Loss 0.396526   Top1 86.492746   Top5 98.671875   BatchTime 0.355508   LR 0.001165
0.83178270
0.83151752
0.83139271
0.83140951
0.83134526
0.83137268
0.83096933
0.83086288
0.83042312
0.83024365
0.83049023
0.83016998
0.82986242
0.83004320
0.83019048
0.83026266
0.83066863
INFO - Training [36][  160/  196]   Loss 0.399942   Top1 86.384277   Top5 98.610840   BatchTime 0.356204   LR 0.001163
0.83090574
0.83090520
0.83119988
0.83155048
0.83184153
0.83217108
0.83289576
0.83312476
0.83311450
0.83316356
0.83318436
0.83334559
0.83326417
0.83293879
0.83234662
0.83216262
0.83210921
0.83203810
0.83195907
0.83172852
0.83176017
INFO - Training [36][  180/  196]   Loss 0.399625   Top1 86.443142   Top5 98.572049   BatchTime 0.357718   LR 0.001160
0.83155566
0.83115828
0.83093512
0.83095896
0.83121383
0.83132660
0.83118528
0.83061510
0.83090597
0.83087593
0.83090401
0.83105201
0.83093315
0.83097684
0.83089119
0.83056843
********************pre-trained*****************
INFO - ==> Top1: 86.458    Top5: 98.552    Loss: 0.399
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 0.326512   Top1 89.453125   Top5 99.531250   BatchTime 0.137643
INFO - Validation [36][   40/   40]   Loss 0.323294   Top1 89.150000   Top5 99.650000   BatchTime 0.119465
INFO - ==> Top1: 89.150    Top5: 99.650    Loss: 0.323
INFO - ==> Sparsity : 0.369
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.3105)
features.1.conv.0 tensor(0.0267)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0508)
features.2.conv.0 tensor(0.0205)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0680)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0360)
features.4.conv.0 tensor(0.0293)
features.4.conv.3 tensor(0.1088)
features.4.conv.6 tensor(0.0872)
features.5.conv.0 tensor(0.0337)
features.5.conv.3 tensor(0.0810)
features.5.conv.6 tensor(0.0864)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0448)
features.7.conv.0 tensor(0.0419)
features.7.conv.3 tensor(0.1126)
features.7.conv.6 tensor(0.1047)
features.8.conv.0 tensor(0.0627)
features.8.conv.3 tensor(0.1305)
features.8.conv.6 tensor(0.1336)
features.9.conv.0 tensor(0.0756)
features.9.conv.3 tensor(0.1487)
features.9.conv.6 tensor(0.1249)
features.10.conv.0 tensor(0.0401)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0688)
features.11.conv.0 tensor(0.1715)
features.11.conv.3 tensor(0.1231)
features.11.conv.6 tensor(0.4138)
features.12.conv.0 tensor(0.1732)
features.12.conv.3 tensor(0.1337)
features.12.conv.6 tensor(0.1987)
features.13.conv.0 tensor(0.0753)
features.13.conv.3 tensor(0.1532)
features.13.conv.6 tensor(0.1001)
features.14.conv.0 tensor(0.9481)
features.14.conv.3 tensor(0.1001)
features.14.conv.6 tensor(0.9818)
features.15.conv.0 tensor(0.9315)
features.15.conv.3 tensor(0.0752)
features.15.conv.6 tensor(0.9544)
features.16.conv.0 tensor(0.0878)
features.16.conv.3 tensor(0.1120)
features.16.conv.6 tensor(0.2188)
conv.0 tensor(0.1182)
tensor(808471.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
0.83062106
0.83078575
0.83093441
0.83106041
0.83084500
0.83066511
0.83047187
0.83012575
0.83009654
0.82988209
0.82971179
0.82959157
0.83102435
0.83104372
0.83087152
0.83076805
0.83081955
0.83073765
INFO - Training [37][   20/  196]   Loss 0.417512   Top1 85.683594   Top5 98.046875   BatchTime 0.467521   LR 0.001155
0.83062500
0.83046609
0.83072603
0.83097571
0.83081299
0.83060980
0.83032447
0.83063227
0.83101517
0.83111650
0.83093029
0.83096105
0.83112711
0.83163357
0.83193779
0.83404779
0.83426833
0.83435798
0.83425921
0.83470720
0.83467329
INFO - Training [37][   40/  196]   Loss 0.407779   Top1 85.888672   Top5 98.310547   BatchTime 0.425129   LR 0.001153
0.83443797
0.83449674
0.83431143
0.83425522
0.83437687
0.83436739
0.83385557
0.83373976
0.83397716
0.83397430
0.83393204
0.83362854
0.83343804
0.83356732
0.83335263
0.83285648
0.83271158
0.83275694
0.83276784
0.83255589
0.83265173
0.83284265
INFO - Training [37][   60/  196]   Loss 0.405812   Top1 86.048177   Top5 98.372396   BatchTime 0.405501   LR 0.001150
0.83282471
0.83272552
0.83273292
0.83254331
0.83233130
0.83215541
0.83223385
0.83241552
0.83255446
0.83241802
0.83189869
0.83113045
0.83005095
0.82978296
0.83038306
0.83052611
0.83060902
0.83078963
0.83092725
0.83116037
0.83222270
INFO - Training [37][   80/  196]   Loss 0.402807   Top1 86.176758   Top5 98.491211   BatchTime 0.399599   LR 0.001147
0.83398169
0.83390081
0.83375019
0.83361292
0.83364284
0.83370680
0.83394408
0.83401388
0.83368516
0.83353400
0.83341545
0.83332491
0.83336103
0.83335906
0.83334303
0.83334750
0.83345234
0.83347929
0.83333772
INFO - Training [37][  100/  196]   Loss 0.394106   Top1 86.511719   Top5 98.546875   BatchTime 0.401614   LR 0.001144
0.83333421
0.83339441
0.83359432
0.83355600
0.83330309
0.83323735
0.83302122
0.83266228
0.83260322
0.83246726
0.83235943
0.83220989
0.83183479
0.83163834
0.83159196
0.83149844
0.83155298
0.83148861
INFO - Training [37][  120/  196]   Loss 0.392443   Top1 86.471354   Top5 98.629557   BatchTime 0.393253   LR 0.001142
0.83140504
0.83140689
0.83096069
0.83081883
0.83089691
0.83080286
0.83071601
0.83053648
0.83011317
0.82989854
0.82977551
0.82953447
0.82939750
0.82920039
0.82906014
0.83045822
0.83116299
0.83086097
INFO - Training [37][  140/  196]   Loss 0.392109   Top1 86.462054   Top5 98.657924   BatchTime 0.382176   LR 0.001139
0.83032805
0.82988793
0.82959503
0.82901478
0.82874614
0.82826245
0.82827532
0.82797104
0.82784146
0.82741791
0.82716948
0.82696903
0.82697451
0.82718855
0.82730931
0.82765043
0.82806689
0.82876647
0.82916826
0.82969975
0.82980472
0.82953221
0.82943344
0.82940042
INFO - Training [37][  160/  196]   Loss 0.391872   Top1 86.489258   Top5 98.645020   BatchTime 0.376405   LR 0.001136
0.82932365
0.82921839
0.82929128
0.82991517
0.84048665
0.84017378
0.84014893
0.84024024
0.83973253
0.83926988
0.83907419
0.83851177
0.83822507
0.83802563
0.83786011
0.83758646
0.83759725
INFO - Training [37][  180/  196]   Loss 0.392280   Top1 86.475694   Top5 98.582899   BatchTime 0.376086   LR 0.001133
0.83773786
0.83772951
0.83744657
0.83718336
0.83689553
0.83664370
0.83638012
0.83604580
0.83540118
0.83476770
0.83435607
0.83382946
0.83364159
0.83345658
0.83306164
0.83251178
0.83216947
0.83226478
********************pre-trained*****************
INFO - ==> Top1: 86.488    Top5: 98.606    Loss: 0.392
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.349943   Top1 88.847656   Top5 99.707031   BatchTime 0.124955
INFO - Validation [37][   40/   40]   Loss 0.333781   Top1 89.230000   Top5 99.730000   BatchTime 0.090732
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3125)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0538)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0424)
features.2.conv.6 tensor(0.0694)
features.3.conv.0 tensor(0.0220)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0341)
features.4.conv.0 tensor(0.0254)
features.4.conv.3 tensor(0.1094)
features.4.conv.6 tensor(0.0781)
features.5.conv.0 tensor(0.0365)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0866)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0449)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.1661)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1282)
features.8.conv.6 tensor(0.1258)
features.9.conv.0 tensor(0.0820)
features.9.conv.3 tensor(0.1490)
features.9.conv.6 tensor(0.1315)
features.10.conv.0 tensor(0.0343)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0807)
features.11.conv.0 tensor(0.1808)
features.11.conv.3 tensor(0.1252)
features.11.conv.6 tensor(0.2194)
features.12.conv.0 tensor(0.1443)
features.12.conv.3 tensor(0.1294)
features.12.conv.6 tensor(0.3725)
features.13.conv.0 tensor(0.0780)
features.13.conv.3 tensor(0.1508)
features.13.conv.6 tensor(0.1013)
features.14.conv.0 tensor(0.9517)
features.14.conv.3 tensor(0.1008)
features.14.conv.6 tensor(0.9677)
features.15.conv.0 tensor(0.9326)
features.15.conv.3 tensor(0.0760)
features.15.conv.6 tensor(0.9631)
features.16.conv.0 tensor(0.0858)
features.16.conv.3 tensor(0.1103)
features.16.conv.6 tensor(0.2056)
conv.0 tensor(0.1200)
tensor(804710.) 2188896.0
INFO - ==> Top1: 89.230    Top5: 99.730    Loss: 0.334
INFO - ==> Sparsity : 0.368
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
0.83198065
0.83308488
0.83276314
0.83215767
0.83193582
0.83201176
0.83171934
0.83133280
0.83085060
0.83046240
0.83021796
0.83001626
0.82998568
0.83031458
0.83091080
0.83085209
0.83082974
0.83080530
0.83081895
0.83170778
0.83333504
0.83361810
0.83333361
INFO - Training [38][   20/  196]   Loss 0.410103   Top1 85.468750   Top5 98.437500   BatchTime 0.492012   LR 0.001128
0.83275962
0.83216780
0.83160388
0.83122593
0.83158475
0.83143020
0.83043975
0.82984072
0.82955313
0.82969940
0.82936984
0.82892054
0.82891440
0.82882118
0.82874590
0.82844073
INFO - Training [38][   40/  196]   Loss 0.404611   Top1 85.986328   Top5 98.476562   BatchTime 0.430941   LR 0.001125
0.82830155
0.82813209
0.82778627
0.82735145
0.82695061
0.82678097
0.82617122
0.82562739
0.82481676
0.82427818
0.82396400
0.82357317
0.82323378
0.82271338
0.82219493
0.82181931
0.82136130
0.82108933
0.82065499
0.82058960
0.82045871
0.82005078
INFO - Training [38][   60/  196]   Loss 0.401906   Top1 85.904948   Top5 98.593750   BatchTime 0.405986   LR 0.001122
0.81943470
0.81917775
0.81925714
0.81918436
0.81907588
0.81900978
0.81900471
0.81903416
0.81923521
0.81941324
0.81983370
0.82042760
0.82345235
0.82372308
0.82372719
0.82376510
0.82399839
0.82596171
0.82621169
0.82631534
0.82653111
INFO - Training [38][   80/  196]   Loss 0.405009   Top1 85.913086   Top5 98.686523   BatchTime 0.401005   LR 0.001119
0.82659584
0.82644850
0.82629061
0.82623452
0.82634878
0.82656235
0.82685167
0.82685238
0.82689494
0.82693911
0.82700747
0.82749581
0.82741570
0.82892692
0.82936692
0.83002424
0.83042121
INFO - Training [38][  100/  196]   Loss 0.394872   Top1 86.234375   Top5 98.765625   BatchTime 0.394267   LR 0.001116
0.83074999
0.83075261
0.83043081
0.83002257
0.82950658
0.82934862
0.82952362
0.82966697
0.82953161
0.82923836
0.82916975
0.82916230
0.82924104
0.82933772
0.82936823
0.82918042
0.82917601
0.82902616
0.82893586
0.82882947
0.82881802
0.82876915
0.82886451
0.82854176
INFO - Training [38][  120/  196]   Loss 0.388716   Top1 86.448568   Top5 98.789062   BatchTime 0.383007   LR 0.001112
0.82823032
0.82802850
0.82785791
0.82777554
0.82753295
0.82725745
0.82717514
0.82707733
0.82679647
0.82634366
0.82577610
0.82544374
0.82521319
0.82503384
INFO - Training [38][  140/  196]   Loss 0.386557   Top1 86.587612   Top5 98.819754   BatchTime 0.369847   LR 0.001109
0.82507461
0.82500893
0.82495528
0.82477349
0.82444596
0.82429135
0.82418388
0.82425421
0.82431954
0.82442802
0.82462710
0.82476622
0.82493955
0.82490385
0.82489014
0.82490492
0.82484198
0.82468730
0.82478982
0.82495803
0.82533288
0.82532310
0.82518208
0.82484001
0.82641345
INFO - Training [38][  160/  196]   Loss 0.389831   Top1 86.494141   Top5 98.803711   BatchTime 0.363667   LR 0.001106
0.82656759
0.82630068
0.82597417
0.82549083
0.82499760
0.82478029
0.82447886
0.82428896
0.82393706
0.82332659
0.82251757
0.82221788
0.82182360
0.82154375
0.82292390
0.82248735
0.82240939
INFO - Training [38][  180/  196]   Loss 0.391420   Top1 86.449653   Top5 98.743490   BatchTime 0.363643   LR 0.001103
0.82164627
0.82134980
0.82086802
0.82124233
0.82163769
0.82142377
0.82147270
0.82165349
0.82182682
0.82131577
0.82155216
0.82157385
0.82147962
0.82142425
0.82134491
0.82137758
INFO - ==> Top1: 86.554    Top5: 98.728    Loss: 0.389
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82118243
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [38][   20/   40]   Loss 0.365499   Top1 87.812500   Top5 99.589844   BatchTime 0.124252
INFO - Validation [38][   40/   40]   Loss 0.353403   Top1 88.170000   Top5 99.650000   BatchTime 0.091340
INFO - ==> Top1: 88.170    Top5: 99.650    Loss: 0.353
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.3066)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0516)
features.2.conv.0 tensor(0.0211)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0700)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0360)
features.4.conv.0 tensor(0.0282)
features.4.conv.3 tensor(0.1100)
features.4.conv.6 tensor(0.0864)
features.5.conv.0 tensor(0.0350)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0908)
features.6.conv.0 tensor(0.0200)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0435)
features.7.conv.0 tensor(0.0413)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.1130)
features.8.conv.0 tensor(0.0612)
features.8.conv.3 tensor(0.1354)
features.8.conv.6 tensor(0.1259)
features.9.conv.0 tensor(0.0873)
features.9.conv.3 tensor(0.1441)
features.9.conv.6 tensor(0.1201)
features.10.conv.0 tensor(0.0370)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0665)
features.11.conv.0 tensor(0.1719)
features.11.conv.3 tensor(0.1219)
features.11.conv.6 tensor(0.3565)
features.12.conv.0 tensor(0.1819)
features.12.conv.3 tensor(0.1285)
features.12.conv.6 tensor(0.4574)
features.13.conv.0 tensor(0.0757)
features.13.conv.3 tensor(0.1522)
features.13.conv.6 tensor(0.1264)
features.14.conv.0 tensor(0.9514)
features.14.conv.3 tensor(0.1020)
features.14.conv.6 tensor(0.9201)
features.15.conv.0 tensor(0.9344)
features.15.conv.3 tensor(0.0748)
features.15.conv.6 tensor(0.9534)
features.16.conv.0 tensor(0.0963)
features.16.conv.3 tensor(0.1128)
features.16.conv.6 tensor(0.2024)
conv.0 tensor(0.1505)
tensor(823329.) 2188896.0
0.82056338
0.82005668
0.81978053
0.81977928
0.81948942
0.81955934
0.81960362
0.81912827
0.81856829
0.81817108
0.81756681
0.81705719
0.81641895
0.81549221
0.81438881
0.81303984
0.81208968
0.81100845
0.81043696
0.80983758
0.80920488
INFO - Training [39][   20/  196]   Loss 0.398739   Top1 85.781250   Top5 98.125000   BatchTime 0.465713   LR 0.001097
0.80861717
0.80801463
0.80662537
0.80669934
0.80787611
0.80801111
0.80822307
0.80822229
0.80800003
0.80764610
0.80777264
0.80783343
0.80813789
0.80808347
0.80829960
0.80886567
0.80946171
0.81005877
0.81048495
0.81080997
INFO - Training [39][   40/  196]   Loss 0.401025   Top1 86.181641   Top5 98.320312   BatchTime 0.431574   LR 0.001094
0.81082487
0.81122690
0.81183469
0.81206954
0.81235462
0.81439269
0.81472927
0.81465650
0.81509364
0.81524795
0.81542319
0.81562412
0.81699497
0.82205540
0.82211339
0.82184637
0.82180876
0.82130963
0.82127291
0.82151502
0.82164520
0.82203633
INFO - Training [39][   60/  196]   Loss 0.399612   Top1 86.230469   Top5 98.430990   BatchTime 0.410440   LR 0.001090
0.82246572
0.82284760
0.82326353
0.82340431
0.82370800
0.82400942
0.82445067
0.82462144
0.82508922
0.82543236
0.82554394
0.82559657
0.82550949
0.82574397
0.82584816
0.82671052
INFO - Training [39][   80/  196]   Loss 0.399709   Top1 86.337891   Top5 98.486328   BatchTime 0.400130   LR 0.001087
0.82705778
0.82700789
0.82698888
0.82703072
0.82693076
0.82676876
0.82668954
0.82635778
0.82622033
0.82633448
0.82599515
0.82558781
0.82535380
0.82520205
0.82524097
0.82539183
0.82516950
0.82512259
0.82511711
0.82504618
0.82511556
0.82496881
INFO - Training [39][  100/  196]   Loss 0.391445   Top1 86.492188   Top5 98.585938   BatchTime 0.393385   LR 0.001084
0.82479340
0.82483166
0.82475889
0.82528460
0.82580471
0.82602340
0.82583219
0.82605284
0.82602513
0.82610643
0.82605827
0.82627171
0.82636279
0.82649684
0.82659900
0.82670021
INFO - Training [39][  120/  196]   Loss 0.384663   Top1 86.722005   Top5 98.681641   BatchTime 0.386712   LR 0.001080
0.82669145
0.82667440
0.82679647
0.82689661
0.82775211
0.82893938
0.82910877
0.82895786
0.82903874
0.82891387
0.82875228
0.82865030
0.82862592
0.82828748
0.82827055
0.82812554
0.82810670
0.82790351
0.82776809
0.82751882
0.82745361
0.82715619
0.82706159
0.82682776
0.82670289
0.82646561
INFO - Training [39][  140/  196]   Loss 0.381251   Top1 86.827567   Top5 98.744420   BatchTime 0.377248   LR 0.001077
0.82591313
0.82554996
0.82541251
0.82524014
0.82515311
0.82518023
0.82532263
0.82524675
0.82550168
0.82524681
0.82521802
0.82530314
0.82516342
0.82514244
0.82518542
0.82495391
INFO - Training [39][  160/  196]   Loss 0.384174   Top1 86.718750   Top5 98.725586   BatchTime 0.374747   LR 0.001073
0.82493490
0.82478863
0.82460409
0.82460725
0.82439131
0.82423586
0.82408607
0.82414395
0.82415605
0.82367253
0.82313615
0.82295889
0.82241923
0.82210308
0.82187825
0.82197750
0.82192111
0.82201523
0.82157582
0.82155329
0.82160783
0.82084632
0.82046485
INFO - Training [39][  180/  196]   Loss 0.384359   Top1 86.681858   Top5 98.637153   BatchTime 0.372190   LR 0.001070
0.82025427
0.81997657
0.81962585
0.81931430
0.81896412
0.81877708
0.81863230
0.81836897
0.81817746
0.81828058
0.81851643
0.81868422
0.81814426
0.81780982
********************pre-trained*****************
INFO - ==> Top1: 86.688    Top5: 98.640    Loss: 0.384
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [39][   20/   40]   Loss 0.340920   Top1 88.652344   Top5 99.531250   BatchTime 0.128614
INFO - Validation [39][   40/   40]   Loss 0.330011   Top1 89.020000   Top5 99.620000   BatchTime 0.092877
INFO - ==> Top1: 89.020    Top5: 99.620    Loss: 0.330
INFO - ==> Sparsity : 0.414
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.3086)
features.1.conv.0 tensor(0.0260)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0469)
features.2.conv.0 tensor(0.0208)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0694)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0367)
features.4.conv.0 tensor(0.0311)
features.4.conv.3 tensor(0.1111)
features.4.conv.6 tensor(0.0869)
features.5.conv.0 tensor(0.0254)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0916)
features.6.conv.0 tensor(0.0194)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0440)
features.7.conv.0 tensor(0.0428)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.1108)
features.8.conv.0 tensor(0.0606)
features.8.conv.3 tensor(0.1343)
features.8.conv.6 tensor(0.1417)
features.9.conv.0 tensor(0.0845)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.1224)
features.10.conv.0 tensor(0.0384)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0677)
features.11.conv.0 tensor(0.1688)
features.11.conv.3 tensor(0.1229)
features.11.conv.6 tensor(0.1946)
features.12.conv.0 tensor(0.1658)
features.12.conv.3 tensor(0.1300)
features.12.conv.6 tensor(0.5204)
features.13.conv.0 tensor(0.0827)
features.13.conv.3 tensor(0.1532)
features.13.conv.6 tensor(0.1429)
features.14.conv.0 tensor(0.9516)
features.14.conv.3 tensor(0.1013)
features.14.conv.6 tensor(0.9309)
features.15.conv.0 tensor(0.9366)
features.15.conv.3 tensor(0.0727)
features.15.conv.6 tensor(0.9686)
features.16.conv.0 tensor(0.1018)
features.16.conv.3 tensor(0.1091)
features.16.conv.6 tensor(0.1829)
conv.0 tensor(0.3641)
tensor(905767.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
0.81729358
0.81724250
0.81583482
0.81291604
0.80832112
0.80282342
0.79761130
0.79559290
0.79765588
0.79762638
0.79863232
0.80027258
0.80114901
0.80241764
0.80446523
0.80584764
0.80903298
0.81195325
INFO - Training [40][   20/  196]   Loss 0.398540   Top1 85.996094   Top5 98.046875   BatchTime 0.483313   LR 0.001064
0.81288892
0.81316090
0.81522387
0.81732064
0.81912321
0.81996810
0.82075602
0.82126009
0.82169127
0.82211900
0.82222593
0.82239056
0.82262105
0.82278007
0.82238513
0.82196003
0.82169008
0.82174861
0.82161796
0.82156140
0.82144791
INFO - Training [40][   40/  196]   Loss 0.406255   Top1 85.966797   Top5 98.105469   BatchTime 0.437967   LR 0.001060
0.82095879
0.82056266
0.82049674
0.82021940
0.81993592
0.81943500
0.81919366
0.81897277
0.81839544
0.81822014
0.81807911
0.81753993
0.81678230
0.81651789
0.81611437
0.81608975
0.81636328
0.81554806
0.81461871
0.81375575
0.81385356
INFO - Training [40][   60/  196]   Loss 0.398185   Top1 86.341146   Top5 98.294271   BatchTime 0.416949   LR 0.001056
0.81443363
0.81851822
0.81858325
0.81867570
0.81870496
0.81862104
0.81966358
0.81940842
0.81951350
0.81975788
0.81966329
0.82081336
0.82124019
0.82109606
0.82084769
0.82105154
0.82172418
0.82155424
0.82097638
0.82038790
0.82018179
INFO - Training [40][   80/  196]   Loss 0.396350   Top1 86.474609   Top5 98.437500   BatchTime 0.405917   LR 0.001053
0.81950486
0.81912780
0.81869179
0.81818080
0.81805700
0.81791461
0.81742716
0.81691718
0.81641752
0.81580442
0.81563157
0.81519252
0.81447881
0.81392974
0.81360489
0.81294614
0.81241471
INFO - Training [40][  100/  196]   Loss 0.388104   Top1 86.757812   Top5 98.527344   BatchTime 0.396825   LR 0.001049
0.81189305
0.81136638
0.81107652
0.81035548
0.80926108
0.80777389
0.80692202
0.80655336
0.80561310
0.80499542
0.80501390
0.80521929
0.80499017
0.80458665
0.80471569
0.80470568
0.80417973
0.80444854
0.80588776
0.80713660
0.80902445
0.81120872
0.81272620
INFO - Training [40][  120/  196]   Loss 0.384176   Top1 86.923828   Top5 98.623047   BatchTime 0.387350   LR 0.001045
0.81344754
0.81442988
0.81532562
0.81629634
0.81725413
0.81900877
0.82092798
0.82145232
0.82142627
0.82142216
0.82168597
0.82170331
0.82160926
0.82165170
0.82187498
0.82195812
0.82181078
0.82186067
INFO - Training [40][  140/  196]   Loss 0.381167   Top1 87.031250   Top5 98.691406   BatchTime 0.381724   LR 0.001042
0.82179224
0.82155305
0.82109219
0.82123554
0.82136518
0.82134742
0.82143998
0.82149720
0.82135803
0.82125652
0.82095140
0.82062912
0.81957221
0.81925470
0.81920010
0.81916809
0.81893378
0.81851447
0.81773543
0.81778002
0.81722182
0.81704503
INFO - Training [40][  160/  196]   Loss 0.381552   Top1 86.945801   Top5 98.669434   BatchTime 0.379247   LR 0.001038
0.81690663
0.81701505
0.81746197
0.81756550
0.81746244
0.81759727
0.81865805
0.81833136
0.81813288
0.81839848
0.81821549
0.81794816
0.81793100
0.81799370
0.81771559
0.81757963
0.81721997
0.81713659
0.81692189
0.81691140
0.81690329
0.81689674
INFO - Training [40][  180/  196]   Loss 0.380929   Top1 86.944444   Top5 98.632812   BatchTime 0.378648   LR 0.001034
0.81713432
0.81699950
0.81703889
0.81701082
0.81702727
0.81774998
0.81848854
0.81927264
0.81940323
0.81941026
0.81922352
0.81880403
0.81859136
********************pre-trained*****************
INFO - ==> Top1: 86.940    Top5: 98.668    Loss: 0.380
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.354855   Top1 88.632812   Top5 99.511719   BatchTime 0.120609
INFO - Validation [40][   40/   40]   Loss 0.349518   Top1 88.730000   Top5 99.590000   BatchTime 0.086738
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.3164)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0438)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0631)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0365)
features.4.conv.0 tensor(0.0347)
features.4.conv.3 tensor(0.1105)
features.4.conv.6 tensor(0.0866)
features.5.conv.0 tensor(0.0356)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0876)
features.6.conv.0 tensor(0.0241)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0442)
features.7.conv.0 tensor(0.0413)
features.7.conv.3 tensor(0.1120)
features.7.conv.6 tensor(0.0976)
features.8.conv.0 tensor(0.0544)
features.8.conv.3 tensor(0.1311)
features.8.conv.6 tensor(0.1084)
features.9.conv.0 tensor(0.0965)
features.9.conv.3 tensor(0.1415)
features.9.conv.6 tensor(0.1257)
features.10.conv.0 tensor(0.0382)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.0658)
features.11.conv.0 tensor(0.1632)
features.11.conv.3 tensor(0.1235)
features.11.conv.6 tensor(0.3299)
features.12.conv.0 tensor(0.1777)
features.12.conv.3 tensor(0.1318)
features.12.conv.6 tensor(0.5150)
features.13.conv.0 tensor(0.0827)
features.13.conv.3 tensor(0.1518)
features.13.conv.6 tensor(0.1489)
features.14.conv.0 tensor(0.9564)
features.14.conv.3 tensor(0.1053)
features.14.conv.6 tensor(0.9347)
features.15.conv.0 tensor(0.9380)
features.15.conv.3 tensor(0.0725)
features.15.conv.6 tensor(0.9720)
features.16.conv.0 tensor(0.0965)
features.16.conv.3 tensor(0.1108)
features.16.conv.6 tensor(0.1812)
conv.0 tensor(0.1088)
tensor(809030.) 2188896.0
INFO - ==> Top1: 88.730    Top5: 99.590    Loss: 0.350
INFO - ==> Sparsity : 0.370
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
0.81846112
0.81846774
0.81851572
0.81866407
0.81835365
0.81839806
0.81844908
0.81848717
0.81840491
0.81834179
0.81821567
0.81799418
0.81768185
0.81749439
0.81726474
0.81728828
0.81702256
0.81693280
0.81671250
INFO - Training [41][   20/  196]   Loss 0.405993   Top1 85.859375   Top5 97.890625   BatchTime 0.468267   LR 0.001027
0.81656504
0.81629312
0.81597632
0.81601828
0.81587499
0.81553036
0.81481785
0.81416106
0.81401259
0.81402475
0.81409973
0.81390941
0.81401765
0.81405741
0.81431353
0.81449294
0.81511277
0.81518453
0.81495512
0.81487995
0.81482995
INFO - Training [41][   40/  196]   Loss 0.408250   Top1 85.537109   Top5 98.076172   BatchTime 0.420106   LR 0.001023
0.81469256
0.81439334
0.81415027
0.81394094
0.81354266
0.81289023
0.81265694
0.81215537
0.81170994
0.81158334
0.81185514
0.81185377
0.81183863
0.81160212
0.81139159
0.81112146
0.81067795
0.81030285
0.81037468
0.81035340
0.81036401
0.81052691
INFO - Training [41][   60/  196]   Loss 0.392633   Top1 86.315104   Top5 98.320312   BatchTime 0.401051   LR 0.001020
0.81048155
0.81053829
0.81119132
0.81161243
0.81116635
0.81122547
0.81068838
0.81050068
0.81064850
0.81116962
0.81160700
0.81170273
0.81189626
0.81305885
0.81362247
0.81377184
0.81402105
INFO - Training [41][   80/  196]   Loss 0.387412   Top1 86.655273   Top5 98.452148   BatchTime 0.392002   LR 0.001016
0.81429571
0.81451851
0.81501871
0.81538218
0.81555343
0.81569666
0.81550562
0.81572765
0.81551659
0.81563973
0.81600410
0.81562883
0.81571424
0.81589216
0.81540477
0.81613576
0.81605011
0.81601840
0.81586391
0.81568730
0.81544071
0.81527305
0.81499225
INFO - Training [41][  100/  196]   Loss 0.380273   Top1 86.890625   Top5 98.535156   BatchTime 0.384895   LR 0.001012
0.81469727
0.81435394
0.81407154
0.81420827
0.81426376
0.81391692
0.81358606
0.81302351
0.81236231
0.81195986
0.81146091
0.81108099
0.81093287
0.81056201
0.81066144
0.81090027
0.81056648
0.81080997
0.81107879
INFO - Training [41][  120/  196]   Loss 0.376263   Top1 87.167969   Top5 98.616536   BatchTime 0.371537   LR 0.001008
0.81178242
0.81293994
0.81278545
0.81219304
0.81178856
0.81136954
0.81108719
0.81073338
0.81063706
0.81033719
0.81011349
0.80996007
0.80956537
0.80897701
0.80842960
0.80807495
0.80742443
0.80662942
INFO - Training [41][  140/  196]   Loss 0.375991   Top1 87.117746   Top5 98.646763   BatchTime 0.366479   LR 0.001004
0.80599529
0.80551797
0.80481684
0.80435658
0.80369663
0.80310875
0.80238444
0.80188024
0.80137777
0.80107325
0.80074817
0.80119914
0.80183768
0.80293393
0.80375469
0.80368167
0.80373776
0.80561507
0.80701101
0.80784953
0.80960763
0.81004709
INFO - Training [41][  160/  196]   Loss 0.381072   Top1 86.882324   Top5 98.640137   BatchTime 0.366034   LR 0.001000
0.81035042
0.81054980
0.81022912
0.80999601
0.80947733
0.81495351
0.82407677
0.82352179
0.82337785
0.82343709
0.82363230
0.82381105
0.82371014
0.82482415
0.82426715
0.82469505
0.82468289
INFO - Training [41][  180/  196]   Loss 0.380896   Top1 86.840278   Top5 98.589410   BatchTime 0.365505   LR 0.000996
0.82479846
0.82482296
0.82500142
0.82512975
0.82509130
0.82502085
0.82525676
0.82591665
0.82625240
0.82641464
0.82621640
0.82656473
0.82679850
0.82695681
0.82711959
0.82731342
INFO - ==> Top1: 86.874    Top5: 98.604    Loss: 0.379
0.82759964
0.82776409
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [41][   20/   40]   Loss 0.324865   Top1 89.140625   Top5 99.687500   BatchTime 0.124015
INFO - Validation [41][   40/   40]   Loss 0.304587   Top1 89.800000   Top5 99.700000   BatchTime 0.087347
INFO - ==> Top1: 89.800    Top5: 99.700    Loss: 0.305
INFO - ==> Sparsity : 0.375
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.3125)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0451)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0680)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0354)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0882)
features.5.conv.0 tensor(0.0327)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1370)
features.6.conv.0 tensor(0.0205)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0440)
features.7.conv.0 tensor(0.0491)
features.7.conv.3 tensor(0.1111)
features.7.conv.6 tensor(0.1218)
features.8.conv.0 tensor(0.0437)
features.8.conv.3 tensor(0.1296)
features.8.conv.6 tensor(0.1100)
features.9.conv.0 tensor(0.0931)
features.9.conv.3 tensor(0.1453)
features.9.conv.6 tensor(0.1236)
features.10.conv.0 tensor(0.0455)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0671)
features.11.conv.0 tensor(0.1847)
features.11.conv.3 tensor(0.1267)
features.11.conv.6 tensor(0.3701)
features.12.conv.0 tensor(0.1718)
features.12.conv.3 tensor(0.1304)
features.12.conv.6 tensor(0.2457)
features.13.conv.0 tensor(0.0756)
features.13.conv.3 tensor(0.1493)
features.13.conv.6 tensor(0.1576)
features.14.conv.0 tensor(0.9536)
features.14.conv.3 tensor(0.1032)
features.14.conv.6 tensor(0.9399)
features.15.conv.0 tensor(0.9389)
features.15.conv.3 tensor(0.0723)
features.15.conv.6 tensor(0.9685)
features.16.conv.0 tensor(0.1012)
features.16.conv.3 tensor(0.1108)
features.16.conv.6 tensor(0.1743)
conv.0 tensor(0.1643)
tensor(819888.) 2188896.0
0.82779157
0.82764924
0.82735729
0.82737118
0.82760823
0.82780313
0.82814300
0.83360726
0.83758783
0.83718175
0.83692396
0.83693922
0.83701169
0.83701146
0.83720797
0.83743954
0.83730072
0.83730823
0.83696020
0.83670694
0.83674699
0.83793300
INFO - Training [42][   20/  196]   Loss 0.379504   Top1 87.031250   Top5 98.105469   BatchTime 0.448243   LR 0.000988
0.83831894
0.83838326
0.83839208
0.83831841
0.83812529
0.83822519
0.83824575
0.83799458
0.83812380
0.83785194
0.83801448
0.83796358
0.83821869
0.83843154
0.83866924
0.83896440
0.83912230
0.83868670
0.83857810
INFO - Training [42][   40/  196]   Loss 0.385215   Top1 86.933594   Top5 98.349609   BatchTime 0.422068   LR 0.000984
0.83865815
0.83872044
0.83862740
0.83874512
0.83871788
0.83854198
0.83831966
0.83819324
0.83831078
0.83801258
0.83767265
0.83754885
0.83746922
0.83711970
0.83673131
0.83621901
0.83626127
0.83635098
0.83615547
0.83546984
INFO - Training [42][   60/  196]   Loss 0.379163   Top1 86.979167   Top5 98.528646   BatchTime 0.408277   LR 0.000980
0.83512080
0.83497775
0.83473510
0.83447951
0.83404160
0.83338994
0.82793564
0.82706273
0.81901377
0.81168801
0.81751299
0.82783544
0.83742613
0.83927101
0.83938044
0.83921987
0.83902514
INFO - Training [42][   80/  196]   Loss 0.377265   Top1 87.060547   Top5 98.637695   BatchTime 0.400027   LR 0.000976
0.83882248
0.83874255
0.83849972
0.83840972
0.83846027
0.83963925
0.83939701
0.83916432
0.83953774
0.83956593
0.83941084
0.83927876
0.83883625
0.83832550
0.83839703
0.83862352
0.83845413
0.83856434
0.83838004
0.83842343
0.83834064
0.83873975
0.83887058
0.83887005
INFO - Training [42][  100/  196]   Loss 0.367009   Top1 87.468750   Top5 98.667969   BatchTime 0.384316   LR 0.000972
0.83885032
0.83889252
0.83890468
0.83855796
0.83819133
0.83772522
0.83741909
0.83730179
0.83723056
0.83722216
0.83739024
0.83727938
0.83713353
0.83722299
0.83718473
0.83718014
0.83738673
INFO - Training [42][  120/  196]   Loss 0.364041   Top1 87.483724   Top5 98.753255   BatchTime 0.377632   LR 0.000968
0.83725625
0.83725703
0.83719546
0.83734876
0.83729452
0.83740723
0.83718407
0.83719265
0.83727962
0.83899909
0.83887178
0.83866936
0.83836251
0.83808088
0.83788282
0.83789569
0.83788770
0.83806765
0.83835155
0.84003037
0.83976591
0.83984947
0.83975834
INFO - Training [42][  140/  196]   Loss 0.362263   Top1 87.586496   Top5 98.800223   BatchTime 0.375230   LR 0.000964
0.83965659
0.83958560
0.83992499
0.83954597
0.83954376
0.83920145
0.83929843
0.83940226
0.83969420
0.83988500
0.84015846
0.84003258
0.83997059
0.83996314
0.83956426
0.83925676
INFO - Training [42][  160/  196]   Loss 0.367529   Top1 87.463379   Top5 98.737793   BatchTime 0.373611   LR 0.000959
0.83894134
0.83849525
0.83814788
0.83797735
0.83807498
0.83797318
0.83793408
0.83805692
0.83784121
0.83784175
0.83772987
0.83787227
0.83771974
0.83758920
0.83732563
0.83740282
0.83724731
0.83737874
0.83696836
0.83703667
0.83706802
0.83692563
INFO - Training [42][  180/  196]   Loss 0.366827   Top1 87.456597   Top5 98.704427   BatchTime 0.372513   LR 0.000955
0.83736885
0.83757108
0.83714306
0.83686233
0.83642864
0.83590257
0.83534455
0.83485830
0.83468211
0.83410048
0.83350134
0.83311284
0.83370221
0.83433020
0.83397567
0.83361095
********************pre-trained*****************
INFO - ==> Top1: 87.448    Top5: 98.726    Loss: 0.366
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [42][   20/   40]   Loss 0.330424   Top1 89.375000   Top5 99.609375   BatchTime 0.123860
INFO - Validation [42][   40/   40]   Loss 0.314757   Top1 89.500000   Top5 99.710000   BatchTime 0.088425
INFO - ==> Top1: 89.500    Top5: 99.710    Loss: 0.315
INFO - ==> Sparsity : 0.364
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
features.0.conv.0 tensor(0.5556)
features.0.conv.3 tensor(0.3066)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0447)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0631)
features.3.conv.0 tensor(0.0220)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0317)
features.4.conv.0 tensor(0.0355)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0885)
features.5.conv.0 tensor(0.0324)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0938)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0347)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0444)
features.7.conv.3 tensor(0.1149)
features.7.conv.6 tensor(0.1103)
features.8.conv.0 tensor(0.0475)
features.8.conv.3 tensor(0.1322)
features.8.conv.6 tensor(0.1057)
features.9.conv.0 tensor(0.0912)
features.9.conv.3 tensor(0.1427)
features.9.conv.6 tensor(0.1200)
features.10.conv.0 tensor(0.0330)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0663)
features.11.conv.0 tensor(0.1815)
features.11.conv.3 tensor(0.1248)
features.11.conv.6 tensor(0.2290)
features.12.conv.0 tensor(0.2319)
features.12.conv.3 tensor(0.1292)
features.12.conv.6 tensor(0.3145)
features.13.conv.0 tensor(0.0890)
features.13.conv.3 tensor(0.1499)
features.13.conv.6 tensor(0.1643)
features.14.conv.0 tensor(0.9541)
features.14.conv.3 tensor(0.1028)
features.14.conv.6 tensor(0.9429)
features.15.conv.0 tensor(0.9395)
features.15.conv.3 tensor(0.0731)
features.15.conv.6 tensor(0.9771)
features.16.conv.0 tensor(0.1045)
features.16.conv.3 tensor(0.1109)
features.16.conv.6 tensor(0.1759)
conv.0 tensor(0.1032)
tensor(797134.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
0.83304572
0.83238518
0.83200258
0.83156258
0.83118457
0.83077294
0.82981962
0.82933009
0.82896292
0.82860345
0.82819366
0.82790804
0.82772559
0.82759321
0.82818377
0.82848519
0.82828331
0.82813323
0.82782024
0.82782286
INFO - Training [43][   20/  196]   Loss 0.390308   Top1 86.679688   Top5 98.027344   BatchTime 0.441701   LR 0.000947
0.82788914
0.82960373
0.82993972
0.82993799
0.82958978
0.82945788
0.82939011
0.82956070
0.82940865
0.82913947
0.82908833
0.82892740
0.82873857
0.82846463
0.82825977
0.82787073
0.82750076
0.82721442
0.82680535
0.82644802
0.82629615
INFO - Training [43][   40/  196]   Loss 0.390212   Top1 86.757812   Top5 98.349609   BatchTime 0.411255   LR 0.000943
0.82622463
0.82639503
0.82591081
0.82537782
0.82503104
0.82480747
0.82442528
0.82434350
0.82421833
0.82387781
0.82368481
0.82353276
0.82332379
0.82305974
0.82304674
0.82312340
0.82301140
0.82293665
0.82271922
0.82240897
INFO - Training [43][   60/  196]   Loss 0.384725   Top1 86.842448   Top5 98.457031   BatchTime 0.401576   LR 0.000939
0.82246172
0.82184845
0.82107347
0.82069457
0.82067084
0.82036173
0.82000405
0.81967741
0.81924003
0.81908494
0.81896710
0.81865019
0.81828445
0.81793094
0.81770456
0.81754082
0.81717551
INFO - Training [43][   80/  196]   Loss 0.380114   Top1 87.006836   Top5 98.569336   BatchTime 0.392166   LR 0.000934
0.81693232
0.81686580
0.81640518
0.81700814
0.81676632
0.81621230
0.81540436
0.81488216
0.81441927
0.81367302
0.81281692
0.81237113
0.81198883
0.81170195
0.81174922
0.81169319
0.81151730
0.81159067
0.81077629
0.81057382
0.81061190
0.80957949
0.80869055
0.80829316
INFO - Training [43][  100/  196]   Loss 0.374641   Top1 87.253906   Top5 98.570312   BatchTime 0.380555   LR 0.000930
0.80858248
0.81038153
0.81237376
0.81380117
0.81607211
0.81826431
0.81874734
0.81886750
0.81912732
0.81953275
0.81939888
0.81929702
0.81988132
0.81973630
0.82011330
0.81994689
0.81983119
INFO - Training [43][  120/  196]   Loss 0.366296   Top1 87.513021   Top5 98.701172   BatchTime 0.375652   LR 0.000926
0.81981748
0.81968284
0.81952363
0.81925660
0.82795388
0.82671750
0.82621318
0.82609594
0.82562262
0.82591516
0.82591981
0.82541007
0.82500964
0.82459998
0.82358104
0.82244474
0.82133144
0.82061356
0.81953633
0.81898665
0.81865788
INFO - Training [43][  140/  196]   Loss 0.367241   Top1 87.500000   Top5 98.741629   BatchTime 0.377144   LR 0.000921
0.81910086
0.82007617
0.81971443
0.81816125
0.81721741
0.81775719
0.81586993
0.81307220
0.81483269
0.81354147
0.80714124
0.80725533
0.80434036
0.80211627
0.80394530
0.80779022
0.81310749
0.81883866
0.82275778
0.82444537
0.82608414
INFO - Training [43][  160/  196]   Loss 0.370059   Top1 87.321777   Top5 98.737793   BatchTime 0.377229   LR 0.000917
0.82529002
0.82429338
0.82401860
0.82427728
0.82433021
0.82480586
0.82497460
0.82556194
0.82565492
0.82581645
0.82607388
0.82616651
0.82577312
0.82557714
0.82513559
0.82483345
INFO - Training [43][  180/  196]   Loss 0.370130   Top1 87.341580   Top5 98.665365   BatchTime 0.376027   LR 0.000912
0.82426751
0.82386428
0.82345831
0.82309872
0.82282102
0.82281679
0.82332474
0.82390422
0.82329428
0.82303047
0.82262951
0.82220894
0.82182729
0.82154340
0.82194990
0.82275110
0.82352030
INFO - ==> Top1: 87.400    Top5: 98.676    Loss: 0.370
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82402653
0.82394063
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [43][   20/   40]   Loss 0.351703   Top1 88.632812   Top5 99.511719   BatchTime 0.117220
INFO - Validation [43][   40/   40]   Loss 0.348670   Top1 88.540000   Top5 99.560000   BatchTime 0.086485
INFO - ==> Top1: 88.540    Top5: 99.560    Loss: 0.349
INFO - ==> Sparsity : 0.371
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 90.020   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5556)
features.0.conv.3 tensor(0.3066)
features.1.conv.0 tensor(0.0260)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0447)
features.2.conv.0 tensor(0.0246)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0654)
features.3.conv.0 tensor(0.0263)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0347)
features.4.conv.0 tensor(0.0378)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0872)
features.5.conv.0 tensor(0.0257)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.0889)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0463)
features.7.conv.0 tensor(0.0450)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.0983)
features.8.conv.0 tensor(0.0439)
features.8.conv.3 tensor(0.1302)
features.8.conv.6 tensor(0.1143)
features.9.conv.0 tensor(0.0944)
features.9.conv.3 tensor(0.1403)
features.9.conv.6 tensor(0.1154)
features.10.conv.0 tensor(0.0323)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.0664)
features.11.conv.0 tensor(0.1976)
features.11.conv.3 tensor(0.1225)
features.11.conv.6 tensor(0.6295)
features.12.conv.0 tensor(0.1644)
features.12.conv.3 tensor(0.1273)
features.12.conv.6 tensor(0.2291)
features.13.conv.0 tensor(0.0970)
features.13.conv.3 tensor(0.1522)
features.13.conv.6 tensor(0.1699)
features.14.conv.0 tensor(0.9564)
features.14.conv.3 tensor(0.1009)
features.14.conv.6 tensor(0.9372)
features.15.conv.0 tensor(0.9401)
features.15.conv.3 tensor(0.0714)
features.15.conv.6 tensor(0.9699)
features.16.conv.0 tensor(0.0968)
features.16.conv.3 tensor(0.1105)
features.16.conv.6 tensor(0.1650)
conv.0 tensor(0.1191)
tensor(812824.) 2188896.0
0.82382977
0.82335889
0.82204080
0.82237786
0.82128280
0.82088834
0.82053363
0.82095987
0.82089865
0.82175213
0.82182032
0.82136047
0.82107848
0.82077384
0.82103908
0.82102865
0.82108355
0.82083905
0.82065505
INFO - Training [44][   20/  196]   Loss 0.376552   Top1 86.679688   Top5 98.222656   BatchTime 0.465382   LR 0.000904
0.82050836
0.82053429
0.82084793
0.82124758
0.82189226
0.82238156
0.82284391
0.82324117
0.82347989
0.82353145
0.82365149
0.82343429
0.82320881
0.82282305
0.82244480
0.82233292
0.82205302
0.82153660
0.82132173
0.82106954
INFO - Training [44][   40/  196]   Loss 0.370755   Top1 87.041016   Top5 98.300781   BatchTime 0.425694   LR 0.000900
0.82092488
0.82059836
0.82029921
0.82009304
0.82009554
0.82014877
0.82118034
0.82157505
0.82155305
0.82142740
0.82122636
0.82099068
0.82079619
0.82078373
0.82100028
0.82119763
0.82137018
0.82139176
INFO - Training [44][   60/  196]   Loss 0.367773   Top1 87.096354   Top5 98.430990   BatchTime 0.396130   LR 0.000895
0.82178843
0.82151127
0.82125640
0.82106316
0.82102764
0.82114971
0.82117885
0.82109487
0.82097244
0.82066119
0.82071662
0.82086313
0.82099658
0.82099587
0.82097906
0.82111901
0.82126701
0.82153291
0.82187194
INFO - Training [44][   80/  196]   Loss 0.365590   Top1 87.236328   Top5 98.598633   BatchTime 0.374146   LR 0.000891
0.82213306
0.82251227
0.82300138
0.82341987
0.82425362
0.82557195
0.82544893
0.82516599
0.82502979
0.82493985
0.82488275
0.82471633
0.82472098
0.82453167
0.82444966
0.82447219
0.82436180
0.82447612
0.82409990
0.82389987
INFO - Training [44][  100/  196]   Loss 0.359324   Top1 87.453125   Top5 98.621094   BatchTime 0.360723   LR 0.000886
0.82407516
0.82533580
0.82604957
0.82620126
0.82643366
0.82634896
0.82623506
0.82644361
0.82678223
0.82641959
0.82642573
0.82640207
0.82666564
0.82666647
0.82626146
0.82547486
0.82569480
0.82560426
0.82545817
0.82523215
0.82468194
0.82425869
INFO - Training [44][  120/  196]   Loss 0.353189   Top1 87.711589   Top5 98.701172   BatchTime 0.346783   LR 0.000882
0.82378840
0.82322907
0.82284623
0.82275921
0.82234925
0.82186043
0.82174754
0.82169807
0.82131183
0.82113200
0.82088351
0.82060015
0.82017094
0.81983882
0.81941009
0.81900126
0.81856942
0.81814623
0.81758481
INFO - Training [44][  140/  196]   Loss 0.352747   Top1 87.700893   Top5 98.761161   BatchTime 0.342865   LR 0.000877
0.81690097
0.81643927
0.81589824
0.81538975
0.81422985
0.81316411
0.81225258
0.81152064
0.81091601
0.81063896
0.81010115
0.80988884
0.80998737
0.80970114
0.81105947
0.81075644
0.81033385
0.81066555
0.81096452
0.81069219
0.81064659
0.81065476
0.81058997
0.81051832
0.81073064
INFO - Training [44][  160/  196]   Loss 0.354686   Top1 87.680664   Top5 98.757324   BatchTime 0.340272   LR 0.000873
0.81063813
0.81063032
0.81040257
0.81017429
0.80977982
0.80924374
0.80893129
0.80880243
0.81016469
0.81294435
0.81285888
0.81257230
0.81254858
0.81228244
INFO - Training [44][  180/  196]   Loss 0.357343   Top1 87.580295   Top5 98.715278   BatchTime 0.333496   LR 0.000868
0.81224096
0.81253725
0.81272852
0.81303930
0.81310755
0.81329852
0.81305504
0.81311673
0.81262898
0.81285232
0.81305557
0.81269562
0.81262362
0.81229043
0.81200349
0.81190467
0.81146628
0.81061894
0.80982548
0.81045163
INFO - ==> Top1: 87.610    Top5: 98.716    Loss: 0.357
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [44][   20/   40]   Loss 0.305038   Top1 90.332031   Top5 99.550781   BatchTime 0.121394
INFO - Validation [44][   40/   40]   Loss 0.296886   Top1 90.100000   Top5 99.670000   BatchTime 0.084823
features.0.conv.0 tensor(0.5382)
features.0.conv.3 tensor(0.3066)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0460)
features.2.conv.0 tensor(0.0165)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0634)
features.3.conv.0 tensor(0.0237)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0374)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0856)
features.5.conv.0 tensor(0.0233)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0916)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0405)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0456)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.1084)
features.8.conv.0 tensor(0.0446)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.1210)
features.9.conv.0 tensor(0.0911)
features.9.conv.3 tensor(0.1421)
features.9.conv.6 tensor(0.1151)
features.10.conv.0 tensor(0.0345)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0722)
features.11.conv.0 tensor(0.1589)
features.11.conv.3 tensor(0.1258)
features.11.conv.6 tensor(0.4735)
features.12.conv.0 tensor(0.2140)
features.12.conv.3 tensor(0.1283)
features.12.conv.6 tensor(0.4707)
features.13.conv.0 tensor(0.0810)
features.13.conv.3 tensor(0.1501)
features.13.conv.6 tensor(0.1754)
features.14.conv.0 tensor(0.9603)
features.14.conv.3 tensor(0.1032)
features.14.conv.6 tensor(0.9436)
features.15.conv.0 tensor(0.9409)
features.15.conv.3 tensor(0.0719)
features.15.conv.6 tensor(0.9706)
features.16.conv.0 tensor(0.0935)
features.16.conv.3 tensor(0.1103)
features.16.conv.6 tensor(0.1817)
conv.0 tensor(0.1162)
tensor(823638.) 2188896.0
INFO - ==> Top1: 90.100    Top5: 99.670    Loss: 0.297
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
0.80937111
0.80862117
0.80811626
0.80774534
0.80732018
0.80689508
0.80661577
0.80630463
0.80589557
0.80562055
0.80565017
0.80576789
0.80548203
0.80543661
0.80516356
0.80514061
0.80537617
0.80715138
0.80700028
INFO - Training [45][   20/  196]   Loss 0.365878   Top1 86.660156   Top5 98.437500   BatchTime 0.348494   LR 0.000860
0.80765790
0.80811352
0.80782318
0.80753320
0.80729759
0.80693090
0.80667013
0.80597371
0.80475742
0.80268615
0.80207241
0.80180126
0.80111641
0.80023068
0.79737335
0.79310519
0.78945386
0.79142720
0.79358137
0.79584175
0.79759610
0.79873121
INFO - Training [45][   40/  196]   Loss 0.371869   Top1 87.089844   Top5 98.369141   BatchTime 0.306311   LR 0.000855
0.79956335
0.80060184
0.80131990
0.80204427
0.80263573
0.80308479
0.80374283
0.80417949
0.80459595
0.80497223
0.80520606
0.80523735
0.80539644
0.80545485
0.80544716
0.80525541
0.80523264
0.80543184
0.80556715
INFO - Training [45][   60/  196]   Loss 0.367639   Top1 87.246094   Top5 98.593750   BatchTime 0.310501   LR 0.000850
0.80577862
0.80568850
0.80585575
0.80582845
0.80589825
0.80600607
0.80604136
0.80610287
0.80603313
0.80589408
0.80646288
0.80599123
0.80690879
0.80691808
0.80686122
0.80692446
INFO - Training [45][   80/  196]   Loss 0.366578   Top1 87.553711   Top5 98.691406   BatchTime 0.297098   LR 0.000846
0.80718398
0.80706835
0.80705863
0.80685008
0.80696273
0.80671465
0.80671543
0.80668777
0.80662525
0.80644888
0.80637300
0.80645907
0.80637294
0.80627662
0.80620092
0.80598575
0.80580789
0.80604607
0.80591297
0.80600953
0.80614644
INFO - Training [45][  100/  196]   Loss 0.359707   Top1 87.792969   Top5 98.730469   BatchTime 0.294268   LR 0.000841
0.80627912
0.80640060
0.80668211
0.80767578
0.80812502
0.80840677
0.81083906
0.81044918
0.81027925
0.81026703
0.80991465
0.80991167
0.80984485
0.80932963
0.80916667
0.80883712
0.80862683
0.80839962
0.80936825
0.80990183
0.80946016
INFO - Training [45][  120/  196]   Loss 0.354679   Top1 87.942708   Top5 98.795573   BatchTime 0.292465   LR 0.000836
0.80921769
0.80908710
0.80899894
0.80875057
0.80862999
0.80839604
0.80808604
0.80800420
0.80751723
0.80708313
0.80626136
0.80613619
0.80592257
0.80587935
0.80545390
0.80512935
0.80486304
0.80636561
0.80568218
0.80455959
0.80377191
0.80320400
INFO - Training [45][  140/  196]   Loss 0.353499   Top1 87.949219   Top5 98.822545   BatchTime 0.291021   LR 0.000832
0.80151725
0.79983050
0.79718745
0.79327899
0.79489249
0.79864222
0.80214757
0.80364740
0.80479342
0.80512077
0.80546141
0.80557650
0.80588800
0.80579889
0.80606657
0.80595523
0.80580997
0.80589294
0.80601674
0.80610633
INFO - Training [45][  160/  196]   Loss 0.356260   Top1 87.890625   Top5 98.798828   BatchTime 0.291781   LR 0.000827
0.80613261
0.80583662
0.80570275
0.80578113
0.80571872
0.80552506
0.80539888
0.80548799
0.80530578
0.80532300
0.80518013
0.80516565
0.80497652
0.80473858
0.80468911
0.80447209
0.80397373
0.80358809
0.80326128
0.80300814
0.80269879
INFO - Training [45][  180/  196]   Loss 0.357948   Top1 87.799479   Top5 98.741319   BatchTime 0.290268   LR 0.000822
0.80212331
0.80170441
0.80115944
0.80047661
0.80012363
0.80043358
0.80117083
0.80142134
0.80133110
0.80125463
0.80136764
0.80146766
0.80150008
0.80151963
0.80165774
INFO - ==> Top1: 87.892    Top5: 98.752    Loss: 0.356
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.347077   Top1 88.750000   Top5 99.609375   BatchTime 0.135807
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.3184)
features.1.conv.0 tensor(0.0267)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0443)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0631)
features.3.conv.0 tensor(0.0223)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0276)
features.4.conv.0 tensor(0.0327)
features.4.conv.3 tensor(0.1082)
features.4.conv.6 tensor(0.0833)
features.5.conv.0 tensor(0.0249)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.1024)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0502)
features.7.conv.3 tensor(0.1082)
features.7.conv.6 tensor(0.1011)
features.8.conv.0 tensor(0.0475)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.1279)
features.9.conv.0 tensor(0.0861)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.1250)
features.10.conv.0 tensor(0.0424)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.0963)
features.11.conv.0 tensor(0.1962)
features.11.conv.3 tensor(0.1217)
features.11.conv.6 tensor(0.4485)
features.12.conv.0 tensor(0.2273)
features.12.conv.3 tensor(0.1325)
features.12.conv.6 tensor(0.4881)
features.13.conv.0 tensor(0.0812)
features.13.conv.3 tensor(0.1464)
features.13.conv.6 tensor(0.1821)
features.14.conv.0 tensor(0.9600)
features.14.conv.3 tensor(0.1021)
features.14.conv.6 tensor(0.9526)
features.15.conv.0 tensor(0.9425)
features.15.conv.3 tensor(0.0713)
features.15.conv.6 tensor(0.9730)
features.16.conv.0 tensor(0.0871)
features.16.conv.3 tensor(0.1094)
features.16.conv.6 tensor(0.1635)
conv.0 tensor(0.1272)
tensor(827889.) 2188896.0
INFO - Validation [45][   40/   40]   Loss 0.338282   Top1 88.920000   Top5 99.690000   BatchTime 0.096573
INFO - ==> Top1: 88.920    Top5: 99.690    Loss: 0.338
INFO - ==> Sparsity : 0.378
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
0.80315048
0.80360365
0.80360633
0.80359358
0.80344427
0.80328792
0.80390209
0.80548841
0.80540645
0.80546588
0.80559278
0.80566168
0.80550230
0.80561018
0.80551487
0.80532789
0.80532163
0.80540729
0.80533713
0.80538458
0.80534261
0.80539757
INFO - Training [46][   20/  196]   Loss 0.359151   Top1 87.851562   Top5 98.183594   BatchTime 0.372983   LR 0.000814
0.80561197
0.80578381
0.80740917
0.80742890
0.80731964
0.80723405
0.80706942
0.80691373
0.80671728
0.80664569
0.80676502
0.80672574
0.80675977
0.80669796
0.80853033
0.80864793
INFO - Training [46][   40/  196]   Loss 0.358650   Top1 87.929688   Top5 98.378906   BatchTime 0.311857   LR 0.000809
0.80868375
0.80844021
0.80814773
0.80797321
0.80758500
0.80745530
0.80727667
0.80696803
0.80662560
0.80627495
0.80589151
0.80599397
0.80603653
0.80605060
0.80581099
0.80575877
0.80570263
0.80527115
0.80478996
0.80467540
0.80471915
INFO - Training [46][   60/  196]   Loss 0.355912   Top1 87.942708   Top5 98.483073   BatchTime 0.307064   LR 0.000804
0.80483937
0.80470127
0.80436128
0.80432749
0.80457622
0.80459297
0.80436784
0.80421394
0.80411553
0.80398130
0.80374026
0.80348867
0.80334657
0.80356485
0.80518281
0.80478126
0.80431622
0.80431783
0.80407107
0.80385411
0.80352724
INFO - Training [46][   80/  196]   Loss 0.353100   Top1 88.041992   Top5 98.618164   BatchTime 0.299352   LR 0.000799
0.80337673
0.80300009
0.80281383
0.80276638
0.80208570
0.80176121
0.80152959
0.80130279
0.80094296
0.80069387
0.80029547
0.79986620
0.79972553
0.79943424
0.79899096
0.79889292
0.79833722
0.79782814
0.79750472
0.79698086
0.79650408
0.79657328
INFO - Training [46][  100/  196]   Loss 0.349228   Top1 88.136719   Top5 98.726562   BatchTime 0.294304   LR 0.000794
0.79651320
0.79643708
0.79614753
0.79592478
0.79551762
0.79730791
0.79727733
0.79741210
0.79748917
0.79754734
0.79766858
0.79778385
0.79771954
0.79815358
0.79864746
0.79893613
INFO - Training [46][  120/  196]   Loss 0.341667   Top1 88.486328   Top5 98.776042   BatchTime 0.287146   LR 0.000789
0.79813313
0.79833221
0.79881752
0.79957271
0.80278838
0.80297446
0.80358922
0.80629730
0.80640197
0.80604416
0.80554789
0.80494231
0.80447632
0.80357802
0.80323690
0.80338669
0.80377972
0.80449587
0.80483067
0.80509984
0.80576885
0.80650544
0.80740076
INFO - Training [46][  140/  196]   Loss 0.343188   Top1 88.468192   Top5 98.803013   BatchTime 0.284209   LR 0.000785
0.80765206
0.80886787
0.80960780
0.80960286
0.81013435
0.81029969
0.81026608
0.81031942
0.81023788
0.81013435
0.81017941
0.81025368
0.81029063
0.81022316
0.81020355
0.80992138
0.80970150
0.80926120
0.80905586
0.80897379
0.80884075
INFO - Training [46][  160/  196]   Loss 0.349028   Top1 88.244629   Top5 98.789062   BatchTime 0.284524   LR 0.000780
0.80880260
0.80883968
0.80855095
0.80851477
0.80828863
0.80799013
0.80778468
0.80742925
0.80728984
0.80749094
0.80756241
0.80798733
0.80812138
0.80814600
0.80820906
0.80839950
0.80842173
0.80811191
0.80802149
0.80784506
INFO - Training [46][  180/  196]   Loss 0.349213   Top1 88.203125   Top5 98.715278   BatchTime 0.285183   LR 0.000775
0.80769145
0.80758196
0.80744272
0.80724728
0.80717212
0.80717480
0.80714822
0.80703336
0.80677801
0.80638975
0.80630529
0.80631709
0.80605608
0.80557638
INFO - ==> Top1: 88.192    Top5: 98.718    Loss: 0.349
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [46][   20/   40]   Loss 0.449373   Top1 85.917969   Top5 99.218750   BatchTime 0.120005
INFO - Validation [46][   40/   40]   Loss 0.435945   Top1 85.990000   Top5 99.370000   BatchTime 0.086611
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.3008)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0451)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0432)
features.2.conv.6 tensor(0.0611)
features.3.conv.0 tensor(0.0223)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0321)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0846)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0547)
features.6.conv.0 tensor(0.0221)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0463)
features.7.conv.0 tensor(0.0522)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.0922)
features.8.conv.0 tensor(0.0461)
features.8.conv.3 tensor(0.1305)
features.8.conv.6 tensor(0.1342)
features.9.conv.0 tensor(0.0944)
features.9.conv.3 tensor(0.1427)
features.9.conv.6 tensor(0.1390)
features.10.conv.0 tensor(0.0347)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0748)
features.11.conv.0 tensor(0.2312)
features.11.conv.3 tensor(0.1231)
features.11.conv.6 tensor(0.4971)
features.12.conv.0 tensor(0.2021)
features.12.conv.3 tensor(0.1370)
features.12.conv.6 tensor(0.5263)
features.13.conv.0 tensor(0.0811)
features.13.conv.3 tensor(0.1437)
features.13.conv.6 tensor(0.1843)
features.14.conv.0 tensor(0.9571)
features.14.conv.3 tensor(0.1002)
features.14.conv.6 tensor(0.9494)
features.15.conv.0 tensor(0.9437)
features.15.conv.3 tensor(0.0706)
features.15.conv.6 tensor(0.9679)
features.16.conv.0 tensor(0.0893)
features.16.conv.3 tensor(0.1086)
features.16.conv.6 tensor(0.1679)
conv.0 tensor(0.1277)
tensor(832970.) 2188896.0
INFO - ==> Top1: 85.990    Top5: 99.370    Loss: 0.436
INFO - ==> Sparsity : 0.381
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
0.80545050
0.80525357
0.80501610
0.80482823
0.80474329
0.80485457
0.80490685
0.80452573
0.80385745
0.80318743
0.80287129
0.80270129
0.80270636
0.80275798
0.80304742
0.80299842
0.80290896
0.80355769
0.80439055
INFO - Training [47][   20/  196]   Loss 0.372879   Top1 86.933594   Top5 98.359375   BatchTime 0.327971   LR 0.000766
0.80456376
0.80453187
0.80450529
0.80444211
0.80451828
0.80453694
0.80463946
0.80473840
0.80477524
0.80488122
0.80493659
0.80497605
0.80491507
0.80497396
0.80489385
0.80469549
0.80421513
0.80403334
0.80364358
0.80325562
0.80323064
0.80320096
INFO - Training [47][   40/  196]   Loss 0.365471   Top1 87.314453   Top5 98.476562   BatchTime 0.302876   LR 0.000761
0.80313450
0.80313551
0.80295044
0.80281281
0.80280411
0.80225706
0.80211085
0.80204493
0.80218560
0.80231106
0.80244559
0.80254072
0.80277389
0.80282402
0.80295742
INFO - Training [47][   60/  196]   Loss 0.354485   Top1 87.766927   Top5 98.593750   BatchTime 0.292011   LR 0.000756
0.80332959
0.80501491
0.80548447
0.80531937
0.80553472
0.80595642
0.80598933
0.80580384
0.80601382
0.80589592
0.80569732
0.80525053
0.80493182
0.80473953
0.80459964
0.80460322
0.80441791
0.80430830
0.80405641
0.80393898
0.80359584
0.80338806
0.80296820
INFO - Training [47][   80/  196]   Loss 0.356915   Top1 87.768555   Top5 98.710938   BatchTime 0.284479   LR 0.000752
0.80258197
0.80250537
0.80247825
0.80247116
0.80250329
0.80232596
0.80215520
0.80195999
0.80180699
0.80156934
0.80117786
0.80090916
0.80054796
0.80063176
0.80077451
0.80153245
0.80151516
0.80126244
0.80124116
0.80139571
0.80141222
INFO - Training [47][  100/  196]   Loss 0.350759   Top1 88.000000   Top5 98.769531   BatchTime 0.284398   LR 0.000747
0.80139482
0.80140305
0.80138397
0.80165213
0.80356187
0.80356318
0.80309361
0.80269587
0.80226660
0.80195910
0.80163461
0.80144501
0.80303258
0.80306458
0.80289221
0.80273652
0.80236530
0.80196422
0.80164212
0.80130219
0.80184925
0.80226129
INFO - Training [47][  120/  196]   Loss 0.345798   Top1 88.157552   Top5 98.850911   BatchTime 0.283252   LR 0.000742
0.80243987
0.80255342
0.80264431
0.80262589
0.80271250
0.80289215
0.80317461
0.80362397
0.80699307
0.80704153
0.80702168
0.80719453
0.80719078
0.80731326
0.80751938
0.80750626
0.80766159
0.80776316
0.80797195
INFO - Training [47][  140/  196]   Loss 0.342379   Top1 88.320312   Top5 98.903460   BatchTime 0.287007   LR 0.000737
0.80824631
0.80827701
0.80807734
0.80807853
0.80815226
0.80828363
0.80826503
0.80855197
0.80897939
0.80952346
0.80959719
0.80975962
0.80999666
0.80983752
0.80957168
0.80954868
0.80962479
0.80960000
0.80952871
INFO - Training [47][  160/  196]   Loss 0.346009   Top1 88.198242   Top5 98.884277   BatchTime 0.290879   LR 0.000732
0.80960250
0.80978829
0.80929834
0.80897510
0.80880982
0.80852181
0.80839223
0.80826402
0.80813617
0.80795515
0.80796427
0.80790007
0.80786568
0.80769616
0.80757987
0.80724436
0.80679518
0.80609161
0.80597377
0.80586272
0.80554777
0.80538803
0.80508608
INFO - Training [47][  180/  196]   Loss 0.346937   Top1 88.155382   Top5 98.819444   BatchTime 0.287646   LR 0.000727
0.80484837
0.80468827
0.80439687
0.80417687
0.80394810
0.80365413
0.80334097
0.80310452
0.80274814
0.80234909
0.80205119
0.80192691
0.80197978
INFO - ==> Top1: 88.308    Top5: 98.844    Loss: 0.343
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [47][   20/   40]   Loss 0.359230   Top1 88.417969   Top5 99.414062   BatchTime 0.120416
INFO - Validation [47][   40/   40]   Loss 0.349484   Top1 88.390000   Top5 99.570000   BatchTime 0.087894
features.0.conv.0 tensor(0.5278)
features.0.conv.3 tensor(0.2988)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0451)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0593)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0308)
features.4.conv.0 tensor(0.0330)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0985)
features.5.conv.0 tensor(0.0236)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.0666)
features.6.conv.0 tensor(0.0254)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0452)
features.7.conv.0 tensor(0.0510)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.1242)
features.8.conv.0 tensor(0.0473)
features.8.conv.3 tensor(0.1288)
features.8.conv.6 tensor(0.1355)
features.9.conv.0 tensor(0.0942)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.1406)
features.10.conv.0 tensor(0.0344)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.0718)
features.11.conv.0 tensor(0.2265)
features.11.conv.3 tensor(0.1221)
features.11.conv.6 tensor(0.5159)
features.12.conv.0 tensor(0.2468)
features.12.conv.3 tensor(0.1337)
features.12.conv.6 tensor(0.5284)
features.13.conv.0 tensor(0.0861)
features.13.conv.3 tensor(0.1451)
features.13.conv.6 tensor(0.1885)
features.14.conv.0 tensor(0.9597)
features.14.conv.3 tensor(0.1036)
features.14.conv.6 tensor(0.9531)
features.15.conv.0 tensor(0.9443)
features.15.conv.3 tensor(0.0700)
features.15.conv.6 tensor(0.9765)
features.16.conv.0 tensor(0.0996)
features.16.conv.3 tensor(0.1073)
features.16.conv.6 tensor(0.1843)
conv.0 tensor(0.1332)
tensor(849143.) 2188896.0
INFO - ==> Top1: 88.390    Top5: 99.570    Loss: 0.349
INFO - ==> Sparsity : 0.388
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
0.80162090
0.80132806
0.80117744
0.80115122
0.80107868
0.80088049
0.80074573
0.80070508
0.80200481
0.80232060
0.80219370
0.80190426
0.80148089
0.80076653
0.80033356
0.80028969
0.80022824
0.79959834
0.79930145
0.79923594
0.79923779
0.79908192
INFO - Training [48][   20/  196]   Loss 0.353933   Top1 87.988281   Top5 98.164062   BatchTime 0.313240   LR 0.000718
0.79897565
0.79886758
0.79895687
0.79885727
0.79862136
0.79845673
0.79844254
0.79850197
0.80005580
0.80017430
0.80029655
0.80030227
0.80199456
0.80202830
0.80200320
0.80171061
INFO - Training [48][   40/  196]   Loss 0.356126   Top1 88.017578   Top5 98.388672   BatchTime 0.280833   LR 0.000713
0.80160624
0.80135131
0.80104464
0.80093747
0.80061483
0.80046177
0.80072808
0.80102533
0.80122912
0.80128962
0.80138385
0.80194753
0.80351090
0.80447781
0.80411214
0.80358809
0.80323571
0.80308861
0.80299389
0.80323374
0.80318749
0.80291229
0.80307287
INFO - Training [48][   60/  196]   Loss 0.350534   Top1 88.085938   Top5 98.522135   BatchTime 0.271766   LR 0.000708
0.80318475
0.80325139
0.80320698
0.80319363
0.80311960
0.80291086
0.80287045
0.80225712
0.80227745
0.80207938
0.80174661
0.80164391
0.80166757
0.80163920
0.80157715
0.80163956
INFO - Training [48][   80/  196]   Loss 0.351202   Top1 88.100586   Top5 98.662109   BatchTime 0.267208   LR 0.000703
0.80159420
0.80158097
0.80146968
0.80110925
0.80079246
0.80072415
0.80058151
0.80086207
0.80093968
0.80284005
0.80253881
0.80230987
0.80225611
0.80212384
0.80218434
0.80227494
0.80219698
0.80212957
0.80223829
0.80226630
0.80223852
INFO - Training [48][  100/  196]   Loss 0.343675   Top1 88.300781   Top5 98.734375   BatchTime 0.271661   LR 0.000698
0.80227244
0.80231488
0.80244368
0.80274189
0.80291343
0.80287921
0.80262554
0.80259693
0.80227000
0.80230844
0.80206198
0.80175591
0.80158579
0.80143404
0.80083251
0.79995018
0.79929847
0.79883575
0.79819411
0.79756093
INFO - Training [48][  120/  196]   Loss 0.340724   Top1 88.382161   Top5 98.782552   BatchTime 0.278780   LR 0.000693
0.79698431
0.79666138
0.79625422
0.79604602
0.79691684
0.79753572
0.79809672
0.79833066
0.79871458
0.80102885
0.80083901
0.80063236
0.80049682
0.80027133
0.80023181
0.79995209
0.79974949
0.79953653
0.79940796
0.79937327
0.79934305
0.79943883
0.79956239
0.79974926
INFO - Training [48][  140/  196]   Loss 0.335564   Top1 88.551897   Top5 98.836496   BatchTime 0.285656   LR 0.000688
0.80141616
0.80329758
0.80327356
0.80342621
0.80357552
0.80366510
0.80374628
0.80365962
0.80371094
0.80385500
0.80383235
0.80372351
0.80352873
0.80332273
0.80309325
0.80296707
0.80275673
0.80256999
0.80246097
INFO - Training [48][  160/  196]   Loss 0.338835   Top1 88.491211   Top5 98.813477   BatchTime 0.288694   LR 0.000683
0.80226624
0.80209213
0.80393523
0.80434686
0.80378467
0.80301130
0.80233836
0.80181628
0.80130374
0.80079895
0.80002975
0.79941881
0.79905373
0.79853600
0.79725683
0.79702342
0.79738855
0.79827595
0.79900998
0.79933536
INFO - Training [48][  180/  196]   Loss 0.338833   Top1 88.478733   Top5 98.739149   BatchTime 0.291372   LR 0.000678
0.80002838
0.80064410
0.80115557
0.80307186
0.80307770
0.80319804
0.80331761
0.80313325
0.80317128
0.80289322
0.80279094
0.80293411
0.80306274
0.80315965
0.80321288
********************pre-trained*****************
INFO - ==> Top1: 88.512    Top5: 98.740    Loss: 0.339
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [48][   20/   40]   Loss 0.359975   Top1 89.277344   Top5 99.609375   BatchTime 0.130752
INFO - Validation [48][   40/   40]   Loss 0.343832   Top1 89.550000   Top5 99.690000   BatchTime 0.093671
INFO - ==> Top1: 89.550    Top5: 99.690    Loss: 0.344
INFO - ==> Sparsity : 0.387
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.050   Top5: 99.690]
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.3008)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0434)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0599)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0340)
features.4.conv.3 tensor(0.1036)
features.4.conv.6 tensor(0.0864)
features.5.conv.0 tensor(0.0299)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0869)
features.6.conv.0 tensor(0.0260)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0455)
features.7.conv.0 tensor(0.0518)
features.7.conv.3 tensor(0.1071)
features.7.conv.6 tensor(0.1176)
features.8.conv.0 tensor(0.0530)
features.8.conv.3 tensor(0.1291)
features.8.conv.6 tensor(0.0903)
features.9.conv.0 tensor(0.0901)
features.9.conv.3 tensor(0.1455)
features.9.conv.6 tensor(0.1710)
features.10.conv.0 tensor(0.0345)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.0691)
features.11.conv.0 tensor(0.1789)
features.11.conv.3 tensor(0.1231)
features.11.conv.6 tensor(0.4782)
features.12.conv.0 tensor(0.2254)
features.12.conv.3 tensor(0.1277)
features.12.conv.6 tensor(0.5686)
features.13.conv.0 tensor(0.0832)
features.13.conv.3 tensor(0.1447)
features.13.conv.6 tensor(0.1905)
features.14.conv.0 tensor(0.9602)
features.14.conv.3 tensor(0.1042)
features.14.conv.6 tensor(0.9487)
features.15.conv.0 tensor(0.9451)
features.15.conv.3 tensor(0.0700)
features.15.conv.6 tensor(0.9724)
features.16.conv.0 tensor(0.1042)
features.16.conv.3 tensor(0.1103)
features.16.conv.6 tensor(0.1877)
conv.0 tensor(0.1382)
tensor(847744.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
0.80326521
0.80331790
0.80337572
0.80346221
0.80351400
0.80368024
0.80377185
0.80369163
0.80372578
0.80372196
0.80350286
0.80349350
0.80361611
0.80375409
0.80368650
0.80374068
0.80380875
0.80347252
0.80366158
INFO - Training [49][   20/  196]   Loss 0.354441   Top1 87.988281   Top5 98.320312   BatchTime 0.359186   LR 0.000669
0.80349636
0.80314010
0.80336893
0.80345768
0.80311257
0.80287212
0.80288363
0.80281347
0.80258358
0.80208206
0.80196863
0.80182713
0.80180913
0.80183488
0.80163902
0.80132109
0.80117130
0.80092162
0.80063933
0.80033994
0.80030113
0.80004448
0.79994440
INFO - Training [49][   40/  196]   Loss 0.348656   Top1 88.017578   Top5 98.564453   BatchTime 0.355268   LR 0.000664
0.79992491
0.79992640
0.79972941
0.79947305
0.79955357
0.79961717
0.79955900
0.79945987
0.79938519
0.79935956
0.79943997
0.79969877
0.80080694
0.80092359
0.80055708
0.80044097
INFO - Training [49][   60/  196]   Loss 0.345331   Top1 88.125000   Top5 98.587240   BatchTime 0.356999   LR 0.000659
0.80042762
0.80062586
0.80040509
0.80036366
0.80027145
0.80032068
0.80141652
0.80283016
0.80275446
0.80276126
0.80270529
0.80261630
0.80193001
0.80187905
0.80217785
0.80229527
0.80242485
0.80237371
0.80228347
0.80199265
0.80172211
0.80148196
0.80145162
INFO - Training [49][   80/  196]   Loss 0.346049   Top1 88.100586   Top5 98.715820   BatchTime 0.356526   LR 0.000654
0.80142063
0.80137104
0.80125958
0.80117697
0.80108440
0.80090117
0.80096245
0.80078369
0.80065739
0.80104774
0.80150872
0.80153769
0.80140692
0.80118626
0.80074972
0.80031431
0.79980540
0.79872900
0.79862100
0.79833126
INFO - Training [49][  100/  196]   Loss 0.341866   Top1 88.222656   Top5 98.765625   BatchTime 0.362359   LR 0.000649
0.79756075
0.79730600
0.79681307
0.79653764
0.79643834
0.79614168
0.79569805
0.79543686
0.79534817
0.79499501
0.79434663
0.79388666
0.79360455
0.79380852
0.79853576
0.79835129
INFO - Training [49][  120/  196]   Loss 0.333172   Top1 88.531901   Top5 98.857422   BatchTime 0.365606   LR 0.000644
0.79808974
0.79780948
0.79770887
0.79761696
0.79873699
0.79861808
0.79852819
0.79832482
0.79817080
0.79822516
0.79819894
0.79834849
0.79890484
0.79929346
0.79984802
0.80035669
0.80092520
0.80132717
0.80157191
0.80194974
0.80217505
0.80219156
INFO - Training [49][  140/  196]   Loss 0.332196   Top1 88.526786   Top5 98.909040   BatchTime 0.364295   LR 0.000639
0.80353922
0.80409122
0.80373955
0.80295080
0.80249000
0.80225331
0.80197769
0.80157906
0.80128324
0.80097860
0.80068463
0.80047119
0.80016643
0.80025518
0.80046356
0.80081147
0.80112422
0.80169731
0.80225128
0.80224937
0.80202115
0.80207759
INFO - Training [49][  160/  196]   Loss 0.334640   Top1 88.483887   Top5 98.894043   BatchTime 0.364184   LR 0.000634
0.80216271
0.80234605
0.80255574
0.80279052
0.80264461
0.80272472
0.80279326
0.80289990
0.80297244
0.80331409
0.80333686
0.80327427
0.80355442
0.80354446
0.80358601
0.80338526
0.80303979
INFO - Training [49][  180/  196]   Loss 0.335529   Top1 88.454861   Top5 98.845486   BatchTime 0.364743   LR 0.000629
0.80309534
0.80296099
0.80288547
0.80319452
0.80382508
0.80451459
0.80448079
0.80433983
0.80410671
0.80389744
0.80383497
0.80388504
0.80511105
0.80550152
0.80500835
0.80457711
INFO - ==> Top1: 88.436    Top5: 98.852    Loss: 0.335
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80453134
0.80420786
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [49][   20/   40]   Loss 0.320796   Top1 90.312500   Top5 99.687500   BatchTime 0.125542
INFO - Validation [49][   40/   40]   Loss 0.301251   Top1 90.470000   Top5 99.760000   BatchTime 0.089427
INFO - ==> Top1: 90.470    Top5: 99.760    Loss: 0.301
INFO - ==> Sparsity : 0.387
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.3125)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0434)
features.2.conv.0 tensor(0.0211)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0596)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0293)
features.4.conv.0 tensor(0.0339)
features.4.conv.3 tensor(0.1071)
features.4.conv.6 tensor(0.0820)
features.5.conv.0 tensor(0.0259)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0874)
features.6.conv.0 tensor(0.0254)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0450)
features.7.conv.0 tensor(0.0542)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.0876)
features.8.conv.0 tensor(0.0493)
features.8.conv.3 tensor(0.1270)
features.8.conv.6 tensor(0.1171)
features.9.conv.0 tensor(0.0891)
features.9.conv.3 tensor(0.1461)
features.9.conv.6 tensor(0.1652)
features.10.conv.0 tensor(0.0310)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0716)
features.11.conv.0 tensor(0.1686)
features.11.conv.3 tensor(0.1227)
features.11.conv.6 tensor(0.4637)
features.12.conv.0 tensor(0.2483)
features.12.conv.3 tensor(0.1289)
features.12.conv.6 tensor(0.5339)
features.13.conv.0 tensor(0.0909)
features.13.conv.3 tensor(0.1460)
features.13.conv.6 tensor(0.1895)
features.14.conv.0 tensor(0.9593)
features.14.conv.3 tensor(0.1034)
features.14.conv.6 tensor(0.9450)
features.15.conv.0 tensor(0.9450)
features.15.conv.3 tensor(0.0694)
features.15.conv.6 tensor(0.9682)
features.16.conv.0 tensor(0.1033)
features.16.conv.3 tensor(0.1073)
features.16.conv.6 tensor(0.1987)
conv.0 tensor(0.1350)
tensor(846256.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
0.80435932
0.80441475
0.80432457
0.80438149
0.80433345
0.80420136
0.80384046
0.80356854
0.80320740
0.80314487
0.80334437
0.80341649
0.80407554
0.80699164
0.80694139
0.80667877
0.80643260
0.80602264
0.80593669
0.80614358
0.80627966
0.80632728
INFO - Training [50][   20/  196]   Loss 0.345144   Top1 88.398438   Top5 98.535156   BatchTime 0.399863   LR 0.000620
0.80609363
0.80610615
0.80591083
0.80582172
0.80554426
0.80527431
0.80522728
0.80549586
0.80549586
0.80514461
0.80508971
0.80484492
0.80458826
0.80417889
0.80389315
INFO - Training [50][   40/  196]   Loss 0.359863   Top1 87.763672   Top5 98.505859   BatchTime 0.340747   LR 0.000615
0.80354577
0.80318516
0.80322230
0.80286914
0.80260009
0.80236155
0.80212837
0.80178934
0.80144083
0.80117661
0.80108565
0.80086571
0.80083722
0.80039787
0.80022603
0.80008370
0.79979974
0.79968858
0.79963505
0.79959375
0.79949546
0.80060321
INFO - Training [50][   60/  196]   Loss 0.343513   Top1 88.229167   Top5 98.717448   BatchTime 0.349724   LR 0.000610
0.80079353
0.80054247
0.80048084
0.80002320
0.79975563
0.79951859
0.79929286
0.79896355
0.79888666
0.79856855
0.79803735
0.79742616
0.79734278
0.79713672
0.79689068
0.79664904
0.79640126
0.79631144
0.79630476
0.79611588
0.79566258
INFO - Training [50][   80/  196]   Loss 0.340622   Top1 88.305664   Top5 98.872070   BatchTime 0.355930   LR 0.000605
0.79567105
0.79580295
0.79576808
0.79546541
0.79556870
0.79576325
0.79539651
0.79525006
0.79522949
0.79518837
0.79524028
0.79641509
0.79642761
0.79620939
0.79591906
0.79545939
0.79508388
0.79474145
0.79448175
0.79421377
0.79403967
0.79396915
INFO - Training [50][  100/  196]   Loss 0.336426   Top1 88.589844   Top5 98.878906   BatchTime 0.359230   LR 0.000600
0.79380846
0.79381537
0.79388005
0.79385597
0.79424232
0.79588401
0.79638201
0.79576069
0.79552722
0.79723316
0.79715776
0.79644978
0.79574573
0.79552633
0.79496574
0.79424912
INFO - Training [50][  120/  196]   Loss 0.329508   Top1 88.779297   Top5 98.958333   BatchTime 0.361922   LR 0.000595
0.79439789
0.79452229
0.79435414
0.79454297
0.79470241
0.79472238
0.79454225
0.79449135
0.79455215
0.79491383
0.79519969
0.79520738
0.79545736
0.79559743
0.80113691
0.80042768
0.80027235
0.80055964
0.80086237
0.80110300
0.80133373
0.80166173
INFO - Training [50][  140/  196]   Loss 0.328583   Top1 88.755580   Top5 98.978795   BatchTime 0.361127   LR 0.000590
0.80206370
0.80408406
0.80395931
0.80378640
0.80370861
0.80363458
0.80363590
0.80373830
0.80388278
0.80444181
0.80598563
0.80573219
0.80563337
0.80558944
0.80515498
0.80474025
0.80454391
INFO - Training [50][  160/  196]   Loss 0.331421   Top1 88.696289   Top5 98.955078   BatchTime 0.359807   LR 0.000585
0.80434763
0.80420828
0.80398053
0.80397224
0.80404705
0.80409938
0.80393708
0.80394500
0.80393654
0.80361050
0.80358058
0.80336171
0.80330479
0.80318719
0.80302989
0.80319369
0.80342025
0.80341274
0.80310214
0.80294865
0.80285859
0.80285442
INFO - Training [50][  180/  196]   Loss 0.332994   Top1 88.619792   Top5 98.912760   BatchTime 0.360466   LR 0.000580
0.80284053
0.80301082
0.80294341
0.80306560
0.80318362
0.80299103
0.80279714
0.80273229
0.80253333
0.80201960
0.80140531
0.80123836
0.80120724
0.80105257
0.80092317
INFO - ==> Top1: 88.728    Top5: 98.916    Loss: 0.331
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80089194
0.80107307
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [50][   20/   40]   Loss 0.310283   Top1 89.902344   Top5 99.570312   BatchTime 0.123791
INFO - Validation [50][   40/   40]   Loss 0.300859   Top1 89.920000   Top5 99.690000   BatchTime 0.089582
INFO - ==> Top1: 89.920    Top5: 99.690    Loss: 0.301
INFO - ==> Sparsity : 0.418
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.3047)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0460)
features.2.conv.0 tensor(0.0208)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0567)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.0312)
features.4.conv.0 tensor(0.0321)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0840)
features.5.conv.0 tensor(0.0282)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0911)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0450)
features.7.conv.0 tensor(0.0484)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.0933)
features.8.conv.0 tensor(0.0494)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.1013)
features.9.conv.0 tensor(0.0862)
features.9.conv.3 tensor(0.1484)
features.9.conv.6 tensor(0.1444)
features.10.conv.0 tensor(0.0365)
features.10.conv.3 tensor(0.0975)
features.10.conv.6 tensor(0.0701)
features.11.conv.0 tensor(0.3435)
features.11.conv.3 tensor(0.1231)
features.11.conv.6 tensor(0.4874)
features.12.conv.0 tensor(0.2217)
features.12.conv.3 tensor(0.1279)
features.12.conv.6 tensor(0.5345)
features.13.conv.0 tensor(0.0887)
features.13.conv.3 tensor(0.1501)
features.13.conv.6 tensor(0.1979)
features.14.conv.0 tensor(0.9607)
features.14.conv.3 tensor(0.1038)
features.14.conv.6 tensor(0.9493)
features.15.conv.0 tensor(0.9459)
features.15.conv.3 tensor(0.0682)
features.15.conv.6 tensor(0.9738)
features.16.conv.0 tensor(0.1163)
features.16.conv.3 tensor(0.1089)
features.16.conv.6 tensor(0.2025)
conv.0 tensor(0.2662)
tensor(914434.) 2188896.0
0.80085164
0.80020803
0.80022788
0.80052185
0.80070436
0.80071759
0.80073142
0.80058545
0.80042225
0.80030215
0.80043525
0.80039412
0.80036622
0.80045646
0.80025131
0.79996353
0.79977989
INFO - Training [51][   20/  196]   Loss 0.322913   Top1 88.828125   Top5 98.613281   BatchTime 0.424427   LR 0.000571
0.79925799
0.79910624
0.79844904
0.79812151
0.79765260
0.79724175
0.79720205
0.79736763
0.79720879
0.79689270
0.79677814
0.79692113
0.79663986
0.79641408
0.79646164
0.79652268
0.79676485
0.79677659
0.79705286
0.79711378
0.79703802
0.79707783
0.79697943
0.79711908
INFO - Training [51][   40/  196]   Loss 0.337557   Top1 88.398438   Top5 98.681641   BatchTime 0.380140   LR 0.000566
0.79707980
0.79689330
0.79705662
0.79848367
0.79865694
0.79866278
0.79856950
0.79857761
0.79891139
0.80015075
0.80025291
0.80023718
0.80044705
0.80063158
0.80067122
0.80070293
0.80072010
INFO - Training [51][   60/  196]   Loss 0.333427   Top1 88.580729   Top5 98.723958   BatchTime 0.368910   LR 0.000561
0.80130810
0.80156684
0.80162519
0.80182749
0.80199736
0.80199200
0.80199772
0.80218261
0.80205685
0.80204129
0.80152029
0.80103022
0.80062443
0.80092824
0.80089694
0.80075109
0.80221021
0.80155808
0.80124736
0.80106211
0.80081803
0.80067688
0.80020970
INFO - Training [51][   80/  196]   Loss 0.329352   Top1 88.715820   Top5 98.852539   BatchTime 0.365047   LR 0.000556
0.80022544
0.80022991
0.80017263
0.80002517
0.79966629
0.79953873
0.79939038
0.79902774
0.79881018
0.79848456
0.79806203
0.79761225
0.79697365
0.79704124
0.79707390
0.79707074
0.79681093
0.79680276
0.79706675
0.79733497
0.79714572
INFO - Training [51][  100/  196]   Loss 0.323669   Top1 88.949219   Top5 98.867188   BatchTime 0.366626   LR 0.000551
0.79702640
0.79689664
0.79669094
0.79647088
0.79629368
0.79616815
0.79614365
0.79623520
0.79612851
0.79596466
0.79581249
0.79558182
0.79548734
0.79739738
0.79743218
0.79711062
INFO - Training [51][  120/  196]   Loss 0.319025   Top1 89.072266   Top5 98.948568   BatchTime 0.368129   LR 0.000546
0.79696393
0.79691607
0.79699522
0.79698783
0.79700291
0.79706365
0.79698688
0.79673809
0.79657918
0.79653496
0.79632908
0.79609579
0.79598373
0.79569387
0.79537672
0.79485959
0.79462874
0.79437500
0.79418576
0.79394883
0.79368836
0.79362184
INFO - Training [51][  140/  196]   Loss 0.319122   Top1 89.042969   Top5 99.012277   BatchTime 0.366946   LR 0.000541
0.79323447
0.79288989
0.79275489
0.79276472
0.79278630
0.79278409
0.79276550
0.79316497
0.79330266
0.79382139
0.79431349
0.79541361
0.79846132
0.79864937
0.79837036
0.79861337
0.79874635
0.80131316
0.80146849
0.80113369
0.80083901
INFO - Training [51][  160/  196]   Loss 0.323023   Top1 88.906250   Top5 98.996582   BatchTime 0.370050   LR 0.000536
0.80038100
0.80023640
0.80026013
0.80032796
0.80029780
0.80030549
0.80021006
0.80011404
0.79987067
0.79963696
0.79957491
0.79931527
0.79905456
0.79916656
0.79928398
0.79905981
0.79896128
0.79889929
0.79887187
INFO - Training [51][  180/  196]   Loss 0.323742   Top1 88.847656   Top5 98.951823   BatchTime 0.374761   LR 0.000531
0.79885918
0.79895371
0.79905576
0.79915285
0.79915988
0.79916608
0.79928845
0.79947901
0.79940450
0.79920012
0.79909915
0.79903221
0.79882884
0.79868531
0.79838181
0.79832041
INFO - ==> Top1: 88.804    Top5: 98.930    Loss: 0.324
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [51][   20/   40]   Loss 0.333392   Top1 89.667969   Top5 99.511719   BatchTime 0.121562
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.3008)
features.1.conv.0 tensor(0.0254)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0456)
features.2.conv.0 tensor(0.0278)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0564)
features.3.conv.0 tensor(0.0197)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0319)
features.4.conv.0 tensor(0.0332)
features.4.conv.3 tensor(0.1094)
features.4.conv.6 tensor(0.0833)
features.5.conv.0 tensor(0.0282)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.1081)
features.6.conv.0 tensor(0.0249)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0443)
features.7.conv.0 tensor(0.0500)
features.7.conv.3 tensor(0.1105)
features.7.conv.6 tensor(0.1084)
features.8.conv.0 tensor(0.0500)
features.8.conv.3 tensor(0.1264)
features.8.conv.6 tensor(0.0824)
features.9.conv.0 tensor(0.0955)
features.9.conv.3 tensor(0.1418)
features.9.conv.6 tensor(0.1725)
features.10.conv.0 tensor(0.0398)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.0726)
features.11.conv.0 tensor(0.2414)
features.11.conv.3 tensor(0.1262)
features.11.conv.6 tensor(0.5010)
features.12.conv.0 tensor(0.2386)
features.12.conv.3 tensor(0.1291)
features.12.conv.6 tensor(0.5702)
features.13.conv.0 tensor(0.0875)
features.13.conv.3 tensor(0.1489)
features.13.conv.6 tensor(0.2012)
features.14.conv.0 tensor(0.9605)
features.14.conv.3 tensor(0.1034)
features.14.conv.6 tensor(0.9619)
features.15.conv.0 tensor(0.9461)
features.15.conv.3 tensor(0.0681)
features.15.conv.6 tensor(0.9776)
features.16.conv.0 tensor(0.1067)
features.16.conv.3 tensor(0.1081)
features.16.conv.6 tensor(0.2520)
conv.0 tensor(0.1303)
tensor(874417.) 2188896.0
INFO - Validation [51][   40/   40]   Loss 0.317426   Top1 89.860000   Top5 99.670000   BatchTime 0.088574
INFO - ==> Top1: 89.860    Top5: 99.670    Loss: 0.317
INFO - ==> Sparsity : 0.399
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
0.79773432
0.79727858
0.79704183
0.79684132
0.79650551
0.79606336
0.79576474
0.79558313
0.79557949
0.79540026
0.79532766
0.79549265
0.79553354
0.79523236
0.79536754
0.79503417
0.79468322
0.79447299
0.79419315
INFO - Training [52][   20/  196]   Loss 0.349121   Top1 87.851562   Top5 98.359375   BatchTime 0.436261   LR 0.000523
0.79385269
0.79376793
0.79376292
0.79373854
0.79356724
0.79353857
0.79349196
0.79339081
0.79319245
0.79312170
0.79296643
0.79274505
0.79272687
0.79275662
0.79278213
0.79293376
0.79288340
0.79292041
0.79307497
0.79336393
0.79351610
0.79349774
INFO - Training [52][   40/  196]   Loss 0.346595   Top1 87.773438   Top5 98.613281   BatchTime 0.400210   LR 0.000518
0.79361475
0.79372406
0.79396510
0.79402208
0.79560351
0.79597765
0.79576832
0.79562062
0.79547560
0.79538566
0.79512882
0.79493737
0.79461044
0.79422891
0.79442376
0.79429883
INFO - Training [52][   60/  196]   Loss 0.344967   Top1 88.059896   Top5 98.691406   BatchTime 0.387116   LR 0.000513
0.79399496
0.79350358
0.79301125
0.79307491
0.79318100
0.79283065
0.79229516
0.79229403
0.79230028
0.79227954
0.79226667
0.79211026
0.79203743
0.79199100
0.79185730
0.79182166
0.79179221
0.79180390
0.79178560
0.79179215
INFO - Training [52][   80/  196]   Loss 0.341309   Top1 88.208008   Top5 98.813477   BatchTime 0.364174   LR 0.000508
0.79427367
0.79451710
0.79447132
0.79421848
0.79393584
0.79383850
0.79371446
0.79366380
0.79351729
0.79344988
0.79331261
0.79302180
0.79287916
0.79285264
0.79282862
0.79271954
0.79263526
0.79252976
0.79254597
0.79260159
0.79264235
0.79281402
0.79603338
0.79602849
INFO - Training [52][  100/  196]   Loss 0.333420   Top1 88.531250   Top5 98.800781   BatchTime 0.360712   LR 0.000503
0.79603118
0.79606450
0.79604381
0.79596359
0.79595196
0.79592776
0.79594159
0.79594904
0.79591459
0.79593599
0.79594469
0.79593223
0.79598844
0.79708838
0.79800135
0.79799968
INFO - Training [52][  120/  196]   Loss 0.326113   Top1 88.873698   Top5 98.893229   BatchTime 0.361349   LR 0.000498
0.79794055
0.79776943
0.79788995
0.79794019
0.79794043
0.79766530
0.79753643
0.79769808
0.79748994
0.79742181
0.79730755
0.79727882
0.79724252
0.79711026
0.79715377
0.79677200
0.79674155
0.79672402
0.79667169
0.79656547
0.79645628
0.79647952
INFO - Training [52][  140/  196]   Loss 0.326892   Top1 88.783482   Top5 98.920201   BatchTime 0.361004   LR 0.000493
0.79670101
0.79715133
0.79743379
0.79769021
0.79765123
0.79724288
0.79711878
0.79681361
0.79620451
0.79573470
0.79540247
0.79534441
0.79510969
0.79460418
0.79431760
0.79388136
0.79352826
0.79307514
0.79254633
0.79201025
0.79180056
0.79143250
0.79120153
INFO - Training [52][  160/  196]   Loss 0.326400   Top1 88.845215   Top5 98.913574   BatchTime 0.360571   LR 0.000488
0.79098296
0.79077071
0.79053032
0.79029375
0.79005605
0.78980637
0.78971857
0.78943431
0.78926086
0.78896385
0.78883982
0.78859681
0.78828979
0.78808790
0.78821307
0.78842455
INFO - Training [52][  180/  196]   Loss 0.326315   Top1 88.836806   Top5 98.891059   BatchTime 0.361994   LR 0.000483
0.78858578
0.78872412
0.78891927
0.78890330
0.78871983
0.78878808
0.78857452
0.78832233
0.78792006
0.78762805
0.78758335
0.78738421
0.78685719
0.78632283
0.78585637
0.78523713
INFO - ==> Top1: 88.826    Top5: 98.904    Loss: 0.326
0.78439736
0.78371674
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [52][   20/   40]   Loss 0.333418   Top1 89.082031   Top5 99.531250   BatchTime 0.123796
INFO - Validation [52][   40/   40]   Loss 0.325737   Top1 89.430000   Top5 99.590000   BatchTime 0.086893
INFO - ==> Top1: 89.430    Top5: 99.590    Loss: 0.326
INFO - ==> Sparsity : 0.411
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 90.100   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.3066)
features.1.conv.0 tensor(0.0221)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0456)
features.2.conv.0 tensor(0.0281)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0544)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0330)
features.4.conv.0 tensor(0.0304)
features.4.conv.3 tensor(0.1071)
features.4.conv.6 tensor(0.1164)
features.5.conv.0 tensor(0.0322)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0964)
features.6.conv.0 tensor(0.0221)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0473)
features.7.conv.0 tensor(0.0524)
features.7.conv.3 tensor(0.1062)
features.7.conv.6 tensor(0.1029)
features.8.conv.0 tensor(0.0556)
features.8.conv.3 tensor(0.1296)
features.8.conv.6 tensor(0.1026)
features.9.conv.0 tensor(0.0971)
features.9.conv.3 tensor(0.1447)
features.9.conv.6 tensor(0.2632)
features.10.conv.0 tensor(0.0437)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0870)
features.11.conv.0 tensor(0.5041)
features.11.conv.3 tensor(0.1236)
features.11.conv.6 tensor(0.5103)
features.12.conv.0 tensor(0.2607)
features.12.conv.3 tensor(0.1310)
features.12.conv.6 tensor(0.5502)
features.13.conv.0 tensor(0.0930)
features.13.conv.3 tensor(0.1518)
features.13.conv.6 tensor(0.2147)
features.14.conv.0 tensor(0.9616)
features.14.conv.3 tensor(0.1030)
features.14.conv.6 tensor(0.9524)
features.15.conv.0 tensor(0.9472)
features.15.conv.3 tensor(0.0689)
features.15.conv.6 tensor(0.9810)
features.16.conv.0 tensor(0.1109)
features.16.conv.3 tensor(0.1078)
features.16.conv.6 tensor(0.2714)
conv.0 tensor(0.1284)
tensor(899964.) 2188896.0
0.78285533
0.78228223
0.78211290
0.78146803
0.78174096
0.78118223
0.78058779
0.78053033
0.78065526
0.78087288
0.78102750
0.78088474
0.78070486
0.78061759
0.78062588
0.78057164
0.78054625
0.78018552
0.77993876
0.77944851
0.77877176
INFO - Training [53][   20/  196]   Loss 0.331182   Top1 88.828125   Top5 98.535156   BatchTime 0.461955   LR 0.000474
0.77843022
0.77778518
0.77701378
0.77697271
0.77678770
0.77563459
0.77412450
0.77197063
0.77032638
0.76784796
0.76499975
0.76367646
0.76345736
0.76216316
0.75972652
0.75871617
0.75955266
0.76110697
0.76452905
0.76768023
INFO - Training [53][   40/  196]   Loss 0.328379   Top1 88.964844   Top5 98.652344   BatchTime 0.421044   LR 0.000470
0.77196848
0.77611560
0.78154922
0.78306270
0.78402162
0.78667104
0.78657109
0.78687561
0.78689998
0.78688014
0.78689981
0.78711385
0.78724331
0.78699672
0.78680778
0.78667665
0.78685367
0.78678310
0.78695148
0.78682607
0.78686351
0.78666246
INFO - Training [53][   60/  196]   Loss 0.327903   Top1 88.906250   Top5 98.697917   BatchTime 0.411936   LR 0.000465
0.78649819
0.78641194
0.78627622
0.78598183
0.78598362
0.78566372
0.78554332
0.78547251
0.78530735
0.78525418
0.78522450
0.78483540
0.78476357
0.78671813
0.78719968
0.78722429
0.78754961
0.78740990
INFO - Training [53][   80/  196]   Loss 0.327369   Top1 88.930664   Top5 98.837891   BatchTime 0.390595   LR 0.000460
0.78736478
0.78767163
0.78779370
0.78800178
0.78802383
0.78813350
0.78834510
0.78879946
0.79117942
0.79114479
0.79108006
0.79101753
0.79086775
0.79081291
0.79068959
0.79053611
0.79053050
0.79059708
0.79056013
0.79063720
INFO - Training [53][  100/  196]   Loss 0.318912   Top1 89.152344   Top5 98.863281   BatchTime 0.369863   LR 0.000455
0.79088962
0.79050678
0.79077876
0.79068291
0.79047865
0.79026306
0.79010892
0.78999436
0.78971672
0.78932339
0.78915155
0.78946090
0.78886241
0.78888839
0.78862095
0.78835887
INFO - Training [53][  120/  196]   Loss 0.314892   Top1 89.270833   Top5 98.942057   BatchTime 0.371158   LR 0.000450
0.78826672
0.78845775
0.78852212
0.78818721
0.78800815
0.78799564
0.78782690
0.78794402
0.78817523
0.78863335
0.78863883
0.78902674
0.78933704
0.78964001
0.78973770
0.79005903
0.79167795
0.79160953
0.79348135
0.79346782
0.79292285
0.79257065
0.79231858
INFO - Training [53][  140/  196]   Loss 0.313495   Top1 89.347098   Top5 99.009487   BatchTime 0.368545   LR 0.000445
0.79215676
0.79197490
0.79208958
0.79175299
0.79194093
0.79165906
0.79157752
0.79162151
0.79182458
0.79163778
0.79165167
0.79104489
0.79104203
0.79059237
0.79065609
0.79062206
0.78983116
0.78899497
0.78851730
0.78809726
0.78768951
0.78729564
INFO - Training [53][  160/  196]   Loss 0.313109   Top1 89.301758   Top5 99.013672   BatchTime 0.367083   LR 0.000441
0.78699625
0.78681242
0.78690690
0.78650588
0.78617340
0.78581285
0.78561586
0.78542292
0.78532177
0.78520352
0.78493220
0.78481883
0.78463399
0.78468633
0.78478169
0.78519738
INFO - Training [53][  180/  196]   Loss 0.316862   Top1 89.205729   Top5 98.960503   BatchTime 0.367051   LR 0.000436
0.78558153
0.78564781
0.78602093
0.78773445
0.78869015
0.78855079
0.78820527
0.78828454
0.78812313
0.78796709
0.78788477
0.78781486
0.78795087
0.78786093
0.78801686
0.78778344
0.78782046
INFO - ==> Top1: 89.196    Top5: 98.964    Loss: 0.317
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.78781313
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [53][   20/   40]   Loss 0.307161   Top1 90.585938   Top5 99.628906   BatchTime 0.121969
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3027)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0447)
features.2.conv.0 tensor(0.0266)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0570)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0304)
features.4.conv.3 tensor(0.1071)
features.4.conv.6 tensor(0.0843)
features.5.conv.0 tensor(0.0317)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0939)
features.6.conv.0 tensor(0.0179)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0554)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.2165)
features.8.conv.0 tensor(0.0534)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.1120)
features.9.conv.0 tensor(0.1049)
features.9.conv.3 tensor(0.1441)
features.9.conv.6 tensor(0.2413)
features.10.conv.0 tensor(0.0417)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0387)
features.11.conv.0 tensor(0.3510)
features.11.conv.3 tensor(0.1221)
features.11.conv.6 tensor(0.5144)
features.12.conv.0 tensor(0.2498)
features.12.conv.3 tensor(0.1302)
features.12.conv.6 tensor(0.5575)
features.13.conv.0 tensor(0.0920)
features.13.conv.3 tensor(0.1508)
features.13.conv.6 tensor(0.2253)
features.14.conv.0 tensor(0.9609)
features.14.conv.3 tensor(0.1027)
features.14.conv.6 tensor(0.9456)
features.15.conv.0 tensor(0.9475)
features.15.conv.3 tensor(0.0678)
features.15.conv.6 tensor(0.9796)
features.16.conv.0 tensor(0.1085)
features.16.conv.3 tensor(0.1082)
features.16.conv.6 tensor(0.2306)
conv.0 tensor(0.1299)
tensor(879371.) 2188896.0
INFO - Validation [53][   40/   40]   Loss 0.290741   Top1 90.400000   Top5 99.730000   BatchTime 0.088274
INFO - ==> Top1: 90.400    Top5: 99.730    Loss: 0.291
INFO - ==> Sparsity : 0.402
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.260   Top5: 99.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
0.78773826
0.78755349
0.78736216
0.78728932
0.78722835
0.78724712
0.78725231
0.78697294
0.78710330
0.78741652
0.78737795
0.78754902
0.78755188
0.78739303
0.78727287
0.78724337
0.78706133
0.78673077
INFO - Training [54][   20/  196]   Loss 0.317820   Top1 88.808594   Top5 98.554688   BatchTime 0.430618   LR 0.000427
0.78650069
0.78662568
0.78597337
0.78574121
0.78564763
0.78554398
0.78553516
0.78568166
0.78563040
0.78575760
0.78578299
0.78587914
0.78609431
0.78581542
0.78567910
0.78563255
0.78551346
0.78549385
0.78549623
0.78574234
0.78579837
INFO - Training [54][   40/  196]   Loss 0.335416   Top1 88.330078   Top5 98.808594   BatchTime 0.413049   LR 0.000423
0.78598797
0.78582340
0.78572416
0.78580815
0.78585792
0.78596264
0.78597909
0.78605157
0.78618622
0.78606200
0.78628695
0.78870118
0.78833771
0.78836268
0.78825128
0.78804529
0.78797710
0.78805745
0.78823727
0.78807259
0.78764069
0.78729123
INFO - Training [54][   60/  196]   Loss 0.331709   Top1 88.463542   Top5 98.867188   BatchTime 0.395164   LR 0.000418
0.78706604
0.78666514
0.78633463
0.78604943
0.78591967
0.78606284
0.78629661
0.78629315
0.78619629
0.78634810
0.78638726
0.78652620
0.78662395
0.78654736
0.78626788
0.78588688
0.78571564
0.78578073
0.78576052
0.78577548
0.78579092
0.78566682
INFO - Training [54][   80/  196]   Loss 0.327031   Top1 88.696289   Top5 98.984375   BatchTime 0.389684   LR 0.000413
0.78557444
0.78549141
0.78534704
0.78477675
0.78396350
0.78382415
0.78349072
0.78345639
0.78336197
0.78312224
0.78297639
0.78280544
0.78260702
0.78241330
0.78243309
0.78235680
0.78208619
0.78187817
INFO - Training [54][  100/  196]   Loss 0.320969   Top1 88.929688   Top5 99.027344   BatchTime 0.375450   LR 0.000408
0.78165054
0.78137165
0.78101712
0.78061062
0.78031784
0.78018492
0.78024441
0.78049976
0.78061950
0.78079569
0.78089404
0.78088415
0.78101689
0.78129280
0.78184801
0.78265363
0.78396279
0.78368235
0.78352982
0.78330344
0.78298426
INFO - Training [54][  120/  196]   Loss 0.315820   Top1 89.153646   Top5 99.062500   BatchTime 0.361751   LR 0.000404
0.78261125
0.78233641
0.78205025
0.78187239
0.78155065
0.78134310
0.78136152
0.78105748
0.78105992
0.78077984
0.78078943
0.78075695
0.78073001
0.78063953
0.78107947
0.78285778
0.78321546
0.78323108
0.78318369
INFO - Training [54][  140/  196]   Loss 0.313428   Top1 89.229911   Top5 99.098772   BatchTime 0.367297   LR 0.000399
0.78304964
0.78310633
0.78296500
0.78288549
0.78281081
0.78291893
0.78286809
0.78301275
0.78272259
0.78273356
0.78265435
0.78248149
0.78213757
0.78188831
0.78201890
0.78197461
0.78215730
0.78252906
0.78237683
0.78218555
INFO - Training [54][  160/  196]   Loss 0.314668   Top1 89.204102   Top5 99.072266   BatchTime 0.371602   LR 0.000394
0.78207189
0.78186470
0.78170526
0.78148836
0.78141701
0.78104734
0.78092712
0.78074223
0.78065979
0.78067905
0.78054976
0.78064781
0.78039867
0.78040087
0.78026217
0.77999699
0.77983683
0.77977383
0.77972502
0.77948707
INFO - Training [54][  180/  196]   Loss 0.313611   Top1 89.251302   Top5 99.042969   BatchTime 0.375572   LR 0.000390
0.77929455
0.77926278
0.77918774
0.77914435
0.77888036
0.77866542
0.77871966
0.77866673
0.77842289
0.77843136
0.77832478
0.77803916
0.77788568
0.77783102
0.77773911
********************pre-trained*****************
INFO - ==> Top1: 89.256    Top5: 99.036    Loss: 0.313
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [54][   20/   40]   Loss 0.302886   Top1 90.234375   Top5 99.726562   BatchTime 0.150175
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.2988)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0399)
features.2.conv.0 tensor(0.0237)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0579)
features.3.conv.0 tensor(0.0197)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0291)
features.4.conv.0 tensor(0.0316)
features.4.conv.3 tensor(0.1036)
features.4.conv.6 tensor(0.0874)
features.5.conv.0 tensor(0.0324)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0913)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0452)
features.7.conv.0 tensor(0.0553)
features.7.conv.3 tensor(0.1030)
features.7.conv.6 tensor(0.1290)
features.8.conv.0 tensor(0.0540)
features.8.conv.3 tensor(0.1259)
features.8.conv.6 tensor(0.1984)
features.9.conv.0 tensor(0.0989)
features.9.conv.3 tensor(0.1455)
features.9.conv.6 tensor(0.2904)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0521)
features.11.conv.0 tensor(0.4106)
features.11.conv.3 tensor(0.1196)
features.11.conv.6 tensor(0.5084)
features.12.conv.0 tensor(0.2821)
features.12.conv.3 tensor(0.1292)
features.12.conv.6 tensor(0.5783)
features.13.conv.0 tensor(0.0862)
features.13.conv.3 tensor(0.1480)
features.13.conv.6 tensor(0.2518)
features.14.conv.0 tensor(0.9628)
features.14.conv.3 tensor(0.1008)
features.14.conv.6 tensor(0.9436)
features.15.conv.0 tensor(0.9481)
features.15.conv.3 tensor(0.0683)
features.15.conv.6 tensor(0.9791)
features.16.conv.0 tensor(0.1257)
features.16.conv.3 tensor(0.1094)
features.16.conv.6 tensor(0.2519)
conv.0 tensor(0.2236)
tensor(936380.) 2188896.0
INFO - Validation [54][   40/   40]   Loss 0.296609   Top1 90.510000   Top5 99.700000   BatchTime 0.100924
INFO - ==> Top1: 90.510    Top5: 99.700    Loss: 0.297
INFO - ==> Sparsity : 0.428
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
0.77747548
0.77728784
0.77747941
0.77759802
0.77766091
0.77767807
0.77766210
0.77740908
0.77724183
0.77730733
0.77740669
0.77730876
0.77710670
0.77712250
0.77704990
0.77696759
0.77680176
0.77646112
INFO - Training [55][   20/  196]   Loss 0.330738   Top1 88.535156   Top5 98.593750   BatchTime 0.460254   LR 0.000381
0.77633351
0.77616727
0.77590954
0.77491695
0.77420390
0.77346534
0.77266049
0.77193189
0.77135974
0.77057779
0.76989931
0.76941484
0.76913482
0.76876104
0.76873219
0.76869035
0.76848572
0.76837212
0.76816493
0.76772195
0.76714861
0.76613122
INFO - Training [55][   40/  196]   Loss 0.331516   Top1 88.613281   Top5 98.710938   BatchTime 0.417793   LR 0.000377
0.76517540
0.76406962
0.76381016
0.76302892
0.76166987
0.76009953
0.75950599
0.75850517
0.75734043
0.75575489
0.75462008
0.75325751
0.75161523
0.75252974
0.75325924
0.75427783
0.75596094
0.75717813
0.75871950
0.75907779
0.75835723
INFO - Training [55][   60/  196]   Loss 0.321068   Top1 88.873698   Top5 98.828125   BatchTime 0.406351   LR 0.000372
0.75840634
0.75832736
0.75826919
0.75855184
0.75979501
0.75934839
0.75913054
0.75871426
0.75842935
0.75716668
0.75621009
0.75539392
0.75470084
0.75413758
0.75408655
0.75454658
0.75483787
INFO - Training [55][   80/  196]   Loss 0.317343   Top1 88.979492   Top5 98.911133   BatchTime 0.389859   LR 0.000368
0.75519049
0.75537169
0.75555056
0.75591612
0.75557941
0.75555205
0.75566900
0.75595820
0.75608212
0.75618321
0.75627285
0.75630373
0.75560242
0.75535822
0.75496197
0.75458008
0.75448495
0.75460118
0.75444084
0.75444067
0.75421149
0.75439036
0.75443608
0.75458270
0.75478220
INFO - Training [55][  100/  196]   Loss 0.309941   Top1 89.285156   Top5 98.949219   BatchTime 0.377354   LR 0.000363
0.75498474
0.75493425
0.75493830
0.75504732
0.75501186
0.75520378
0.75550210
0.75571686
0.75610900
0.75636286
0.75658000
0.75670141
0.75684142
0.75671732
0.75670189
0.75669843
0.75673681
0.75686562
INFO - Training [55][  120/  196]   Loss 0.306163   Top1 89.404297   Top5 99.036458   BatchTime 0.368642   LR 0.000358
0.75690895
0.75708753
0.75700402
0.75695264
0.75710005
0.75668442
0.75657636
0.75642407
0.75617701
0.75602776
0.75605327
0.75621253
0.75674146
0.75769502
0.75781161
0.75786638
0.75788999
0.75772452
INFO - Training [55][  140/  196]   Loss 0.305780   Top1 89.453125   Top5 99.087612   BatchTime 0.363591   LR 0.000354
0.75770086
0.75739861
0.75704187
0.75696540
0.75705123
0.75680524
0.75680113
0.75679857
0.75672185
0.75687206
0.75684816
0.75683665
0.75693893
0.75685012
0.75690323
0.75667393
0.75665015
0.75669068
INFO - Training [55][  160/  196]   Loss 0.306250   Top1 89.440918   Top5 99.079590   BatchTime 0.360145   LR 0.000349
0.75667572
0.75652009
0.75656658
0.75631386
0.75576872
0.75597560
0.75583929
0.75574762
0.75549340
0.75520796
0.75480509
0.75431699
0.75387001
0.75311905
0.75255048
0.75222260
0.75204360
0.75172091
0.75170326
0.75175232
0.75178820
0.75188613
0.75207037
0.75219804
INFO - Training [55][  180/  196]   Loss 0.307148   Top1 89.398872   Top5 99.040799   BatchTime 0.356959   LR 0.000345
0.75229728
0.75229210
0.75239110
0.75243115
0.75260144
0.75269592
0.75263375
0.75288707
0.75316137
0.75321758
0.75326538
0.75305253
0.75274009
0.75251031
0.75224262
INFO - ==> Top1: 89.430    Top5: 99.040    Loss: 0.306
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [55][   20/   40]   Loss 0.459396   Top1 86.132812   Top5 99.414062   BatchTime 0.125037
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3418)
features.1.conv.0 tensor(0.0254)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0570)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0310)
features.4.conv.0 tensor(0.0288)
features.4.conv.3 tensor(0.1036)
features.4.conv.6 tensor(0.1016)
features.5.conv.0 tensor(0.0243)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0938)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0464)
features.7.conv.0 tensor(0.0553)
features.7.conv.3 tensor(0.1039)
features.7.conv.6 tensor(0.1749)
features.8.conv.0 tensor(0.0555)
features.8.conv.3 tensor(0.1230)
features.8.conv.6 tensor(0.1546)
features.9.conv.0 tensor(0.0970)
features.9.conv.3 tensor(0.1467)
features.9.conv.6 tensor(0.2785)
features.10.conv.0 tensor(0.0338)
features.10.conv.3 tensor(0.0998)
features.10.conv.6 tensor(0.0702)
features.11.conv.0 tensor(0.2599)
features.11.conv.3 tensor(0.1208)
features.11.conv.6 tensor(0.5175)
features.12.conv.0 tensor(0.2801)
features.12.conv.3 tensor(0.1285)
features.12.conv.6 tensor(0.5919)
features.13.conv.0 tensor(0.0852)
features.13.conv.3 tensor(0.1460)
features.13.conv.6 tensor(0.2538)
features.14.conv.0 tensor(0.9630)
features.14.conv.3 tensor(0.0999)
features.14.conv.6 tensor(0.9489)
features.15.conv.0 tensor(0.9482)
features.15.conv.3 tensor(0.0682)
features.15.conv.6 tensor(0.9797)
features.16.conv.0 tensor(0.1185)
features.16.conv.3 tensor(0.1082)
features.16.conv.6 tensor(0.2697)
conv.0 tensor(0.6365)
tensor(1104129.) 2188896.0
INFO - Validation [55][   40/   40]   Loss 0.446707   Top1 86.070000   Top5 99.460000   BatchTime 0.088611
INFO - ==> Top1: 86.070    Top5: 99.460    Loss: 0.447
INFO - ==> Sparsity : 0.504
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
0.75206792
0.75202024
0.75181818
0.75158018
0.75154328
0.75166100
0.75172329
0.75170517
0.75174940
0.75225013
0.75253844
0.75293678
0.75316918
0.75343055
0.75407153
0.75446630
0.75474852
INFO - Training [56][   20/  196]   Loss 0.307057   Top1 89.433594   Top5 98.554688   BatchTime 0.447665   LR 0.000337
0.75498253
0.75528693
0.75558543
0.75640666
0.75739717
0.75821304
0.76076835
0.76225358
0.76551056
0.76531583
0.76526004
0.76539272
0.76519889
0.76495200
0.76524037
0.76534128
0.76528025
0.76544505
0.76549888
0.76568139
0.76543283
INFO - Training [56][   40/  196]   Loss 0.311219   Top1 89.384766   Top5 98.671875   BatchTime 0.417505   LR 0.000333
0.76529157
0.76530755
0.76531845
0.76537198
0.76543528
0.76565844
0.76578963
0.76572394
0.76589292
0.76604831
0.76613957
0.76633668
0.76653296
0.76680207
0.76673800
0.76683080
0.76707143
0.76705158
0.76692492
0.76690400
0.76679397
0.76673752
INFO - Training [56][   60/  196]   Loss 0.316325   Top1 89.218750   Top5 98.678385   BatchTime 0.401147   LR 0.000328
0.76672643
0.76669705
0.76649719
0.76651520
0.76651984
0.76670349
0.76673275
0.76688176
0.76669842
0.76663464
0.76668102
0.76681668
0.76678300
0.76676887
0.76650274
0.76627237
0.76603746
0.76586252
0.76571321
0.76548487
0.76539624
0.76540351
INFO - Training [56][   80/  196]   Loss 0.317903   Top1 89.218750   Top5 98.798828   BatchTime 0.390430   LR 0.000324
0.76562589
0.76556295
0.76569039
0.76586813
0.76576561
0.76567268
0.76568139
0.76575536
0.76583457
0.76574045
0.76583242
0.76582104
0.76573080
0.76584864
0.76590872
0.76579398
0.76588839
0.76584077
0.76580507
0.76566339
INFO - Training [56][  100/  196]   Loss 0.308521   Top1 89.531250   Top5 98.855469   BatchTime 0.374922   LR 0.000319
0.76547641
0.76554346
0.76553500
0.76558870
0.76554042
0.76548046
0.76557302
0.76575226
0.76559752
0.76549011
0.76514685
0.76494956
0.76483279
0.76498705
0.76484501
0.76433450
0.76418674
0.76406342
0.76375896
INFO - Training [56][  120/  196]   Loss 0.304408   Top1 89.641927   Top5 98.909505   BatchTime 0.364913   LR 0.000315
0.76389772
0.76380605
0.76366699
0.76359278
0.76373380
0.76332325
0.76354194
0.76342010
0.76326776
0.76329589
0.76308352
0.76314330
0.76283038
0.76258886
0.76244891
0.76193041
INFO - Training [56][  140/  196]   Loss 0.304996   Top1 89.612165   Top5 98.967634   BatchTime 0.350671   LR 0.000311
0.76139718
0.76081699
0.76010990
0.75989771
0.75987345
0.75970507
0.75961453
0.75949234
0.75933254
0.75914687
0.75892997
0.75863773
0.75834411
0.75813168
0.75786537
0.75753516
0.75707555
0.75650179
0.75604898
0.75570947
0.75545007
0.75517297
INFO - Training [56][  160/  196]   Loss 0.306837   Top1 89.575195   Top5 98.967285   BatchTime 0.353067   LR 0.000306
0.75493264
0.75477487
0.75462890
0.75442821
0.75421661
0.75403059
0.75388682
0.75362831
0.75354576
0.75358075
0.75363183
0.75368798
0.75393456
0.75400937
0.75398254
0.75389159
0.75380707
0.75363731
0.75358778
0.75344455
0.75335336
INFO - Training [56][  180/  196]   Loss 0.310346   Top1 89.466146   Top5 98.945312   BatchTime 0.357165   LR 0.000302
0.75326699
0.75312448
0.75287920
0.75266176
0.75236100
0.75211692
0.75182527
0.75140876
0.75090367
0.75062168
0.75031078
0.75001383
0.74981064
0.74969876
0.74965346
0.74985182
INFO - ==> Top1: 89.452    Top5: 98.954    Loss: 0.310
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [56][   20/   40]   Loss 0.430017   Top1 86.269531   Top5 99.316406   BatchTime 0.125161
features.0.conv.0 tensor(0.5556)
features.0.conv.3 tensor(0.3711)
features.1.conv.0 tensor(0.0260)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0404)
features.2.conv.0 tensor(0.0220)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0570)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0312)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.0942)
features.5.conv.0 tensor(0.0293)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.1035)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0458)
features.7.conv.0 tensor(0.0527)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.1526)
features.8.conv.0 tensor(0.0559)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.1669)
features.9.conv.0 tensor(0.0970)
features.9.conv.3 tensor(0.1441)
features.9.conv.6 tensor(0.2552)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.0806)
features.11.conv.0 tensor(0.2599)
features.11.conv.3 tensor(0.1202)
features.11.conv.6 tensor(0.5281)
features.12.conv.0 tensor(0.2820)
features.12.conv.3 tensor(0.1279)
features.12.conv.6 tensor(0.5736)
features.13.conv.0 tensor(0.0859)
features.13.conv.3 tensor(0.1468)
features.13.conv.6 tensor(0.2520)
features.14.conv.0 tensor(0.9641)
features.14.conv.3 tensor(0.0998)
features.14.conv.6 tensor(0.9569)
features.15.conv.0 tensor(0.9485)
features.15.conv.3 tensor(0.0662)
features.15.conv.6 tensor(0.9794)
features.16.conv.0 tensor(0.1176)
features.16.conv.3 tensor(0.1074)
features.16.conv.6 tensor(0.2911)
conv.0 tensor(0.5357)
tensor(1069842.) 2188896.0
INFO - Validation [56][   40/   40]   Loss 0.414828   Top1 86.530000   Top5 99.420000   BatchTime 0.088916
INFO - ==> Top1: 86.530    Top5: 99.420    Loss: 0.415
INFO - ==> Sparsity : 0.489
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
0.75018388
0.75054657
0.75087392
0.75109947
0.75143206
0.75163013
0.75179470
0.75198323
0.75204176
0.75193220
0.75192690
0.75187743
0.75184816
0.75187308
0.75192910
0.75187927
0.75191128
0.75190026
INFO - Training [57][   20/  196]   Loss 0.314172   Top1 89.335938   Top5 98.574219   BatchTime 0.447802   LR 0.000294
0.75195932
0.75206518
0.75210160
0.75222015
0.75253171
0.75275004
0.75292957
0.75307399
0.75305945
0.75301689
0.75311393
0.75327784
0.75308162
0.75284749
0.75291759
0.75268185
0.75261074
0.75245500
0.75244439
0.75248843
0.75230217
0.75207263
0.75199753
INFO - Training [57][   40/  196]   Loss 0.316492   Top1 89.228516   Top5 98.662109   BatchTime 0.400290   LR 0.000290
0.75188571
0.75173628
0.75172806
0.75167948
0.75162840
0.75158918
0.75160635
0.75154889
0.75166571
0.75178677
0.75178969
0.75187337
0.75201380
0.75213975
0.75219685
0.75220186
0.75217396
0.75220591
0.75220883
0.75218213
0.75212222
0.75210685
INFO - Training [57][   60/  196]   Loss 0.319114   Top1 89.153646   Top5 98.769531   BatchTime 0.389895   LR 0.000286
0.75215656
0.75213826
0.75215012
0.75220650
0.75221580
0.75222337
0.75221980
0.75226045
0.75224078
0.75221276
0.75223786
0.75221509
0.75218379
0.75214469
0.75207978
0.75197858
INFO - Training [57][   80/  196]   Loss 0.315055   Top1 89.252930   Top5 98.896484   BatchTime 0.382077   LR 0.000282
0.75191462
0.75187808
0.75176632
0.75169283
0.75167662
0.75165075
0.75167400
0.75168365
0.75165695
0.75165200
0.75164467
0.75166947
0.75165504
0.75167513
0.75164562
0.75166488
0.75167066
0.75170124
0.75165135
0.75164831
0.75157881
0.75147444
INFO - Training [57][  100/  196]   Loss 0.309086   Top1 89.394531   Top5 98.964844   BatchTime 0.379626   LR 0.000277
0.75136411
0.75125313
0.75118572
0.75113690
0.75109941
0.75100446
0.75098407
0.75094765
0.75086111
0.75082231
0.75080651
0.75083089
0.75086516
0.75089699
0.75094247
0.75094193
INFO - Training [57][  120/  196]   Loss 0.305077   Top1 89.505208   Top5 99.036458   BatchTime 0.376723   LR 0.000273
0.75094408
0.75093752
0.75089872
0.75093627
0.75091904
0.75091571
0.75095153
0.75098324
0.75102526
0.75101763
0.75102007
0.75098348
0.75101143
0.75095904
0.75085253
0.75075686
0.75075287
0.75066638
0.75053579
0.75044417
0.75036097
0.75263786
INFO - Training [57][  140/  196]   Loss 0.306180   Top1 89.559152   Top5 99.070871   BatchTime 0.376191   LR 0.000269
0.75240284
0.75226057
0.75209320
0.75195974
0.75194693
0.75208789
0.75217021
0.75221789
0.75238913
0.75231212
0.75242490
0.75247341
0.75243062
0.75236839
0.75225317
0.75226802
0.75227368
0.75214511
0.75205559
0.75201035
0.75192606
INFO - Training [57][  160/  196]   Loss 0.308764   Top1 89.414062   Top5 99.033203   BatchTime 0.378683   LR 0.000265
0.75174391
0.75158858
0.75153226
0.75141948
0.75137150
0.75129062
0.75116819
0.75124043
0.75128525
0.75117356
0.75122750
0.75128466
0.75120199
0.75122976
0.75122690
0.75130618
0.75124222
0.75130093
0.75128341
0.75134915
0.75133312
INFO - Training [57][  180/  196]   Loss 0.308863   Top1 89.414062   Top5 99.006076   BatchTime 0.377665   LR 0.000261
0.75132811
0.75124776
0.75123352
0.75104558
0.75104189
0.75120455
0.75147235
0.75154269
0.75168186
0.75177050
0.75181162
0.75194645
0.75205153
0.75200397
0.75206029
********************pre-trained*****************
INFO - ==> Top1: 89.462    Top5: 98.996    Loss: 0.308
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [57][   20/   40]   Loss 0.312732   Top1 90.253906   Top5 99.648438   BatchTime 0.118768
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.3184)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0365)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0570)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0317)
features.4.conv.0 tensor(0.0299)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.0897)
features.5.conv.0 tensor(0.0272)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.0902)
features.6.conv.0 tensor(0.0228)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0457)
features.7.conv.0 tensor(0.0532)
features.7.conv.3 tensor(0.1071)
features.7.conv.6 tensor(0.1473)
features.8.conv.0 tensor(0.0594)
features.8.conv.3 tensor(0.1183)
features.8.conv.6 tensor(0.1688)
features.9.conv.0 tensor(0.0970)
features.9.conv.3 tensor(0.1453)
features.9.conv.6 tensor(0.2845)
features.10.conv.0 tensor(0.0364)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.1027)
features.11.conv.0 tensor(0.2649)
features.11.conv.3 tensor(0.1196)
features.11.conv.6 tensor(0.5242)
features.12.conv.0 tensor(0.2900)
features.12.conv.3 tensor(0.1292)
features.12.conv.6 tensor(0.5941)
features.13.conv.0 tensor(0.0875)
features.13.conv.3 tensor(0.1495)
features.13.conv.6 tensor(0.2545)
features.14.conv.0 tensor(0.9645)
features.14.conv.3 tensor(0.1015)
features.14.conv.6 tensor(0.9563)
features.15.conv.0 tensor(0.9489)
features.15.conv.3 tensor(0.0656)
features.15.conv.6 tensor(0.9802)
features.16.conv.0 tensor(0.1270)
features.16.conv.3 tensor(0.1081)
features.16.conv.6 tensor(0.2474)
conv.0 tensor(0.5071)
tensor(1049644.) 2188896.0
INFO - Validation [57][   40/   40]   Loss 0.303739   Top1 90.190000   Top5 99.720000   BatchTime 0.084471
INFO - ==> Top1: 90.190    Top5: 99.720    Loss: 0.304
INFO - ==> Sparsity : 0.480
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
0.75213325
0.75221592
0.75232661
0.75237441
0.75243402
0.75250947
0.75253552
0.75258166
0.75263059
0.75269854
0.75270551
0.75279862
0.75277293
0.75267947
0.75271285
0.75272393
0.75269264
0.75273895
INFO - Training [58][   20/  196]   Loss 0.314113   Top1 89.296875   Top5 98.554688   BatchTime 0.457349   LR 0.000254
0.75277859
0.75288779
0.75291371
0.75301206
0.75305504
0.75314230
0.75315577
0.75314814
0.75314045
0.75301462
0.75302255
0.75286531
0.75275093
0.75259095
0.75244117
0.75227624
0.75208551
0.75198579
0.75165462
0.75163764
0.75150847
INFO - Training [58][   40/  196]   Loss 0.320593   Top1 88.916016   Top5 98.769531   BatchTime 0.414663   LR 0.000250
0.75138909
0.75130814
0.75124907
0.75122505
0.75108111
0.75100267
0.75091642
0.75089562
0.75091350
0.75089860
0.75088364
0.75091565
0.75092584
0.75091970
0.75094807
0.75091952
0.75089848
0.75089782
0.75083947
0.75092721
0.75097185
0.75093156
INFO - Training [58][   60/  196]   Loss 0.315345   Top1 89.108073   Top5 98.860677   BatchTime 0.398291   LR 0.000246
0.75109750
0.75117922
0.75120449
0.75108182
0.75099331
0.75057960
0.75059754
0.75065267
0.75074154
0.75077271
0.75077868
0.75078857
0.75074703
0.75075853
0.75074810
0.75048691
0.75049865
0.75052696
0.75048774
0.75039709
0.74994677
0.74974090
INFO - Training [58][   80/  196]   Loss 0.316779   Top1 89.013672   Top5 98.950195   BatchTime 0.392316   LR 0.000242
0.74965388
0.74945688
0.74927038
0.74913156
0.74905151
0.74901736
0.74894053
0.74884701
0.74878120
0.74878997
0.74881989
0.74879199
0.74881095
0.74874932
0.74864459
0.74853760
INFO - Training [58][  100/  196]   Loss 0.308426   Top1 89.332031   Top5 98.980469   BatchTime 0.386930   LR 0.000238
0.74842811
0.74835813
0.74834520
0.74823940
0.74817115
0.74801803
0.74796253
0.74785906
0.74795049
0.74784124
0.74791932
0.74780846
0.74799573
0.74796057
0.74787027
0.74786669
0.74797648
0.74812859
INFO - Training [58][  120/  196]   Loss 0.304027   Top1 89.527995   Top5 99.023438   BatchTime 0.377618   LR 0.000234
0.74788404
0.74789286
0.74795842
0.74806190
0.74802440
0.74797148
0.74793750
0.74797279
0.74818045
0.75021690
0.75022751
0.75007808
0.75004822
0.75003868
0.74993783
0.74971974
0.74957591
0.74943632
0.74932629
0.74919498
0.74908322
0.74887574
0.74873173
0.74858934
0.74845529
INFO - Training [58][  140/  196]   Loss 0.302346   Top1 89.648438   Top5 99.068080   BatchTime 0.369780   LR 0.000230
0.74827945
0.74816728
0.74810421
0.74803281
0.74800920
0.74800986
0.74806887
0.74821168
0.74839997
0.74853188
0.74860394
0.74869138
0.74875963
0.74880749
0.74888456
0.74886739
0.74886781
0.74886411
0.74882823
0.74883133
INFO - Training [58][  160/  196]   Loss 0.306637   Top1 89.526367   Top5 99.069824   BatchTime 0.373732   LR 0.000226
0.74883807
0.74876720
0.74880862
0.74880171
0.74882674
0.74888277
0.74898034
0.74905801
0.74906522
0.74917012
0.74928552
0.74937451
0.74944687
0.74953610
0.74970728
0.74973077
INFO - Training [58][  180/  196]   Loss 0.305311   Top1 89.563802   Top5 99.010417   BatchTime 0.372667   LR 0.000222
0.74984640
0.74983555
0.74979454
0.74973363
0.74967790
0.74969393
0.74968886
0.74967247
0.74962199
0.74964792
0.74960393
0.74957103
0.74944925
0.74943250
0.74941725
0.74938774
0.74936712
INFO - ==> Top1: 89.696    Top5: 99.024    Loss: 0.302
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.74942666
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [58][   20/   40]   Loss 0.333139   Top1 90.058594   Top5 99.531250   BatchTime 0.122963
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3203)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0373)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0553)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0334)
features.4.conv.0 tensor(0.0329)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.0916)
features.5.conv.0 tensor(0.0282)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.0980)
features.6.conv.0 tensor(0.0228)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0475)
features.7.conv.0 tensor(0.0549)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.1506)
features.8.conv.0 tensor(0.0618)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.1667)
features.9.conv.0 tensor(0.0985)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.2671)
features.10.conv.0 tensor(0.0348)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0778)
features.11.conv.0 tensor(0.2964)
features.11.conv.3 tensor(0.1182)
features.11.conv.6 tensor(0.5369)
features.12.conv.0 tensor(0.2771)
features.12.conv.3 tensor(0.1291)
features.12.conv.6 tensor(0.5899)
features.13.conv.0 tensor(0.0911)
features.13.conv.3 tensor(0.1497)
features.13.conv.6 tensor(0.2676)
features.14.conv.0 tensor(0.9645)
features.14.conv.3 tensor(0.1008)
features.14.conv.6 tensor(0.9572)
features.15.conv.0 tensor(0.9493)
features.15.conv.3 tensor(0.0663)
features.15.conv.6 tensor(0.9823)
features.16.conv.0 tensor(0.1179)
features.16.conv.3 tensor(0.1068)
features.16.conv.6 tensor(0.2678)
conv.0 tensor(0.5363)
tensor(1068779.) 2188896.0
INFO - Validation [58][   40/   40]   Loss 0.323112   Top1 89.960000   Top5 99.610000   BatchTime 0.088347
INFO - ==> Top1: 89.960    Top5: 99.610    Loss: 0.323
INFO - ==> Sparsity : 0.488
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
0.74951810
0.74949086
0.74942434
0.74945801
0.74942315
0.74940634
0.74945360
0.74941128
0.74934977
0.74934018
0.74934125
0.74935299
0.74941218
0.74949443
0.74949545
0.74942476
0.74940652
0.74942881
0.74937487
0.74932861
0.74922967
0.74921471
0.74920964
INFO - Training [59][   20/  196]   Loss 0.302353   Top1 89.687500   Top5 98.417969   BatchTime 0.458141   LR 0.000215
0.74919844
0.74921471
0.74928391
0.74936652
0.74938911
0.74929398
0.74923378
0.74917448
0.74908853
0.74908257
0.74904573
0.74904495
0.74909687
0.74913585
0.74910772
0.74915695
INFO - Training [59][   40/  196]   Loss 0.310342   Top1 89.619141   Top5 98.583984   BatchTime 0.410387   LR 0.000212
0.74904889
0.74909693
0.74910533
0.74910468
0.74915963
0.74919254
0.74920130
0.74924624
0.74917477
0.74912429
0.74910808
0.74910581
0.74907839
0.74901927
0.74909449
0.74896085
0.74890977
0.74889690
0.74896121
0.74887490
0.74881059
0.74874008
INFO - Training [59][   60/  196]   Loss 0.303998   Top1 89.778646   Top5 98.750000   BatchTime 0.395286   LR 0.000208
0.74872106
0.74871188
0.74850148
0.74826473
0.74819988
0.74825656
0.74821699
0.74796396
0.74773532
0.74761701
0.74755561
0.74753714
0.74756271
0.74752951
0.74753022
0.74749017
0.74752605
INFO - Training [59][   80/  196]   Loss 0.305592   Top1 89.667969   Top5 98.876953   BatchTime 0.386950   LR 0.000204
0.74751812
0.74745417
0.74739879
0.74730921
0.74722075
0.74710959
0.74706131
0.74702489
0.74694973
0.74690551
0.74689448
0.74690562
0.74689347
0.74691641
0.74691182
0.74696755
0.74696314
0.74700189
0.74702889
0.74705487
0.74705958
0.74707484
INFO - Training [59][  100/  196]   Loss 0.298588   Top1 89.898438   Top5 98.968750   BatchTime 0.381111   LR 0.000201
0.74703532
0.74705261
0.74701202
0.74698162
0.74697226
0.74696451
0.74695402
0.74693924
0.74691719
0.74689448
0.74687833
0.74680865
0.74674833
0.74664938
0.74653202
0.74640197
0.74625474
0.74610513
0.74596840
0.74583268
0.74567276
0.74559134
0.74555236
INFO - Training [59][  120/  196]   Loss 0.292609   Top1 90.087891   Top5 99.010417   BatchTime 0.375776   LR 0.000197
0.74552917
0.74549711
0.74542952
0.74536949
0.74527967
0.74521011
0.74512929
0.74507570
0.74509114
0.74507779
0.74506122
0.74506235
0.74509275
0.74516374
INFO - Training [59][  140/  196]   Loss 0.293200   Top1 90.094866   Top5 99.051339   BatchTime 0.363781   LR 0.000193
0.74524319
0.74529862
0.74531537
0.74533612
0.74533218
0.74540091
0.74540192
0.74530637
0.74520874
0.74505883
0.74485803
0.74467808
0.74451190
0.74437982
0.74430364
0.74428362
0.74423575
0.74414611
0.74408686
0.74395078
0.74382585
0.74367923
0.74351537
0.74338198
0.74319774
INFO - Training [59][  160/  196]   Loss 0.296292   Top1 89.960938   Top5 99.064941   BatchTime 0.358300   LR 0.000190
0.74305552
0.74296927
0.74283767
0.74271661
0.74258155
0.74252689
0.74242973
0.74237597
0.74236214
0.74237013
0.74238485
0.74232823
0.74223185
0.74213457
0.74206531
0.74198377
INFO - Training [59][  180/  196]   Loss 0.297911   Top1 89.858941   Top5 99.014757   BatchTime 0.360553   LR 0.000186
0.74189782
0.74202675
0.74244225
0.74264902
0.74275309
0.74282956
0.74296534
0.74301732
0.74302512
0.74297178
0.74290138
0.74299949
0.74321342
0.74345243
0.74378359
0.74404454
0.74431264
INFO - ==> Top1: 89.908    Top5: 99.018    Loss: 0.297
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.74452782
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [59][   20/   40]   Loss 0.325186   Top1 90.097656   Top5 99.628906   BatchTime 0.116803
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.3320)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0382)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0556)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0368)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0954)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.1351)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0591)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.1564)
features.8.conv.0 tensor(0.0630)
features.8.conv.3 tensor(0.1218)
features.8.conv.6 tensor(0.1813)
features.9.conv.0 tensor(0.0995)
features.9.conv.3 tensor(0.1444)
features.9.conv.6 tensor(0.2640)
features.10.conv.0 tensor(0.0354)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.0960)
features.11.conv.0 tensor(0.2984)
features.11.conv.3 tensor(0.1188)
features.11.conv.6 tensor(0.5298)
features.12.conv.0 tensor(0.2815)
features.12.conv.3 tensor(0.1289)
features.12.conv.6 tensor(0.5907)
features.13.conv.0 tensor(0.0921)
features.13.conv.3 tensor(0.1495)
features.13.conv.6 tensor(0.2640)
features.14.conv.0 tensor(0.9648)
features.14.conv.3 tensor(0.1005)
features.14.conv.6 tensor(0.9587)
features.15.conv.0 tensor(0.9498)
features.15.conv.3 tensor(0.0668)
features.15.conv.6 tensor(0.9855)
features.16.conv.0 tensor(0.1214)
features.16.conv.3 tensor(0.1066)
features.16.conv.6 tensor(0.3689)
conv.0 tensor(0.5308)
tensor(1100230.) 2188896.0
INFO - Validation [59][   40/   40]   Loss 0.310260   Top1 90.340000   Top5 99.710000   BatchTime 0.085184
INFO - ==> Top1: 90.340    Top5: 99.710    Loss: 0.310
INFO - ==> Sparsity : 0.503
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
0.74478889
0.74501282
0.74501973
0.74501324
0.74500167
0.74511844
0.74505794
0.74494129
0.74492890
0.74498522
0.74501610
0.74510872
0.74523330
0.74527639
0.74533868
0.74537009
0.74536109
0.74530613
0.74523860
0.74522555
0.74525905
0.74524063
INFO - Training [60][   20/  196]   Loss 0.320206   Top1 89.277344   Top5 98.613281   BatchTime 0.453775   LR 0.000180
0.74525100
0.74523032
0.74512577
0.74501956
0.74489731
0.74480844
0.74470383
0.74460942
0.74448913
0.74445903
0.74439800
0.74425310
0.74416786
0.74404210
0.74397767
0.74394709
INFO - Training [60][   40/  196]   Loss 0.312999   Top1 89.433594   Top5 98.818359   BatchTime 0.415548   LR 0.000176
0.74390984
0.74385780
0.74387425
0.74393761
0.74393529
0.74393880
0.74401516
0.74408007
0.74415362
0.74426395
0.74437219
0.74444288
0.74449897
0.74450070
0.74448752
0.74451560
0.74451500
0.74447292
0.74438500
0.74426967
INFO - Training [60][   60/  196]   Loss 0.308008   Top1 89.635417   Top5 98.893229   BatchTime 0.409128   LR 0.000173
0.74419940
0.74410301
0.74401754
0.74386919
0.74373776
0.74360758
0.74349248
0.74341375
0.74330801
0.74317217
0.74305159
0.74286133
0.74261248
0.74241990
0.74226809
0.74216229
0.74202853
0.74186629
0.74177843
0.74175340
0.74177068
0.74187684
INFO - Training [60][   80/  196]   Loss 0.301097   Top1 89.819336   Top5 98.974609   BatchTime 0.397466   LR 0.000169
0.74188185
0.74191391
0.74189383
0.74184430
0.74182826
0.74187022
0.74186313
0.74192595
0.74206859
0.74219304
0.74228221
0.74234873
0.74235398
0.74239206
0.74241978
0.74247909
0.74248654
0.74251163
0.74257571
0.74260849
0.74270296
0.74276781
0.74280798
INFO - Training [60][  100/  196]   Loss 0.297649   Top1 89.886719   Top5 98.984375   BatchTime 0.390242   LR 0.000166
0.74285442
0.74284619
0.74282348
0.74279940
0.74286371
0.74284017
0.74279732
0.74278909
0.74271685
0.74261874
0.74250996
0.74226153
0.74204117
0.74180144
0.74163932
INFO - Training [60][  120/  196]   Loss 0.291182   Top1 90.104167   Top5 99.059245   BatchTime 0.387393   LR 0.000162
0.74167597
0.74170548
0.74183542
0.74186701
0.74201155
0.74220514
0.74231362
0.74241555
0.74245316
0.74255520
0.74262697
0.74271548
0.74284911
0.74295491
0.74303508
0.74303299
0.74299264
0.74287367
0.74272019
0.74249232
0.74233371
0.74221063
0.74199945
0.74188000
INFO - Training [60][  140/  196]   Loss 0.290458   Top1 90.108817   Top5 99.104353   BatchTime 0.381425   LR 0.000159
0.74179971
0.74165928
0.74151218
0.74145555
0.74132651
0.74122137
0.74119532
0.74114621
0.74094152
0.74069649
0.74028385
0.73974925
0.73921913
0.73852766
0.73769253
0.73680168
0.73602724
0.73575377
0.73534960
INFO - Training [60][  160/  196]   Loss 0.296635   Top1 89.865723   Top5 99.072266   BatchTime 0.372277   LR 0.000156
0.73490876
0.73441595
0.73454517
0.73477095
0.73501736
0.73523319
0.73515671
0.73521388
0.73535180
0.73540401
0.73564577
0.73597240
0.73614866
0.73629153
0.73673427
0.73707682
0.73754603
0.73793727
0.73845106
0.73877072
0.73898274
INFO - Training [60][  180/  196]   Loss 0.296651   Top1 89.826389   Top5 99.051649   BatchTime 0.374266   LR 0.000152
0.73922819
0.73930651
0.73926431
0.73925906
0.73923844
0.73932743
0.73939705
0.73945910
0.73948359
0.73954374
0.73957217
0.73966080
0.73970258
0.73975807
********************pre-trained*****************
INFO - ==> Top1: 89.814    Top5: 99.044    Loss: 0.297
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [60][   20/   40]   Loss 0.342023   Top1 89.785156   Top5 99.628906   BatchTime 0.119572
features.0.conv.0 tensor(0.5382)
features.0.conv.3 tensor(0.3457)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0404)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0553)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0317)
features.4.conv.0 tensor(0.0361)
features.4.conv.3 tensor(0.1024)
features.4.conv.6 tensor(0.0905)
features.5.conv.0 tensor(0.0277)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.1437)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.1809)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.1530)
features.8.conv.0 tensor(0.0629)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.1697)
features.9.conv.0 tensor(0.1020)
features.9.conv.3 tensor(0.1444)
features.9.conv.6 tensor(0.2694)
features.10.conv.0 tensor(0.0352)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.1052)
features.11.conv.0 tensor(0.3176)
features.11.conv.3 tensor(0.1184)
features.11.conv.6 tensor(0.5340)
features.12.conv.0 tensor(0.3139)
features.12.conv.3 tensor(0.1281)
features.12.conv.6 tensor(0.5983)
features.13.conv.0 tensor(0.0943)
features.13.conv.3 tensor(0.1476)
features.13.conv.6 tensor(0.3055)
features.14.conv.0 tensor(0.9650)
features.14.conv.3 tensor(0.0994)
features.14.conv.6 tensor(0.9535)
features.15.conv.0 tensor(0.9503)
features.15.conv.3 tensor(0.0671)
features.15.conv.6 tensor(0.9850)
features.16.conv.0 tensor(0.1265)
features.16.conv.3 tensor(0.1049)
features.16.conv.6 tensor(0.3390)
conv.0 tensor(0.5424)
tensor(1106376.) 2188896.0
INFO - Validation [60][   40/   40]   Loss 0.329906   Top1 89.760000   Top5 99.730000   BatchTime 0.085657
INFO - ==> Top1: 89.760    Top5: 99.730    Loss: 0.330
INFO - ==> Sparsity : 0.505
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
0.73988742
0.73993891
0.73999143
0.74002355
0.74012226
0.74022830
0.74030548
0.74039561
0.74046683
0.74053496
0.74057442
0.74060106
0.74058282
0.74056149
0.74056393
0.74056154
0.74054742
0.74052787
0.74053049
INFO - Training [61][   20/  196]   Loss 0.291196   Top1 89.707031   Top5 98.613281   BatchTime 0.448581   LR 0.000147
0.74051440
0.74050128
0.74052322
0.74057424
0.74064994
0.74069977
0.74078244
0.74088389
0.74104553
0.74111897
0.74116206
0.74121511
0.74129891
0.74130762
0.74126935
0.74128979
0.74133956
0.74138808
0.74142098
0.74139768
0.74135351
INFO - Training [61][   40/  196]   Loss 0.293401   Top1 89.716797   Top5 98.759766   BatchTime 0.418011   LR 0.000143
0.74132717
0.74138945
0.74143189
0.74148142
0.74153417
0.74152118
0.74150449
0.74152911
0.74156278
0.74157310
0.74156898
0.74155527
0.74152535
0.74152631
0.74150759
0.74148202
0.74144912
0.74144655
0.74146891
0.74152815
0.74152499
0.74149638
INFO - Training [61][   60/  196]   Loss 0.293388   Top1 89.661458   Top5 98.854167   BatchTime 0.399261   LR 0.000140
0.74146461
0.74146998
0.74145544
0.74144721
0.74145472
0.74150884
0.74151003
0.74146384
0.74139583
0.74133033
0.74125212
0.74116951
0.74110562
0.74101907
0.74091578
0.74080718
0.74072224
INFO - Training [61][   80/  196]   Loss 0.297998   Top1 89.555664   Top5 98.984375   BatchTime 0.390755   LR 0.000137
0.74071223
0.74075240
0.74078417
0.74073774
0.74068415
0.74063730
0.74056429
0.74051577
0.74047685
0.74038547
0.74030018
0.74025303
0.74022061
0.74013746
0.74009383
0.74006283
0.74006158
0.74003315
0.73998523
0.73994702
0.73994792
0.73989099
INFO - Training [61][  100/  196]   Loss 0.297483   Top1 89.578125   Top5 99.011719   BatchTime 0.385594   LR 0.000134
0.73980725
0.73972315
0.73964608
0.73963225
0.73959321
0.73959887
0.73965603
0.73968434
0.73971206
0.73971134
0.73972410
0.73973435
0.73973137
0.73978347
0.73988241
0.73994493
0.73998600
0.74001926
0.74007124
0.74007875
0.74010623
INFO - Training [61][  120/  196]   Loss 0.292035   Top1 89.840495   Top5 99.069010   BatchTime 0.382811   LR 0.000131
0.74013609
0.74013096
0.74009961
0.74008971
0.74005944
0.74006373
0.74004894
0.74002099
0.74000216
0.74001211
0.74001771
0.73999459
0.73998249
0.73994839
0.73991615
0.73988384
INFO - Training [61][  140/  196]   Loss 0.290055   Top1 89.933036   Top5 99.107143   BatchTime 0.382099   LR 0.000128
0.73985904
0.73977506
0.73973835
0.73973280
0.73969316
0.73969799
0.73972011
0.73977870
0.73979366
0.73984516
0.73985636
0.73981422
0.73981822
0.73981667
0.73978472
0.73971802
0.73965025
0.73957539
0.73950422
0.73937607
0.73924118
0.73904091
0.73889720
INFO - Training [61][  160/  196]   Loss 0.295217   Top1 89.753418   Top5 99.089355   BatchTime 0.377914   LR 0.000125
0.73872322
0.73846006
0.73825240
0.73791957
0.73772436
0.73753715
0.73750633
0.73750043
0.73733443
0.73693591
0.73632538
0.73592919
0.73575485
0.73575938
0.73584133
0.73596841
0.73627722
0.73648608
0.73662907
INFO - Training [61][  180/  196]   Loss 0.296209   Top1 89.670139   Top5 99.062500   BatchTime 0.382363   LR 0.000122
0.73666817
0.73672205
0.73693722
0.73715448
0.73735148
0.73751897
0.73772037
0.73783875
0.73784614
0.73787701
0.73797172
0.73799521
0.73795718
0.73794109
0.73792928
INFO - ==> Top1: 89.752    Top5: 99.086    Loss: 0.295
0.73783839
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [61][   20/   40]   Loss 0.324584   Top1 90.097656   Top5 99.511719   BatchTime 0.122784
INFO - Validation [61][   40/   40]   Loss 0.311641   Top1 90.060000   Top5 99.630000   BatchTime 0.086186
INFO - ==> Top1: 90.060    Top5: 99.630    Loss: 0.312
INFO - ==> Sparsity : 0.516
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.400   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.3379)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0399)
features.2.conv.0 tensor(0.0179)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0561)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0323)
features.4.conv.0 tensor(0.0356)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.0975)
features.5.conv.0 tensor(0.0270)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.1406)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0388)
features.6.conv.6 tensor(0.0471)
features.7.conv.0 tensor(0.0608)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.1649)
features.8.conv.0 tensor(0.0627)
features.8.conv.3 tensor(0.1233)
features.8.conv.6 tensor(0.1821)
features.9.conv.0 tensor(0.1016)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.2702)
features.10.conv.0 tensor(0.0340)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.1031)
features.11.conv.0 tensor(0.3078)
features.11.conv.3 tensor(0.1184)
features.11.conv.6 tensor(0.5380)
features.12.conv.0 tensor(0.3014)
features.12.conv.3 tensor(0.1281)
features.12.conv.6 tensor(0.5988)
features.13.conv.0 tensor(0.0914)
features.13.conv.3 tensor(0.1474)
features.13.conv.6 tensor(0.2806)
features.14.conv.0 tensor(0.9653)
features.14.conv.3 tensor(0.0991)
features.14.conv.6 tensor(0.9551)
features.15.conv.0 tensor(0.9505)
features.15.conv.3 tensor(0.0671)
features.15.conv.6 tensor(0.9858)
features.16.conv.0 tensor(0.1804)
features.16.conv.3 tensor(0.1060)
features.16.conv.6 tensor(0.4251)
conv.0 tensor(0.5262)
tensor(1129064.) 2188896.0
0.73773450
0.73769087
0.73765308
0.73760700
0.73754388
0.73738152
0.73719561
0.73719072
0.73723847
0.73723978
0.73720324
0.73718137
0.73709381
0.73704964
0.73704183
0.73703343
0.73710024
0.73724937
0.73734635
0.73735631
0.73736191
INFO - Training [62][   20/  196]   Loss 0.317384   Top1 88.945312   Top5 98.535156   BatchTime 0.460772   LR 0.000117
0.73742127
0.73745024
0.73747504
0.73756951
0.73760331
0.73760468
0.73762619
0.73759127
0.73761278
0.73761201
0.73758399
0.73756486
0.73757178
0.73758608
0.73757887
0.73758906
0.73765427
0.73775226
0.73778582
0.73781544
0.73785836
INFO - Training [62][   40/  196]   Loss 0.309848   Top1 89.150391   Top5 98.750000   BatchTime 0.423648   LR 0.000114
0.73792636
0.73796350
0.73796856
0.73805678
0.73813403
0.73813391
0.73817551
0.73818737
0.73821533
0.73825455
0.73819840
0.73812258
0.73807186
0.73801255
0.73799217
0.73805970
INFO - Training [62][   60/  196]   Loss 0.302451   Top1 89.505208   Top5 98.815104   BatchTime 0.404438   LR 0.000111
0.73812729
0.73822987
0.73831224
0.73839021
0.73842907
0.73847795
0.73846549
0.73846191
0.73850751
0.73850811
0.73850787
0.73851341
0.73848116
0.73844486
0.73839176
0.73832756
0.73826754
0.73822111
0.73816144
0.73809242
0.73807335
0.73805094
INFO - Training [62][   80/  196]   Loss 0.297739   Top1 89.589844   Top5 98.994141   BatchTime 0.395684   LR 0.000108
0.73797339
0.73793191
0.73789912
0.73787105
0.73778123
0.73769897
0.73763192
0.73758304
0.73752397
0.73748344
0.73741949
0.73738825
0.73739851
0.73741794
0.73742908
0.73748845
0.73752058
0.73753399
0.73751867
0.73746949
0.73734474
INFO - Training [62][  100/  196]   Loss 0.292055   Top1 89.792969   Top5 99.019531   BatchTime 0.390864   LR 0.000105
0.73724139
0.73708940
0.73698020
0.73686218
0.73673022
0.73660117
0.73649168
0.73638719
0.73628193
0.73616666
0.73604256
0.73596650
0.73586732
0.73571801
0.73562461
0.73549128
0.73533934
0.73511606
0.73487139
0.73465282
0.73439825
0.73420995
INFO - Training [62][  120/  196]   Loss 0.286149   Top1 90.048828   Top5 99.111328   BatchTime 0.387984   LR 0.000102
0.73414969
0.73413694
0.73406559
0.73397547
0.73389637
0.73381287
0.73376858
0.73378325
0.73371512
0.73357582
0.73352265
0.73368555
0.73392266
0.73410583
0.73418862
0.73418975
0.73420554
INFO - Training [62][  140/  196]   Loss 0.286650   Top1 90.064174   Top5 99.151786   BatchTime 0.381347   LR 0.000100
0.73420709
0.73413694
0.73395842
0.73391062
0.73390728
0.73392004
0.73401046
0.73412549
0.73423707
0.73439133
0.73452210
0.73457539
0.73465687
0.73470175
0.73474967
0.73484778
0.73492700
0.73492986
0.73495680
0.73499364
0.73511237
0.73523599
INFO - Training [62][  160/  196]   Loss 0.291516   Top1 89.907227   Top5 99.167480   BatchTime 0.380292   LR 0.000097
0.73535287
0.73547643
0.73560697
0.73575598
0.73592860
0.73614365
0.73631918
0.73646641
0.73659480
0.73671061
0.73675591
0.73678166
0.73677391
0.73677039
0.73678154
0.73681653
INFO - Training [62][  180/  196]   Loss 0.290730   Top1 89.913194   Top5 99.127604   BatchTime 0.379849   LR 0.000094
0.73688722
0.73694128
0.73699158
0.73702133
0.73708493
0.73711967
0.73714852
0.73717600
0.73717320
0.73720568
0.73724622
0.73727506
0.73728162
0.73728752
0.73731077
0.73737001
INFO - ==> Top1: 89.928    Top5: 99.108    Loss: 0.291
0.73736000
0.73733991
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [62][   20/   40]   Loss 0.309234   Top1 90.722656   Top5 99.648438   BatchTime 0.123538
INFO - Validation [62][   40/   40]   Loss 0.296243   Top1 90.890000   Top5 99.720000   BatchTime 0.087430
INFO - ==> Top1: 90.890    Top5: 99.720    Loss: 0.296
INFO - ==> Sparsity : 0.512
INFO - Scoreboard best 1 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [49][Top1: 90.470   Top5: 99.760]
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.3457)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0404)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0576)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0326)
features.4.conv.0 tensor(0.0352)
features.4.conv.3 tensor(0.1019)
features.4.conv.6 tensor(0.0905)
features.5.conv.0 tensor(0.0257)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1323)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0457)
features.7.conv.0 tensor(0.0610)
features.7.conv.3 tensor(0.1062)
features.7.conv.6 tensor(0.1639)
features.8.conv.0 tensor(0.0625)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.1944)
features.9.conv.0 tensor(0.1031)
features.9.conv.3 tensor(0.1418)
features.9.conv.6 tensor(0.2712)
features.10.conv.0 tensor(0.0354)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.1043)
features.11.conv.0 tensor(0.3328)
features.11.conv.3 tensor(0.1179)
features.11.conv.6 tensor(0.5356)
features.12.conv.0 tensor(0.2947)
features.12.conv.3 tensor(0.1273)
features.12.conv.6 tensor(0.5975)
features.13.conv.0 tensor(0.0909)
features.13.conv.3 tensor(0.1456)
features.13.conv.6 tensor(0.2995)
features.14.conv.0 tensor(0.9656)
features.14.conv.3 tensor(0.0995)
features.14.conv.6 tensor(0.9568)
features.15.conv.0 tensor(0.9504)
features.15.conv.3 tensor(0.0670)
features.15.conv.6 tensor(0.9862)
features.16.conv.0 tensor(0.1773)
features.16.conv.3 tensor(0.1053)
features.16.conv.6 tensor(0.3540)
conv.0 tensor(0.5520)
tensor(1120466.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
0.73730808
0.73728484
0.73728037
0.73726219
0.73725176
0.73722172
0.73717678
0.73710877
0.73705649
0.73704112
0.73704451
0.73700780
0.73699528
0.73697782
0.73695952
0.73686892
0.73680258
0.73672879
0.73668289
0.73667479
INFO - Training [63][   20/  196]   Loss 0.321935   Top1 89.082031   Top5 98.515625   BatchTime 0.441032   LR 0.000090
0.73669726
0.73668659
0.73667908
0.73669571
0.73671252
0.73668224
0.73667312
0.73667133
0.73665577
0.73667753
0.73669130
0.73672080
0.73673731
0.73676217
0.73680079
0.73684204
0.73681045
0.73676950
0.73672426
0.73669368
0.73666549
INFO - Training [63][   40/  196]   Loss 0.313486   Top1 89.238281   Top5 98.662109   BatchTime 0.408000   LR 0.000087
0.73664665
0.73664373
0.73663270
0.73663241
0.73659617
0.73658168
0.73654163
0.73650998
0.73651695
0.73653018
0.73651028
0.73649788
0.73649287
0.73648810
0.73649025
0.73647457
0.73647523
0.73643559
0.73641282
0.73635745
0.73630869
0.73624253
INFO - Training [63][   60/  196]   Loss 0.304476   Top1 89.628906   Top5 98.723958   BatchTime 0.396063   LR 0.000085
0.73617709
0.73612326
0.73608077
0.73604017
0.73598003
0.73588461
0.73580498
0.73575956
0.73572993
0.73569828
0.73568624
0.73564601
0.73559535
0.73555672
0.73550421
0.73545694
INFO - Training [63][   80/  196]   Loss 0.299936   Top1 89.775391   Top5 98.862305   BatchTime 0.390378   LR 0.000082
0.73538607
0.73533452
0.73528159
0.73523396
0.73518842
0.73515373
0.73511362
0.73513550
0.73516732
0.73516196
0.73518229
0.73521292
0.73522377
0.73525113
0.73530275
0.73535675
0.73540550
0.73541987
0.73536706
0.73532373
0.73530579
0.73527116
INFO - Training [63][  100/  196]   Loss 0.293587   Top1 90.089844   Top5 98.910156   BatchTime 0.383619   LR 0.000080
0.73522937
0.73521048
0.73518306
0.73518026
0.73515338
0.73517847
0.73518640
0.73521912
0.73520297
0.73518902
0.73522353
0.73524451
0.73528409
0.73529410
0.73531711
0.73532152
0.73531395
0.73528087
0.73527932
0.73529738
0.73532552
INFO - Training [63][  120/  196]   Loss 0.289002   Top1 90.224609   Top5 98.981120   BatchTime 0.370641   LR 0.000077
0.73533988
0.73532182
0.73528892
0.73526311
0.73525375
0.73525441
0.73522210
0.73520714
0.73523635
0.73524290
0.73527753
0.73528898
0.73526114
0.73524863
0.73526853
0.73531806
0.73536927
0.73541093
0.73544145
0.73547518
INFO - Training [63][  140/  196]   Loss 0.285442   Top1 90.401786   Top5 99.031808   BatchTime 0.372292   LR 0.000075
0.73547733
0.73547542
0.73545796
0.73539376
0.73532420
0.73525691
0.73523927
0.73523217
0.73521423
0.73520684
0.73521292
0.73521197
0.73520064
0.73515755
0.73512435
0.73512334
0.73509568
0.73504049
0.73497748
INFO - Training [63][  160/  196]   Loss 0.287709   Top1 90.283203   Top5 99.057617   BatchTime 0.376972   LR 0.000072
0.73486632
0.73478800
0.73473662
0.73467928
0.73462683
0.73458099
0.73455071
0.73447371
0.73443043
0.73437268
0.73432821
0.73432404
0.73431444
0.73429245
0.73430198
0.73431391
0.73432404
0.73435217
0.73437172
0.73438090
INFO - Training [63][  180/  196]   Loss 0.288247   Top1 90.203993   Top5 99.040799   BatchTime 0.380536   LR 0.000070
0.73439461
0.73441809
0.73446161
0.73450786
0.73457998
0.73468369
0.73475885
0.73479789
0.73479980
0.73481673
0.73484731
0.73485523
0.73483717
0.73478830
0.73477805
INFO - ==> Top1: 90.224    Top5: 99.042    Loss: 0.287
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [63][   20/   40]   Loss 0.303910   Top1 90.996094   Top5 99.667969   BatchTime 0.176145
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3496)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0399)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0573)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0540)
features.3.conv.6 tensor(0.0310)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.1013)
features.4.conv.6 tensor(0.0951)
features.5.conv.0 tensor(0.0262)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.1284)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0459)
features.7.conv.0 tensor(0.0609)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.1744)
features.8.conv.0 tensor(0.0730)
features.8.conv.3 tensor(0.1218)
features.8.conv.6 tensor(0.1819)
features.9.conv.0 tensor(0.1022)
features.9.conv.3 tensor(0.1447)
features.9.conv.6 tensor(0.2804)
features.10.conv.0 tensor(0.0356)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.1025)
features.11.conv.0 tensor(0.3346)
features.11.conv.3 tensor(0.1190)
features.11.conv.6 tensor(0.5389)
features.12.conv.0 tensor(0.3360)
features.12.conv.3 tensor(0.1285)
features.12.conv.6 tensor(0.5987)
features.13.conv.0 tensor(0.0929)
features.13.conv.3 tensor(0.1472)
features.13.conv.6 tensor(0.3143)
features.14.conv.0 tensor(0.9660)
features.14.conv.3 tensor(0.0997)
features.14.conv.6 tensor(0.9570)
features.15.conv.0 tensor(0.9506)
features.15.conv.3 tensor(0.0671)
features.15.conv.6 tensor(0.9865)
features.16.conv.0 tensor(0.1521)
features.16.conv.3 tensor(0.1035)
features.16.conv.6 tensor(0.3786)
conv.0 tensor(0.5534)
tensor(1129358.) 2188896.0
INFO - Validation [63][   40/   40]   Loss 0.284531   Top1 91.060000   Top5 99.730000   BatchTime 0.126344
INFO - ==> Top1: 91.060    Top5: 99.730    Loss: 0.285
INFO - ==> Sparsity : 0.516
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 91.060   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
0.73480797
0.73483717
0.73486173
0.73487967
0.73490310
0.73494577
0.73496258
0.73494422
0.73494440
0.73495609
0.73497212
0.73497087
0.73497164
0.73497987
0.73496884
0.73491889
0.73490018
0.73488003
0.73486418
0.73483622
0.73480088
INFO - Training [64][   20/  196]   Loss 0.301761   Top1 89.550781   Top5 98.554688   BatchTime 0.446339   LR 0.000066
0.73479426
0.73475707
0.73471081
0.73467904
0.73463935
0.73461479
0.73460072
0.73459738
0.73457706
0.73456562
0.73459959
0.73462009
0.73463684
0.73466700
0.73467767
0.73467880
0.73470074
0.73469996
0.73469770
0.73464239
0.73459560
0.73456925
INFO - Training [64][   40/  196]   Loss 0.307118   Top1 89.433594   Top5 98.681641   BatchTime 0.405133   LR 0.000064
0.73454201
0.73451579
0.73450065
0.73447651
0.73443830
0.73441392
0.73441088
0.73440146
0.73439586
0.73439282
0.73438746
0.73438567
0.73438972
0.73441976
0.73441845
0.73443532
INFO - Training [64][   60/  196]   Loss 0.303114   Top1 89.635417   Top5 98.763021   BatchTime 0.391221   LR 0.000062
0.73445320
0.73448598
0.73451918
0.73453885
0.73453373
0.73453808
0.73451376
0.73448455
0.73445660
0.73443913
0.73442674
0.73440295
0.73440599
0.73436034
0.73434573
0.73430902
0.73426235
0.73424625
0.73421186
0.73420882
0.73420006
0.73415399
0.73411041
0.73406082
INFO - Training [64][   80/  196]   Loss 0.296572   Top1 89.785156   Top5 98.920898   BatchTime 0.381270   LR 0.000059
0.73399210
0.73395878
0.73394543
0.73392117
0.73386276
0.73381472
0.73374420
0.73371315
0.73369384
0.73364979
0.73359364
0.73353416
0.73350936
0.73346978
0.73341650
0.73337972
0.73332560
0.73326838
INFO - Training [64][  100/  196]   Loss 0.294102   Top1 89.867188   Top5 98.949219   BatchTime 0.369739   LR 0.000057
0.73320657
0.73320800
0.73315203
0.73309338
0.73306203
0.73300797
0.73294687
0.73290211
0.73284340
0.73281050
0.73277903
0.73273402
0.73266548
0.73262370
0.73256379
0.73253405
0.73249662
0.73249882
0.73247135
0.73247558
INFO - Training [64][  120/  196]   Loss 0.287026   Top1 90.081380   Top5 99.033203   BatchTime 0.373368   LR 0.000055
0.73245311
0.73248029
0.73249680
0.73245972
0.73246056
0.73243803
0.73241889
0.73242819
0.73240346
0.73234785
0.73232931
0.73229367
0.73225492
0.73222005
0.73219836
0.73218256
0.73217583
0.73218775
0.73213398
0.73206699
INFO - Training [64][  140/  196]   Loss 0.285500   Top1 90.153460   Top5 99.076451   BatchTime 0.376545   LR 0.000053
0.73198622
0.73192173
0.73186201
0.73181891
0.73179740
0.73177785
0.73175657
0.73167717
0.73160142
0.73152524
0.73139662
0.73128879
0.73117024
0.73108310
0.73097456
0.73090225
0.73091388
0.73094887
0.73096508
0.73096400
0.73094916
INFO - Training [64][  160/  196]   Loss 0.287613   Top1 90.083008   Top5 99.074707   BatchTime 0.377550   LR 0.000051
0.73093271
0.73090374
0.73095423
0.73093456
0.73090923
0.73085999
0.73082709
0.73078603
0.73077542
0.73076457
0.73073769
0.73074013
0.73067719
0.73065454
0.73063707
0.73064882
0.73069012
0.73067790
0.73061788
0.73056751
INFO - Training [64][  180/  196]   Loss 0.290060   Top1 90.008681   Top5 99.029948   BatchTime 0.380356   LR 0.000049
0.73052698
0.73051590
0.73053902
0.73056650
0.73057747
0.73059613
0.73062867
0.73061460
0.73060966
0.73060465
0.73060173
0.73059583
0.73058027
0.73055238
********************pre-trained*****************
INFO - ==> Top1: 90.074    Top5: 99.022    Loss: 0.289
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [64][   20/   40]   Loss 0.344365   Top1 89.316406   Top5 99.492188   BatchTime 0.124945
INFO - Validation [64][   40/   40]   Loss 0.330003   Top1 89.510000   Top5 99.670000   BatchTime 0.086575
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0395)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0582)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0347)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.0957)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1211)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0619)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.1632)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1195)
features.8.conv.6 tensor(0.1840)
features.9.conv.0 tensor(0.1036)
features.9.conv.3 tensor(0.1441)
features.9.conv.6 tensor(0.2776)
features.10.conv.0 tensor(0.0365)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.1074)
features.11.conv.0 tensor(0.3395)
features.11.conv.3 tensor(0.1179)
features.11.conv.6 tensor(0.5354)
features.12.conv.0 tensor(0.3579)
features.12.conv.3 tensor(0.1273)
features.12.conv.6 tensor(0.5985)
features.13.conv.0 tensor(0.0934)
features.13.conv.3 tensor(0.1485)
features.13.conv.6 tensor(0.3229)
features.14.conv.0 tensor(0.9661)
features.14.conv.3 tensor(0.0994)
features.14.conv.6 tensor(0.9570)
features.15.conv.0 tensor(0.9508)
features.15.conv.3 tensor(0.0662)
features.15.conv.6 tensor(0.9866)
features.16.conv.0 tensor(0.1685)
features.16.conv.3 tensor(0.1047)
features.16.conv.6 tensor(0.4735)
conv.0 tensor(0.5615)
tensor(1166263.) 2188896.0
INFO - ==> Top1: 89.510    Top5: 99.670    Loss: 0.330
INFO - ==> Sparsity : 0.533
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 91.060   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
0.73052573
0.73047829
0.73033565
0.73021883
0.73008502
0.72992003
0.72982186
0.72970897
0.72957277
0.72946304
0.72935581
0.72925329
0.72915584
0.72917986
0.72919005
0.72919387
0.72925723
0.72925776
0.72921836
0.72918868
INFO - Training [65][   20/  196]   Loss 0.323953   Top1 88.320312   Top5 98.496094   BatchTime 0.453010   LR 0.000046
0.72910982
0.72906816
0.72908938
0.72908020
0.72915447
0.72924626
0.72933972
0.72942859
0.72950554
0.72952294
0.72954243
0.72952920
0.72953123
0.72957492
0.72956836
0.72955811
0.72952718
0.72954828
0.72954357
0.72959161
0.72960770
0.72959542
INFO - Training [65][   40/  196]   Loss 0.316872   Top1 88.730469   Top5 98.769531   BatchTime 0.405960   LR 0.000044
0.72961593
0.72958976
0.72953922
0.72947031
0.72936296
0.72926384
0.72914821
0.72904837
0.72899705
0.72891283
0.72887969
0.72885728
0.72882068
0.72876799
0.72863197
0.72854817
0.72850895
0.72853619
0.72856289
0.72854537
INFO - Training [65][   60/  196]   Loss 0.313599   Top1 89.082031   Top5 98.847656   BatchTime 0.402222   LR 0.000042
0.72859597
0.72866547
0.72870064
0.72870862
0.72865194
0.72867924
0.72872835
0.72880352
0.72889292
0.72895390
0.72902411
0.72909021
0.72913677
0.72925830
0.72938776
0.72951305
0.72968918
0.72985035
0.73000598
INFO - Training [65][   80/  196]   Loss 0.307213   Top1 89.287109   Top5 99.008789   BatchTime 0.381779   LR 0.000040
0.73013151
0.73027194
0.73036957
0.73045057
0.73053128
0.73064727
0.73073894
0.73079926
0.73087287
0.73092258
0.73097491
0.73096669
0.73098159
0.73098689
0.73096484
0.73097676
0.73093480
0.73089969
0.73089027
0.73087627
INFO - Training [65][  100/  196]   Loss 0.299401   Top1 89.546875   Top5 99.070312   BatchTime 0.385313   LR 0.000039
0.73087323
0.73086828
0.73086065
0.73085839
0.73087841
0.73090416
0.73094815
0.73097664
0.73098004
0.73099822
0.73100853
0.73102480
0.73099065
0.73096329
0.73093784
0.73089701
0.73087448
0.73080975
0.73074663
0.73065680
0.73058254
INFO - Training [65][  120/  196]   Loss 0.293367   Top1 89.830729   Top5 99.101562   BatchTime 0.384776   LR 0.000037
0.73052269
0.73042619
0.73034859
0.73028713
0.73023337
0.73019361
0.73018312
0.73014122
0.73011106
0.73008794
0.73002833
0.73000968
0.72998029
0.72994208
0.72989547
0.72984111
0.72982734
INFO - Training [65][  140/  196]   Loss 0.292897   Top1 89.902344   Top5 99.121094   BatchTime 0.382341   LR 0.000035
0.72982025
0.72979403
0.72976178
0.72970217
0.72967565
0.72963804
0.72956711
0.72946787
0.72943443
0.72940558
0.72940511
0.72938603
0.72938311
0.72937280
0.72938585
0.72940379
0.72937065
0.72931862
0.72929496
0.72927105
0.72923255
INFO - Training [65][  160/  196]   Loss 0.293149   Top1 89.934082   Top5 99.096680   BatchTime 0.381294   LR 0.000033
0.72917640
0.72916323
0.72914666
0.72914135
0.72912198
0.72910255
0.72911441
0.72911263
0.72913939
0.72915101
0.72913963
0.72909421
0.72907102
0.72905040
0.72906965
0.72906238
0.72905743
0.72909254
0.72912186
0.72914535
0.72918248
INFO - Training [65][  180/  196]   Loss 0.292363   Top1 89.991319   Top5 99.034288   BatchTime 0.381312   LR 0.000032
0.72924924
0.72927070
0.72931904
0.72934222
0.72935396
0.72937077
0.72940719
0.72941262
0.72942877
0.72944933
0.72949749
0.72949916
0.72951460
0.72951657
0.72952306
INFO - ==> Top1: 90.000    Top5: 99.032    Loss: 0.292
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [65][   20/   40]   Loss 0.401857   Top1 87.714844   Top5 99.433594   BatchTime 0.120660
features.0.conv.0 tensor(0.5903)
features.0.conv.3 tensor(0.3379)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0576)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0540)
features.3.conv.6 tensor(0.0312)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.1007)
features.4.conv.6 tensor(0.1021)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.1211)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0623)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.1679)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.1813)
features.9.conv.0 tensor(0.1042)
features.9.conv.3 tensor(0.1424)
features.9.conv.6 tensor(0.2801)
features.10.conv.0 tensor(0.0365)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.1155)
features.11.conv.0 tensor(0.3451)
features.11.conv.3 tensor(0.1188)
features.11.conv.6 tensor(0.5397)
features.12.conv.0 tensor(0.3468)
features.12.conv.3 tensor(0.1262)
features.12.conv.6 tensor(0.6014)
features.13.conv.0 tensor(0.0937)
features.13.conv.3 tensor(0.1481)
features.13.conv.6 tensor(0.3351)
features.14.conv.0 tensor(0.9662)
features.14.conv.3 tensor(0.0993)
features.14.conv.6 tensor(0.9578)
features.15.conv.0 tensor(0.9508)
features.15.conv.3 tensor(0.0662)
features.15.conv.6 tensor(0.9865)
features.16.conv.0 tensor(0.1572)
features.16.conv.3 tensor(0.1049)
features.16.conv.6 tensor(0.4731)
conv.0 tensor(0.5620)
tensor(1166443.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.384993   Top1 87.910000   Top5 99.540000   BatchTime 0.086072
INFO - ==> Top1: 87.910    Top5: 99.540    Loss: 0.385
INFO - ==> Sparsity : 0.533
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 91.060   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
0.72954124
0.72958308
0.72962862
0.72964978
0.72968394
0.72971833
0.72973812
0.72974390
0.72975636
0.72977126
0.72979659
0.72980404
0.72981226
0.72981167
0.72982192
0.72985911
0.72989815
0.72992247
0.72995007
INFO - Training [66][   20/  196]   Loss 0.312144   Top1 89.101562   Top5 98.574219   BatchTime 0.438551   LR 0.000029
0.72997242
0.72995079
0.72992891
0.72988409
0.72983682
0.72981405
0.72976893
0.72974128
0.72972918
0.72970909
0.72968799
0.72965264
0.72960562
0.72956955
0.72954267
0.72954422
0.72950703
0.72950703
0.72949475
0.72947085
0.72944397
0.72943383
0.72940892
INFO - Training [66][   40/  196]   Loss 0.305017   Top1 89.638672   Top5 98.818359   BatchTime 0.396479   LR 0.000028
0.72938854
0.72937274
0.72933495
0.72933066
0.72929054
0.72926039
0.72921318
0.72919846
0.72915435
0.72909558
0.72905582
0.72901994
0.72897798
0.72896916
0.72896779
0.72897786
0.72898448
0.72898763
0.72899270
0.72898090
0.72898304
INFO - Training [66][   60/  196]   Loss 0.305576   Top1 89.687500   Top5 98.854167   BatchTime 0.387750   LR 0.000026
0.72899002
0.72900754
0.72904021
0.72903574
0.72903687
0.72903317
0.72905034
0.72904980
0.72908169
0.72907674
0.72907692
0.72908068
0.72908705
0.72911316
0.72914791
0.72914064
0.72913677
0.72914684
0.72917825
0.72919923
INFO - Training [66][   80/  196]   Loss 0.302526   Top1 89.707031   Top5 98.955078   BatchTime 0.371213   LR 0.000025
0.72920841
0.72923380
0.72928965
0.72931808
0.72936845
0.72943825
0.72946173
0.72952223
0.72954118
0.72957611
0.72959900
0.72964180
0.72968358
0.72968864
0.72970474
0.72972691
INFO - Training [66][  100/  196]   Loss 0.295286   Top1 89.894531   Top5 99.000000   BatchTime 0.370771   LR 0.000023
0.72974926
0.72976452
0.72978079
0.72979122
0.72978389
0.72978401
0.72978169
0.72980613
0.72982544
0.72983015
0.72982556
0.72981554
0.72981912
0.72983229
0.72981125
0.72978514
0.72977918
0.72974372
0.72971076
0.72968102
0.72966248
INFO - Training [66][  120/  196]   Loss 0.290912   Top1 90.081380   Top5 99.036458   BatchTime 0.369617   LR 0.000022
0.72962916
0.72961873
0.72960699
0.72958404
0.72955185
0.72951466
0.72946930
0.72943544
0.72940964
0.72939104
0.72936350
0.72937542
0.72934926
0.72933996
0.72933918
0.72931868
0.72927827
0.72925919
0.72925431
0.72925031
0.72924858
0.72926104
INFO - Training [66][  140/  196]   Loss 0.289814   Top1 90.114397   Top5 99.107143   BatchTime 0.369742   LR 0.000021
0.72925425
0.72923547
0.72923046
0.72919816
0.72917366
0.72914618
0.72913563
0.72912127
0.72910607
0.72910553
0.72909737
0.72910202
0.72908759
0.72906727
0.72906911
0.72907424
0.72906327
0.72904003
INFO - Training [66][  160/  196]   Loss 0.292073   Top1 89.987793   Top5 99.089355   BatchTime 0.367202   LR 0.000019
0.72900796
0.72898322
0.72898459
0.72897798
0.72899508
0.72899717
0.72899103
0.72899300
0.72898299
0.72901118
0.72900897
0.72899431
0.72898930
0.72900671
0.72900844
0.72899967
0.72901523
0.72903812
0.72906363
0.72908568
0.72909170
INFO - Training [66][  180/  196]   Loss 0.291248   Top1 90.008681   Top5 99.066840   BatchTime 0.367437   LR 0.000018
0.72910184
0.72910029
0.72910362
0.72908592
0.72907931
0.72904509
0.72902334
0.72900158
0.72899967
0.72898418
0.72898698
0.72896916
0.72896230
0.72901028
0.72902387
********************pre-trained*****************
INFO - ==> Top1: 90.082    Top5: 99.058    Loss: 0.289
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [66][   20/   40]   Loss 0.343060   Top1 89.375000   Top5 99.472656   BatchTime 0.119947
INFO - Validation [66][   40/   40]   Loss 0.326857   Top1 89.670000   Top5 99.590000   BatchTime 0.085796
features.0.conv.0 tensor(0.5868)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0573)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0352)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.1071)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1188)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0467)
features.7.conv.0 tensor(0.0630)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.1681)
features.8.conv.0 tensor(0.0651)
features.8.conv.3 tensor(0.1195)
features.8.conv.6 tensor(0.1825)
features.9.conv.0 tensor(0.1040)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.2802)
features.10.conv.0 tensor(0.0368)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.1100)
features.11.conv.0 tensor(0.3449)
features.11.conv.3 tensor(0.1177)
features.11.conv.6 tensor(0.5445)
features.12.conv.0 tensor(0.3409)
features.12.conv.3 tensor(0.1265)
features.12.conv.6 tensor(0.6018)
features.13.conv.0 tensor(0.0940)
features.13.conv.3 tensor(0.1466)
features.13.conv.6 tensor(0.3352)
features.14.conv.0 tensor(0.9662)
features.14.conv.3 tensor(0.0991)
features.14.conv.6 tensor(0.9582)
features.15.conv.0 tensor(0.9508)
features.15.conv.3 tensor(0.0669)
features.15.conv.6 tensor(0.9871)
features.16.conv.0 tensor(0.1741)
features.16.conv.3 tensor(0.1061)
features.16.conv.6 tensor(0.4693)
conv.0 tensor(0.5633)
tensor(1168438.) 2188896.0
INFO - ==> Top1: 89.670    Top5: 99.590    Loss: 0.327
INFO - ==> Sparsity : 0.534
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 91.060   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
0.72906381
0.72907269
0.72908467
0.72909486
0.72912925
0.72915244
0.72916794
0.72916865
0.72918397
0.72919083
0.72918409
0.72920024
0.72922444
0.72924858
0.72924256
0.72925287
0.72925055
0.72926861
0.72928077
0.72928768
INFO - Training [67][   20/  196]   Loss 0.311924   Top1 89.218750   Top5 98.671875   BatchTime 0.440286   LR 0.000016
0.72931427
0.72934061
0.72936511
0.72937709
0.72939795
0.72940522
0.72940713
0.72940785
0.72940010
0.72939110
0.72940111
0.72939312
0.72938484
0.72938627
0.72938973
0.72939146
0.72939104
0.72937441
0.72937632
0.72937077
INFO - Training [67][   40/  196]   Loss 0.310370   Top1 89.257812   Top5 98.798828   BatchTime 0.411675   LR 0.000015
0.72937107
0.72936887
0.72936887
0.72936893
0.72937328
0.72936779
0.72938335
0.72938108
0.72938877
0.72938836
0.72935385
0.72932994
0.72930485
0.72928005
0.72924209
0.72922605
0.72920442
0.72918087
0.72916359
0.72914249
0.72912037
0.72911221
INFO - Training [67][   60/  196]   Loss 0.301491   Top1 89.700521   Top5 98.841146   BatchTime 0.396285   LR 0.000014
0.72909826
0.72908819
0.72907627
0.72906423
0.72904122
0.72901493
0.72900450
0.72897923
0.72894675
0.72894353
0.72893280
0.72893834
0.72891325
0.72890621
0.72888666
0.72886521
0.72885412
0.72885370
0.72883523
0.72883332
INFO - Training [67][   80/  196]   Loss 0.301345   Top1 89.721680   Top5 99.003906   BatchTime 0.375676   LR 0.000013
0.72883314
0.72881866
0.72880769
0.72882038
0.72881323
0.72880119
0.72878009
0.72877961
0.72876102
0.72872502
0.72870368
0.72868145
0.72868133
0.72868150
0.72868001
0.72868019
0.72868317
0.72866398
0.72862774
INFO - Training [67][  100/  196]   Loss 0.298692   Top1 89.851562   Top5 99.035156   BatchTime 0.363143   LR 0.000012
0.72861737
0.72860211
0.72861314
0.72860742
0.72859430
0.72858649
0.72858363
0.72857934
0.72856820
0.72857088
0.72857171
0.72857815
0.72856069
0.72855830
0.72856373
0.72855818
0.72855490
0.72855991
0.72854447
0.72852743
0.72853577
0.72854143
INFO - Training [67][  120/  196]   Loss 0.292524   Top1 90.042318   Top5 99.098307   BatchTime 0.362415   LR 0.000011
0.72853535
0.72852987
0.72852355
0.72852594
0.72852302
0.72850281
0.72852033
0.72851115
0.72852772
0.72851813
0.72853154
0.72852969
0.72852826
0.72851419
0.72850394
0.72849566
INFO - Training [67][  140/  196]   Loss 0.291534   Top1 90.103237   Top5 99.151786   BatchTime 0.364682   LR 0.000010
0.72850329
0.72849274
0.72849578
0.72848260
0.72847801
0.72848880
0.72848469
0.72847724
0.72846889
0.72845608
0.72842675
0.72840744
0.72839522
0.72837603
0.72836542
0.72836006
0.72835195
0.72834182
0.72834575
0.72833282
0.72833252
INFO - Training [67][  160/  196]   Loss 0.292822   Top1 90.014648   Top5 99.140625   BatchTime 0.365741   LR 0.000009
0.72833860
0.72832680
0.72830570
0.72829044
0.72828782
0.72827154
0.72826952
0.72826642
0.72827142
0.72827953
0.72828156
0.72827667
0.72828537
0.72828484
0.72827619
0.72827494
0.72827959
0.72829312
0.72830099
0.72830403
0.72832906
INFO - Training [67][  180/  196]   Loss 0.292294   Top1 90.032552   Top5 99.099392   BatchTime 0.367964   LR 0.000008
0.72832286
0.72833252
0.72833484
0.72832984
0.72832918
0.72831941
0.72831738
0.72831702
0.72834253
0.72836769
0.72836572
0.72836620
0.72836506
0.72836918
0.72837526
********************pre-trained*****************
INFO - ==> Top1: 90.098    Top5: 99.088    Loss: 0.290
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [67][   20/   40]   Loss 0.314443   Top1 90.253906   Top5 99.531250   BatchTime 0.125416
INFO - Validation [67][   40/   40]   Loss 0.299343   Top1 90.470000   Top5 99.710000   BatchTime 0.090849
INFO - ==> Top1: 90.470    Top5: 99.710    Loss: 0.299
INFO - ==> Sparsity : 0.535
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 91.060   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 90.510   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5972)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0576)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0312)
features.4.conv.0 tensor(0.0348)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.1073)
features.5.conv.0 tensor(0.0272)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.1190)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0468)
features.7.conv.0 tensor(0.0625)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.1650)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.1825)
features.9.conv.0 tensor(0.1037)
features.9.conv.3 tensor(0.1429)
features.9.conv.6 tensor(0.2819)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.1100)
features.11.conv.0 tensor(0.3482)
features.11.conv.3 tensor(0.1182)
features.11.conv.6 tensor(0.5471)
features.12.conv.0 tensor(0.3389)
features.12.conv.3 tensor(0.1262)
features.12.conv.6 tensor(0.6016)
features.13.conv.0 tensor(0.0941)
features.13.conv.3 tensor(0.1474)
features.13.conv.6 tensor(0.3390)
features.14.conv.0 tensor(0.9663)
features.14.conv.3 tensor(0.0988)
features.14.conv.6 tensor(0.9585)
features.15.conv.0 tensor(0.9509)
features.15.conv.3 tensor(0.0666)
features.15.conv.6 tensor(0.9870)
features.16.conv.0 tensor(0.1770)
features.16.conv.3 tensor(0.1063)
features.16.conv.6 tensor(0.4717)
conv.0 tensor(0.5678)
tensor(1171974.) 2188896.0
0.72838628
0.72840077
0.72842717
0.72842890
0.72843003
0.72843295
0.72843724
0.72843081
0.72842664
0.72840947
0.72839010
0.72837222
0.72837502
0.72834855
0.72835171
0.72834599
0.72836101
0.72835273
0.72834587
0.72832793
0.72833890
INFO - Training [68][   20/  196]   Loss 0.301421   Top1 89.335938   Top5 98.613281   BatchTime 0.440813   LR 0.000007
0.72833848
0.72834039
0.72834599
0.72834641
0.72834182
0.72834033
0.72833723
0.72832984
0.72831655
0.72831172
0.72829133
0.72829491
0.72830421
0.72829896
0.72829384
0.72830063
0.72829479
0.72827613
0.72828418
0.72826362
0.72825521
0.72825086
INFO - Training [68][   40/  196]   Loss 0.306573   Top1 89.375000   Top5 98.740234   BatchTime 0.402851   LR 0.000006
0.72823864
0.72822577
0.72819686
0.72819877
0.72819668
0.72819364
0.72818369
0.72817624
0.72817588
0.72816825
0.72817075
0.72816795
0.72816873
0.72817963
0.72818857
0.72819018
INFO - Training [68][   60/  196]   Loss 0.299816   Top1 89.602865   Top5 98.776042   BatchTime 0.393145   LR 0.000006
0.72818083
0.72817916
0.72817564
0.72817981
0.72819233
0.72819841
0.72820407
0.72820121
0.72821581
0.72820550
0.72822255
0.72823840
0.72824222
0.72826028
0.72825545
0.72826654
0.72826713
0.72827548
0.72827750
0.72827798
0.72827047
0.72827083
INFO - Training [68][   80/  196]   Loss 0.296916   Top1 89.794922   Top5 98.867188   BatchTime 0.387532   LR 0.000005
0.72827202
0.72827441
0.72826886
0.72825974
0.72824395
0.72823364
0.72824436
0.72823918
0.72823513
0.72822398
0.72821605
0.72822124
0.72821057
0.72819287
0.72820157
0.72819173
0.72819656
0.72819978
0.72820663
0.72820312
0.72819936
0.72818941
INFO - Training [68][  100/  196]   Loss 0.288949   Top1 90.101562   Top5 98.906250   BatchTime 0.382192   LR 0.000004
0.72820270
0.72820562
0.72820550
0.72820485
0.72820550
0.72821397
0.72820550
0.72821665
0.72820711
0.72820240
0.72817957
0.72817749
0.72818524
0.72818410
0.72817647
0.72816598
0.72816062
INFO - Training [68][  120/  196]   Loss 0.284387   Top1 90.302734   Top5 98.987630   BatchTime 0.378529   LR 0.000004
0.72815931
0.72816056
0.72815031
0.72814304
0.72814369
0.72815156
0.72814804
0.72814417
0.72813952
0.72814506
0.72814941
0.72814810
0.72815651
0.72815043
0.72816199
0.72816950
0.72816426
0.72817647
0.72815651
0.72817028
0.72818112
0.72817659
INFO - Training [68][  140/  196]   Loss 0.283198   Top1 90.348772   Top5 99.048549   BatchTime 0.377480   LR 0.000003
0.72817433
0.72816354
0.72816467
0.72815675
0.72814745
0.72815233
0.72814846
0.72813183
0.72814459
0.72814143
0.72813874
0.72812897
0.72813296
0.72812414
0.72811264
0.72811800
0.72810173
0.72810256
0.72807527
0.72807395
0.72805798
INFO - Training [68][  160/  196]   Loss 0.286740   Top1 90.202637   Top5 99.035645   BatchTime 0.376449   LR 0.000003
0.72804928
0.72805840
0.72804880
0.72804362
0.72804654
0.72804028
0.72802669
0.72803408
0.72803169
0.72803766
0.72803688
0.72801703
0.72801614
0.72801411
0.72801304
0.72801280
0.72800606
INFO - Training [68][  180/  196]   Loss 0.285947   Top1 90.199653   Top5 99.016927   BatchTime 0.375421   LR 0.000002
0.72800189
0.72800320
0.72799706
0.72800893
0.72799897
0.72799760
0.72801000
0.72801971
0.72800601
0.72801590
0.72800410
0.72800756
0.72800767
0.72800845
0.72798693
0.72799671
********************pre-trained*****************
INFO - ==> Top1: 90.206    Top5: 99.020    Loss: 0.286
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [68][   20/   40]   Loss 0.312208   Top1 90.839844   Top5 99.707031   BatchTime 0.125407
features.0.conv.0 tensor(0.5903)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0408)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0573)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0348)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.1074)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1187)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0466)
features.7.conv.0 tensor(0.0628)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.1649)
features.8.conv.0 tensor(0.0647)
features.8.conv.3 tensor(0.1192)
features.8.conv.6 tensor(0.1823)
features.9.conv.0 tensor(0.1040)
features.9.conv.3 tensor(0.1441)
features.9.conv.6 tensor(0.2828)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.1099)
features.11.conv.0 tensor(0.3480)
features.11.conv.3 tensor(0.1184)
features.11.conv.6 tensor(0.5473)
features.12.conv.0 tensor(0.3393)
features.12.conv.3 tensor(0.1265)
features.12.conv.6 tensor(0.6017)
features.13.conv.0 tensor(0.0946)
features.13.conv.3 tensor(0.1478)
features.13.conv.6 tensor(0.3410)
features.14.conv.0 tensor(0.9663)
features.14.conv.3 tensor(0.0992)
features.14.conv.6 tensor(0.9585)
features.15.conv.0 tensor(0.9509)
features.15.conv.3 tensor(0.0669)
features.15.conv.6 tensor(0.9870)
features.16.conv.0 tensor(0.1796)
features.16.conv.3 tensor(0.1054)
features.16.conv.6 tensor(0.4759)
conv.0 tensor(0.5696)
tensor(1174694.) 2188896.0
INFO - Validation [68][   40/   40]   Loss 0.297833   Top1 90.930000   Top5 99.740000   BatchTime 0.090711
INFO - ==> Top1: 90.930    Top5: 99.740    Loss: 0.298
INFO - ==> Sparsity : 0.537
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 91.060   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [68][Top1: 90.930   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
0.72796482
0.72797316
0.72796875
0.72796679
0.72797358
0.72797722
0.72797853
0.72798145
0.72797376
0.72798753
0.72799069
0.72800350
0.72799486
0.72800046
0.72800165
0.72799659
0.72797519
0.72796530
0.72795701
0.72795653
0.72795343
0.72795075
0.72795320
INFO - Training [69][   20/  196]   Loss 0.304932   Top1 89.570312   Top5 98.730469   BatchTime 0.456948   LR 0.000002
0.72797078
0.72794724
0.72795027
0.72794390
0.72794658
0.72795075
0.72794771
0.72795755
0.72795546
0.72795177
0.72795039
0.72795421
0.72795182
0.72795653
0.72796988
INFO - Training [69][   40/  196]   Loss 0.308232   Top1 89.375000   Top5 98.798828   BatchTime 0.418030   LR 0.000001
0.72796041
0.72795433
0.72795659
0.72795141
0.72797751
0.72797436
0.72797585
0.72795784
0.72796261
0.72796601
0.72796667
0.72796118
0.72795612
0.72797257
0.72798008
0.72796655
0.72796965
0.72797066
0.72798818
0.72799110
0.72799700
0.72798312
INFO - Training [69][   60/  196]   Loss 0.298780   Top1 89.589844   Top5 98.964844   BatchTime 0.400488   LR 0.000001
0.72797930
0.72799438
0.72799432
0.72798640
0.72799397
0.72798359
0.72797859
0.72797400
0.72795832
0.72795582
0.72795045
0.72794640
0.72795898
0.72798133
0.72799468
0.72799718
0.72798818
0.72798103
0.72798502
0.72797686
0.72797674
INFO - Training [69][   80/  196]   Loss 0.294701   Top1 89.750977   Top5 99.057617   BatchTime 0.399987   LR 0.000001
0.72797740
0.72798574
0.72798455
0.72797787
0.72799450
0.72797722
0.72799093
0.72798675
0.72798014
0.72797543
0.72796971
0.72795433
0.72797269
0.72796822
0.72796273
0.72795653
0.72794771
0.72795153
0.72793645
INFO - Training [69][  100/  196]   Loss 0.286733   Top1 90.101562   Top5 99.089844   BatchTime 0.381503   LR 0.000000
0.72793645
0.72793794
0.72795695
0.72793907
0.72794384
0.72794682
0.72794342
0.72793674
0.72795409
0.72794110
0.72793728
0.72792494
0.72791803
0.72793144
0.72793746
0.72794926
0.72794014
0.72794086
0.72792983
0.72794598
0.72794890
0.72794187
0.72795594
0.72797424
INFO - Training [69][  120/  196]   Loss 0.282372   Top1 90.227865   Top5 99.147135   BatchTime 0.359827   LR 0.000000
0.72797835
0.72797722
0.72797829
0.72797340
0.72796917
0.72797364
0.72797662
0.72798955
0.72799146
0.72797900
0.72797805
0.72797573
0.72798687
0.72800785
0.72800279
0.72800332
0.72800475
INFO - Training [69][  140/  196]   Loss 0.280882   Top1 90.292969   Top5 99.202009   BatchTime 0.356556   LR 0.000000
0.72799009
0.72799212
0.72799450
0.72799152
0.72799546
0.72799176
0.72797751
0.72796422
0.72796810
0.72796375
0.72795981
0.72795463
0.72795564
0.72795779
0.72796357
0.72795880
0.72795814
0.72795534
0.72794873
0.72794676
0.72795182
0.72794396
INFO - Training [69][  160/  196]   Loss 0.283572   Top1 90.170898   Top5 99.189453   BatchTime 0.357172   LR 0.000000
0.72792226
0.72792202
0.72791827
0.72792399
0.72792202
0.72790956
0.72791201
0.72791231
0.72791511
0.72791988
0.72791022
0.72792345
0.72792137
0.72792614
0.72791630
0.72790903
INFO - Training [69][  180/  196]   Loss 0.285020   Top1 90.141059   Top5 99.123264   BatchTime 0.358929   LR 0.000000
0.72790873
0.72791743
0.72793633
0.72794062
0.72793472
0.72793365
0.72793245
0.72791606
0.72791952
0.72791326
0.72790867
0.72791326
0.72790676
0.72792232
0.72793055
0.72792280
0.72791916
INFO - ==> Top1: 90.168    Top5: 99.108    Loss: 0.284
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5938)
features.0.conv.3 tensor(0.3398)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0408)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0576)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0343)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.1074)
features.5.conv.0 tensor(0.0273)
features.5.conv.3
INFO - Validation [69][   20/   40]   Loss 0.360704   Top1 89.414062   Top5 99.472656   BatchTime 0.125516
INFO - Validation [69][   40/   40]   Loss 0.352113   Top1 89.160000   Top5 99.640000   BatchTime 0.088896
INFO - ==> Top1: 89.160    Top5: 99.640    Loss: 0.352
INFO - ==> Sparsity : 0.537
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 91.060   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [68][Top1: 90.930   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [62][Top1: 90.890   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/_checkpoint.pth.tar
INFO - >>>>>> Epoch -1 (final model evaluation)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1187)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0466)
features.7.conv.0 tensor(0.0627)
features.7.conv.3 tensor(0.1062)
features.7.conv.6 tensor(0.1649)
features.8.conv.0 tensor(0.0645)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.1823)
features.9.conv.0 tensor(0.1039)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.2828)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.1099)
features.11.conv.0 tensor(0.3478)
features.11.conv.3 tensor(0.1184)
features.11.conv.6 tensor(0.5474)
features.12.conv.0 tensor(0.3390)
features.12.conv.3 tensor(0.1267)
features.12.conv.6 tensor(0.6017)
features.13.conv.0 tensor(0.0945)
features.13.conv.3 tensor(0.1466)
features.13.conv.6 tensor(0.3409)
features.14.conv.0 tensor(0.9663)
features.14.conv.3 tensor(0.0990)
features.14.conv.6 tensor(0.9585)
features.15.conv.0 tensor(0.9509)
features.15.conv.3 tensor(0.0670)
features.15.conv.6 tensor(0.9870)
features.16.conv.0 tensor(0.1806)
features.16.conv.3 tensor(0.1064)
features.16.conv.6 tensor(0.4772)
conv.0 tensor(0.5695)
tensor(1175167.) 2188896.0
*************hard_pruning_mode*******************
INFO - Validation [   20/   40]   Loss 0.360704   Top1 89.414062   Top5 99.472656   BatchTime 0.125096
INFO - Validation [   40/   40]   Loss 0.352113   Top1 89.160000   Top5 99.640000   BatchTime 0.088098
INFO - ==> Top1: 89.160    Top5: 99.640    Loss: 0.352
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [0][   20/  196]   Loss 0.522496   Top1 81.464844   Top5 97.832031   BatchTime 0.408779   LR 0.004999
INFO - Training [0][   40/  196]   Loss 0.534686   Top1 81.347656   Top5 97.763672   BatchTime 0.373963   LR 0.004995
INFO - Training [0][   60/  196]   Loss 0.534703   Top1 81.367188   Top5 97.858073   BatchTime 0.362494   LR 0.004989
INFO - Training [0][   80/  196]   Loss 0.598289   Top1 79.804688   Top5 97.377930   BatchTime 0.362217   LR 0.004980
INFO - Training [0][  100/  196]   Loss 0.602411   Top1 79.507812   Top5 97.449219   BatchTime 0.359413   LR 0.004968
INFO - Training [0][  120/  196]   Loss 0.595269   Top1 79.762370   Top5 97.555339   BatchTime 0.346032   LR 0.004954
INFO - Training [0][  140/  196]   Loss 0.585381   Top1 80.036272   Top5 97.664621   BatchTime 0.348157   LR 0.004938
INFO - Training [0][  160/  196]   Loss 0.580252   Top1 80.197754   Top5 97.717285   BatchTime 0.350574   LR 0.004919
INFO - Training [0][  180/  196]   Loss 0.573890   Top1 80.353733   Top5 97.708333   BatchTime 0.349694   LR 0.004897
INFO - ==> Top1: 80.452    Top5: 97.758    Loss: 0.570
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.416289   Top1 85.800781   Top5 99.414062   BatchTime 0.126241
INFO - Validation [0][   40/   40]   Loss 0.415516   Top1 85.350000   Top5 99.560000   BatchTime 0.090308
INFO - ==> Top1: 85.350    Top5: 99.560    Loss: 0.416
INFO - ==> Sparsity : 0.605
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 85.350   Top5: 99.560]
features.0.conv.0 tensor(0.5312)
features.0.conv.3 tensor(0.3535)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0564)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0278)
features.4.conv.0 tensor(0.0365)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.1689)
features.5.conv.0 tensor(0.0244)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.1820)
features.6.conv.0 tensor(0.0187)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0475)
features.7.conv.0 tensor(0.0670)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.2422)
features.8.conv.0 tensor(0.0621)
features.8.conv.3 tensor(0.1288)
features.8.conv.6 tensor(0.2651)
features.9.conv.0 tensor(0.1122)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.3634)
features.10.conv.0 tensor(0.0438)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.1760)
features.11.conv.0 tensor(0.4362)
features.11.conv.3 tensor(0.1422)
features.11.conv.6 tensor(0.6420)
features.12.conv.0 tensor(0.4154)
features.12.conv.3 tensor(0.1499)
features.12.conv.6 tensor(0.6831)
features.13.conv.0 tensor(0.1077)
features.13.conv.3 tensor(0.1578)
features.13.conv.6 tensor(0.1953)
features.14.conv.0 tensor(0.9609)
features.14.conv.3 tensor(0.1192)
features.14.conv.6 tensor(0.9681)
features.15.conv.0 tensor(0.9716)
features.15.conv.3 tensor(0.1020)
features.15.conv.6 tensor(0.9646)
features.16.conv.0 tensor(0.2995)
features.16.conv.3 tensor(0.1557)
features.16.conv.6 tensor(0.5966)
conv.0 tensor(0.7552)
tensor(1323458.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [1][   20/  196]   Loss 0.536056   Top1 81.035156   Top5 98.144531   BatchTime 0.409701   LR 0.004853
INFO - Training [1][   40/  196]   Loss 0.531802   Top1 81.542969   Top5 98.017578   BatchTime 0.375688   LR 0.004825
INFO - Training [1][   60/  196]   Loss 0.525367   Top1 81.712240   Top5 98.138021   BatchTime 0.372223   LR 0.004794
INFO - Training [1][   80/  196]   Loss 0.519640   Top1 81.962891   Top5 98.251953   BatchTime 0.367870   LR 0.004761
INFO - Training [1][  100/  196]   Loss 0.509280   Top1 82.269531   Top5 98.273438   BatchTime 0.359293   LR 0.004725
INFO - Training [1][  120/  196]   Loss 0.503178   Top1 82.539062   Top5 98.333333   BatchTime 0.347358   LR 0.004687
INFO - Training [1][  140/  196]   Loss 0.499888   Top1 82.647879   Top5 98.390067   BatchTime 0.348964   LR 0.004647
INFO - Training [1][  160/  196]   Loss 0.512559   Top1 82.133789   Top5 98.088379   BatchTime 0.352653   LR 0.004605
INFO - Training [1][  180/  196]   Loss 0.511966   Top1 82.135417   Top5 98.070747   BatchTime 0.351004   LR 0.004560
********************pre-trained*****************
INFO - ==> Top1: 82.206    Top5: 98.086    Loss: 0.510
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.397145   Top1 86.640625   Top5 99.453125   BatchTime 0.119432
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.3535)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0369)
features.2.conv.0 tensor(0.0226)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0556)
features.3.conv.0 tensor(0.0139)
features.3.conv.3 tensor(0.0540)
features.3.conv.6 tensor(0.0291)
features.4.conv.0 tensor(0.0389)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.2007)
features.5.conv.0 tensor(0.0176)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.2145)
features.6.conv.0 tensor(0.0265)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0481)
features.7.conv.0 tensor(0.0702)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.2747)
features.8.conv.0 tensor(0.0778)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.2965)
features.9.conv.0 tensor(0.1298)
features.9.conv.3 tensor(0.1461)
features.9.conv.6 tensor(0.3901)
features.10.conv.0 tensor(0.0431)
features.10.conv.3 tensor(0.0911)
features.10.conv.6 tensor(0.2084)
features.11.conv.0 tensor(0.4625)
features.11.conv.3 tensor(0.1468)
features.11.conv.6 tensor(0.6583)
features.12.conv.0 tensor(0.4432)
features.12.conv.3 tensor(0.1518)
features.12.conv.6 tensor(0.6966)
features.13.conv.0 tensor(0.1011)
features.13.conv.3 tensor(0.1674)
features.13.conv.6 tensor(0.1960)
features.14.conv.0 tensor(0.9575)
features.14.conv.3 tensor(0.1264)
features.14.conv.6 tensor(0.9715)
features.15.conv.0 tensor(0.9681)
features.15.conv.3 tensor(0.1120)
features.15.conv.6 tensor(0.9557)
features.16.conv.0 tensor(0.3286)
features.16.conv.3 tensor(0.1615)
features.16.conv.6 tensor(0.6177)
conv.0 tensor(0.7834)
tensor(1353464.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 0.392260   Top1 86.670000   Top5 99.410000   BatchTime 0.085714
INFO - ==> Top1: 86.670    Top5: 99.410    Loss: 0.392
INFO - ==> Sparsity : 0.618
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 86.670   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 85.350   Top5: 99.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [2][   20/  196]   Loss 0.519744   Top1 82.421875   Top5 97.285156   BatchTime 0.416510   LR 0.004477
INFO - Training [2][   40/  196]   Loss 0.505375   Top1 82.578125   Top5 97.714844   BatchTime 0.375786   LR 0.004426
INFO - Training [2][   60/  196]   Loss 0.504509   Top1 82.486979   Top5 97.877604   BatchTime 0.366388   LR 0.004374
INFO - Training [2][   80/  196]   Loss 0.498037   Top1 82.827148   Top5 98.061523   BatchTime 0.362338   LR 0.004320
INFO - Training [2][  100/  196]   Loss 0.495019   Top1 82.894531   Top5 98.070312   BatchTime 0.349555   LR 0.004264
INFO - Training [2][  120/  196]   Loss 0.490476   Top1 82.994792   Top5 98.170573   BatchTime 0.342688   LR 0.004206
INFO - Training [2][  140/  196]   Loss 0.487125   Top1 83.155692   Top5 98.250558   BatchTime 0.342976   LR 0.004146
INFO - Training [2][  160/  196]   Loss 0.487945   Top1 83.105469   Top5 98.254395   BatchTime 0.345285   LR 0.004085
INFO - Training [2][  180/  196]   Loss 0.486131   Top1 83.111979   Top5 98.213976   BatchTime 0.345319   LR 0.004022
INFO - ==> Top1: 83.202    Top5: 98.230    Loss: 0.484
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5278)
features.0.conv.3 tensor(0.3594)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0255)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0573)
features.3.conv.0 tensor(0.0260)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0258)
features.4.conv.0 tensor(0.0366)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.2199)
features.5.conv.0 tensor(0.0210)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.2327)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0474)
features.7.conv.0 tensor(0.0691)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.2984)
features.8.conv.0 tensor(0.0840)
features.8.conv.3 tensor(0.1337)
features.8.conv.6 tensor(0.3197)
features.9.conv.0 tensor(0.1268)
features.9.conv.3 tensor(0.1447)
features.9.conv.6 tensor(0.4071)
features.10.conv.0 tensor(0.0403)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.2304)
features.11.conv.0 tensor(0.4818)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.6714)
features.12.conv.0 tensor(0.4586)
features.12.conv.3 tensor(0.1520)
features.12.conv.6 tensor(0.7057)
features.13.conv.0 tensor(0.1219)
features.13.conv.3 tensor(0.1620)
features.13.conv.6 tensor(0.1971)
features.14.conv.0 tensor(0.9550)
features.14.conv.3 tensor(0.1251)
features.14.conv.6 tensor(0.9731)
features.15.conv.0 tensor(0.9661)
features.15.conv.3 tensor(0.1147)
features.15.conv.6 tensor(0.9401)
features.16.conv.0 tensor(0.3448)
features.16.conv.3 tensor(0.1623)
features.16.conv.6 tensor(0.6265)
conv.0 tensor(0.8004)
tensor(1369754.) 2188896.0
INFO - Validation [2][   20/   40]   Loss 0.367656   Top1 87.382812   Top5 99.492188   BatchTime 0.126248
INFO - Validation [2][   40/   40]   Loss 0.363592   Top1 87.330000   Top5 99.570000   BatchTime 0.090092
INFO - ==> Top1: 87.330    Top5: 99.570    Loss: 0.364
INFO - ==> Sparsity : 0.626
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 87.330   Top5: 99.570]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 86.670   Top5: 99.410]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 85.350   Top5: 99.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [3][   20/  196]   Loss 0.511206   Top1 82.558594   Top5 97.832031   BatchTime 0.405954   LR 0.003907
INFO - Training [3][   40/  196]   Loss 0.500295   Top1 82.675781   Top5 97.939453   BatchTime 0.376673   LR 0.003840
INFO - Training [3][   60/  196]   Loss 0.484453   Top1 83.170573   Top5 98.111979   BatchTime 0.364285   LR 0.003771
INFO - Training [3][   80/  196]   Loss 0.479964   Top1 83.339844   Top5 98.203125   BatchTime 0.359936   LR 0.003701
INFO - Training [3][  100/  196]   Loss 0.471665   Top1 83.648438   Top5 98.261719   BatchTime 0.346950   LR 0.003630
INFO - Training [3][  120/  196]   Loss 0.462653   Top1 83.984375   Top5 98.375651   BatchTime 0.345670   LR 0.003558
INFO - Training [3][  140/  196]   Loss 0.456803   Top1 84.165737   Top5 98.440290   BatchTime 0.348862   LR 0.003484
INFO - Training [3][  160/  196]   Loss 0.456768   Top1 84.191895   Top5 98.444824   BatchTime 0.350054   LR 0.003410
INFO - Training [3][  180/  196]   Loss 0.456806   Top1 84.140625   Top5 98.374566   BatchTime 0.348857   LR 0.003335
********************pre-trained*****************
INFO - ==> Top1: 84.238    Top5: 98.402    Loss: 0.454
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.386206   Top1 87.539062   Top5 99.375000   BatchTime 0.121462
features.0.conv.0 tensor(0.5208)
features.0.conv.3 tensor(0.3633)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0365)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0590)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0353)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.2301)
features.5.conv.0 tensor(0.0254)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.2467)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0447)
features.7.conv.0 tensor(0.0739)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.3142)
features.8.conv.0 tensor(0.0853)
features.8.conv.3 tensor(0.1308)
features.8.conv.6 tensor(0.3344)
features.9.conv.0 tensor(0.1202)
features.9.conv.3 tensor(0.1421)
features.9.conv.6 tensor(0.4183)
features.10.conv.0 tensor(0.0470)
features.10.conv.3 tensor(0.0969)
features.10.conv.6 tensor(0.2456)
features.11.conv.0 tensor(0.4942)
features.11.conv.3 tensor(0.1582)
features.11.conv.6 tensor(0.6748)
features.12.conv.0 tensor(0.4746)
features.12.conv.3 tensor(0.1541)
features.12.conv.6 tensor(0.7109)
features.13.conv.0 tensor(0.1102)
features.13.conv.3 tensor(0.1618)
features.13.conv.6 tensor(0.1976)
features.14.conv.0 tensor(0.9543)
features.14.conv.3 tensor(0.1264)
features.14.conv.6 tensor(0.9753)
features.15.conv.0 tensor(0.9657)
features.15.conv.3 tensor(0.1137)
features.15.conv.6 tensor(0.9413)
features.16.conv.0 tensor(0.3562)
features.16.conv.3 tensor(0.1640)
features.16.conv.6 tensor(0.6348)
conv.0 tensor(0.8117)
tensor(1382400.) 2188896.0
INFO - Validation [3][   40/   40]   Loss 0.383602   Top1 87.530000   Top5 99.480000   BatchTime 0.085682
INFO - ==> Top1: 87.530    Top5: 99.480    Loss: 0.384
INFO - ==> Sparsity : 0.632
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 87.530   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 87.330   Top5: 99.570]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 86.670   Top5: 99.410]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [4][   20/  196]   Loss 0.466152   Top1 84.003906   Top5 98.066406   BatchTime 0.414442   LR 0.003200
INFO - Training [4][   40/  196]   Loss 0.458221   Top1 84.306641   Top5 98.105469   BatchTime 0.381000   LR 0.003122
INFO - Training [4][   60/  196]   Loss 0.457330   Top1 84.055990   Top5 98.274740   BatchTime 0.366837   LR 0.003044
INFO - Training [4][   80/  196]   Loss 0.450956   Top1 84.272461   Top5 98.437500   BatchTime 0.351908   LR 0.002965
INFO - Training [4][  100/  196]   Loss 0.445843   Top1 84.476562   Top5 98.425781   BatchTime 0.335812   LR 0.002886
INFO - Training [4][  120/  196]   Loss 0.434515   Top1 84.801432   Top5 98.525391   BatchTime 0.324704   LR 0.002806
INFO - Training [4][  140/  196]   Loss 0.435114   Top1 84.863281   Top5 98.565848   BatchTime 0.331436   LR 0.002726
INFO - Training [4][  160/  196]   Loss 0.435729   Top1 84.853516   Top5 98.540039   BatchTime 0.336549   LR 0.002646
INFO - Training [4][  180/  196]   Loss 0.437003   Top1 84.774306   Top5 98.454861   BatchTime 0.337329   LR 0.002566
********************pre-trained*****************
INFO - ==> Top1: 84.936    Top5: 98.466    Loss: 0.433
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 0.350066   Top1 88.632812   Top5 99.433594   BatchTime 0.122996
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0330)
features.2.conv.0 tensor(0.0260)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0558)
features.3.conv.0 tensor(0.0243)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0382)
features.4.conv.3 tensor(0.0984)
features.4.conv.6 tensor(0.2389)
features.5.conv.0 tensor(0.0272)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.2550)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0464)
features.7.conv.0 tensor(0.0748)
features.7.conv.3 tensor(0.1149)
features.7.conv.6 tensor(0.3228)
features.8.conv.0 tensor(0.0780)
features.8.conv.3 tensor(0.1322)
features.8.conv.6 tensor(0.3440)
features.9.conv.0 tensor(0.0980)
features.9.conv.3 tensor(0.1429)
features.9.conv.6 tensor(0.4267)
features.10.conv.0 tensor(0.0452)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.2563)
features.11.conv.0 tensor(0.5019)
features.11.conv.3 tensor(0.1574)
features.11.conv.6 tensor(0.6772)
features.12.conv.0 tensor(0.4798)
features.12.conv.3 tensor(0.1564)
features.12.conv.6 tensor(0.7153)
features.13.conv.0 tensor(0.1156)
features.13.conv.3 tensor(0.1593)
features.13.conv.6 tensor(0.1982)
features.14.conv.0 tensor(0.9540)
features.14.conv.3 tensor(0.1297)
features.14.conv.6 tensor(0.9747)
features.15.conv.0 tensor(0.9652)
features.15.conv.3 tensor(0.1138)
features.15.conv.6 tensor(0.9373)
features.16.conv.0 tensor(0.3653)
features.16.conv.3 tensor(0.1678)
features.16.conv.6 tensor(0.6389)
conv.0 tensor(0.8181)
tensor(1388797.) 2188896.0
INFO - Validation [4][   40/   40]   Loss 0.343547   Top1 88.490000   Top5 99.660000   BatchTime 0.087603
INFO - ==> Top1: 88.490    Top5: 99.660    Loss: 0.344
INFO - ==> Sparsity : 0.634
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.490   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 87.530   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 87.330   Top5: 99.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [5][   20/  196]   Loss 0.400913   Top1 85.449219   Top5 98.144531   BatchTime 0.417362   LR 0.002424
INFO - Training [5][   40/  196]   Loss 0.413873   Top1 85.419922   Top5 98.427734   BatchTime 0.394274   LR 0.002343
INFO - Training [5][   60/  196]   Loss 0.414226   Top1 85.462240   Top5 98.522135   BatchTime 0.380064   LR 0.002263
INFO - Training [5][   80/  196]   Loss 0.433507   Top1 84.619141   Top5 98.051758   BatchTime 0.375006   LR 0.002183
INFO - Training [5][  100/  196]   Loss 0.420575   Top1 85.003906   Top5 98.218750   BatchTime 0.357510   LR 0.002104
INFO - Training [5][  120/  196]   Loss 0.412052   Top1 85.374349   Top5 98.330078   BatchTime 0.354994   LR 0.002024
INFO - Training [5][  140/  196]   Loss 0.407137   Top1 85.530134   Top5 98.470982   BatchTime 0.356391   LR 0.001946
INFO - Training [5][  160/  196]   Loss 0.410133   Top1 85.415039   Top5 98.461914   BatchTime 0.356184   LR 0.001868
INFO - Training [5][  180/  196]   Loss 0.408673   Top1 85.392795   Top5 98.439670   BatchTime 0.353048   LR 0.001790
********************pre-trained*****************
INFO - ==> Top1: 85.468    Top5: 98.452    Loss: 0.408
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [5][   20/   40]   Loss 0.325917   Top1 88.574219   Top5 99.687500   BatchTime 0.125147
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0291)
features.2.conv.0 tensor(0.0275)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0535)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0297)
features.4.conv.0 tensor(0.0430)
features.4.conv.3 tensor(0.1013)
features.4.conv.6 tensor(0.2430)
features.5.conv.0 tensor(0.0244)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.2601)
features.6.conv.0 tensor(0.0241)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0489)
features.7.conv.0 tensor(0.0763)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.3287)
features.8.conv.0 tensor(0.0830)
features.8.conv.3 tensor(0.1317)
features.8.conv.6 tensor(0.3499)
features.9.conv.0 tensor(0.1107)
features.9.conv.3 tensor(0.1453)
features.9.conv.6 tensor(0.4297)
features.10.conv.0 tensor(0.0473)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.2620)
features.11.conv.0 tensor(0.5058)
features.11.conv.3 tensor(0.1620)
features.11.conv.6 tensor(0.6795)
features.12.conv.0 tensor(0.4856)
features.12.conv.3 tensor(0.1562)
features.12.conv.6 tensor(0.7189)
features.13.conv.0 tensor(0.1157)
features.13.conv.3 tensor(0.1617)
features.13.conv.6 tensor(0.1976)
features.14.conv.0 tensor(0.9538)
features.14.conv.3 tensor(0.1274)
features.14.conv.6 tensor(0.9749)
features.15.conv.0 tensor(0.9648)
features.15.conv.3 tensor(0.1166)
features.15.conv.6 tensor(0.9504)
features.16.conv.0 tensor(0.3701)
features.16.conv.3 tensor(0.1664)
features.16.conv.6 tensor(0.6418)
conv.0 tensor(0.8205)
tensor(1395357.) 2188896.0
INFO - Validation [5][   40/   40]   Loss 0.319515   Top1 88.870000   Top5 99.690000   BatchTime 0.086774
INFO - ==> Top1: 88.870    Top5: 99.690    Loss: 0.320
INFO - ==> Sparsity : 0.637
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.870   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.490   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 87.530   Top5: 99.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [6][   20/  196]   Loss 0.406124   Top1 85.683594   Top5 98.027344   BatchTime 0.428774   LR 0.001655
INFO - Training [6][   40/  196]   Loss 0.396876   Top1 85.791016   Top5 98.222656   BatchTime 0.390625   LR 0.001580
INFO - Training [6][   60/  196]   Loss 0.387346   Top1 86.256510   Top5 98.417969   BatchTime 0.374204   LR 0.001506
INFO - Training [6][   80/  196]   Loss 0.379821   Top1 86.630859   Top5 98.569336   BatchTime 0.363988   LR 0.001432
INFO - Training [6][  100/  196]   Loss 0.377222   Top1 86.695312   Top5 98.621094   BatchTime 0.346323   LR 0.001360
INFO - Training [6][  120/  196]   Loss 0.375337   Top1 86.832682   Top5 98.688151   BatchTime 0.340717   LR 0.001289
INFO - Training [6][  140/  196]   Loss 0.372952   Top1 86.930804   Top5 98.766741   BatchTime 0.336574   LR 0.001220
INFO - Training [6][  160/  196]   Loss 0.373943   Top1 86.889648   Top5 98.718262   BatchTime 0.340441   LR 0.001151
INFO - Training [6][  180/  196]   Loss 0.375683   Top1 86.827257   Top5 98.708767   BatchTime 0.340723   LR 0.001084
********************pre-trained*****************
INFO - ==> Top1: 86.850    Top5: 98.714    Loss: 0.374
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.296263   Top1 89.824219   Top5 99.609375   BatchTime 0.124907
INFO - Validation [6][   40/   40]   Loss 0.283257   Top1 90.060000   Top5 99.700000   BatchTime 0.087924
INFO - ==> Top1: 90.060    Top5: 99.700    Loss: 0.283
INFO - ==> Sparsity : 0.639
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.870   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.490   Top5: 99.660]
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.3730)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0343)
features.2.conv.0 tensor(0.0263)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0556)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0407)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.2446)
features.5.conv.0 tensor(0.0277)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.2612)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0463)
features.7.conv.0 tensor(0.0772)
features.7.conv.3 tensor(0.1128)
features.7.conv.6 tensor(0.3313)
features.8.conv.0 tensor(0.0809)
features.8.conv.3 tensor(0.1345)
features.8.conv.6 tensor(0.3527)
features.9.conv.0 tensor(0.1125)
features.9.conv.3 tensor(0.1467)
features.9.conv.6 tensor(0.4315)
features.10.conv.0 tensor(0.0452)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.2643)
features.11.conv.0 tensor(0.5090)
features.11.conv.3 tensor(0.1584)
features.11.conv.6 tensor(0.6799)
features.12.conv.0 tensor(0.4898)
features.12.conv.3 tensor(0.1559)
features.12.conv.6 tensor(0.7194)
features.13.conv.0 tensor(0.1080)
features.13.conv.3 tensor(0.1603)
features.13.conv.6 tensor(0.1974)
features.14.conv.0 tensor(0.9532)
features.14.conv.3 tensor(0.1291)
features.14.conv.6 tensor(0.9746)
features.15.conv.0 tensor(0.9646)
features.15.conv.3 tensor(0.1175)
features.15.conv.6 tensor(0.9612)
features.16.conv.0 tensor(0.3715)
features.16.conv.3 tensor(0.1642)
features.16.conv.6 tensor(0.6427)
conv.0 tensor(0.8208)
tensor(1397753.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [7][   20/  196]   Loss 0.380780   Top1 86.425781   Top5 98.281250   BatchTime 0.420734   LR 0.000969
INFO - Training [7][   40/  196]   Loss 0.373204   Top1 86.757812   Top5 98.496094   BatchTime 0.381301   LR 0.000907
INFO - Training [7][   60/  196]   Loss 0.370274   Top1 86.816406   Top5 98.502604   BatchTime 0.369975   LR 0.000845
INFO - Training [7][   80/  196]   Loss 0.370550   Top1 87.016602   Top5 98.652344   BatchTime 0.361125   LR 0.000786
INFO - Training [7][  100/  196]   Loss 0.362526   Top1 87.332031   Top5 98.699219   BatchTime 0.358511   LR 0.000728
INFO - Training [7][  120/  196]   Loss 0.356959   Top1 87.587891   Top5 98.776042   BatchTime 0.346810   LR 0.000673
INFO - Training [7][  140/  196]   Loss 0.352791   Top1 87.787388   Top5 98.808594   BatchTime 0.346456   LR 0.000619
INFO - Training [7][  160/  196]   Loss 0.353047   Top1 87.763672   Top5 98.793945   BatchTime 0.347646   LR 0.000567
INFO - Training [7][  180/  196]   Loss 0.353952   Top1 87.736545   Top5 98.756510   BatchTime 0.347394   LR 0.000517
********************pre-trained*****************
INFO - ==> Top1: 87.768    Top5: 98.750    Loss: 0.352
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 0.330204   Top1 89.140625   Top5 99.531250   BatchTime 0.126471
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0310)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0538)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0405)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.2451)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0828)
features.5.conv.6 tensor(0.2620)
features.6.conv.0 tensor(0.0207)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0474)
features.7.conv.0 tensor(0.0773)
features.7.conv.3 tensor(0.1128)
features.7.conv.6 tensor(0.3322)
features.8.conv.0 tensor(0.0830)
features.8.conv.3 tensor(0.1340)
features.8.conv.6 tensor(0.3531)
features.9.conv.0 tensor(0.1129)
features.9.conv.3 tensor(0.1450)
features.9.conv.6 tensor(0.4318)
features.10.conv.0 tensor(0.0439)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.2652)
features.11.conv.0 tensor(0.5100)
features.11.conv.3 tensor(0.1562)
features.11.conv.6 tensor(0.6801)
features.12.conv.0 tensor(0.4911)
features.12.conv.3 tensor(0.1561)
features.12.conv.6 tensor(0.7190)
features.13.conv.0 tensor(0.1085)
features.13.conv.3 tensor(0.1595)
features.13.conv.6 tensor(0.1976)
features.14.conv.0 tensor(0.9532)
features.14.conv.3 tensor(0.1273)
features.14.conv.6 tensor(0.9750)
features.15.conv.0 tensor(0.9648)
features.15.conv.3 tensor(0.1182)
features.15.conv.6 tensor(0.9581)
features.16.conv.0 tensor(0.3731)
features.16.conv.3 tensor(0.1634)
features.16.conv.6 tensor(0.6428)
conv.0 tensor(0.8208)
tensor(1397883.) 2188896.0
INFO - Validation [7][   40/   40]   Loss 0.318779   Top1 89.330000   Top5 99.640000   BatchTime 0.088841
INFO - ==> Top1: 89.330    Top5: 99.640    Loss: 0.319
INFO - ==> Sparsity : 0.639
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.330   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 88.870   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [8][   20/  196]   Loss 0.362890   Top1 87.480469   Top5 98.378906   BatchTime 0.421509   LR 0.000434
INFO - Training [8][   40/  196]   Loss 0.359743   Top1 87.314453   Top5 98.496094   BatchTime 0.394575   LR 0.000389
INFO - Training [8][   60/  196]   Loss 0.353190   Top1 87.552083   Top5 98.593750   BatchTime 0.383023   LR 0.000347
INFO - Training [8][   80/  196]   Loss 0.344926   Top1 87.929688   Top5 98.750000   BatchTime 0.372258   LR 0.000308
INFO - Training [8][  100/  196]   Loss 0.339844   Top1 88.132812   Top5 98.792969   BatchTime 0.369361   LR 0.000270
INFO - Training [8][  120/  196]   Loss 0.333596   Top1 88.375651   Top5 98.857422   BatchTime 0.360991   LR 0.000235
INFO - Training [8][  140/  196]   Loss 0.332051   Top1 88.415179   Top5 98.925781   BatchTime 0.347183   LR 0.000202
INFO - Training [8][  160/  196]   Loss 0.335169   Top1 88.269043   Top5 98.940430   BatchTime 0.334762   LR 0.000172
INFO - Training [8][  180/  196]   Loss 0.333332   Top1 88.300781   Top5 98.912760   BatchTime 0.336704   LR 0.000143
********************pre-trained*****************
INFO - ==> Top1: 88.320    Top5: 98.906    Loss: 0.332
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.282665   Top1 90.722656   Top5 99.687500   BatchTime 0.144198
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0312)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0541)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0394)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.2451)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0822)
features.5.conv.6 tensor(0.2622)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0473)
features.7.conv.0 tensor(0.0772)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.3322)
features.8.conv.0 tensor(0.0825)
features.8.conv.3 tensor(0.1337)
features.8.conv.6 tensor(0.3532)
features.9.conv.0 tensor(0.1108)
features.9.conv.3 tensor(0.1470)
features.9.conv.6 tensor(0.4318)
features.10.conv.0 tensor(0.0430)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.2652)
features.11.conv.0 tensor(0.5102)
features.11.conv.3 tensor(0.1555)
features.11.conv.6 tensor(0.6800)
features.12.conv.0 tensor(0.4909)
features.12.conv.3 tensor(0.1586)
features.12.conv.6 tensor(0.7183)
features.13.conv.0 tensor(0.1104)
features.13.conv.3 tensor(0.1618)
features.13.conv.6 tensor(0.1977)
features.14.conv.0 tensor(0.9533)
features.14.conv.3 tensor(0.1281)
features.14.conv.6 tensor(0.9748)
features.15.conv.0 tensor(0.9645)
features.15.conv.3 tensor(0.1177)
features.15.conv.6 tensor(0.9564)
features.16.conv.0 tensor(0.3733)
features.16.conv.3 tensor(0.1624)
features.16.conv.6 tensor(0.6427)
conv.0 tensor(0.8205)
tensor(1397433.) 2188896.0
INFO - Validation [8][   40/   40]   Loss 0.266247   Top1 91.050000   Top5 99.760000   BatchTime 0.098876
INFO - ==> Top1: 91.050    Top5: 99.760    Loss: 0.266
INFO - ==> Sparsity : 0.638
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.330   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [9][   20/  196]   Loss 0.343568   Top1 87.597656   Top5 98.378906   BatchTime 0.410466   LR 0.000100
INFO - Training [9][   40/  196]   Loss 0.339858   Top1 87.714844   Top5 98.613281   BatchTime 0.375869   LR 0.000079
INFO - Training [9][   60/  196]   Loss 0.338066   Top1 87.662760   Top5 98.717448   BatchTime 0.362081   LR 0.000060
INFO - Training [9][   80/  196]   Loss 0.334331   Top1 87.944336   Top5 98.837891   BatchTime 0.359182   LR 0.000044
INFO - Training [9][  100/  196]   Loss 0.328031   Top1 88.187500   Top5 98.878906   BatchTime 0.355627   LR 0.000030
INFO - Training [9][  120/  196]   Loss 0.319939   Top1 88.574219   Top5 98.958333   BatchTime 0.354688   LR 0.000019
INFO - Training [9][  140/  196]   Loss 0.316968   Top1 88.669085   Top5 99.029018   BatchTime 0.355307   LR 0.000010
INFO - Training [9][  160/  196]   Loss 0.318124   Top1 88.713379   Top5 99.030762   BatchTime 0.344879   LR 0.000004
INFO - Training [9][  180/  196]   Loss 0.319212   Top1 88.658854   Top5 98.960503   BatchTime 0.345704   LR 0.000001
********************pre-trained*****************
INFO - ==> Top1: 88.712    Top5: 98.968    Loss: 0.319
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [9][   20/   40]   Loss 0.291151   Top1 90.468750   Top5 99.707031   BatchTime 0.130884
INFO - Validation [9][   40/   40]   Loss 0.276423   Top1 90.770000   Top5 99.730000   BatchTime 0.092222
INFO - ==> Top1: 90.770    Top5: 99.730    Loss: 0.276
INFO - ==> Sparsity : 0.638
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0360)
features.2.conv.0 tensor(0.0321)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0547)
features.3.conv.0 tensor(0.0211)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0392)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.2451)
features.5.conv.0 tensor(0.0283)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.2620)
features.6.conv.0 tensor(0.0194)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0479)
features.7.conv.0 tensor(0.0772)
features.7.conv.3 tensor(0.1128)
features.7.conv.6 tensor(0.3322)
features.8.conv.0 tensor(0.0826)
features.8.conv.3 tensor(0.1337)
features.8.conv.6 tensor(0.3531)
features.9.conv.0 tensor(0.1102)
features.9.conv.3 tensor(0.1467)
features.9.conv.6 tensor(0.4318)
features.10.conv.0 tensor(0.0431)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.2652)
features.11.conv.0 tensor(0.5099)
features.11.conv.3 tensor(0.1551)
features.11.conv.6 tensor(0.6797)
features.12.conv.0 tensor(0.4908)
features.12.conv.3 tensor(0.1572)
features.12.conv.6 tensor(0.7184)
features.13.conv.0 tensor(0.1108)
features.13.conv.3 tensor(0.1617)
features.13.conv.6 tensor(0.1977)
features.14.conv.0 tensor(0.9532)
features.14.conv.3 tensor(0.1286)
features.14.conv.6 tensor(0.9750)
features.15.conv.0 tensor(0.9645)
features.15.conv.3 tensor(0.1176)
features.15.conv.6 tensor(0.9562)
features.16.conv.0 tensor(0.3733)
features.16.conv.3 tensor(0.1632)
features.16.conv.6 tensor(0.6426)
conv.0 tensor(0.8204)
tensor(1397306.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [10][   20/  196]   Loss 0.364713   Top1 87.031250   Top5 98.378906   BatchTime 0.415435   LR 0.002500
INFO - Training [10][   40/  196]   Loss 0.377668   Top1 86.757812   Top5 98.564453   BatchTime 0.377840   LR 0.002499
INFO - Training [10][   60/  196]   Loss 0.377670   Top1 86.803385   Top5 98.639323   BatchTime 0.366713   LR 0.002499
INFO - Training [10][   80/  196]   Loss 0.380628   Top1 86.855469   Top5 98.715820   BatchTime 0.363349   LR 0.002497
INFO - Training [10][  100/  196]   Loss 0.377947   Top1 86.957031   Top5 98.714844   BatchTime 0.364017   LR 0.002496
INFO - Training [10][  120/  196]   Loss 0.373340   Top1 87.024740   Top5 98.795573   BatchTime 0.359846   LR 0.002494
INFO - Training [10][  140/  196]   Loss 0.374591   Top1 87.047991   Top5 98.828125   BatchTime 0.356964   LR 0.002492
INFO - Training [10][  160/  196]   Loss 0.381969   Top1 86.728516   Top5 98.774414   BatchTime 0.346323   LR 0.002490
INFO - Training [10][  180/  196]   Loss 0.386177   Top1 86.621094   Top5 98.730469   BatchTime 0.335605   LR 0.002487
INFO - ==> Top1: 86.648    Top5: 98.720    Loss: 0.386
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.307214   Top1 89.648438   Top5 99.531250   BatchTime 0.147205
INFO - Validation [10][   40/   40]   Loss 0.306522   Top1 89.800000   Top5 99.540000   BatchTime 0.119535
INFO - ==> Top1: 89.800    Top5: 99.540    Loss: 0.307
INFO - ==> Sparsity : 0.642
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0360)
features.2.conv.0 tensor(0.0347)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0486)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0363)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0433)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.2524)
features.5.conv.0 tensor(0.0256)
features.5.conv.3 tensor(0.0787)
features.5.conv.6 tensor(0.2694)
features.6.conv.0 tensor(0.0205)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0470)
features.7.conv.0 tensor(0.0741)
features.7.conv.3 tensor(0.1071)
features.7.conv.6 tensor(0.3409)
features.8.conv.0 tensor(0.0780)
features.8.conv.3 tensor(0.1299)
features.8.conv.6 tensor(0.3620)
features.9.conv.0 tensor(0.1108)
features.9.conv.3 tensor(0.1473)
features.9.conv.6 tensor(0.4379)
features.10.conv.0 tensor(0.0457)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.2732)
features.11.conv.0 tensor(0.5168)
features.11.conv.3 tensor(0.1568)
features.11.conv.6 tensor(0.6865)
features.12.conv.0 tensor(0.4943)
features.12.conv.3 tensor(0.1580)
features.12.conv.6 tensor(0.7229)
features.13.conv.0 tensor(0.1181)
features.13.conv.3 tensor(0.1611)
features.13.conv.6 tensor(0.1969)
features.14.conv.0 tensor(0.9521)
features.14.conv.3 tensor(0.1307)
features.14.conv.6 tensor(0.9761)
features.15.conv.0 tensor(0.9647)
features.15.conv.3 tensor(0.1149)
features.15.conv.6 tensor(0.9625)
features.16.conv.0 tensor(0.3802)
features.16.conv.3 tensor(0.1628)
features.16.conv.6 tensor(0.6485)
conv.0 tensor(0.8274)
tensor(1406330.) 2188896.0
INFO - Training [11][   20/  196]   Loss 0.402264   Top1 86.093750   Top5 97.929688   BatchTime 0.471724   LR 0.002481
INFO - Training [11][   40/  196]   Loss 0.411638   Top1 85.878906   Top5 98.212891   BatchTime 0.406203   LR 0.002478
INFO - Training [11][   60/  196]   Loss 0.407965   Top1 86.015625   Top5 98.391927   BatchTime 0.386448   LR 0.002474
INFO - Training [11][   80/  196]   Loss 0.403191   Top1 86.054688   Top5 98.535156   BatchTime 0.376670   LR 0.002470
INFO - Training [11][  100/  196]   Loss 0.397670   Top1 86.160156   Top5 98.582031   BatchTime 0.369655   LR 0.002465
INFO - Training [11][  120/  196]   Loss 0.391371   Top1 86.344401   Top5 98.688151   BatchTime 0.365903   LR 0.002460
INFO - Training [11][  140/  196]   Loss 0.391976   Top1 86.325335   Top5 98.722098   BatchTime 0.361973   LR 0.002455
INFO - Training [11][  160/  196]   Loss 0.394709   Top1 86.230469   Top5 98.715820   BatchTime 0.359021   LR 0.002450
INFO - Training [11][  180/  196]   Loss 0.393697   Top1 86.254340   Top5 98.667535   BatchTime 0.350792   LR 0.002444
********************pre-trained*****************
INFO - ==> Top1: 86.322    Top5: 98.660    Loss: 0.393
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.372839   Top1 87.402344   Top5 99.472656   BatchTime 0.130762
INFO - Validation [11][   40/   40]   Loss 0.372324   Top1 87.380000   Top5 99.550000   BatchTime 0.092559
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0339)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0532)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0310)
features.4.conv.0 tensor(0.0397)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.2583)
features.5.conv.0 tensor(0.0220)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.2757)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0492)
features.7.conv.0 tensor(0.0777)
features.7.conv.3 tensor(0.1047)
features.7.conv.6 tensor(0.3492)
features.8.conv.0 tensor(0.0836)
features.8.conv.3 tensor(0.1270)
features.8.conv.6 tensor(0.3686)
features.9.conv.0 tensor(0.1070)
features.9.conv.3 tensor(0.1487)
features.9.conv.6 tensor(0.4438)
features.10.conv.0 tensor(0.0463)
features.10.conv.3 tensor(0.0932)
features.10.conv.6 tensor(0.2818)
features.11.conv.0 tensor(0.5237)
features.11.conv.3 tensor(0.1588)
features.11.conv.6 tensor(0.6918)
features.12.conv.0 tensor(0.4997)
features.12.conv.3 tensor(0.1580)
features.12.conv.6 tensor(0.7271)
features.13.conv.0 tensor(0.1059)
features.13.conv.3 tensor(0.1605)
features.13.conv.6 tensor(0.1982)
features.14.conv.0 tensor(0.9524)
features.14.conv.3 tensor(0.1333)
features.14.conv.6 tensor(0.9761)
features.15.conv.0 tensor(0.9647)
features.15.conv.3 tensor(0.1175)
features.15.conv.6 tensor(0.9472)
features.16.conv.0 tensor(0.3881)
features.16.conv.3 tensor(0.1661)
features.16.conv.6 tensor(0.6505)
conv.0 tensor(0.8338)
tensor(1410267.) 2188896.0
INFO - ==> Top1: 87.380    Top5: 99.550    Loss: 0.372
INFO - ==> Sparsity : 0.644
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [12][   20/  196]   Loss 0.419289   Top1 85.351562   Top5 98.164062   BatchTime 0.461662   LR 0.002433
INFO - Training [12][   40/  196]   Loss 0.421337   Top1 85.634766   Top5 98.271484   BatchTime 0.407332   LR 0.002426
INFO - Training [12][   60/  196]   Loss 0.411711   Top1 85.898438   Top5 98.385417   BatchTime 0.385337   LR 0.002419
INFO - Training [12][   80/  196]   Loss 0.412001   Top1 85.957031   Top5 98.500977   BatchTime 0.372951   LR 0.002412
INFO - Training [12][  100/  196]   Loss 0.399796   Top1 86.308594   Top5 98.601562   BatchTime 0.364550   LR 0.002404
INFO - Training [12][  120/  196]   Loss 0.394069   Top1 86.546224   Top5 98.658854   BatchTime 0.361794   LR 0.002396
INFO - Training [12][  140/  196]   Loss 0.393706   Top1 86.531808   Top5 98.713728   BatchTime 0.365408   LR 0.002388
INFO - Training [12][  160/  196]   Loss 0.398989   Top1 86.376953   Top5 98.681641   BatchTime 0.364927   LR 0.002380
INFO - Training [12][  180/  196]   Loss 0.396263   Top1 86.464844   Top5 98.650174   BatchTime 0.359668   LR 0.002371
INFO - ==> Top1: 86.454    Top5: 98.656    Loss: 0.396
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [12][   20/   40]   Loss 0.378449   Top1 87.617188   Top5 99.394531   BatchTime 0.130073
INFO - Validation [12][   40/   40]   Loss 0.367136   Top1 87.630000   Top5 99.520000   BatchTime 0.090340
INFO - ==> Top1: 87.630    Top5: 99.520    Loss: 0.367
INFO - ==> Sparsity : 0.646
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0360)
features.2.conv.0 tensor(0.0370)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0498)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0271)
features.4.conv.0 tensor(0.0392)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.2629)
features.5.conv.0 tensor(0.0260)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.2811)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0482)
features.7.conv.0 tensor(0.0861)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.3563)
features.8.conv.0 tensor(0.0919)
features.8.conv.3 tensor(0.1233)
features.8.conv.6 tensor(0.3741)
features.9.conv.0 tensor(0.1243)
features.9.conv.3 tensor(0.1484)
features.9.conv.6 tensor(0.4494)
features.10.conv.0 tensor(0.0468)
features.10.conv.3 tensor(0.0911)
features.10.conv.6 tensor(0.2880)
features.11.conv.0 tensor(0.5297)
features.11.conv.3 tensor(0.1580)
features.11.conv.6 tensor(0.6950)
features.12.conv.0 tensor(0.5046)
features.12.conv.3 tensor(0.1613)
features.12.conv.6 tensor(0.7294)
features.13.conv.0 tensor(0.1183)
features.13.conv.3 tensor(0.1572)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9529)
features.14.conv.3 tensor(0.1347)
features.14.conv.6 tensor(0.9764)
features.15.conv.0 tensor(0.9650)
features.15.conv.3 tensor(0.1187)
features.15.conv.6 tensor(0.9148)
features.16.conv.0 tensor(0.3961)
features.16.conv.3 tensor(0.1681)
features.16.conv.6 tensor(0.6549)
conv.0 tensor(0.8385)
tensor(1413176.) 2188896.0
INFO - Training [13][   20/  196]   Loss 0.382678   Top1 86.328125   Top5 98.183594   BatchTime 0.425733   LR 0.002355
INFO - Training [13][   40/  196]   Loss 0.383393   Top1 86.542969   Top5 98.437500   BatchTime 0.405234   LR 0.002345
INFO - Training [13][   60/  196]   Loss 0.383142   Top1 86.582031   Top5 98.554688   BatchTime 0.392445   LR 0.002336
INFO - Training [13][   80/  196]   Loss 0.386508   Top1 86.376953   Top5 98.681641   BatchTime 0.379055   LR 0.002325
INFO - Training [13][  100/  196]   Loss 0.385744   Top1 86.347656   Top5 98.730469   BatchTime 0.371449   LR 0.002315
INFO - Training [13][  120/  196]   Loss 0.381988   Top1 86.608073   Top5 98.763021   BatchTime 0.366372   LR 0.002304
INFO - Training [13][  140/  196]   Loss 0.385621   Top1 86.476004   Top5 98.758371   BatchTime 0.362052   LR 0.002293
INFO - Training [13][  160/  196]   Loss 0.387784   Top1 86.418457   Top5 98.740234   BatchTime 0.359300   LR 0.002282
INFO - Training [13][  180/  196]   Loss 0.386518   Top1 86.434462   Top5 98.687066   BatchTime 0.358214   LR 0.002271
********************pre-trained*****************
INFO - ==> Top1: 86.352    Top5: 98.670    Loss: 0.389
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.447074   Top1 84.824219   Top5 99.160156   BatchTime 0.160112
INFO - Validation [13][   40/   40]   Loss 0.444537   Top1 85.080000   Top5 99.260000   BatchTime 0.108622
INFO - ==> Top1: 85.080    Top5: 99.260    Loss: 0.445
INFO - ==> Sparsity : 0.650
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.3809)
features.1.conv.0 tensor(0.0111)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0356)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0438)
features.4.conv.3 tensor(0.0770)
features.4.conv.6 tensor(0.2684)
features.5.conv.0 tensor(0.0272)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.2865)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0430)
features.7.conv.0 tensor(0.0876)
features.7.conv.3 tensor(0.1097)
features.7.conv.6 tensor(0.3610)
features.8.conv.0 tensor(0.0936)
features.8.conv.3 tensor(0.1273)
features.8.conv.6 tensor(0.3797)
features.9.conv.0 tensor(0.1211)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.4528)
features.10.conv.0 tensor(0.0463)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2937)
features.11.conv.0 tensor(0.5363)
features.11.conv.3 tensor(0.1620)
features.11.conv.6 tensor(0.6997)
features.12.conv.0 tensor(0.5035)
features.12.conv.3 tensor(0.1622)
features.12.conv.6 tensor(0.7329)
features.13.conv.0 tensor(0.1094)
features.13.conv.3 tensor(0.1630)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9520)
features.14.conv.3 tensor(0.1324)
features.14.conv.6 tensor(0.9769)
features.15.conv.0 tensor(0.9651)
features.15.conv.3 tensor(0.1191)
features.15.conv.6 tensor(0.9439)
features.16.conv.0 tensor(0.4005)
features.16.conv.3 tensor(0.1678)
features.16.conv.6 tensor(0.6574)
conv.0 tensor(0.8436)
tensor(1421977.) 2188896.0
INFO - Training [14][   20/  196]   Loss 0.476452   Top1 83.164062   Top5 97.792969   BatchTime 0.421325   LR 0.002250
INFO - Training [14][   40/  196]   Loss 0.455690   Top1 84.052734   Top5 97.968750   BatchTime 0.389563   LR 0.002238
INFO - Training [14][   60/  196]   Loss 0.443040   Top1 84.557292   Top5 98.203125   BatchTime 0.376056   LR 0.002225
INFO - Training [14][   80/  196]   Loss 0.431862   Top1 84.946289   Top5 98.364258   BatchTime 0.364044   LR 0.002213
INFO - Training [14][  100/  196]   Loss 0.421401   Top1 85.289062   Top5 98.449219   BatchTime 0.359433   LR 0.002200
INFO - Training [14][  120/  196]   Loss 0.411674   Top1 85.615234   Top5 98.583984   BatchTime 0.356509   LR 0.002186
INFO - Training [14][  140/  196]   Loss 0.411066   Top1 85.630580   Top5 98.663504   BatchTime 0.356696   LR 0.002173
INFO - Training [14][  160/  196]   Loss 0.413895   Top1 85.561523   Top5 98.640137   BatchTime 0.356935   LR 0.002159
INFO - Training [14][  180/  196]   Loss 0.411625   Top1 85.635851   Top5 98.621962   BatchTime 0.356004   LR 0.002145
INFO - ==> Top1: 85.698    Top5: 98.630    Loss: 0.411
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [14][   20/   40]   Loss 0.343484   Top1 88.593750   Top5 99.472656   BatchTime 0.144813
INFO - Validation [14][   40/   40]   Loss 0.334738   Top1 88.850000   Top5 99.590000   BatchTime 0.122945
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0417)
features.2.conv.0 tensor(0.0341)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0370)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0391)
features.4.conv.3 tensor(0.0862)
features.4.conv.6 tensor(0.2743)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2913)
features.6.conv.0 tensor(0.0192)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0817)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.3669)
features.8.conv.0 tensor(0.0935)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.3849)
features.9.conv.0 tensor(0.1188)
features.9.conv.3 tensor(0.1400)
features.9.conv.6 tensor(0.4563)
features.10.conv.0 tensor(0.0464)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.2995)
features.11.conv.0 tensor(0.5419)
features.11.conv.3 tensor(0.1584)
features.11.conv.6 tensor(0.7030)
features.12.conv.0 tensor(0.5137)
features.12.conv.3 tensor(0.1601)
features.12.conv.6 tensor(0.7359)
features.13.conv.0 tensor(0.1170)
features.13.conv.3 tensor(0.1622)
features.13.conv.6 tensor(0.1982)
features.14.conv.0 tensor(0.9514)
features.14.conv.3 tensor(0.1330)
features.14.conv.6 tensor(0.9769)
features.15.conv.0 tensor(0.9652)
features.15.conv.3 tensor(0.1193)
features.15.conv.6 tensor(0.8951)
features.16.conv.0 tensor(0.4032)
features.16.conv.3 tensor(0.1667)
features.16.conv.6 tensor(0.6601)
conv.0 tensor(0.8465)
tensor(1418841.) 2188896.0
INFO - ==> Top1: 88.850    Top5: 99.590    Loss: 0.335
INFO - ==> Sparsity : 0.648
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [15][   20/  196]   Loss 0.401477   Top1 85.449219   Top5 98.359375   BatchTime 0.443638   LR 0.002120
INFO - Training [15][   40/  196]   Loss 0.396778   Top1 85.869141   Top5 98.544922   BatchTime 0.389657   LR 0.002106
INFO - Training [15][   60/  196]   Loss 0.393996   Top1 86.132812   Top5 98.580729   BatchTime 0.388638   LR 0.002091
INFO - Training [15][   80/  196]   Loss 0.389100   Top1 86.337891   Top5 98.701172   BatchTime 0.381808   LR 0.002076
INFO - Training [15][  100/  196]   Loss 0.381770   Top1 86.640625   Top5 98.726562   BatchTime 0.372826   LR 0.002061
INFO - Training [15][  120/  196]   Loss 0.374793   Top1 86.881510   Top5 98.792318   BatchTime 0.367640   LR 0.002045
INFO - Training [15][  140/  196]   Loss 0.373897   Top1 86.936384   Top5 98.794643   BatchTime 0.364829   LR 0.002030
INFO - Training [15][  160/  196]   Loss 0.377346   Top1 86.799316   Top5 98.750000   BatchTime 0.361805   LR 0.002014
INFO - Training [15][  180/  196]   Loss 0.377223   Top1 86.846788   Top5 98.693576   BatchTime 0.359305   LR 0.001998
********************pre-trained*****************
INFO - ==> Top1: 86.822    Top5: 98.678    Loss: 0.377
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.349159   Top1 88.417969   Top5 99.550781   BatchTime 0.132297
INFO - Validation [15][   40/   40]   Loss 0.336871   Top1 88.610000   Top5 99.680000   BatchTime 0.094719
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.3809)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0373)
features.2.conv.0 tensor(0.0324)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0378)
features.3.conv.6 tensor(0.0312)
features.4.conv.0 tensor(0.0311)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.2772)
features.5.conv.0 tensor(0.0234)
features.5.conv.3 tensor(0.0642)
features.5.conv.6 tensor(0.2951)
features.6.conv.0 tensor(0.0221)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0457)
features.7.conv.0 tensor(0.0824)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.3710)
features.8.conv.0 tensor(0.0942)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.3888)
features.9.conv.0 tensor(0.1252)
features.9.conv.3 tensor(0.1392)
features.9.conv.6 tensor(0.4596)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.3043)
features.11.conv.0 tensor(0.5425)
features.11.conv.3 tensor(0.1584)
features.11.conv.6 tensor(0.7044)
features.12.conv.0 tensor(0.5187)
features.12.conv.3 tensor(0.1607)
features.12.conv.6 tensor(0.7392)
features.13.conv.0 tensor(0.1159)
features.13.conv.3 tensor(0.1578)
features.13.conv.6 tensor(0.1986)
features.14.conv.0 tensor(0.9508)
features.14.conv.3 tensor(0.1322)
features.14.conv.6 tensor(0.9764)
features.15.conv.0 tensor(0.9656)
features.15.conv.3 tensor(0.1215)
features.15.conv.6 tensor(0.9531)
features.16.conv.0 tensor(0.4068)
features.16.conv.3 tensor(0.1670)
features.16.conv.6 tensor(0.6611)
conv.0 tensor(0.8499)
tensor(1431143.) 2188896.0
INFO - ==> Top1: 88.610    Top5: 99.680    Loss: 0.337
INFO - ==> Sparsity : 0.654
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [16][   20/  196]   Loss 0.387689   Top1 86.367188   Top5 98.359375   BatchTime 0.368603   LR 0.001969
INFO - Training [16][   40/  196]   Loss 0.385601   Top1 86.318359   Top5 98.388672   BatchTime 0.357290   LR 0.001953
INFO - Training [16][   60/  196]   Loss 0.378753   Top1 86.582031   Top5 98.541667   BatchTime 0.359992   LR 0.001936
INFO - Training [16][   80/  196]   Loss 0.373597   Top1 86.723633   Top5 98.706055   BatchTime 0.361688   LR 0.001919
INFO - Training [16][  100/  196]   Loss 0.367207   Top1 87.015625   Top5 98.742188   BatchTime 0.356569   LR 0.001902
INFO - Training [16][  120/  196]   Loss 0.359968   Top1 87.272135   Top5 98.837891   BatchTime 0.352908   LR 0.001885
INFO - Training [16][  140/  196]   Loss 0.358239   Top1 87.340960   Top5 98.911830   BatchTime 0.350037   LR 0.001867
INFO - Training [16][  160/  196]   Loss 0.363735   Top1 87.136230   Top5 98.901367   BatchTime 0.348122   LR 0.001850
INFO - Training [16][  180/  196]   Loss 0.364224   Top1 87.096354   Top5 98.880208   BatchTime 0.346216   LR 0.001832
********************pre-trained*****************
INFO - ==> Top1: 87.110    Top5: 98.876    Loss: 0.364
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.367488   Top1 88.027344   Top5 99.472656   BatchTime 0.128684
INFO - Validation [16][   40/   40]   Loss 0.366004   Top1 87.840000   Top5 99.520000   BatchTime 0.090302
INFO - ==> Top1: 87.840    Top5: 99.520    Loss: 0.366
INFO - ==> Sparsity : 0.656
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.060   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5208)
features.0.conv.3 tensor(0.3789)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0360)
features.2.conv.0 tensor(0.0310)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0428)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0319)
features.4.conv.0 tensor(0.0314)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2791)
features.5.conv.0 tensor(0.0260)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2992)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0442)
features.7.conv.0 tensor(0.0891)
features.7.conv.3 tensor(0.1128)
features.7.conv.6 tensor(0.3744)
features.8.conv.0 tensor(0.0900)
features.8.conv.3 tensor(0.1282)
features.8.conv.6 tensor(0.3909)
features.9.conv.0 tensor(0.1203)
features.9.conv.3 tensor(0.1415)
features.9.conv.6 tensor(0.4619)
features.10.conv.0 tensor(0.0495)
features.10.conv.3 tensor(0.0897)
features.10.conv.6 tensor(0.3075)
features.11.conv.0 tensor(0.5467)
features.11.conv.3 tensor(0.1562)
features.11.conv.6 tensor(0.7056)
features.12.conv.0 tensor(0.5231)
features.12.conv.3 tensor(0.1539)
features.12.conv.6 tensor(0.7399)
features.13.conv.0 tensor(0.1162)
features.13.conv.3 tensor(0.1555)
features.13.conv.6 tensor(0.1981)
features.14.conv.0 tensor(0.9509)
features.14.conv.3 tensor(0.1322)
features.14.conv.6 tensor(0.9770)
features.15.conv.0 tensor(0.9660)
features.15.conv.3 tensor(0.1216)
features.15.conv.6 tensor(0.9726)
features.16.conv.0 tensor(0.4107)
features.16.conv.3 tensor(0.1670)
features.16.conv.6 tensor(0.6628)
conv.0 tensor(0.8514)
tensor(1436864.) 2188896.0
INFO - Training [17][   20/  196]   Loss 0.375141   Top1 86.601562   Top5 98.164062   BatchTime 0.387023   LR 0.001800
INFO - Training [17][   40/  196]   Loss 0.365117   Top1 87.041016   Top5 98.544922   BatchTime 0.358719   LR 0.001782
INFO - Training [17][   60/  196]   Loss 0.364086   Top1 87.135417   Top5 98.619792   BatchTime 0.352666   LR 0.001764
INFO - Training [17][   80/  196]   Loss 0.364396   Top1 87.182617   Top5 98.720703   BatchTime 0.351311   LR 0.001746
INFO - Training [17][  100/  196]   Loss 0.360992   Top1 87.265625   Top5 98.792969   BatchTime 0.354347   LR 0.001727
INFO - Training [17][  120/  196]   Loss 0.358739   Top1 87.434896   Top5 98.841146   BatchTime 0.352873   LR 0.001708
INFO - Training [17][  140/  196]   Loss 0.359859   Top1 87.477679   Top5 98.864397   BatchTime 0.353408   LR 0.001690
INFO - Training [17][  160/  196]   Loss 0.358860   Top1 87.492676   Top5 98.850098   BatchTime 0.353454   LR 0.001671
INFO - Training [17][  180/  196]   Loss 0.355598   Top1 87.595486   Top5 98.834635   BatchTime 0.351671   LR 0.001652
********************pre-trained*****************
INFO - ==> Top1: 87.594    Top5: 98.834    Loss: 0.355
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [17][   20/   40]   Loss 0.304963   Top1 90.136719   Top5 99.648438   BatchTime 0.133029
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0369)
features.2.conv.0 tensor(0.0336)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0446)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0348)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.2811)
features.5.conv.0 tensor(0.0260)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3008)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0470)
features.7.conv.0 tensor(0.0874)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.3775)
features.8.conv.0 tensor(0.0905)
features.8.conv.3 tensor(0.1348)
features.8.conv.6 tensor(0.3940)
features.9.conv.0 tensor(0.1168)
features.9.conv.3 tensor(0.1374)
features.9.conv.6 tensor(0.4642)
features.10.conv.0 tensor(0.0521)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.3105)
features.11.conv.0 tensor(0.5474)
features.11.conv.3 tensor(0.1624)
features.11.conv.6 tensor(0.7063)
features.12.conv.0 tensor(0.5246)
features.12.conv.3 tensor(0.1574)
features.12.conv.6 tensor(0.7423)
features.13.conv.0 tensor(0.1181)
features.13.conv.3 tensor(0.1626)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9510)
features.14.conv.3 tensor(0.1319)
features.14.conv.6 tensor(0.9759)
features.15.conv.0 tensor(0.9677)
features.15.conv.3 tensor(0.1240)
features.15.conv.6 tensor(0.9767)
features.16.conv.0 tensor(0.4125)
features.16.conv.3 tensor(0.1690)
features.16.conv.6 tensor(0.6666)
conv.0 tensor(0.8526)
tensor(1440387.) 2188896.0
INFO - Validation [17][   40/   40]   Loss 0.295041   Top1 90.260000   Top5 99.710000   BatchTime 0.091523
INFO - ==> Top1: 90.260    Top5: 99.710    Loss: 0.295
INFO - ==> Sparsity : 0.658
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 90.260   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [18][   20/  196]   Loss 0.355992   Top1 87.324219   Top5 98.125000   BatchTime 0.399786   LR 0.001618
INFO - Training [18][   40/  196]   Loss 0.358722   Top1 87.412109   Top5 98.300781   BatchTime 0.348022   LR 0.001599
INFO - Training [18][   60/  196]   Loss 0.359249   Top1 87.500000   Top5 98.411458   BatchTime 0.346698   LR 0.001579
INFO - Training [18][   80/  196]   Loss 0.355790   Top1 87.666016   Top5 98.598633   BatchTime 0.348204   LR 0.001560
INFO - Training [18][  100/  196]   Loss 0.348151   Top1 88.039062   Top5 98.667969   BatchTime 0.347932   LR 0.001540
INFO - Training [18][  120/  196]   Loss 0.343380   Top1 88.196615   Top5 98.753255   BatchTime 0.352456   LR 0.001521
INFO - Training [18][  140/  196]   Loss 0.344469   Top1 88.147321   Top5 98.816964   BatchTime 0.351124   LR 0.001501
INFO - Training [18][  160/  196]   Loss 0.350423   Top1 87.873535   Top5 98.791504   BatchTime 0.352923   LR 0.001482
INFO - Training [18][  180/  196]   Loss 0.347608   Top1 87.912326   Top5 98.760851   BatchTime 0.355837   LR 0.001462
INFO - ==> Top1: 87.924    Top5: 98.786    Loss: 0.347
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 0.305396   Top1 90.332031   Top5 99.531250   BatchTime 0.132417
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.3848)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0385)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.2830)
features.5.conv.0 tensor(0.0277)
features.5.conv.3 tensor(0.0637)
features.5.conv.6 tensor(0.3035)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0459)
features.7.conv.0 tensor(0.0907)
features.7.conv.3 tensor(0.1036)
features.7.conv.6 tensor(0.3800)
features.8.conv.0 tensor(0.0837)
features.8.conv.3 tensor(0.1351)
features.8.conv.6 tensor(0.3964)
features.9.conv.0 tensor(0.1172)
features.9.conv.3 tensor(0.1418)
features.9.conv.6 tensor(0.4668)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0975)
features.10.conv.6 tensor(0.3136)
features.11.conv.0 tensor(0.5491)
features.11.conv.3 tensor(0.1607)
features.11.conv.6 tensor(0.7067)
features.12.conv.0 tensor(0.5246)
features.12.conv.3 tensor(0.1557)
features.12.conv.6 tensor(0.7419)
features.13.conv.0 tensor(0.1121)
features.13.conv.3 tensor(0.1605)
features.13.conv.6 tensor(0.1986)
features.14.conv.0 tensor(0.9505)
features.14.conv.3 tensor(0.1332)
features.14.conv.6 tensor(0.9755)
features.15.conv.0 tensor(0.9677)
features.15.conv.3 tensor(0.1227)
features.15.conv.6 tensor(0.9788)
features.16.conv.0 tensor(0.4138)
features.16.conv.3 tensor(0.1693)
features.16.conv.6 tensor(0.6664)
conv.0 tensor(0.8523)
tensor(1440542.) 2188896.0
INFO - Validation [18][   40/   40]   Loss 0.293203   Top1 90.530000   Top5 99.650000   BatchTime 0.093038
INFO - ==> Top1: 90.530    Top5: 99.650    Loss: 0.293
INFO - ==> Sparsity : 0.658
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 90.530   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [19][   20/  196]   Loss 0.378918   Top1 86.386719   Top5 98.281250   BatchTime 0.422410   LR 0.001427
INFO - Training [19][   40/  196]   Loss 0.376711   Top1 86.679688   Top5 98.496094   BatchTime 0.352867   LR 0.001407
INFO - Training [19][   60/  196]   Loss 0.369540   Top1 86.972656   Top5 98.639323   BatchTime 0.341383   LR 0.001387
INFO - Training [19][   80/  196]   Loss 0.363398   Top1 87.192383   Top5 98.759766   BatchTime 0.341712   LR 0.001367
INFO - Training [19][  100/  196]   Loss 0.355471   Top1 87.570312   Top5 98.742188   BatchTime 0.342809   LR 0.001347
INFO - Training [19][  120/  196]   Loss 0.348751   Top1 87.698568   Top5 98.792318   BatchTime 0.347933   LR 0.001327
INFO - Training [19][  140/  196]   Loss 0.343591   Top1 87.887835   Top5 98.861607   BatchTime 0.347245   LR 0.001307
INFO - Training [19][  160/  196]   Loss 0.345664   Top1 87.841797   Top5 98.857422   BatchTime 0.347166   LR 0.001287
INFO - Training [19][  180/  196]   Loss 0.344178   Top1 87.936198   Top5 98.817274   BatchTime 0.345671   LR 0.001266
INFO - ==> Top1: 88.000    Top5: 98.820    Loss: 0.342
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [19][   20/   40]   Loss 0.303796   Top1 90.332031   Top5 99.570312   BatchTime 0.134866
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0379)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0370)
features.3.conv.6 tensor(0.0291)
features.4.conv.0 tensor(0.0329)
features.4.conv.3 tensor(0.0862)
features.4.conv.6 tensor(0.2843)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.3045)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0476)
features.7.conv.0 tensor(0.0896)
features.7.conv.3 tensor(0.1082)
features.7.conv.6
INFO - Validation [19][   40/   40]   Loss 0.285417   Top1 90.530000   Top5 99.680000   BatchTime 0.095588
INFO - ==> Top1: 90.530    Top5: 99.680    Loss: 0.285
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 90.530   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
features.7.conv.6 tensor(0.3815)
features.8.conv.0 tensor(0.0786)
features.8.conv.3 tensor(0.1317)
features.8.conv.6 tensor(0.3979)
features.9.conv.0 tensor(0.1155)
features.9.conv.3 tensor(0.1389)
features.9.conv.6 tensor(0.4685)
features.10.conv.0 tensor(0.0481)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.3152)
features.11.conv.0 tensor(0.5514)
features.11.conv.3 tensor(0.1601)
features.11.conv.6 tensor(0.7070)
features.12.conv.0 tensor(0.5247)
features.12.conv.3 tensor(0.1591)
features.12.conv.6 tensor(0.7423)
features.13.conv.0 tensor(0.1189)
features.13.conv.3 tensor(0.1611)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9510)
features.14.conv.3 tensor(0.1344)
features.14.conv.6 tensor(0.9756)
features.15.conv.0 tensor(0.9680)
features.15.conv.3 tensor(0.1235)
features.15.conv.6 tensor(0.9775)
features.16.conv.0 tensor(0.4161)
features.16.conv.3 tensor(0.1666)
features.16.conv.6 tensor(0.6665)
conv.0 tensor(0.8539)
tensor(1442059.) 2188896.0
INFO - Training [20][   20/  196]   Loss 0.345458   Top1 87.968750   Top5 98.261719   BatchTime 0.414925   LR 0.001231
INFO - Training [20][   40/  196]   Loss 0.347455   Top1 87.724609   Top5 98.398438   BatchTime 0.370764   LR 0.001211
INFO - Training [20][   60/  196]   Loss 0.345802   Top1 87.805990   Top5 98.522135   BatchTime 0.338106   LR 0.001191
INFO - Training [20][   80/  196]   Loss 0.344527   Top1 87.978516   Top5 98.642578   BatchTime 0.337212   LR 0.001171
INFO - Training [20][  100/  196]   Loss 0.335396   Top1 88.242188   Top5 98.742188   BatchTime 0.340154   LR 0.001151
INFO - Training [20][  120/  196]   Loss 0.329730   Top1 88.395182   Top5 98.808594   BatchTime 0.345758   LR 0.001131
INFO - Training [20][  140/  196]   Loss 0.327399   Top1 88.459821   Top5 98.883929   BatchTime 0.349402   LR 0.001111
INFO - Training [20][  160/  196]   Loss 0.331073   Top1 88.361816   Top5 98.869629   BatchTime 0.352100   LR 0.001091
INFO - Training [20][  180/  196]   Loss 0.329425   Top1 88.450521   Top5 98.845486   BatchTime 0.349965   LR 0.001071
INFO - ==> Top1: 88.498    Top5: 98.872    Loss: 0.328
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 0.346008   Top1 89.882812   Top5 99.433594   BatchTime 0.126234
INFO - Validation [20][   40/   40]   Loss 0.325823   Top1 90.010000   Top5 99.610000   BatchTime 0.088891
INFO - ==> Top1: 90.010    Top5: 99.610    Loss: 0.326
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 90.530   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.3848)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0326)
features.2.conv.0 tensor(0.0350)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0353)
features.4.conv.3 tensor(0.0851)
features.4.conv.6 tensor(0.2847)
features.5.conv.0 tensor(0.0254)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.3065)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0910)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.3826)
features.8.conv.0 tensor(0.0810)
features.8.conv.3 tensor(0.1308)
features.8.conv.6 tensor(0.3990)
features.9.conv.0 tensor(0.1176)
features.9.conv.3 tensor(0.1424)
features.9.conv.6 tensor(0.4690)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.3168)
features.11.conv.0 tensor(0.5518)
features.11.conv.3 tensor(0.1590)
features.11.conv.6 tensor(0.7074)
features.12.conv.0 tensor(0.5258)
features.12.conv.3 tensor(0.1564)
features.12.conv.6 tensor(0.7432)
features.13.conv.0 tensor(0.1220)
features.13.conv.3 tensor(0.1618)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9507)
features.14.conv.3 tensor(0.1337)
features.14.conv.6 tensor(0.9766)
features.15.conv.0 tensor(0.9669)
features.15.conv.3 tensor(0.1231)
features.15.conv.6 tensor(0.9753)
features.16.conv.0 tensor(0.4168)
features.16.conv.3 tensor(0.1696)
features.16.conv.6 tensor(0.6669)
conv.0 tensor(0.8534)
tensor(1442248.) 2188896.0
INFO - Training [21][   20/  196]   Loss 0.334729   Top1 88.300781   Top5 98.652344   BatchTime 0.486646   LR 0.001036
INFO - Training [21][   40/  196]   Loss 0.332076   Top1 88.369141   Top5 98.740234   BatchTime 0.410947   LR 0.001016
INFO - Training [21][   60/  196]   Loss 0.329674   Top1 88.535156   Top5 98.808594   BatchTime 0.373150   LR 0.000996
INFO - Training [21][   80/  196]   Loss 0.325812   Top1 88.657227   Top5 98.925781   BatchTime 0.350602   LR 0.000976
INFO - Training [21][  100/  196]   Loss 0.317133   Top1 88.851562   Top5 98.964844   BatchTime 0.350410   LR 0.000957
INFO - Training [21][  120/  196]   Loss 0.312560   Top1 89.049479   Top5 99.052734   BatchTime 0.350938   LR 0.000937
INFO - Training [21][  140/  196]   Loss 0.312722   Top1 89.082031   Top5 99.101562   BatchTime 0.357414   LR 0.000918
INFO - Training [21][  160/  196]   Loss 0.315140   Top1 88.962402   Top5 99.074707   BatchTime 0.357013   LR 0.000899
INFO - Training [21][  180/  196]   Loss 0.316773   Top1 88.860677   Top5 99.027778   BatchTime 0.355779   LR 0.000879
INFO - ==> Top1: 88.912    Top5: 99.012    Loss: 0.316
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [21][   20/   40]   Loss 0.299658   Top1 90.546875   Top5 99.511719   BatchTime 0.124696
INFO - Validation [21][   40/   40]   Loss 0.282895   Top1 90.820000   Top5 99.630000   BatchTime 0.086080
INFO - ==> Top1: 90.820    Top5: 99.630    Loss: 0.283
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 90.820   Top5: 99.630]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 90.770   Top5: 99.730]
features.0.conv.0 tensor(0.5278)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0395)
features.2.conv.0 tensor(0.0367)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0340)
features.3.conv.6 tensor(0.0310)
features.4.conv.0 tensor(0.0355)
features.4.conv.3 tensor(0.0793)
features.4.conv.6 tensor(0.2855)
features.5.conv.0 tensor(0.0249)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.3073)
features.6.conv.0 tensor(0.0233)
features.6.conv.3 tensor(0.0347)
features.6.conv.6 tensor(0.0470)
features.7.conv.0 tensor(0.0890)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.3835)
features.8.conv.0 tensor(0.0765)
features.8.conv.3 tensor(0.1314)
features.8.conv.6 tensor(0.4003)
features.9.conv.0 tensor(0.1190)
features.9.conv.3 tensor(0.1435)
features.9.conv.6 tensor(0.4698)
features.10.conv.0 tensor(0.0473)
features.10.conv.3 tensor(0.0906)
features.10.conv.6 tensor(0.3177)
features.11.conv.0 tensor(0.5528)
features.11.conv.3 tensor(0.1599)
features.11.conv.6 tensor(0.7076)
features.12.conv.0 tensor(0.5281)
features.12.conv.3 tensor(0.1570)
features.12.conv.6 tensor(0.7435)
features.13.conv.0 tensor(0.1245)
features.13.conv.3 tensor(0.1588)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9505)
features.14.conv.3 tensor(0.1331)
features.14.conv.6 tensor(0.9762)
features.15.conv.0 tensor(0.9670)
features.15.conv.3 tensor(0.1242)
features.15.conv.6 tensor(0.9739)
features.16.conv.0 tensor(0.4181)
features.16.conv.3 tensor(0.1692)
features.16.conv.6 tensor(0.6677)
conv.0 tensor(0.8533)
tensor(1442717.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [22][   20/  196]   Loss 0.311997   Top1 89.179688   Top5 98.632812   BatchTime 0.409162   LR 0.000846
INFO - Training [22][   40/  196]   Loss 0.323633   Top1 88.652344   Top5 98.886719   BatchTime 0.382088   LR 0.000827
INFO - Training [22][   60/  196]   Loss 0.313272   Top1 88.893229   Top5 98.958333   BatchTime 0.368434   LR 0.000808
INFO - Training [22][   80/  196]   Loss 0.316364   Top1 88.754883   Top5 99.003906   BatchTime 0.352557   LR 0.000789
INFO - Training [22][  100/  196]   Loss 0.310096   Top1 88.960938   Top5 99.019531   BatchTime 0.350704   LR 0.000770
INFO - Training [22][  120/  196]   Loss 0.304592   Top1 89.163411   Top5 99.072266   BatchTime 0.349996   LR 0.000752
INFO - Training [22][  140/  196]   Loss 0.302205   Top1 89.246652   Top5 99.118304   BatchTime 0.353963   LR 0.000734
INFO - Training [22][  160/  196]   Loss 0.304923   Top1 89.096680   Top5 99.079590   BatchTime 0.354929   LR 0.000715
INFO - Training [22][  180/  196]   Loss 0.307253   Top1 88.990885   Top5 99.036458   BatchTime 0.353676   LR 0.000697
INFO - ==> Top1: 89.074    Top5: 99.046    Loss: 0.305
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.285229   Top1 91.152344   Top5 99.609375   BatchTime 0.128010
INFO - Validation [22][   40/   40]   Loss 0.270419   Top1 91.470000   Top5 99.700000   BatchTime 0.088865
INFO - ==> Top1: 91.470    Top5: 99.700    Loss: 0.270
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 91.470   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 90.820   Top5: 99.630]
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0370)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0443)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0363)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.0781)
features.4.conv.6 tensor(0.2863)
features.5.conv.0 tensor(0.0269)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.3084)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0473)
features.7.conv.0 tensor(0.0913)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.3840)
features.8.conv.0 tensor(0.0792)
features.8.conv.3 tensor(0.1354)
features.8.conv.6 tensor(0.4012)
features.9.conv.0 tensor(0.1186)
features.9.conv.3 tensor(0.1429)
features.9.conv.6 tensor(0.4703)
features.10.conv.0 tensor(0.0448)
features.10.conv.3 tensor(0.0897)
features.10.conv.6 tensor(0.3183)
features.11.conv.0 tensor(0.5522)
features.11.conv.3 tensor(0.1591)
features.11.conv.6 tensor(0.7075)
features.12.conv.0 tensor(0.5294)
features.12.conv.3 tensor(0.1541)
features.12.conv.6 tensor(0.7426)
features.13.conv.0 tensor(0.1275)
features.13.conv.3 tensor(0.1576)
features.13.conv.6 tensor(0.1979)
features.14.conv.0 tensor(0.9500)
features.14.conv.3 tensor(0.1332)
features.14.conv.6 tensor(0.9767)
features.15.conv.0 tensor(0.9673)
features.15.conv.3 tensor(0.1234)
features.15.conv.6 tensor(0.9746)
features.16.conv.0 tensor(0.4181)
features.16.conv.3 tensor(0.1685)
features.16.conv.6 tensor(0.6676)
conv.0 tensor(0.8530)
tensor(1442932.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [23][   20/  196]   Loss 0.315351   Top1 89.394531   Top5 98.476562   BatchTime 0.413375   LR 0.000666
INFO - Training [23][   40/  196]   Loss 0.319492   Top1 89.023438   Top5 98.740234   BatchTime 0.375029   LR 0.000648
INFO - Training [23][   60/  196]   Loss 0.318360   Top1 88.906250   Top5 98.821615   BatchTime 0.359885   LR 0.000630
INFO - Training [23][   80/  196]   Loss 0.313277   Top1 89.101562   Top5 98.945312   BatchTime 0.350661   LR 0.000613
INFO - Training [23][  100/  196]   Loss 0.305720   Top1 89.414062   Top5 98.957031   BatchTime 0.347165   LR 0.000596
INFO - Training [23][  120/  196]   Loss 0.300197   Top1 89.622396   Top5 99.010417   BatchTime 0.345911   LR 0.000579
INFO - Training [23][  140/  196]   Loss 0.299367   Top1 89.659598   Top5 99.073661   BatchTime 0.349233   LR 0.000562
INFO - Training [23][  160/  196]   Loss 0.302522   Top1 89.511719   Top5 99.052734   BatchTime 0.350161   LR 0.000545
INFO - Training [23][  180/  196]   Loss 0.301762   Top1 89.496528   Top5 99.025608   BatchTime 0.351165   LR 0.000529
INFO - ==> Top1: 89.472    Top5: 99.008    Loss: 0.302
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.305807   Top1 90.976562   Top5 99.589844   BatchTime 0.136088
INFO - Validation [23][   40/   40]   Loss 0.291634   Top1 91.130000   Top5 99.740000   BatchTime 0.095305
INFO - ==> Top1: 91.130    Top5: 99.740    Loss: 0.292
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 91.470   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 91.130   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 91.050   Top5: 99.760]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0391)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0417)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0324)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0353)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.2866)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.3084)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0479)
features.7.conv.0 tensor(0.0891)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.3846)
features.8.conv.0 tensor(0.0776)
features.8.conv.3 tensor(0.1325)
features.8.conv.6 tensor(0.4015)
features.9.conv.0 tensor(0.1184)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.4704)
features.10.conv.0 tensor(0.0472)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.3187)
features.11.conv.0 tensor(0.5531)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.7065)
features.12.conv.0 tensor(0.5301)
features.12.conv.3 tensor(0.1553)
features.12.conv.6 tensor(0.7430)
features.13.conv.0 tensor(0.1297)
features.13.conv.3 tensor(0.1549)
features.13.conv.6 tensor(0.1982)
features.14.conv.0 tensor(0.9504)
features.14.conv.3 tensor(0.1321)
features.14.conv.6 tensor(0.9761)
features.15.conv.0 tensor(0.9675)
features.15.conv.3 tensor(0.1231)
features.15.conv.6 tensor(0.9731)
features.16.conv.0 tensor(0.4182)
features.16.conv.3 tensor(0.1677)
features.16.conv.6 tensor(0.6681)
conv.0 tensor(0.8532)
tensor(1443087.) 2188896.0
INFO - Training [24][   20/  196]   Loss 0.324502   Top1 88.554688   Top5 98.554688   BatchTime 0.430381   LR 0.000500
INFO - Training [24][   40/  196]   Loss 0.315297   Top1 88.720703   Top5 98.837891   BatchTime 0.380829   LR 0.000484
INFO - Training [24][   60/  196]   Loss 0.305730   Top1 89.173177   Top5 98.847656   BatchTime 0.366946   LR 0.000468
INFO - Training [24][   80/  196]   Loss 0.300622   Top1 89.443359   Top5 98.964844   BatchTime 0.344938   LR 0.000453
INFO - Training [24][  100/  196]   Loss 0.293726   Top1 89.578125   Top5 99.039062   BatchTime 0.338487   LR 0.000437
INFO - Training [24][  120/  196]   Loss 0.289618   Top1 89.710286   Top5 99.085286   BatchTime 0.337642   LR 0.000422
INFO - Training [24][  140/  196]   Loss 0.288390   Top1 89.843750   Top5 99.093192   BatchTime 0.340788   LR 0.000407
INFO - Training [24][  160/  196]   Loss 0.289940   Top1 89.807129   Top5 99.089355   BatchTime 0.340753   LR 0.000392
INFO - Training [24][  180/  196]   Loss 0.291308   Top1 89.713542   Top5 99.058160   BatchTime 0.341810   LR 0.000378
INFO - ==> Top1: 89.790    Top5: 99.056    Loss: 0.291
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [24][   20/   40]   Loss 0.302188   Top1 91.015625   Top5 99.589844   BatchTime 0.130134
INFO - Validation [24][   40/   40]   Loss 0.284655   Top1 91.320000   Top5 99.710000   BatchTime 0.090387
INFO - ==> Top1: 91.320    Top5: 99.710    Loss: 0.285
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 91.470   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.320   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 91.130   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0369)
features.2.conv.0 tensor(0.0396)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0420)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.2868)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.3089)
features.6.conv.0 tensor(0.0207)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0458)
features.7.conv.0 tensor(0.0874)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.3846)
features.8.conv.0 tensor(0.0772)
features.8.conv.3 tensor(0.1305)
features.8.conv.6 tensor(0.4016)
features.9.conv.0 tensor(0.1184)
features.9.conv.3 tensor(0.1415)
features.9.conv.6 tensor(0.4706)
features.10.conv.0 tensor(0.0460)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.3190)
features.11.conv.0 tensor(0.5532)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.7063)
features.12.conv.0 tensor(0.5306)
features.12.conv.3 tensor(0.1553)
features.12.conv.6 tensor(0.7428)
features.13.conv.0 tensor(0.1279)
features.13.conv.3 tensor(0.1582)
features.13.conv.6 tensor(0.1981)
features.14.conv.0 tensor(0.9501)
features.14.conv.3 tensor(0.1333)
features.14.conv.6 tensor(0.9764)
features.15.conv.0 tensor(0.9679)
features.15.conv.3 tensor(0.1247)
features.15.conv.6 tensor(0.9742)
features.16.conv.0 tensor(0.4189)
features.16.conv.3 tensor(0.1672)
features.16.conv.6 tensor(0.6678)
conv.0 tensor(0.8532)
tensor(1443174.) 2188896.0
INFO - Training [25][   20/  196]   Loss 0.302497   Top1 89.160156   Top5 98.632812   BatchTime 0.412540   LR 0.000353
INFO - Training [25][   40/  196]   Loss 0.305608   Top1 89.130859   Top5 98.603516   BatchTime 0.376088   LR 0.000339
INFO - Training [25][   60/  196]   Loss 0.300237   Top1 89.290365   Top5 98.776042   BatchTime 0.362207   LR 0.000325
INFO - Training [25][   80/  196]   Loss 0.296145   Top1 89.418945   Top5 98.916016   BatchTime 0.356717   LR 0.000312
INFO - Training [25][  100/  196]   Loss 0.288666   Top1 89.761719   Top5 98.968750   BatchTime 0.342527   LR 0.000299
INFO - Training [25][  120/  196]   Loss 0.286853   Top1 89.856771   Top5 99.036458   BatchTime 0.340215   LR 0.000286
INFO - Training [25][  140/  196]   Loss 0.286185   Top1 89.910714   Top5 99.076451   BatchTime 0.341240   LR 0.000273
INFO - Training [25][  160/  196]   Loss 0.288370   Top1 89.797363   Top5 99.094238   BatchTime 0.346263   LR 0.000261
INFO - Training [25][  180/  196]   Loss 0.288009   Top1 89.791667   Top5 99.045139   BatchTime 0.348638   LR 0.000248
INFO - ==> Top1: 89.780    Top5: 99.008    Loss: 0.287
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 0.527850   Top1 84.414062   Top5 99.101562   BatchTime 0.124611
INFO - Validation [25][   40/   40]   Loss 0.518948   Top1 84.440000   Top5 99.210000   BatchTime 0.085947
INFO - ==> Top1: 84.440    Top5: 99.210    Loss: 0.519
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 91.470   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.320   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 91.130   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0365)
features.2.conv.0 tensor(0.0399)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0417)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.2868)
features.5.conv.0 tensor(0.0277)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.3089)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0471)
features.7.conv.0 tensor(0.0868)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.3846)
features.8.conv.0 tensor(0.0760)
features.8.conv.3 tensor(0.1311)
features.8.conv.6 tensor(0.4020)
features.9.conv.0 tensor(0.1166)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.4707)
features.10.conv.0 tensor(0.0461)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.3191)
features.11.conv.0 tensor(0.5532)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.7064)
features.12.conv.0 tensor(0.5296)
features.12.conv.3 tensor(0.1549)
features.12.conv.6 tensor(0.7426)
features.13.conv.0 tensor(0.1290)
features.13.conv.3 tensor(0.1557)
features.13.conv.6 tensor(0.1981)
features.14.conv.0 tensor(0.9497)
features.14.conv.3 tensor(0.1329)
features.14.conv.6 tensor(0.9762)
features.15.conv.0 tensor(0.9676)
features.15.conv.3 tensor(0.1248)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.4193)
features.16.conv.3 tensor(0.1669)
features.16.conv.6 tensor(0.6679)
conv.0 tensor(0.8528)
tensor(1443092.) 2188896.0
INFO - Training [26][   20/  196]   Loss 0.298260   Top1 89.726562   Top5 98.593750   BatchTime 0.412345   LR 0.000228
INFO - Training [26][   40/  196]   Loss 0.288645   Top1 89.746094   Top5 98.720703   BatchTime 0.373893   LR 0.000216
INFO - Training [26][   60/  196]   Loss 0.287639   Top1 89.967448   Top5 98.789062   BatchTime 0.359606   LR 0.000205
INFO - Training [26][   80/  196]   Loss 0.285845   Top1 89.951172   Top5 98.935547   BatchTime 0.354377   LR 0.000194
INFO - Training [26][  100/  196]   Loss 0.281445   Top1 90.074219   Top5 98.996094   BatchTime 0.347262   LR 0.000183
INFO - Training [26][  120/  196]   Loss 0.276444   Top1 90.244141   Top5 99.082031   BatchTime 0.335215   LR 0.000173
INFO - Training [26][  140/  196]   Loss 0.276143   Top1 90.220424   Top5 99.132254   BatchTime 0.338221   LR 0.000163
INFO - Training [26][  160/  196]   Loss 0.280918   Top1 90.109863   Top5 99.106445   BatchTime 0.341322   LR 0.000153
INFO - Training [26][  180/  196]   Loss 0.280211   Top1 90.097656   Top5 99.095052   BatchTime 0.345349   LR 0.000144
INFO - ==> Top1: 90.128    Top5: 99.094    Loss: 0.280
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [26][   20/   40]   Loss 0.439732   Top1 86.835938   Top5 99.277344   BatchTime 0.132505
INFO - Validation [26][   40/   40]   Loss 0.431312   Top1 86.800000   Top5 99.360000   BatchTime 0.093341
INFO - ==> Top1: 86.800    Top5: 99.360    Loss: 0.431
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 91.470   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.320   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 91.130   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.3848)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0339)
features.2.conv.0 tensor(0.0408)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0425)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0353)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.2869)
features.5.conv.0 tensor(0.0280)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.3092)
features.6.conv.0 tensor(0.0208)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0478)
features.7.conv.0 tensor(0.0868)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.3849)
features.8.conv.0 tensor(0.0758)
features.8.conv.3 tensor(0.1305)
features.8.conv.6 tensor(0.4018)
features.9.conv.0 tensor(0.1152)
features.9.conv.3 tensor(0.1412)
features.9.conv.6 tensor(0.4705)
features.10.conv.0 tensor(0.0457)
features.10.conv.3 tensor(0.0909)
features.10.conv.6 tensor(0.3192)
features.11.conv.0 tensor(0.5529)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.7061)
features.12.conv.0 tensor(0.5299)
features.12.conv.3 tensor(0.1566)
features.12.conv.6 tensor(0.7427)
features.13.conv.0 tensor(0.1276)
features.13.conv.3 tensor(0.1559)
features.13.conv.6 tensor(0.1981)
features.14.conv.0 tensor(0.9500)
features.14.conv.3 tensor(0.1328)
features.14.conv.6 tensor(0.9766)
features.15.conv.0 tensor(0.9678)
features.15.conv.3 tensor(0.1253)
features.15.conv.6 tensor(0.9756)
features.16.conv.0 tensor(0.4192)
features.16.conv.3 tensor(0.1667)
features.16.conv.6 tensor(0.6677)
conv.0 tensor(0.8531)
tensor(1443192.) 2188896.0
INFO - Training [27][   20/  196]   Loss 0.306716   Top1 89.257812   Top5 98.593750   BatchTime 0.419964   LR 0.000128
INFO - Training [27][   40/  196]   Loss 0.291933   Top1 89.746094   Top5 98.847656   BatchTime 0.377203   LR 0.000119
INFO - Training [27][   60/  196]   Loss 0.286083   Top1 89.902344   Top5 98.945312   BatchTime 0.364920   LR 0.000111
INFO - Training [27][   80/  196]   Loss 0.288001   Top1 89.843750   Top5 98.999023   BatchTime 0.357044   LR 0.000102
INFO - Training [27][  100/  196]   Loss 0.283549   Top1 90.054688   Top5 99.023438   BatchTime 0.353210   LR 0.000095
INFO - Training [27][  120/  196]   Loss 0.280104   Top1 90.221354   Top5 99.065755   BatchTime 0.345565   LR 0.000087
INFO - Training [27][  140/  196]   Loss 0.277659   Top1 90.318080   Top5 99.070871   BatchTime 0.338851   LR 0.000080
INFO - Training [27][  160/  196]   Loss 0.277165   Top1 90.319824   Top5 99.050293   BatchTime 0.339639   LR 0.000073
INFO - Training [27][  180/  196]   Loss 0.277199   Top1 90.323351   Top5 99.045139   BatchTime 0.344427   LR 0.000066
INFO - ==> Top1: 90.392    Top5: 99.060    Loss: 0.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [27][   20/   40]   Loss 0.286411   Top1 91.347656   Top5 99.667969   BatchTime 0.129870
INFO - Validation [27][   40/   40]   Loss 0.270951   Top1 91.630000   Top5 99.790000   BatchTime 0.091029
INFO - ==> Top1: 91.630    Top5: 99.790    Loss: 0.271
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 91.470   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 91.320   Top5: 99.710]
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0414)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0414)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0297)
features.4.conv.0 tensor(0.0361)
features.4.conv.3 tensor(0.0845)
features.4.conv.6 tensor(0.2871)
features.5.conv.0 tensor(0.0272)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3091)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0478)
features.7.conv.0 tensor(0.0868)
features.7.conv.3 tensor(0.1065)
features.7.conv.6 tensor(0.3849)
features.8.conv.0 tensor(0.0763)
features.8.conv.3 tensor(0.1322)
features.8.conv.6 tensor(0.4018)
features.9.conv.0 tensor(0.1139)
features.9.conv.3 tensor(0.1412)
features.9.conv.6 tensor(0.4707)
features.10.conv.0 tensor(0.0454)
features.10.conv.3 tensor(0.0917)
features.10.conv.6 tensor(0.3193)
features.11.conv.0 tensor(0.5527)
features.11.conv.3 tensor(0.1561)
features.11.conv.6 tensor(0.7063)
features.12.conv.0 tensor(0.5299)
features.12.conv.3 tensor(0.1553)
features.12.conv.6 tensor(0.7425)
features.13.conv.0 tensor(0.1268)
features.13.conv.3 tensor(0.1561)
features.13.conv.6 tensor(0.1979)
features.14.conv.0 tensor(0.9502)
features.14.conv.3 tensor(0.1330)
features.14.conv.6 tensor(0.9767)
features.15.conv.0 tensor(0.9679)
features.15.conv.3 tensor(0.1252)
features.15.conv.6 tensor(0.9758)
features.16.conv.0 tensor(0.4193)
features.16.conv.3 tensor(0.1672)
features.16.conv.6 tensor(0.6678)
conv.0 tensor(0.8530)
tensor(1443166.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [28][   20/  196]   Loss 0.294121   Top1 89.316406   Top5 98.613281   BatchTime 0.422854   LR 0.000055
INFO - Training [28][   40/  196]   Loss 0.288202   Top1 89.687500   Top5 98.779297   BatchTime 0.388660   LR 0.000050
INFO - Training [28][   60/  196]   Loss 0.285761   Top1 89.843750   Top5 98.880208   BatchTime 0.374288   LR 0.000044
INFO - Training [28][   80/  196]   Loss 0.282507   Top1 90.014648   Top5 98.989258   BatchTime 0.367679   LR 0.000039
INFO - Training [28][  100/  196]   Loss 0.279888   Top1 90.125000   Top5 99.031250   BatchTime 0.362682   LR 0.000034
INFO - Training [28][  120/  196]   Loss 0.275035   Top1 90.338542   Top5 99.127604   BatchTime 0.355826   LR 0.000030
INFO - Training [28][  140/  196]   Loss 0.274199   Top1 90.421317   Top5 99.162946   BatchTime 0.352397   LR 0.000026
INFO - Training [28][  160/  196]   Loss 0.275215   Top1 90.371094   Top5 99.145508   BatchTime 0.349337   LR 0.000022
INFO - Training [28][  180/  196]   Loss 0.276028   Top1 90.366753   Top5 99.075521   BatchTime 0.350383   LR 0.000018
INFO - ==> Top1: 90.380    Top5: 99.076    Loss: 0.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [28][   20/   40]   Loss 0.280046   Top1 91.484375   Top5 99.667969   BatchTime 0.129712
INFO - Validation [28][   40/   40]   Loss 0.265471   Top1 91.610000   Top5 99.740000   BatchTime 0.093781
INFO - ==> Top1: 91.610    Top5: 99.740    Loss: 0.265
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 91.470   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3848)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0356)
features.2.conv.0 tensor(0.0402)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0422)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0361)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.2869)
features.5.conv.0 tensor(0.0270)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.3089)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0472)
features.7.conv.0 tensor(0.0868)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.3849)
features.8.conv.0 tensor(0.0761)
features.8.conv.3 tensor(0.1314)
features.8.conv.6 tensor(0.4019)
features.9.conv.0 tensor(0.1139)
features.9.conv.3 tensor(0.1400)
features.9.conv.6 tensor(0.4706)
features.10.conv.0 tensor(0.0452)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.3193)
features.11.conv.0 tensor(0.5527)
features.11.conv.3 tensor(0.1574)
features.11.conv.6 tensor(0.7063)
features.12.conv.0 tensor(0.5299)
features.12.conv.3 tensor(0.1547)
features.12.conv.6 tensor(0.7425)
features.13.conv.0 tensor(0.1267)
features.13.conv.3 tensor(0.1545)
features.13.conv.6 tensor(0.1981)
features.14.conv.0 tensor(0.9502)
features.14.conv.3 tensor(0.1323)
features.14.conv.6 tensor(0.9765)
features.15.conv.0 tensor(0.9678)
features.15.conv.3 tensor(0.1250)
features.15.conv.6 tensor(0.9756)
features.16.conv.0 tensor(0.4192)
features.16.conv.3 tensor(0.1667)
features.16.conv.6 tensor(0.6677)
conv.0 tensor(0.8530)
tensor(1443017.) 2188896.0
INFO - Training [29][   20/  196]   Loss 0.279947   Top1 90.000000   Top5 98.535156   BatchTime 0.427609   LR 0.000013
INFO - Training [29][   40/  196]   Loss 0.286155   Top1 89.824219   Top5 98.701172   BatchTime 0.388264   LR 0.000010
INFO - Training [29][   60/  196]   Loss 0.281487   Top1 89.993490   Top5 98.795573   BatchTime 0.373750   LR 0.000008
INFO - Training [29][   80/  196]   Loss 0.277965   Top1 90.268555   Top5 98.935547   BatchTime 0.367059   LR 0.000005
INFO - Training [29][  100/  196]   Loss 0.271932   Top1 90.500000   Top5 98.976562   BatchTime 0.365495   LR 0.000004
INFO - Training [29][  120/  196]   Loss 0.267949   Top1 90.654297   Top5 99.055990   BatchTime 0.358856   LR 0.000002
INFO - Training [29][  140/  196]   Loss 0.270652   Top1 90.552455   Top5 99.090402   BatchTime 0.348865   LR 0.000001
INFO - Training [29][  160/  196]   Loss 0.271844   Top1 90.488281   Top5 99.052734   BatchTime 0.344835   LR 0.000001
INFO - Training [29][  180/  196]   Loss 0.273539   Top1 90.375434   Top5 99.036458   BatchTime 0.346938   LR 0.000000
INFO - ==> Top1: 90.416    Top5: 99.040    Loss: 0.272
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [29][   20/   40]   Loss 0.273592   Top1 91.621094   Top5 99.667969   BatchTime 0.127900
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3848)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0405)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0420)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0360)
features.4.conv.3 tensor(0.0828)
features.4.conv.6 tensor(0.2871)
features.5.conv.0 tensor(0.0267)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.3091)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0471)
features.7.conv.0 tensor(0.0868)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.3849)
INFO - Validation [29][   40/   40]   Loss 0.260588   Top1 91.730000   Top5 99.740000   BatchTime 0.087818
INFO - ==> Top1: 91.730    Top5: 99.740    Loss: 0.261
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
features.8.conv.0 tensor(0.0759)
features.8.conv.3 tensor(0.1311)
features.8.conv.6 tensor(0.4019)
features.9.conv.0 tensor(0.1136)
features.9.conv.3 tensor(0.1406)
features.9.conv.6 tensor(0.4706)
features.10.conv.0 tensor(0.0452)
features.10.conv.3 tensor(0.0909)
features.10.conv.6 tensor(0.3193)
features.11.conv.0 tensor(0.5527)
features.11.conv.3 tensor(0.1566)
features.11.conv.6 tensor(0.7063)
features.12.conv.0 tensor(0.5299)
features.12.conv.3 tensor(0.1564)
features.12.conv.6 tensor(0.7424)
features.13.conv.0 tensor(0.1267)
features.13.conv.3 tensor(0.1549)
features.13.conv.6 tensor(0.1981)
features.14.conv.0 tensor(0.9499)
features.14.conv.3 tensor(0.1317)
features.14.conv.6 tensor(0.9763)
features.15.conv.0 tensor(0.9678)
features.15.conv.3 tensor(0.1251)
features.15.conv.6 tensor(0.9752)
features.16.conv.0 tensor(0.4192)
features.16.conv.3 tensor(0.1659)
features.16.conv.6 tensor(0.6677)
conv.0 tensor(0.8530)
tensor(1442876.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [30][   20/  196]   Loss 0.300925   Top1 89.394531   Top5 98.457031   BatchTime 0.422725   LR 0.001250
INFO - Training [30][   40/  196]   Loss 0.305592   Top1 89.287109   Top5 98.632812   BatchTime 0.379929   LR 0.001250
INFO - Training [30][   60/  196]   Loss 0.305767   Top1 89.322917   Top5 98.782552   BatchTime 0.367706   LR 0.001250
INFO - Training [30][   80/  196]   Loss 0.303725   Top1 89.243164   Top5 98.935547   BatchTime 0.361679   LR 0.001250
INFO - Training [30][  100/  196]   Loss 0.302750   Top1 89.296875   Top5 98.941406   BatchTime 0.360803   LR 0.001250
INFO - Training [30][  120/  196]   Loss 0.300448   Top1 89.361979   Top5 99.000651   BatchTime 0.355470   LR 0.001249
INFO - Training [30][  140/  196]   Loss 0.298999   Top1 89.402902   Top5 99.073661   BatchTime 0.346684   LR 0.001249
INFO - Training [30][  160/  196]   Loss 0.303658   Top1 89.211426   Top5 99.062500   BatchTime 0.347238   LR 0.001249
INFO - Training [30][  180/  196]   Loss 0.304685   Top1 89.188368   Top5 99.027778   BatchTime 0.348977   LR 0.001248
INFO - ==> Top1: 89.148    Top5: 99.022    Loss: 0.306
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.304764   Top1 90.859375   Top5 99.570312   BatchTime 0.130620
INFO - Validation [30][   40/   40]   Loss 0.288132   Top1 91.130000   Top5 99.690000   BatchTime 0.090205
INFO - ==> Top1: 91.130    Top5: 99.690    Loss: 0.288
INFO - ==> Sparsity : 0.659
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0365)
features.2.conv.0 tensor(0.0446)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0440)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0363)
features.4.conv.3 tensor(0.0822)
features.4.conv.6 tensor(0.2887)
features.5.conv.0 tensor(0.0288)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.3102)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0284)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0883)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.3863)
features.8.conv.0 tensor(0.0785)
features.8.conv.3 tensor(0.1305)
features.8.conv.6 tensor(0.4035)
features.9.conv.0 tensor(0.1098)
features.9.conv.3 tensor(0.1395)
features.9.conv.6 tensor(0.4718)
features.10.conv.0 tensor(0.0416)
features.10.conv.3 tensor(0.0903)
features.10.conv.6 tensor(0.3210)
features.11.conv.0 tensor(0.5554)
features.11.conv.3 tensor(0.1586)
features.11.conv.6 tensor(0.7064)
features.12.conv.0 tensor(0.5260)
features.12.conv.3 tensor(0.1561)
features.12.conv.6 tensor(0.7426)
features.13.conv.0 tensor(0.1194)
features.13.conv.3 tensor(0.1576)
features.13.conv.6 tensor(0.1981)
features.14.conv.0 tensor(0.9503)
features.14.conv.3 tensor(0.1324)
features.14.conv.6 tensor(0.9748)
features.15.conv.0 tensor(0.9687)
features.15.conv.3 tensor(0.1243)
features.15.conv.6 tensor(0.9740)
features.16.conv.0 tensor(0.4210)
features.16.conv.3 tensor(0.1666)
features.16.conv.6 tensor(0.6686)
conv.0 tensor(0.8541)
tensor(1443382.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [31][   20/  196]   Loss 0.318085   Top1 88.632812   Top5 98.671875   BatchTime 0.428580   LR 0.001248
INFO - Training [31][   40/  196]   Loss 0.327416   Top1 88.271484   Top5 98.701172   BatchTime 0.395355   LR 0.001247
INFO - Training [31][   60/  196]   Loss 0.323157   Top1 88.639323   Top5 98.782552   BatchTime 0.384920   LR 0.001247
INFO - Training [31][   80/  196]   Loss 0.317064   Top1 88.813477   Top5 98.945312   BatchTime 0.378207   LR 0.001246
INFO - Training [31][  100/  196]   Loss 0.313346   Top1 88.890625   Top5 98.972656   BatchTime 0.372891   LR 0.001246
INFO - Training [31][  120/  196]   Loss 0.308886   Top1 89.065755   Top5 99.042969   BatchTime 0.369024   LR 0.001245
INFO - Training [31][  140/  196]   Loss 0.305843   Top1 89.160156   Top5 99.079241   BatchTime 0.360666   LR 0.001244
INFO - Training [31][  160/  196]   Loss 0.310534   Top1 88.986816   Top5 99.069824   BatchTime 0.351665   LR 0.001244
INFO - Training [31][  180/  196]   Loss 0.311146   Top1 88.962674   Top5 99.025608   BatchTime 0.350798   LR 0.001243
INFO - ==> Top1: 89.068    Top5: 99.000    Loss: 0.309
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [31][   20/   40]   Loss 0.330051   Top1 90.332031   Top5 99.492188   BatchTime 0.138383
INFO - Validation [31][   40/   40]   Loss 0.310552   Top1 90.540000   Top5 99.620000   BatchTime 0.095280
INFO - ==> Top1: 90.540    Top5: 99.620    Loss: 0.311
INFO - ==> Sparsity : 0.660
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0567)
features.1.conv.6 tensor(0.0339)
features.2.conv.0 tensor(0.0443)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0417)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0384)
features.4.conv.3 tensor(0.0764)
features.4.conv.6 tensor(0.2902)
features.5.conv.0 tensor(0.0280)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.3118)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0457)
features.7.conv.0 tensor(0.0870)
features.7.conv.3 tensor(0.1097)
features.7.conv.6 tensor(0.3884)
features.8.conv.0 tensor(0.0850)
features.8.conv.3 tensor(0.1259)
features.8.conv.6 tensor(0.4049)
features.9.conv.0 tensor(0.1188)
features.9.conv.3 tensor(0.1400)
features.9.conv.6 tensor(0.4728)
features.10.conv.0 tensor(0.0380)
features.10.conv.3 tensor(0.0877)
features.10.conv.6 tensor(0.3226)
features.11.conv.0 tensor(0.5569)
features.11.conv.3 tensor(0.1601)
features.11.conv.6 tensor(0.7073)
features.12.conv.0 tensor(0.5314)
features.12.conv.3 tensor(0.1535)
features.12.conv.6 tensor(0.7441)
features.13.conv.0 tensor(0.1158)
features.13.conv.3 tensor(0.1568)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9499)
features.14.conv.3 tensor(0.1345)
features.14.conv.6 tensor(0.9768)
features.15.conv.0 tensor(0.9693)
features.15.conv.3 tensor(0.1242)
features.15.conv.6 tensor(0.9774)
features.16.conv.0 tensor(0.4225)
features.16.conv.3 tensor(0.1697)
features.16.conv.6 tensor(0.6678)
conv.0 tensor(0.8550)
tensor(1445359.) 2188896.0
INFO - Training [32][   20/  196]   Loss 0.325141   Top1 88.593750   Top5 98.496094   BatchTime 0.424299   LR 0.001242
INFO - Training [32][   40/  196]   Loss 0.327434   Top1 88.642578   Top5 98.642578   BatchTime 0.386468   LR 0.001241
INFO - Training [32][   60/  196]   Loss 0.327444   Top1 88.665365   Top5 98.730469   BatchTime 0.370665   LR 0.001240
INFO - Training [32][   80/  196]   Loss 0.320401   Top1 88.906250   Top5 98.862305   BatchTime 0.364527   LR 0.001239
INFO - Training [32][  100/  196]   Loss 0.313682   Top1 89.148438   Top5 98.937500   BatchTime 0.358726   LR 0.001238
INFO - Training [32][  120/  196]   Loss 0.311497   Top1 89.173177   Top5 99.000651   BatchTime 0.355430   LR 0.001237
INFO - Training [32][  140/  196]   Loss 0.310211   Top1 89.185268   Top5 99.026228   BatchTime 0.352585   LR 0.001236
INFO - Training [32][  160/  196]   Loss 0.313391   Top1 89.079590   Top5 99.035645   BatchTime 0.350555   LR 0.001235
INFO - Training [32][  180/  196]   Loss 0.312650   Top1 89.118924   Top5 99.012587   BatchTime 0.343987   LR 0.001234
INFO - ==> Top1: 89.128    Top5: 98.996    Loss: 0.313
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.579225   Top1 82.304688   Top5 98.925781   BatchTime 0.202232
INFO - Validation [32][   40/   40]   Loss 0.570408   Top1 82.440000   Top5 99.070000   BatchTime 0.135001
INFO - ==> Top1: 82.440    Top5: 99.070    Loss: 0.570
INFO - ==> Sparsity : 0.661
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.3828)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0339)
features.2.conv.0 tensor(0.0437)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0411)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0363)
features.4.conv.3 tensor(0.0839)
features.4.conv.6 tensor(0.2920)
features.5.conv.0 tensor(0.0277)
features.5.conv.3 tensor(0.0608)
features.5.conv.6 tensor(0.3128)
features.6.conv.0 tensor(0.0199)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0474)
features.7.conv.0 tensor(0.0819)
features.7.conv.3 tensor(0.1062)
features.7.conv.6 tensor(0.3899)
features.8.conv.0 tensor(0.0846)
features.8.conv.3 tensor(0.1264)
features.8.conv.6 tensor(0.4062)
features.9.conv.0 tensor(0.1195)
features.9.conv.3 tensor(0.1412)
features.9.conv.6 tensor(0.4741)
features.10.conv.0 tensor(0.0437)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.3243)
features.11.conv.0 tensor(0.5585)
features.11.conv.3 tensor(0.1593)
features.11.conv.6 tensor(0.7083)
features.12.conv.0 tensor(0.5327)
features.12.conv.3 tensor(0.1532)
features.12.conv.6 tensor(0.7456)
features.13.conv.0 tensor(0.1139)
features.13.conv.3 tensor(0.1578)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9512)
features.14.conv.3 tensor(0.1336)
features.14.conv.6 tensor(0.9762)
features.15.conv.0 tensor(0.9714)
features.15.conv.3 tensor(0.1248)
features.15.conv.6 tensor(0.9792)
features.16.conv.0 tensor(0.4253)
features.16.conv.3 tensor(0.1674)
features.16.conv.6 tensor(0.6689)
conv.0 tensor(0.8561)
tensor(1447608.) 2188896.0
INFO - Training [33][   20/  196]   Loss 0.322185   Top1 88.437500   Top5 98.750000   BatchTime 0.428725   LR 0.001232
INFO - Training [33][   40/  196]   Loss 0.327048   Top1 88.642578   Top5 98.681641   BatchTime 0.385776   LR 0.001230
INFO - Training [33][   60/  196]   Loss 0.318034   Top1 88.841146   Top5 98.750000   BatchTime 0.376594   LR 0.001229
INFO - Training [33][   80/  196]   Loss 0.315021   Top1 88.984375   Top5 98.862305   BatchTime 0.372017   LR 0.001228
INFO - Training [33][  100/  196]   Loss 0.308484   Top1 89.191406   Top5 98.902344   BatchTime 0.366093   LR 0.001226
INFO - Training [33][  120/  196]   Loss 0.305882   Top1 89.342448   Top5 98.977865   BatchTime 0.362681   LR 0.001225
INFO - Training [33][  140/  196]   Loss 0.305510   Top1 89.363839   Top5 99.056920   BatchTime 0.360532   LR 0.001224
INFO - Training [33][  160/  196]   Loss 0.307197   Top1 89.299316   Top5 98.984375   BatchTime 0.355330   LR 0.001222
INFO - Training [33][  180/  196]   Loss 0.308016   Top1 89.264323   Top5 98.953993   BatchTime 0.346279   LR 0.001221
INFO - ==> Top1: 89.274    Top5: 98.938    Loss: 0.307
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 0.503425   Top1 85.292969   Top5 99.335938   BatchTime 0.133754
INFO - Validation [33][   40/   40]   Loss 0.496700   Top1 85.260000   Top5 99.360000   BatchTime 0.094557
INFO - ==> Top1: 85.260    Top5: 99.360    Loss: 0.497
INFO - ==> Sparsity : 0.662
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.3867)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0330)
features.2.conv.0 tensor(0.0448)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0378)
features.3.conv.6 tensor(0.0269)
features.4.conv.0 tensor(0.0355)
features.4.conv.3 tensor(0.0822)
features.4.conv.6 tensor(0.2936)
features.5.conv.0 tensor(0.0269)
features.5.conv.3 tensor(0.0648)
features.5.conv.6 tensor(0.3148)
features.6.conv.0 tensor(0.0189)
features.6.conv.3 tensor(0.0324)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0811)
features.7.conv.3 tensor(0.1024)
features.7.conv.6 tensor(0.3918)
features.8.conv.0 tensor(0.0832)
features.8.conv.3 tensor(0.1256)
features.8.conv.6 tensor(0.4082)
features.9.conv.0 tensor(0.1217)
features.9.conv.3 tensor(0.1369)
features.9.conv.6 tensor(0.4752)
features.10.conv.0 tensor(0.0426)
features.10.conv.3 tensor(0.0888)
features.10.conv.6 tensor(0.3262)
features.11.conv.0 tensor(0.5586)
features.11.conv.3 tensor(0.1590)
features.11.conv.6 tensor(0.7085)
features.12.conv.0 tensor(0.5344)
features.12.conv.3 tensor(0.1541)
features.12.conv.6 tensor(0.7458)
features.13.conv.0 tensor(0.1115)
features.13.conv.3 tensor(0.1532)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9512)
features.14.conv.3 tensor(0.1346)
features.14.conv.6 tensor(0.9763)
features.15.conv.0 tensor(0.9715)
features.15.conv.3 tensor(0.1257)
features.15.conv.6 tensor(0.9779)
features.16.conv.0 tensor(0.4276)
features.16.conv.3 tensor(0.1716)
features.16.conv.6 tensor(0.6698)
conv.0 tensor(0.8571)
tensor(1448696.) 2188896.0
INFO - Training [34][   20/  196]   Loss 0.300681   Top1 88.945312   Top5 98.652344   BatchTime 0.448231   LR 0.001218
INFO - Training [34][   40/  196]   Loss 0.307529   Top1 89.130859   Top5 98.681641   BatchTime 0.412767   LR 0.001216
INFO - Training [34][   60/  196]   Loss 0.306170   Top1 89.147135   Top5 98.743490   BatchTime 0.395331   LR 0.001215
INFO - Training [34][   80/  196]   Loss 0.310211   Top1 88.964844   Top5 98.881836   BatchTime 0.386178   LR 0.001213
INFO - Training [34][  100/  196]   Loss 0.304676   Top1 89.179688   Top5 98.941406   BatchTime 0.376239   LR 0.001211
INFO - Training [34][  120/  196]   Loss 0.298880   Top1 89.410807   Top5 98.994141   BatchTime 0.369095   LR 0.001209
INFO - Training [34][  140/  196]   Loss 0.300043   Top1 89.425223   Top5 99.012277   BatchTime 0.364048   LR 0.001208
INFO - Training [34][  160/  196]   Loss 0.303339   Top1 89.313965   Top5 99.008789   BatchTime 0.360233   LR 0.001206
INFO - Training [34][  180/  196]   Loss 0.307006   Top1 89.220920   Top5 98.951823   BatchTime 0.357336   LR 0.001204
INFO - ==> Top1: 89.212    Top5: 98.966    Loss: 0.307
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [34][   20/   40]   Loss 0.299665   Top1 90.527344   Top5 99.589844   BatchTime 0.211318
INFO - Validation [34][   40/   40]   Loss 0.284517   Top1 90.860000   Top5 99.680000   BatchTime 0.131529
INFO - ==> Top1: 90.860    Top5: 99.680    Loss: 0.285
INFO - ==> Sparsity : 0.663
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.3848)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0460)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0363)
features.3.conv.6 tensor(0.0284)
features.4.conv.0 tensor(0.0365)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.2941)
features.5.conv.0 tensor(0.0269)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.3162)
features.6.conv.0 tensor(0.0202)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0830)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.3929)
features.8.conv.0 tensor(0.0850)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.4098)
features.9.conv.0 tensor(0.1212)
features.9.conv.3 tensor(0.1369)
features.9.conv.6 tensor(0.4760)
features.10.conv.0 tensor(0.0474)
features.10.conv.3 tensor(0.0871)
features.10.conv.6 tensor(0.3274)
features.11.conv.0 tensor(0.5609)
features.11.conv.3 tensor(0.1576)
features.11.conv.6 tensor(0.7091)
features.12.conv.0 tensor(0.5335)
features.12.conv.3 tensor(0.1510)
features.12.conv.6 tensor(0.7462)
features.13.conv.0 tensor(0.1174)
features.13.conv.3 tensor(0.1524)
features.13.conv.6 tensor(0.1987)
features.14.conv.0 tensor(0.9519)
features.14.conv.3 tensor(0.1365)
features.14.conv.6 tensor(0.9765)
features.15.conv.0 tensor(0.9708)
features.15.conv.3 tensor(0.1259)
features.15.conv.6 tensor(0.9796)
features.16.conv.0 tensor(0.4283)
features.16.conv.3 tensor(0.1692)
features.16.conv.6 tensor(0.6711)
conv.0 tensor(0.8583)
tensor(1450809.) 2188896.0
INFO - Training [35][   20/  196]   Loss 0.295920   Top1 89.414062   Top5 98.671875   BatchTime 0.455203   LR 0.001201
INFO - Training [35][   40/  196]   Loss 0.305382   Top1 88.974609   Top5 98.740234   BatchTime 0.416101   LR 0.001199
INFO - Training [35][   60/  196]   Loss 0.302717   Top1 89.153646   Top5 98.763021   BatchTime 0.394695   LR 0.001197
INFO - Training [35][   80/  196]   Loss 0.307189   Top1 88.974609   Top5 98.901367   BatchTime 0.380457   LR 0.001195
INFO - Training [35][  100/  196]   Loss 0.306203   Top1 89.011719   Top5 98.890625   BatchTime 0.369841   LR 0.001192
INFO - Training [35][  120/  196]   Loss 0.301435   Top1 89.267578   Top5 98.987630   BatchTime 0.360773   LR 0.001190
INFO - Training [35][  140/  196]   Loss 0.301338   Top1 89.347098   Top5 99.048549   BatchTime 0.357706   LR 0.001188
INFO - Training [35][  160/  196]   Loss 0.302823   Top1 89.296875   Top5 99.040527   BatchTime 0.355903   LR 0.001186
INFO - Training [35][  180/  196]   Loss 0.304306   Top1 89.194878   Top5 99.027778   BatchTime 0.354426   LR 0.001184
INFO - ==> Top1: 89.178    Top5: 99.024    Loss: 0.304
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [35][   20/   40]   Loss 0.292917   Top1 91.035156   Top5 99.609375   BatchTime 0.139325
INFO - Validation [35][   40/   40]   Loss 0.277170   Top1 91.350000   Top5 99.700000   BatchTime 0.101112
INFO - ==> Top1: 91.350    Top5: 99.700    Loss: 0.277
INFO - ==> Sparsity : 0.663
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.3848)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0567)
features.1.conv.6 tensor(0.0326)
features.2.conv.0 tensor(0.0454)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0355)
features.3.conv.6 tensor(0.0310)
features.4.conv.0 tensor(0.0355)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2954)
features.5.conv.0 tensor(0.0251)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.3175)
features.6.conv.0 tensor(0.0208)
features.6.conv.3 tensor(0.0324)
features.6.conv.6 tensor(0.0460)
features.7.conv.0 tensor(0.0785)
features.7.conv.3 tensor(0.1047)
features.7.conv.6 tensor(0.3940)
features.8.conv.0 tensor(0.0860)
features.8.conv.3 tensor(0.1285)
features.8.conv.6 tensor(0.4115)
features.9.conv.0 tensor(0.1166)
features.9.conv.3 tensor(0.1392)
features.9.conv.6 tensor(0.4771)
features.10.conv.0 tensor(0.0478)
features.10.conv.3 tensor(0.0859)
features.10.conv.6 tensor(0.3287)
features.11.conv.0 tensor(0.5641)
features.11.conv.3 tensor(0.1537)
features.11.conv.6 tensor(0.7095)
features.12.conv.0 tensor(0.5360)
features.12.conv.3 tensor(0.1508)
features.12.conv.6 tensor(0.7465)
features.13.conv.0 tensor(0.1163)
features.13.conv.3 tensor(0.1499)
features.13.conv.6 tensor(0.1988)
features.14.conv.0 tensor(0.9508)
features.14.conv.3 tensor(0.1365)
features.14.conv.6 tensor(0.9761)
features.15.conv.0 tensor(0.9678)
features.15.conv.3 tensor(0.1259)
features.15.conv.6 tensor(0.9824)
features.16.conv.0 tensor(0.4280)
features.16.conv.3 tensor(0.1676)
features.16.conv.6 tensor(0.6721)
conv.0 tensor(0.8587)
tensor(1451235.) 2188896.0
INFO - Training [36][   20/  196]   Loss 0.320594   Top1 88.378906   Top5 98.613281   BatchTime 0.376112   LR 0.001180
INFO - Training [36][   40/  196]   Loss 0.329823   Top1 88.037109   Top5 98.652344   BatchTime 0.346870   LR 0.001177
INFO - Training [36][   60/  196]   Loss 0.320413   Top1 88.424479   Top5 98.750000   BatchTime 0.357023   LR 0.001175
INFO - Training [36][   80/  196]   Loss 0.315390   Top1 88.579102   Top5 98.867188   BatchTime 0.362395   LR 0.001173
INFO - Training [36][  100/  196]   Loss 0.306486   Top1 88.906250   Top5 98.937500   BatchTime 0.363892   LR 0.001170
INFO - Training [36][  120/  196]   Loss 0.305012   Top1 89.039714   Top5 98.990885   BatchTime 0.360537   LR 0.001168
INFO - Training [36][  140/  196]   Loss 0.301827   Top1 89.171317   Top5 99.042969   BatchTime 0.357651   LR 0.001165
INFO - Training [36][  160/  196]   Loss 0.304217   Top1 89.147949   Top5 99.040527   BatchTime 0.355301   LR 0.001163
INFO - Training [36][  180/  196]   Loss 0.305336   Top1 89.105903   Top5 98.971354   BatchTime 0.352677   LR 0.001160
INFO - ==> Top1: 89.084    Top5: 98.962    Loss: 0.306
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [36][   20/   40]   Loss 0.335306   Top1 90.351562   Top5 99.492188   BatchTime 0.128924
INFO - Validation [36][   40/   40]   Loss 0.323674   Top1 90.200000   Top5 99.650000   BatchTime 0.090322
INFO - ==> Top1: 90.200    Top5: 99.650    Loss: 0.324
INFO - ==> Sparsity : 0.664
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.3867)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0460)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0443)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0340)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0345)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2961)
features.5.conv.0 tensor(0.0252)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.3187)
features.6.conv.0 tensor(0.0212)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0477)
features.7.conv.0 tensor(0.0830)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.3951)
features.8.conv.0 tensor(0.0843)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.4130)
features.9.conv.0 tensor(0.1141)
features.9.conv.3 tensor(0.1435)
features.9.conv.6 tensor(0.4778)
features.10.conv.0 tensor(0.0466)
features.10.conv.3 tensor(0.0868)
features.10.conv.6 tensor(0.3298)
features.11.conv.0 tensor(0.5649)
features.11.conv.3 tensor(0.1590)
features.11.conv.6 tensor(0.7105)
features.12.conv.0 tensor(0.5352)
features.12.conv.3 tensor(0.1491)
features.12.conv.6 tensor(0.7464)
features.13.conv.0 tensor(0.1101)
features.13.conv.3 tensor(0.1507)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9506)
features.14.conv.3 tensor(0.1360)
features.14.conv.6 tensor(0.9765)
features.15.conv.0 tensor(0.9685)
features.15.conv.3 tensor(0.1275)
features.15.conv.6 tensor(0.9845)
features.16.conv.0 tensor(0.4312)
features.16.conv.3 tensor(0.1701)
features.16.conv.6 tensor(0.6726)
conv.0 tensor(0.8597)
tensor(1452599.) 2188896.0
INFO - Training [37][   20/  196]   Loss 0.312549   Top1 89.140625   Top5 98.496094   BatchTime 0.400449   LR 0.001155
INFO - Training [37][   40/  196]   Loss 0.310921   Top1 89.082031   Top5 98.681641   BatchTime 0.336349   LR 0.001153
INFO - Training [37][   60/  196]   Loss 0.311987   Top1 89.029948   Top5 98.736979   BatchTime 0.346384   LR 0.001150
INFO - Training [37][   80/  196]   Loss 0.306632   Top1 89.101562   Top5 98.857422   BatchTime 0.355588   LR 0.001147
INFO - Training [37][  100/  196]   Loss 0.301109   Top1 89.273438   Top5 98.949219   BatchTime 0.354142   LR 0.001144
INFO - Training [37][  120/  196]   Loss 0.296431   Top1 89.417318   Top5 99.039714   BatchTime 0.350760   LR 0.001142
INFO - Training [37][  140/  196]   Loss 0.294333   Top1 89.464286   Top5 99.065290   BatchTime 0.349008   LR 0.001139
INFO - Training [37][  160/  196]   Loss 0.296581   Top1 89.499512   Top5 99.025879   BatchTime 0.347369   LR 0.001136
INFO - Training [37][  180/  196]   Loss 0.297707   Top1 89.463976   Top5 99.010417   BatchTime 0.345518   LR 0.001133
INFO - ==> Top1: 89.538    Top5: 99.018    Loss: 0.296
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [37][   20/   40]   Loss 0.394469   Top1 88.125000   Top5 99.316406   BatchTime 0.132794
INFO - Validation [37][   40/   40]   Loss 0.377349   Top1 88.290000   Top5 99.490000   BatchTime 0.093674
INFO - ==> Top1: 88.290    Top5: 99.490    Loss: 0.377
INFO - ==> Sparsity : 0.664
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.3867)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0321)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0446)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0293)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0342)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2972)
features.5.conv.0 tensor(0.0236)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.3198)
features.6.conv.0 tensor(0.0225)
features.6.conv.3 tensor(0.0347)
features.6.conv.6 tensor(0.0484)
features.7.conv.0 tensor(0.0866)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.3959)
features.8.conv.0 tensor(0.0832)
features.8.conv.3 tensor(0.1256)
features.8.conv.6 tensor(0.4138)
features.9.conv.0 tensor(0.1191)
features.9.conv.3 tensor(0.1400)
features.9.conv.6 tensor(0.4784)
features.10.conv.0 tensor(0.0463)
features.10.conv.3 tensor(0.0862)
features.10.conv.6 tensor(0.3313)
features.11.conv.0 tensor(0.5660)
features.11.conv.3 tensor(0.1580)
features.11.conv.6 tensor(0.7111)
features.12.conv.0 tensor(0.5354)
features.12.conv.3 tensor(0.1505)
features.12.conv.6 tensor(0.7475)
features.13.conv.0 tensor(0.1114)
features.13.conv.3 tensor(0.1478)
features.13.conv.6 tensor(0.1980)
features.14.conv.0 tensor(0.9498)
features.14.conv.3 tensor(0.1359)
features.14.conv.6 tensor(0.9763)
features.15.conv.0 tensor(0.9679)
features.15.conv.3 tensor(0.1284)
features.15.conv.6 tensor(0.9856)
features.16.conv.0 tensor(0.4328)
features.16.conv.3 tensor(0.1691)
features.16.conv.6 tensor(0.6729)
conv.0 tensor(0.8599)
tensor(1453423.) 2188896.0
INFO - Training [38][   20/  196]   Loss 0.319344   Top1 88.886719   Top5 98.417969   BatchTime 0.404927   LR 0.001128
INFO - Training [38][   40/  196]   Loss 0.321090   Top1 88.798828   Top5 98.496094   BatchTime 0.355847   LR 0.001125
INFO - Training [38][   60/  196]   Loss 0.315409   Top1 88.932292   Top5 98.684896   BatchTime 0.334205   LR 0.001122
INFO - Training [38][   80/  196]   Loss 0.312021   Top1 88.935547   Top5 98.862305   BatchTime 0.345053   LR 0.001119
INFO - Training [38][  100/  196]   Loss 0.306683   Top1 89.128906   Top5 98.941406   BatchTime 0.349617   LR 0.001116
INFO - Training [38][  120/  196]   Loss 0.300225   Top1 89.404297   Top5 99.020182   BatchTime 0.349545   LR 0.001112
INFO - Training [38][  140/  196]   Loss 0.296726   Top1 89.547991   Top5 99.068080   BatchTime 0.347175   LR 0.001109
INFO - Training [38][  160/  196]   Loss 0.300500   Top1 89.494629   Top5 99.052734   BatchTime 0.345665   LR 0.001106
INFO - Training [38][  180/  196]   Loss 0.299508   Top1 89.507378   Top5 99.038628   BatchTime 0.344630   LR 0.001103
INFO - ==> Top1: 89.558    Top5: 99.040    Loss: 0.299
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [38][   20/   40]   Loss 0.301646   Top1 90.859375   Top5 99.726562   BatchTime 0.137137
INFO - Validation [38][   40/   40]   Loss 0.288634   Top1 90.860000   Top5 99.780000   BatchTime 0.097215
INFO - ==> Top1: 90.860    Top5: 99.780    Loss: 0.289
INFO - ==> Sparsity : 0.665
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.3867)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0541)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0460)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0370)
features.3.conv.6 tensor(0.0295)
features.4.conv.0 tensor(0.0366)
features.4.conv.3 tensor(0.0764)
features.4.conv.6 tensor(0.2975)
features.5.conv.0 tensor(0.0256)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.3205)
features.6.conv.0 tensor(0.0212)
features.6.conv.3 tensor(0.0347)
features.6.conv.6 tensor(0.0461)
features.7.conv.0 tensor(0.0835)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.3973)
features.8.conv.0 tensor(0.0848)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.4148)
features.9.conv.0 tensor(0.1182)
features.9.conv.3 tensor(0.1412)
features.9.conv.6 tensor(0.4795)
features.10.conv.0 tensor(0.0436)
features.10.conv.3 tensor(0.0828)
features.10.conv.6 tensor(0.3321)
features.11.conv.0 tensor(0.5643)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.7109)
features.12.conv.0 tensor(0.5334)
features.12.conv.3 tensor(0.1512)
features.12.conv.6 tensor(0.7486)
features.13.conv.0 tensor(0.1099)
features.13.conv.3 tensor(0.1487)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9503)
features.14.conv.3 tensor(0.1348)
features.14.conv.6 tensor(0.9771)
features.15.conv.0 tensor(0.9703)
features.15.conv.3 tensor(0.1275)
features.15.conv.6 tensor(0.9896)
features.16.conv.0 tensor(0.4343)
features.16.conv.3 tensor(0.1648)
features.16.conv.6 tensor(0.6734)
conv.0 tensor(0.8611)
tensor(1455267.) 2188896.0
INFO - Training [39][   20/  196]   Loss 0.320427   Top1 88.320312   Top5 98.632812   BatchTime 0.438129   LR 0.001097
INFO - Training [39][   40/  196]   Loss 0.318301   Top1 88.447266   Top5 98.828125   BatchTime 0.393132   LR 0.001094
INFO - Training [39][   60/  196]   Loss 0.313722   Top1 88.678385   Top5 98.893229   BatchTime 0.356952   LR 0.001090
INFO - Training [39][   80/  196]   Loss 0.311865   Top1 88.999023   Top5 98.979492   BatchTime 0.353013   LR 0.001087
INFO - Training [39][  100/  196]   Loss 0.302185   Top1 89.289062   Top5 99.000000   BatchTime 0.352108   LR 0.001084
INFO - Training [39][  120/  196]   Loss 0.294274   Top1 89.514974   Top5 99.069010   BatchTime 0.350272   LR 0.001080
INFO - Training [39][  140/  196]   Loss 0.293843   Top1 89.547991   Top5 99.104353   BatchTime 0.352654   LR 0.001077
INFO - Training [39][  160/  196]   Loss 0.294431   Top1 89.541016   Top5 99.084473   BatchTime 0.356607   LR 0.001073
INFO - Training [39][  180/  196]   Loss 0.296234   Top1 89.479167   Top5 99.049479   BatchTime 0.355870   LR 0.001070
INFO - ==> Top1: 89.534    Top5: 99.052    Loss: 0.295
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [39][   20/   40]   Loss 0.297567   Top1 91.171875   Top5 99.628906   BatchTime 0.133133
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.3887)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0365)
features.2.conv.0 tensor(0.0524)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0434)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0319)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.0770)
features.4.conv.6 tensor(0.2988)
features.5.conv.0 tensor(0.0249)
features.5.conv.3 tensor(0.0648)
features.5.conv.6 tensor(0.3206)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0457)
features.7.conv.0 tensor(0.0819)
features.7.conv.3 tensor(0.1065)
features.7.conv.6 tensor(0.3986)
features.8.conv.0 tensor(0.0854)
features.8.conv.3 tensor(0.1244)
features.8.conv.6 tensor(0.4159)
features.9.conv.0 tensor(0.1190)
features.9.conv.3 tensor(0.1395)
features.9.conv.6 tensor(0.4805)
features.10.conv.0 tensor(0.0426)
features.10.conv.3 tensor(0.0825)
features.10.conv.6 tensor(0.3332)
features.11.conv.0 tensor(0.5668)
features.11.conv.3 tensor(0.1562)
features.11.conv.6 tensor(0.7111)
features.12.conv.0 tensor(0.5351)
features.12.conv.3 tensor(0.1507)
INFO - Validation [39][   40/   40]   Loss 0.281534   Top1 91.370000   Top5 99.690000   BatchTime 0.092228
INFO - ==> Top1: 91.370    Top5: 99.690    Loss: 0.282
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
features.12.conv.6 tensor(0.7492)
features.13.conv.0 tensor(0.1144)
features.13.conv.3 tensor(0.1520)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9520)
features.14.conv.3 tensor(0.1370)
features.14.conv.6 tensor(0.9773)
features.15.conv.0 tensor(0.9724)
features.15.conv.3 tensor(0.1281)
features.15.conv.6 tensor(0.9895)
features.16.conv.0 tensor(0.4369)
features.16.conv.3 tensor(0.1684)
features.16.conv.6 tensor(0.6739)
conv.0 tensor(0.8620)
tensor(1457418.) 2188896.0
INFO - Training [40][   20/  196]   Loss 0.305302   Top1 89.101562   Top5 98.945312   BatchTime 0.407063   LR 0.001064
INFO - Training [40][   40/  196]   Loss 0.305828   Top1 89.150391   Top5 98.867188   BatchTime 0.375018   LR 0.001060
INFO - Training [40][   60/  196]   Loss 0.298688   Top1 89.518229   Top5 98.880208   BatchTime 0.361767   LR 0.001056
INFO - Training [40][   80/  196]   Loss 0.299792   Top1 89.536133   Top5 98.974609   BatchTime 0.346747   LR 0.001053
INFO - Training [40][  100/  196]   Loss 0.293056   Top1 89.777344   Top5 99.042969   BatchTime 0.345034   LR 0.001049
INFO - Training [40][  120/  196]   Loss 0.288140   Top1 89.977214   Top5 99.091797   BatchTime 0.349496   LR 0.001045
INFO - Training [40][  140/  196]   Loss 0.287690   Top1 90.008371   Top5 99.135045   BatchTime 0.350477   LR 0.001042
INFO - Training [40][  160/  196]   Loss 0.288367   Top1 89.931641   Top5 99.113770   BatchTime 0.350462   LR 0.001038
INFO - Training [40][  180/  196]   Loss 0.289822   Top1 89.913194   Top5 99.071181   BatchTime 0.348504   LR 0.001034
INFO - ==> Top1: 89.896    Top5: 99.076    Loss: 0.289
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.301564   Top1 90.761719   Top5 99.648438   BatchTime 0.135625
INFO - Validation [40][   40/   40]   Loss 0.289826   Top1 90.840000   Top5 99.740000   BatchTime 0.094777
INFO - ==> Top1: 90.840    Top5: 99.740    Loss: 0.290
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3887)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0386)
features.2.conv.0 tensor(0.0527)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0425)
features.3.conv.0 tensor(0.0211)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0256)
features.4.conv.0 tensor(0.0374)
features.4.conv.3 tensor(0.0781)
features.4.conv.6 tensor(0.3000)
features.5.conv.0 tensor(0.0233)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.3218)
features.6.conv.0 tensor(0.0213)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0478)
features.7.conv.0 tensor(0.0785)
features.7.conv.3 tensor(0.1045)
features.7.conv.6 tensor(0.3997)
features.8.conv.0 tensor(0.0838)
features.8.conv.3 tensor(0.1236)
features.8.conv.6 tensor(0.4167)
features.9.conv.0 tensor(0.1198)
features.9.conv.3 tensor(0.1392)
features.9.conv.6 tensor(0.4813)
features.10.conv.0 tensor(0.0417)
features.10.conv.3 tensor(0.0799)
features.10.conv.6 tensor(0.3343)
features.11.conv.0 tensor(0.5669)
features.11.conv.3 tensor(0.1586)
features.11.conv.6 tensor(0.7121)
features.12.conv.0 tensor(0.5369)
features.12.conv.3 tensor(0.1505)
features.12.conv.6 tensor(0.7485)
features.13.conv.0 tensor(0.1134)
features.13.conv.3 tensor(0.1518)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9528)
features.14.conv.3 tensor(0.1366)
features.14.conv.6 tensor(0.9773)
features.15.conv.0 tensor(0.9726)
features.15.conv.3 tensor(0.1297)
features.15.conv.6 tensor(0.9879)
features.16.conv.0 tensor(0.4377)
features.16.conv.3 tensor(0.1689)
features.16.conv.6 tensor(0.6746)
conv.0 tensor(0.8625)
tensor(1457937.) 2188896.0
INFO - Training [41][   20/  196]   Loss 0.302416   Top1 89.179688   Top5 98.417969   BatchTime 0.439911   LR 0.001027
INFO - Training [41][   40/  196]   Loss 0.296397   Top1 89.521484   Top5 98.632812   BatchTime 0.393657   LR 0.001023
INFO - Training [41][   60/  196]   Loss 0.294616   Top1 89.544271   Top5 98.730469   BatchTime 0.377263   LR 0.001020
INFO - Training [41][   80/  196]   Loss 0.294249   Top1 89.545898   Top5 98.842773   BatchTime 0.364986   LR 0.001016
INFO - Training [41][  100/  196]   Loss 0.289126   Top1 89.757812   Top5 98.882812   BatchTime 0.354071   LR 0.001012
INFO - Training [41][  120/  196]   Loss 0.283760   Top1 89.921875   Top5 98.964844   BatchTime 0.352658   LR 0.001008
INFO - Training [41][  140/  196]   Loss 0.280316   Top1 90.103237   Top5 99.037388   BatchTime 0.355725   LR 0.001004
INFO - Training [41][  160/  196]   Loss 0.283556   Top1 89.992676   Top5 99.033203   BatchTime 0.353444   LR 0.001000
INFO - Training [41][  180/  196]   Loss 0.285448   Top1 89.928385   Top5 99.023438   BatchTime 0.352252   LR 0.000996
INFO - ==> Top1: 89.920    Top5: 99.010    Loss: 0.286
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [41][   20/   40]   Loss 0.301799   Top1 91.113281   Top5 99.667969   BatchTime 0.134665
INFO - Validation [41][   40/   40]   Loss 0.284946   Top1 91.040000   Top5 99.740000   BatchTime 0.094174
INFO - ==> Top1: 91.040    Top5: 99.740    Loss: 0.285
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0648)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0463)
features.3.conv.0 tensor(0.0252)
features.3.conv.3 tensor(0.0378)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0378)
features.4.conv.3 tensor(0.0752)
features.4.conv.6 tensor(0.3009)
features.5.conv.0 tensor(0.0254)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.3218)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0461)
features.7.conv.0 tensor(0.0748)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.4006)
features.8.conv.0 tensor(0.0875)
features.8.conv.3 tensor(0.1230)
features.8.conv.6 tensor(0.4174)
features.9.conv.0 tensor(0.1192)
features.9.conv.3 tensor(0.1392)
features.9.conv.6 tensor(0.4827)
features.10.conv.0 tensor(0.0417)
features.10.conv.3 tensor(0.0859)
features.10.conv.6 tensor(0.3352)
features.11.conv.0 tensor(0.5704)
features.11.conv.3 tensor(0.1624)
features.11.conv.6 tensor(0.7127)
features.12.conv.0 tensor(0.5381)
features.12.conv.3 tensor(0.1518)
features.12.conv.6 tensor(0.7491)
features.13.conv.0 tensor(0.1150)
features.13.conv.3 tensor(0.1497)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9524)
features.14.conv.3 tensor(0.1376)
features.14.conv.6 tensor(0.9774)
features.15.conv.0 tensor(0.9732)
features.15.conv.3 tensor(0.1295)
features.15.conv.6 tensor(0.9853)
features.16.conv.0 tensor(0.4386)
features.16.conv.3 tensor(0.1721)
features.16.conv.6 tensor(0.6747)
conv.0 tensor(0.8626)
tensor(1458446.) 2188896.0
INFO - Training [42][   20/  196]   Loss 0.293079   Top1 89.492188   Top5 98.593750   BatchTime 0.412652   LR 0.000988
INFO - Training [42][   40/  196]   Loss 0.304360   Top1 89.296875   Top5 98.710938   BatchTime 0.386983   LR 0.000984
INFO - Training [42][   60/  196]   Loss 0.301895   Top1 89.368490   Top5 98.808594   BatchTime 0.375545   LR 0.000980
INFO - Training [42][   80/  196]   Loss 0.298596   Top1 89.477539   Top5 98.945312   BatchTime 0.374403   LR 0.000976
INFO - Training [42][  100/  196]   Loss 0.293714   Top1 89.628906   Top5 98.988281   BatchTime 0.365399   LR 0.000972
INFO - Training [42][  120/  196]   Loss 0.288998   Top1 89.817708   Top5 99.036458   BatchTime 0.345544   LR 0.000968
INFO - Training [42][  140/  196]   Loss 0.285431   Top1 89.907924   Top5 99.090402   BatchTime 0.345230   LR 0.000964
INFO - Training [42][  160/  196]   Loss 0.289803   Top1 89.716797   Top5 99.069824   BatchTime 0.345076   LR 0.000959
INFO - Training [42][  180/  196]   Loss 0.289211   Top1 89.806858   Top5 99.027778   BatchTime 0.344196   LR 0.000955
INFO - ==> Top1: 89.820    Top5: 99.028    Loss: 0.288
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [42][   20/   40]   Loss 0.303657   Top1 90.507812   Top5 99.628906   BatchTime 0.135178
INFO - Validation [42][   40/   40]   Loss 0.284760   Top1 91.030000   Top5 99.710000   BatchTime 0.094927
INFO - ==> Top1: 91.030    Top5: 99.710    Loss: 0.285
INFO - ==> Sparsity : 0.667
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5208)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0538)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0448)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0278)
features.4.conv.0 tensor(0.0356)
features.4.conv.3 tensor(0.0770)
features.4.conv.6 tensor(0.3018)
features.5.conv.0 tensor(0.0239)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.3223)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0471)
features.7.conv.0 tensor(0.0789)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.4014)
features.8.conv.0 tensor(0.0878)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.4183)
features.9.conv.0 tensor(0.1198)
features.9.conv.3 tensor(0.1374)
features.9.conv.6 tensor(0.4834)
features.10.conv.0 tensor(0.0472)
features.10.conv.3 tensor(0.0851)
features.10.conv.6 tensor(0.3358)
features.11.conv.0 tensor(0.5706)
features.11.conv.3 tensor(0.1601)
features.11.conv.6 tensor(0.7125)
features.12.conv.0 tensor(0.5401)
features.12.conv.3 tensor(0.1510)
features.12.conv.6 tensor(0.7490)
features.13.conv.0 tensor(0.1119)
features.13.conv.3 tensor(0.1474)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9526)
features.14.conv.3 tensor(0.1387)
features.14.conv.6 tensor(0.9769)
features.15.conv.0 tensor(0.9741)
features.15.conv.3 tensor(0.1288)
features.15.conv.6 tensor(0.9848)
features.16.conv.0 tensor(0.4389)
features.16.conv.3 tensor(0.1711)
features.16.conv.6 tensor(0.6752)
conv.0 tensor(0.8633)
tensor(1459132.) 2188896.0
INFO - Training [43][   20/  196]   Loss 0.283272   Top1 90.585938   Top5 98.652344   BatchTime 0.417271   LR 0.000947
INFO - Training [43][   40/  196]   Loss 0.294750   Top1 89.931641   Top5 98.632812   BatchTime 0.393525   LR 0.000943
INFO - Training [43][   60/  196]   Loss 0.286061   Top1 90.058594   Top5 98.795573   BatchTime 0.380281   LR 0.000939
INFO - Training [43][   80/  196]   Loss 0.287739   Top1 89.985352   Top5 98.911133   BatchTime 0.372007   LR 0.000934
INFO - Training [43][  100/  196]   Loss 0.282594   Top1 90.078125   Top5 98.984375   BatchTime 0.366160   LR 0.000930
INFO - Training [43][  120/  196]   Loss 0.276621   Top1 90.315755   Top5 99.078776   BatchTime 0.358118   LR 0.000926
INFO - Training [43][  140/  196]   Loss 0.276451   Top1 90.348772   Top5 99.140625   BatchTime 0.349518   LR 0.000921
INFO - Training [43][  160/  196]   Loss 0.281090   Top1 90.175781   Top5 99.123535   BatchTime 0.341253   LR 0.000917
INFO - Training [43][  180/  196]   Loss 0.282725   Top1 90.095486   Top5 99.097222   BatchTime 0.337760   LR 0.000912
INFO - ==> Top1: 90.084    Top5: 99.090    Loss: 0.282
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [43][   20/   40]   Loss 0.372582   Top1 88.984375   Top5 99.511719   BatchTime 0.127376
INFO - Validation [43][   40/   40]   Loss 0.362731   Top1 89.120000   Top5 99.600000   BatchTime 0.088826
INFO - ==> Top1: 89.120    Top5: 99.600    Loss: 0.363
INFO - ==> Sparsity : 0.667
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0544)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0446)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0271)
features.4.conv.0 tensor(0.0373)
features.4.conv.3 tensor(0.0764)
features.4.conv.6 tensor(0.3021)
features.5.conv.0 tensor(0.0236)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.3236)
features.6.conv.0 tensor(0.0241)
features.6.conv.3 tensor(0.0272)
features.6.conv.6 tensor(0.0486)
features.7.conv.0 tensor(0.0792)
features.7.conv.3 tensor(0.1042)
features.7.conv.6 tensor(0.4023)
features.8.conv.0 tensor(0.0883)
features.8.conv.3 tensor(0.1221)
features.8.conv.6 tensor(0.4195)
features.9.conv.0 tensor(0.1162)
features.9.conv.3 tensor(0.1395)
features.9.conv.6 tensor(0.4841)
features.10.conv.0 tensor(0.0475)
features.10.conv.3 tensor(0.0839)
features.10.conv.6 tensor(0.3368)
features.11.conv.0 tensor(0.5716)
features.11.conv.3 tensor(0.1593)
features.11.conv.6 tensor(0.7130)
features.12.conv.0 tensor(0.5419)
features.12.conv.3 tensor(0.1534)
features.12.conv.6 tensor(0.7494)
features.13.conv.0 tensor(0.1101)
features.13.conv.3 tensor(0.1481)
features.13.conv.6 tensor(0.1986)
features.14.conv.0 tensor(0.9527)
features.14.conv.3 tensor(0.1374)
features.14.conv.6 tensor(0.9772)
features.15.conv.0 tensor(0.9742)
features.15.conv.3 tensor(0.1267)
features.15.conv.6 tensor(0.9844)
features.16.conv.0 tensor(0.4404)
features.16.conv.3 tensor(0.1676)
features.16.conv.6 tensor(0.6762)
conv.0 tensor(0.8636)
tensor(1459961.) 2188896.0
INFO - Training [44][   20/  196]   Loss 0.294789   Top1 89.277344   Top5 98.906250   BatchTime 0.410886   LR 0.000904
INFO - Training [44][   40/  196]   Loss 0.287927   Top1 89.873047   Top5 98.886719   BatchTime 0.374513   LR 0.000900
INFO - Training [44][   60/  196]   Loss 0.287475   Top1 89.934896   Top5 98.899740   BatchTime 0.366964   LR 0.000895
INFO - Training [44][   80/  196]   Loss 0.287357   Top1 89.853516   Top5 98.955078   BatchTime 0.362649   LR 0.000891
INFO - Training [44][  100/  196]   Loss 0.282479   Top1 89.992188   Top5 98.988281   BatchTime 0.361435   LR 0.000886
INFO - Training [44][  120/  196]   Loss 0.275649   Top1 90.322266   Top5 99.052734   BatchTime 0.357026   LR 0.000882
INFO - Training [44][  140/  196]   Loss 0.276216   Top1 90.306920   Top5 99.082031   BatchTime 0.351414   LR 0.000877
INFO - Training [44][  160/  196]   Loss 0.279507   Top1 90.214844   Top5 99.086914   BatchTime 0.351073   LR 0.000873
INFO - Training [44][  180/  196]   Loss 0.280267   Top1 90.162760   Top5 99.053819   BatchTime 0.345470   LR 0.000868
INFO - ==> Top1: 90.194    Top5: 99.062    Loss: 0.280
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [44][   20/   40]   Loss 0.468831   Top1 86.210938   Top5 99.257812   BatchTime 0.128814
INFO - Validation [44][   40/   40]   Loss 0.458564   Top1 86.470000   Top5 99.370000   BatchTime 0.091100
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0544)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0440)
features.3.conv.0 tensor(0.0237)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0392)
features.4.conv.3 tensor(0.0775)
features.4.conv.6 tensor(0.3029)
features.5.conv.0 tensor(0.0246)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.3241)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0284)
features.6.conv.6 tensor(0.0461)
features.7.conv.0 tensor(0.0785)
features.7.conv.3 tensor(0.1036)
features.7.conv.6 tensor(0.4027)
features.8.conv.0 tensor(0.0885)
features.8.conv.3 tensor(0.1264)
features.8.conv.6 tensor(0.4207)
features.9.conv.0 tensor(0.1212)
features.9.conv.3 tensor(0.1374)
features.9.conv.6 tensor(0.4849)
features.10.conv.0 tensor(0.0467)
features.10.conv.3 tensor(0.0848)
features.10.conv.6 tensor(0.3374)
features.11.conv.0 tensor(0.5718)
features.11.conv.3 tensor(0.1626)
features.11.conv.6 tensor(0.7134)
features.12.conv.0 tensor(0.5440)
features.12.conv.3 tensor(0.1543)
features.12.conv.6 tensor(0.7497)
features.13.conv.0 tensor(0.1050)
features.13.conv.3 tensor(0.1476)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9538)
features.14.conv.3 tensor(0.1372)
features.14.conv.6 tensor(0.9775)
features.15.conv.0 tensor(0.9720)
features.15.conv.3 tensor(0.1294)
features.15.conv.6 tensor(0.9847)
features.16.conv.0 tensor(0.4408)
features.16.conv.3 tensor(0.1674)
features.16.conv.6 tensor(0.6764)
conv.0 tensor(0.8637)
tensor(1460136.) 2188896.0
INFO - ==> Top1: 86.470    Top5: 99.370    Loss: 0.459
INFO - ==> Sparsity : 0.667
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [45][   20/  196]   Loss 0.291842   Top1 89.628906   Top5 98.671875   BatchTime 0.445779   LR 0.000860
INFO - Training [45][   40/  196]   Loss 0.282425   Top1 89.873047   Top5 98.837891   BatchTime 0.413381   LR 0.000855
INFO - Training [45][   60/  196]   Loss 0.274958   Top1 90.260417   Top5 98.938802   BatchTime 0.405684   LR 0.000850
INFO - Training [45][   80/  196]   Loss 0.277062   Top1 90.107422   Top5 99.038086   BatchTime 0.392922   LR 0.000846
INFO - Training [45][  100/  196]   Loss 0.274664   Top1 90.179688   Top5 99.050781   BatchTime 0.388922   LR 0.000841
INFO - Training [45][  120/  196]   Loss 0.273334   Top1 90.328776   Top5 99.098307   BatchTime 0.380067   LR 0.000836
INFO - Training [45][  140/  196]   Loss 0.272560   Top1 90.368304   Top5 99.140625   BatchTime 0.373809   LR 0.000832
INFO - Training [45][  160/  196]   Loss 0.275929   Top1 90.270996   Top5 99.130859   BatchTime 0.369333   LR 0.000827
INFO - Training [45][  180/  196]   Loss 0.277509   Top1 90.221354   Top5 99.095052   BatchTime 0.363692   LR 0.000822
INFO - ==> Top1: 90.272    Top5: 99.092    Loss: 0.277
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.316139   Top1 90.214844   Top5 99.550781   BatchTime 0.129831
INFO - Validation [45][   40/   40]   Loss 0.300781   Top1 90.780000   Top5 99.670000   BatchTime 0.090374
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0544)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0553)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0443)
features.3.conv.0 tensor(0.0205)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0394)
features.4.conv.3 tensor(0.0741)
features.4.conv.6 tensor(0.3029)
features.5.conv.0 tensor(0.0265)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.3244)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0471)
features.7.conv.0 tensor(0.0807)
features.7.conv.3 tensor(0.0987)
features.7.conv.6 tensor(0.4036)
features.8.conv.0 tensor(0.0901)
features.8.conv.3 tensor(0.1250)
features.8.conv.6 tensor(0.4215)
features.9.conv.0 tensor(0.1246)
features.9.conv.3 tensor(0.1383)
features.9.conv.6 tensor(0.4856)
features.10.conv.0 tensor(0.0485)
features.10.conv.3 tensor(0.0819)
features.10.conv.6 tensor(0.3384)
features.11.conv.0 tensor(0.5718)
features.11.conv.3 tensor(0.1591)
features.11.conv.6 tensor(0.7136)
features.12.conv.0 tensor(0.5456)
features.12.conv.3 tensor(0.1535)
features.12.conv.6 tensor(0.7504)
features.13.conv.0 tensor(0.1069)
features.13.conv.3 tensor(0.1478)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9551)
features.14.conv.3 tensor(0.1369)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9722)
features.15.conv.3 tensor(0.1275)
features.15.conv.6 tensor(0.9851)
features.16.conv.0 tensor(0.4417)
features.16.conv.3 tensor(0.1675)
features.16.conv.6 tensor(0.6766)
conv.0 tensor(0.8642)
tensor(1461428.) 2188896.0
INFO - ==> Top1: 90.780    Top5: 99.670    Loss: 0.301
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [46][   20/  196]   Loss 0.313324   Top1 89.082031   Top5 98.320312   BatchTime 0.454743   LR 0.000814
INFO - Training [46][   40/  196]   Loss 0.296285   Top1 89.472656   Top5 98.671875   BatchTime 0.412305   LR 0.000809
INFO - Training [46][   60/  196]   Loss 0.291817   Top1 89.687500   Top5 98.795573   BatchTime 0.390180   LR 0.000804
INFO - Training [46][   80/  196]   Loss 0.287401   Top1 89.873047   Top5 98.896484   BatchTime 0.378981   LR 0.000799
INFO - Training [46][  100/  196]   Loss 0.284007   Top1 89.953125   Top5 98.953125   BatchTime 0.374425   LR 0.000794
INFO - Training [46][  120/  196]   Loss 0.276908   Top1 90.234375   Top5 99.046224   BatchTime 0.374855   LR 0.000789
INFO - Training [46][  140/  196]   Loss 0.274305   Top1 90.365513   Top5 99.121094   BatchTime 0.369404   LR 0.000785
INFO - Training [46][  160/  196]   Loss 0.276241   Top1 90.288086   Top5 99.140625   BatchTime 0.366842   LR 0.000780
INFO - Training [46][  180/  196]   Loss 0.278320   Top1 90.203993   Top5 99.116753   BatchTime 0.367234   LR 0.000775
INFO - ==> Top1: 90.278    Top5: 99.108    Loss: 0.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [46][   20/   40]   Loss 0.312770   Top1 90.507812   Top5 99.589844   BatchTime 0.135762
INFO - Validation [46][   40/   40]   Loss 0.290271   Top1 90.970000   Top5 99.690000   BatchTime 0.094533
INFO - ==> Top1: 90.970    Top5: 99.690    Loss: 0.290
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0558)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0417)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0254)
features.4.conv.0 tensor(0.0387)
features.4.conv.3 tensor(0.0787)
features.4.conv.6 tensor(0.3035)
features.5.conv.0 tensor(0.0238)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.3249)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0289)
features.6.conv.6 tensor(0.0468)
features.7.conv.0 tensor(0.0804)
features.7.conv.3 tensor(0.0995)
features.7.conv.6 tensor(0.4041)
features.8.conv.0 tensor(0.0888)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.4221)
features.9.conv.0 tensor(0.1217)
features.9.conv.3 tensor(0.1340)
features.9.conv.6 tensor(0.4864)
features.10.conv.0 tensor(0.0497)
features.10.conv.3 tensor(0.0819)
features.10.conv.6 tensor(0.3392)
features.11.conv.0 tensor(0.5715)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.7136)
features.12.conv.0 tensor(0.5457)
features.12.conv.3 tensor(0.1547)
features.12.conv.6 tensor(0.7500)
features.13.conv.0 tensor(0.1062)
features.13.conv.3 tensor(0.1458)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9560)
features.14.conv.3 tensor(0.1377)
features.14.conv.6 tensor(0.9777)
features.15.conv.0 tensor(0.9712)
features.15.conv.3 tensor(0.1278)
features.15.conv.6 tensor(0.9837)
features.16.conv.0 tensor(0.4428)
features.16.conv.3 tensor(0.1685)
features.16.conv.6 tensor(0.6771)
conv.0 tensor(0.8646)
tensor(1461509.) 2188896.0
INFO - Training [47][   20/  196]   Loss 0.298732   Top1 89.667969   Top5 98.808594   BatchTime 0.432688   LR 0.000766
INFO - Training [47][   40/  196]   Loss 0.291246   Top1 89.794922   Top5 98.906250   BatchTime 0.403406   LR 0.000761
INFO - Training [47][   60/  196]   Loss 0.288607   Top1 89.850260   Top5 98.938802   BatchTime 0.386811   LR 0.000756
INFO - Training [47][   80/  196]   Loss 0.284072   Top1 90.014648   Top5 99.086914   BatchTime 0.386562   LR 0.000752
INFO - Training [47][  100/  196]   Loss 0.276168   Top1 90.324219   Top5 99.132812   BatchTime 0.385515   LR 0.000747
INFO - Training [47][  120/  196]   Loss 0.267244   Top1 90.634766   Top5 99.182943   BatchTime 0.382400   LR 0.000742
INFO - Training [47][  140/  196]   Loss 0.267409   Top1 90.594308   Top5 99.238281   BatchTime 0.376756   LR 0.000737
INFO - Training [47][  160/  196]   Loss 0.268441   Top1 90.568848   Top5 99.204102   BatchTime 0.371159   LR 0.000732
INFO - Training [47][  180/  196]   Loss 0.269081   Top1 90.538194   Top5 99.166667   BatchTime 0.369568   LR 0.000727
INFO - ==> Top1: 90.538    Top5: 99.170    Loss: 0.268
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [47][   20/   40]   Loss 0.308024   Top1 90.917969   Top5 99.667969   BatchTime 0.170647
INFO - Validation [47][   40/   40]   Loss 0.291090   Top1 91.350000   Top5 99.720000   BatchTime 0.110618
INFO - ==> Top1: 91.350    Top5: 99.720    Loss: 0.291
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 91.610   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0330)
features.2.conv.0 tensor(0.0570)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0425)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0379)
features.4.conv.3 tensor(0.0752)
features.4.conv.6 tensor(0.3035)
features.5.conv.0 tensor(0.0283)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.3250)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0255)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0790)
features.7.conv.3 tensor(0.1001)
features.7.conv.6 tensor(0.4047)
features.8.conv.0 tensor(0.0906)
features.8.conv.3 tensor(0.1236)
features.8.conv.6 tensor(0.4226)
features.9.conv.0 tensor(0.1231)
features.9.conv.3 tensor(0.1372)
features.9.conv.6 tensor(0.4863)
features.10.conv.0 tensor(0.0488)
features.10.conv.3 tensor(0.0833)
features.10.conv.6 tensor(0.3397)
features.11.conv.0 tensor(0.5716)
features.11.conv.3 tensor(0.1580)
features.11.conv.6 tensor(0.7139)
features.12.conv.0 tensor(0.5457)
features.12.conv.3 tensor(0.1528)
features.12.conv.6 tensor(0.7504)
features.13.conv.0 tensor(0.1080)
features.13.conv.3 tensor(0.1491)
features.13.conv.6 tensor(0.1987)
features.14.conv.0 tensor(0.9565)
features.14.conv.3 tensor(0.1352)
features.14.conv.6 tensor(0.9780)
features.15.conv.0 tensor(0.9731)
features.15.conv.3 tensor(0.1281)
features.15.conv.6 tensor(0.9842)
features.16.conv.0 tensor(0.4432)
features.16.conv.3 tensor(0.1682)
features.16.conv.6 tensor(0.6773)
conv.0 tensor(0.8647)
tensor(1462381.) 2188896.0
INFO - Training [48][   20/  196]   Loss 0.280988   Top1 89.785156   Top5 98.496094   BatchTime 0.413006   LR 0.000718
INFO - Training [48][   40/  196]   Loss 0.288498   Top1 89.707031   Top5 98.652344   BatchTime 0.378264   LR 0.000713
INFO - Training [48][   60/  196]   Loss 0.290780   Top1 89.537760   Top5 98.802083   BatchTime 0.365486   LR 0.000708
INFO - Training [48][   80/  196]   Loss 0.289134   Top1 89.526367   Top5 98.994141   BatchTime 0.363204   LR 0.000703
INFO - Training [48][  100/  196]   Loss 0.285105   Top1 89.621094   Top5 99.027344   BatchTime 0.359205   LR 0.000698
INFO - Training [48][  120/  196]   Loss 0.277614   Top1 90.006510   Top5 99.078776   BatchTime 0.354730   LR 0.000693
INFO - Training [48][  140/  196]   Loss 0.276956   Top1 90.066964   Top5 99.123884   BatchTime 0.352983   LR 0.000688
INFO - Training [48][  160/  196]   Loss 0.277745   Top1 90.087891   Top5 99.123535   BatchTime 0.350496   LR 0.000683
INFO - Training [48][  180/  196]   Loss 0.275575   Top1 90.182292   Top5 99.101562   BatchTime 0.348616   LR 0.000678
INFO - ==> Top1: 90.272    Top5: 99.100    Loss: 0.274
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [48][   20/   40]   Loss 0.295197   Top1 91.523438   Top5 99.667969   BatchTime 0.149829
INFO - Validation [48][   40/   40]   Loss 0.277255   Top1 91.620000   Top5 99.720000   BatchTime 0.139850
INFO - ==> Top1: 91.620    Top5: 99.720    Loss: 0.277
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 91.620   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0532)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0579)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0159)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0278)
features.4.conv.0 tensor(0.0407)
features.4.conv.3 tensor(0.0770)
features.4.conv.6 tensor(0.3040)
features.5.conv.0 tensor(0.0252)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.3258)
features.6.conv.0 tensor(0.0278)
features.6.conv.3 tensor(0.0266)
features.6.conv.6 tensor(0.0470)
features.7.conv.0 tensor(0.0785)
features.7.conv.3 tensor(0.0992)
features.7.conv.6 tensor(0.4049)
features.8.conv.0 tensor(0.0896)
features.8.conv.3 tensor(0.1221)
features.8.conv.6 tensor(0.4231)
features.9.conv.0 tensor(0.1202)
features.9.conv.3 tensor(0.1345)
features.9.conv.6 tensor(0.4868)
features.10.conv.0 tensor(0.0487)
features.10.conv.3 tensor(0.0816)
features.10.conv.6 tensor(0.3402)
features.11.conv.0 tensor(0.5725)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.7141)
features.12.conv.0 tensor(0.5455)
features.12.conv.3 tensor(0.1507)
features.12.conv.6 tensor(0.7503)
features.13.conv.0 tensor(0.1088)
features.13.conv.3 tensor(0.1466)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9585)
features.14.conv.3 tensor(0.1355)
features.14.conv.6 tensor(0.9773)
features.15.conv.0 tensor(0.9727)
features.15.conv.3 tensor(0.1280)
features.15.conv.6 tensor(0.9838)
features.16.conv.0 tensor(0.4434)
features.16.conv.3 tensor(0.1683)
features.16.conv.6 tensor(0.6773)
conv.0 tensor(0.8650)
tensor(1462661.) 2188896.0
INFO - Training [49][   20/  196]   Loss 0.283134   Top1 90.039062   Top5 98.691406   BatchTime 0.442500   LR 0.000669
INFO - Training [49][   40/  196]   Loss 0.284586   Top1 90.009766   Top5 98.964844   BatchTime 0.385271   LR 0.000664
INFO - Training [49][   60/  196]   Loss 0.275245   Top1 90.292969   Top5 99.029948   BatchTime 0.372969   LR 0.000659
INFO - Training [49][   80/  196]   Loss 0.274395   Top1 90.239258   Top5 99.116211   BatchTime 0.368171   LR 0.000654
INFO - Training [49][  100/  196]   Loss 0.268331   Top1 90.464844   Top5 99.148438   BatchTime 0.361096   LR 0.000649
INFO - Training [49][  120/  196]   Loss 0.263562   Top1 90.696615   Top5 99.199219   BatchTime 0.357939   LR 0.000644
INFO - Training [49][  140/  196]   Loss 0.260407   Top1 90.778460   Top5 99.243862   BatchTime 0.354396   LR 0.000639
INFO - Training [49][  160/  196]   Loss 0.263973   Top1 90.646973   Top5 99.206543   BatchTime 0.353910   LR 0.000634
INFO - Training [49][  180/  196]   Loss 0.264901   Top1 90.625000   Top5 99.179688   BatchTime 0.352563   LR 0.000629
INFO - ==> Top1: 90.642    Top5: 99.192    Loss: 0.265
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [49][   20/   40]   Loss 0.299158   Top1 90.800781   Top5 99.648438   BatchTime 0.136333
INFO - Validation [49][   40/   40]   Loss 0.290657   Top1 90.980000   Top5 99.750000   BatchTime 0.101593
INFO - ==> Top1: 90.980    Top5: 99.750    Loss: 0.291
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 91.620   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0584)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0443)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0271)
features.4.conv.0 tensor(0.0399)
features.4.conv.3 tensor(0.0735)
features.4.conv.6 tensor(0.3047)
features.5.conv.0 tensor(0.0244)
features.5.conv.3 tensor(0.0642)
features.5.conv.6 tensor(0.3257)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0231)
features.6.conv.6 tensor(0.0466)
features.7.conv.0 tensor(0.0807)
features.7.conv.3 tensor(0.1045)
features.7.conv.6 tensor(0.4053)
features.8.conv.0 tensor(0.0884)
features.8.conv.3 tensor(0.1218)
features.8.conv.6 tensor(0.4234)
features.9.conv.0 tensor(0.1231)
features.9.conv.3 tensor(0.1317)
features.9.conv.6 tensor(0.4868)
features.10.conv.0 tensor(0.0495)
features.10.conv.3 tensor(0.0842)
features.10.conv.6 tensor(0.3406)
features.11.conv.0 tensor(0.5726)
features.11.conv.3 tensor(0.1564)
features.11.conv.6 tensor(0.7140)
features.12.conv.0 tensor(0.5470)
features.12.conv.3 tensor(0.1526)
features.12.conv.6 tensor(0.7508)
features.13.conv.0 tensor(0.1121)
features.13.conv.3 tensor(0.1470)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9587)
features.14.conv.3 tensor(0.1362)
features.14.conv.6 tensor(0.9774)
features.15.conv.0 tensor(0.9742)
features.15.conv.3 tensor(0.1289)
features.15.conv.6 tensor(0.9845)
features.16.conv.0 tensor(0.4445)
features.16.conv.3 tensor(0.1689)
features.16.conv.6 tensor(0.6777)
conv.0 tensor(0.8650)
tensor(1463733.) 2188896.0
INFO - Training [50][   20/  196]   Loss 0.288788   Top1 89.570312   Top5 98.769531   BatchTime 0.421924   LR 0.000620
INFO - Training [50][   40/  196]   Loss 0.281977   Top1 89.941406   Top5 98.916016   BatchTime 0.386156   LR 0.000615
INFO - Training [50][   60/  196]   Loss 0.276210   Top1 90.162760   Top5 98.977865   BatchTime 0.378507   LR 0.000610
INFO - Training [50][   80/  196]   Loss 0.272236   Top1 90.263672   Top5 99.082031   BatchTime 0.369794   LR 0.000605
INFO - Training [50][  100/  196]   Loss 0.265194   Top1 90.542969   Top5 99.164062   BatchTime 0.363740   LR 0.000600
INFO - Training [50][  120/  196]   Loss 0.258228   Top1 90.843099   Top5 99.208984   BatchTime 0.363534   LR 0.000595
INFO - Training [50][  140/  196]   Loss 0.253634   Top1 91.071429   Top5 99.271763   BatchTime 0.360175   LR 0.000590
INFO - Training [50][  160/  196]   Loss 0.256338   Top1 90.957031   Top5 99.257812   BatchTime 0.356903   LR 0.000585
INFO - Training [50][  180/  196]   Loss 0.259069   Top1 90.950521   Top5 99.194878   BatchTime 0.354607   LR 0.000580
INFO - ==> Top1: 90.978    Top5: 99.168    Loss: 0.258
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [50][   20/   40]   Loss 0.299255   Top1 90.937500   Top5 99.648438   BatchTime 0.133169
INFO - Validation [50][   40/   40]   Loss 0.279621   Top1 91.380000   Top5 99.720000   BatchTime 0.098982
INFO - ==> Top1: 91.380    Top5: 99.720    Loss: 0.280
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 91.620   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0648)
features.1.conv.6 tensor(0.0278)
features.2.conv.0 tensor(0.0576)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0448)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0267)
features.4.conv.0 tensor(0.0378)
features.4.conv.3 tensor(0.0781)
features.4.conv.6 tensor(0.3055)
features.5.conv.0 tensor(0.0231)
features.5.conv.3 tensor(0.0631)
features.5.conv.6 tensor(0.3257)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0237)
features.6.conv.6 tensor(0.0461)
features.7.conv.0 tensor(0.0798)
features.7.conv.3 tensor(0.1030)
features.7.conv.6 tensor(0.4054)
features.8.conv.0 tensor(0.0860)
features.8.conv.3 tensor(0.1195)
features.8.conv.6 tensor(0.4237)
features.9.conv.0 tensor(0.1230)
features.9.conv.3 tensor(0.1302)
features.9.conv.6 tensor(0.4869)
features.10.conv.0 tensor(0.0489)
features.10.conv.3 tensor(0.0825)
features.10.conv.6 tensor(0.3410)
features.11.conv.0 tensor(0.5733)
features.11.conv.3 tensor(0.1547)
features.11.conv.6 tensor(0.7142)
features.12.conv.0 tensor(0.5472)
features.12.conv.3 tensor(0.1524)
features.12.conv.6 tensor(0.7507)
features.13.conv.0 tensor(0.1137)
features.13.conv.3 tensor(0.1480)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9594)
features.14.conv.3 tensor(0.1372)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9733)
features.15.conv.3 tensor(0.1282)
features.15.conv.6 tensor(0.9849)
features.16.conv.0 tensor(0.4453)
features.16.conv.3 tensor(0.1693)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8649)
tensor(1463917.) 2188896.0
INFO - Training [51][   20/  196]   Loss 0.271247   Top1 90.019531   Top5 98.847656   BatchTime 0.371925   LR 0.000571
INFO - Training [51][   40/  196]   Loss 0.268008   Top1 90.341797   Top5 98.886719   BatchTime 0.358646   LR 0.000566
INFO - Training [51][   60/  196]   Loss 0.265841   Top1 90.475260   Top5 99.010417   BatchTime 0.355118   LR 0.000561
INFO - Training [51][   80/  196]   Loss 0.263089   Top1 90.629883   Top5 99.067383   BatchTime 0.353408   LR 0.000556
INFO - Training [51][  100/  196]   Loss 0.258391   Top1 90.789062   Top5 99.070312   BatchTime 0.357521   LR 0.000551
INFO - Training [51][  120/  196]   Loss 0.251851   Top1 90.989583   Top5 99.140625   BatchTime 0.354278   LR 0.000546
INFO - Training [51][  140/  196]   Loss 0.250384   Top1 91.102121   Top5 99.171317   BatchTime 0.352110   LR 0.000541
INFO - Training [51][  160/  196]   Loss 0.255457   Top1 90.917969   Top5 99.172363   BatchTime 0.350786   LR 0.000536
INFO - Training [51][  180/  196]   Loss 0.256981   Top1 90.811632   Top5 99.142795   BatchTime 0.352771   LR 0.000531
INFO - ==> Top1: 90.866    Top5: 99.158    Loss: 0.256
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [51][   20/   40]   Loss 0.297916   Top1 91.308594   Top5 99.707031   BatchTime 0.134458
INFO - Validation [51][   40/   40]   Loss 0.286027   Top1 91.450000   Top5 99.740000   BatchTime 0.095385
INFO - ==> Top1: 91.450    Top5: 99.740    Loss: 0.286
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 91.620   Top5: 99.720]
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0278)
features.2.conv.0 tensor(0.0599)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0446)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0256)
features.4.conv.0 tensor(0.0397)
features.4.conv.3 tensor(0.0781)
features.4.conv.6 tensor(0.3053)
features.5.conv.0 tensor(0.0236)
features.5.conv.3 tensor(0.0637)
features.5.conv.6 tensor(0.3258)
features.6.conv.0 tensor(0.0259)
features.6.conv.3 tensor(0.0226)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0773)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.4056)
features.8.conv.0 tensor(0.0847)
features.8.conv.3 tensor(0.1230)
features.8.conv.6 tensor(0.4241)
features.9.conv.0 tensor(0.1237)
features.9.conv.3 tensor(0.1282)
features.9.conv.6 tensor(0.4873)
features.10.conv.0 tensor(0.0481)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.3412)
features.11.conv.0 tensor(0.5739)
features.11.conv.3 tensor(0.1551)
features.11.conv.6 tensor(0.7142)
features.12.conv.0 tensor(0.5471)
features.12.conv.3 tensor(0.1514)
features.12.conv.6 tensor(0.7510)
features.13.conv.0 tensor(0.1152)
features.13.conv.3 tensor(0.1458)
features.13.conv.6 tensor(0.1986)
features.14.conv.0 tensor(0.9591)
features.14.conv.3 tensor(0.1361)
features.14.conv.6 tensor(0.9776)
features.15.conv.0 tensor(0.9705)
features.15.conv.3 tensor(0.1286)
features.15.conv.6 tensor(0.9840)
features.16.conv.0 tensor(0.4447)
features.16.conv.3 tensor(0.1691)
features.16.conv.6 tensor(0.6774)
conv.0 tensor(0.8650)
tensor(1463252.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [52][   20/  196]   Loss 0.278738   Top1 90.468750   Top5 98.554688   BatchTime 0.412828   LR 0.000523
INFO - Training [52][   40/  196]   Loss 0.272983   Top1 90.546875   Top5 98.720703   BatchTime 0.372162   LR 0.000518
INFO - Training [52][   60/  196]   Loss 0.267187   Top1 90.787760   Top5 98.795573   BatchTime 0.335384   LR 0.000513
INFO - Training [52][   80/  196]   Loss 0.271457   Top1 90.566406   Top5 98.940430   BatchTime 0.324434   LR 0.000508
INFO - Training [52][  100/  196]   Loss 0.264920   Top1 90.804688   Top5 99.019531   BatchTime 0.326734   LR 0.000503
INFO - Training [52][  120/  196]   Loss 0.260126   Top1 90.950521   Top5 99.091797   BatchTime 0.330303   LR 0.000498
INFO - Training [52][  140/  196]   Loss 0.257722   Top1 91.074219   Top5 99.148996   BatchTime 0.332296   LR 0.000493
INFO - Training [52][  160/  196]   Loss 0.258528   Top1 91.047363   Top5 99.128418   BatchTime 0.333670   LR 0.000488
INFO - Training [52][  180/  196]   Loss 0.257980   Top1 91.063368   Top5 99.108073   BatchTime 0.335000   LR 0.000483
INFO - ==> Top1: 91.108    Top5: 99.094    Loss: 0.258
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [52][   20/   40]   Loss 0.297878   Top1 91.250000   Top5 99.648438   BatchTime 0.129185
INFO - Validation [52][   40/   40]   Loss 0.282687   Top1 91.510000   Top5 99.730000   BatchTime 0.090036
INFO - ==> Top1: 91.510    Top5: 99.730    Loss: 0.283
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 91.620   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0291)
features.2.conv.0 tensor(0.0599)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0443)
features.3.conv.0 tensor(0.0269)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0284)
features.4.conv.0 tensor(0.0389)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.3055)
features.5.conv.0 tensor(0.0246)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.3260)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0231)
features.6.conv.6 tensor(0.0453)
features.7.conv.0 tensor(0.0763)
features.7.conv.3 tensor(0.1036)
features.7.conv.6 tensor(0.4059)
features.8.conv.0 tensor(0.0861)
features.8.conv.3 tensor(0.1244)
features.8.conv.6 tensor(0.4240)
features.9.conv.0 tensor(0.1230)
features.9.conv.3 tensor(0.1296)
features.9.conv.6 tensor(0.4875)
features.10.conv.0 tensor(0.0491)
features.10.conv.3 tensor(0.0816)
features.10.conv.6 tensor(0.3415)
features.11.conv.0 tensor(0.5737)
features.11.conv.3 tensor(0.1561)
features.11.conv.6 tensor(0.7137)
features.12.conv.0 tensor(0.5470)
features.12.conv.3 tensor(0.1516)
features.12.conv.6 tensor(0.7512)
features.13.conv.0 tensor(0.1163)
features.13.conv.3 tensor(0.1466)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9606)
features.14.conv.3 tensor(0.1362)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9713)
features.15.conv.3 tensor(0.1282)
features.15.conv.6 tensor(0.9844)
features.16.conv.0 tensor(0.4444)
features.16.conv.3 tensor(0.1674)
features.16.conv.6 tensor(0.6772)
conv.0 tensor(0.8648)
tensor(1463611.) 2188896.0
INFO - Training [53][   20/  196]   Loss 0.274171   Top1 90.292969   Top5 98.847656   BatchTime 0.461325   LR 0.000474
INFO - Training [53][   40/  196]   Loss 0.269652   Top1 90.615234   Top5 98.876953   BatchTime 0.410246   LR 0.000470
INFO - Training [53][   60/  196]   Loss 0.266282   Top1 90.768229   Top5 98.958333   BatchTime 0.392282   LR 0.000465
INFO - Training [53][   80/  196]   Loss 0.263917   Top1 90.844727   Top5 99.096680   BatchTime 0.372826   LR 0.000460
INFO - Training [53][  100/  196]   Loss 0.258966   Top1 91.031250   Top5 99.089844   BatchTime 0.348887   LR 0.000455
INFO - Training [53][  120/  196]   Loss 0.252846   Top1 91.171875   Top5 99.163411   BatchTime 0.349043   LR 0.000450
INFO - Training [53][  140/  196]   Loss 0.251010   Top1 91.210938   Top5 99.196429   BatchTime 0.347380   LR 0.000445
INFO - Training [53][  160/  196]   Loss 0.252545   Top1 91.118164   Top5 99.182129   BatchTime 0.348111   LR 0.000441
INFO - Training [53][  180/  196]   Loss 0.253350   Top1 91.078559   Top5 99.151476   BatchTime 0.348302   LR 0.000436
INFO - ==> Top1: 91.092    Top5: 99.170    Loss: 0.253
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [53][   20/   40]   Loss 0.293508   Top1 91.406250   Top5 99.589844   BatchTime 0.134086
INFO - Validation [53][   40/   40]   Loss 0.274254   Top1 91.780000   Top5 99.700000   BatchTime 0.093477
INFO - ==> Top1: 91.780    Top5: 99.700    Loss: 0.274
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 91.780   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 91.630   Top5: 99.790]
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0567)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0613)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0263)
features.4.conv.0 tensor(0.0366)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.3053)
features.5.conv.0 tensor(0.0259)
features.5.conv.3 tensor(0.0642)
features.5.conv.6 tensor(0.3262)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0249)
features.6.conv.6 tensor(0.0463)
features.7.conv.0 tensor(0.0765)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.4064)
features.8.conv.0 tensor(0.0857)
features.8.conv.3 tensor(0.1218)
features.8.conv.6 tensor(0.4244)
features.9.conv.0 tensor(0.1251)
features.9.conv.3 tensor(0.1319)
features.9.conv.6 tensor(0.4875)
features.10.conv.0 tensor(0.0492)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.3418)
features.11.conv.0 tensor(0.5729)
features.11.conv.3 tensor(0.1578)
features.11.conv.6 tensor(0.7138)
features.12.conv.0 tensor(0.5476)
features.12.conv.3 tensor(0.1510)
features.12.conv.6 tensor(0.7517)
features.13.conv.0 tensor(0.1140)
features.13.conv.3 tensor(0.1453)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9606)
features.14.conv.3 tensor(0.1366)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9719)
features.15.conv.3 tensor(0.1293)
features.15.conv.6 tensor(0.9847)
features.16.conv.0 tensor(0.4443)
features.16.conv.3 tensor(0.1672)
features.16.conv.6 tensor(0.6774)
conv.0 tensor(0.8650)
tensor(1463828.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [54][   20/  196]   Loss 0.261052   Top1 90.781250   Top5 98.632812   BatchTime 0.439104   LR 0.000427
INFO - Training [54][   40/  196]   Loss 0.261844   Top1 90.664062   Top5 98.837891   BatchTime 0.406439   LR 0.000423
INFO - Training [54][   60/  196]   Loss 0.265555   Top1 90.462240   Top5 98.880208   BatchTime 0.382554   LR 0.000418
INFO - Training [54][   80/  196]   Loss 0.264135   Top1 90.546875   Top5 99.023438   BatchTime 0.353046   LR 0.000413
INFO - Training [54][  100/  196]   Loss 0.253329   Top1 90.972656   Top5 99.085938   BatchTime 0.344427   LR 0.000408
INFO - Training [54][  120/  196]   Loss 0.247514   Top1 91.155599   Top5 99.169922   BatchTime 0.345068   LR 0.000404
INFO - Training [54][  140/  196]   Loss 0.247033   Top1 91.185826   Top5 99.215960   BatchTime 0.345730   LR 0.000399
INFO - Training [54][  160/  196]   Loss 0.250462   Top1 91.027832   Top5 99.208984   BatchTime 0.346099   LR 0.000394
INFO - Training [54][  180/  196]   Loss 0.250047   Top1 91.056858   Top5 99.175347   BatchTime 0.346175   LR 0.000390
INFO - ==> Top1: 91.106    Top5: 99.166    Loss: 0.249
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [54][   20/   40]   Loss 0.285382   Top1 91.738281   Top5 99.628906   BatchTime 0.132450
INFO - Validation [54][   40/   40]   Loss 0.270251   Top1 91.960000   Top5 99.740000   BatchTime 0.091059
INFO - ==> Top1: 91.960    Top5: 99.740    Loss: 0.270
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 91.780   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.730   Top5: 99.740]
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0613)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0448)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0340)
features.3.conv.6 tensor(0.0293)
features.4.conv.0 tensor(0.0379)
features.4.conv.3 tensor(0.0758)
features.4.conv.6 tensor(0.3057)
features.5.conv.0 tensor(0.0244)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3262)
features.6.conv.0 tensor(0.0241)
features.6.conv.3 tensor(0.0249)
features.6.conv.6 tensor(0.0463)
features.7.conv.0 tensor(0.0783)
features.7.conv.3 tensor(0.1036)
features.7.conv.6 tensor(0.4065)
features.8.conv.0 tensor(0.0858)
features.8.conv.3 tensor(0.1227)
features.8.conv.6 tensor(0.4246)
features.9.conv.0 tensor(0.1259)
features.9.conv.3 tensor(0.1305)
features.9.conv.6 tensor(0.4880)
features.10.conv.0 tensor(0.0474)
features.10.conv.3 tensor(0.0816)
features.10.conv.6 tensor(0.3420)
features.11.conv.0 tensor(0.5734)
features.11.conv.3 tensor(0.1588)
features.11.conv.6 tensor(0.7137)
features.12.conv.0 tensor(0.5486)
features.12.conv.3 tensor(0.1505)
features.12.conv.6 tensor(0.7517)
features.13.conv.0 tensor(0.1131)
features.13.conv.3 tensor(0.1458)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9596)
features.14.conv.3 tensor(0.1373)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9717)
features.15.conv.3 tensor(0.1299)
features.15.conv.6 tensor(0.9848)
features.16.conv.0 tensor(0.4449)
features.16.conv.3 tensor(0.1683)
features.16.conv.6 tensor(0.6776)
conv.0 tensor(0.8650)
tensor(1463967.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [55][   20/  196]   Loss 0.263783   Top1 90.683594   Top5 98.750000   BatchTime 0.452263   LR 0.000381
INFO - Training [55][   40/  196]   Loss 0.262124   Top1 90.742188   Top5 98.896484   BatchTime 0.399233   LR 0.000377
INFO - Training [55][   60/  196]   Loss 0.266582   Top1 90.618490   Top5 99.016927   BatchTime 0.373153   LR 0.000372
INFO - Training [55][   80/  196]   Loss 0.260734   Top1 90.878906   Top5 99.101562   BatchTime 0.346977   LR 0.000368
INFO - Training [55][  100/  196]   Loss 0.256080   Top1 90.988281   Top5 99.132812   BatchTime 0.335055   LR 0.000363
INFO - Training [55][  120/  196]   Loss 0.252837   Top1 91.106771   Top5 99.199219   BatchTime 0.337118   LR 0.000358
INFO - Training [55][  140/  196]   Loss 0.249626   Top1 91.222098   Top5 99.266183   BatchTime 0.338845   LR 0.000354
INFO - Training [55][  160/  196]   Loss 0.251141   Top1 91.152344   Top5 99.228516   BatchTime 0.341703   LR 0.000349
INFO - Training [55][  180/  196]   Loss 0.251261   Top1 91.115451   Top5 99.227431   BatchTime 0.346934   LR 0.000345
INFO - ==> Top1: 91.118    Top5: 99.214    Loss: 0.251
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [55][   20/   40]   Loss 0.284162   Top1 91.914062   Top5 99.707031   BatchTime 0.151155
INFO - Validation [55][   40/   40]   Loss 0.270740   Top1 92.030000   Top5 99.780000   BatchTime 0.111342
INFO - ==> Top1: 92.030    Top5: 99.780    Loss: 0.271
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 91.780   Top5: 99.700]
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0278)
features.2.conv.0 tensor(0.0616)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0422)
features.3.conv.0 tensor(0.0217)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0389)
features.4.conv.3 tensor(0.0775)
features.4.conv.6 tensor(0.3057)
features.5.conv.0 tensor(0.0262)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.3262)
features.6.conv.0 tensor(0.0252)
features.6.conv.3 tensor(0.0226)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0782)
features.7.conv.3 tensor(0.1027)
features.7.conv.6 tensor(0.4066)
features.8.conv.0 tensor(0.0849)
features.8.conv.3 tensor(0.1212)
features.8.conv.6 tensor(0.4246)
features.9.conv.0 tensor(0.1249)
features.9.conv.3 tensor(0.1311)
features.9.conv.6 tensor(0.4879)
features.10.conv.0 tensor(0.0485)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.3421)
features.11.conv.0 tensor(0.5728)
features.11.conv.3 tensor(0.1562)
features.11.conv.6 tensor(0.7137)
features.12.conv.0 tensor(0.5469)
features.12.conv.3 tensor(0.1508)
features.12.conv.6 tensor(0.7516)
features.13.conv.0 tensor(0.1136)
features.13.conv.3 tensor(0.1443)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9594)
features.14.conv.3 tensor(0.1363)
features.14.conv.6 tensor(0.9778)
features.15.conv.0 tensor(0.9719)
features.15.conv.3 tensor(0.1302)
features.15.conv.6 tensor(0.9843)
features.16.conv.0 tensor(0.4451)
features.16.conv.3 tensor(0.1667)
features.16.conv.6 tensor(0.6776)
conv.0 tensor(0.8649)
tensor(1463673.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [56][   20/  196]   Loss 0.268668   Top1 90.527344   Top5 98.632812   BatchTime 0.438234   LR 0.000337
INFO - Training [56][   40/  196]   Loss 0.268271   Top1 90.751953   Top5 98.759766   BatchTime 0.399676   LR 0.000333
INFO - Training [56][   60/  196]   Loss 0.262625   Top1 90.781250   Top5 98.906250   BatchTime 0.373119   LR 0.000328
INFO - Training [56][   80/  196]   Loss 0.261958   Top1 90.830078   Top5 99.018555   BatchTime 0.356988   LR 0.000324
INFO - Training [56][  100/  196]   Loss 0.254564   Top1 91.140625   Top5 99.105469   BatchTime 0.352216   LR 0.000319
INFO - Training [56][  120/  196]   Loss 0.250356   Top1 91.302083   Top5 99.127604   BatchTime 0.350462   LR 0.000315
INFO - Training [56][  140/  196]   Loss 0.248039   Top1 91.383929   Top5 99.182478   BatchTime 0.349162   LR 0.000311
INFO - Training [56][  160/  196]   Loss 0.250186   Top1 91.323242   Top5 99.172363   BatchTime 0.348418   LR 0.000306
INFO - Training [56][  180/  196]   Loss 0.249289   Top1 91.328125   Top5 99.162326   BatchTime 0.348123   LR 0.000302
INFO - ==> Top1: 91.362    Top5: 99.156    Loss: 0.249
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [56][   20/   40]   Loss 0.285325   Top1 91.875000   Top5 99.707031   BatchTime 0.131867
INFO - Validation [56][   40/   40]   Loss 0.272514   Top1 91.940000   Top5 99.740000   BatchTime 0.089606
INFO - ==> Top1: 91.940    Top5: 99.740    Loss: 0.273
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0616)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0267)
features.4.conv.0 tensor(0.0361)
features.4.conv.3 tensor(0.0752)
features.4.conv.6 tensor(0.3057)
features.5.conv.0 tensor(0.0254)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0214)
features.6.conv.6 tensor(0.0451)
features.7.conv.0 tensor(0.0788)
features.7.conv.3 tensor(0.1027)
features.7.conv.6 tensor(0.4065)
features.8.conv.0 tensor(0.0850)
features.8.conv.3 tensor(0.1186)
features.8.conv.6 tensor(0.4247)
features.9.conv.0 tensor(0.1257)
features.9.conv.3 tensor(0.1311)
features.9.conv.6 tensor(0.4879)
features.10.conv.0 tensor(0.0482)
features.10.conv.3 tensor(0.0828)
features.10.conv.6 tensor(0.3422)
features.11.conv.0 tensor(0.5734)
features.11.conv.3 tensor(0.1568)
features.11.conv.6 tensor(0.7140)
features.12.conv.0 tensor(0.5464)
features.12.conv.3 tensor(0.1526)
features.12.conv.6 tensor(0.7519)
features.13.conv.0 tensor(0.1140)
features.13.conv.3 tensor(0.1458)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9592)
features.14.conv.3 tensor(0.1367)
features.14.conv.6 tensor(0.9776)
features.15.conv.0 tensor(0.9726)
features.15.conv.3 tensor(0.1291)
features.15.conv.6 tensor(0.9839)
features.16.conv.0 tensor(0.4446)
features.16.conv.3 tensor(0.1686)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8648)
tensor(1463609.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [57][   20/  196]   Loss 0.269200   Top1 90.820312   Top5 98.613281   BatchTime 0.419658   LR 0.000294
INFO - Training [57][   40/  196]   Loss 0.261955   Top1 90.849609   Top5 98.779297   BatchTime 0.381159   LR 0.000290
INFO - Training [57][   60/  196]   Loss 0.252845   Top1 91.204427   Top5 98.893229   BatchTime 0.370129   LR 0.000286
INFO - Training [57][   80/  196]   Loss 0.250684   Top1 91.303711   Top5 99.033203   BatchTime 0.344153   LR 0.000282
INFO - Training [57][  100/  196]   Loss 0.245013   Top1 91.484375   Top5 99.105469   BatchTime 0.325079   LR 0.000277
INFO - Training [57][  120/  196]   Loss 0.240914   Top1 91.630859   Top5 99.160156   BatchTime 0.322875   LR 0.000273
INFO - Training [57][  140/  196]   Loss 0.239502   Top1 91.651786   Top5 99.207589   BatchTime 0.326055   LR 0.000269
INFO - Training [57][  160/  196]   Loss 0.242382   Top1 91.533203   Top5 99.194336   BatchTime 0.328987   LR 0.000265
INFO - Training [57][  180/  196]   Loss 0.244478   Top1 91.482205   Top5 99.155816   BatchTime 0.330293   LR 0.000261
********************pre-trained*****************
INFO - ==> Top1: 91.532    Top5: 99.174    Loss: 0.243
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [57][   20/   40]   Loss 0.318540   Top1 90.859375   Top5 99.648438   BatchTime 0.136183
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0619)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0273)
features.4.conv.0 tensor(0.0365)
features.4.conv.3 tensor(0.0770)
features.4.conv.6 tensor(0.3058)
features.5.conv.0 tensor(0.0262)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3262)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0197)
features.6.conv.6 tensor(0.0459)
features.7.conv.0 tensor(0.0785)
features.7.conv.3 tensor(0.1016)
features.7.conv.6 tensor(0.4067)
features.8.conv.0 tensor(0.0865)
features.8.conv.3 tensor(0.1212)
features.8.conv.6 tensor(0.4249)
features.9.conv.0 tensor(0.1264)
features.9.conv.3 tensor(0.1331)
features.9.conv.6 tensor(0.4881)
features.10.conv.0 tensor(0.0476)
features.10.conv.3 tensor(0.0807)
features.10.conv.6 tensor(0.3423)
features.11.conv.0 tensor(0.5728)
features.11.conv.3 tensor(0.1549)
features.11.conv.6 tensor(0.7137)
features.12.conv.0 tensor(0.5461)
features.12.conv.3 tensor(0.1524)
features.12.conv.6 tensor(0.7517)
features.13.conv.0 tensor(0.1143)
features.13.conv.3 tensor(0.1472)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9596)
features.14.conv.3 tensor(0.1365)
features.14.conv.6 tensor(0.9774)
features.15.conv.0 tensor(0.9730)
features.15.conv.3 tensor(0.1300)
features.15.conv.6 tensor(0.9839)
features.16.conv.0 tensor(0.4447)
features.16.conv.3 tensor(0.1683)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8648)
tensor(1463727.) 2188896.0
INFO - Validation [57][   40/   40]   Loss 0.304279   Top1 91.020000   Top5 99.720000   BatchTime 0.092137
INFO - ==> Top1: 91.020    Top5: 99.720    Loss: 0.304
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [58][   20/  196]   Loss 0.260357   Top1 90.488281   Top5 98.554688   BatchTime 0.439457   LR 0.000254
INFO - Training [58][   40/  196]   Loss 0.271129   Top1 90.341797   Top5 98.779297   BatchTime 0.385586   LR 0.000250
INFO - Training [58][   60/  196]   Loss 0.263319   Top1 90.722656   Top5 98.880208   BatchTime 0.372821   LR 0.000246
INFO - Training [58][   80/  196]   Loss 0.259467   Top1 90.800781   Top5 99.072266   BatchTime 0.368492   LR 0.000242
INFO - Training [58][  100/  196]   Loss 0.250127   Top1 91.164062   Top5 99.140625   BatchTime 0.352839   LR 0.000238
INFO - Training [58][  120/  196]   Loss 0.244571   Top1 91.435547   Top5 99.192708   BatchTime 0.335638   LR 0.000234
INFO - Training [58][  140/  196]   Loss 0.243580   Top1 91.506696   Top5 99.243862   BatchTime 0.331910   LR 0.000230
INFO - Training [58][  160/  196]   Loss 0.243789   Top1 91.567383   Top5 99.216309   BatchTime 0.332505   LR 0.000226
INFO - Training [58][  180/  196]   Loss 0.245184   Top1 91.508247   Top5 99.190538   BatchTime 0.330891   LR 0.000222
********************pre-trained*****************
INFO - ==> Top1: 91.526    Top5: 99.178    Loss: 0.245
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0613)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0278)
features.4.conv.0 tensor(0.0387)
features.4.conv.3 tensor(0.0775)
features.4.conv.6 tensor(0.3057)
features.5.conv.0 tensor(0.0259)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.3262)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0208)
features.6.conv.6 tensor(0.0452)
features.7.conv.0 tensor(0.0793)
features.7.conv.3 tensor(0.1007)
features.7.conv.6 tensor(0.4069)
features.8.conv.0 tensor(0.0858)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.4251)
features.9.conv.0 tensor(0.1255)
features.9.conv.3 tensor(0.1317)
features.9.conv.6 tensor(0.4881)
features.10.conv.0 tensor(0.0477)
features.10.conv.3 tensor(0.0804)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5726)
features.11.conv.3 tensor(0.1566)
features.11.conv.6 tensor(0.7137)
features.12.conv.0 tensor(0.5462)
features.12.conv.3 tensor(0.1512)
features.12.conv.6 tensor(0.7517)
features.13.conv.0 tensor(0.1147)
features.13.conv.3 tensor(0.1460)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9590)
features.14.conv.3 tensor(0.1366)
features.14.conv.6 tensor(0.9777)
features.15.conv.0 tensor(0.9718)
features.15.conv.3 tensor(0.1301)
features.15.conv.6 tensor(0.9846)
features.16.conv.0 tensor(0.4447)
features.16.conv.3 tensor(0.1675)
features.16.conv.6 tensor(0.6774)
conv.0 tensor(0.8649)
tensor(1463548.) 2188896.0
INFO - Validation [58][   20/   40]   Loss 0.285968   Top1 91.699219   Top5 99.687500   BatchTime 0.143928
INFO - Validation [58][   40/   40]   Loss 0.270913   Top1 91.940000   Top5 99.720000   BatchTime 0.096523
INFO - ==> Top1: 91.940    Top5: 99.720    Loss: 0.271
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [59][   20/  196]   Loss 0.269887   Top1 90.273438   Top5 98.593750   BatchTime 0.409641   LR 0.000215
INFO - Training [59][   40/  196]   Loss 0.266981   Top1 90.468750   Top5 98.691406   BatchTime 0.373746   LR 0.000212
INFO - Training [59][   60/  196]   Loss 0.256431   Top1 90.787760   Top5 98.932292   BatchTime 0.364316   LR 0.000208
INFO - Training [59][   80/  196]   Loss 0.253159   Top1 91.000977   Top5 99.086914   BatchTime 0.368325   LR 0.000204
INFO - Training [59][  100/  196]   Loss 0.250368   Top1 91.164062   Top5 99.097656   BatchTime 0.365683   LR 0.000201
INFO - Training [59][  120/  196]   Loss 0.245063   Top1 91.321615   Top5 99.134115   BatchTime 0.352829   LR 0.000197
INFO - Training [59][  140/  196]   Loss 0.244290   Top1 91.372768   Top5 99.168527   BatchTime 0.339520   LR 0.000193
INFO - Training [59][  160/  196]   Loss 0.243649   Top1 91.379395   Top5 99.189453   BatchTime 0.342855   LR 0.000190
INFO - Training [59][  180/  196]   Loss 0.242619   Top1 91.397569   Top5 99.155816   BatchTime 0.343522   LR 0.000186
********************pre-trained*****************
INFO - ==> Top1: 91.424    Top5: 99.144    Loss: 0.242
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5208)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0611)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0434)
features.3.conv.0 tensor(0.0243)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0260)
features.4.conv.0 tensor(0.0373)
features.4.conv.3 tensor(0.0764)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0251)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3262)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0203)
features.6.conv.6 tensor(0.0451)
features.7.conv.0 tensor(0.0794)
features.7.conv.3 tensor(0.1016)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0850)
features.8.conv.3 tensor(0.1198)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1254)
features.9.conv.3 tensor(0.1331)
features.9.conv.6 tensor(0.4882)
features.10.conv.0 tensor(0.0470)
features.10.conv.3 tensor(0.0813)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5724)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.7137)
features.12.conv.0 tensor(0.5460)
features.12.conv.3 tensor(0.1507)
features.12.conv.6 tensor(0.7517)
features.13.conv.0 tensor(0.1152)
features.13.conv.3 tensor(0.1453)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9593)
features.14.conv.3 tensor(0.1374)
features.14.conv.6 tensor(0.9776)
features.15.conv.0 tensor(0.9717)
features.15.conv.3 tensor(0.1297)
features.15.conv.6 tensor(0.9842)
features.16.conv.0 tensor(0.4449)
features.16.conv.3 tensor(0.1696)
features.16.conv.6 tensor(0.6773)
conv.0 tensor(0.8649)
tensor(1463536.) 2188896.0
INFO - Validation [59][   20/   40]   Loss 0.289602   Top1 91.757812   Top5 99.687500   BatchTime 0.134579
INFO - Validation [59][   40/   40]   Loss 0.273526   Top1 91.900000   Top5 99.760000   BatchTime 0.092102
INFO - ==> Top1: 91.900    Top5: 99.760    Loss: 0.274
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [60][   20/  196]   Loss 0.271978   Top1 90.664062   Top5 98.671875   BatchTime 0.464662   LR 0.000180
INFO - Training [60][   40/  196]   Loss 0.268120   Top1 90.839844   Top5 98.808594   BatchTime 0.407945   LR 0.000176
INFO - Training [60][   60/  196]   Loss 0.252223   Top1 91.367188   Top5 98.977865   BatchTime 0.393670   LR 0.000173
INFO - Training [60][   80/  196]   Loss 0.249515   Top1 91.445312   Top5 99.077148   BatchTime 0.377197   LR 0.000169
INFO - Training [60][  100/  196]   Loss 0.243349   Top1 91.636719   Top5 99.121094   BatchTime 0.371785   LR 0.000166
INFO - Training [60][  120/  196]   Loss 0.239263   Top1 91.806641   Top5 99.208984   BatchTime 0.363209   LR 0.000162
INFO - Training [60][  140/  196]   Loss 0.237738   Top1 91.827567   Top5 99.257812   BatchTime 0.351017   LR 0.000159
INFO - Training [60][  160/  196]   Loss 0.240774   Top1 91.716309   Top5 99.240723   BatchTime 0.340798   LR 0.000156
INFO - Training [60][  180/  196]   Loss 0.239947   Top1 91.705729   Top5 99.227431   BatchTime 0.341038   LR 0.000152
********************pre-trained*****************
INFO - ==> Top1: 91.772    Top5: 99.222    Loss: 0.237
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [60][   20/   40]   Loss 0.300589   Top1 91.406250   Top5 99.707031   BatchTime 0.133315
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0622)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0440)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0269)
features.4.conv.0 tensor(0.0374)
features.4.conv.3 tensor(0.0758)
features.4.conv.6 tensor(0.3058)
features.5.conv.0 tensor(0.0262)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0197)
features.6.conv.6 tensor(0.0455)
features.7.conv.0 tensor(0.0786)
features.7.conv.3 tensor(0.1010)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0845)
features.8.conv.3 tensor(0.1189)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1251)
features.9.conv.3 tensor(0.1319)
features.9.conv.6 tensor(0.4883)
features.10.conv.0 tensor(0.0470)
features.10.conv.3 tensor(0.0807)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5723)
features.11.conv.3 tensor(0.1561)
features.11.conv.6 tensor(0.7135)
features.12.conv.0 tensor(0.5460)
features.12.conv.3 tensor(0.1495)
features.12.conv.6 tensor(0.7515)
features.13.conv.0 tensor(0.1153)
features.13.conv.3 tensor(0.1435)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9591)
features.14.conv.3 tensor(0.1373)
features.14.conv.6 tensor(0.9775)
features.15.conv.0 tensor(0.9713)
features.15.conv.3 tensor(0.1299)
features.15.conv.6 tensor(0.9835)
features.16.conv.0 tensor(0.4450)
features.16.conv.3 tensor(0.1682)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8647)
tensor(1463224.) 2188896.0
INFO - Validation [60][   40/   40]   Loss 0.288655   Top1 91.590000   Top5 99.750000   BatchTime 0.095637
INFO - ==> Top1: 91.590    Top5: 99.750    Loss: 0.289
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [61][   20/  196]   Loss 0.258800   Top1 90.839844   Top5 98.769531   BatchTime 0.449155   LR 0.000147
INFO - Training [61][   40/  196]   Loss 0.249488   Top1 91.015625   Top5 98.925781   BatchTime 0.415623   LR 0.000143
INFO - Training [61][   60/  196]   Loss 0.246559   Top1 91.282552   Top5 99.003906   BatchTime 0.397058   LR 0.000140
INFO - Training [61][   80/  196]   Loss 0.247811   Top1 91.293945   Top5 99.082031   BatchTime 0.383015   LR 0.000137
INFO - Training [61][  100/  196]   Loss 0.242573   Top1 91.558594   Top5 99.085938   BatchTime 0.372853   LR 0.000134
INFO - Training [61][  120/  196]   Loss 0.237701   Top1 91.725260   Top5 99.147135   BatchTime 0.373184   LR 0.000131
INFO - Training [61][  140/  196]   Loss 0.235177   Top1 91.788504   Top5 99.199219   BatchTime 0.362035   LR 0.000128
INFO - Training [61][  160/  196]   Loss 0.239403   Top1 91.630859   Top5 99.182129   BatchTime 0.355367   LR 0.000125
INFO - Training [61][  180/  196]   Loss 0.239621   Top1 91.657986   Top5 99.147135   BatchTime 0.353871   LR 0.000122
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 91.638    Top5: 99.148    Loss: 0.239
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0619)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0280)
features.4.conv.0 tensor(0.0373)
features.4.conv.3 tensor(0.0764)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0256)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0239)
features.6.conv.3 tensor(0.0185)
features.6.conv.6 tensor(0.0451)
features.7.conv.0 tensor(0.0791)
features.7.conv.3 tensor(0.1021)
features.7.conv.6 tensor(0.4069)
features.8.conv.0 tensor(0.0848)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.4251)
features.9.conv.0 tensor(0.1252)
features.9.conv.3 tensor(0.1334)
features.9.conv.6 tensor(0.4884)
features.10.conv.0 tensor(0.0460)
features.10.conv.3 tensor(0.0813)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5722)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.7134)
features.12.conv.0 tensor(0.5457)
features.12.conv.3 tensor(0.1510)
features.12.conv.6 tensor(0.7514)
features.13.conv.0 tensor(0.1160)
features.13.conv.3 tensor(0.1460)
features.13.conv.6 tensor(0.1982)
features.14.conv.0 tensor(0.9592)
features.14.conv.3 tensor(0.1372)
features.14.conv.6 tensor(0.9776)
features.15.conv.0 tensor(0.9719)
features.15.conv.3 tensor(0.1314)
features.15.conv.6 tensor(0.9842)
features.16.conv.0 tensor(0.4450)
features.16.conv.3 tensor(0.1676)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8648)
tensor(1463530.) 2188896.0
INFO - Validation [61][   20/   40]   Loss 0.303480   Top1 91.523438   Top5 99.667969   BatchTime 0.132623
INFO - Validation [61][   40/   40]   Loss 0.287205   Top1 91.830000   Top5 99.750000   BatchTime 0.093352
INFO - ==> Top1: 91.830    Top5: 99.750    Loss: 0.287
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [62][   20/  196]   Loss 0.260496   Top1 91.113281   Top5 98.730469   BatchTime 0.430856   LR 0.000117
INFO - Training [62][   40/  196]   Loss 0.260874   Top1 91.015625   Top5 98.857422   BatchTime 0.392844   LR 0.000114
INFO - Training [62][   60/  196]   Loss 0.252986   Top1 91.236979   Top5 98.977865   BatchTime 0.385246   LR 0.000111
INFO - Training [62][   80/  196]   Loss 0.250301   Top1 91.298828   Top5 99.077148   BatchTime 0.374424   LR 0.000108
INFO - Training [62][  100/  196]   Loss 0.237443   Top1 91.703125   Top5 99.183594   BatchTime 0.368343   LR 0.000105
INFO - Training [62][  120/  196]   Loss 0.233810   Top1 91.845703   Top5 99.225260   BatchTime 0.364815   LR 0.000102
INFO - Training [62][  140/  196]   Loss 0.231036   Top1 91.925223   Top5 99.274554   BatchTime 0.361778   LR 0.000100
INFO - Training [62][  160/  196]   Loss 0.236072   Top1 91.796875   Top5 99.226074   BatchTime 0.350667   LR 0.000097
INFO - Training [62][  180/  196]   Loss 0.235027   Top1 91.809896   Top5 99.229601   BatchTime 0.346874   LR 0.000094
********************pre-trained*****************
INFO - ==> Top1: 91.834    Top5: 99.226    Loss: 0.234
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [62][   20/   40]   Loss 0.339931   Top1 90.429688   Top5 99.609375   BatchTime 0.136106
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0295)
features.2.conv.0 tensor(0.0634)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0440)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0291)
features.4.conv.0 tensor(0.0374)
features.4.conv.3 tensor(0.0775)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0256)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0244)
features.6.conv.3 tensor(0.0208)
features.6.conv.6 tensor(0.0451)
features.7.conv.0 tensor(0.0782)
features.7.conv.3 tensor(0.1013)
features.7.conv.6 tensor(0.4069)
features.8.conv.0 tensor(0.0854)
features.8.conv.3 tensor(0.1189)
features.8.conv.6 tensor(0.4249)
features.9.conv.0 tensor(0.1249)
features.9.conv.3 tensor(0.1328)
features.9.conv.6 tensor(0.4884)
features.10.conv.0 tensor(0.0457)
features.10.conv.3 tensor(0.0825)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5724)
features.11.conv.3 tensor(0.1586)
features.11.conv.6 tensor(0.7134)
features.12.conv.0 tensor(0.5456)
features.12.conv.3 tensor(0.1520)
features.12.conv.6 tensor(0.7516)
features.13.conv.0 tensor(0.1159)
features.13.conv.3 tensor(0.1460)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9594)
features.14.conv.3 tensor(0.1366)
features.14.conv.6 tensor(0.9774)
features.15.conv.0 tensor(0.9719)
features.15.conv.3 tensor(0.1302)
features.15.conv.6 tensor(0.9840)
features.16.conv.0 tensor(0.4451)
features.16.conv.3 tensor(0.1678)
features.16.conv.6 tensor(0.6776)
conv.0 tensor(0.8648)
tensor(1463532.) 2188896.0
INFO - Validation [62][   40/   40]   Loss 0.323783   Top1 90.670000   Top5 99.690000   BatchTime 0.094181
INFO - ==> Top1: 90.670    Top5: 99.690    Loss: 0.324
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [63][   20/  196]   Loss 0.247931   Top1 91.289062   Top5 98.457031   BatchTime 0.424539   LR 0.000090
INFO - Training [63][   40/  196]   Loss 0.255770   Top1 90.927734   Top5 98.750000   BatchTime 0.383474   LR 0.000087
INFO - Training [63][   60/  196]   Loss 0.249715   Top1 91.236979   Top5 98.860677   BatchTime 0.378421   LR 0.000085
INFO - Training [63][   80/  196]   Loss 0.249378   Top1 91.235352   Top5 99.018555   BatchTime 0.378400   LR 0.000082
INFO - Training [63][  100/  196]   Loss 0.240889   Top1 91.542969   Top5 99.121094   BatchTime 0.377923   LR 0.000080
INFO - Training [63][  120/  196]   Loss 0.233979   Top1 91.845703   Top5 99.205729   BatchTime 0.377638   LR 0.000077
INFO - Training [63][  140/  196]   Loss 0.233070   Top1 91.883371   Top5 99.260603   BatchTime 0.374561   LR 0.000075
INFO - Training [63][  160/  196]   Loss 0.236651   Top1 91.784668   Top5 99.235840   BatchTime 0.366063   LR 0.000072
INFO - Training [63][  180/  196]   Loss 0.236347   Top1 91.759983   Top5 99.194878   BatchTime 0.366002   LR 0.000070
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 91.820    Top5: 99.204    Loss: 0.235
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0628)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0378)
features.4.conv.3 tensor(0.0787)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0260)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0191)
features.6.conv.6 tensor(0.0458)
features.7.conv.0 tensor(0.0786)
features.7.conv.3 tensor(0.1010)
features.7.conv.6 tensor(0.4069)
features.8.conv.0 tensor(0.0858)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1243)
features.9.conv.3 tensor(0.1337)
features.9.conv.6 tensor(0.4883)
features.10.conv.0 tensor(0.0455)
features.10.conv.3 tensor(0.0833)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5724)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.7132)
features.12.conv.0 tensor(0.5458)
features.12.conv.3 tensor(0.1514)
features.12.conv.6 tensor(0.7515)
features.13.conv.0 tensor(0.1155)
features.13.conv.3 tensor(0.1456)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9592)
features.14.conv.3 tensor(0.1377)
features.14.conv.6 tensor(0.9774)
features.15.conv.0 tensor(0.9715)
features.15.conv.3 tensor(0.1297)
features.15.conv.6 tensor(0.9841)
features.16.conv.0 tensor(0.4452)
features.16.conv.3 tensor(0.1669)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8646)
tensor(1463391.) 2188896.0
INFO - Validation [63][   20/   40]   Loss 0.513863   Top1 85.957031   Top5 99.218750   BatchTime 0.130131
INFO - Validation [63][   40/   40]   Loss 0.499413   Top1 86.060000   Top5 99.400000   BatchTime 0.090515
INFO - ==> Top1: 86.060    Top5: 99.400    Loss: 0.499
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [64][   20/  196]   Loss 0.252483   Top1 91.230469   Top5 98.750000   BatchTime 0.437787   LR 0.000066
INFO - Training [64][   40/  196]   Loss 0.245547   Top1 91.250000   Top5 98.857422   BatchTime 0.395535   LR 0.000064
INFO - Training [64][   60/  196]   Loss 0.240662   Top1 91.477865   Top5 98.997396   BatchTime 0.388732   LR 0.000062
INFO - Training [64][   80/  196]   Loss 0.238690   Top1 91.601562   Top5 99.125977   BatchTime 0.383180   LR 0.000059
INFO - Training [64][  100/  196]   Loss 0.233474   Top1 91.796875   Top5 99.195312   BatchTime 0.375513   LR 0.000057
INFO - Training [64][  120/  196]   Loss 0.229625   Top1 91.995443   Top5 99.228516   BatchTime 0.371063   LR 0.000055
INFO - Training [64][  140/  196]   Loss 0.228225   Top1 92.061942   Top5 99.282924   BatchTime 0.358384   LR 0.000053
INFO - Training [64][  160/  196]   Loss 0.231135   Top1 91.953125   Top5 99.262695   BatchTime 0.350367   LR 0.000051
INFO - Training [64][  180/  196]   Loss 0.231454   Top1 91.888021   Top5 99.270833   BatchTime 0.353501   LR 0.000049
********************pre-trained*****************
INFO - ==> Top1: 91.904    Top5: 99.260    Loss: 0.231
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [64][   20/   40]   Loss 0.501992   Top1 85.742188   Top5 99.218750   BatchTime 0.129964
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0631)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0368)
features.4.conv.3 tensor(0.0781)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0259)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0203)
features.6.conv.6 tensor(0.0452)
features.7.conv.0 tensor(0.0781)
features.7.conv.3 tensor(0.1010)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0861)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1250)
features.9.conv.3 tensor(0.1311)
features.9.conv.6 tensor(0.4884)
features.10.conv.0 tensor(0.0452)
features.10.conv.3 tensor(0.0819)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5726)
features.11.conv.3 tensor(0.1568)
features.11.conv.6 tensor(0.7134)
features.12.conv.0 tensor(0.5458)
features.12.conv.3 tensor(0.1520)
features.12.conv.6 tensor(0.7515)
features.13.conv.0 tensor(0.1155)
features.13.conv.3 tensor(0.1449)
features.13.conv.6 tensor(0.1982)
features.14.conv.0 tensor(0.9592)
features.14.conv.3 tensor(0.1377)
features.14.conv.6 tensor(0.9773)
features.15.conv.0 tensor(0.9714)
features.15.conv.3 tensor(0.1301)
features.15.conv.6 tensor(0.9843)
features.16.conv.0 tensor(0.4452)
features.16.conv.3 tensor(0.1670)
features.16.conv.6 tensor(0.6776)
conv.0 tensor(0.8646)
tensor(1463379.) 2188896.0
INFO - Validation [64][   40/   40]   Loss 0.488853   Top1 86.070000   Top5 99.320000   BatchTime 0.091434
INFO - ==> Top1: 86.070    Top5: 99.320    Loss: 0.489
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [65][   20/  196]   Loss 0.258006   Top1 90.781250   Top5 98.652344   BatchTime 0.453549   LR 0.000046
INFO - Training [65][   40/  196]   Loss 0.250997   Top1 91.289062   Top5 98.769531   BatchTime 0.414196   LR 0.000044
INFO - Training [65][   60/  196]   Loss 0.244624   Top1 91.510417   Top5 98.821615   BatchTime 0.395213   LR 0.000042
INFO - Training [65][   80/  196]   Loss 0.243367   Top1 91.523438   Top5 98.979492   BatchTime 0.391253   LR 0.000040
INFO - Training [65][  100/  196]   Loss 0.237108   Top1 91.738281   Top5 99.089844   BatchTime 0.387891   LR 0.000039
INFO - Training [65][  120/  196]   Loss 0.231742   Top1 91.940104   Top5 99.124349   BatchTime 0.377651   LR 0.000037
INFO - Training [65][  140/  196]   Loss 0.231728   Top1 91.981027   Top5 99.171317   BatchTime 0.364962   LR 0.000035
INFO - Training [65][  160/  196]   Loss 0.233100   Top1 91.887207   Top5 99.177246   BatchTime 0.358168   LR 0.000033
INFO - Training [65][  180/  196]   Loss 0.232377   Top1 91.888021   Top5 99.153646   BatchTime 0.359207   LR 0.000032
********************pre-trained*****************
INFO - ==> Top1: 91.904    Top5: 99.142    Loss: 0.232
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [65][   20/   40]   Loss 0.295948   Top1 91.347656   Top5 99.687500   BatchTime 0.135462
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0622)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0460)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0280)
features.4.conv.0 tensor(0.0369)
features.4.conv.3 tensor(0.0787)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0256)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0233)
features.6.conv.3 tensor(0.0197)
features.6.conv.6 tensor(0.0452)
features.7.conv.0 tensor(0.0788)
features.7.conv.3 tensor(0.1019)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0861)
features.8.conv.3 tensor(0.1204)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1249)
features.9.conv.3 tensor(0.1340)
features.9.conv.6 tensor(0.4883)
features.10.conv.0 tensor(0.0455)
features.10.conv.3 tensor(0.0822)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5723)
features.11.conv.3 tensor(0.1576)
features.11.conv.6 tensor(0.7132)
features.12.conv.0 tensor(0.5459)
features.12.conv.3 tensor(0.1505)
features.12.conv.6 tensor(0.7516)
features.13.conv.0 tensor(0.1150)
features.13.conv.3 tensor(0.1456)
features.13.conv.6 tensor(0.1983)
features.14.conv.0 tensor(0.9592)
features.14.conv.3 tensor(0.1372)
features.14.conv.6 tensor(0.9775)
features.15.conv.0 tensor(0.9716)
features.15.conv.3 tensor(0.1295)
features.15.conv.6 tensor(0.9839)
features.16.conv.0 tensor(0.4451)
features.16.conv.3 tensor(0.1677)
features.16.conv.6 tensor(0.6776)
conv.0 tensor(0.8648)
tensor(1463398.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.278338   Top1 91.700000   Top5 99.730000   BatchTime 0.095866
INFO - ==> Top1: 91.700    Top5: 99.730    Loss: 0.278
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [66][   20/  196]   Loss 0.250944   Top1 91.289062   Top5 98.574219   BatchTime 0.432207   LR 0.000029
INFO - Training [66][   40/  196]   Loss 0.247593   Top1 91.445312   Top5 98.720703   BatchTime 0.399353   LR 0.000028
INFO - Training [66][   60/  196]   Loss 0.244078   Top1 91.497396   Top5 98.899740   BatchTime 0.383832   LR 0.000026
INFO - Training [66][   80/  196]   Loss 0.239468   Top1 91.591797   Top5 99.067383   BatchTime 0.373993   LR 0.000025
INFO - Training [66][  100/  196]   Loss 0.234511   Top1 91.761719   Top5 99.128906   BatchTime 0.367595   LR 0.000023
INFO - Training [66][  120/  196]   Loss 0.229632   Top1 91.962891   Top5 99.192708   BatchTime 0.365063   LR 0.000022
INFO - Training [66][  140/  196]   Loss 0.229123   Top1 92.022879   Top5 99.227121   BatchTime 0.354430   LR 0.000021
INFO - Training [66][  160/  196]   Loss 0.231767   Top1 91.940918   Top5 99.216309   BatchTime 0.351872   LR 0.000019
INFO - Training [66][  180/  196]   Loss 0.233092   Top1 91.866319   Top5 99.190538   BatchTime 0.353011   LR 0.000018
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 91.882    Top5: 99.200    Loss: 0.232
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0625)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0463)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0324)
features.3.conv.6 tensor(0.0280)
features.4.conv.0 tensor(0.0365)
features.4.conv.3 tensor(0.0787)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0259)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0191)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0781)
features.7.conv.3 tensor(0.0995)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0864)
features.8.conv.3 tensor(0.1201)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1251)
features.9.conv.3 tensor(0.1334)
features.9.conv.6 tensor(0.4884)
features.10.conv.0 tensor(0.0454)
features.10.conv.3 tensor(0.0819)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5720)
features.11.conv.3 tensor(0.1562)
features.11.conv.6 tensor(0.7133)
features.12.conv.0 tensor(0.5458)
features.12.conv.3 tensor(0.1507)
features.12.conv.6 tensor(0.7516)
features.13.conv.0 tensor(0.1149)
features.13.conv.3 tensor(0.1451)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9591)
features.14.conv.3 tensor(0.1373)
features.14.conv.6 tensor(0.9772)
features.15.conv.0 tensor(0.9714)
features.15.conv.3 tensor(0.1296)
features.15.conv.6 tensor(0.9840)
INFO - Validation [66][   20/   40]   Loss 0.312266   Top1 91.171875   Top5 99.609375   BatchTime 0.133435
INFO - Validation [66][   40/   40]   Loss 0.290786   Top1 91.500000   Top5 99.700000   BatchTime 0.093792
features.16.conv.0 tensor(0.4452)
features.16.conv.3 tensor(0.1674)
features.16.conv.6 tensor(0.6776)
conv.0 tensor(0.8647)
tensor(1463259.) 2188896.0
INFO - ==> Top1: 91.500    Top5: 99.700    Loss: 0.291
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 91.940   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [67][   20/  196]   Loss 0.259147   Top1 90.605469   Top5 98.808594   BatchTime 0.427677   LR 0.000016
INFO - Training [67][   40/  196]   Loss 0.257780   Top1 90.615234   Top5 98.916016   BatchTime 0.395746   LR 0.000015
INFO - Training [67][   60/  196]   Loss 0.246431   Top1 91.171875   Top5 99.036458   BatchTime 0.386007   LR 0.000014
INFO - Training [67][   80/  196]   Loss 0.244631   Top1 91.274414   Top5 99.140625   BatchTime 0.374486   LR 0.000013
INFO - Training [67][  100/  196]   Loss 0.237598   Top1 91.546875   Top5 99.179688   BatchTime 0.367821   LR 0.000012
INFO - Training [67][  120/  196]   Loss 0.231324   Top1 91.839193   Top5 99.225260   BatchTime 0.355898   LR 0.000011
INFO - Training [67][  140/  196]   Loss 0.229003   Top1 91.955915   Top5 99.241071   BatchTime 0.343409   LR 0.000010
INFO - Training [67][  160/  196]   Loss 0.230521   Top1 91.877441   Top5 99.221191   BatchTime 0.345035   LR 0.000009
INFO - Training [67][  180/  196]   Loss 0.231217   Top1 91.853299   Top5 99.166667   BatchTime 0.347087   LR 0.000008
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 91.868    Top5: 99.166    Loss: 0.231
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [67][   20/   40]   Loss 0.289139   Top1 91.757812   Top5 99.667969   BatchTime 0.138410
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0321)
features.2.conv.0 tensor(0.0625)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0324)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0360)
features.4.conv.3 tensor(0.0793)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0257)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0197)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0784)
features.7.conv.3 tensor(0.1004)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0861)
features.8.conv.3 tensor(0.1209)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1253)
features.9.conv.3 tensor(0.1340)
features.9.conv.6 tensor(0.4884)
features.10.conv.0 tensor(0.0454)
features.10.conv.3 tensor(0.0804)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5722)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.7133)
features.12.conv.0 tensor(0.5460)
features.12.conv.3 tensor(0.1499)
features.12.conv.6 tensor(0.7516)
features.13.conv.0 tensor(0.1149)
features.13.conv.3 tensor(0.1464)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9589)
features.14.conv.3 tensor(0.1372)
features.14.conv.6 tensor(0.9774)
features.15.conv.0 tensor(0.9714)
features.15.conv.3 tensor(0.1302)
features.15.conv.6 tensor(0.9844)
features.16.conv.0 tensor(0.4451)
features.16.conv.3 tensor(0.1660)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8647)
tensor(1463335.) 2188896.0
INFO - Validation [67][   40/   40]   Loss 0.271291   Top1 92.000000   Top5 99.710000   BatchTime 0.096646
INFO - ==> Top1: 92.000    Top5: 99.710    Loss: 0.271
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [67][Top1: 92.000   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [68][   20/  196]   Loss 0.238842   Top1 91.132812   Top5 98.867188   BatchTime 0.440554   LR 0.000007
INFO - Training [68][   40/  196]   Loss 0.245905   Top1 91.230469   Top5 99.003906   BatchTime 0.391364   LR 0.000006
INFO - Training [68][   60/  196]   Loss 0.237922   Top1 91.542969   Top5 99.055990   BatchTime 0.371724   LR 0.000006
INFO - Training [68][   80/  196]   Loss 0.237989   Top1 91.547852   Top5 99.150391   BatchTime 0.363647   LR 0.000005
INFO - Training [68][  100/  196]   Loss 0.232560   Top1 91.757812   Top5 99.167969   BatchTime 0.357577   LR 0.000004
INFO - Training [68][  120/  196]   Loss 0.229232   Top1 91.897786   Top5 99.208984   BatchTime 0.350246   LR 0.000004
INFO - Training [68][  140/  196]   Loss 0.227792   Top1 92.003348   Top5 99.263393   BatchTime 0.343960   LR 0.000003
INFO - Training [68][  160/  196]   Loss 0.229230   Top1 91.914062   Top5 99.250488   BatchTime 0.344894   LR 0.000003
INFO - Training [68][  180/  196]   Loss 0.229727   Top1 91.946615   Top5 99.203559   BatchTime 0.351814   LR 0.000002
********************pre-trained*****************
INFO - ==> Top1: 91.972    Top5: 99.202    Loss: 0.229
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0622)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0324)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0360)
features.4.conv.3 tensor(0.0787)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0259)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0233)
features.6.conv.3 tensor(0.0185)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0785)
features.7.conv.3 tensor(0.1007)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0863)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1251)
features.9.conv.3 tensor(0.1314)
features.9.conv.6 tensor(0.4884)
features.10.conv.0 tensor(0.0454)
features.10.conv.3 tensor(0.0813)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5720)
features.11.conv.3 tensor(0.1570)
features.11.conv.6 tensor(0.7133)
features.12.conv.0 tensor(0.5455)
features.12.conv.3 tensor(0.1512)
features.12.conv.6 tensor(0.7515)
features.13.conv.0 tensor(0.1149)
features.13.conv.3 tensor(0.1462)
features.13.conv.6 tensor(0.1984)
features.14.conv.0 tensor(0.9589)
features.14.conv.3 tensor(0.1365)
features.14.conv.6 tensor(0.9774)
features.15.conv.0 tensor(0.9714)
features.15.conv.3 tensor(0.1299)
features.15.conv.6 tensor(0.9839)
features.16.conv.0 tensor(0.4450)
features.16.conv.3 tensor(0.1662)
features.16.conv.6 tensor(0.6775)
conv.0 tensor(0.8648)
tensor(1463221.) 2188896.0
INFO - Validation [68][   20/   40]   Loss 0.382885   Top1 88.808594   Top5 99.550781   BatchTime 0.141328
INFO - Validation [68][   40/   40]   Loss 0.368296   Top1 89.200000   Top5 99.590000   BatchTime 0.099274
INFO - ==> Top1: 89.200    Top5: 99.590    Loss: 0.368
INFO - ==> Sparsity : 0.668
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 2 ==> Epoch [67][Top1: 92.000   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 91.960   Top5: 99.740]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [69][   20/  196]   Loss 0.248196   Top1 91.035156   Top5 98.417969   BatchTime 0.465655   LR 0.000002
INFO - Training [69][   40/  196]   Loss 0.255638   Top1 91.035156   Top5 98.662109   BatchTime 0.408695   LR 0.000001
INFO - Training [69][   60/  196]   Loss 0.245286   Top1 91.354167   Top5 98.795573   BatchTime 0.387616   LR 0.000001
INFO - Training [69][   80/  196]   Loss 0.242016   Top1 91.435547   Top5 98.979492   BatchTime 0.376102   LR 0.000001
INFO - Training [69][  100/  196]   Loss 0.235829   Top1 91.695312   Top5 99.039062   BatchTime 0.371436   LR 0.000000
INFO - Training [69][  120/  196]   Loss 0.228398   Top1 92.018229   Top5 99.104818   BatchTime 0.361661   LR 0.000000
INFO - Training [69][  140/  196]   Loss 0.226587   Top1 92.109375   Top5 99.162946   BatchTime 0.353286   LR 0.000000
INFO - Training [69][  160/  196]   Loss 0.227910   Top1 92.026367   Top5 99.174805   BatchTime 0.355562   LR 0.000000
INFO - Training [69][  180/  196]   Loss 0.228493   Top1 92.007378   Top5 99.164497   BatchTime 0.354400   LR 0.000000
********************pre-trained*****************
INFO - ==> Top1: 92.048    Top5: 99.176    Loss: 0.227
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [69][   20/   40]   Loss 0.292446   Top1 91.855469   Top5 99.609375   BatchTime 0.141142
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0321)
features.2.conv.0 tensor(0.0622)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0356)
features.4.conv.3 tensor(0.0793)
features.4.conv.6 tensor(0.3060)
features.5.conv.0 tensor(0.0257)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.3263)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0179)
features.6.conv.6 tensor(0.0456)
features.7.conv.0 tensor(0.0785)
features.7.conv.3 tensor(0.0998)
features.7.conv.6 tensor(0.4068)
features.8.conv.0 tensor(0.0861)
features.8.conv.3 tensor(0.1195)
features.8.conv.6 tensor(0.4250)
features.9.conv.0 tensor(0.1251)
features.9.conv.3 tensor(0.1317)
features.9.conv.6 tensor(0.4884)
features.10.conv.0 tensor(0.0455)
features.10.conv.3 tensor(0.0810)
features.10.conv.6 tensor(0.3426)
features.11.conv.0 tensor(0.5724)
features.11.conv.3 tensor(0.1572)
features.11.conv.6 tensor(0.7133)
features.12.conv.0 tensor(0.5458)
features.12.conv.3 tensor(0.1508)
features.12.conv.6 tensor(0.7515)
features.13.conv.0 tensor(0.1150)
features.13.conv.3 tensor(0.1454)
features.13.conv.6 tensor(0.1985)
features.14.conv.0 tensor(0.9590)
features.14.conv.3 tensor(0.1372)
features.14.conv.6 tensor(0.9775)
features.15.conv.0 tensor(0.9716)
features.15.conv.3 tensor(0.1302)
features.15.conv.6 tensor(0.9841)
features.16.conv.0 tensor(0.4452)
features.16.conv.3 tensor(0.1676)
features.16.conv.6 tensor(0.6777)
conv.0 tensor(0.8647)
tensor(1463432.) 2188896.0
INFO - Validation [69][   40/   40]   Loss 0.274526   Top1 92.070000   Top5 99.690000   BatchTime 0.097908
INFO - ==> Top1: 92.070    Top5: 99.690    Loss: 0.275
INFO - ==> Sparsity : 0.669
INFO - Scoreboard best 1 ==> Epoch [69][Top1: 92.070   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 92.030   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [67][Top1: 92.000   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-101258/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [   20/   40]   Loss 0.292446   Top1 91.855469   Top5 99.609375   BatchTime 0.136250
INFO - Validation [   40/   40]   Loss 0.274526   Top1 92.070000   Top5 99.690000   BatchTime 0.094945
INFO - ==> Top1: 92.070    Top5: 99.690    Loss: 0.275
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...