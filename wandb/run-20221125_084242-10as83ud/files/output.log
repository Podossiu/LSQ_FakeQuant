Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.95438832
0.94989508
0.91378331
0.85887057
0.82477003
0.81540394
INFO - Training [0][   20/  196]   Loss 1.770930   Top1 53.476562   Top5 89.296875   BatchTime 0.339413   LR 0.004999
0.81452143
0.81097806
0.80684066
0.80014998
0.79922467
0.79687911
0.78972280
0.77959263
0.76909178
0.75923508
0.75568217
0.74719912
0.73902476
0.73423219
0.73200399
0.73064333
0.72912079
0.72883773
0.72951877
0.72973901
0.73271424
INFO - Training [0][   40/  196]   Loss 1.769645   Top1 52.509766   Top5 89.658203   BatchTime 0.312449   LR 0.004995
0.73423892
0.73454958
0.73346567
0.73091781
0.72826797
0.72732466
0.72759920
0.72801363
0.72844762
0.72943127
0.73003298
0.72996700
0.72842216
0.72662354
0.72494054
0.72359031
0.72340298
0.72233248
0.72201335
0.72086370
0.71742809
INFO - Training [0][   60/  196]   Loss 1.693836   Top1 54.837240   Top5 90.755208   BatchTime 0.308634   LR 0.004989
0.70953226
0.69793087
0.68999851
0.68755466
0.68672901
0.68419343
0.68229866
0.67948854
0.67679000
0.67362255
0.67127585
0.66918337
0.66786522
0.66798335
0.66831547
0.66962761
0.66873622
0.66827637
0.66836965
INFO - Training [0][   80/  196]   Loss 1.633258   Top1 56.650391   Top5 91.704102   BatchTime 0.335690   LR 0.004980
0.66888225
0.66929251
0.66766506
0.67067593
0.67127353
0.66620952
0.65955490
0.64924979
0.63866103
0.63151050
0.63027883
0.62787771
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [0][  100/  196]   Loss nan   Top1 53.378906   Top5 88.460938   BatchTime 0.350873   LR 0.004968
nan
nan
nan
nan
nan
nan
nan
nan
nan
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 77, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader,qat_model, teacher_model,criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 186, in train_one_epoch_slsq
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
nan