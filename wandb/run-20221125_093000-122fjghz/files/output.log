Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
*************soft_pruning_mode*******************
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.95438832
0.95018005
0.92689770
0.94540429
0.93443906
0.92238998
0.91767931
0.92605686
0.92514038
INFO - Training [0][   20/  196]   Loss 1.598686   Top1 53.750000   Top5 88.808594   BatchTime 0.376707   LR 0.004999
0.92293155
0.91801268
0.91730845
0.91392809
0.91560644
0.91656083
0.91803557
0.91678447
0.91532755
0.91397512
0.91372097
0.91298640
0.91288531
0.91267914
0.91241020
0.91236305
0.91232795
0.91192359
0.91169953
0.91202891
0.91182643
INFO - Training [0][   40/  196]   Loss 1.617351   Top1 49.111328   Top5 87.880859   BatchTime 0.331867   LR 0.004995
0.91176528
0.91182041
0.91195452
0.91244292
0.91257721
0.91265213
0.91308331
0.91328913
0.91329789
0.91334516
0.91334385
0.91342497
0.91355228
0.91343004
0.91333872
0.91332334
0.91346645
0.91323906
0.91315329
INFO - Training [0][   60/  196]   Loss 1.559282   Top1 49.863281   Top5 88.580729   BatchTime 0.328731   LR 0.004989
0.91315323
0.91299587
0.91303241
0.91316468
0.91334879
0.91312653
0.91300523
0.91296077
0.91301155
0.91310066
0.91299915
0.91285110
0.91258067
0.91257244
0.91252971
0.91220421
0.91201055
0.91231024
0.91246778
0.91199797
INFO - Training [0][   80/  196]   Loss 1.510892   Top1 50.971680   Top5 89.433594   BatchTime 0.322031   LR 0.004980
0.90975904
0.91142577
0.91192681
0.91106021
0.91283190
0.91285574
0.91291898
0.91275698
0.91255760
0.91254675
0.91267824
0.91267127
0.91272682
0.91276771
0.91226983
0.91090989
0.91245675
0.91257375
0.91245395
0.91242033
0.91265070
INFO - Training [0][  100/  196]   Loss 1.460392   Top1 52.355469   Top5 90.203125   BatchTime 0.315730   LR 0.004968
0.91267741
0.91262895
0.91257209
0.91235763
0.91235876
0.91235542
0.91238248
0.91247839
0.91264367
0.91266763
0.91222036
0.91279346
0.91285872
0.91287100
0.91273981
0.91174954
0.91098112
0.91067964
0.90917653
INFO - Training [0][  120/  196]   Loss 1.420127   Top1 53.551432   Top5 90.716146   BatchTime 0.315369   LR 0.004954
0.90678364
0.90560049
0.90666038
0.90752894
0.90811270
0.90973026
0.91258508
0.91332901
0.91332424
0.91322130
0.91317677
0.91309708
0.91309649
0.91312778
0.91317356
0.91308439
0.91318822
0.91310692
0.91308683
0.91186386
INFO - Training [0][  140/  196]   Loss 1.388969   Top1 54.458705   Top5 91.146763   BatchTime 0.312863   LR 0.004938
0.91331238
0.91337490
0.91326511
0.91310489
0.91306132
0.91314232
0.91308206
0.91304159
0.91329437
0.91324574
0.91304862
0.91293806
0.91285831
0.91101146
0.91304410
0.91313595
0.91314417
0.91312116
INFO - Training [0][  160/  196]   Loss 1.367151   Top1 55.119629   Top5 91.416016   BatchTime 0.328130   LR 0.004919
0.91309762
0.91284001
0.91177416
0.91024429
0.90634573
0.90852636
0.90917403
0.90997189
0.90891123
0.90715653
0.90398961
0.89784533
0.89171666
0.88781625
0.89007741
0.89184076
0.89332289
0.89304644
0.89303559
INFO - Training [0][  180/  196]   Loss 1.350450   Top1 55.562066   Top5 91.401910   BatchTime 0.339828   LR 0.004897
0.89308876
0.89255357
0.89143181
0.89073491
0.89117175
0.89165211
0.89092886
0.89086789
0.88869530
0.89320993
0.89288700
0.89252204
0.89212334
0.89172119
0.89161468
0.89146197
0.88996542
0.89180279
INFO - ==> Top1: 56.262    Top5: 91.642    Loss: 1.330
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.89213449
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.928237   Top1 70.019531   Top5 97.324219   BatchTime 0.152945
INFO - Validation [0][   40/   40]   Loss 0.924852   Top1 70.330000   Top5 97.390000   BatchTime 0.114191
INFO - ==> Top1: 70.330    Top5: 97.390    Loss: 0.925
INFO - ==> Sparsity : 0.198
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 70.330   Top5: 97.390]
features.0.conv.0 tensor(0.6007)
features.0.conv.3 tensor(0.4238)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0751)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.0671)
features.2.conv.6 tensor(0.1027)
features.3.conv.0 tensor(0.0399)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.2335)
features.4.conv.0 tensor(0.0630)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.1055)
features.5.conv.0 tensor(0.3571)
features.5.conv.3 tensor(0.1001)
features.5.conv.6 tensor(0.1094)
features.6.conv.0 tensor(0.0529)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0877)
features.7.conv.0 tensor(0.0789)
features.7.conv.3 tensor(0.0998)
features.7.conv.6 tensor(0.1265)
features.8.conv.0 tensor(0.1322)
features.8.conv.3 tensor(0.1033)
features.8.conv.6 tensor(0.1444)
features.9.conv.0 tensor(0.1143)
features.9.conv.3 tensor(0.1166)
features.9.conv.6 tensor(0.1332)
features.10.conv.0 tensor(0.0839)
features.10.conv.3 tensor(0.0830)
features.10.conv.6 tensor(0.1051)
features.11.conv.0 tensor(0.1260)
features.11.conv.3 tensor(0.0874)
features.11.conv.6 tensor(0.1822)
features.12.conv.0 tensor(0.1636)
features.12.conv.3 tensor(0.0959)
features.12.conv.6 tensor(0.6941)
features.13.conv.0 tensor(0.0984)
features.13.conv.3 tensor(0.1192)
features.13.conv.6 tensor(0.1140)
features.14.conv.0 tensor(0.1320)
features.14.conv.3 tensor(0.0751)
features.14.conv.6 tensor(0.3233)
features.15.conv.0 tensor(0.8880)
features.15.conv.3 tensor(0.0691)
features.15.conv.6 tensor(0.2670)
features.16.conv.0 tensor(0.0748)
features.16.conv.3 tensor(0.0738)
features.16.conv.6 tensor(0.1102)
conv.0 tensor(0.0584)
tensor(434034.) 2188896.0
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.89244384
0.89282793
0.89277965
0.89275426
0.89269030
0.89288902
0.89150500
0.89236218
0.89145637
0.89002740
0.88733721
0.88071483
0.86807472
0.87206173
0.87287140
0.86989647
0.86803138
0.87094295
0.87318772
0.87702864
0.88088405
INFO - Training [1][   20/  196]   Loss 1.136294   Top1 62.734375   Top5 93.671875   BatchTime 0.426003   LR 0.004853
0.88178784
0.88170600
0.88094056
0.87912327
0.87654954
0.87365073
0.87195015
0.86956972
0.86828285
0.86994427
0.87201220
0.87277168
0.87288123
0.87224793
0.87090623
0.86995876
INFO - Training [1][   40/  196]   Loss 1.122122   Top1 63.085938   Top5 93.818359   BatchTime 0.395598   LR 0.004825
0.86997002
0.87008238
0.87014097
0.87063384
0.87032777
0.86991256
0.86976820
0.87033516
0.87108135
0.87072301
0.86968410
0.87020028
0.86850488
0.86318356
0.85313177
0.84778059
0.84454131
0.84370601
0.84457785
0.84749305
0.85246599
INFO - Training [1][   60/  196]   Loss 1.120949   Top1 63.085938   Top5 93.886719   BatchTime 0.389300   LR 0.004794
0.85861927
0.86363101
0.86731231
0.86980361
0.87174225
0.87157691
0.87120533
0.87143552
0.87156367
0.87149465
0.87136477
0.87126118
0.87156332
0.87231362
0.87256300
0.87276733
0.87271011
0.86677331
0.86489093
0.87285644
0.87302738
0.87329704
0.87361526
INFO - Training [1][   80/  196]   Loss 1.106637   Top1 63.569336   Top5 94.091797   BatchTime 0.381866   LR 0.004761
0.87385422
0.87412965
0.87465906
0.87471932
0.87512851
0.87539345
0.87556338
0.87570405
0.87563926
0.87550628
0.87519521
0.87501019
0.87447578
0.87307692
0.87098783
0.86770380
0.86934394
INFO - Training [1][  100/  196]   Loss 1.093952   Top1 63.976562   Top5 94.300781   BatchTime 0.372193   LR 0.004725
0.87208784
0.87113130
0.86943585
0.86396766
0.86998379
0.87002140
0.86959785
0.86934429
0.86892736
0.86857849
0.86842668
0.86802989
0.86755675
0.86747146
0.86726153
0.86696851
0.86663395
0.86623287
0.86625761
0.86631554
INFO - Training [1][  120/  196]   Loss 1.082156   Top1 64.430339   Top5 94.475911   BatchTime 0.361694   LR 0.004687
0.86621130
0.86591470
0.86569309
0.86550510
0.86511534
0.86430734
0.86371845
0.86122286
0.85737002
0.85369754
0.85090584
0.84951502
0.85123605
0.85425401
0.85803741
0.85974652
0.86033171
0.85854441
0.86233795
0.86206961
INFO - Training [1][  140/  196]   Loss 1.068323   Top1 64.902344   Top5 94.704241   BatchTime 0.352923   LR 0.004647
0.86181504
0.86192209
0.86157978
0.86187649
0.86174971
0.86379844
0.86516291
0.86545175
0.86696190
0.86876231
0.85984439
0.86601305
0.86871445
0.86979216
0.87194270
0.87506348
0.87539697
0.87553787
0.87559533
0.87544286
0.87542319
INFO - Training [1][  160/  196]   Loss 1.061843   Top1 65.053711   Top5 94.821777   BatchTime 0.356803   LR 0.004605
0.87506717
0.87493074
0.87490076
0.87478375
0.87467462
0.87454343
0.87440443
0.87411046
0.87411541
0.87425953
0.87388605
0.87358725
0.87349546
0.87311840
0.87285519
0.87250006
0.87216061
0.87202835
0.87163216
0.87128687
0.87080097
0.86963212
INFO - Training [1][  180/  196]   Loss 1.050016   Top1 65.373264   Top5 94.904514   BatchTime 0.358230   LR 0.004560
0.86757511
0.86228234
0.84698826
0.85143775
0.84177572
0.84187013
0.83978081
0.83997047
0.83982086
0.83947617
0.83910674
0.83865684
0.83791560
0.83706862
0.83499461
********************pre-trained*****************
INFO - ==> Top1: 65.564    Top5: 94.916    Loss: 1.043
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [1][   20/   40]   Loss 1.434128   Top1 55.351562   Top5 92.656250   BatchTime 0.116341
INFO - Validation [1][   40/   40]   Loss 1.458105   Top1 55.270000   Top5 92.400000   BatchTime 0.085148
INFO - ==> Top1: 55.270    Top5: 92.400    Loss: 1.458
INFO - ==> Sparsity : 0.317
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 70.330   Top5: 97.390]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 55.270   Top5: 92.400]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.6007)
features.0.conv.3 tensor(0.4258)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0738)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.1021)
features.3.conv.0 tensor(0.0460)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.2062)
features.4.conv.0 tensor(0.0576)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.1027)
features.5.conv.0 tensor(0.3210)
features.5.conv.3 tensor(0.0995)
features.5.conv.6 tensor(0.1060)
features.6.conv.0 tensor(0.0527)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0909)
features.7.conv.0 tensor(0.0848)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.2864)
features.8.conv.0 tensor(0.1260)
features.8.conv.3 tensor(0.1088)
features.8.conv.6 tensor(0.1418)
features.9.conv.0 tensor(0.1072)
features.9.conv.3 tensor(0.1305)
features.9.conv.6 tensor(0.1477)
features.10.conv.0 tensor(0.0856)
features.10.conv.3 tensor(0.0816)
features.10.conv.6 tensor(0.0927)
features.11.conv.0 tensor(0.1545)
features.11.conv.3 tensor(0.0914)
features.11.conv.6 tensor(0.1755)
features.12.conv.0 tensor(0.4126)
features.12.conv.3 tensor(0.0966)
features.12.conv.6 tensor(0.6541)
features.13.conv.0 tensor(0.0970)
features.13.conv.3 tensor(0.1381)
features.13.conv.6 tensor(0.1247)
features.14.conv.0 tensor(0.8592)
features.14.conv.3 tensor(0.0881)
features.14.conv.6 tensor(0.3399)
features.15.conv.0 tensor(0.9204)
features.15.conv.3 tensor(0.0719)
features.15.conv.6 tensor(1.0000)
features.16.conv.0 tensor(0.0693)
features.16.conv.3 tensor(0.0868)
features.16.conv.6 tensor(0.1248)
conv.0 tensor(0.0735)
tensor(693006.) 2188896.0
0.83178163
0.83076543
0.82830393
0.82496870
0.82250273
0.82117987
0.81825018
0.81626755
0.81195128
0.81261563
0.81510961
0.81591755
0.81691623
0.81849515
0.82027024
0.82093179
0.82118618
0.82135308
0.82161289
0.82146454
INFO - Training [2][   20/  196]   Loss 0.977011   Top1 67.890625   Top5 94.804688   BatchTime 0.453865   LR 0.004477
0.82228577
0.82351959
0.82600313
0.82743406
0.82834911
0.82917279
0.82984459
0.83069855
0.83261847
0.83359081
0.83403212
0.83396596
0.83287078
0.83310032
0.83229506
0.83382368
0.83480322
0.83612782
0.83621573
0.83603984
0.83579534
INFO - Training [2][   40/  196]   Loss 0.985617   Top1 67.607422   Top5 95.068359   BatchTime 0.413919   LR 0.004426
0.83529991
0.83240724
0.82761449
0.83579844
0.83941096
0.84171849
0.84376562
0.84484327
0.84536707
0.84565216
0.84654546
0.84726316
0.84696871
0.84684390
0.84694725
0.84752095
0.84749603
INFO - Training [2][   60/  196]   Loss 0.967201   Top1 68.085938   Top5 95.371094   BatchTime 0.399649   LR 0.004374
0.84760022
0.84731889
0.84524542
0.84371030
0.84261966
0.84220845
0.84076297
0.85964829
0.86568594
0.86237401
0.86152947
0.85633481
0.84897095
0.85746473
0.86382467
0.86551523
0.86528409
0.86280876
0.86010134
0.85027415
0.84982985
INFO - Training [2][   80/  196]   Loss 0.960701   Top1 68.457031   Top5 95.590820   BatchTime 0.395136   LR 0.004320
0.85381997
0.85919666
0.86181521
0.86358309
0.86532676
0.86698329
0.86655784
0.86581051
0.86473477
0.86445814
0.86455876
0.86449879
0.86469501
0.86489058
0.86506689
0.86533397
0.86495924
0.86469734
0.86423922
INFO - Training [2][  100/  196]   Loss 0.949220   Top1 68.761719   Top5 95.734375   BatchTime 0.396407   LR 0.004264
0.86448753
0.86622626
0.86885250
0.86905718
0.86828309
0.86604148
0.86979169
0.87088484
0.87195390
0.87359166
0.87501484
0.87552774
0.87526339
0.87438774
0.87333614
0.87228286
0.87146175
0.87063509
0.86987561
INFO - Training [2][  120/  196]   Loss 0.936864   Top1 69.205729   Top5 95.852865   BatchTime 0.384400   LR 0.004206
0.86903638
0.86843967
0.86807358
0.86773646
0.86717200
0.86667776
0.86641568
0.86642414
0.86646634
0.86626291
0.86600995
0.86562157
0.86546528
0.86547297
0.86512947
0.86494052
0.86445504
0.86378366
0.86342061
0.86302477
INFO - Training [2][  140/  196]   Loss 0.933415   Top1 69.218750   Top5 95.943080   BatchTime 0.372179   LR 0.004146
0.86234486
0.86096084
0.85743064
0.85148185
0.84174997
0.83663291
0.83562171
0.83362961
0.81832236
0.83352661
0.83268875
0.83352315
0.84086305
0.85018849
0.85794777
0.86224627
0.86465287
0.86668062
0.86684591
0.86677033
0.86675715
0.86658984
0.86663002
0.86652988
INFO - Training [2][  160/  196]   Loss 0.934604   Top1 69.191895   Top5 95.947266   BatchTime 0.366690   LR 0.004085
0.86641657
0.86634439
0.86623412
0.86616594
0.86599272
0.86583912
0.86575204
0.86418599
0.86402923
0.86362678
0.86312056
0.86450708
0.86421633
0.86479431
0.86423391
0.86381412
INFO - Training [2][  180/  196]   Loss 0.931072   Top1 69.320747   Top5 95.930990   BatchTime 0.368073   LR 0.004022
0.86325610
0.86297977
0.86270356
0.86229521
0.86233008
0.86222100
0.86220151
0.86197990
0.86178660
0.86169648
0.86154842
0.86137021
0.86120665
0.86110830
0.86060268
0.85978222
INFO - ==> Top1: 69.422    Top5: 95.984    Loss: 0.927
0.85914665
0.85853440
0.85800380
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.649223   Top1 77.539062   Top5 98.671875   BatchTime 0.130984
INFO - Validation [2][   40/   40]   Loss 0.641239   Top1 77.720000   Top5 98.770000   BatchTime 0.093499
INFO - ==> Top1: 77.720    Top5: 98.770    Loss: 0.641
INFO - ==> Sparsity : 0.271
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 77.720   Top5: 98.770]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 70.330   Top5: 97.390]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 55.270   Top5: 92.400]
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.4102)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0524)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.1010)
features.3.conv.0 tensor(0.0475)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.2190)
features.4.conv.0 tensor(0.0592)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.1121)
features.5.conv.0 tensor(0.3236)
features.5.conv.3 tensor(0.0926)
features.5.conv.6 tensor(0.1056)
features.6.conv.0 tensor(0.0475)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0920)
features.7.conv.0 tensor(0.0949)
features.7.conv.3 tensor(0.1195)
features.7.conv.6 tensor(0.1302)
features.8.conv.0 tensor(0.1104)
features.8.conv.3 tensor(0.1088)
features.8.conv.6 tensor(0.2649)
features.9.conv.0 tensor(0.1210)
features.9.conv.3 tensor(0.1343)
features.9.conv.6 tensor(0.1399)
features.10.conv.0 tensor(0.0715)
features.10.conv.3 tensor(0.0929)
features.10.conv.6 tensor(0.0921)
features.11.conv.0 tensor(0.1187)
features.11.conv.3 tensor(0.1233)
features.11.conv.6 tensor(0.1784)
features.12.conv.0 tensor(0.3388)
features.12.conv.3 tensor(0.1101)
features.12.conv.6 tensor(0.6732)
features.13.conv.0 tensor(0.0941)
features.13.conv.3 tensor(0.1443)
features.13.conv.6 tensor(0.1066)
features.14.conv.0 tensor(0.8874)
features.14.conv.3 tensor(0.0895)
features.14.conv.6 tensor(0.3743)
features.15.conv.0 tensor(0.9087)
features.15.conv.3 tensor(0.0736)
features.15.conv.6 tensor(0.2215)
features.16.conv.0 tensor(0.0744)
features.16.conv.3 tensor(0.0899)
features.16.conv.6 tensor(0.1582)
conv.0 tensor(0.0954)
tensor(593962.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.85734355
0.85722893
0.85757524
0.85755336
0.85678875
0.85583079
0.85455322
0.85316557
0.85260242
0.85275680
0.85349971
0.85418499
0.85519266
0.85700041
0.85978675
0.86109352
0.86151975
0.86183596
0.86217135
0.86248529
0.86251909
INFO - Training [3][   20/  196]   Loss 0.899869   Top1 69.824219   Top5 95.761719   BatchTime 0.440695   LR 0.003907
0.86243123
0.86258274
0.86235291
0.86157513
0.86069578
0.85915428
0.85947114
0.86039662
0.86083603
0.86192065
0.86290669
0.86339813
0.86364400
0.86399740
0.86450052
0.86463827
INFO - Training [3][   40/  196]   Loss 0.890098   Top1 70.693359   Top5 95.810547   BatchTime 0.413371   LR 0.003840
0.86491418
0.86537498
0.86592734
0.86683422
0.86689585
0.86654383
0.86617339
0.86577052
0.86565661
0.86508816
0.86487907
0.86455011
0.86462057
0.86422163
0.86404383
0.86402047
0.86394167
0.86360294
0.86320424
0.86280710
0.86271477
0.86266100
INFO - Training [3][   60/  196]   Loss 0.878094   Top1 71.282552   Top5 95.970052   BatchTime 0.396689   LR 0.003771
0.86290616
0.86306566
0.86263871
0.86107796
0.84374201
0.83585960
0.84947169
0.86406052
0.86413157
0.86442471
0.86466825
0.86461920
0.86448902
0.86480772
0.86478442
0.86481398
0.86488962
0.86491543
0.86462134
0.86432493
0.86408979
0.86352402
INFO - Training [3][   80/  196]   Loss 0.873506   Top1 71.420898   Top5 96.108398   BatchTime 0.387183   LR 0.003701
0.86342615
0.86321849
0.86273223
0.86244076
0.86219823
0.86194891
0.86183262
0.86178750
0.86145908
0.86092997
0.86042500
0.85978514
0.85942143
0.85916686
0.85868657
0.85818535
INFO - Training [3][  100/  196]   Loss 0.866945   Top1 71.683594   Top5 96.218750   BatchTime 0.382987   LR 0.003630
0.85792023
0.85765952
0.85728288
0.85592127
0.85529035
0.85468340
0.85552937
0.85487986
0.85444820
0.85390347
0.85353100
0.85186690
0.85068691
0.84875554
0.84571606
0.84646076
0.84586596
0.84453827
0.84267372
0.83995980
0.83837658
0.83691162
0.83661926
0.83692420
0.83697367
INFO - Training [3][  120/  196]   Loss 0.857317   Top1 71.998698   Top5 96.344401   BatchTime 0.373244   LR 0.003558
0.83624327
0.83517444
0.83426106
0.83477920
0.83483279
0.83429879
0.83395171
0.83345741
0.83336979
0.83326721
0.83312845
0.83300209
0.83278871
0.83251470
0.83202618
0.83138520
0.83065498
INFO - Training [3][  140/  196]   Loss 0.855657   Top1 71.992188   Top5 96.422991   BatchTime 0.370719   LR 0.003484
0.82993370
0.82925284
0.82831937
0.82748139
0.82665199
0.82609940
0.82561427
0.82441109
0.82385868
0.82303059
0.82317090
0.82350320
0.82345319
0.82310116
0.82265157
0.82219839
0.82299536
0.82225889
0.82144332
0.82011402
0.81869853
INFO - Training [3][  160/  196]   Loss 0.858279   Top1 71.928711   Top5 96.425781   BatchTime 0.370639   LR 0.003410
0.81653875
0.81391662
0.80810016
0.81065625
0.81267291
0.81226552
0.81169534
0.81059891
0.81022930
0.80977857
0.81124395
0.81154782
0.81283581
0.81580126
0.81828070
0.82015133
INFO - Training [3][  180/  196]   Loss 0.855724   Top1 72.016059   Top5 96.388889   BatchTime 0.371137   LR 0.003335
0.82209730
0.82384312
0.82586652
0.82749265
0.82987767
0.83197647
0.83546031
0.83806533
0.83805871
0.83622396
0.83403635
0.83299339
0.82934999
0.82458711
0.82033694
0.82420021
0.82405305
0.82550114
0.82546681
0.82662028
INFO - ==> Top1: 72.136    Top5: 96.452    Loss: 0.851
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.719594   Top1 76.777344   Top5 98.125000   BatchTime 0.127326
features.0.conv.0 tensor(0.5868)
features.0.conv.3 tensor(0.4141)
features.1.conv.0 tensor(0.0371)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.0679)
features.2.conv.6 tensor(0.1033)
features.3.conv.0 tensor(0.0475)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2261)
features.4.conv.0 tensor(0.0571)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.1034)
features.5.conv.0 tensor(0.3066)
features.5.conv.3 tensor(0.1007)
features.5.conv.6 tensor(0.1043)
features.6.conv.0 tensor(0.0448)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0894)
features.7.conv.0 tensor(0.0970)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.1602)
features.8.conv.0 tensor(0.1121)
features.8.conv.3 tensor(0.1065)
features.8.conv.6 tensor(0.1267)
features.9.conv.0 tensor(0.1054)
features.9.conv.3 tensor(0.1432)
features.9.conv.6 tensor(0.1371)
features.10.conv.0 tensor(0.0720)
features.10.conv.3 tensor(0.0888)
features.10.conv.6 tensor(0.0873)
features.11.conv.0 tensor(0.4805)
features.11.conv.3 tensor(0.1221)
features.11.conv.6 tensor(0.2012)
features.12.conv.0 tensor(0.1731)
features.12.conv.3 tensor(0.1123)
features.12.conv.6 tensor(0.7214)
features.13.conv.0 tensor(0.0956)
features.13.conv.3 tensor(0.1539)
features.13.conv.6 tensor(0.1435)
features.14.conv.0 tensor(0.9097)
features.14.conv.3 tensor(0.0920)
features.14.conv.6 tensor(0.8524)
features.15.conv.0 tensor(0.9194)
features.15.conv.3 tensor(0.0719)
features.15.conv.6 tensor(0.2455)
features.16.conv.0 tensor(0.0812)
features.16.conv.3 tensor(0.0897)
features.16.conv.6 tensor(0.1868)
conv.0 tensor(0.1232)
tensor(712366.)
INFO - Validation [3][   40/   40]   Loss 0.708344   Top1 76.640000   Top5 98.210000   BatchTime 0.092641
INFO - ==> Top1: 76.640    Top5: 98.210    Loss: 0.708
INFO - ==> Sparsity : 0.325
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 77.720   Top5: 98.770]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 76.640   Top5: 98.210]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 70.330   Top5: 97.390]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   4
tensor(712366.) 2188896.0
0.82767236
0.82823956
0.82881880
0.82795888
0.82742143
0.82645810
0.82464623
0.82255578
0.82181960
0.82136220
0.82185853
0.82290381
0.82599032
0.82784116
0.82947528
0.83061254
0.83161974
0.83251357
INFO - Training [4][   20/  196]   Loss 0.845025   Top1 72.050781   Top5 96.132812   BatchTime 0.442661   LR 0.003200
0.83307022
0.83366263
0.83423954
0.83445817
0.83484179
0.83538693
0.83600020
0.83618116
0.83621705
0.83626962
0.83642089
0.83656079
0.83666790
0.83667129
0.83680391
0.83839214
0.83839279
0.83817393
0.83776134
0.83744812
INFO - Training [4][   40/  196]   Loss 0.836292   Top1 72.656250   Top5 96.386719   BatchTime 0.415747   LR 0.003122
0.83721137
0.83720124
0.83707219
0.83713925
0.83732229
0.83737236
0.83772570
0.83781725
0.83775127
0.83756047
0.83728635
0.83717477
0.83707887
0.83676201
0.83660311
0.83651185
0.83613354
0.83591813
0.83574307
0.83529139
0.83510423
INFO - Training [4][   60/  196]   Loss 0.825427   Top1 72.838542   Top5 96.516927   BatchTime 0.402380   LR 0.003044
0.83485782
0.83493328
0.83466047
0.83438379
0.83413595
0.83399689
0.83392835
0.83369905
0.83352029
0.83323908
0.83318126
0.83334708
0.83333528
0.83343333
0.83326668
0.83320010
0.83301061
0.83291221
0.83301610
0.83293873
0.83274305
0.83262008
INFO - Training [4][   80/  196]   Loss 0.818837   Top1 73.110352   Top5 96.772461   BatchTime 0.392432   LR 0.002965
0.83248949
0.83274424
0.83279681
0.83273709
0.83275419
0.83275771
0.83281755
0.83290589
0.83277732
0.83305889
0.83316320
0.83321929
0.83329248
0.83341181
0.83360296
0.83374953
0.83376777
0.83381993
INFO - Training [4][  100/  196]   Loss 0.805414   Top1 73.609375   Top5 96.867188   BatchTime 0.381719   LR 0.002886
0.83397299
0.83435363
0.83545125
0.83512336
0.83456343
0.83416075
0.83375621
0.83322817
0.83219749
0.83102661
0.82941139
0.83117586
0.83147246
0.83218980
0.83290231
0.83377498
0.83425260
0.83455616
INFO - Training [4][  120/  196]   Loss 0.798377   Top1 73.844401   Top5 96.966146   BatchTime 0.375515   LR 0.002806
0.83432454
0.83418542
0.83414471
0.83379984
0.83323532
0.83299994
0.83259559
0.83237356
0.83226967
0.83199632
0.83176857
0.83170360
0.83152634
0.83104360
0.83063382
0.83059335
0.83054334
0.83023590
0.82983869
0.82977384
0.82987708
INFO - Training [4][  140/  196]   Loss 0.792535   Top1 74.037388   Top5 97.025670   BatchTime 0.375686   LR 0.002726
0.82991058
0.82997537
0.83042401
0.83092040
0.83116519
0.83271646
0.83197051
0.83122891
0.82972509
0.82914788
0.82794505
0.82484680
0.82255250
0.81999952
0.81772625
0.81425881
0.81537718
0.81526130
0.81365913
0.81448448
INFO - Training [4][  160/  196]   Loss 0.793443   Top1 74.074707   Top5 96.997070   BatchTime 0.378125   LR 0.002646
0.81395811
0.81329882
0.81248689
0.81047463
0.80761671
0.80666912
0.80743653
0.80864930
0.80908120
0.80880642
0.80907393
0.80895084
0.80902416
0.80948156
0.81018847
0.81086814
0.81086689
0.81100935
0.81053299
0.81026590
0.81046325
0.81158394
INFO - Training [4][  180/  196]   Loss 0.789635   Top1 74.153646   Top5 96.970486   BatchTime 0.377906   LR 0.002566
0.81128365
0.81024224
0.81024438
0.81026417
0.80976713
0.80927801
0.80872589
0.80802107
0.80735743
0.80635285
0.80379671
0.80105007
0.79846960
0.80004829
0.80015802
0.80020636
INFO - ==> Top1: 74.232    Top5: 96.984    Loss: 0.786
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 0.591085   Top1 80.019531   Top5 98.554688   BatchTime 0.124086
features.0.conv.0 tensor(0.5903)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0365)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0638)
features.2.conv.0 tensor(0.0483)
features.2.conv.3 tensor(0.0687)
features.2.conv.6 tensor(0.1013)
features.3.conv.0 tensor(0.0434)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2177)
features.4.conv.0 tensor(0.0552)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.1003)
features.5.conv.0 tensor(0.3019)
features.5.conv.3 tensor(0.0990)
features.5.conv.6 tensor(0.1066)
features.6.conv.0 tensor(0.0381)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0859)
features.7.conv.0 tensor(0.1022)
features.7.conv.3 tensor(0.1117)
features.7.conv.6 tensor(0.1685)
features.8.conv.0 tensor(0.1183)
features.8.conv.3 tensor(0.1137)
features.8.conv.6 tensor(0.1530)
features.9.conv.0 tensor(0.1150)
features.9.conv.3 tensor(0.1438)
features.9.conv.6 tensor(0.1449)
features.10.conv.0 tensor(0.0652)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.0840)
features.11.conv.0 tensor(0.1245)
features.11.conv.3 tensor(0.1360)
features.11.conv.6 tensor(0.6244)
features.12.conv.0 tensor(0.7315)
features.12.conv.3 tensor(0.1127)
features.12.conv.6 tensor(0.6526)
features.13.conv.0 tensor(0.1104)
features.13.conv.3 tensor(0.1507)
features.13.conv.6 tensor(0.1658)
features.14.conv.0 tensor(0.9233)
features.14.conv.3 tensor(0.0940)
features.14.conv.6 tensor(0.9409)
features.15.conv.0 tensor(0.9198)
features.15.conv.3 tensor(0.0753)
features.15.conv.6 tensor(0.2483)
features.16.conv.0 tensor(0.0889)
features.16.conv.3 tensor(0.0935)
features.16.conv.6 tensor(0.1905)
conv.0 tensor(0.1291)
tensor(768095.) 2188896.0
INFO - Validation [4][   40/   40]   Loss 0.575301   Top1 80.620000   Top5 98.680000   BatchTime 0.088756
INFO - ==> Top1: 80.620    Top5: 98.680    Loss: 0.575
INFO - ==> Sparsity : 0.351
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 80.620   Top5: 98.680]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 77.720   Top5: 98.770]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 76.640   Top5: 98.210]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.79738915
0.79526442
0.79744750
0.79752976
0.79828995
0.79827553
0.79849207
0.79815799
0.79776013
0.79701829
0.79670382
0.79577762
0.79456377
0.79334694
0.79220045
0.79297960
0.79256970
0.79201758
0.79176497
INFO - Training [5][   20/  196]   Loss 0.754766   Top1 75.312500   Top5 96.503906   BatchTime 0.437212   LR 0.002424
0.79193908
0.79179507
0.79229170
0.79293823
0.79356962
0.79417330
0.79452717
0.79444653
0.79454184
0.79471993
0.79537958
0.79574180
0.79650944
0.79686093
0.79776472
0.79907966
0.80019301
0.80252850
0.80517834
0.80582505
0.80695850
0.80800855
0.80931038
INFO - Training [5][   40/  196]   Loss 0.766011   Top1 75.126953   Top5 96.796875   BatchTime 0.399972   LR 0.002343
0.81014127
0.81112558
0.81182116
0.81249684
0.81318980
0.81397796
0.81443167
0.81463301
0.81491786
0.81482965
0.81466466
0.81401724
0.81416637
0.81562763
0.81729442
0.81785727
INFO - Training [5][   60/  196]   Loss 0.754218   Top1 75.371094   Top5 96.985677   BatchTime 0.388620   LR 0.002263
0.81885296
0.81867397
0.81843460
0.81855088
0.81871796
0.81843257
0.81827801
0.81810665
0.81812775
0.81805420
0.81862098
0.82013482
0.82018411
0.82024968
0.82022601
0.82041943
0.82051337
0.82045543
0.82054049
0.82057136
0.82048273
0.82052433
INFO - Training [5][   80/  196]   Loss 0.756940   Top1 75.195312   Top5 97.099609   BatchTime 0.383642   LR 0.002183
0.82049066
0.82054532
0.82058275
0.82041949
0.82031661
0.82025331
0.82023519
0.82022637
0.82012874
0.81984079
0.81933045
0.81864995
0.81777149
0.81548101
0.81215560
0.81503880
0.81635755
0.81585068
INFO - Training [5][  100/  196]   Loss 0.747913   Top1 75.523438   Top5 97.226562   BatchTime 0.370660   LR 0.002104
0.81519532
0.81441605
0.81341588
0.81151825
0.80793649
0.80240273
0.79208827
0.78970659
0.78912485
0.79533315
0.80766541
0.81298482
0.81396776
0.81523752
0.81506515
0.81485134
0.81453395
0.81442034
0.81432140
0.81399912
0.81375831
0.81339759
INFO - Training [5][  120/  196]   Loss 0.744348   Top1 75.742188   Top5 97.265625   BatchTime 0.371262   LR 0.002024
0.81318527
0.81316131
0.81299061
0.81275433
0.81244069
0.81234699
0.81211996
0.81185102
0.81150681
0.81111449
0.81058919
0.81027627
0.81001538
0.80953842
0.80900866
0.80850762
0.80808979
INFO - Training [5][  140/  196]   Loss 0.741058   Top1 75.884487   Top5 97.301897   BatchTime 0.370449   LR 0.001946
0.80759978
0.80703920
0.80624527
0.80558711
0.80480230
0.80422527
0.80409020
0.80363446
0.80311948
0.80242467
0.80215508
0.80160618
0.80112445
0.80089772
0.80044138
0.80017817
0.80004370
0.80005831
0.79961169
0.79942983
0.79913974
INFO - Training [5][  160/  196]   Loss 0.743332   Top1 75.812988   Top5 97.290039   BatchTime 0.371409   LR 0.001868
0.79878944
0.79893315
0.79901665
0.79884714
0.79872298
0.79871446
0.79823852
0.79773229
0.79744488
0.79717058
0.79725152
0.79730976
0.79712784
0.79713041
0.79698628
0.79661888
0.79594284
0.79539317
0.79519230
0.79546887
0.79508066
INFO - Training [5][  180/  196]   Loss 0.741017   Top1 75.883247   Top5 97.200521   BatchTime 0.372167   LR 0.001790
0.79438359
0.79435235
0.79471231
0.79474908
0.79418737
0.79396778
0.79407310
0.79418021
0.79429549
0.79451776
0.79513812
0.79579693
0.79638010
0.79742861
0.79775345
0.79751980
INFO - ==> Top1: 75.920    Top5: 97.192    Loss: 0.739
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79746521
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.616378   Top1 79.316406   Top5 98.671875   BatchTime 0.129946
INFO - Validation [5][   40/   40]   Loss 0.617281   Top1 79.360000   Top5 98.640000   BatchTime 0.092724
INFO - ==> Top1: 79.360    Top5: 98.640    Loss: 0.617
INFO - ==> Sparsity : 0.350
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 80.620   Top5: 98.680]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 79.360   Top5: 98.640]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 77.720   Top5: 98.770]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.4297)
features.1.conv.0 tensor(0.0371)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0621)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0987)
features.3.conv.0 tensor(0.0446)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.2220)
features.4.conv.0 tensor(0.0617)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.1007)
features.5.conv.0 tensor(0.3044)
features.5.conv.3 tensor(0.1030)
features.5.conv.6 tensor(0.1414)
features.6.conv.0 tensor(0.0358)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0851)
features.7.conv.0 tensor(0.1032)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.1863)
features.8.conv.0 tensor(0.1093)
features.8.conv.3 tensor(0.1126)
features.8.conv.6 tensor(0.2157)
features.9.conv.0 tensor(0.1174)
features.9.conv.3 tensor(0.1435)
features.9.conv.6 tensor(0.1971)
features.10.conv.0 tensor(0.0611)
features.10.conv.3 tensor(0.0932)
features.10.conv.6 tensor(0.0942)
features.11.conv.0 tensor(0.2600)
features.11.conv.3 tensor(0.1331)
features.11.conv.6 tensor(0.4315)
features.12.conv.0 tensor(0.6009)
features.12.conv.3 tensor(0.1204)
features.12.conv.6 tensor(0.7061)
features.13.conv.0 tensor(0.1029)
features.13.conv.3 tensor(0.1503)
features.13.conv.6 tensor(0.1754)
features.14.conv.0 tensor(0.9345)
features.14.conv.3 tensor(0.0929)
features.14.conv.6 tensor(0.8812)
features.15.conv.0 tensor(0.9248)
features.15.conv.3 tensor(0.0778)
features.15.conv.6 tensor(0.2557)
features.16.conv.0 tensor(0.1003)
features.16.conv.3 tensor(0.0910)
features.16.conv.6 tensor(0.1744)
conv.0 tensor(0.1525)
tensor(765675.) 2188896.0
0.79769975
0.79833096
0.79815823
0.79774141
0.79711205
0.79658586
0.79569495
0.79485554
0.79471970
0.79450589
0.79424137
0.79394561
0.79401624
0.79386181
0.79351598
0.79323322
0.79331613
INFO - Training [6][   20/  196]   Loss 0.723852   Top1 76.152344   Top5 96.660156   BatchTime 0.425997   LR 0.001655
0.79341096
0.79337645
0.79350692
0.79369032
0.79403561
0.79448354
0.79505479
0.79578424
0.79651123
0.79730797
0.79962873
0.80033988
0.80024517
0.79990923
0.79959565
0.79942006
0.79961699
0.79946244
0.79957783
0.80056548
0.80279744
INFO - Training [6][   40/  196]   Loss 0.729894   Top1 76.220703   Top5 96.953125   BatchTime 0.405322   LR 0.001580
0.80193454
0.80118424
0.79972869
0.79829764
0.79657400
0.79426116
0.79437929
0.79320389
0.79512626
0.79695827
0.79788756
0.79883349
0.79931551
0.79920805
0.79888308
0.79925245
0.79971248
0.80001116
0.80027252
0.80067855
0.80077505
INFO - Training [6][   60/  196]   Loss 0.718034   Top1 76.490885   Top5 97.128906   BatchTime 0.395512   LR 0.001506
0.80069554
0.80034214
0.80026543
0.80045944
0.80061096
0.80052537
0.80047089
0.80043817
0.80049872
0.80038345
0.80046922
0.80067825
0.80102831
0.80148846
0.80206233
0.80260736
0.80401945
0.80437994
0.80453157
0.80449593
0.80424142
INFO - Training [6][   80/  196]   Loss 0.714822   Top1 76.533203   Top5 97.211914   BatchTime 0.391604   LR 0.001432
0.80392033
0.80393857
0.80404222
0.80460864
0.80465233
0.80458689
0.80439502
0.80418801
0.80385703
0.80367237
0.80340463
0.80311424
0.80278003
0.80250031
0.80227953
0.80175275
0.80090159
0.80022699
INFO - Training [6][  100/  196]   Loss 0.708857   Top1 76.796875   Top5 97.273438   BatchTime 0.383709   LR 0.001360
0.79974520
0.79941458
0.79923689
0.79896933
0.79866135
0.79829580
0.79799104
0.79749072
0.79728758
0.79712397
0.79730761
0.79741395
0.79754102
0.79779643
0.79957920
0.79954714
0.79952788
0.79945654
0.79951370
INFO - Training [6][  120/  196]   Loss 0.700611   Top1 77.031250   Top5 97.434896   BatchTime 0.386150   LR 0.001289
0.79946005
0.79944140
0.79931504
0.79938084
0.79928446
0.79932237
0.79925740
0.79928702
0.79920316
0.79915547
0.79920375
0.79915822
0.79909712
0.79893166
0.79900730
0.79884416
0.79849637
0.79823548
0.79799169
0.79790461
0.79769754
0.79756033
INFO - Training [6][  140/  196]   Loss 0.698556   Top1 77.101004   Top5 97.466518   BatchTime 0.384041   LR 0.001220
0.79728371
0.79714161
0.79687572
0.79673398
0.79655457
0.79645866
0.79642731
0.79651213
0.79658812
0.79660267
0.79669607
0.79645342
0.79630518
0.79618460
0.79590505
0.79567069
0.79526371
0.79483503
0.79433799
0.79381526
0.79357755
INFO - Training [6][  160/  196]   Loss 0.699968   Top1 77.023926   Top5 97.443848   BatchTime 0.383844   LR 0.001151
0.79344368
0.79322904
0.79328716
0.79300386
0.79279143
0.79253948
0.79218888
0.79206043
0.79189128
0.79185331
0.79189807
0.79209000
0.79210800
0.79214627
0.79193205
0.79176766
0.79166371
0.79154718
0.79137230
0.79101580
0.79077393
0.79051179
INFO - Training [6][  180/  196]   Loss 0.699491   Top1 77.078993   Top5 97.417535   BatchTime 0.382101   LR 0.001084
0.79034346
0.79024976
0.79005235
0.78976208
0.78919387
0.78896177
0.78891766
0.78826183
0.78777903
0.78739035
0.78715336
0.78667104
0.78657103
0.78654879
********************pre-trained*****************
INFO - ==> Top1: 77.200    Top5: 97.424    Loss: 0.698
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.580947   Top1 80.468750   Top5 98.613281   BatchTime 0.123051
INFO - Validation [6][   40/   40]   Loss 0.576368   Top1 80.560000   Top5 98.690000   BatchTime 0.093398
INFO - ==> Top1: 80.560    Top5: 98.690    Loss: 0.576
INFO - ==> Sparsity : 0.362
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 80.620   Top5: 98.680]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 80.560   Top5: 98.690]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 79.360   Top5: 98.640]
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.4121)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0638)
features.2.conv.0 tensor(0.0472)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0978)
features.3.conv.0 tensor(0.0475)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.2205)
features.4.conv.0 tensor(0.0708)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.0993)
features.5.conv.0 tensor(0.3088)
features.5.conv.3 tensor(0.1047)
features.5.conv.6 tensor(0.1286)
features.6.conv.0 tensor(0.0329)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0850)
features.7.conv.0 tensor(0.0957)
features.7.conv.3 tensor(0.1108)
features.7.conv.6 tensor(0.1943)
features.8.conv.0 tensor(0.2948)
features.8.conv.3 tensor(0.1131)
features.8.conv.6 tensor(0.1241)
features.9.conv.0 tensor(0.1212)
features.9.conv.3 tensor(0.1383)
features.9.conv.6 tensor(0.2035)
features.10.conv.0 tensor(0.0612)
features.10.conv.3 tensor(0.0917)
features.10.conv.6 tensor(0.0847)
features.11.conv.0 tensor(0.4619)
features.11.conv.3 tensor(0.1346)
features.11.conv.6 tensor(0.5194)
features.12.conv.0 tensor(0.5748)
features.12.conv.3 tensor(0.1169)
features.12.conv.6 tensor(0.7298)
features.13.conv.0 tensor(0.1204)
features.13.conv.3 tensor(0.1483)
features.13.conv.6 tensor(0.1837)
features.14.conv.0 tensor(0.9405)
features.14.conv.3 tensor(0.0907)
features.14.conv.6 tensor(0.9372)
features.15.conv.0 tensor(0.9270)
features.15.conv.3 tensor(0.0771)
features.15.conv.6 tensor(0.2588)
features.16.conv.0 tensor(0.1000)
features.16.conv.3 tensor(0.0907)
features.16.conv.6 tensor(0.1350)
conv.0 tensor(0.1753)
tensor(792983.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.78745323
0.78811377
0.78834671
0.78863466
0.78892058
0.78907382
0.78911167
0.78913271
0.78905731
0.78911924
0.78931910
0.78923565
0.78906959
0.78890175
0.78882635
0.78863329
0.78835201
INFO - Training [7][   20/  196]   Loss 0.681465   Top1 77.148438   Top5 97.011719   BatchTime 0.482487   LR 0.000969
0.78759354
0.78708267
0.78686523
0.78651994
0.78612566
0.78572041
0.78534317
0.78496027
0.78460962
0.78417671
0.78351784
0.78308713
0.78292823
0.78284192
0.78275377
0.78249538
0.78210980
0.78167951
0.78131336
0.78090805
0.78053510
0.78023672
INFO - Training [7][   40/  196]   Loss 0.686659   Top1 77.529297   Top5 97.080078   BatchTime 0.426983   LR 0.000907
0.77996719
0.77966195
0.77946728
0.77922761
0.77902871
0.77890933
0.77875197
0.77866811
0.77870131
0.77865040
0.77864569
0.77989686
0.78000212
0.77987874
0.77985150
0.77994931
0.77994317
0.77985477
0.77984357
0.77993745
0.77986813
INFO - Training [7][   60/  196]   Loss 0.675715   Top1 77.779948   Top5 97.278646   BatchTime 0.409290   LR 0.000845
0.77977526
0.77979738
0.78163123
0.78133082
0.78121424
0.78104740
0.78094059
0.78074795
0.78055006
0.78045666
0.78041792
0.78054005
0.78064877
0.78056616
0.78056067
0.78052485
0.78063279
0.78062421
0.78056091
INFO - Training [7][   80/  196]   Loss 0.671794   Top1 77.871094   Top5 97.495117   BatchTime 0.386698   LR 0.000786
0.78061879
0.78076178
0.78098875
0.78078353
0.78059947
0.78033435
0.78001696
0.77959853
0.77931440
0.77853090
0.77749097
0.77659869
0.77568507
0.77413875
0.77143365
0.76812822
0.76389825
0.76308972
INFO - Training [7][  100/  196]   Loss 0.669891   Top1 78.074219   Top5 97.507812   BatchTime 0.378519   LR 0.000728
0.76452750
0.76518387
0.76844114
0.77072334
0.77305365
0.77491862
0.77616656
0.77725649
0.77799374
0.77845430
0.77900165
0.77916270
0.77926892
0.77930939
0.77913600
0.77908456
0.77920902
0.77920043
0.77926755
0.77936655
0.77929908
0.77913386
INFO - Training [7][  120/  196]   Loss 0.661791   Top1 78.382161   Top5 97.646484   BatchTime 0.375931   LR 0.000673
0.77918404
0.77911919
0.77915090
0.77922094
0.77894878
0.77878129
0.77846968
0.77839810
0.77830094
0.77814770
0.77798325
0.77788091
0.77788341
0.77779442
0.77761537
0.77748525
0.77732259
0.77703267
INFO - Training [7][  140/  196]   Loss 0.660173   Top1 78.532366   Top5 97.723214   BatchTime 0.370829   LR 0.000619
0.77691430
0.77678651
0.77644724
0.77603978
0.77555215
0.77501339
0.77402455
0.77323401
0.77283520
0.77259374
0.77237892
0.77225250
0.77201265
0.77141875
0.77070671
0.76990902
0.76958716
0.76900572
0.76846933
0.76842743
0.76846981
0.76852262
0.76844794
INFO - Training [7][  160/  196]   Loss 0.661838   Top1 78.427734   Top5 97.712402   BatchTime 0.367982   LR 0.000567
0.76880187
0.76941609
0.76985466
0.77025980
0.77060223
0.77110898
0.77147758
0.77264392
0.77261704
0.77270365
0.77256560
0.77224898
0.77207661
0.77168423
0.77136105
0.77097166
0.77082992
0.77074468
0.77049679
0.77021384
0.77028883
INFO - Training [7][  180/  196]   Loss 0.660613   Top1 78.461372   Top5 97.671441   BatchTime 0.369452   LR 0.000517
0.76999819
0.76980436
0.76961875
0.76942313
0.76903808
0.76856166
0.76827645
0.76788306
0.76757413
0.76723164
0.76686472
0.76641792
0.76612878
0.76537675
0.76447362
INFO - ==> Top1: 78.548    Top5: 97.692    Loss: 0.658
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 0.558351   Top1 80.625000   Top5 99.160156   BatchTime 0.123292
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0469)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0998)
features.3.conv.0 tensor(0.0489)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.2218)
features.4.conv.0 tensor(0.0698)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.1032)
features.5.conv.0 tensor(0.3081)
features.5.conv.3 tensor(0.1030)
features.5.conv.6 tensor(0.1549)
features.6.conv.0 tensor(0.0343)
features.6.conv.3 tensor(0.0521)
features.6.conv.6 tensor(0.0841)
features.7.conv.0 tensor(0.0984)
features.7.conv.3 tensor(0.1128)
features.7.conv.6 tensor(0.2008)
features.8.conv.0 tensor(0.1197)
features.8.conv.3 tensor(0.1120)
features.8.conv.6 tensor(0.1416)
features.9.conv.0 tensor(0.1210)
features.9.conv.3 tensor(0.1348)
features.9.conv.6 tensor(0.2768)
features.10.conv.0 tensor(0.0524)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.0865)
features.11.conv.0 tensor(0.4937)
features.11.conv.3 tensor(0.1348)
features.11.conv.6 tensor(0.6080)
features.12.conv.0 tensor(0.5446)
features.12.conv.3 tensor(0.1150)
features.12.conv.6 tensor(0.7567)
features.13.conv.0 tensor(0.1287)
features.13.conv.3 tensor(0.1485)
features.13.conv.6 tensor(0.1881)
features.14.conv.0 tensor(0.9468)
features.14.conv.3 tensor(0.0919)
features.14.conv.6 tensor(0.9524)
features.15.conv.0 tensor(0.9279)
features.15.conv.3 tensor(0.0767)
features.15.conv.6 tensor(0.2594)
features.16.conv.0 tensor(0.0789)
features.16.conv.3 tensor(0.0910)
features.16.conv.6 tensor(0.1952)
conv.0 tensor(0.5497)
tensor(970661.) 2188896.0
INFO - Validation [7][   40/   40]   Loss 0.557051   Top1 80.630000   Top5 99.250000   BatchTime 0.088445
INFO - ==> Top1: 80.630    Top5: 99.250    Loss: 0.557
INFO - ==> Sparsity : 0.443
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 80.630   Top5: 99.250]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 80.620   Top5: 98.680]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 80.560   Top5: 98.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.76388967
0.76286197
0.76226830
0.76179224
0.76120502
0.76005954
0.76015717
0.75994396
0.75973177
0.75945282
0.76011902
0.76054174
0.76034975
0.76026964
0.75994265
0.75962114
0.75942111
0.75971651
INFO - Training [8][   20/  196]   Loss 0.652126   Top1 78.437500   Top5 97.285156   BatchTime 0.448668   LR 0.000434
0.75963724
0.75953704
0.75933081
0.75906330
0.75880736
0.75868851
0.75854325
0.75834811
0.75827473
0.75828147
0.75819308
0.75798947
0.75773406
0.75753516
0.75773019
0.75777781
0.75807220
0.75831133
0.75802600
0.75755268
0.75720036
0.75675064
INFO - Training [8][   40/  196]   Loss 0.651632   Top1 78.759766   Top5 97.607422   BatchTime 0.411549   LR 0.000389
0.75658429
0.75584656
0.75496167
0.75398296
0.75311011
0.75282878
0.75258046
0.75212532
0.75236291
0.75265044
0.75282031
0.75302440
0.75319570
0.75348133
0.75384915
0.75414044
0.75444257
0.75459951
INFO - Training [8][   60/  196]   Loss 0.657353   Top1 78.522135   Top5 97.760417   BatchTime 0.381870   LR 0.000347
0.75486219
0.75486296
0.75442630
0.75410908
0.75390708
0.75369728
0.75336081
0.75285822
0.75261050
0.75252813
0.75237644
0.75223869
0.75209558
0.75204492
0.75193119
0.75193769
0.75188684
0.75190890
0.75185245
0.75183296
0.75183451
0.75172842
0.75160110
INFO - Training [8][   80/  196]   Loss 0.648341   Top1 78.886719   Top5 97.768555   BatchTime 0.375718   LR 0.000308
0.75151592
0.75138444
0.75126535
0.75118458
0.75109661
0.75099492
0.75072718
0.75051904
0.75025469
0.75002444
0.74981737
0.74963009
0.74939573
0.74917650
0.74899411
0.74878412
0.74864471
0.74835765
0.74824095
0.74823964
0.74835521
INFO - Training [8][  100/  196]   Loss 0.642622   Top1 79.011719   Top5 97.812500   BatchTime 0.376134   LR 0.000270
0.74841744
0.74846327
0.74840993
0.74830079
0.74824780
0.74826151
0.74830884
0.74829710
0.74827009
0.74825162
0.74818355
0.74824798
0.74833292
0.74840981
0.74857050
0.74875057
0.74890673
0.74907899
0.74920666
INFO - Training [8][  120/  196]   Loss 0.637007   Top1 79.231771   Top5 97.903646   BatchTime 0.383480   LR 0.000235
0.74926519
0.74932915
0.74938405
0.74942505
0.74941361
0.74939036
0.74941975
0.74949533
0.74955606
0.74960190
0.74962771
0.74960977
0.74965423
0.74966860
0.74961662
0.74964243
0.74978203
0.75175411
0.75125349
0.75138289
0.75125605
INFO - Training [8][  140/  196]   Loss 0.636557   Top1 79.246652   Top5 97.960379   BatchTime 0.383010   LR 0.000202
0.75116807
0.75102794
0.75090176
0.75078350
0.75071949
0.75067967
0.75057131
0.75050062
0.75047493
0.75041515
0.75038195
0.75037122
0.75038236
0.75033897
0.75025100
INFO - Training [8][  160/  196]   Loss 0.639511   Top1 79.182129   Top5 97.922363   BatchTime 0.382907   LR 0.000172
0.75014752
0.75003636
0.74994016
0.74987537
0.74980354
0.74974805
0.74967712
0.74962735
0.74957812
0.74953753
0.74949610
0.74945462
0.74941742
0.74934971
0.74930114
0.74926555
0.74924242
0.74921566
0.74921787
0.74920136
0.74910122
INFO - Training [8][  180/  196]   Loss 0.637335   Top1 79.181858   Top5 97.858073   BatchTime 0.384910   LR 0.000143
0.74906552
0.74901062
0.74897200
0.74890620
0.74886090
0.74884635
0.74887198
0.74884963
0.74883986
0.74880165
0.74878156
0.74869537
0.74860257
0.74852622
0.74849111
INFO - ==> Top1: 79.238    Top5: 97.852    Loss: 0.636
0.74841946
0.74840277
0.74838865
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [8][   20/   40]   Loss 0.589226   Top1 80.312500   Top5 98.828125   BatchTime 0.131761
INFO - Validation [8][   40/   40]   Loss 0.585463   Top1 80.300000   Top5 98.910000   BatchTime 0.093551
INFO - ==> Top1: 80.300    Top5: 98.910    Loss: 0.585
INFO - ==> Sparsity : 0.475
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 80.630   Top5: 99.250]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 80.620   Top5: 98.680]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 80.560   Top5: 98.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0378)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0469)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0998)
features.3.conv.0 tensor(0.0498)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.2244)
features.4.conv.0 tensor(0.0716)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.1064)
features.5.conv.0 tensor(0.3075)
features.5.conv.3 tensor(0.1036)
features.5.conv.6 tensor(0.1481)
features.6.conv.0 tensor(0.0337)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0850)
features.7.conv.0 tensor(0.0979)
features.7.conv.3 tensor(0.1105)
features.7.conv.6 tensor(0.2118)
features.8.conv.0 tensor(0.1213)
features.8.conv.3 tensor(0.1131)
features.8.conv.6 tensor(0.2006)
features.9.conv.0 tensor(0.1239)
features.9.conv.3 tensor(0.1354)
features.9.conv.6 tensor(0.2915)
features.10.conv.0 tensor(0.0553)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.0942)
features.11.conv.0 tensor(0.5537)
features.11.conv.3 tensor(0.1331)
features.11.conv.6 tensor(0.6217)
features.12.conv.0 tensor(0.6002)
features.12.conv.3 tensor(0.1150)
features.12.conv.6 tensor(0.7657)
features.13.conv.0 tensor(0.1337)
features.13.conv.3 tensor(0.1476)
features.13.conv.6 tensor(0.2010)
features.14.conv.0 tensor(0.9487)
features.14.conv.3 tensor(0.0918)
features.14.conv.6 tensor(0.9490)
features.15.conv.0 tensor(0.9284)
features.15.conv.3 tensor(0.0773)
features.15.conv.6 tensor(0.2599)
features.16.conv.0 tensor(0.0810)
features.16.conv.3 tensor(0.0900)
features.16.conv.6 tensor(0.2332)
conv.0 tensor(0.6618)
tensor(1040069.) 2188896.0
0.74836940
0.74831539
0.74829322
0.74825621
0.74822885
0.74823070
0.74817288
0.74810266
0.74804115
0.74797106
0.74790555
0.74782574
0.74778521
0.74774933
0.74775267
0.74771118
0.74766487
0.74754441
0.74750745
0.74746943
INFO - Training [9][   20/  196]   Loss 0.649296   Top1 78.476562   Top5 97.304688   BatchTime 0.443110   LR 0.000100
0.74743325
0.74736595
0.74729854
0.74728864
0.74725848
0.74721348
0.74715817
0.74708527
0.74702972
0.74700177
0.74689752
0.74677390
0.74662364
0.74661154
0.74652654
0.74643159
0.74636215
0.74630028
0.74628115
INFO - Training [9][   40/  196]   Loss 0.641506   Top1 78.593750   Top5 97.539062   BatchTime 0.380665   LR 0.000079
0.74619049
0.74607307
0.74600458
0.74593025
0.74585289
0.74582827
0.74578530
0.74575931
0.74564731
0.74557108
0.74549562
0.74541223
0.74532837
0.74521118
0.74502474
0.74490547
0.74469858
0.74457270
0.74450469
0.74445623
0.74441940
0.74437839
INFO - Training [9][   60/  196]   Loss 0.635062   Top1 78.977865   Top5 97.682292   BatchTime 0.372308   LR 0.000060
0.74436074
0.74433577
0.74431115
0.74427563
0.74422377
0.74419755
0.74413633
0.74409699
0.74406004
0.74403930
0.74398571
0.74391574
0.74387109
0.74382210
0.74379706
0.74376601
INFO - Training [9][   80/  196]   Loss 0.631888   Top1 79.160156   Top5 97.822266   BatchTime 0.374516   LR 0.000044
0.74373615
0.74370378
0.74367958
0.74366122
0.74365133
0.74361897
0.74362820
0.74360377
0.74356985
0.74356002
0.74352479
0.74349385
0.74348283
0.74345362
0.74340641
0.74336725
0.74332684
0.74326950
0.74324304
0.74319053
0.74315476
INFO - Training [9][  100/  196]   Loss 0.626350   Top1 79.355469   Top5 97.910156   BatchTime 0.375124   LR 0.000030
0.74314523
0.74311626
0.74305397
0.74301755
0.74303341
0.74300051
0.74298996
0.74295390
0.74293125
0.74290848
0.74289614
0.74288285
0.74285376
0.74283141
0.74279517
0.74278677
0.74276644
0.74273217
0.74271303
0.74268317
0.74265963
0.74261832
INFO - Training [9][  120/  196]   Loss 0.625769   Top1 79.391276   Top5 97.897135   BatchTime 0.373816   LR 0.000019
0.74259692
0.74259615
0.74257886
0.74256408
0.74254495
0.74252248
0.74251831
0.74250197
0.74248368
0.74248934
0.74248886
0.74247921
0.74249011
0.74247080
0.74247104
0.74249262
0.74249494
0.74247277
0.74246764
0.74246901
0.74247831
0.74249953
INFO - Training [9][  140/  196]   Loss 0.619955   Top1 79.617746   Top5 97.991071   BatchTime 0.374128   LR 0.000010
0.74248970
0.74247730
0.74245077
0.74244457
0.74244684
0.74245477
0.74245644
0.74245751
0.74245107
0.74243671
0.74244171
0.74242818
0.74243557
0.74239618
0.74239343
0.74241143
INFO - Training [9][  160/  196]   Loss 0.623687   Top1 79.470215   Top5 97.905273   BatchTime 0.373322   LR 0.000004
0.74239498
0.74239045
0.74236751
0.74235582
0.74234384
0.74234045
0.74235779
0.74234593
0.74235803
0.74234897
0.74232727
0.74234450
0.74233675
0.74231899
0.74233973
0.74234706
0.74231786
0.74232286
0.74231952
0.74233550
0.74233288
INFO - Training [9][  180/  196]   Loss 0.623857   Top1 79.422743   Top5 97.855903   BatchTime 0.374237   LR 0.000001
0.74230361
0.74232239
0.74233204
0.74230963
0.74231553
0.74231696
0.74233520
0.74232721
0.74232787
0.74232942
0.74233162
0.74233836
0.74231857
0.74232602
0.74234861
INFO - ==> Top1: 79.368    Top5: 97.840    Loss: 0.625
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.74234712
0.74231642
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.527732   Top1 81.796875   Top5 98.964844   BatchTime 0.121588
INFO - Validation [9][   40/   40]   Loss 0.525864   Top1 81.890000   Top5 99.100000   BatchTime 0.089525
INFO - ==> Top1: 81.890    Top5: 99.100    Loss: 0.526
INFO - ==> Sparsity : 0.489
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 81.890   Top5: 99.100]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 80.630   Top5: 99.250]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 80.620   Top5: 98.680]
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0638)
features.2.conv.0 tensor(0.0475)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.1001)
features.3.conv.0 tensor(0.0506)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.2255)
features.4.conv.0 tensor(0.0715)
features.4.conv.3 tensor(0.0926)
features.4.conv.6 tensor(0.1160)
features.5.conv.0 tensor(0.3078)
features.5.conv.3 tensor(0.1036)
features.5.conv.6 tensor(0.1494)
features.6.conv.0 tensor(0.0342)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.0990)
features.7.conv.3 tensor(0.1126)
features.7.conv.6 tensor(0.2144)
features.8.conv.0 tensor(0.1215)
features.8.conv.3 tensor(0.1128)
features.8.conv.6 tensor(0.2110)
features.9.conv.0 tensor(0.1269)
features.9.conv.3 tensor(0.1363)
features.9.conv.6 tensor(0.2987)
features.10.conv.0 tensor(0.0554)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.0998)
features.11.conv.0 tensor(0.5843)
features.11.conv.3 tensor(0.1339)
features.11.conv.6 tensor(0.6279)
features.12.conv.0 tensor(0.6041)
features.12.conv.3 tensor(0.1140)
features.12.conv.6 tensor(0.7673)
features.13.conv.0 tensor(0.1459)
features.13.conv.3 tensor(0.1478)
features.13.conv.6 tensor(0.2040)
features.14.conv.0 tensor(0.9490)
features.14.conv.3 tensor(0.0922)
features.14.conv.6 tensor(0.9506)
features.15.conv.0 tensor(0.9287)
features.15.conv.3 tensor(0.0772)
features.15.conv.6 tensor(0.2601)
features.16.conv.0 tensor(0.0822)
features.16.conv.3 tensor(0.0897)
features.16.conv.6 tensor(0.2915)
conv.0 tensor(0.6792)
tensor(1069865.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.74230880
0.74257809
0.74296939
0.74284846
0.74270451
0.74242568
0.74157357
0.73809648
0.73283768
0.73295873
0.73935300
0.74309719
0.74682730
0.75313723
0.75709486
0.76192242
0.76499611
0.76725113
INFO - Training [10][   20/  196]   Loss 0.672092   Top1 78.105469   Top5 97.246094   BatchTime 0.402768   LR 0.002500
0.76951915
0.77161169
0.77344209
0.77559340
0.77625483
0.77621084
0.77605110
0.77647191
0.77746505
0.77820355
0.77934986
0.78055912
0.78138161
0.78165156
0.78195459
0.78246862
0.78333086
0.78406131
0.78440309
0.78087848
0.77173841
0.78171927
INFO - Training [10][   40/  196]   Loss 0.683434   Top1 77.714844   Top5 97.402344   BatchTime 0.381234   LR 0.002499
0.78596866
0.78834581
0.78907424
0.78999710
0.79085082
0.79109728
0.79143333
0.79154992
0.79187292
0.79214108
0.79217738
0.79233974
0.79272765
0.79289198
0.79305840
0.79311806
0.79298663
INFO - Training [10][   60/  196]   Loss 0.688490   Top1 77.662760   Top5 97.278646   BatchTime 0.378189   LR 0.002499
0.79260021
0.79257470
0.79280013
0.79321909
0.79330796
0.79360110
0.79361320
0.79371160
0.79346335
0.79357004
0.79367191
0.79370320
0.79352349
0.79340559
0.79332507
0.79358494
0.79420191
0.79483080
0.79586756
0.79667181
0.79710102
INFO - Training [10][   80/  196]   Loss 0.695772   Top1 77.436523   Top5 97.343750   BatchTime 0.378652   LR 0.002497
0.79776621
0.79790491
0.79806966
0.79820991
0.79809308
0.79799199
0.79804271
0.79786181
0.79777294
0.79759079
0.79737365
0.79693848
0.79684669
0.79673779
0.79666555
0.79703993
0.79709297
0.79719841
0.79739779
0.79755849
INFO - Training [10][  100/  196]   Loss 0.697300   Top1 77.414062   Top5 97.363281   BatchTime 0.382938   LR 0.002496
0.79740763
0.79742622
0.79734385
0.79725641
0.79726285
0.79715610
0.79716390
0.79734927
0.79746842
0.79749256
0.79745722
0.79743403
0.79731029
0.79723817
0.79713827
0.79710263
0.79705667
0.79708415
0.79717427
0.79705071
0.79661971
INFO - Training [10][  120/  196]   Loss 0.697819   Top1 77.402344   Top5 97.434896   BatchTime 0.382877   LR 0.002494
0.79595888
0.79594707
0.79626065
0.79592854
0.79497999
0.79423410
0.79276043
0.79099476
0.78905946
0.78704727
0.78363991
0.77920675
0.78871429
0.79258639
0.79489148
0.79671454
0.79780334
0.79818708
0.79807168
0.79778320
0.79743004
INFO - Training [10][  140/  196]   Loss 0.696974   Top1 77.374442   Top5 97.427455   BatchTime 0.382563   LR 0.002492
0.79691684
0.79652709
0.79610276
0.79588157
0.79579520
0.79604059
0.79589850
0.79565930
0.79549354
0.79643375
0.80257344
0.80211216
0.80188423
0.80179483
0.80169016
0.80176216
0.80173534
0.80160546
0.80121934
0.80077112
0.80036175
INFO - Training [10][  160/  196]   Loss 0.702739   Top1 77.138672   Top5 97.397461   BatchTime 0.383397   LR 0.002490
0.80008757
0.79984856
0.79972291
0.80078405
0.80044192
0.80025917
0.80013609
0.79974139
0.79944569
0.79936862
0.79930037
0.79920632
0.79886323
0.79864675
0.79852635
0.79820907
0.79775560
0.79733789
0.79693657
0.79670405
INFO - Training [10][  180/  196]   Loss 0.698621   Top1 77.296007   Top5 97.356771   BatchTime 0.384196   LR 0.002487
0.79648083
0.79623216
0.79565454
0.79537755
0.79518837
0.79509473
0.79512966
0.79538202
0.79587948
0.79620600
0.79599917
0.79573691
0.79553211
0.79554164
0.79522198
********************pre-trained*****************
INFO - ==> Top1: 77.330    Top5: 97.416    Loss: 0.696
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [10][   20/   40]   Loss 0.527131   Top1 82.480469   Top5 98.925781   BatchTime 0.126726
INFO - Validation [10][   40/   40]   Loss 0.514366   Top1 82.830000   Top5 99.130000   BatchTime 0.089586
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.4160)
features.1.conv.0 tensor(0.0378)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0686)
features.2.conv.0 tensor(0.0521)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0955)
features.3.conv.0 tensor(0.0489)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.2244)
features.4.conv.0 tensor(0.0617)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.1007)
features.5.conv.0 tensor(0.3029)
features.5.conv.3 tensor(0.1065)
features.5.conv.6 tensor(0.1535)
features.6.conv.0 tensor(0.0454)
features.6.conv.3 tensor(0.0521)
features.6.conv.6 tensor(0.0783)
features.7.conv.0 tensor(0.1029)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.2018)
features.8.conv.0 tensor(0.0898)
features.8.conv.3 tensor(0.1050)
features.8.conv.6 tensor(0.2345)
features.9.conv.0 tensor(0.0786)
features.9.conv.3 tensor(0.1505)
features.9.conv.6 tensor(0.1615)
features.10.conv.0 tensor(0.0457)
features.10.conv.3 tensor(0.0883)
features.10.conv.6 tensor(0.0845)
features.11.conv.0 tensor(0.1344)
features.11.conv.3 tensor(0.1427)
features.11.conv.6 tensor(0.7405)
features.12.conv.0 tensor(0.5019)
features.12.conv.3 tensor(0.1179)
features.12.conv.6 tensor(0.7096)
features.13.conv.0 tensor(0.1125)
features.13.conv.3 tensor(0.1518)
features.13.conv.6 tensor(0.1859)
features.14.conv.0 tensor(0.9473)
features.14.conv.3 tensor(0.0912)
features.14.conv.6 tensor(0.9204)
features.15.conv.0 tensor(0.9326)
features.15.conv.3 tensor(0.0747)
features.15.conv.6 tensor(0.2776)
features.16.conv.0 tensor(0.0618)
features.16.conv.3 tensor(0.0896)
features.16.conv.6 tensor(0.1601)
conv.0 tensor(0.1655)
tensor(777429.) 2188896.0
INFO - ==> Top1: 82.830    Top5: 99.130    Loss: 0.514
INFO - ==> Sparsity : 0.355
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 82.830   Top5: 99.130]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 81.890   Top5: 99.100]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 80.630   Top5: 99.250]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
0.79456371
0.79409629
0.79385239
0.79347473
0.79310662
0.79301268
0.79261142
0.79218948
0.79240000
0.79381609
0.79320627
0.79339969
0.79323620
0.79305685
0.79313123
0.79316765
0.79317141
0.79278475
0.79268020
0.79264134
INFO - Training [11][   20/  196]   Loss 0.684969   Top1 77.558594   Top5 96.835938   BatchTime 0.456265   LR 0.002481
0.79253715
0.79269266
0.79704785
0.79731607
0.79760957
0.79770857
0.79818445
0.79807431
0.79802144
0.79762077
0.79719454
0.79706436
0.79701096
0.79669988
0.79640281
0.79583663
0.79566103
0.79574478
0.79628599
0.79679978
0.79790109
0.79870772
INFO - Training [11][   40/  196]   Loss 0.706079   Top1 77.080078   Top5 97.060547   BatchTime 0.409988   LR 0.002478
0.79942203
0.80006462
0.80119967
0.80205494
0.80261952
0.80319262
0.80362457
0.80424732
0.80474246
0.80520189
0.80557716
0.80589902
0.80635273
0.80691653
0.80711532
0.80756676
INFO - Training [11][   60/  196]   Loss 0.700188   Top1 77.246094   Top5 97.213542   BatchTime 0.402950   LR 0.002474
0.80793136
0.80832624
0.80866683
0.80909735
0.80972105
0.81008846
0.81034517
0.81099182
0.81124061
0.81152296
0.81193697
0.81220829
0.81273115
0.81302226
0.81334877
0.81314415
0.81324786
0.81341451
0.81358707
0.81351292
INFO - Training [11][   80/  196]   Loss 0.695975   Top1 77.265625   Top5 97.353516   BatchTime 0.397565   LR 0.002470
0.81339705
0.81342852
0.81346864
0.81342924
0.81361639
0.81365955
0.81348896
0.81349498
0.81324083
0.81293106
0.81273121
0.81251144
0.81233650
0.81224018
0.81229430
0.81288141
0.81419528
0.81594139
0.81834286
0.82092339
INFO - Training [11][  100/  196]   Loss 0.686074   Top1 77.648438   Top5 97.410156   BatchTime 0.401656   LR 0.002465
0.82313353
0.82453424
0.82524711
0.82535571
0.82437557
0.82337433
0.82194984
0.82031399
0.81796640
0.81569463
0.81347710
0.81117773
0.80874634
0.80626655
0.80492985
0.80384892
0.80328774
0.80231768
0.80153650
0.80083448
INFO - Training [11][  120/  196]   Loss 0.684622   Top1 77.669271   Top5 97.500000   BatchTime 0.398770   LR 0.002460
0.80061740
0.80006462
0.79954201
0.79926646
0.79931122
0.79923487
0.79867750
0.79811668
0.79789978
0.79753923
0.79673523
0.79657322
0.79673624
0.79711431
0.79747289
0.79789543
0.79843622
0.80057997
0.80254769
0.80648559
0.80611151
0.80596697
INFO - Training [11][  140/  196]   Loss 0.684437   Top1 77.678571   Top5 97.589286   BatchTime 0.395091   LR 0.002455
0.80585361
0.80557024
0.80545574
0.80520308
0.80506158
0.80485022
0.80466592
0.80474377
0.80460435
0.80428028
0.80360740
0.80323970
0.80256706
0.80185473
0.80130953
0.80023021
0.79898888
0.79885828
0.80016345
0.80129009
INFO - Training [11][  160/  196]   Loss 0.686655   Top1 77.653809   Top5 97.526855   BatchTime 0.395457   LR 0.002450
0.80198985
0.80275446
0.80302525
0.80280304
0.80260885
0.80221403
0.80154699
0.80092400
0.80085373
0.80081117
0.80094296
0.80085057
0.80049944
0.80041218
0.80040807
0.80059445
INFO - Training [11][  180/  196]   Loss 0.685334   Top1 77.651910   Top5 97.489149   BatchTime 0.391274   LR 0.002444
0.80194819
0.80180079
0.80157191
0.80144602
0.80114216
0.80099946
0.80086440
0.80053109
0.80029160
0.80011201
0.80005509
0.79990703
0.79990959
0.79966855
0.79967099
0.79959691
0.79943502
0.79935855
0.79899943
INFO - ==> Top1: 77.716    Top5: 97.478    Loss: 0.685
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79897720
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.580436   Top1 80.351562   Top5 98.613281   BatchTime 0.122485
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.4102)
features.1.conv.0 tensor(0.0378)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0489)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0981)
features.3.conv.0 tensor(0.0434)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.2270)
features.4.conv.0 tensor(0.0653)
features.4.conv.3 tensor(0.0862)
features.4.conv.6 tensor(0.1019)
features.5.conv.0 tensor(0.2982)
features.5.conv.3 tensor(0.1117)
features.5.conv.6 tensor(0.1037)
features.6.conv.0 tensor(0.0277)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0789)
features.7.conv.0 tensor(0.0886)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.2194)
features.8.conv.0 tensor(0.0559)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.1498)
features.9.conv.0 tensor(0.0825)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.1640)
features.10.conv.0 tensor(0.0483)
features.10.conv.3 tensor(0.0894)
features.10.conv.6 tensor(0.0887)
features.11.conv.0 tensor(0.1530)
features.11.conv.3 tensor(0.1414)
features.11.conv.6 tensor(0.5622)
features.12.conv.0 tensor(0.5469)
features.12.conv.3 tensor(0.1225)
features.12.conv.6 tensor(0.7231)
features.13.conv.0 tensor(0.0869)
features.13.conv.3 tensor(0.1516)
features.13.conv.6 tensor(0.1904)
features.14.conv.0 tensor(0.9473)
features.14.conv.3 tensor(0.0969)
features.14.conv.6 tensor(0.9780)
features.15.conv.0 tensor(0.9373)
features.15.conv.3 tensor(0.0765)
features.15.conv.6 tensor(0.2955)
features.16.conv.0 tensor(0.0703)
features.16.conv.3 tensor(0.0955)
features.16.conv.6 tensor(0.1866)
conv.0 tensor(0.2224)
tensor(813241.) 2188896.0
INFO - Validation [11][   40/   40]   Loss 0.578953   Top1 80.710000   Top5 98.730000   BatchTime 0.090027
INFO - ==> Top1: 80.710    Top5: 98.730    Loss: 0.579
INFO - ==> Sparsity : 0.372
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 82.830   Top5: 99.130]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 81.890   Top5: 99.100]
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 80.710   Top5: 98.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
0.79917449
0.79922473
0.79931718
0.79935932
0.79930997
0.79924405
0.79910743
0.79911608
0.79906613
0.79899698
0.79911953
0.79873735
0.79840380
0.79819661
0.79781330
0.79757196
0.79732805
0.79724091
INFO - Training [12][   20/  196]   Loss 0.674206   Top1 78.007812   Top5 97.187500   BatchTime 0.433518   LR 0.002433
0.79725403
0.79746193
0.79749352
0.79734707
0.79735219
0.79708964
0.79680550
0.79654628
0.79667825
0.79666412
0.79685336
0.79713690
0.79733837
0.79734826
0.79746085
0.79526490
0.79751652
0.79960549
0.80051810
0.80038887
0.80015087
0.79991025
INFO - Training [12][   40/  196]   Loss 0.685180   Top1 77.568359   Top5 97.363281   BatchTime 0.399689   LR 0.002426
0.79974264
0.79939342
0.79922140
0.79888874
0.79850620
0.79813510
0.79799765
0.79800069
0.79797572
0.79789966
0.79791266
0.79818082
0.79837137
0.79846549
0.79839784
0.79917789
0.79902446
0.79879445
0.79842663
0.79819012
INFO - Training [12][   60/  196]   Loss 0.684738   Top1 77.656250   Top5 97.389323   BatchTime 0.396562   LR 0.002419
0.79808122
0.79793477
0.79776382
0.79767174
0.79757869
0.79739600
0.79732364
0.79724348
0.79729843
0.79749149
0.79791033
0.79960197
0.80049324
0.80014873
0.79992855
0.79977667
0.79952228
0.79931319
0.79899061
0.79849970
0.79781413
0.79711795
INFO - Training [12][   80/  196]   Loss 0.679698   Top1 77.783203   Top5 97.529297   BatchTime 0.390703   LR 0.002412
0.79659349
0.79612607
0.79523057
0.79459012
0.79410458
0.79369521
0.79268098
0.79206437
0.79150569
0.79099983
0.79021364
0.78958488
0.78899097
0.78872299
0.78861135
INFO - Training [12][  100/  196]   Loss 0.668122   Top1 78.269531   Top5 97.589844   BatchTime 0.389512   LR 0.002404
0.78853709
0.78840721
0.78826332
0.78895861
0.78919417
0.78910315
0.78905457
0.78878641
0.78863806
0.78859228
0.78857666
0.78855926
0.78867584
0.78910905
0.78954959
0.79026616
0.79293293
0.79572797
0.79582268
0.79563737
0.79547548
0.79532647
INFO - Training [12][  120/  196]   Loss 0.661434   Top1 78.512370   Top5 97.659505   BatchTime 0.384104   LR 0.002396
0.79505867
0.79485422
0.79615611
0.79562855
0.79536623
0.79516160
0.79495949
0.79462868
0.79433036
0.79379731
0.79365212
0.79347199
0.79313904
0.79298329
0.79288381
0.79286093
0.79291737
0.79301935
0.79301709
0.79296368
0.79300523
INFO - Training [12][  140/  196]   Loss 0.659456   Top1 78.543527   Top5 97.720424   BatchTime 0.383395   LR 0.002388
0.79304236
0.79297471
0.79292738
0.79301596
0.79283941
0.79256779
0.79226780
0.79245907
0.79379040
0.79332066
0.79242820
0.79241580
0.79206431
0.79174906
0.79123509
0.79088593
0.79080212
INFO - Training [12][  160/  196]   Loss 0.663662   Top1 78.381348   Top5 97.653809   BatchTime 0.380625   LR 0.002380
0.79064041
0.79095632
0.79120231
0.79104137
0.79078108
0.79121107
0.79187059
0.79129356
0.79037762
0.78947145
0.78927833
0.78898966
0.78843981
0.78796536
0.78847045
0.78937310
0.78976339
0.79021776
0.79010481
0.79003417
0.78984821
0.78976196
0.79000276
0.79022396
0.79041225
INFO - Training [12][  180/  196]   Loss 0.663642   Top1 78.370226   Top5 97.573785   BatchTime 0.374721   LR 0.002371
0.79086167
0.79122782
0.79155833
0.79170126
0.79108685
0.79102886
0.79084104
0.79036945
0.78993219
0.78958559
0.78948551
0.78930235
INFO - ==> Top1: 78.386    Top5: 97.586    Loss: 0.664
0.78906673
0.78874165
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.558280   Top1 81.074219   Top5 98.808594   BatchTime 0.137280
INFO - Validation [12][   40/   40]   Loss 0.549049   Top1 81.600000   Top5 98.940000   BatchTime 0.095580
INFO - ==> Top1: 81.600    Top5: 98.940    Loss: 0.549
INFO - ==> Sparsity : 0.378
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 82.830   Top5: 99.130]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 81.890   Top5: 99.100]
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 81.600   Top5: 98.940]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0699)
features.2.conv.0 tensor(0.0509)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0940)
features.3.conv.0 tensor(0.0489)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.2309)
features.4.conv.0 tensor(0.0612)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0978)
features.5.conv.0 tensor(0.3143)
features.5.conv.3 tensor(0.1100)
features.5.conv.6 tensor(0.0924)
features.6.conv.0 tensor(0.0267)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0783)
features.7.conv.0 tensor(0.1155)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.1336)
features.8.conv.0 tensor(0.0836)
features.8.conv.3 tensor(0.1126)
features.8.conv.6 tensor(0.1080)
features.9.conv.0 tensor(0.0816)
features.9.conv.3 tensor(0.1687)
features.9.conv.6 tensor(0.1697)
features.10.conv.0 tensor(0.0491)
features.10.conv.3 tensor(0.0900)
features.10.conv.6 tensor(0.1092)
features.11.conv.0 tensor(0.5377)
features.11.conv.3 tensor(0.1451)
features.11.conv.6 tensor(0.5741)
features.12.conv.0 tensor(0.5311)
features.12.conv.3 tensor(0.1190)
features.12.conv.6 tensor(0.7235)
features.13.conv.0 tensor(0.1069)
features.13.conv.3 tensor(0.1532)
features.13.conv.6 tensor(0.1994)
features.14.conv.0 tensor(0.9520)
features.14.conv.3 tensor(0.0973)
features.14.conv.6 tensor(0.9493)
features.15.conv.0 tensor(0.9402)
features.15.conv.3 tensor(0.0759)
features.15.conv.6 tensor(0.3014)
features.16.conv.0 tensor(0.0708)
features.16.conv.3 tensor(0.1009)
features.16.conv.6 tensor(0.1955)
conv.0 tensor(0.2018)
tensor(827484.) 2188896.0
0.78851449
0.78857458
0.78849906
0.78832018
0.78890991
0.78902382
0.78860205
0.78824300
0.78818876
0.78808659
0.78821617
0.78833538
0.78843039
0.78845137
0.78840482
0.78829205
0.78835773
0.78848648
0.78872442
INFO - Training [13][   20/  196]   Loss 0.672951   Top1 77.734375   Top5 96.992188   BatchTime 0.478321   LR 0.002355
0.78937489
0.78995818
0.79023015
0.79038161
0.79052407
0.79067248
0.79064143
0.79051727
0.79037911
0.79018974
0.78987950
0.78933716
0.78848821
0.78788936
0.78729087
0.78665257
0.78639966
0.78621626
0.78592491
0.78493387
0.78326124
INFO - Training [13][   40/  196]   Loss 0.653510   Top1 78.330078   Top5 97.451172   BatchTime 0.426185   LR 0.002345
0.78165406
0.77934235
0.77575046
0.77187967
0.76854324
0.76865453
0.76762080
0.76737428
0.76911706
0.77585560
0.78244132
0.78818852
0.79134136
0.79387337
0.79375225
0.79368889
0.79341710
0.79338163
0.79312420
0.79263002
0.79231232
INFO - Training [13][   60/  196]   Loss 0.650350   Top1 78.613281   Top5 97.532552   BatchTime 0.413368   LR 0.002336
0.79188782
0.79142034
0.79109299
0.79091769
0.79067725
0.79054111
0.79013389
0.78983104
0.78957343
0.78926343
0.78899729
0.78870612
0.78832805
0.78830421
0.78822547
0.78797489
0.78793103
INFO - Training [13][   80/  196]   Loss 0.650130   Top1 78.627930   Top5 97.675781   BatchTime 0.400818   LR 0.002325
0.78782171
0.78762841
0.78759205
0.78740078
0.78750193
0.78751642
0.78824615
0.78926343
0.78877473
0.78869563
0.78841752
0.78782415
0.78684157
0.78647745
0.78641659
0.78639197
0.78633845
0.78614503
0.78597283
0.78570443
0.78553909
INFO - Training [13][  100/  196]   Loss 0.647477   Top1 78.707031   Top5 97.660156   BatchTime 0.394606   LR 0.002315
0.78535032
0.78521335
0.78599787
0.78677917
0.78687173
0.78700596
0.78698421
0.78703189
0.78692096
0.78685391
0.78667480
0.78675842
0.78783172
0.78770012
0.78740770
0.78703701
0.78692681
0.78671253
0.78656268
0.78643197
0.78638643
INFO - Training [13][  120/  196]   Loss 0.646799   Top1 78.779297   Top5 97.727865   BatchTime 0.392397   LR 0.002304
0.78621989
0.78618497
0.78614008
0.78601342
0.78595811
0.78597903
0.78604901
0.78613800
0.78626400
0.78703475
0.78904271
0.79264879
0.79323345
0.79319274
0.79330456
0.79329115
0.79323697
0.79334992
0.79376185
0.79380995
0.79390579
INFO - Training [13][  140/  196]   Loss 0.645827   Top1 78.830915   Top5 97.784598   BatchTime 0.391473   LR 0.002293
0.79405212
0.79412895
0.79408288
0.79431188
0.79441184
0.79453003
0.79464525
0.79469806
0.79477042
0.79460841
0.79441142
0.79425794
0.79447663
0.79515165
0.79492432
0.79486620
0.79479939
0.79518759
0.79599309
0.79651719
0.79661816
INFO - Training [13][  160/  196]   Loss 0.647283   Top1 78.876953   Top5 97.770996   BatchTime 0.391921   LR 0.002282
0.79662871
0.79668522
0.79661942
0.79653233
0.79649967
0.79640007
0.79624581
0.79622108
0.79623359
0.79598945
0.79588574
0.79587078
0.79587525
0.79550987
0.79544628
0.79527014
0.79499358
0.79473472
INFO - Training [13][  180/  196]   Loss 0.647278   Top1 78.834635   Top5 97.723524   BatchTime 0.384835   LR 0.002271
0.79445481
0.79422367
0.79401392
0.79392046
0.79343402
0.79308110
0.79251158
0.79215413
0.79209739
0.79191375
0.79171193
0.79143131
0.79117733
INFO - ==> Top1: 78.822    Top5: 97.720    Loss: 0.647
0.79080445
0.79055887
0.79043025
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.516347   Top1 82.050781   Top5 99.003906   BatchTime 0.130460
INFO - Validation [13][   40/   40]   Loss 0.514837   Top1 82.170000   Top5 99.070000   BatchTime 0.093033
INFO - ==> Top1: 82.170    Top5: 99.070    Loss: 0.515
INFO - ==> Sparsity : 0.385
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 82.830   Top5: 99.130]
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 82.170   Top5: 99.070]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 81.890   Top5: 99.100]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5278)
features.0.conv.3 tensor(0.4180)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0690)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0877)
features.3.conv.0 tensor(0.0443)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.2320)
features.4.conv.0 tensor(0.0625)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.0977)
features.5.conv.0 tensor(0.2930)
features.5.conv.3 tensor(0.1013)
features.5.conv.6 tensor(0.1097)
features.6.conv.0 tensor(0.0275)
features.6.conv.3 tensor(0.0590)
features.6.conv.6 tensor(0.0759)
features.7.conv.0 tensor(0.0859)
features.7.conv.3 tensor(0.1149)
features.7.conv.6 tensor(0.1197)
features.8.conv.0 tensor(0.0903)
features.8.conv.3 tensor(0.1065)
features.8.conv.6 tensor(0.1485)
features.9.conv.0 tensor(0.0992)
features.9.conv.3 tensor(0.1681)
features.9.conv.6 tensor(0.1428)
features.10.conv.0 tensor(0.0485)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.1002)
features.11.conv.0 tensor(0.4819)
features.11.conv.3 tensor(0.1441)
features.11.conv.6 tensor(0.6007)
features.12.conv.0 tensor(0.5332)
features.12.conv.3 tensor(0.1213)
features.12.conv.6 tensor(0.7360)
features.13.conv.0 tensor(0.1319)
features.13.conv.3 tensor(0.1561)
features.13.conv.6 tensor(0.1316)
features.14.conv.0 tensor(0.9561)
features.14.conv.3 tensor(0.0962)
features.14.conv.6 tensor(0.9390)
features.15.conv.0 tensor(0.9421)
features.15.conv.3 tensor(0.0730)
features.15.conv.6 tensor(0.3073)
features.16.conv.0 tensor(0.0830)
features.16.conv.3 tensor(0.0994)
features.16.conv.6 tensor(0.2220)
conv.0 tensor(0.2276)
tensor(842002.) 2188896.0
0.79036021
0.78991562
0.78977126
0.78963810
0.78934205
0.78947514
0.78958660
0.78959453
0.78947932
0.78937566
0.78948617
0.78954029
0.78951794
0.79059750
0.79180712
0.79274857
0.79273182
0.79283881
0.79287595
0.79298073
0.79292846
INFO - Training [14][   20/  196]   Loss 0.672787   Top1 77.773438   Top5 97.246094   BatchTime 0.450711   LR 0.002250
0.79293019
0.79279971
0.79292089
0.79295033
0.79274559
0.79278851
0.79292166
0.79297501
0.79275525
0.79256827
0.79234964
0.79197752
0.79160058
0.79147655
0.79143018
0.79122519
0.79105115
0.79118335
0.79093528
0.79077286
0.79050094
INFO - Training [14][   40/  196]   Loss 0.663915   Top1 78.261719   Top5 97.617188   BatchTime 0.410640   LR 0.002238
0.79037690
0.79005116
0.78979975
0.78934282
0.78900743
0.78876817
0.78861386
0.78853530
0.78830636
0.78802836
0.78781354
0.78739148
0.78774291
0.78751451
0.78703707
0.78647715
0.78640634
INFO - Training [14][   60/  196]   Loss 0.650836   Top1 78.828125   Top5 97.675781   BatchTime 0.395401   LR 0.002225
0.78632408
0.78622526
0.78599286
0.78535944
0.78460211
0.78377581
0.78304553
0.78239405
0.78191084
0.78137606
0.78116620
0.78075981
0.78020322
0.77955765
0.77923316
0.77879578
0.77857339
0.77814984
0.77795875
0.77794087
0.77818364
0.77890468
INFO - Training [14][   80/  196]   Loss 0.645951   Top1 78.959961   Top5 97.729492   BatchTime 0.387793   LR 0.002213
0.77951467
0.77984625
0.77984542
0.78006274
0.78041798
0.78058541
0.78262734
0.78296506
0.78379071
0.78621423
0.78600413
0.78542650
0.78467458
0.78368682
0.78249520
0.78151613
INFO - Training [14][  100/  196]   Loss 0.639341   Top1 79.179688   Top5 97.742188   BatchTime 0.383747   LR 0.002200
0.78058255
0.77965915
0.77893889
0.77862346
0.77888048
0.77905846
0.77869183
0.77811313
0.77771664
0.77692479
0.77620929
0.77540320
0.77565181
0.77654243
0.77784717
0.77897221
0.77991402
0.78084642
0.78308094
0.78306246
0.78320932
INFO - Training [14][  120/  196]   Loss 0.633163   Top1 79.410807   Top5 97.809245   BatchTime 0.382099   LR 0.002186
0.78337646
0.78352600
0.78391159
0.78412867
0.78438920
0.78448802
0.78473586
0.78524643
0.78601390
0.78761637
0.78915966
0.79109925
0.79098195
0.79102027
0.79103643
0.79090291
0.79090488
0.79083961
0.79112858
0.79104143
0.79106748
0.79110372
INFO - Training [14][  140/  196]   Loss 0.635470   Top1 79.383371   Top5 97.779018   BatchTime 0.381476   LR 0.002173
0.79114717
0.79118967
0.79127198
0.79128730
0.79126018
0.79104477
0.79074973
0.79060686
0.79032451
0.79010254
0.78997254
0.78981376
0.78950077
0.78927273
0.78899509
0.78893143
0.78909612
0.78897882
0.78882086
0.78866863
0.78824145
0.78778529
0.78760493
INFO - Training [14][  160/  196]   Loss 0.633689   Top1 79.392090   Top5 97.775879   BatchTime 0.377919   LR 0.002159
0.78737444
0.78704083
0.78687358
0.78693885
0.78680694
0.78653610
0.78646809
0.78629589
0.78623199
0.78628415
0.78636956
0.78659344
0.78670985
0.78666997
0.78669524
0.78673947
0.78678262
INFO - Training [14][  180/  196]   Loss 0.632761   Top1 79.390191   Top5 97.719184   BatchTime 0.374503   LR 0.002145
0.78868794
0.78832197
0.78770584
0.78716624
0.78638256
0.78500372
0.78394645
0.78253901
0.78004473
0.77981442
0.78049034
0.78144771
0.78167838
0.78203505
0.78244394
0.78307831
INFO - ==> Top1: 79.454    Top5: 97.762    Loss: 0.632
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [14][   20/   40]   Loss 0.531806   Top1 83.281250   Top5 99.023438   BatchTime 0.128869
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.4199)
features.1.conv.0 tensor(0.0378)
features.1.conv.3 tensor(0.0787)
features.1.conv.6
INFO - Validation [14][   40/   40]   Loss 0.520357   Top1 83.300000   Top5 99.160000   BatchTime 0.091344
INFO - ==> Top1: 83.300    Top5: 99.160    Loss: 0.520
INFO - ==> Sparsity : 0.405
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 83.300   Top5: 99.160]
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 82.830   Top5: 99.130]
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.0486)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0877)
features.3.conv.0 tensor(0.0443)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.2400)
features.4.conv.0 tensor(0.0649)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.1019)
features.5.conv.0 tensor(0.2814)
features.5.conv.3 tensor(0.1019)
features.5.conv.6 tensor(0.1157)
features.6.conv.0 tensor(0.0280)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0732)
features.7.conv.0 tensor(0.0892)
features.7.conv.3 tensor(0.1097)
features.7.conv.6 tensor(0.1461)
features.8.conv.0 tensor(0.0926)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.2030)
features.9.conv.0 tensor(0.1088)
features.9.conv.3 tensor(0.1675)
features.9.conv.6 tensor(0.1606)
features.10.conv.0 tensor(0.0444)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.0808)
features.11.conv.0 tensor(0.5150)
features.11.conv.3 tensor(0.1480)
features.11.conv.6 tensor(0.6098)
features.12.conv.0 tensor(0.5524)
features.12.conv.3 tensor(0.1208)
features.12.conv.6 tensor(0.7485)
features.13.conv.0 tensor(0.1232)
features.13.conv.3 tensor(0.1547)
features.13.conv.6 tensor(0.1139)
features.14.conv.0 tensor(0.9580)
features.14.conv.3 tensor(0.0970)
features.14.conv.6 tensor(0.9556)
features.15.conv.0 tensor(0.9441)
features.15.conv.3 tensor(0.0752)
features.15.conv.6 tensor(0.3179)
features.16.conv.0 tensor(0.0941)
features.16.conv.3 tensor(0.1016)
features.16.conv.6 tensor(0.3084)
conv.0 tensor(0.2448)
tensor(886176.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
0.78398091
0.78471839
0.78515261
0.78623301
0.78613657
0.78604048
0.78607178
0.78573936
0.78523839
0.78466493
0.78421295
0.78397536
0.78393227
0.78411406
0.78401923
0.78296500
0.78217226
0.78182179
0.78168923
INFO - Training [15][   20/  196]   Loss 0.633971   Top1 79.726562   Top5 97.265625   BatchTime 0.417723   LR 0.002120
0.78091568
0.77887201
0.77750009
0.77540970
0.77632964
0.77739918
0.77807385
0.77874142
0.77893919
0.77951330
0.78057450
0.78190261
0.78290802
0.78360677
0.78409714
0.78401589
0.78467536
0.78532654
0.78593481
0.78641748
0.78782499
0.78761250
INFO - Training [15][   40/  196]   Loss 0.629117   Top1 79.843750   Top5 97.597656   BatchTime 0.388595   LR 0.002106
0.78723967
0.78688812
0.78654909
0.78626734
0.78596932
0.78589845
0.78555220
0.78551739
0.78535169
0.78532362
0.78549641
0.78665125
0.78645366
0.78601104
0.78539956
0.78499120
INFO - Training [15][   60/  196]   Loss 0.633999   Top1 79.459635   Top5 97.669271   BatchTime 0.383032   LR 0.002091
0.78426117
0.78369647
0.78338474
0.78320128
0.78325820
0.78350562
0.78352123
0.78340644
0.78314871
0.78296256
0.78277653
0.78278625
0.78279519
0.78293222
0.78378016
0.78590101
0.78718710
0.78661257
0.78614163
0.78550261
0.78497165
0.78446031
0.78413546
INFO - Training [15][   80/  196]   Loss 0.632540   Top1 79.467773   Top5 97.758789   BatchTime 0.372476   LR 0.002076
0.78353840
0.78305662
0.78225058
0.78145033
0.78111857
0.78099620
0.78061640
0.78005469
0.77957618
0.77902573
0.77868015
0.77853495
0.77848661
0.77831250
0.77794594
0.77752072
0.77690011
0.77621460
0.77521056
0.77447402
0.77403498
INFO - Training [15][  100/  196]   Loss 0.624025   Top1 79.640625   Top5 97.824219   BatchTime 0.375827   LR 0.002061
0.77370453
0.77395022
0.77172291
0.76890999
0.76661408
0.76713037
0.76615751
0.76705891
0.77026421
0.77101946
0.77130151
0.77110660
0.76993167
0.76819366
0.76763791
0.77130073
INFO - Training [15][  120/  196]   Loss 0.619627   Top1 79.736328   Top5 97.913411   BatchTime 0.374676   LR 0.002045
0.77295846
0.77436060
0.77628618
0.77817625
0.78008956
0.78214586
0.78487462
0.78762543
0.78895330
0.78886944
0.78855950
0.78804260
0.78802747
0.78792059
0.78759634
0.78727525
0.78705239
0.78707105
0.78725332
0.78703433
0.78676659
0.78649002
0.78618753
0.78589541
0.78568524
INFO - Training [15][  140/  196]   Loss 0.616504   Top1 79.958147   Top5 97.971540   BatchTime 0.367423   LR 0.002030
0.78513610
0.78393286
0.78360379
0.78275049
0.78279018
0.78286499
0.78266627
0.78254575
0.78221411
0.78189796
0.78182000
0.78140897
0.78117627
0.78104776
0.78081560
0.78012335
0.77945256
0.77864051
INFO - Training [15][  160/  196]   Loss 0.619484   Top1 79.880371   Top5 97.939453   BatchTime 0.361632   LR 0.002014
0.77776784
0.77701312
0.77634829
0.77581459
0.77494174
0.77419347
0.77316010
0.77155906
0.77003682
0.76865935
0.76827133
0.76862699
0.76936334
0.76894706
0.76916558
0.76808286
0.76744932
0.76782763
0.76757628
0.76681340
0.76845729
INFO - Training [15][  180/  196]   Loss 0.618341   Top1 79.852431   Top5 97.912326   BatchTime 0.364840   LR 0.001998
0.76926196
0.77024472
0.77058321
0.77328187
0.77502614
0.77804679
0.77942973
0.78088194
0.78222489
0.78332734
0.78445292
0.78526515
0.78629917
0.79086423
0.79098409
INFO - ==> Top1: 79.894    Top5: 97.896    Loss: 0.617
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [15][   20/   40]   Loss 0.532389   Top1 83.007812   Top5 98.906250   BatchTime 0.131779
INFO - Validation [15][   40/   40]   Loss 0.527124   Top1 82.940000   Top5 99.070000   BatchTime 0.093183
INFO - ==> Top1: 82.940    Top5: 99.070    Loss: 0.527
INFO - ==> Sparsity : 0.395
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 83.300   Top5: 99.160]
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 82.940   Top5: 99.070]
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 82.830   Top5: 99.130]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.4277)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0734)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0900)
features.3.conv.0 tensor(0.0437)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.2201)
features.4.conv.0 tensor(0.0617)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.1232)
features.5.conv.0 tensor(0.2515)
features.5.conv.3 tensor(0.1042)
features.5.conv.6 tensor(0.1126)
features.6.conv.0 tensor(0.0299)
features.6.conv.3 tensor(0.0613)
features.6.conv.6 tensor(0.0763)
features.7.conv.0 tensor(0.0689)
features.7.conv.3 tensor(0.1183)
features.7.conv.6 tensor(0.1660)
features.8.conv.0 tensor(0.0981)
features.8.conv.3 tensor(0.1152)
features.8.conv.6 tensor(0.1250)
features.9.conv.0 tensor(0.0965)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.1469)
features.10.conv.0 tensor(0.0475)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.0792)
features.11.conv.0 tensor(0.4905)
features.11.conv.3 tensor(0.1431)
features.11.conv.6 tensor(0.5799)
features.12.conv.0 tensor(0.5996)
features.12.conv.3 tensor(0.1213)
features.12.conv.6 tensor(0.7528)
features.13.conv.0 tensor(0.1196)
features.13.conv.3 tensor(0.1562)
features.13.conv.6 tensor(0.1321)
features.14.conv.0 tensor(0.9640)
features.14.conv.3 tensor(0.1012)
features.14.conv.6 tensor(0.9653)
features.15.conv.0 tensor(0.9467)
features.15.conv.3 tensor(0.0740)
features.15.conv.6 tensor(0.3347)
features.16.conv.0 tensor(0.0787)
features.16.conv.3 tensor(0.1067)
features.16.conv.6 tensor(0.2253)
conv.0 tensor(0.2513)
tensor(865141.) 2188896.0
0.79054672
0.79029769
0.78996098
0.78976846
0.78934211
0.78890836
0.78866816
0.78850704
0.78839016
0.78831303
0.78814977
0.78785408
0.78769147
0.78755796
0.78767920
0.78774619
0.78742707
0.78729367
0.78698736
0.78669667
INFO - Training [16][   20/  196]   Loss 0.613367   Top1 79.589844   Top5 97.617188   BatchTime 0.472736   LR 0.001969
0.78618085
0.78561437
0.78526837
0.78507370
0.78498608
0.78506243
0.78506738
0.78480762
0.78460413
0.78410888
0.78356844
0.78325629
0.78294903
0.78270394
0.78239912
0.78219438
0.78224504
0.78188580
0.78169179
0.78105062
0.78059715
INFO - Training [16][   40/  196]   Loss 0.613797   Top1 79.726562   Top5 97.753906   BatchTime 0.420258   LR 0.001953
0.78019601
0.77969414
0.77924484
0.77867240
0.77814341
0.77766746
0.77703124
0.77660352
0.77620965
0.77569187
0.77514088
0.77463365
0.77407682
0.77310431
0.77164251
0.77227551
0.77252537
0.77305794
0.77367675
0.77397054
0.77427369
0.77427959
INFO - Training [16][   60/  196]   Loss 0.612723   Top1 79.752604   Top5 97.858073   BatchTime 0.406617   LR 0.001936
0.77455562
0.77462757
0.77462995
0.77461761
0.77469182
0.77484113
0.77494669
0.77513164
0.77540845
0.77557600
0.77566552
0.77571428
0.77597481
0.77613765
0.77624029
0.77646029
INFO - Training [16][   80/  196]   Loss 0.607780   Top1 80.068359   Top5 97.983398   BatchTime 0.394442   LR 0.001919
0.77678883
0.77694130
0.77721602
0.77798444
0.77863926
0.78028387
0.78043836
0.78047407
0.78070086
0.78043973
0.77985573
0.77976042
0.77952814
0.77951086
0.77937406
0.77923650
0.77904755
0.77859336
0.77810097
0.77742970
0.77697736
0.77654833
0.77609950
INFO - Training [16][  100/  196]   Loss 0.597821   Top1 80.402344   Top5 98.023438   BatchTime 0.388269   LR 0.001902
0.77557492
0.77524066
0.77492851
0.77480608
0.77458793
0.77442807
0.77410048
0.77361053
0.77313441
0.77272493
0.77248698
0.77217633
0.77190262
0.77188581
0.77315545
0.77227926
0.77162975
0.77101201
INFO - Training [16][  120/  196]   Loss 0.598088   Top1 80.387370   Top5 98.069661   BatchTime 0.378951   LR 0.001885
0.77062160
0.77031749
0.76996088
0.76956922
0.76921123
0.77016789
0.76998287
0.76930803
0.76695496
0.76465452
0.76156902
0.75555712
0.75035256
0.75649625
0.76303375
0.76625961
0.76804799
0.76841962
0.76833099
INFO - Training [16][  140/  196]   Loss 0.596277   Top1 80.485491   Top5 98.108259   BatchTime 0.369145   LR 0.001867
0.76827586
0.76803964
0.76753056
0.76710856
0.76663959
0.76619762
0.76576030
0.76494241
0.76401520
0.76272321
0.76170969
0.76060539
0.75932962
0.75757891
0.75514108
0.75346398
0.75156230
0.75085181
0.75204200
0.75162023
0.75022691
0.74954069
INFO - Training [16][  160/  196]   Loss 0.595820   Top1 80.544434   Top5 98.125000   BatchTime 0.367792   LR 0.001850
0.74722737
0.74457449
0.74452478
0.74655622
0.74766225
0.74860543
0.74897426
0.74866778
0.74883062
0.74970776
0.75224066
0.75475144
0.75664037
0.75774264
0.75906932
0.76025963
0.76198918
0.76322973
0.76348603
0.76364052
0.76418525
INFO - Training [16][  180/  196]   Loss 0.595412   Top1 80.598958   Top5 98.096788   BatchTime 0.369880   LR 0.001832
0.76430434
0.76456589
0.76462734
0.76478773
0.76500213
0.76522893
0.76569599
0.76614386
0.76645702
0.76667935
0.76708943
0.76748490
INFO - ==> Top1: 80.642    Top5: 98.106    Loss: 0.593
0.76776946
0.76801556
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.456452   Top1 85.273438   Top5 99.062500   BatchTime 0.129649
INFO - Validation [16][   40/   40]   Loss 0.449590   Top1 85.250000   Top5 99.160000   BatchTime 0.091016
INFO - ==> Top1: 85.250    Top5: 99.160    Loss: 0.450
INFO - ==> Sparsity : 0.395
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 85.250   Top5: 99.160]
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 83.300   Top5: 99.160]
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 82.940   Top5: 99.070]
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.4141)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0707)
features.2.conv.0 tensor(0.0417)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0880)
features.3.conv.0 tensor(0.0425)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2142)
features.4.conv.0 tensor(0.0610)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0986)
features.5.conv.0 tensor(0.2775)
features.5.conv.3 tensor(0.1007)
features.5.conv.6 tensor(0.1112)
features.6.conv.0 tensor(0.0298)
features.6.conv.3 tensor(0.0602)
features.6.conv.6 tensor(0.0724)
features.7.conv.0 tensor(0.0762)
features.7.conv.3 tensor(0.1166)
features.7.conv.6 tensor(0.1595)
features.8.conv.0 tensor(0.0844)
features.8.conv.3 tensor(0.1134)
features.8.conv.6 tensor(0.2144)
features.9.conv.0 tensor(0.1141)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.3111)
features.10.conv.0 tensor(0.0435)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.0761)
features.11.conv.0 tensor(0.4901)
features.11.conv.3 tensor(0.1433)
features.11.conv.6 tensor(0.6012)
features.12.conv.0 tensor(0.5888)
features.12.conv.3 tensor(0.1194)
features.12.conv.6 tensor(0.7582)
features.13.conv.0 tensor(0.1264)
features.13.conv.3 tensor(0.1534)
features.13.conv.6 tensor(0.2659)
features.14.conv.0 tensor(0.9660)
features.14.conv.3 tensor(0.0984)
features.14.conv.6 tensor(0.9647)
features.15.conv.0 tensor(0.9492)
features.15.conv.3 tensor(0.0745)
features.15.conv.6 tensor(0.3389)
features.16.conv.0 tensor(0.0766)
features.16.conv.3 tensor(0.1046)
features.16.conv.6 tensor(0.2601)
conv.0 tensor(0.1748)
tensor(864931.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
0.76830339
0.76863718
0.76886898
0.76892889
0.76887858
0.76946938
0.77728611
0.77707028
0.77700561
0.77671039
0.77649778
0.77624053
0.77606565
0.77588898
0.77567327
0.77563649
0.77558881
0.77550441
INFO - Training [17][   20/  196]   Loss 0.615549   Top1 79.394531   Top5 97.597656   BatchTime 0.500162   LR 0.001800
0.77525061
0.77496463
0.77465969
0.77435470
0.77408266
0.77365404
0.77326673
0.77300465
0.77250373
0.77209371
0.77167803
0.77126569
0.77073199
0.77013332
0.76926762
0.76859587
0.76795006
0.76733488
0.76656020
0.76606852
0.76549667
INFO - Training [17][   40/  196]   Loss 0.598043   Top1 80.341797   Top5 97.753906   BatchTime 0.440454   LR 0.001782
0.76503485
0.76398355
0.76280421
0.76112199
0.75970840
0.75816995
0.75780505
0.75737607
0.75739074
0.75739622
0.75666660
0.75691748
0.75739217
0.75703365
0.75709200
0.75678849
0.75627816
0.75591046
0.75634527
0.75298363
0.75000340
0.74760693
INFO - Training [17][   60/  196]   Loss 0.589818   Top1 80.598958   Top5 97.805990   BatchTime 0.415077   LR 0.001764
0.74600077
0.74398041
0.74352986
0.74331027
0.74382919
0.74463451
0.74467444
0.74470627
0.74493027
0.74535620
0.74577516
0.74684334
0.74732727
0.74865234
0.74910218
0.74895412
0.75073498
INFO - Training [17][   80/  196]   Loss 0.592961   Top1 80.502930   Top5 97.905273   BatchTime 0.399335   LR 0.001746
0.75269848
0.75376946
0.75523067
0.75647825
0.75753587
0.75892258
0.76055485
0.76192814
0.76326185
0.76437682
0.76522791
0.76579058
0.76624316
0.76653796
0.76685423
0.76722473
0.76770771
0.76790184
0.76806706
0.76833278
0.76796126
INFO - Training [17][  100/  196]   Loss 0.588534   Top1 80.820312   Top5 97.925781   BatchTime 0.394083   LR 0.001727
0.76799011
0.76779860
0.76767445
0.76776224
0.76813054
0.76831394
0.76847297
0.76836032
0.76837939
0.76850247
0.76847494
0.76820320
0.76846558
0.76916856
0.77231926
0.77190882
0.77159840
0.77144891
0.77109009
INFO - Training [17][  120/  196]   Loss 0.584138   Top1 80.983073   Top5 97.998047   BatchTime 0.383719   LR 0.001708
0.77078736
0.77070773
0.77055472
0.77000874
0.76933962
0.76865792
0.76775730
0.76687223
0.76597369
0.76551473
0.76464176
0.76401466
0.76361328
0.76282805
0.76153779
0.76018751
0.75946486
0.75772363
0.75626665
0.75561094
0.75639242
0.75602543
INFO - Training [17][  140/  196]   Loss 0.580615   Top1 81.222098   Top5 98.063616   BatchTime 0.379726   LR 0.001690
0.75590897
0.75558180
0.75535303
0.75588900
0.75612891
0.75634438
0.75625139
0.75607443
0.75504071
0.75315142
0.75187975
0.75154877
0.75116998
0.75097281
0.75071788
0.75159740
0.75119489
0.75082946
0.75077438
0.75055015
0.74808210
INFO - Training [17][  160/  196]   Loss 0.582422   Top1 81.157227   Top5 98.063965   BatchTime 0.381458   LR 0.001671
0.74667448
0.74633175
0.74957383
0.75494385
0.75805557
0.76208550
0.76785326
0.77156669
0.77423519
0.77637202
0.77580374
0.77548945
0.77518880
0.77475190
0.77438557
0.77405727
0.77379614
0.77347267
0.77324116
0.77294600
0.77258897
INFO - Training [17][  180/  196]   Loss 0.581109   Top1 81.223958   Top5 98.036024   BatchTime 0.381052   LR 0.001652
0.77256584
0.77256125
0.77257031
0.77249807
0.77255327
0.77251494
0.77264798
0.77270848
0.77267957
0.77264810
0.77269220
0.77255177
0.77246529
0.77243811
********************pre-trained*****************
INFO - ==> Top1: 81.218    Top5: 98.040    Loss: 0.581
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [17][   20/   40]   Loss 0.604357   Top1 80.761719   Top5 98.593750   BatchTime 0.125750
features.0.conv.0 tensor(0.5278)
features.0.conv.3 tensor(0.4297)
features.1.conv.0 tensor(0.0430)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0454)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0894)
features.3.conv.0 tensor(0.0411)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.2296)
features.4.conv.0 tensor(0.0623)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.1144)
features.5.conv.0 tensor(0.2842)
features.5.conv.3 tensor(0.1019)
features.5.conv.6 tensor(0.1302)
features.6.conv.0 tensor(0.0355)
features.6.conv.3 tensor(0.0613)
features.6.conv.6 tensor(0.0716)
features.7.conv.0 tensor(0.0826)
features.7.conv.3 tensor(0.1192)
features.7.conv.6 tensor(0.1287)
features.8.conv.0 tensor(0.0913)
features.8.conv.3 tensor(0.1155)
features.8.conv.6 tensor(0.1337)
features.9.conv.0 tensor(0.1150)
features.9.conv.3 tensor(0.1629)
features.9.conv.6 tensor(0.1709)
features.10.conv.0 tensor(0.0459)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.0751)
features.11.conv.0 tensor(0.4860)
features.11.conv.3 tensor(0.1406)
features.11.conv.6 tensor(0.6219)
features.12.conv.0 tensor(0.6564)
features.12.conv.3 tensor(0.1179)
features.12.conv.6 tensor(0.7924)
features.13.conv.0 tensor(0.1244)
features.13.conv.3 tensor(0.1537)
features.13.conv.6 tensor(0.2917)
features.14.conv.0 tensor(0.9653)
features.14.conv.3 tensor(0.0994)
features.14.conv.6 tensor(0.9697)
features.15.conv.0 tensor(0.9517)
features.15.conv.3 tensor(0.0730)
features.15.conv.6 tensor(0.3572)
features.16.conv.0 tensor(0.0893)
features.16.conv.3 tensor(0.1056)
features.16.conv.6 tensor(0.2829)
conv.0 tensor(0.1436)
tensor(868377.) 2188896.0
INFO - Validation [17][   40/   40]   Loss 0.590395   Top1 80.680000   Top5 98.820000   BatchTime 0.089478
INFO - ==> Top1: 80.680    Top5: 98.820    Loss: 0.590
INFO - ==> Sparsity : 0.397
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 85.250   Top5: 99.160]
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 83.300   Top5: 99.160]
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 82.940   Top5: 99.070]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.77230424
0.77643013
0.77895719
0.78121823
0.78111285
0.78109539
0.78093857
0.78088552
0.78081912
0.78086561
0.78078145
0.78071851
0.78066987
0.78073269
0.78082538
0.78095335
0.78101134
0.78119725
0.78128201
0.78127533
0.78130186
0.78134125
INFO - Training [18][   20/  196]   Loss 0.584606   Top1 80.820312   Top5 97.402344   BatchTime 0.478473   LR 0.001618
0.78127223
0.78128296
0.78125429
0.78139609
0.78132123
0.78133518
0.78190017
0.78212357
0.78188413
0.78159481
0.78129572
0.78048944
0.77991945
0.77924651
0.77871954
0.77812958
INFO - Training [18][   40/  196]   Loss 0.587288   Top1 80.732422   Top5 97.626953   BatchTime 0.431193   LR 0.001599
0.77791584
0.77772170
0.77797079
0.77811688
0.77806789
0.77754736
0.77770656
0.77783269
0.77770787
0.77785450
0.77807182
0.77784020
0.77734315
0.77695334
0.77656579
0.77634305
0.77621984
0.77609432
0.77558798
0.77459961
0.77400124
INFO - Training [18][   60/  196]   Loss 0.583127   Top1 80.885417   Top5 97.832031   BatchTime 0.414141   LR 0.001579
0.77349001
0.77312076
0.77218693
0.77162045
0.77128178
0.77096939
0.77037197
0.76990891
0.76974356
0.76996207
0.77007407
0.77068675
0.77183163
0.77353948
0.77495795
0.77633202
0.77747244
0.77865922
0.78008103
0.78253615
0.78273922
0.78268778
INFO - Training [18][   80/  196]   Loss 0.583556   Top1 80.908203   Top5 97.924805   BatchTime 0.400298   LR 0.001560
0.78257030
0.78237742
0.78224337
0.78207463
0.78186339
0.78169525
0.78164619
0.78312159
0.78275096
0.78260964
0.78252202
0.78250009
0.78254002
0.78257853
0.78273726
0.78268456
0.78280878
0.78289479
0.78297007
INFO - Training [18][  100/  196]   Loss 0.575767   Top1 81.214844   Top5 98.000000   BatchTime 0.384387   LR 0.001540
0.78301120
0.78308779
0.78293282
0.78273606
0.78240073
0.78210843
0.78202730
0.78205508
0.78193527
0.78193969
0.78189224
0.78181219
0.78161466
0.78158313
0.78147179
0.78146034
0.78165418
0.78318149
0.78304255
0.78269261
0.78240496
0.78193420
0.78139883
INFO - Training [18][  120/  196]   Loss 0.564127   Top1 81.621094   Top5 98.108724   BatchTime 0.378045   LR 0.001521
0.78072333
0.78005803
0.77951032
0.77901733
0.77854306
0.77822876
0.77805674
0.77792478
0.77759832
0.77720743
0.77718407
0.77671957
0.77653068
0.77616715
0.77555466
0.77484661
0.77402383
0.77337593
0.77244997
INFO - Training [18][  140/  196]   Loss 0.564751   Top1 81.704799   Top5 98.186384   BatchTime 0.381244   LR 0.001501
0.77169043
0.77015102
0.76862514
0.76668131
0.76410532
0.76201940
0.76120806
0.76094484
0.76111722
0.76140428
0.76124495
0.76065046
0.76027501
0.75972706
0.75951737
0.75881332
INFO - Training [18][  160/  196]   Loss 0.566853   Top1 81.606445   Top5 98.171387   BatchTime 0.382027   LR 0.001482
0.75820386
0.75697929
0.75639588
0.75499177
0.75446731
0.75269485
0.75119972
0.74942911
0.75017411
0.75045103
0.74898487
0.74798191
0.74823582
0.74831617
0.74865758
0.74858832
0.74800730
0.74802828
0.74899912
0.74916363
0.74952644
0.75001770
INFO - Training [18][  180/  196]   Loss 0.565555   Top1 81.614583   Top5 98.140191   BatchTime 0.380601   LR 0.001462
0.75022811
0.75071716
0.75263929
0.75573665
0.75823849
0.76032966
0.76308733
0.76676124
0.76917505
0.77164626
0.77397555
0.77598226
0.77552706
0.77381998
0.76957184
0.77200848
INFO - ==> Top1: 81.714    Top5: 98.150    Loss: 0.562
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 0.578993   Top1 82.460938   Top5 99.003906   BatchTime 0.137066
features.0.conv.0 tensor(0.5208)
features.0.conv.3 tensor(0.4375)
features.1.conv.0 tensor(0.0423)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0773)
features.2.conv.0 tensor(0.0527)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0929)
features.3.conv.0 tensor(0.0356)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2300)
features.4.conv.0 tensor(0.0684)
features.4.conv.3 tensor(0.0926)
features.4.conv.6 tensor(0.1029)
features.5.conv.0 tensor(0.2606)
features.5.conv.3 tensor(0.1024)
features.5.conv.6 tensor(0.1159)
features.6.conv.0 tensor(0.0265)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0706)
features.7.conv.0 tensor(0.0696)
features.7.conv.3 tensor(0.1146)
features.7.conv.6 tensor(0.1107)
features.8.conv.0 tensor(0.1075)
features.8.conv.3 tensor(0.1137)
features.8.conv.6 tensor(0.1275)
features.9.conv.0 tensor(0.1188)
features.9.conv.3 tensor(0.1661)
features.9.conv.6 tensor(0.1394)
features.10.conv.0 tensor(0.0486)
features.10.conv.3 tensor(0.0946)
features.10.conv.6 tensor(0.0810)
features.11.conv.0 tensor(0.5392)
features.11.conv.3 tensor(0.1412)
features.11.conv.6 tensor(0.6315)
features.12.conv.0 tensor(0.6002)
features.12.conv.3 tensor(0.1161)
features.12.conv.6 tensor(0.7580)
features.13.conv.0 tensor(0.2587)
features.13.conv.3 tensor(0.1557)
features.13.conv.6 tensor(0.3583)
features.14.conv.0 tensor(0.9661)
features.14.conv.3 tensor(0.0980)
features.14.conv.6 tensor(0.9700)
features.15.conv.0 tensor(0.9522)
features.15.conv.3 tensor(0.0721)
features.15.conv.6 tensor(0.3769)
features.16.conv.0 tensor(0.1502)
features.16.conv.3 tensor(0.1036)
features.16.conv.6 tensor(0.2803)
conv.0 tensor(0.1352)
tensor(887545.) 2188896.0
INFO - Validation [18][   40/   40]   Loss 0.569601   Top1 82.580000   Top5 99.140000   BatchTime 0.094835
INFO - ==> Top1: 82.580    Top5: 99.140    Loss: 0.570
INFO - ==> Sparsity : 0.405
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 85.250   Top5: 99.160]
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 83.300   Top5: 99.160]
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 82.940   Top5: 99.070]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
0.77452260
0.77562863
0.77540463
0.77554005
0.77515894
0.77509725
0.77500176
0.77487278
0.77496439
0.77493304
0.77496558
0.77497059
0.77490664
0.77482909
0.77472645
0.77466661
0.77452809
0.77446473
0.77432114
0.77442664
0.77443659
0.77436119
0.77445894
INFO - Training [19][   20/  196]   Loss 0.569059   Top1 81.308594   Top5 97.812500   BatchTime 0.449988   LR 0.001427
0.77436262
0.77436000
0.77437621
0.77448702
0.77462894
0.77498841
0.77524263
0.77551782
0.77560973
0.77581066
0.77611166
0.77611035
0.77632791
0.77650261
0.77673006
0.77671397
INFO - Training [19][   40/  196]   Loss 0.553617   Top1 82.060547   Top5 98.007812   BatchTime 0.408366   LR 0.001407
0.77676779
0.77672130
0.77672637
0.77673090
0.77668810
0.77669865
0.77660179
0.77654332
0.77651489
0.77648002
0.77646142
0.77637547
0.77612847
0.77590853
0.77576077
0.77574801
0.77543443
0.77527362
0.77522010
0.77495962
0.77464688
INFO - Training [19][   60/  196]   Loss 0.552040   Top1 82.141927   Top5 98.059896   BatchTime 0.402888   LR 0.001387
0.77468759
0.77462363
0.77452683
0.77442914
0.77452910
0.77447724
0.77454352
0.77433449
0.77421480
0.77400196
0.77400434
0.77381831
0.77367121
0.77320892
0.77283996
0.77257234
0.77227181
0.77192533
INFO - Training [19][   80/  196]   Loss 0.555816   Top1 81.992188   Top5 98.076172   BatchTime 0.383490   LR 0.001367
0.77128804
0.77099419
0.77075344
0.77045864
0.77008986
0.76984936
0.76952338
0.76927543
0.76908714
0.76857436
0.76810330
0.76793343
0.76757520
0.76741821
0.76734519
0.76690853
0.76646191
0.76612073
0.76584750
0.76556653
0.76534790
0.76517475
0.76519781
0.76527995
INFO - Training [19][  100/  196]   Loss 0.548711   Top1 82.175781   Top5 98.187500   BatchTime 0.374847   LR 0.001347
0.76543391
0.76561409
0.76557231
0.76531005
0.76567757
0.76608771
0.76640749
0.77584422
0.77584016
0.77792054
0.77774489
0.77744919
0.77712369
0.77705610
0.77693737
0.77670997
INFO - Training [19][  120/  196]   Loss 0.543345   Top1 82.382812   Top5 98.291016   BatchTime 0.372572   LR 0.001327
0.77643120
0.77596337
0.77548766
0.77516896
0.77500957
0.77471071
0.77446526
0.77416003
0.77391917
0.77376932
0.77360761
0.77333772
0.77315623
0.77306771
0.77293259
0.77291757
0.77284014
0.77285755
0.77282321
0.77257794
0.77239472
0.77215022
INFO - Training [19][  140/  196]   Loss 0.544825   Top1 82.329799   Top5 98.300781   BatchTime 0.372592   LR 0.001307
0.77183241
0.77156061
0.77126366
0.77097887
0.77062708
0.77004254
0.76921523
0.76871145
0.76833779
0.76809573
0.76776361
0.76750678
0.76735836
0.76714069
0.76696306
0.76687121
0.76667964
0.76635373
0.76614785
0.76606381
0.76570427
INFO - Training [19][  160/  196]   Loss 0.549443   Top1 82.197266   Top5 98.269043   BatchTime 0.374510   LR 0.001287
0.76549983
0.76534176
0.76499301
0.76457357
0.76411808
0.76368040
0.76325095
0.76302236
0.76292723
0.76292294
0.76268363
0.76261890
0.76254511
0.76235312
0.76221341
0.76232547
0.76218945
0.76216942
0.76216882
0.76212305
0.76216775
0.76173663
INFO - Training [19][  180/  196]   Loss 0.548745   Top1 82.274306   Top5 98.235677   BatchTime 0.372760   LR 0.001266
0.76171851
0.76162219
0.76163059
0.76145494
0.76121610
0.76111275
0.76089275
0.76073742
0.76075506
0.76077664
0.76073867
0.76053905
0.76028305
********************pre-trained*****************
INFO - ==> Top1: 82.322    Top5: 98.244    Loss: 0.547
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [19][   20/   40]   Loss 0.699928   Top1 77.500000   Top5 98.457031   BatchTime 0.141460
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.4316)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0790)
features.2.conv.0 tensor(0.0498)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0946)
features.3.conv.0 tensor(0.0388)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.2313)
features.4.conv.0 tensor(0.0669)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.1214)
features.5.conv.0 tensor(0.2508)
features.5.conv.3 tensor(0.1036)
features.5.conv.6 tensor(0.1917)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0710)
features.7.conv.0 tensor(0.0675)
features.7.conv.3 tensor(0.1137)
features.7.conv.6 tensor(0.2421)
features.8.conv.0 tensor(0.1098)
features.8.conv.3 tensor(0.1114)
features.8.conv.6 tensor(0.2166)
features.9.conv.0 tensor(0.1192)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.2754)
features.10.conv.0 tensor(0.0511)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.0769)
features.11.conv.0 tensor(0.5671)
features.11.conv.3 tensor(0.1397)
features.11.conv.6 tensor(0.6262)
features.12.conv.0 tensor(0.6010)
features.12.conv.3 tensor(0.1167)
features.12.conv.6 tensor(0.7699)
features.13.conv.0 tensor(0.1119)
features.13.conv.3 tensor(0.1508)
features.13.conv.6 tensor(0.1016)
features.14.conv.0 tensor(0.9696)
features.14.conv.3 tensor(0.0944)
features.14.conv.6 tensor(0.9668)
features.15.conv.0 tensor(0.9545)
features.15.conv.3 tensor(0.0699)
features.15.conv.6 tensor(0.3838)
features.16.conv.0 tensor(0.1699)
features.16.conv.3 tensor(0.1046)
features.16.conv.6 tensor(0.4193)
conv.0 tensor(0.1784)
tensor(931659.) 2188896.0
INFO - Validation [19][   40/   40]   Loss 0.692527   Top1 77.790000   Top5 98.410000   BatchTime 0.098060
INFO - ==> Top1: 77.790    Top5: 98.410    Loss: 0.693
INFO - ==> Sparsity : 0.426
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 85.250   Top5: 99.160]
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 83.300   Top5: 99.160]
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 82.940   Top5: 99.070]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.75961912
0.75900507
0.75870705
0.75830591
0.75786078
0.75726563
0.75702542
0.75659829
0.75591606
0.75519192
0.75329596
0.75177872
0.75075871
0.74939424
0.74768776
0.74866378
0.75051290
0.75598985
0.75753641
INFO - Training [20][   20/  196]   Loss 0.553093   Top1 82.070312   Top5 97.558594   BatchTime 0.441932   LR 0.001231
0.76060027
0.76246935
0.76381940
0.76497716
0.76612496
0.76723301
0.76838571
0.76932126
0.77191275
0.77222401
0.77240187
0.77268457
0.77289522
0.77361685
0.77485251
0.77726531
0.77827984
0.77824265
0.77813232
0.77870476
0.78098232
0.78076899
INFO - Training [20][   40/  196]   Loss 0.550732   Top1 82.119141   Top5 97.939453   BatchTime 0.404226   LR 0.001211
0.78049946
0.78019732
0.78003746
0.77978939
0.77951580
0.77907073
0.77854973
0.77836967
0.77809495
0.77761257
0.77730614
0.77702761
0.77680051
0.77649480
0.77626473
0.77603918
0.77592063
INFO - Training [20][   60/  196]   Loss 0.545613   Top1 82.460938   Top5 98.020833   BatchTime 0.390998   LR 0.001191
0.77570587
0.77579153
0.77569830
0.77535176
0.77523756
0.77488476
0.77443576
0.77407700
0.77366269
0.77324700
0.77289128
0.77259701
0.77220052
0.77202213
0.77175063
0.77155542
0.77134323
0.77117616
0.77102470
0.77067178
0.77043992
0.77014786
INFO - Training [20][   80/  196]   Loss 0.543936   Top1 82.465820   Top5 98.159180   BatchTime 0.381035   LR 0.001171
0.76981038
0.76936680
0.76903653
0.76860368
0.76816249
0.76788825
0.76770455
0.76742935
0.76703697
0.76661664
0.76611990
0.76580209
0.76545209
0.76500261
0.76463443
0.76411110
0.76369858
0.76307821
0.76237082
0.76200813
INFO - Training [20][  100/  196]   Loss 0.542268   Top1 82.527344   Top5 98.292969   BatchTime 0.385092   LR 0.001151
0.76123387
0.76090389
0.76075643
0.76060641
0.76045328
0.76009506
0.75995117
0.75953460
0.75900656
0.75853574
0.75836796
0.75824505
0.75772798
0.75758272
0.75733590
0.75717193
0.75693417
0.75638127
0.75628245
0.75603735
0.75577128
0.75562394
INFO - Training [20][  120/  196]   Loss 0.538451   Top1 82.600911   Top5 98.336589   BatchTime 0.383430   LR 0.001131
0.75559413
0.75882089
0.75877732
0.75869995
0.75879884
0.75889653
0.75888902
0.75874013
0.75853753
0.75845909
0.75847870
0.75839913
0.75826186
0.75793958
0.75734997
0.75684446
0.75621289
0.75555778
0.75493008
0.75466460
0.75437659
INFO - Training [20][  140/  196]   Loss 0.536530   Top1 82.720424   Top5 98.404018   BatchTime 0.383316   LR 0.001111
0.75358540
0.75259513
0.75240034
0.75194716
0.75149012
0.75182307
0.75203425
0.75294042
0.75365454
0.75475156
0.75525528
0.75589204
0.75620472
0.75668210
0.75715119
INFO - Training [20][  160/  196]   Loss 0.537934   Top1 82.614746   Top5 98.381348   BatchTime 0.383350   LR 0.001091
0.75746143
0.75807822
0.75867981
0.75931019
0.75985241
0.76058751
0.76141709
0.76210111
0.76247913
0.76255971
0.76279515
0.76301032
0.76320481
0.76599491
0.76710343
0.76669466
0.76624334
0.76570368
0.76505911
0.76435399
0.76376855
0.76328152
0.76286745
INFO - Training [20][  180/  196]   Loss 0.535691   Top1 82.701823   Top5 98.339844   BatchTime 0.379951   LR 0.001071
0.76239306
0.76225775
0.76190341
0.76175827
0.76155949
0.76131147
0.76101875
0.76069796
0.76024652
0.75982666
0.76140380
0.76077998
0.75996584
0.75922227
0.75792855
********************pre-trained*****************
INFO - ==> Top1: 82.754    Top5: 98.346    Loss: 0.534
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [20][   20/   40]   Loss 0.399779   Top1 86.660156   Top5 99.335938   BatchTime 0.135527
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.4258)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0747)
features.2.conv.0 tensor(0.0512)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0903)
features.3.conv.0 tensor(0.0437)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.2415)
features.4.conv.0 tensor(0.0614)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.1095)
features.5.conv.0 tensor(0.2542)
features.5.conv.3 tensor(0.0943)
features.5.conv.6 tensor(0.1211)
features.6.conv.0 tensor(0.0264)
features.6.conv.3 tensor(0.0602)
features.6.conv.6 tensor(0.0712)
features.7.conv.0 tensor(0.0818)
features.7.conv.3 tensor(0.1131)
features.7.conv.6 tensor(0.1461)
features.8.conv.0 tensor(0.1103)
features.8.conv.3 tensor(0.1059)
features.8.conv.6 tensor(0.1244)
features.9.conv.0 tensor(0.1248)
features.9.conv.3 tensor(0.1606)
features.9.conv.6 tensor(0.2788)
features.10.conv.0 tensor(0.0526)
features.10.conv.3 tensor(0.0900)
features.10.conv.6 tensor(0.0567)
features.11.conv.0 tensor(0.5909)
features.11.conv.3 tensor(0.1395)
features.11.conv.6 tensor(0.6387)
features.12.conv.0 tensor(0.6409)
features.12.conv.3 tensor(0.1107)
features.12.conv.6 tensor(0.7785)
features.13.conv.0 tensor(0.1187)
features.13.conv.3 tensor(0.1551)
features.13.conv.6 tensor(0.1025)
features.14.conv.0 tensor(0.9707)
features.14.conv.3 tensor(0.0950)
features.14.conv.6 tensor(0.9744)
features.15.conv.0 tensor(0.9560)
features.15.conv.3 tensor(0.0682)
features.15.conv.6 tensor(0.3845)
features.16.conv.0 tensor(0.1861)
features.16.conv.3 tensor(0.1037)
features.16.conv.6 tensor(0.5322)
conv.0 tensor(0.2155)
tensor(985567.) 2188896.0
INFO - Validation [20][   40/   40]   Loss 0.393591   Top1 86.470000   Top5 99.420000   BatchTime 0.095431
INFO - ==> Top1: 86.470    Top5: 99.420    Loss: 0.394
INFO - ==> Sparsity : 0.450
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 85.250   Top5: 99.160]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 83.300   Top5: 99.160]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
0.75679052
0.75588846
0.75423253
0.75283355
0.75139672
0.75072330
0.75047266
0.75040931
0.75031811
0.75013250
0.75023228
0.75034571
0.75029427
0.75015980
0.75032765
0.75046885
0.75072354
0.75113660
0.75124961
INFO - Training [21][   20/  196]   Loss 0.533370   Top1 82.460938   Top5 97.890625   BatchTime 0.445090   LR 0.001036
0.75127524
0.75049388
0.74994105
0.74901026
0.74847007
0.74805731
0.74793208
0.74735564
0.74671906
0.74584669
0.74584842
0.74619204
0.74658471
0.74685532
0.74667299
0.74651068
0.74663866
0.74677056
0.74676877
0.74714214
0.74714077
0.74737841
INFO - Training [21][   40/  196]   Loss 0.532274   Top1 82.558594   Top5 98.125000   BatchTime 0.406662   LR 0.001016
0.74788916
0.74883479
0.75000936
0.75078493
0.75139540
0.75174570
0.75205159
0.75227481
0.75232917
0.75229669
0.75246322
0.75260174
0.75275570
0.75268233
0.75266683
0.75246489
0.75211090
INFO - Training [21][   60/  196]   Loss 0.529791   Top1 82.656250   Top5 98.125000   BatchTime 0.388314   LR 0.000996
0.75180942
0.75164670
0.75187403
0.75192779
0.75219983
0.75214946
0.75216764
0.75199234
0.75181895
0.75166601
0.75139862
0.75106186
0.75080502
0.75067824
0.75045967
0.75033289
0.75006127
0.74971348
0.74909794
0.74845654
INFO - Training [21][   80/  196]   Loss 0.529625   Top1 82.792969   Top5 98.222656   BatchTime 0.391760   LR 0.000976
0.74755925
0.74646592
0.74516243
0.74328083
0.74117750
0.74034685
0.73966748
0.73957157
0.73992300
0.73959023
0.73868901
0.73804498
0.73836714
0.73934549
0.74142033
0.74315387
0.74442136
0.74542081
0.74613011
0.74645638
0.74633634
INFO - Training [21][  100/  196]   Loss 0.526107   Top1 82.890625   Top5 98.230469   BatchTime 0.389449   LR 0.000957
0.74620146
0.74545074
0.74482369
0.74443913
0.74397242
0.74371845
0.74355400
0.74334311
0.74367040
0.74412328
0.74377811
0.74372154
0.74356991
0.74341339
0.74358433
0.74386102
0.74409360
0.74419326
0.74426711
0.74431205
0.74467021
0.74482548
INFO - Training [21][  120/  196]   Loss 0.518795   Top1 83.121745   Top5 98.330078   BatchTime 0.384902   LR 0.000937
0.74523175
0.74570453
0.74597776
0.74658191
0.74753290
0.74793905
0.74843138
0.74894512
0.74917406
0.74955755
0.74998015
0.75014091
0.75019258
0.75041062
0.75065541
0.75070816
0.75070775
0.75063998
0.75073850
0.75062823
INFO - Training [21][  140/  196]   Loss 0.519354   Top1 83.211496   Top5 98.359375   BatchTime 0.387077   LR 0.000918
0.75035059
0.75000048
0.74974275
0.74959284
0.74931508
0.74896705
0.74863064
0.74813652
0.74752825
0.74697471
0.74644142
0.74597734
0.74565172
0.74529475
0.74488664
0.74450159
0.74393219
0.74341446
0.74272692
0.74199742
0.74118954
INFO - Training [21][  160/  196]   Loss 0.521168   Top1 83.112793   Top5 98.371582   BatchTime 0.387002   LR 0.000899
0.74024296
0.73942220
0.73869759
0.73789519
0.73707986
0.73634833
0.73607057
0.73592997
0.73575830
0.73542076
0.73482805
0.73416823
0.73344761
0.73294199
0.73235077
0.73144299
0.73022199
0.72972947
0.72934592
0.72894734
INFO - Training [21][  180/  196]   Loss 0.520292   Top1 83.187934   Top5 98.328993   BatchTime 0.387031   LR 0.000879
0.72858953
0.72862244
0.72881901
0.72873741
0.72855979
0.72856838
0.72846067
0.72839135
0.72848064
0.72830927
0.72813380
0.72790939
0.72748399
0.72686970
********************pre-trained*****************
INFO - ==> Top1: 83.310    Top5: 98.346    Loss: 0.518
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [21][   20/   40]   Loss 0.450629   Top1 85.058594   Top5 99.199219   BatchTime 0.126924
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.4258)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0781)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0906)
features.3.conv.0 tensor(0.0428)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.2309)
features.4.conv.0 tensor(0.0607)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.1126)
features.5.conv.0 tensor(0.2673)
features.5.conv.3 tensor(0.0955)
features.5.conv.6 tensor(0.1732)
features.6.conv.0 tensor(0.0296)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0694)
features.7.conv.0 tensor(0.0811)
features.7.conv.3 tensor(0.1123)
features.7.conv.6 tensor(0.1926)
features.8.conv.0 tensor(0.1051)
features.8.conv.3 tensor(0.1073)
features.8.conv.6 tensor(0.2050)
features.9.conv.0 tensor(0.1246)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.3330)
features.10.conv.0 tensor(0.0549)
features.10.conv.3 tensor(0.0877)
features.10.conv.6 tensor(0.0617)
features.11.conv.0 tensor(0.5782)
features.11.conv.3 tensor(0.1385)
features.11.conv.6 tensor(0.6521)
features.12.conv.0 tensor(0.6398)
features.12.conv.3 tensor(0.1119)
features.12.conv.6 tensor(0.7821)
features.13.conv.0 tensor(0.1356)
features.13.conv.3 tensor(0.1528)
features.13.conv.6 tensor(0.4285)
features.14.conv.0 tensor(0.9732)
features.14.conv.3 tensor(0.0961)
features.14.conv.6 tensor(0.9706)
features.15.conv.0 tensor(0.9574)
features.15.conv.3 tensor(0.0684)
features.15.conv.6 tensor(0.3936)
features.16.conv.0 tensor(0.2000)
features.16.conv.3 tensor(0.1036)
features.16.conv.6 tensor(0.6507)
conv.0 tensor(0.3018)
tensor(1096944.) 2188896.0
INFO - Validation [21][   40/   40]   Loss 0.442036   Top1 85.270000   Top5 99.250000   BatchTime 0.089398
INFO - ==> Top1: 85.270    Top5: 99.250    Loss: 0.442
INFO - ==> Sparsity : 0.501
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 85.270   Top5: 99.250]
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 85.250   Top5: 99.160]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.72561812
0.72440362
0.72252578
0.72128803
0.72034103
0.71894974
0.71820021
0.71770513
0.71780705
0.71750337
0.71843541
0.71956915
0.72057915
0.72173828
0.72330636
0.72478169
0.72614276
0.72755587
0.72905308
0.73062217
0.73226291
INFO - Training [22][   20/  196]   Loss 0.542702   Top1 82.558594   Top5 97.539062   BatchTime 0.421371   LR 0.000846
0.73356503
0.73464346
0.73560804
0.73676026
0.73814583
0.74092269
0.74163431
0.74167323
0.74164796
0.74174869
0.74212420
0.74257815
0.74288923
0.74307352
0.74316883
0.74330926
0.74340588
0.74339092
INFO - Training [22][   40/  196]   Loss 0.521406   Top1 83.046875   Top5 98.076172   BatchTime 0.376147   LR 0.000827
0.74334103
0.74333417
0.74319410
0.74305218
0.74285251
0.74273175
0.74259043
0.74255764
0.74246019
0.74236208
0.74241096
0.74246776
0.74257898
0.74266571
0.74281532
0.74296117
0.74305534
0.74319112
0.74314409
0.74302167
0.74291790
0.74276447
INFO - Training [22][   60/  196]   Loss 0.520497   Top1 83.177083   Top5 98.177083   BatchTime 0.370266   LR 0.000808
0.74257952
0.74236846
0.74220425
0.74204075
0.74182045
0.74170154
0.74157977
0.74151844
0.74153173
0.74161255
0.74165756
0.74167854
0.74166971
0.74151558
0.74137622
0.74129212
0.74109334
INFO - Training [22][   80/  196]   Loss 0.515355   Top1 83.256836   Top5 98.330078   BatchTime 0.365535   LR 0.000789
0.74083889
0.74052292
0.74020445
0.73999965
0.73974597
0.73958033
0.73945558
0.73937744
0.73923355
0.73902565
0.73878974
0.73861969
0.73841077
0.73815906
0.73799068
0.73785752
0.73769838
0.73763174
0.73763680
0.73746783
0.73709542
0.73677498
0.73653871
INFO - Training [22][  100/  196]   Loss 0.511234   Top1 83.449219   Top5 98.339844   BatchTime 0.362321   LR 0.000770
0.73632157
0.73610455
0.73586839
0.73552525
0.73524815
0.73467875
0.73409027
0.73373467
0.73336536
0.73303884
0.73269194
0.73237950
0.73184621
0.73117948
0.73064387
0.72983921
0.72898841
0.72809047
0.72718608
0.72642279
0.72585642
INFO - Training [22][  120/  196]   Loss 0.504616   Top1 83.688151   Top5 98.378906   BatchTime 0.365247   LR 0.000752
0.72556752
0.72552836
0.72524130
0.72514743
0.72468311
0.72448725
0.72413468
0.72374904
0.72373503
0.72365701
0.72387630
0.72411841
0.72439563
0.72449350
0.72471696
0.72512090
INFO - Training [22][  140/  196]   Loss 0.503010   Top1 83.724888   Top5 98.423549   BatchTime 0.368077   LR 0.000734
0.72572041
0.72591394
0.72634697
0.72703439
0.72809041
0.72894394
0.73006362
0.73083168
0.73165369
0.73225832
0.73285478
0.73339784
0.73390543
0.73415029
0.73422271
0.73421133
0.73430401
0.73441327
0.73450601
0.73474896
0.73498380
0.73519933
INFO - Training [22][  160/  196]   Loss 0.504838   Top1 83.715820   Top5 98.410645   BatchTime 0.367460   LR 0.000715
0.73541969
0.73561376
0.73572755
0.73579788
0.73591053
0.73610932
0.73618084
0.73626405
0.73617601
0.73606414
0.73588371
0.73570430
0.73550290
0.73521882
0.73486787
0.73453474
0.73418999
0.73392022
0.73371774
0.73364985
0.73350561
0.73320705
INFO - Training [22][  180/  196]   Loss 0.503466   Top1 83.708767   Top5 98.404948   BatchTime 0.366716   LR 0.000697
0.73286897
0.73241168
0.73217458
0.73210549
0.73197728
0.73191857
0.73186404
0.73187643
0.73185360
0.73167747
0.73138899
0.73120075
0.73107624
0.73084098
********************pre-trained*****************
INFO - ==> Top1: 83.764    Top5: 98.404    Loss: 0.503
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [22][   20/   40]   Loss 0.729263   Top1 76.308594   Top5 97.753906   BatchTime 0.126507
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.4316)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0932)
features.3.conv.0 tensor(0.0443)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.2318)
features.4.conv.0 tensor(0.0653)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.1079)
features.5.conv.0 tensor(0.2637)
features.5.conv.3 tensor(0.0926)
features.5.conv.6 tensor(0.1683)
features.6.conv.0 tensor(0.0314)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0676)
features.7.conv.0 tensor(0.0897)
features.7.conv.3 tensor(0.1108)
features.7.conv.6 tensor(0.1945)
features.8.conv.0 tensor(0.1085)
features.8.conv.3 tensor(0.1076)
features.8.conv.6 tensor(0.2112)
features.9.conv.0 tensor(0.1278)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.3424)
features.10.conv.0 tensor(0.0507)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.0601)
features.11.conv.0 tensor(0.5898)
features.11.conv.3 tensor(0.1373)
features.11.conv.6 tensor(0.6405)
features.12.conv.0 tensor(0.6962)
features.12.conv.3 tensor(0.1115)
features.12.conv.6 tensor(0.7936)
features.13.conv.0 tensor(0.1361)
features.13.conv.3 tensor(0.1483)
features.13.conv.6 tensor(0.4057)
features.14.conv.0 tensor(0.9746)
features.14.conv.3 tensor(0.0962)
features.14.conv.6 tensor(0.9701)
features.15.conv.0 tensor(0.9587)
features.15.conv.3 tensor(0.0690)
features.15.conv.6 tensor(0.3949)
features.16.conv.0 tensor(0.2163)
features.16.conv.3 tensor(0.1053)
features.16.conv.6 tensor(0.4230)
conv.0 tensor(0.4123)
tensor(1077580.) 2188896.0
INFO - Validation [22][   40/   40]   Loss 0.733109   Top1 76.020000   Top5 97.810000   BatchTime 0.087582
INFO - ==> Top1: 76.020    Top5: 97.810    Loss: 0.733
INFO - ==> Sparsity : 0.492
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 85.270   Top5: 99.250]
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 85.250   Top5: 99.160]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
0.73067552
0.73049039
0.73029381
0.73010576
0.72997802
0.72977501
0.72967196
0.72967881
0.72967553
0.72983843
0.72988486
0.72975910
0.72966754
0.72961056
0.72929460
0.72903967
0.72867572
0.72829002
0.72789925
0.72747779
INFO - Training [23][   20/  196]   Loss 0.510079   Top1 82.949219   Top5 98.125000   BatchTime 0.408259   LR 0.000666
0.72703439
0.72674650
0.72635144
0.72577333
0.72541147
0.72506118
0.72466135
0.72430545
0.72389787
0.72348189
0.72308689
0.72298843
0.72296906
0.72321004
0.72335941
0.72347081
0.72339797
0.72336131
0.72305918
0.72272778
0.72212023
0.72151852
0.72081035
INFO - Training [23][   40/  196]   Loss 0.511038   Top1 83.525391   Top5 98.164062   BatchTime 0.376995   LR 0.000648
0.71993703
0.71905047
0.71831119
0.71703130
0.71561265
0.71433669
0.71327740
0.71254832
0.71169776
0.71087420
0.71029931
0.70984817
0.70948613
0.70938152
0.70948172
0.70945626
0.70933914
INFO - Training [23][   60/  196]   Loss 0.510385   Top1 83.541667   Top5 98.235677   BatchTime 0.367675   LR 0.000630
0.70884961
0.70859134
0.70809567
0.70774645
0.70762932
0.70750982
0.70743424
0.70733768
0.70802373
0.70868415
0.70942444
0.71014845
0.71082282
0.71121001
0.71163332
0.71237600
0.71297592
0.71352077
0.71385741
0.71411157
0.71435058
0.71466762
INFO - Training [23][   80/  196]   Loss 0.507965   Top1 83.676758   Top5 98.334961   BatchTime 0.369372   LR 0.000613
0.71506143
0.71527135
0.71543008
0.71549976
0.71547371
0.71549666
0.71548390
0.71549976
0.71545613
0.71540225
0.71525776
0.71508843
0.71491247
0.71476364
0.71482658
0.71464759
0.71452624
0.71447843
0.71446109
0.71429014
INFO - Training [23][  100/  196]   Loss 0.498413   Top1 83.906250   Top5 98.402344   BatchTime 0.375021   LR 0.000596
0.71417964
0.71406835
0.71391982
0.71388012
0.71396911
0.71391195
0.71394473
0.71400034
0.71417856
0.71431690
0.71436340
0.71450257
0.71470577
0.71493435
0.71508330
0.71522486
INFO - Training [23][  120/  196]   Loss 0.496561   Top1 84.016927   Top5 98.457031   BatchTime 0.375153   LR 0.000579
0.71534401
0.71546274
0.71548676
0.71548128
0.71546870
0.71556145
0.71556920
0.71530873
0.71492141
0.71456444
0.71423376
0.71390200
0.71354336
0.71326518
0.71294403
0.71269166
0.71236658
0.71222484
0.71218145
0.71222675
0.71215516
0.71212059
INFO - Training [23][  140/  196]   Loss 0.492666   Top1 84.174107   Top5 98.535156   BatchTime 0.373831   LR 0.000562
0.71221185
0.71228111
0.71236867
0.71234238
0.71221560
0.71213359
0.71179181
0.71139872
0.71103632
0.71084458
0.71057183
0.71026546
0.71000725
0.71000415
0.70997262
0.71007514
0.71008795
0.71005756
0.70998943
0.70986873
0.70995104
INFO - Training [23][  160/  196]   Loss 0.493902   Top1 84.094238   Top5 98.496094   BatchTime 0.374300   LR 0.000545
0.70989609
0.70993972
0.71005529
0.71006268
0.71000415
0.70986050
0.70983315
0.70976883
0.70985079
0.71003079
0.71027452
0.71074420
0.71115291
0.71134275
0.71149123
0.71162391
0.71193171
0.71204871
0.71212804
0.71207637
0.71215385
0.71212184
INFO - Training [23][  180/  196]   Loss 0.492039   Top1 84.149306   Top5 98.470052   BatchTime 0.373107   LR 0.000529
0.71209908
0.71197510
0.71198517
0.71205384
0.71223640
0.71223605
0.71204537
0.71200228
0.71206659
0.71206754
0.71198219
INFO - ==> Top1: 84.208    Top5: 98.478    Loss: 0.490
0.71183443
0.71168315
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.406179   Top1 86.367188   Top5 99.511719   BatchTime 0.127690
INFO - Validation [23][   40/   40]   Loss 0.390165   Top1 86.860000   Top5 99.590000   BatchTime 0.090388
INFO - ==> Top1: 86.860    Top5: 99.590    Loss: 0.390
INFO - ==> Sparsity : 0.550
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 85.270   Top5: 99.250]
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.4277)
features.1.conv.0 tensor(0.0371)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0495)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0949)
features.3.conv.0 tensor(0.0394)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.2363)
features.4.conv.0 tensor(0.0654)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.1107)
features.5.conv.0 tensor(0.2528)
features.5.conv.3 tensor(0.0978)
features.5.conv.6 tensor(0.1470)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0673)
features.7.conv.0 tensor(0.0915)
features.7.conv.3 tensor(0.1100)
features.7.conv.6 tensor(0.2275)
features.8.conv.0 tensor(0.1164)
features.8.conv.3 tensor(0.1059)
features.8.conv.6 tensor(0.2163)
features.9.conv.0 tensor(0.1327)
features.9.conv.3 tensor(0.1670)
features.9.conv.6 tensor(0.3662)
features.10.conv.0 tensor(0.0465)
features.10.conv.3 tensor(0.0897)
features.10.conv.6 tensor(0.0624)
features.11.conv.0 tensor(0.6075)
features.11.conv.3 tensor(0.1399)
features.11.conv.6 tensor(0.6730)
features.12.conv.0 tensor(0.7128)
features.12.conv.3 tensor(0.1117)
features.12.conv.6 tensor(0.7973)
features.13.conv.0 tensor(0.1409)
features.13.conv.3 tensor(0.1460)
features.13.conv.6 tensor(0.3473)
features.14.conv.0 tensor(0.9749)
features.14.conv.3 tensor(0.0947)
features.14.conv.6 tensor(0.9723)
features.15.conv.0 tensor(0.9598)
features.15.conv.3 tensor(0.0677)
features.15.conv.6 tensor(0.4011)
features.16.conv.0 tensor(0.2107)
features.16.conv.3 tensor(0.1060)
features.16.conv.6 tensor(0.5193)
conv.0 tensor(0.6459)
tensor(1203947.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.71151084
0.71122372
0.71101242
0.71086794
0.71065879
0.71048737
0.71030575
0.71006811
0.70977634
0.70946604
0.70916402
0.70888287
0.70867205
0.70849735
0.70831436
0.70819318
0.70804542
0.70797831
0.70791751
0.70774436
0.70744550
0.70737010
INFO - Training [24][   20/  196]   Loss 0.504607   Top1 83.125000   Top5 98.027344   BatchTime 0.473466   LR 0.000500
0.70742273
0.70754802
0.70766383
0.70789081
0.70794404
0.70793229
0.70788556
0.70783556
0.70763910
0.70769083
0.70771092
0.70782727
0.70784044
0.70777535
0.70745236
0.70726889
0.70684952
INFO - Training [24][   40/  196]   Loss 0.496771   Top1 83.652344   Top5 98.144531   BatchTime 0.418396   LR 0.000484
0.70646828
0.70596617
0.70565182
0.70514911
0.70450675
0.70393479
0.70389843
0.70413947
0.70455921
0.70484728
0.70522231
0.70552802
0.70570821
0.70582652
0.70594341
0.70611036
0.70632893
0.70643884
0.70639908
0.70632511
0.70642000
INFO - Training [24][   60/  196]   Loss 0.494462   Top1 83.743490   Top5 98.196615   BatchTime 0.405725   LR 0.000468
0.70616633
0.70595574
0.70561463
0.70522714
0.70490032
0.70453656
0.70413554
0.70369899
0.70337588
0.70299470
0.70261812
0.70207775
0.70183194
0.70152009
0.70105296
0.70049524
0.70066208
0.70089048
0.70083559
0.70099717
0.70107383
0.70140380
INFO - Training [24][   80/  196]   Loss 0.495401   Top1 83.857422   Top5 98.339844   BatchTime 0.395423   LR 0.000453
0.70161402
0.70196056
0.70234823
0.70249808
0.70271593
0.70298070
0.70331877
0.70338595
0.70342201
0.70373482
0.70377278
0.70375723
0.70370328
0.70358670
0.70356077
0.70359230
INFO - Training [24][  100/  196]   Loss 0.486052   Top1 84.218750   Top5 98.429688   BatchTime 0.389797   LR 0.000437
0.70368052
0.70377308
0.70384800
0.70385963
0.70394695
0.70391041
0.70389533
0.70394480
0.70394158
0.70395815
0.70403898
0.70421928
0.70436692
0.70440644
0.70456654
0.70479578
0.70492148
0.70505178
0.70522231
0.70550680
0.70563036
0.70584619
INFO - Training [24][  120/  196]   Loss 0.479385   Top1 84.485677   Top5 98.548177   BatchTime 0.384927   LR 0.000422
0.70604867
0.70610380
0.70614189
0.70599020
0.70579708
0.70566016
0.70559996
0.70553768
0.70547432
0.70540744
0.70539355
0.70531452
0.70529801
0.70526224
0.70513099
0.70510054
0.70510346
0.70498294
0.70504695
0.70507294
INFO - Training [24][  140/  196]   Loss 0.476868   Top1 84.642857   Top5 98.630022   BatchTime 0.387339   LR 0.000407
0.70497739
0.70482051
0.70456845
0.70418537
0.70381558
0.70354944
0.70343387
0.70460528
0.70532715
0.70558888
0.70577037
0.70579594
0.70579046
0.70563394
0.70540172
0.70491898
0.70449984
0.70427233
0.70387840
0.70363510
0.70324486
0.70274925
INFO - Training [24][  160/  196]   Loss 0.478051   Top1 84.614258   Top5 98.627930   BatchTime 0.385112   LR 0.000392
0.70281994
0.70329005
0.70381671
0.70410842
0.70428669
0.70460927
0.70486641
0.70501482
0.70530015
0.70542604
0.70550156
0.70567876
0.70591539
0.70604253
0.70608658
0.70610946
INFO - Training [24][  180/  196]   Loss 0.479265   Top1 84.526910   Top5 98.576389   BatchTime 0.383633   LR 0.000378
0.70617664
0.70624560
0.70635366
0.70637852
0.70635569
0.70627439
0.70622939
0.70614791
0.70614648
0.70623368
0.70633966
0.70644623
0.70652407
0.70660144
0.70664650
0.70677298
0.70680714
0.70667368
INFO - ==> Top1: 84.488    Top5: 98.562    Loss: 0.481
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 0.453815   Top1 85.781250   Top5 99.160156   BatchTime 0.145579
INFO - Validation [24][   40/   40]   Loss 0.440520   Top1 85.660000   Top5 99.250000   BatchTime 0.103021
INFO - ==> Top1: 85.660    Top5: 99.250    Loss: 0.441
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 85.660   Top5: 99.250]
features.0.conv.0 tensor(0.5312)
features.0.conv.3 tensor(0.4355)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0807)
features.2.conv.0 tensor(0.0498)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0964)
features.3.conv.0 tensor(0.0399)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.2346)
features.4.conv.0 tensor(0.0636)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.1068)
features.5.conv.0 tensor(0.2599)
features.5.conv.3 tensor(0.0972)
features.5.conv.6 tensor(0.1562)
features.6.conv.0 tensor(0.0275)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0684)
features.7.conv.0 tensor(0.0907)
features.7.conv.3 tensor(0.1120)
features.7.conv.6 tensor(0.2286)
features.8.conv.0 tensor(0.1155)
features.8.conv.3 tensor(0.1045)
features.8.conv.6 tensor(0.2384)
features.9.conv.0 tensor(0.1421)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.3726)
features.10.conv.0 tensor(0.0486)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.0632)
features.11.conv.0 tensor(0.5936)
features.11.conv.3 tensor(0.1381)
features.11.conv.6 tensor(0.6572)
features.12.conv.0 tensor(0.6782)
features.12.conv.3 tensor(0.1130)
features.12.conv.6 tensor(0.7940)
features.13.conv.0 tensor(0.1448)
features.13.conv.3 tensor(0.1470)
features.13.conv.6 tensor(0.3944)
features.14.conv.0 tensor(0.9755)
features.14.conv.3 tensor(0.0927)
features.14.conv.6 tensor(0.9719)
features.15.conv.0 tensor(0.9603)
features.15.conv.3 tensor(0.0662)
features.15.conv.6 tensor(0.4001)
features.16.conv.0 tensor(0.2477)
features.16.conv.3 tensor(0.1038)
features.16.conv.6 tensor(0.6328)
conv.0 tensor(0.6403)
tensor(1243984.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.70630217
0.70606369
0.70573956
0.70547384
0.70514137
0.70496213
0.70467031
0.70449513
0.70429659
0.70431530
0.70426977
0.70412809
0.70377779
0.70354462
0.70324683
0.70313793
0.70304447
0.70301712
0.70283037
0.70257509
0.70222586
0.70183778
INFO - Training [25][   20/  196]   Loss 0.488460   Top1 83.886719   Top5 97.929688   BatchTime 0.416299   LR 0.000353
0.70145178
0.70105147
0.70091617
0.70105958
0.70121026
0.70123839
0.70129710
0.70151329
0.70175308
0.70183527
0.70215279
0.70232886
0.70253742
0.70272112
0.70280772
0.70303655
INFO - Training [25][   40/  196]   Loss 0.488458   Top1 84.169922   Top5 98.222656   BatchTime 0.392595   LR 0.000339
0.70321149
0.70314842
0.70303893
0.70296496
0.70307970
0.70325953
0.70356733
0.70384550
0.70399666
0.70413351
0.70426011
0.70437735
0.70441121
0.70438099
0.70444614
0.70451123
0.70457697
0.70460522
0.70463890
0.70457703
0.70445395
0.70427269
INFO - Training [25][   60/  196]   Loss 0.485577   Top1 84.153646   Top5 98.378906   BatchTime 0.383203   LR 0.000325
0.70419258
0.70412701
0.70394593
0.70383406
0.70377225
0.70363438
0.70344502
0.70334107
0.70328724
0.70319760
0.70304388
0.70287788
0.70294058
0.70305026
0.70326561
0.70336622
0.70342964
0.70349193
0.70363027
0.70373166
0.70391572
0.70406550
INFO - Training [25][   80/  196]   Loss 0.479177   Top1 84.375000   Top5 98.505859   BatchTime 0.378917   LR 0.000312
0.70422125
0.70435482
0.70453846
0.70449740
0.70446587
0.70438546
0.70425016
0.70414579
0.70404446
0.70388603
0.70372945
0.70359468
0.70341694
0.70327365
0.70326477
0.70325041
INFO - Training [25][  100/  196]   Loss 0.473713   Top1 84.515625   Top5 98.570312   BatchTime 0.376569   LR 0.000299
0.70320755
0.70310742
0.70305055
0.70296723
0.70284569
0.70266443
0.70249850
0.70238215
0.70231789
0.70227998
0.70215017
0.70205015
0.70196015
0.70176178
0.70166063
0.70153600
0.70151913
0.70152360
0.70134962
0.70139211
0.70126849
0.70122856
0.70104194
INFO - Training [25][  120/  196]   Loss 0.468267   Top1 84.700521   Top5 98.639323   BatchTime 0.373116   LR 0.000286
0.70097136
0.70096064
0.70106262
0.70094490
0.70099294
0.70100224
0.70103413
0.70104814
0.70111215
0.70112354
0.70105261
0.70087129
0.70073366
0.70064408
0.70053059
0.70037699
0.70021009
0.69996029
0.69974083
0.69949871
0.69931126
0.69914895
INFO - Training [25][  140/  196]   Loss 0.466264   Top1 84.773996   Top5 98.705357   BatchTime 0.372200   LR 0.000273
0.69887388
0.69842696
0.69815332
0.69797903
0.69791955
0.69778329
0.69770533
0.69771695
0.69766325
0.69763583
0.69762915
0.69756937
0.69746697
0.69745231
0.69747788
0.69749153
INFO - Training [25][  160/  196]   Loss 0.471309   Top1 84.614258   Top5 98.659668   BatchTime 0.371728   LR 0.000261
0.69753605
0.69759476
0.69762081
0.69757092
0.69754541
0.69758332
0.69760525
0.69771767
0.69782013
0.69792336
0.69802111
0.69805473
0.69807011
0.69810259
0.69808835
0.69806582
0.69803709
0.69799232
0.69800085
0.69808602
0.69816595
0.69834405
INFO - Training [25][  180/  196]   Loss 0.469250   Top1 84.704861   Top5 98.650174   BatchTime 0.370371   LR 0.000248
0.69851214
0.69866967
0.69882673
0.69894493
0.69896448
0.69893223
0.69888800
0.69879997
0.69878060
0.69880706
0.69883877
0.69887745
0.69882554
0.69873607
0.69866568
********************pre-trained*****************
INFO - ==> Top1: 84.766    Top5: 98.634    Loss: 0.469
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [25][   20/   40]   Loss 0.555431   Top1 82.324219   Top5 98.750000   BatchTime 0.134524
features.0.conv.0 tensor(0.5243)
features.0.conv.3 tensor(0.4375)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0938)
features.3.conv.0 tensor(0.0402)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2387)
features.4.conv.0 tensor(0.0643)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.1149)
features.5.conv.0 tensor(0.2617)
features.5.conv.3 tensor(0.0961)
features.5.conv.6 tensor(0.1722)
features.6.conv.0 tensor(0.0278)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0675)
features.7.conv.0 tensor(0.0978)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.2266)
features.8.conv.0 tensor(0.1225)
features.8.conv.3 tensor(0.1050)
features.8.conv.6 tensor(0.2399)
features.9.conv.0 tensor(0.1480)
features.9.conv.3 tensor(0.1681)
features.9.conv.6 tensor(0.3742)
features.10.conv.0 tensor(0.0516)
features.10.conv.3 tensor(0.0903)
features.10.conv.6 tensor(0.0711)
features.11.conv.0 tensor(0.6280)
features.11.conv.3 tensor(0.1387)
features.11.conv.6 tensor(0.6749)
features.12.conv.0 tensor(0.7032)
features.12.conv.3 tensor(0.1113)
features.12.conv.6 tensor(0.8002)
features.13.conv.0 tensor(0.1570)
features.13.conv.3 tensor(0.1474)
features.13.conv.6 tensor(0.4563)
features.14.conv.0 tensor(0.9761)
features.14.conv.3 tensor(0.0913)
features.14.conv.6 tensor(0.9749)
features.15.conv.0 tensor(0.9614)
features.15.conv.3 tensor(0.0659)
features.15.conv.6 tensor(0.3939)
features.16.conv.0 tensor(0.2469)
features.16.conv.3 tensor(0.1038)
features.16.conv.6 tensor(0.6373)
conv.0 tensor(0.6501)
tensor(1261057.) 2188896.0
INFO - Validation [25][   40/   40]   Loss 0.547961   Top1 81.950000   Top5 98.900000   BatchTime 0.097374
INFO - ==> Top1: 81.950    Top5: 98.900    Loss: 0.548
INFO - ==> Sparsity : 0.576
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 85.660   Top5: 99.250]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
0.69862521
0.69856322
0.69855845
0.69854289
0.69857049
0.69852650
0.69855171
0.69861358
0.69856417
0.69855869
0.69854414
0.69846308
0.69836909
0.69828272
0.69825327
0.69822031
0.69829714
0.69835418
0.69838905
0.69835889
0.69830567
0.69823861
0.69827318
INFO - Training [26][   20/  196]   Loss 0.486573   Top1 83.535156   Top5 98.105469   BatchTime 0.468312   LR 0.000228
0.69831252
0.69833767
0.69840199
0.69853818
0.69869077
0.69880664
0.69884580
0.69882751
0.69882280
0.69881880
0.69871861
0.69864231
0.69862634
0.69860393
0.69856852
0.69851607
INFO - Training [26][   40/  196]   Loss 0.487217   Top1 83.544922   Top5 98.359375   BatchTime 0.420781   LR 0.000216
0.69844818
0.69839311
0.69839162
0.69841033
0.69834429
0.69828999
0.69824243
0.69808441
0.69797170
0.69786006
0.69779593
0.69770205
0.69763118
0.69763005
0.69769567
0.69773608
0.69779974
0.69780028
0.69779932
0.69780564
0.69780797
INFO - Training [26][   60/  196]   Loss 0.486966   Top1 83.561198   Top5 98.398438   BatchTime 0.403867   LR 0.000205
0.69785839
0.69789416
0.69783455
0.69783354
0.69779027
0.69782072
0.69781893
0.69779676
0.69773620
0.69768459
0.69760692
0.69750363
0.69741732
0.69737685
0.69729984
0.69726545
0.69724536
0.69722164
0.69718397
0.69716531
INFO - Training [26][   80/  196]   Loss 0.480594   Top1 83.876953   Top5 98.525391   BatchTime 0.405834   LR 0.000194
0.69711703
0.69708127
0.69695210
0.69682008
0.69667536
0.69655156
0.69653589
0.69653064
0.69645172
0.69638699
0.69633228
0.69623387
0.69617754
0.69613284
0.69613403
0.69601619
0.69588101
0.69576252
0.69568402
INFO - Training [26][  100/  196]   Loss 0.475226   Top1 84.125000   Top5 98.562500   BatchTime 0.406212   LR 0.000183
0.69572222
0.69567728
0.69571173
0.69577253
0.69572991
0.69571757
0.69570655
0.69571632
0.69579506
0.69579506
0.69580245
0.69580930
0.69578284
0.69580412
0.69583774
0.69581378
0.69576353
0.69566917
0.69549900
0.69540662
0.69530892
0.69530153
0.69526821
INFO - Training [26][  120/  196]   Loss 0.470667   Top1 84.466146   Top5 98.632812   BatchTime 0.396705   LR 0.000173
0.69525665
0.69515777
0.69501644
0.69493622
0.69482118
0.69474691
0.69461852
0.69449735
0.69447303
0.69441974
0.69436973
0.69437408
0.69435614
0.69439012
0.69439518
0.69435042
0.69430584
INFO - Training [26][  140/  196]   Loss 0.469156   Top1 84.430804   Top5 98.699777   BatchTime 0.391420   LR 0.000163
0.69432741
0.69430733
0.69434178
0.69430679
0.69424975
0.69417495
0.69401902
0.69389200
0.69384420
0.69385272
0.69386250
0.69386333
0.69389689
0.69389307
0.69388670
0.69385231
0.69382870
0.69382924
0.69382679
0.69380623
0.69374388
INFO - Training [26][  160/  196]   Loss 0.470097   Top1 84.501953   Top5 98.662109   BatchTime 0.391299   LR 0.000153
0.69368446
0.69360960
0.69352025
0.69343984
0.69329911
0.69319397
0.69307983
0.69297516
0.69296229
0.69290400
0.69288725
0.69281870
0.69273245
0.69267243
0.69263190
0.69257116
0.69252974
0.69251925
0.69249696
0.69238621
0.69233346
0.69221783
INFO - Training [26][  180/  196]   Loss 0.469527   Top1 84.576823   Top5 98.617622   BatchTime 0.388189   LR 0.000144
0.69210690
0.69199777
0.69187492
0.69187176
0.69183058
0.69181162
0.69169807
0.69157302
0.69149905
0.69146323
0.69132978
0.69132197
0.69137233
0.69146335
********************pre-trained*****************
INFO - ==> Top1: 84.700    Top5: 98.622    Loss: 0.467
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.612007   Top1 80.488281   Top5 98.554688   BatchTime 0.178542
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0803)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0923)
features.3.conv.0 tensor(0.0391)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.2355)
features.4.conv.0 tensor(0.0628)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.1281)
features.5.conv.0 tensor(0.2671)
features.5.conv.3 tensor(0.0961)
features.5.conv.6 tensor(0.1733)
features.6.conv.0 tensor(0.0299)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0675)
features.7.conv.0 tensor(0.1029)
features.7.conv.3 tensor(0.1123)
features.7.conv.6 tensor(0.2468)
features.8.conv.0 tensor(0.1271)
features.8.conv.3 tensor(0.1050)
features.8.conv.6 tensor(0.2430)
features.9.conv.0 tensor(0.1496)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.3768)
features.10.conv.0 tensor(0.0527)
features.10.conv.3 tensor(0.0909)
features.10.conv.6 tensor(0.0709)
features.11.conv.0 tensor(0.6690)
features.11.conv.3 tensor(0.1373)
features.11.conv.6 tensor(0.6757)
features.12.conv.0 tensor(0.7001)
features.12.conv.3 tensor(0.1123)
features.12.conv.6 tensor(0.8001)
features.13.conv.0 tensor(0.1629)
features.13.conv.3 tensor(0.1485)
features.13.conv.6 tensor(0.4553)
features.14.conv.0 tensor(0.9769)
features.14.conv.3 tensor(0.0914)
features.14.conv.6 tensor(0.9756)
features.15.conv.0 tensor(0.9619)
features.15.conv.3 tensor(0.0657)
features.15.conv.6 tensor(0.3890)
features.16.conv.0 tensor(0.2672)
features.16.conv.3 tensor(0.1035)
features.16.conv.6 tensor(0.7245)
conv.0 tensor(0.6770)
tensor(1304933.) 2188896.0
INFO - Validation [26][   40/   40]   Loss 0.604029   Top1 80.470000   Top5 98.690000   BatchTime 0.145300
INFO - ==> Top1: 80.470    Top5: 98.690    Loss: 0.604
INFO - ==> Sparsity : 0.596
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 85.660   Top5: 99.250]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
0.69148409
0.69161624
0.69172984
0.69186455
0.69200099
0.69212693
0.69233757
0.69258130
0.69277227
0.69288594
0.69298416
0.69304514
0.69311011
0.69318545
0.69326621
0.69330585
0.69330567
0.69320393
0.69314104
INFO - Training [27][   20/  196]   Loss 0.478161   Top1 84.179688   Top5 98.125000   BatchTime 0.460919   LR 0.000128
0.69308126
0.69307494
0.69301087
0.69293791
0.69284767
0.69281816
0.69280380
0.69283152
0.69282126
0.69286269
0.69292372
0.69295883
0.69302994
0.69309235
0.69322813
0.69333494
0.69342571
0.69346279
0.69348335
0.69348580
0.69351560
0.69353169
INFO - Training [27][   40/  196]   Loss 0.490316   Top1 83.837891   Top5 98.222656   BatchTime 0.410709   LR 0.000119
0.69354355
0.69348800
0.69348633
0.69350010
0.69346958
0.69345361
0.69345790
0.69345510
0.69342893
0.69340247
0.69336981
0.69335610
0.69333369
0.69330472
0.69327682
0.69323242
0.69319171
0.69314390
0.69313478
0.69313115
0.69310951
0.69306928
INFO - Training [27][   60/  196]   Loss 0.473794   Top1 84.277344   Top5 98.346354   BatchTime 0.393290   LR 0.000111
0.69303852
0.69296801
0.69286841
0.69280702
0.69272488
0.69264585
0.69253159
0.69246292
0.69239831
0.69231021
0.69229907
0.69226164
0.69226921
0.69222498
0.69218409
0.69213992
0.69210166
INFO - Training [27][   80/  196]   Loss 0.469394   Top1 84.560547   Top5 98.471680   BatchTime 0.384031   LR 0.000102
0.69206011
0.69205403
0.69205081
0.69199991
0.69197971
0.69199955
0.69200379
0.69196153
0.69198912
0.69199175
0.69198787
0.69202912
0.69201207
0.69196886
0.69193196
0.69188523
0.69180930
0.69173813
0.69164187
0.69155449
0.69152504
0.69148457
0.69147134
INFO - Training [27][  100/  196]   Loss 0.466247   Top1 84.750000   Top5 98.500000   BatchTime 0.375822   LR 0.000095
0.69145304
0.69141895
0.69133842
0.69125038
0.69118410
0.69113863
0.69107974
0.69105506
0.69105506
0.69105107
0.69103199
0.69096339
0.69089752
0.69084561
0.69077504
0.69074577
0.69074148
INFO - Training [27][  120/  196]   Loss 0.459409   Top1 84.996745   Top5 98.610026   BatchTime 0.373602   LR 0.000087
0.69072241
0.69071102
0.69068068
0.69064254
0.69056875
0.69056576
0.69052845
0.69048738
0.69045532
0.69041085
0.69036669
0.69032615
0.69029593
0.69026631
0.69025099
0.69024432
0.69026732
0.69030100
0.69035316
0.69035631
0.69036484
0.69034499
0.69031626
INFO - Training [27][  140/  196]   Loss 0.457389   Top1 85.125558   Top5 98.649554   BatchTime 0.371020   LR 0.000080
0.69027275
0.69022882
0.69015539
0.69008625
0.69005823
0.69006103
0.69007629
0.69007587
0.69009793
0.69013101
0.69016904
0.69018120
0.69014609
0.69017655
0.69018036
0.69018757
0.69022328
INFO - Training [27][  160/  196]   Loss 0.459117   Top1 85.126953   Top5 98.598633   BatchTime 0.366775   LR 0.000073
0.69025707
0.69027823
0.69028211
0.69023663
0.69021088
0.69015759
0.69011647
0.69010365
0.69005722
0.69001007
0.68999797
0.68996888
0.68995184
0.68990827
0.68985814
0.68982071
0.68979770
0.68977243
0.68977994
0.68978208
0.68979955
0.68980396
0.68976480
INFO - Training [27][  180/  196]   Loss 0.457898   Top1 85.162760   Top5 98.602431   BatchTime 0.366293   LR 0.000066
0.68977618
0.68975461
0.68973595
0.68973708
0.68974161
0.68972373
0.68970245
0.68971199
0.68972927
0.68970251
0.68965417
0.68966252
INFO - ==> Top1: 85.174    Top5: 98.602    Loss: 0.457
0.68960530
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 0.430821   Top1 86.074219   Top5 99.238281   BatchTime 0.218097
INFO - Validation [27][   40/   40]   Loss 0.415671   Top1 86.480000   Top5 99.380000   BatchTime 0.149593
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0790)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0935)
features.3.conv.0 tensor(0.0394)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2357)
features.4.conv.0 tensor(0.0638)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.1343)
features.5.conv.0 tensor(0.2689)
features.5.conv.3 tensor(0.0943)
features.5.conv.6 tensor(0.1742)
features.6.conv.0 tensor(0.0283)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0674)
features.7.conv.0 tensor(0.1043)
features.7.conv.3 tensor(0.1128)
features.7.conv.6 tensor(0.2678)
features.8.conv.0 tensor(0.1285)
features.8.conv.3 tensor(0.1039)
features.8.conv.6 tensor(0.2522)
features.9.conv.0 tensor(0.1597)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.3816)
features.10.conv.0 tensor(0.0527)
features.10.conv.3 tensor(0.0917)
features.10.conv.6 tensor(0.0867)
features.11.conv.0 tensor(0.6614)
features.11.conv.3 tensor(0.1379)
features.11.conv.6 tensor(0.6823)
features.12.conv.0 tensor(0.7025)
features.12.conv.3 tensor(0.1127)
features.12.conv.6 tensor(0.8037)
features.13.conv.0 tensor(0.1681)
features.13.conv.3 tensor(0.1493)
features.13.conv.6 tensor(0.4725)
features.14.conv.0 tensor(0.9772)
features.14.conv.3 tensor(0.0916)
features.14.conv.6 tensor(0.9761)
features.15.conv.0 tensor(0.9623)
features.15.conv.3 tensor(0.0660)
features.15.conv.6 tensor(0.3848)
features.16.conv.0 tensor(0.2927)
features.16.conv.3 tensor(0.1049)
features.16.conv.6 tensor(0.6891)
conv.0 tensor(0.6809)
tensor(1303083.) 2188896.0
INFO - ==> Top1: 86.480    Top5: 99.380    Loss: 0.416
INFO - ==> Sparsity : 0.595
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 86.480   Top5: 99.380]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 86.470   Top5: 99.420]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
0.68956870
0.68952495
0.68948716
0.68945920
0.68944627
0.68938142
0.68935943
0.68932521
0.68929511
0.68923646
0.68918049
0.68916243
0.68915516
0.68916529
0.68915093
0.68915474
0.68915898
0.68919349
0.68914062
0.68912965
0.68913382
0.68913573
0.68907708
INFO - Training [28][   20/  196]   Loss 0.465974   Top1 84.804688   Top5 98.320312   BatchTime 0.492282   LR 0.000055
0.68904549
0.68903589
0.68899894
0.68897951
0.68894988
0.68893784
0.68885785
0.68879688
0.68873930
0.68870628
0.68863273
0.68855691
0.68850183
0.68844843
0.68838084
0.68832296
0.68828630
0.68824553
0.68823713
0.68821055
INFO - Training [28][   40/  196]   Loss 0.471322   Top1 84.687500   Top5 98.515625   BatchTime 0.447462   LR 0.000050
0.68818235
0.68817657
0.68818325
0.68816262
0.68815500
0.68812168
0.68813318
0.68812776
0.68814832
0.68814254
0.68815511
0.68820280
0.68820000
0.68823594
0.68821436
0.68821460
0.68823153
0.68822277
0.68821561
0.68822229
0.68820691
INFO - Training [28][   60/  196]   Loss 0.475010   Top1 84.674479   Top5 98.535156   BatchTime 0.430450   LR 0.000044
0.68818015
0.68818587
0.68816251
0.68816781
0.68815345
0.68815076
0.68814355
0.68813223
0.68814474
0.68816185
0.68814623
0.68816030
0.68819100
0.68820405
0.68820739
INFO - Training [28][   80/  196]   Loss 0.473061   Top1 84.750977   Top5 98.681641   BatchTime 0.417038   LR 0.000039
0.68819696
0.68820900
0.68820244
0.68822718
0.68819594
0.68819785
0.68816614
0.68813181
0.68812418
0.68811202
0.68810654
0.68810016
0.68808657
0.68806106
0.68803006
0.68800217
0.68795878
0.68791336
0.68790680
0.68788451
0.68787724
INFO - Training [28][  100/  196]   Loss 0.465094   Top1 84.984375   Top5 98.675781   BatchTime 0.412007   LR 0.000034
0.68783855
0.68785310
0.68785542
0.68782383
0.68782943
0.68782657
0.68781143
0.68779284
0.68775648
0.68775135
0.68777072
0.68775547
0.68772078
0.68771797
0.68771237
0.68772709
0.68773443
0.68775517
0.68771982
0.68773395
0.68775862
INFO - Training [28][  120/  196]   Loss 0.454803   Top1 85.364583   Top5 98.763021   BatchTime 0.407385   LR 0.000030
0.68776035
0.68776613
0.68775463
0.68772620
0.68772244
0.68768388
0.68764853
0.68761170
0.68761569
0.68761706
0.68760067
0.68758225
0.68756789
0.68754268
0.68750370
0.68749774
0.68748021
0.68745559
0.68743283
0.68740618
0.68738031
0.68736941
INFO - Training [28][  140/  196]   Loss 0.453661   Top1 85.415737   Top5 98.805804   BatchTime 0.400159   LR 0.000026
0.68734848
0.68730700
0.68726707
0.68723100
0.68720394
0.68720907
0.68718743
0.68715912
0.68713319
0.68712366
0.68709528
0.68706954
0.68702614
0.68700427
0.68698269
0.68697482
INFO - Training [28][  160/  196]   Loss 0.456975   Top1 85.234375   Top5 98.776855   BatchTime 0.396115   LR 0.000022
0.68696487
0.68693262
0.68692392
0.68690771
0.68690139
0.68688291
0.68687308
0.68686384
0.68684018
0.68683815
0.68684232
0.68681008
0.68681312
0.68680674
0.68676227
0.68675971
0.68675357
0.68675441
0.68676156
0.68676746
0.68676376
0.68675345
INFO - Training [28][  180/  196]   Loss 0.456601   Top1 85.190972   Top5 98.700087   BatchTime 0.392194   LR 0.000018
0.68673348
0.68673193
0.68673778
0.68673211
0.68671393
0.68673378
0.68674916
0.68677169
0.68676662
0.68677485
0.68676710
0.68677968
0.68677455
0.68677431
0.68678397
********************pre-trained*****************
INFO - ==> Top1: 85.292    Top5: 98.702    Loss: 0.454
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.366365   Top1 88.496094   Top5 99.531250   BatchTime 0.133311
INFO - Validation [28][   40/   40]   Loss 0.350466   Top1 88.660000   Top5 99.590000   BatchTime 0.094141
INFO - ==> Top1: 88.660    Top5: 99.590    Loss: 0.350
INFO - ==> Sparsity : 0.600
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 86.480   Top5: 99.380]
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0794)
features.2.conv.0 tensor(0.0512)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0938)
features.3.conv.0 tensor(0.0382)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.2363)
features.4.conv.0 tensor(0.0628)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.1313)
features.5.conv.0 tensor(0.2697)
features.5.conv.3 tensor(0.0943)
features.5.conv.6 tensor(0.1743)
features.6.conv.0 tensor(0.0290)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0674)
features.7.conv.0 tensor(0.1050)
features.7.conv.3 tensor(0.1117)
features.7.conv.6 tensor(0.2777)
features.8.conv.0 tensor(0.1289)
features.8.conv.3 tensor(0.1056)
features.8.conv.6 tensor(0.2531)
features.9.conv.0 tensor(0.1804)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.3866)
features.10.conv.0 tensor(0.0524)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.0889)
features.11.conv.0 tensor(0.6645)
features.11.conv.3 tensor(0.1370)
features.11.conv.6 tensor(0.6848)
features.12.conv.0 tensor(0.7065)
features.12.conv.3 tensor(0.1117)
features.12.conv.6 tensor(0.8061)
features.13.conv.0 tensor(0.1694)
features.13.conv.3 tensor(0.1487)
features.13.conv.6 tensor(0.4767)
features.14.conv.0 tensor(0.9772)
features.14.conv.3 tensor(0.0920)
features.14.conv.6 tensor(0.9763)
features.15.conv.0 tensor(0.9624)
features.15.conv.3 tensor(0.0656)
features.15.conv.6 tensor(0.3865)
features.16.conv.0 tensor(0.3306)
features.16.conv.3 tensor(0.1053)
features.16.conv.6 tensor(0.6914)
conv.0 tensor(0.6838)
tensor(1313253.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
0.68677521
0.68673813
0.68674940
0.68675762
0.68674618
0.68672246
0.68673098
0.68672144
0.68671346
0.68671119
0.68669689
0.68669432
0.68669099
0.68668163
0.68667144
0.68665147
0.68664211
0.68664002
0.68665874
0.68665326
0.68664140
0.68663996
INFO - Training [29][   20/  196]   Loss 0.477132   Top1 84.277344   Top5 98.066406   BatchTime 0.442384   LR 0.000013
0.68664318
0.68662697
0.68661487
0.68658155
0.68656820
0.68658423
0.68655854
0.68656033
0.68654859
0.68655211
0.68654329
0.68654591
0.68655014
0.68656689
0.68656641
0.68656504
0.68654972
0.68655467
0.68654501
0.68657577
0.68658346
INFO - Training [29][   40/  196]   Loss 0.467907   Top1 84.667969   Top5 98.242188   BatchTime 0.410508   LR 0.000010
0.68658173
0.68657112
0.68654692
0.68652201
0.68653822
0.68651015
0.68649018
0.68647140
0.68646294
0.68648171
0.68651795
0.68651038
0.68649441
0.68647575
0.68645298
0.68643242
0.68635523
INFO - Training [29][   60/  196]   Loss 0.471246   Top1 84.667969   Top5 98.378906   BatchTime 0.395377   LR 0.000008
0.68634707
0.68638217
0.68640476
0.68641388
0.68642181
0.68643963
0.68642533
0.68642098
0.68639857
0.68634391
0.68623185
0.68624318
0.68635058
0.68640733
0.68641710
0.68642116
0.68643057
0.68644637
0.68642974
0.68640935
0.68639183
INFO - Training [29][   80/  196]   Loss 0.460224   Top1 85.078125   Top5 98.471680   BatchTime 0.389984   LR 0.000005
0.68635833
0.68629789
0.68623453
0.68619335
0.68602371
0.68589085
0.68603855
0.68617040
0.68626952
0.68631542
0.68633795
0.68633485
0.68631309
0.68625337
0.68621737
0.68615735
0.68616104
0.68611068
0.68602777
0.68578094
0.68562555
0.68568987
INFO - Training [29][  100/  196]   Loss 0.453417   Top1 85.414062   Top5 98.546875   BatchTime 0.386229   LR 0.000004
0.68600893
0.68611592
0.68617219
0.68617666
0.68618119
0.68616563
0.68612856
0.68610293
0.68602246
0.68590504
0.68564314
0.68557465
0.68559372
0.68558604
0.68557471
0.68556362
INFO - Training [29][  120/  196]   Loss 0.449544   Top1 85.569661   Top5 98.613281   BatchTime 0.384166   LR 0.000002
0.68557221
0.68557918
0.68558109
0.68556261
0.68557853
0.68559188
0.68559897
0.68556517
0.68556088
0.68554813
0.68554306
0.68554080
0.68558478
0.68553364
0.68550104
0.68550450
0.68549687
0.68549246
0.68550277
0.68548727
0.68549132
INFO - Training [29][  140/  196]   Loss 0.450951   Top1 85.544085   Top5 98.652344   BatchTime 0.382485   LR 0.000001
0.68548650
0.68547505
0.68546724
0.68545979
0.68547541
0.68546730
0.68545693
0.68544137
0.68543667
0.68544871
0.68546069
0.68544555
0.68545109
0.68544805
0.68543589
0.68543100
0.68543106
0.68540853
0.68541294
INFO - Training [29][  160/  196]   Loss 0.454611   Top1 85.371094   Top5 98.632812   BatchTime 0.375767   LR 0.000001
0.68541783
0.68541312
0.68540835
0.68540978
0.68539566
0.68539214
0.68536758
0.68536729
0.68536752
0.68536466
0.68536121
0.68536019
0.68536150
0.68536502
0.68536210
0.68535757
0.68534929
0.68534303
0.68533313
0.68532896
0.68533492
0.68535852
0.68536353
INFO - Training [29][  180/  196]   Loss 0.453850   Top1 85.401476   Top5 98.613281   BatchTime 0.371793   LR 0.000000
0.68537265
0.68537408
0.68536288
0.68536693
0.68535817
0.68536133
0.68534809
0.68533856
0.68536454
0.68536156
0.68536937
0.68536484
0.68537122
0.68537098
********************pre-trained*****************
INFO - ==> Top1: 85.402    Top5: 98.608    Loss: 0.453
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [29][   20/   40]   Loss 0.381476   Top1 87.402344   Top5 99.550781   BatchTime 0.134815
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0790)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0938)
features.3.conv.0 tensor(0.0379)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2363)
features.4.conv.0 tensor(0.0630)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.1313)
features.5.conv.0 tensor(0.2690)
features.5.conv.3 tensor(0.0943)
features.5.conv.6 tensor(0.1743)
features.6.conv.0 tensor(0.0293)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0675)
features.7.conv.0 tensor(0.1055)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.2777)
features.8.conv.0 tensor(0.1292)
features.8.conv.3 tensor(0.1053)
features.8.conv.6 tensor(0.2539)
features.9.conv.0 tensor(0.1838)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.3861)
features.10.conv.0 tensor(0.0529)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.0873)
features.11.conv.0 tensor(0.6644)
features.11.conv.3 tensor(0.1364)
features.11.conv.6 tensor(0.6851)
features.12.conv.0 tensor(0.7060)
features.12.conv.3 tensor(0.1115)
features.12.conv.6 tensor(0.8059)
features.13.conv.0 tensor(0.1700)
features.13.conv.3 tensor(0.1487)
features.13.conv.6 tensor(0.4785)
features.14.conv.0 tensor(0.9772)
features.14.conv.3 tensor(0.0910)
features.14.conv.6 tensor(0.9759)
features.15.conv.0 tensor(0.9625)
features.15.conv.3 tensor(0.0656)
features.15.conv.6 tensor(0.3871)
features.16.conv.0 tensor(0.3355)
features.16.conv.3 tensor(0.1047)
features.16.conv.6 tensor(0.6908)
conv.0 tensor(0.6851)
tensor(1314561.) 2188896.0
INFO - Validation [29][   40/   40]   Loss 0.371006   Top1 87.360000   Top5 99.600000   BatchTime 0.096105
INFO - ==> Top1: 87.360    Top5: 99.600    Loss: 0.371
INFO - ==> Sparsity : 0.601
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
0.68536693
0.68661892
0.68507290
0.68427050
0.68398768
0.68396086
0.68407804
0.68421608
0.68435800
0.68439001
0.68457478
0.68385381
0.68221194
0.68181527
0.67891347
0.67824864
0.67670947
0.67541742
0.67658627
0.67524934
0.67387789
0.67477131
0.67407531
0.67592430
INFO - Training [30][   20/  196]   Loss 0.489592   Top1 84.003906   Top5 98.222656   BatchTime 0.448367   LR 0.001250
0.67903805
0.68011028
0.68081397
0.68062067
0.68174022
0.68373346
0.68509674
0.68610382
0.68774873
0.69003242
0.69297183
0.69557315
0.69754100
0.69931751
0.70084101
INFO - Training [30][   40/  196]   Loss 0.491876   Top1 84.033203   Top5 98.310547   BatchTime 0.416047   LR 0.001250
0.70206124
0.70291352
0.70318168
0.70369142
0.70386177
0.70422024
0.70413715
0.70440382
0.70459938
0.70477539
0.70550048
0.70608848
0.70632517
0.70626044
0.70607561
0.70605278
0.70578408
0.70591265
0.70585889
0.70591891
0.70585573
0.70580161
INFO - Training [30][   60/  196]   Loss 0.503221   Top1 83.619792   Top5 98.313802   BatchTime 0.396495   LR 0.001250
0.70578611
0.70602924
0.70594883
0.70602572
0.70663351
0.70844281
0.71008909
0.71089375
0.71118122
0.71137023
0.71154720
0.71159756
0.71179491
0.71252936
0.71371269
0.71545577
0.71541995
0.71510202
0.71468097
0.71417314
0.71362138
0.71323723
INFO - Training [30][   80/  196]   Loss 0.507106   Top1 83.417969   Top5 98.388672   BatchTime 0.391646   LR 0.001250
0.71277887
0.71267921
0.71260929
0.71219844
0.71182966
0.71195823
0.71212006
0.71216279
0.71214873
0.71203095
0.71195191
0.71182406
0.71186715
0.71173936
0.71181834
INFO - Training [30][  100/  196]   Loss 0.502589   Top1 83.726562   Top5 98.464844   BatchTime 0.389690   LR 0.001250
0.71186483
0.71190542
0.71209335
0.71245217
0.71290970
0.71356672
0.71402246
0.71434134
0.71484792
0.71508229
0.71503699
0.71516019
0.71524304
0.71505028
0.71472549
0.71473211
0.71489865
0.71500379
0.71485198
0.71468693
0.71447158
0.71377259
INFO - Training [30][  120/  196]   Loss 0.496589   Top1 83.997396   Top5 98.512370   BatchTime 0.386016   LR 0.001249
0.71318525
0.71240658
0.71169996
0.71122038
0.71119195
0.71153307
0.71129012
0.71142703
0.71209073
0.71271133
0.71330947
0.71278328
0.71208423
0.71213722
0.71221083
0.71213013
0.71247721
0.71291506
0.71355414
0.71464705
0.71651661
0.71877819
INFO - Training [30][  140/  196]   Loss 0.495232   Top1 84.123884   Top5 98.537946   BatchTime 0.382858   LR 0.001249
0.71939033
0.72018510
0.72136152
0.72277480
0.72412300
0.72551894
0.72689873
0.72809714
0.72925639
0.73021108
0.73554051
0.73517299
0.73515725
0.73504108
0.73486507
0.73473865
0.73460937
0.73435450
0.73419803
0.73402202
0.73388940
INFO - Training [30][  160/  196]   Loss 0.495170   Top1 84.121094   Top5 98.564453   BatchTime 0.373670   LR 0.001249
0.73364675
0.73344767
0.73328412
0.73324591
0.73313957
0.73280102
0.73260903
0.73224878
0.73203516
0.73172140
0.73088902
0.73046267
0.72973162
0.72971386
0.73020709
0.73086333
0.73109841
INFO - Training [30][  180/  196]   Loss 0.498508   Top1 83.999566   Top5 98.515625   BatchTime 0.368203   LR 0.001248
0.73176402
0.73204613
0.73199904
0.73199028
0.73185456
0.73173529
0.73151970
0.73131770
0.73121464
0.73118711
0.73111540
0.73095489
0.73105472
0.73121697
0.73139203
0.73178011
********************pre-trained*****************
INFO - ==> Top1: 83.952    Top5: 98.498    Loss: 0.500
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.501358   Top1 84.355469   Top5 99.335938   BatchTime 0.129846
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.4688)
features.1.conv.0 tensor(0.0397)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0768)
features.2.conv.0 tensor(0.0498)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0929)
features.3.conv.0 tensor(0.0379)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.2001)
features.4.conv.0 tensor(0.0604)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.1027)
features.5.conv.0 tensor(0.2257)
features.5.conv.3 tensor(0.0938)
features.5.conv.6 tensor(0.1252)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0680)
features.7.conv.0 tensor(0.0881)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.2450)
features.8.conv.0 tensor(0.1166)
features.8.conv.3 tensor(0.1085)
features.8.conv.6 tensor(0.2770)
features.9.conv.0 tensor(0.0888)
features.9.conv.3 tensor(0.1687)
features.9.conv.6 tensor(0.3521)
features.10.conv.0 tensor(0.0452)
features.10.conv.3 tensor(0.0909)
features.10.conv.6 tensor(0.0817)
features.11.conv.0 tensor(0.6124)
features.11.conv.3 tensor(0.1387)
features.11.conv.6 tensor(0.6483)
features.12.conv.0 tensor(0.6770)
features.12.conv.3 tensor(0.1130)
features.12.conv.6 tensor(0.7979)
features.13.conv.0 tensor(0.1357)
features.13.conv.3 tensor(0.1468)
features.13.conv.6 tensor(0.3860)
features.14.conv.0 tensor(0.9763)
features.14.conv.3 tensor(0.0918)
features.14.conv.6 tensor(0.9708)
features.15.conv.0 tensor(0.9625)
features.15.conv.3 tensor(0.0632)
features.15.conv.6 tensor(0.4284)
features.16.conv.0 tensor(0.0750)
features.16.conv.3 tensor(0.1064)
features.16.conv.6 tensor(0.3255)
conv.0 tensor(0.5072)
tensor(1072032.) 2188896.0
INFO - Validation [30][   40/   40]   Loss 0.484971   Top1 84.600000   Top5 99.410000   BatchTime 0.091318
INFO - ==> Top1: 84.600    Top5: 99.410    Loss: 0.485
INFO - ==> Sparsity : 0.490
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
0.73203659
0.73377663
0.73373860
0.73370105
0.73368967
0.73372048
0.73378134
0.73378807
0.73378098
0.73368490
0.73354745
0.73349255
0.73342675
0.73322523
0.73308104
0.73314679
0.73321563
0.73330647
0.73357415
0.73381972
0.73401922
INFO - Training [31][   20/  196]   Loss 0.528666   Top1 82.617188   Top5 97.812500   BatchTime 0.386935   LR 0.001248
0.73411739
0.73418325
0.73418868
0.73437709
0.73455733
0.73436421
0.73450863
0.73447067
0.73441249
0.73432857
0.73408800
0.73376560
0.73367828
0.73363543
0.73349136
0.73337507
0.73318928
0.73303968
0.73292059
0.73288947
0.73294294
0.73279387
INFO - Training [31][   40/  196]   Loss 0.524968   Top1 82.705078   Top5 97.968750   BatchTime 0.323422   LR 0.001247
0.73271543
0.73262936
0.73266470
0.73276931
0.73265415
0.73259765
0.73256522
0.73244095
0.73233849
0.73240155
0.73239386
0.73236448
0.73260725
0.73245049
0.73234427
0.73234516
0.73220527
0.73225737
0.73221558
0.73236382
0.73243588
0.73248851
INFO - Training [31][   60/  196]   Loss 0.516382   Top1 82.988281   Top5 98.190104   BatchTime 0.312720   LR 0.001247
0.73254788
0.73274660
0.73290801
0.73308551
0.73311365
0.73315507
0.73321378
0.73316932
0.73317558
0.73312849
0.73311222
0.73305202
0.73306340
0.73305458
INFO - Training [31][   80/  196]   Loss 0.510567   Top1 83.222656   Top5 98.334961   BatchTime 0.304401   LR 0.001246
0.73292553
0.73297459
0.73270088
0.73252875
0.73285753
0.73306346
0.73296893
0.73278528
0.73881406
0.74473339
0.74768609
0.74799269
0.74776268
0.74748194
0.74746561
0.74742883
0.74734318
0.74737483
0.74729449
0.74705714
INFO - Training [31][  100/  196]   Loss 0.504646   Top1 83.574219   Top5 98.367188   BatchTime 0.305454   LR 0.001246
0.74678719
0.74691820
0.74696326
0.74761492
0.74817550
0.74826527
0.74791288
0.74775171
0.74750566
0.74720049
0.74666262
0.74628770
0.74586332
0.74558443
0.74553043
0.74543077
0.74512720
0.74492991
0.74478579
0.74475718
0.74475747
INFO - Training [31][  120/  196]   Loss 0.503322   Top1 83.675130   Top5 98.437500   BatchTime 0.300187   LR 0.001245
0.74461526
0.74457192
0.74454564
0.74431598
0.74386227
0.74366009
0.74363285
0.74369329
0.74366647
0.74354732
0.74364066
0.74387664
0.74374640
0.74374539
0.74382365
0.74382812
0.74391121
0.74409360
0.74490219
0.74477488
0.74439991
INFO - Training [31][  140/  196]   Loss 0.505353   Top1 83.627232   Top5 98.440290   BatchTime 0.298803   LR 0.001244
0.74408585
0.74380964
0.74346906
0.74295980
0.74257386
0.74225563
0.74221021
0.74209428
0.74165040
0.74129927
0.74084204
0.74048507
0.74010199
0.73958910
0.73918211
0.73869687
0.73841220
0.73790896
0.73711956
0.73651230
0.73592627
0.73520118
INFO - Training [31][  160/  196]   Loss 0.507737   Top1 83.535156   Top5 98.425293   BatchTime 0.306094   LR 0.001244
0.73457241
0.73338455
0.73205805
0.73080546
0.72986943
0.72893196
0.72839051
0.72792846
0.72745460
0.72688317
0.72640085
0.72624904
0.72595745
0.72566301
0.72492737
0.72439498
0.72384870
0.72313547
INFO - Training [31][  180/  196]   Loss 0.506416   Top1 83.615451   Top5 98.372396   BatchTime 0.310920   LR 0.001243
0.72256619
0.72239810
0.72257715
0.72284615
0.72293293
0.72302592
0.72326142
0.72348458
0.72382551
0.72392613
0.72408253
0.72415745
0.72429675
0.72390604
0.72388816
********************pre-trained*****************
INFO - ==> Top1: 83.654    Top5: 98.390    Loss: 0.505
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [31][   20/   40]   Loss 0.416449   Top1 86.210938   Top5 99.453125   BatchTime 0.135089
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.4668)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0729)
features.2.conv.0 tensor(0.0495)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0932)
features.3.conv.0 tensor(0.0454)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.2322)
features.4.conv.0 tensor(0.0645)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.0986)
features.5.conv.0 tensor(0.2360)
features.5.conv.3 tensor(0.0851)
features.5.conv.6 tensor(0.1408)
features.6.conv.0 tensor(0.0249)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0659)
features.7.conv.0 tensor(0.0782)
features.7.conv.3 tensor(0.1030)
features.7.conv.6 tensor(0.1107)
features.8.conv.0 tensor(0.1008)
features.8.conv.3 tensor(0.1082)
features.8.conv.6 tensor(0.2652)
features.9.conv.0 tensor(0.0959)
features.9.conv.3 tensor(0.1693)
features.9.conv.6 tensor(0.1202)
features.10.conv.0 tensor(0.0423)
features.10.conv.3 tensor(0.0880)
features.10.conv.6 tensor(0.0679)
features.11.conv.0 tensor(0.6371)
features.11.conv.3 tensor(0.1410)
features.11.conv.6 tensor(0.6720)
features.12.conv.0 tensor(0.6547)
features.12.conv.3 tensor(0.1134)
features.12.conv.6 tensor(0.7973)
features.13.conv.0 tensor(0.1361)
features.13.conv.3 tensor(0.1462)
features.13.conv.6 tensor(0.3727)
features.14.conv.0 tensor(0.9757)
features.14.conv.3 tensor(0.0949)
features.14.conv.6 tensor(0.9724)
features.15.conv.0 tensor(0.9646)
features.15.conv.3 tensor(0.0634)
features.15.conv.6 tensor(0.4380)
features.16.conv.0 tensor(0.0769)
features.16.conv.3 tensor(0.1071)
features.16.conv.6 tensor(0.7701)
conv.0 tensor(0.5006)
tensor(1198333.) 2188896.0
INFO - Validation [31][   40/   40]   Loss 0.407952   Top1 86.160000   Top5 99.520000   BatchTime 0.093000
INFO - ==> Top1: 86.160    Top5: 99.520    Loss: 0.408
INFO - ==> Sparsity : 0.547
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
0.72334480
0.72326142
0.72301155
0.72277057
0.72347534
0.72976559
0.73007923
0.73014009
0.73020422
0.73023212
0.73002660
0.72988480
0.72988814
0.72994858
0.72981155
0.72995317
0.72972924
0.72981095
0.73000550
0.73008400
0.73013151
0.73007065
0.72994399
0.72946358
INFO - Training [32][   20/  196]   Loss 0.513219   Top1 83.339844   Top5 98.027344   BatchTime 0.444378   LR 0.001242
0.72914547
0.72897208
0.72874552
0.72874504
0.72883236
0.72915000
0.72921365
0.72917330
0.72907257
0.72922832
0.72903216
0.72884607
0.72860676
0.72818911
0.72790909
0.72789621
0.72752875
0.72757441
0.72766089
0.72769535
INFO - Training [32][   40/  196]   Loss 0.517268   Top1 83.447266   Top5 98.164062   BatchTime 0.421645   LR 0.001241
0.72777873
0.72800034
0.72857291
0.72936970
0.73034859
0.73099810
0.73159355
0.73246372
0.73326427
0.73386472
0.73415428
0.73475045
0.73525184
0.73573136
0.73603398
0.73639745
0.73681217
INFO - Training [32][   60/  196]   Loss 0.509576   Top1 83.665365   Top5 98.300781   BatchTime 0.401576   LR 0.001240
0.73719776
0.73759609
0.73777562
0.73773992
0.73765409
0.73786569
0.73823220
0.73837727
0.73838955
0.73837435
0.73846132
0.73850876
0.73844141
0.73835653
0.73800749
0.73760289
0.73729211
0.73686701
0.73647588
0.73753154
0.73703498
0.73643452
INFO - Training [32][   80/  196]   Loss 0.506395   Top1 83.808594   Top5 98.422852   BatchTime 0.392068   LR 0.001239
0.73602539
0.73545456
0.73440999
0.73345482
0.73324126
0.73247004
0.73177302
0.73109299
0.73054171
0.73055512
0.73115528
0.73198915
0.73327595
0.73425388
0.73507541
0.73582625
0.73651254
INFO - Training [32][  100/  196]   Loss 0.498714   Top1 84.058594   Top5 98.492188   BatchTime 0.383257   LR 0.001238
0.73717666
0.73787141
0.73872161
0.73936081
0.73985600
0.74032664
0.74082613
0.74161965
0.74219018
0.74317205
0.74338973
0.74315280
0.74295652
0.74281436
0.74257851
0.74240440
0.74233973
0.74218547
0.74211985
0.74225402
0.74227881
0.74228650
0.74212974
0.74220783
INFO - Training [32][  120/  196]   Loss 0.493247   Top1 84.251302   Top5 98.593750   BatchTime 0.374858   LR 0.001237
0.74223447
0.74213898
0.74225312
0.74218017
0.74210727
0.74208874
0.74191993
0.74198824
0.74204093
0.74198937
0.74200869
0.74198735
0.74191219
0.74175501
0.74137539
0.74123973
0.74102455
0.74071223
INFO - Training [32][  140/  196]   Loss 0.496350   Top1 84.143415   Top5 98.618862   BatchTime 0.370107   LR 0.001236
0.74014950
0.73960531
0.73895401
0.73810542
0.73723793
0.73633641
0.73557538
0.73463595
0.73378450
0.73308843
0.73247874
0.73194110
0.73147607
0.73114598
0.73061979
0.72995985
0.72922713
0.72834092
INFO - Training [32][  160/  196]   Loss 0.498198   Top1 84.025879   Top5 98.579102   BatchTime 0.363804   LR 0.001235
0.72755772
0.72665793
0.72564632
0.72452319
0.72352374
0.72285080
0.72231621
0.72180688
0.72114038
0.72040433
0.71983039
0.71977133
0.71957958
0.71939826
0.71901166
0.71833473
0.71770966
0.71705472
0.71612155
0.71536797
0.71427673
0.71302694
0.71288377
0.71297795
INFO - Training [32][  180/  196]   Loss 0.496796   Top1 84.025608   Top5 98.546007   BatchTime 0.351732   LR 0.001234
0.71274424
0.71238863
0.71162212
0.71080661
0.70955247
0.70896858
0.70892632
0.70928019
0.70881027
0.70821685
0.70757663
INFO - ==> Top1: 84.128    Top5: 98.534    Loss: 0.495
0.70680100
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [32][   20/   40]   Loss 0.565584   Top1 80.859375   Top5 99.160156   BatchTime 0.143916
INFO - Validation [32][   40/   40]   Loss 0.569154   Top1 80.890000   Top5 99.090000   BatchTime 0.098689
INFO - ==> Top1: 80.890    Top5: 99.090    Loss: 0.569
INFO - ==> Sparsity : 0.549
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 86.860   Top5: 99.590]
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.4473)
features.1.conv.0 tensor(0.0417)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0720)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0914)
features.3.conv.0 tensor(0.0454)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.1851)
features.4.conv.0 tensor(0.0703)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.1447)
features.5.conv.0 tensor(0.2349)
features.5.conv.3 tensor(0.0856)
features.5.conv.6 tensor(0.1576)
features.6.conv.0 tensor(0.0288)
features.6.conv.3 tensor(0.0608)
features.6.conv.6 tensor(0.0643)
features.7.conv.0 tensor(0.0728)
features.7.conv.3 tensor(0.1102)
features.7.conv.6 tensor(0.2102)
features.8.conv.0 tensor(0.1053)
features.8.conv.3 tensor(0.1097)
features.8.conv.6 tensor(0.2354)
features.9.conv.0 tensor(0.1049)
features.9.conv.3 tensor(0.1670)
features.9.conv.6 tensor(0.2874)
features.10.conv.0 tensor(0.0564)
features.10.conv.3 tensor(0.0909)
features.10.conv.6 tensor(0.1067)
features.11.conv.0 tensor(0.6102)
features.11.conv.3 tensor(0.1427)
features.11.conv.6 tensor(0.6756)
features.12.conv.0 tensor(0.6974)
features.12.conv.3 tensor(0.1130)
features.12.conv.6 tensor(0.8080)
features.13.conv.0 tensor(0.1395)
features.13.conv.3 tensor(0.1476)
features.13.conv.6 tensor(0.3927)
features.14.conv.0 tensor(0.9766)
features.14.conv.3 tensor(0.0950)
features.14.conv.6 tensor(0.9801)
features.15.conv.0 tensor(0.9646)
features.15.conv.3 tensor(0.0634)
features.15.conv.6 tensor(0.4388)
features.16.conv.0 tensor(0.0887)
features.16.conv.3 tensor(0.1082)
features.16.conv.6 tensor(0.7103)
conv.0 tensor(0.5185)
tensor(1202275.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
0.70743257
0.70740771
0.70731539
0.70674491
0.70623654
0.70550650
0.70486474
0.70464748
0.70530069
0.70507795
0.70571274
0.70616144
0.70671308
0.70716923
0.70746696
0.70738530
0.70718038
0.70707434
0.70722425
0.70730639
0.70754421
0.70749789
INFO - Training [33][   20/  196]   Loss 0.519354   Top1 82.792969   Top5 98.007812   BatchTime 0.442213   LR 0.001232
0.70745200
0.70727801
0.70712084
0.70685375
0.70678836
0.70632744
0.70613581
0.70601410
0.70573223
0.70558596
0.70546287
0.70652390
0.70856029
0.70888180
0.70924264
0.70997721
0.71080744
INFO - Training [33][   40/  196]   Loss 0.508692   Top1 83.359375   Top5 98.212891   BatchTime 0.393792   LR 0.001230
0.71168494
0.71244216
0.71292043
0.71319145
0.71350712
0.71373433
0.71381998
0.71405321
0.71434683
0.71405524
0.71374840
0.71352917
0.71325529
0.71295804
0.71286762
0.71293837
0.71304196
0.71305668
0.71318078
0.71335310
0.71329248
INFO - Training [33][   60/  196]   Loss 0.507832   Top1 83.470052   Top5 98.287760   BatchTime 0.389625   LR 0.001229
0.71307427
0.71283299
0.71236902
0.71198946
0.71186262
0.71157050
0.71121383
0.71110433
0.71081382
0.71073973
0.71011937
0.70972669
0.70943618
0.70954841
0.70937306
0.70892602
0.70854115
0.70826298
0.70845389
0.70838028
0.70849383
0.70832276
INFO - Training [33][   80/  196]   Loss 0.501832   Top1 83.813477   Top5 98.413086   BatchTime 0.384620   LR 0.001228
0.70827335
0.70829517
0.70844328
0.70849317
0.70862859
0.70866495
0.70880634
0.70874047
0.70867306
0.70867002
0.70846134
0.70833164
0.70838273
0.70863825
0.70901197
0.70970970
0.71043271
0.71074218
0.71043313
0.71014303
0.70979184
INFO - Training [33][  100/  196]   Loss 0.497673   Top1 83.812500   Top5 98.460938   BatchTime 0.383661   LR 0.001226
0.70942104
0.70947206
0.70958453
0.70968986
0.70973283
0.70929945
0.70920432
0.70907116
0.70860702
0.70806414
0.70745540
0.70720875
0.70699179
0.70690286
0.70701396
0.70698905
0.70705050
INFO - Training [33][  120/  196]   Loss 0.492308   Top1 84.026693   Top5 98.515625   BatchTime 0.380169   LR 0.001225
0.70715767
0.70677507
0.70714670
0.70836145
0.70951629
0.71039754
0.71124727
0.71261543
0.71436328
0.71583670
0.71713930
0.71848005
0.72005999
0.72186196
0.72399741
0.72639579
0.72652841
0.72656024
0.72645038
0.72637433
0.72633433
INFO - Training [33][  140/  196]   Loss 0.491423   Top1 84.023438   Top5 98.579799   BatchTime 0.379269   LR 0.001224
0.72624493
0.72600198
0.72574311
0.72552639
0.72531003
0.72501284
0.72480637
0.72465485
0.72460109
0.72463137
0.72461665
0.72480440
0.72494978
0.72496414
0.72498006
0.72506493
0.72543401
0.72601318
0.72667134
0.72713560
0.72747719
0.72780824
0.72795922
INFO - Training [33][  160/  196]   Loss 0.494254   Top1 83.918457   Top5 98.581543   BatchTime 0.377741   LR 0.001222
0.72809041
0.72814703
0.72822928
0.73467904
0.73450726
0.73433810
0.73426396
0.73434335
0.73438132
0.73442525
0.73444492
0.73454487
0.73650044
0.73653871
0.73637503
0.73626250
0.73622203
0.73628825
0.73614401
0.73601758
INFO - Training [33][  180/  196]   Loss 0.495113   Top1 83.875868   Top5 98.543837   BatchTime 0.369931   LR 0.001221
0.73581004
0.73561490
0.73563468
0.73543692
0.73534435
0.73520142
0.73516202
0.73508102
0.73492795
0.73484421
0.73471081
0.73464215
********************pre-trained*****************
INFO - ==> Top1: 83.996    Top5: 98.554    Loss: 0.493
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [33][   20/   40]   Loss 0.386318   Top1 87.734375   Top5 99.375000   BatchTime 0.136703
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.4824)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0725)
features.2.conv.0 tensor(0.0486)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0894)
features.3.conv.0 tensor(0.0428)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.2023)
features.4.conv.0 tensor(0.0635)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0967)
features.5.conv.0 tensor(0.2064)
features.5.conv.3 tensor(0.0810)
features.5.conv.6 tensor(0.1496)
features.6.conv.0 tensor(0.0262)
features.6.conv.3 tensor(0.0596)
features.6.conv.6 tensor(0.0668)
features.7.conv.0 tensor(0.0841)
features.7.conv.3 tensor(0.1108)
features.7.conv.6 tensor(0.3042)
features.8.conv.0 tensor(0.0932)
features.8.conv.3 tensor(0.1091)
features.8.conv.6 tensor(0.1270)
features.9.conv.0 tensor(0.1010)
features.9.conv.3 tensor(0.1733)
features.9.conv.6 tensor(0.3214)
features.10.conv.0 tensor(0.0486)
features.10.conv.3 tensor(0.0880)
features.10.conv.6 tensor(0.0263)
features.11.conv.0 tensor(0.5988)
features.11.conv.3 tensor(0.1395)
features.11.conv.6 tensor(0.6665)
features.12.conv.0 tensor(0.6466)
features.12.conv.3 tensor(0.1123)
features.12.conv.6 tensor(0.8055)
features.13.conv.0 tensor(0.1283)
features.13.conv.3 tensor(0.1449)
features.13.conv.6 tensor(0.4027)
features.14.conv.0 tensor(0.9762)
features.14.conv.3 tensor(0.0962)
features.14.conv.6 tensor(0.9748)
features.15.conv.0 tensor(0.9674)
features.15.conv.3 tensor(0.0666)
features.15.conv.6 tensor(0.4771)
features.16.conv.0 tensor(0.1374)
features.16.conv.3 tensor(0.1067)
features.16.conv.6 tensor(0.3654)
conv.0 tensor(0.5248)
tensor(1104812.) 2188896.0
INFO - Validation [33][   40/   40]   Loss 0.368664   Top1 87.970000   Top5 99.510000   BatchTime 0.093799
INFO - ==> Top1: 87.970    Top5: 99.510    Loss: 0.369
INFO - ==> Sparsity : 0.505
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 87.970   Top5: 99.510]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
0.73460156
0.73472941
0.73486263
0.73483551
0.73485476
0.73478824
0.73477530
0.73480922
0.73482066
0.73485786
0.73483390
0.73487753
0.73481232
0.73471051
0.73483372
0.73481011
0.73480111
0.73486894
0.73488557
0.73492920
INFO - Training [34][   20/  196]   Loss 0.511152   Top1 83.496094   Top5 97.871094   BatchTime 0.450128   LR 0.001218
0.73483157
0.73475426
0.73481262
0.74068505
0.74057233
0.74071187
0.74059260
0.74053568
0.74068731
0.74072444
0.74076623
0.74087965
0.74097884
0.74097854
0.74098080
0.74093682
0.74092001
0.74088627
0.74092728
0.74102849
0.74109906
0.74116105
INFO - Training [34][   40/  196]   Loss 0.506081   Top1 83.486328   Top5 98.134766   BatchTime 0.408455   LR 0.001216
0.74123341
0.74126065
0.74130511
0.74135071
0.74137288
0.74139273
0.74146068
0.74150610
0.74148941
0.74159807
0.74155629
0.74154013
0.74142534
0.74136925
0.74127388
0.74103642
0.74083048
0.74079984
0.74059755
0.74077469
0.74058735
0.74043614
INFO - Training [34][   60/  196]   Loss 0.500099   Top1 83.919271   Top5 98.177083   BatchTime 0.394811   LR 0.001215
0.74039346
0.74034977
0.74024928
0.74005169
0.73997527
0.73975128
0.73957932
0.73925638
0.73898584
0.73883247
0.73868400
0.73844558
0.73822880
0.73798150
0.73771334
0.73767042
0.73764747
INFO - Training [34][   80/  196]   Loss 0.490488   Top1 84.165039   Top5 98.364258   BatchTime 0.383996   LR 0.001213
0.73764318
0.73766047
0.73782134
0.73811382
0.73855728
0.73904055
0.73955649
0.73987234
0.74000198
0.73995900
0.74012536
0.74006653
0.73980397
0.73964298
0.73934692
0.73908514
0.73892146
0.73873127
0.73860312
0.73841149
0.73810434
0.73786330
0.73765963
INFO - Training [34][  100/  196]   Loss 0.485468   Top1 84.308594   Top5 98.460938   BatchTime 0.375901   LR 0.001211
0.73735219
0.73721027
0.73698997
0.73666102
0.73640281
0.73606718
0.73581469
0.73552591
0.73511034
0.73467231
0.73421901
0.73381031
0.73351240
0.73311132
0.73244375
INFO - Training [34][  120/  196]   Loss 0.481886   Top1 84.567057   Top5 98.479818   BatchTime 0.376457   LR 0.001209
0.73171395
0.73105103
0.73027194
0.72927415
0.72885597
0.72836185
0.72748315
0.72684366
0.72609198
0.72536594
0.72443157
0.72357613
0.72289580
0.72229332
0.72142601
0.72047186
0.71967298
0.71880597
0.71816623
0.71776938
0.71751362
0.71685547
0.71592247
INFO - Training [34][  140/  196]   Loss 0.480995   Top1 84.575893   Top5 98.523996   BatchTime 0.374312   LR 0.001208
0.71549177
0.71491653
0.71380860
0.71334088
0.71263641
0.71173382
0.71155661
0.71158558
0.71151453
0.71105838
0.71043724
0.71028239
0.71004111
0.70975441
0.70959967
0.70910537
0.70855266
0.70781356
0.70673138
0.70608956
0.70536584
0.70511472
INFO - Training [34][  160/  196]   Loss 0.482815   Top1 84.494629   Top5 98.520508   BatchTime 0.372275   LR 0.001206
0.70475978
0.70476013
0.70514309
0.70488983
0.70436436
0.70417774
0.70434159
0.70446068
0.70450437
0.70382315
0.70310581
0.70282471
0.70286161
0.70335108
0.70360374
0.70457584
INFO - Training [34][  180/  196]   Loss 0.484742   Top1 84.379340   Top5 98.496094   BatchTime 0.371882   LR 0.001204
0.70556688
0.70645601
0.70720148
0.70798987
0.70908022
0.71022451
0.71139735
0.71275902
0.71432418
0.71595496
0.71745759
0.71887827
0.72109783
0.72409362
0.72393239
0.72395754
********************pre-trained*****************
INFO - ==> Top1: 84.358    Top5: 98.496    Loss: 0.484
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.455394   Top1 86.289062   Top5 99.199219   BatchTime 0.140913
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0751)
features.2.conv.0 tensor(0.0460)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0903)
features.3.conv.0 tensor(0.0382)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.1923)
features.4.conv.0 tensor(0.0633)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.1545)
features.5.conv.0 tensor(0.2178)
features.5.conv.3 tensor(0.0885)
features.5.conv.6 tensor(0.1603)
features.6.conv.0 tensor(0.0285)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0638)
features.7.conv.0 tensor(0.0819)
features.7.conv.3 tensor(0.1117)
features.7.conv.6 tensor(0.1994)
features.8.conv.0 tensor(0.0998)
features.8.conv.3 tensor(0.1042)
features.8.conv.6 tensor(0.2575)
features.9.conv.0 tensor(0.1100)
features.9.conv.3 tensor(0.1678)
features.9.conv.6 tensor(0.3965)
features.10.conv.0 tensor(0.0586)
features.10.conv.3 tensor(0.0874)
features.10.conv.6 tensor(0.0819)
features.11.conv.0 tensor(0.6117)
features.11.conv.3 tensor(0.1387)
features.11.conv.6 tensor(0.6805)
features.12.conv.0 tensor(0.6567)
features.12.conv.3 tensor(0.1136)
features.12.conv.6 tensor(0.7971)
features.13.conv.0 tensor(0.1369)
features.13.conv.3 tensor(0.1437)
features.13.conv.6 tensor(0.4022)
features.14.conv.0 tensor(0.9774)
features.14.conv.3 tensor(0.0936)
features.14.conv.6 tensor(0.9756)
features.15.conv.0 tensor(0.9686)
features.15.conv.3 tensor(0.0670)
features.15.conv.6 tensor(0.4662)
features.16.conv.0 tensor(0.1602)
features.16.conv.3 tensor(0.1076)
features.16.conv.6 tensor(0.3455)
conv.0 tensor(0.5241)
tensor(1108233.) 2188896.0
INFO - Validation [34][   40/   40]   Loss 0.436784   Top1 86.570000   Top5 99.370000   BatchTime 0.099092
INFO - ==> Top1: 86.570    Top5: 99.370    Loss: 0.437
INFO - ==> Sparsity : 0.506
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 87.970   Top5: 99.510]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
0.72429299
0.72369432
0.72327650
0.72292328
0.72264129
0.72247553
0.72246897
0.72251850
0.72257215
0.72258955
0.72272879
0.72286874
0.72294706
0.72300142
0.72307324
0.72306037
0.72299111
0.72285730
0.72278249
0.72257710
INFO - Training [35][   20/  196]   Loss 0.487882   Top1 84.062500   Top5 97.910156   BatchTime 0.447210   LR 0.001201
0.72240150
0.72219700
0.72189271
0.72172701
0.72435629
0.73034376
0.73015654
0.72981983
0.72953743
0.72935671
0.72938764
0.72947186
0.72978604
0.73065120
0.73348874
0.73507303
0.73555338
0.73597765
0.73619628
0.73834068
0.73795855
INFO - Training [35][   40/  196]   Loss 0.489753   Top1 84.042969   Top5 98.261719   BatchTime 0.411591   LR 0.001199
0.73820734
0.73816413
0.73813605
0.73839670
0.74004221
0.74392241
0.74379081
0.74347878
0.74337858
0.74350250
0.74358040
0.74356532
0.74374920
0.74396664
0.74408597
0.74420023
0.74435228
0.74438417
0.74443269
0.74452263
0.74447429
INFO - Training [35][   60/  196]   Loss 0.482165   Top1 84.322917   Top5 98.391927   BatchTime 0.403311   LR 0.001197
0.74430329
0.74424595
0.74402422
0.74392194
0.74392724
0.74381131
0.74376208
0.74362469
0.74359757
0.74357951
0.74350125
0.74353409
0.74353260
0.74358219
0.74354506
0.74356520
0.74354148
0.74355459
0.74346501
0.74351186
0.74343157
INFO - Training [35][   80/  196]   Loss 0.483615   Top1 84.462891   Top5 98.457031   BatchTime 0.398999   LR 0.001195
0.74341667
0.74355966
0.74345219
0.74343067
0.74332094
0.74327886
0.74324375
0.74329352
0.74317390
0.74319851
0.74321496
0.74315983
0.74305588
0.74302024
0.74301618
0.74285227
0.74274486
0.74273753
0.74281824
0.74288404
0.74274713
INFO - Training [35][  100/  196]   Loss 0.480593   Top1 84.523438   Top5 98.515625   BatchTime 0.393466   LR 0.001192
0.74245203
0.74228680
0.74217886
0.74206436
0.74185443
0.74163681
0.74151284
0.74130476
0.74090868
0.74065620
0.74056298
0.74063236
0.74053830
0.74026269
0.74004817
0.73984015
0.73961586
INFO - Training [35][  120/  196]   Loss 0.479497   Top1 84.570312   Top5 98.554688   BatchTime 0.387531   LR 0.001190
0.73947525
0.73943973
0.73941076
0.73917449
0.73898453
0.73880464
0.73871493
0.73864180
0.73828954
0.73830158
0.73833603
0.73827189
0.73810613
0.73796225
0.73766583
0.73732281
0.73711038
0.73681337
0.73634833
0.73592877
0.73588920
0.73588967
INFO - Training [35][  140/  196]   Loss 0.476973   Top1 84.634487   Top5 98.610491   BatchTime 0.384263   LR 0.001188
0.73622245
0.73597026
0.73556811
0.73568898
0.73609835
0.73636967
0.73652375
0.73672062
0.73702347
0.73738265
0.73740536
0.73736763
0.73705924
0.73681933
0.73681539
0.73688614
INFO - Training [35][  160/  196]   Loss 0.480723   Top1 84.501953   Top5 98.569336   BatchTime 0.382317   LR 0.001186
0.74476105
0.74578500
0.74544132
0.74528033
0.74483263
0.74437207
0.74387729
0.74339104
0.74296892
0.74260515
0.74219602
0.74176431
0.74148422
0.74106151
0.74077487
0.74051708
0.74016798
0.73990530
0.73940426
0.73901635
0.73838836
0.73781353
INFO - Training [35][  180/  196]   Loss 0.480737   Top1 84.459635   Top5 98.554688   BatchTime 0.380635   LR 0.001184
0.73737580
0.73676646
0.73604101
0.73549736
0.73497444
0.73449606
0.73381782
0.73328644
0.73280954
0.73233020
0.73169154
0.73085952
0.72996747
0.72957766
0.72936112
********************pre-trained*****************
INFO - ==> Top1: 84.488    Top5: 98.562    Loss: 0.479
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [35][   20/   40]   Loss 0.394624   Top1 86.386719   Top5 99.316406   BatchTime 0.182183
INFO - Validation [35][   40/   40]   Loss 0.375699   Top1 87.010000   Top5 99.410000   BatchTime 0.142552
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.4570)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0914)
features.3.conv.0 tensor(0.0379)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.1552)
features.4.conv.0 tensor(0.0664)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.1001)
features.5.conv.0 tensor(0.0871)
features.5.conv.3 tensor(0.0880)
features.5.conv.6 tensor(0.1522)
features.6.conv.0 tensor(0.0321)
features.6.conv.3 tensor(0.0521)
features.6.conv.6 tensor(0.0618)
features.7.conv.0 tensor(0.0851)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.1303)
features.8.conv.0 tensor(0.0892)
features.8.conv.3 tensor(0.1019)
features.8.conv.6 tensor(0.1034)
features.9.conv.0 tensor(0.1142)
features.9.conv.3 tensor(0.1672)
features.9.conv.6 tensor(0.1955)
features.10.conv.0 tensor(0.0546)
features.10.conv.3 tensor(0.0900)
features.10.conv.6 tensor(0.0774)
features.11.conv.0 tensor(0.5720)
features.11.conv.3 tensor(0.1354)
features.11.conv.6 tensor(0.6968)
features.12.conv.0 tensor(0.6993)
features.12.conv.3 tensor(0.1123)
features.12.conv.6 tensor(0.7973)
features.13.conv.0 tensor(0.1282)
features.13.conv.3 tensor(0.1451)
features.13.conv.6 tensor(0.4136)
features.14.conv.0 tensor(0.9777)
features.14.conv.3 tensor(0.0975)
features.14.conv.6 tensor(0.9758)
features.15.conv.0 tensor(0.9695)
features.15.conv.3 tensor(0.0653)
features.15.conv.6 tensor(0.4640)
features.16.conv.0 tensor(0.1747)
features.16.conv.3 tensor(0.1119)
features.16.conv.6 tensor(0.5777)
conv.0 tensor(0.5299)
tensor(1173561.) 2188896.0
INFO - ==> Top1: 87.010    Top5: 99.410    Loss: 0.376
INFO - ==> Sparsity : 0.536
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 87.970   Top5: 99.510]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
0.73140222
0.73069638
0.73013210
0.72929305
0.72834665
0.72722870
0.72616881
0.72520334
0.72437042
0.72364259
0.72284830
0.72284269
0.72252500
0.72206575
0.72179180
0.72152430
0.72094035
0.72055870
0.72029799
0.72000402
INFO - Training [36][   20/  196]   Loss 0.504932   Top1 83.730469   Top5 97.851562   BatchTime 0.460977   LR 0.001180
0.71966970
0.71927881
0.71867806
0.71833956
0.71784055
0.71768826
0.71760380
0.71719956
0.71668601
0.71652603
0.71679151
0.71700019
0.71702701
0.71720666
0.71756899
0.71800500
0.71854538
0.71913904
0.71977651
0.72030395
0.72082365
0.72117960
0.72137135
INFO - Training [36][   40/  196]   Loss 0.497213   Top1 83.896484   Top5 98.144531   BatchTime 0.405004   LR 0.001177
0.72167236
0.72202164
0.72219354
0.72244519
0.72272968
0.72319430
0.72390133
0.72471249
0.72583705
0.72675180
0.72819990
0.72899526
0.73019910
0.73095065
0.73187029
0.73261619
INFO - Training [36][   60/  196]   Loss 0.486056   Top1 84.440104   Top5 98.339844   BatchTime 0.391194   LR 0.001175
0.73341560
0.73404402
0.73435473
0.73437184
0.73439032
0.73392308
0.73288393
0.73214209
0.73142660
0.73078305
0.73018312
0.72951865
0.72924215
0.72879374
0.72848088
0.72793692
0.72717172
0.72656667
0.72643989
0.72594142
0.72536504
0.72525430
0.72496200
INFO - Training [36][   80/  196]   Loss 0.485600   Top1 84.516602   Top5 98.461914   BatchTime 0.381708   LR 0.001173
0.72486150
0.72465831
0.72429633
0.72380352
0.72346407
0.72322613
0.72279716
0.72252071
0.72213680
0.72202122
0.72164792
0.72112846
0.72050756
0.72004813
0.71946418
0.71840930
0.71723783
0.71647209
0.71596408
0.71536702
0.71510047
INFO - Training [36][  100/  196]   Loss 0.480926   Top1 84.636719   Top5 98.464844   BatchTime 0.380590   LR 0.001170
0.71511406
0.71543890
0.71575820
0.71820891
0.71835721
0.71902210
0.71950740
0.71965152
0.71970010
0.71992284
0.72025567
0.72029001
0.72054380
0.71994364
0.71947837
0.71912813
0.71903270
INFO - Training [36][  120/  196]   Loss 0.473593   Top1 84.899089   Top5 98.548177   BatchTime 0.378644   LR 0.001168
0.71923512
0.71913439
0.71909469
0.71907270
0.71893609
0.71881825
0.71877676
0.71867549
0.71890694
0.71939391
0.71981889
0.72033006
0.72081351
0.72127622
0.72147506
0.72174412
0.72207385
0.72231406
0.72290051
0.72329289
0.72366887
0.72401863
INFO - Training [36][  140/  196]   Loss 0.471595   Top1 84.919085   Top5 98.596540   BatchTime 0.375557   LR 0.001165
0.72628504
0.72636503
0.72627878
0.72627705
0.72618651
0.72638196
0.72671670
0.72687876
0.72695392
0.72715622
0.72711694
0.72721535
0.72699940
0.72665596
0.72629166
0.72593081
0.72578228
0.72566849
0.72529685
0.72488040
0.72458512
0.72422093
INFO - Training [36][  160/  196]   Loss 0.477291   Top1 84.770508   Top5 98.559570   BatchTime 0.373784   LR 0.001163
0.72412783
0.72379023
0.72326827
0.72275722
0.72220224
0.72166443
0.72133976
0.72099239
0.72061372
0.72036809
0.72009003
0.71972609
0.71955299
0.71944249
0.71939796
0.71928090
INFO - Training [36][  180/  196]   Loss 0.476106   Top1 84.778646   Top5 98.500434   BatchTime 0.373646   LR 0.001160
0.71915299
0.71879482
0.71848351
0.71833712
0.71811736
0.71774089
0.71745509
0.71714044
0.71668857
0.71634924
0.71594298
0.71555257
0.71530783
0.71500677
0.71477264
0.71461338
********************pre-trained*****************
INFO - ==> Top1: 84.734    Top5: 98.510    Loss: 0.476
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 0.469235   Top1 84.531250   Top5 99.101562   BatchTime 0.152315
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.4629)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.0463)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0906)
features.3.conv.0 tensor(0.0382)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.1578)
features.4.conv.0 tensor(0.0636)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.1037)
features.5.conv.0 tensor(0.0763)
features.5.conv.3 tensor(0.0926)
features.5.conv.6 tensor(0.1424)
features.6.conv.0 tensor(0.0314)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0636)
features.7.conv.0 tensor(0.0791)
features.7.conv.3 tensor(0.1105)
features.7.conv.6 tensor(0.2108)
features.8.conv.0 tensor(0.1049)
features.8.conv.3 tensor(0.1016)
features.8.conv.6 tensor(0.1294)
features.9.conv.0 tensor(0.1112)
features.9.conv.3 tensor(0.1675)
features.9.conv.6 tensor(0.3775)
features.10.conv.0 tensor(0.0586)
features.10.conv.3 tensor(0.0877)
features.10.conv.6 tensor(0.0817)
features.11.conv.0 tensor(0.5993)
features.11.conv.3 tensor(0.1393)
features.11.conv.6 tensor(0.7023)
features.12.conv.0 tensor(0.6972)
features.12.conv.3 tensor(0.1134)
features.12.conv.6 tensor(0.8009)
features.13.conv.0 tensor(0.1301)
features.13.conv.3 tensor(0.1400)
features.13.conv.6 tensor(0.4052)
features.14.conv.0 tensor(0.9783)
features.14.conv.3 tensor(0.0957)
features.14.conv.6 tensor(0.9796)
features.15.conv.0 tensor(0.9709)
features.15.conv.3 tensor(0.0661)
features.15.conv.6 tensor(0.4979)
features.16.conv.0 tensor(0.1790)
features.16.conv.3 tensor(0.1111)
features.16.conv.6 tensor(0.5780)
conv.0 tensor(0.5242)
tensor(1186698.) 2188896.0
INFO - Validation [36][   40/   40]   Loss 0.471515   Top1 84.380000   Top5 99.280000   BatchTime 0.109365
INFO - ==> Top1: 84.380    Top5: 99.280    Loss: 0.472
INFO - ==> Sparsity : 0.542
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 87.970   Top5: 99.510]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 87.360   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
0.71409154
0.71343416
0.71310127
0.71298206
0.71303141
0.71359783
0.71903622
0.71896559
0.71903479
0.71882659
0.71834701
0.71797329
0.71743011
0.71674401
0.71654117
0.71629769
0.71579915
0.71547699
0.71526515
0.71530902
0.71533740
0.71493262
0.71445930
0.71446073
INFO - Training [37][   20/  196]   Loss 0.502388   Top1 83.652344   Top5 97.968750   BatchTime 0.349067   LR 0.001155
0.71422219
0.71405339
0.71376997
0.71344161
0.71330601
0.71302277
0.71277910
0.71213478
0.71129543
0.71067631
0.71027690
0.71030629
0.71023679
0.70997697
0.70981365
0.70977706
0.70976728
INFO - Training [37][   40/  196]   Loss 0.500438   Top1 84.033203   Top5 98.173828   BatchTime 0.339798   LR 0.001153
0.70934117
0.70855600
0.70849144
0.70724195
0.70579922
0.70503187
0.70499480
0.70470953
0.70440501
0.70471281
0.70504630
0.70625561
0.70783919
0.70926052
0.71063614
0.71218926
0.71370339
0.71528906
0.71710801
0.71891862
0.72090870
0.72373939
INFO - Training [37][   60/  196]   Loss 0.488819   Top1 84.329427   Top5 98.287760   BatchTime 0.349204   LR 0.001150
0.73198050
0.73194975
0.73205376
0.73195732
0.73158586
0.73107940
0.73075306
0.73089373
0.73099864
0.73106372
0.73108810
0.73112476
0.73140520
0.73180997
0.73393971
0.73408490
0.73413676
0.73411870
0.73422521
0.73417294
0.73410165
INFO - Training [37][   80/  196]   Loss 0.488206   Top1 84.296875   Top5 98.417969   BatchTime 0.356778   LR 0.001147
0.73405951
0.73403704
0.73408961
0.73407322
0.73393035
0.73379439
0.73367584
0.73351210
0.73336017
0.73319054
0.73311311
0.73311955
0.73302847
0.73301291
0.73294109
0.73290390
0.73289055
INFO - Training [37][  100/  196]   Loss 0.480269   Top1 84.503906   Top5 98.484375   BatchTime 0.356785   LR 0.001144
0.73295897
0.73299605
0.73296213
0.73293328
0.73286551
0.73271054
0.73255569
0.73242372
0.73234177
0.73220825
0.73204589
0.73208374
0.73391205
0.73442835
0.73449999
0.73461801
0.73462260
0.73473233
0.73487240
0.73495466
0.73504251
INFO - Training [37][  120/  196]   Loss 0.475041   Top1 84.661458   Top5 98.548177   BatchTime 0.360890   LR 0.001142
0.73517841
0.73518080
0.73525149
0.73529780
0.73538834
0.73540014
0.73544830
0.73545110
0.73538345
0.73529178
0.73516995
0.73493224
0.73477179
0.73455030
0.73426014
0.73386461
0.73345459
0.73289031
0.73247266
0.73198217
INFO - Training [37][  140/  196]   Loss 0.472019   Top1 84.807478   Top5 98.602121   BatchTime 0.364109   LR 0.001139
0.73163807
0.73126167
0.73093235
0.73069513
0.73038077
0.73010468
0.72981536
0.72947896
0.72919613
0.72897351
0.72886831
0.72887576
0.72877210
0.72864079
0.72851449
0.72830707
0.72805774
0.72786838
INFO - Training [37][  160/  196]   Loss 0.473950   Top1 84.719238   Top5 98.627930   BatchTime 0.362863   LR 0.001136
0.72772127
0.72751385
0.72732115
0.72718048
0.72704649
0.72700942
0.72702622
0.72680068
0.72709870
0.72722167
0.72730249
0.72724557
0.72714233
0.72657228
0.72590071
0.72564262
0.72548848
0.72534406
0.72514582
0.72488153
0.72473520
INFO - Training [37][  180/  196]   Loss 0.475921   Top1 84.674479   Top5 98.565538   BatchTime 0.364207   LR 0.001133
0.72452611
0.72446579
0.72441727
0.72430092
0.72424054
0.72421569
0.72409016
0.72406965
0.72401673
0.72399026
0.72400242
0.72382635
0.72373331
0.72357607
0.72343791
********************pre-trained*****************
INFO - ==> Top1: 84.752    Top5: 98.600    Loss: 0.472
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.357450   Top1 88.437500   Top5 99.531250   BatchTime 0.137259
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.4844)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.0431)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0897)
features.3.conv.0 tensor(0.0408)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.1528)
features.4.conv.0 tensor(0.0664)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0636)
features.5.conv.0 tensor(0.1061)
features.5.conv.3 tensor(0.0874)
features.5.conv.6 tensor(0.1997)
features.6.conv.0 tensor(0.0317)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0636)
features.7.conv.0 tensor(0.0831)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.2250)
features.8.conv.0 tensor(0.1028)
features.8.conv.3 tensor(0.1001)
features.8.conv.6 tensor(0.2547)
features.9.conv.0 tensor(0.1160)
features.9.conv.3 tensor(0.1655)
features.9.conv.6 tensor(0.3640)
features.10.conv.0 tensor(0.0496)
features.10.conv.3 tensor(0.0874)
features.10.conv.6 tensor(0.1071)
features.11.conv.0 tensor(0.6255)
features.11.conv.3 tensor(0.1410)
features.11.conv.6 tensor(0.6854)
features.12.conv.0 tensor(0.6418)
features.12.conv.3 tensor(0.1138)
features.12.conv.6 tensor(0.8100)
features.13.conv.0 tensor(0.1445)
features.13.conv.3 tensor(0.1373)
features.13.conv.6 tensor(0.4118)
features.14.conv.0 tensor(0.9807)
features.14.conv.3 tensor(0.0953)
features.14.conv.6 tensor(0.9792)
features.15.conv.0 tensor(0.9722)
features.15.conv.3 tensor(0.0638)
features.15.conv.6 tensor(0.4854)
features.16.conv.0 tensor(0.1919)
features.16.conv.3 tensor(0.1128)
features.16.conv.6 tensor(0.3969)
conv.0 tensor(0.5289)
tensor(1137176.) 2188896.0
INFO - Validation [37][   40/   40]   Loss 0.344319   Top1 88.610000   Top5 99.660000   BatchTime 0.094449
INFO - ==> Top1: 88.610    Top5: 99.660    Loss: 0.344
INFO - ==> Sparsity : 0.520
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 87.970   Top5: 99.510]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
0.72324246
0.72295469
0.72264284
0.72234875
0.72209573
0.72188312
0.72167242
0.72144663
0.72117847
0.72095871
0.72097993
0.72257435
0.72239381
0.72206777
0.72198033
0.72192073
0.72201705
0.72203100
0.72205836
0.72206455
0.72213113
0.72218651
INFO - Training [38][   20/  196]   Loss 0.476278   Top1 85.078125   Top5 98.183594   BatchTime 0.388785   LR 0.001128
0.72225213
0.72213358
0.72196537
0.72174770
0.72149163
0.72111309
0.72054833
0.72015214
0.71974713
0.71944946
0.71934730
0.71929014
0.71924049
0.71913272
0.71889901
0.71853584
0.71824795
0.71804988
0.71772003
INFO - Training [38][   40/  196]   Loss 0.481140   Top1 84.667969   Top5 98.398438   BatchTime 0.344668   LR 0.001125
0.71741062
0.71718854
0.71696335
0.71692806
0.71670389
0.71630019
0.71588069
0.71554410
0.71517235
0.71457297
0.71396631
0.71336067
0.71288568
0.71255201
0.71231717
0.71185482
0.71072346
0.70802385
0.70385218
INFO - Training [38][   60/  196]   Loss 0.479355   Top1 84.680990   Top5 98.489583   BatchTime 0.370374   LR 0.001122
0.69860435
0.69480747
0.69162929
0.69334972
0.69383317
0.69192809
0.68948954
0.68657124
0.68808246
0.68903214
0.68940133
0.69074506
0.69197047
0.69372851
0.69464105
0.69579089
0.69699627
0.69818991
0.69918758
0.69984269
0.70084620
0.70704532
INFO - Training [38][   80/  196]   Loss 0.478294   Top1 84.599609   Top5 98.500977   BatchTime 0.371144   LR 0.001119
0.70689529
0.70738590
0.70758015
0.70761895
0.70756882
0.70736235
0.70713967
0.70705104
0.70698673
0.70687765
0.70668638
0.70644683
0.70610464
0.70575035
0.70548028
0.70520073
0.70474041
0.70421106
0.70338094
0.70320028
0.70329362
INFO - Training [38][  100/  196]   Loss 0.470849   Top1 84.781250   Top5 98.578125   BatchTime 0.371797   LR 0.001116
0.70389986
0.70422739
0.70466524
0.70502114
0.70544600
0.70564312
0.70601821
0.70658356
0.70707411
0.70732605
0.70744377
0.70735073
0.70718575
0.70693970
0.70662224
0.70628792
0.70602483
INFO - Training [38][  120/  196]   Loss 0.465521   Top1 84.980469   Top5 98.665365   BatchTime 0.370517   LR 0.001112
0.70560551
0.70560241
0.70598686
0.70596141
0.70603180
0.70596105
0.70581514
0.70653433
0.70892131
0.70927733
0.70976478
0.71016985
0.71048462
0.71077907
0.71095014
0.71116829
0.71132952
0.71173108
0.71211737
0.71236092
0.71237797
INFO - Training [38][  140/  196]   Loss 0.464308   Top1 85.050223   Top5 98.713728   BatchTime 0.372243   LR 0.001109
0.71256930
0.71236509
0.71210438
0.71182781
0.71159166
0.71145421
0.71155864
0.71167141
0.71247226
0.71352690
0.71415085
0.71467298
0.71507257
0.71500903
0.71481192
0.71477997
0.71460712
0.71452028
0.71443659
0.71430463
0.71420848
0.71423268
INFO - Training [38][  160/  196]   Loss 0.468305   Top1 84.943848   Top5 98.691406   BatchTime 0.370885   LR 0.001106
0.71429020
0.71411526
0.71388507
0.71377891
0.71378303
0.71382457
0.71394068
0.71413952
0.71452653
0.71498787
0.71535450
0.71584088
0.71643388
0.71702754
0.71768659
0.71833670
0.71892321
INFO - Training [38][  180/  196]   Loss 0.466096   Top1 85.047743   Top5 98.687066   BatchTime 0.370026   LR 0.001103
0.71934378
0.71967459
0.71992987
0.71987098
0.71980995
0.71973377
0.71961212
0.71924973
0.71895939
0.71887273
0.71874607
0.71841598
0.71816874
0.71803224
0.71788865
0.71766287
INFO - ==> Top1: 85.150    Top5: 98.686    Loss: 0.463
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [38][   20/   40]   Loss 0.384577   Top1 87.714844   Top5 99.570312   BatchTime 0.133089
INFO - Validation [38][   40/   40]   Loss 0.377989   Top1 87.650000   Top5 99.580000   BatchTime 0.093551
INFO - ==> Top1: 87.650    Top5: 99.580    Loss: 0.378
INFO - ==> Sparsity : 0.531
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 87.970   Top5: 99.510]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0358)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0707)
features.2.conv.0 tensor(0.0457)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0888)
features.3.conv.0 tensor(0.0388)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.1382)
features.4.conv.0 tensor(0.0601)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0949)
features.5.conv.0 tensor(0.0596)
features.5.conv.3 tensor(0.0851)
features.5.conv.6 tensor(0.1785)
features.6.conv.0 tensor(0.0308)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0637)
features.7.conv.0 tensor(0.0776)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.1148)
features.8.conv.0 tensor(0.0887)
features.8.conv.3 tensor(0.1021)
features.8.conv.6 tensor(0.2705)
features.9.conv.0 tensor(0.1246)
features.9.conv.3 tensor(0.1687)
features.9.conv.6 tensor(0.3521)
features.10.conv.0 tensor(0.0525)
features.10.conv.3 tensor(0.0862)
features.10.conv.6 tensor(0.0821)
features.11.conv.0 tensor(0.5903)
features.11.conv.3 tensor(0.1391)
features.11.conv.6 tensor(0.6976)
features.12.conv.0 tensor(0.6522)
features.12.conv.3 tensor(0.1136)
features.12.conv.6 tensor(0.8030)
features.13.conv.0 tensor(0.2156)
features.13.conv.3 tensor(0.1414)
features.13.conv.6 tensor(0.4322)
features.14.conv.0 tensor(0.9809)
features.14.conv.3 tensor(0.0969)
features.14.conv.6 tensor(0.9780)
features.15.conv.0 tensor(0.9727)
features.15.conv.3 tensor(0.0654)
features.15.conv.6 tensor(0.4845)
features.16.conv.0 tensor(0.1990)
features.16.conv.3 tensor(0.1109)
features.16.conv.6 tensor(0.4523)
conv.0 tensor(0.5464)
tensor(1162918.) 2188896.0
0.71725297
0.71707177
0.71683770
0.71634537
0.71625310
0.71610421
0.71615565
0.71600592
0.71588129
0.71576947
0.71558446
0.71554905
0.71539623
0.71508843
0.71480900
0.71453720
0.71418077
0.71386820
0.71354598
0.71306509
0.71241874
0.71171975
0.71110582
0.71002769
INFO - Training [39][   20/  196]   Loss 0.493443   Top1 83.925781   Top5 98.125000   BatchTime 0.427843   LR 0.001097
0.70905733
0.70841306
0.70790190
0.70728278
0.70659977
0.70597142
0.70540214
0.70483762
0.70416576
0.70354497
0.70298815
0.70282334
0.70240718
0.70207459
0.70188165
0.70176458
0.70164424
0.70124120
0.70099807
INFO - Training [39][   40/  196]   Loss 0.486264   Top1 84.042969   Top5 98.330078   BatchTime 0.369376   LR 0.001094
0.70072258
0.70043999
0.69981140
0.69897872
0.69817162
0.69763935
0.69740921
0.69724935
0.69722742
0.69741642
0.69744235
0.69773567
0.69782323
0.69896561
0.70001507
0.69976741
0.69980860
0.69964558
0.69938332
0.69907075
INFO - Training [39][   60/  196]   Loss 0.476737   Top1 84.251302   Top5 98.333333   BatchTime 0.347217   LR 0.001090
0.69881862
0.69865876
0.69846010
0.69833237
0.69837242
0.69836187
0.69832182
0.69813341
0.69785130
0.69759125
0.69709337
0.69634706
0.69561410
0.69484258
0.69345576
0.69246304
0.69160420
INFO - Training [39][   80/  196]   Loss 0.476968   Top1 84.365234   Top5 98.447266   BatchTime 0.348498   LR 0.001087
0.69114935
0.69124025
0.69188726
0.69257885
0.69310725
0.69360107
0.69433182
0.69440067
0.69422740
0.69478530
0.69651079
0.69835693
0.69974774
0.70161456
0.70438766
0.70550847
0.70690143
0.70845693
0.71031922
0.71438122
INFO - Training [39][  100/  196]   Loss 0.467620   Top1 84.769531   Top5 98.546875   BatchTime 0.356279   LR 0.001084
0.72135085
0.72160250
0.72176552
0.72182441
0.72191566
0.72194743
0.72206587
0.72224283
0.72250736
0.72445065
0.72541773
0.72535598
0.72515875
0.72502333
0.72489393
0.72488642
0.72481877
0.72478551
0.72475564
0.72477067
INFO - Training [39][  120/  196]   Loss 0.460172   Top1 85.045573   Top5 98.623047   BatchTime 0.364621   LR 0.001080
0.72471350
0.72474289
0.72481942
0.72499204
0.72702104
0.73133361
0.73153245
0.73145670
0.73125643
0.73115706
0.73110694
0.73121041
0.73114878
0.73115075
0.73108304
0.73109072
0.73110902
0.73117167
0.73123294
0.73128104
0.73132873
0.73132068
INFO - Training [39][  140/  196]   Loss 0.456347   Top1 85.159040   Top5 98.699777   BatchTime 0.364125   LR 0.001077
0.73126155
0.73114115
0.73093367
0.73082119
0.73076934
0.73074931
0.73078382
0.73074400
0.73067480
0.73072374
0.73070920
0.73077458
0.73066616
0.73069054
0.73061115
0.73052227
0.73048347
0.73048466
0.73048097
0.73044950
0.73047829
0.73041707
INFO - Training [39][  160/  196]   Loss 0.458942   Top1 85.100098   Top5 98.718262   BatchTime 0.364050   LR 0.001073
0.73040390
0.73032022
0.73041517
0.73059779
0.73086458
0.73113251
0.73127073
0.73143327
0.73156381
0.73162478
0.73165309
0.73169786
0.73162842
0.73147851
0.73105198
0.73100597
INFO - Training [39][  180/  196]   Loss 0.457984   Top1 85.130208   Top5 98.704427   BatchTime 0.364805   LR 0.001070
0.73078197
0.73064190
0.73047405
0.73036128
0.73022819
0.73009926
0.72992283
0.72970480
0.72955769
0.72935981
0.72923499
0.72910553
0.72894824
0.72882742
0.72878939
0.72865391
INFO - ==> Top1: 85.214    Top5: 98.710    Loss: 0.456
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [39][   20/   40]   Loss 0.418066   Top1 86.171875   Top5 99.160156   BatchTime 0.137902
INFO - Validation [39][   40/   40]   Loss 0.401768   Top1 86.440000   Top5 99.340000   BatchTime 0.096498
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0764)
features.2.conv.0 tensor(0.0475)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0874)
features.3.conv.0 tensor(0.0373)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0946)
features.4.conv.0 tensor(0.0658)
features.4.conv.3 tensor(0.0874)
features.4.conv.6 tensor(0.0955)
features.5.conv.0 tensor(0.0522)
features.5.conv.3 tensor(0.0862)
features.5.conv.6 tensor(0.1055)
features.6.conv.0 tensor(0.0321)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0595)
features.7.conv.0 tensor(0.0844)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.2163)
features.8.conv.0 tensor(0.0925)
features.8.conv.3 tensor(0.1024)
features.8.conv.6 tensor(0.1178)
features.9.conv.0 tensor(0.1257)
features.9.conv.3 tensor(0.1701)
features.9.conv.6 tensor(0.3832)
features.10.conv.0 tensor(0.0499)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.1298)
features.11.conv.0 tensor(0.6502)
features.11.conv.3 tensor(0.1368)
features.11.conv.6 tensor(0.6687)
features.12.conv.0 tensor(0.6345)
features.12.conv.3 tensor(0.1101)
features.12.conv.6 tensor(0.8103)
features.13.conv.0 tensor(0.1959)
features.13.conv.3 tensor(0.1437)
features.13.conv.6 tensor(0.4250)
features.14.conv.0 tensor(0.9815)
features.14.conv.3 tensor(0.0948)
features.14.conv.6 tensor(0.9758)
features.15.conv.0 tensor(0.9732)
features.15.conv.3 tensor(0.0639)
features.15.conv.6 tensor(0.4893)
features.16.conv.0 tensor(0.2085)
features.16.conv.3 tensor(0.1127)
features.16.conv.6 tensor(0.4129)
conv.0 tensor(0.5129)
tensor(1139254.) 2188896.0
INFO - ==> Top1: 86.440    Top5: 99.340    Loss: 0.402
INFO - ==> Sparsity : 0.520
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 87.970   Top5: 99.510]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
0.72859913
0.72861260
0.72846526
0.72844303
0.72826284
0.72819632
0.72811180
0.72814184
0.72815645
0.72816110
0.72808248
0.72800994
0.72796762
0.72799671
0.72797787
0.72817868
0.72843003
0.72976601
0.73106050
0.73095244
0.73103094
INFO - Training [40][   20/  196]   Loss 0.464695   Top1 84.746094   Top5 98.378906   BatchTime 0.459263   LR 0.001064
0.73114425
0.73110610
0.73102748
0.73099810
0.73105061
0.73107094
0.73104346
0.73104399
0.73100656
0.73097688
0.73094487
0.73092067
0.73103106
0.73107195
0.73105228
0.73110688
0.73115468
0.73109311
0.73100036
INFO - Training [40][   40/  196]   Loss 0.459996   Top1 85.283203   Top5 98.593750   BatchTime 0.388137   LR 0.001060
0.73077315
0.73066956
0.73051888
0.73037034
0.73032099
0.73026353
0.73020810
0.73013264
0.72998250
0.72986513
0.72969592
0.72939104
0.72914439
0.72883588
0.72849864
0.72818869
0.72773165
0.72737080
0.72706705
0.72663486
0.72629100
0.72591054
0.72542149
0.72481996
0.72407925
INFO - Training [40][   60/  196]   Loss 0.461471   Top1 85.084635   Top5 98.723958   BatchTime 0.370633   LR 0.001056
0.72325540
0.72283751
0.72233063
0.72171736
0.72122204
0.72066975
0.72028786
0.71998805
0.71966577
0.71916533
0.71859676
0.71815354
0.71763062
0.71712011
0.71668613
0.71624982
INFO - Training [40][   80/  196]   Loss 0.462296   Top1 85.078125   Top5 98.769531   BatchTime 0.366296   LR 0.001053
0.71586591
0.71544600
0.71497160
0.71455014
0.71416205
0.71394920
0.71367341
0.71336335
0.71316749
0.71290064
0.71249950
0.71211171
0.71175343
0.71149343
0.71125031
0.71105385
0.71076810
0.71030766
0.70979995
0.70919734
0.70859295
INFO - Training [40][  100/  196]   Loss 0.455893   Top1 85.320312   Top5 98.789062   BatchTime 0.368999   LR 0.001049
0.70804721
0.70760244
0.70699567
0.70641422
0.70590550
0.70547587
0.70508111
0.70492792
0.70473719
0.70453793
0.70430905
0.70401055
0.70378762
0.70359659
0.70351321
0.70339483
0.70330465
0.70310998
0.70283073
0.70256478
INFO - Training [40][  120/  196]   Loss 0.451010   Top1 85.468750   Top5 98.857422   BatchTime 0.373490   LR 0.001045
0.70235020
0.70199353
0.70142186
0.70101994
0.70062101
0.70016593
0.69968128
0.69920295
0.69884402
0.69859356
0.69823337
0.69790477
0.69768190
0.69732052
0.69704670
0.69652873
0.69608510
0.69565248
0.69528401
0.69522858
0.69523859
0.69520843
0.69518828
INFO - Training [40][  140/  196]   Loss 0.447429   Top1 85.597098   Top5 98.900670   BatchTime 0.370839   LR 0.001042
0.69525284
0.69566190
0.69596237
0.69621712
0.69617546
0.69633156
0.69665527
0.69707853
0.69740170
0.69756341
0.69756472
0.69761479
0.69792539
0.69814324
0.69841719
0.69848245
0.69871193
INFO - Training [40][  160/  196]   Loss 0.449883   Top1 85.500488   Top5 98.840332   BatchTime 0.368559   LR 0.001038
0.69895238
0.69920081
0.69958472
0.69991010
0.70004767
0.70030487
0.70069349
0.70102257
0.70136392
0.70164353
0.70174891
0.70203328
0.70210218
0.70227987
0.70246661
0.70263660
0.70285761
0.70307881
0.70352513
0.70401770
0.70435870
INFO - Training [40][  180/  196]   Loss 0.448319   Top1 85.540365   Top5 98.812934   BatchTime 0.369619   LR 0.001034
0.70450401
0.70639318
0.70664328
0.70663697
0.70657694
0.70647800
0.70628816
0.70611274
0.70605546
0.70598656
0.70602030
0.70598060
0.70598352
********************pre-trained*****************
INFO - ==> Top1: 85.576    Top5: 98.822    Loss: 0.447
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.368785   Top1 88.027344   Top5 99.511719   BatchTime 0.134434
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.4629)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0451)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0851)
features.3.conv.0 tensor(0.0379)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0903)
features.4.conv.0 tensor(0.0654)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.1294)
features.5.conv.0 tensor(0.0545)
features.5.conv.3 tensor(0.0839)
features.5.conv.6 tensor(0.1126)
features.6.conv.0 tensor(0.0306)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0641)
features.7.conv.0 tensor(0.0877)
features.7.conv.3 tensor(0.1117)
features.7.conv.6 tensor(0.2259)
features.8.conv.0 tensor(0.1050)
features.8.conv.3 tensor(0.1045)
features.8.conv.6 tensor(0.2649)
features.9.conv.0 tensor(0.1359)
features.9.conv.3 tensor(0.1701)
features.9.conv.6 tensor(0.3758)
features.10.conv.0 tensor(0.0645)
features.10.conv.3 tensor(0.0839)
features.10.conv.6 tensor(0.0843)
features.11.conv.0 tensor(0.6223)
features.11.conv.3 tensor(0.1391)
features.11.conv.6 tensor(0.6992)
features.12.conv.0 tensor(0.6576)
features.12.conv.3 tensor(0.1117)
features.12.conv.6 tensor(0.8249)
features.13.conv.0 tensor(0.2088)
features.13.conv.3 tensor(0.1412)
features.13.conv.6 tensor(0.4300)
features.14.conv.0 tensor(0.9821)
features.14.conv.3 tensor(0.0971)
features.14.conv.6 tensor(0.9804)
features.15.conv.0 tensor(0.9742)
features.15.conv.3 tensor(0.0641)
features.15.conv.6 tensor(0.4952)
features.16.conv.0 tensor(0.2203)
features.16.conv.3 tensor(0.1113)
features.16.conv.6 tensor(0.5280)
conv.0 tensor(0.5439)
tensor(1197650.) 2188896.0
INFO - Validation [40][   40/   40]   Loss 0.350648   Top1 88.170000   Top5 99.610000   BatchTime 0.094789
INFO - ==> Top1: 88.170    Top5: 99.610    Loss: 0.351
INFO - ==> Sparsity : 0.547
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 88.170   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
0.70629168
0.70651293
0.70668566
0.70693678
0.70704991
0.70705169
0.70694697
0.70703012
0.70806319
0.71047205
0.71259373
0.71238101
0.71199584
0.71165818
0.71119934
0.71078455
0.71054536
0.71056575
0.71064031
INFO - Training [41][   20/  196]   Loss 0.445477   Top1 85.507812   Top5 98.359375   BatchTime 0.455351   LR 0.001027
0.71058953
0.71065354
0.71075213
0.71068788
0.71066910
0.71059936
0.71063447
0.71059185
0.71193177
0.71268159
0.71263570
0.71267152
0.71234614
0.71190035
0.71131641
0.71066785
0.71028399
0.70991379
0.70954949
0.70924616
INFO - Training [41][   40/  196]   Loss 0.442169   Top1 85.849609   Top5 98.398438   BatchTime 0.430048   LR 0.001023
0.70895475
0.70868659
0.70819485
0.70782286
0.70747578
0.70689940
0.70644444
0.70647383
0.70609629
0.70569855
0.70524174
0.70488012
0.70454818
0.70418042
0.70414871
0.70402664
0.70361906
0.70330405
0.70272213
0.70204848
0.70171583
0.70171052
0.70157647
0.70130736
INFO - Training [41][   60/  196]   Loss 0.447832   Top1 85.611979   Top5 98.391927   BatchTime 0.397335   LR 0.001020
0.70106381
0.70087558
0.70078188
0.70070022
0.70057702
0.70122677
0.70187908
0.70254230
0.70313412
0.70351195
0.70372456
0.70400000
0.70433050
0.70457011
0.70494437
INFO - Training [41][   80/  196]   Loss 0.445335   Top1 85.634766   Top5 98.530273   BatchTime 0.393094   LR 0.001016
0.70539713
0.70589238
0.70624399
0.70682561
0.70741409
0.70791054
0.70837635
0.70891464
0.70939267
0.70973617
0.71007639
0.71029633
0.71052486
0.71066421
0.71094340
0.71103209
0.71107316
0.71110904
0.71100575
0.71067768
0.71049601
0.71055168
INFO - Training [41][  100/  196]   Loss 0.439527   Top1 85.796875   Top5 98.570312   BatchTime 0.390117   LR 0.001012
0.71060985
0.71067131
0.71077967
0.71078146
0.71087837
0.71107978
0.71111417
0.71111995
0.71093988
0.71071583
0.71066141
0.71040225
0.71013731
0.70999002
0.70983416
0.70969027
0.70943058
0.70893842
0.70841742
0.70765215
0.70726633
INFO - Training [41][  120/  196]   Loss 0.438982   Top1 85.810547   Top5 98.636068   BatchTime 0.386034   LR 0.001008
0.70700657
0.70689052
0.70666659
0.70657414
0.70633888
0.70615476
0.70590514
0.70582056
0.70551717
0.70527852
0.70492685
0.70462191
0.70446318
0.70468950
0.70495975
0.70496631
0.70518041
INFO - Training [41][  140/  196]   Loss 0.437974   Top1 85.792411   Top5 98.691406   BatchTime 0.381974   LR 0.001004
0.70518833
0.70532012
0.70562929
0.70761949
0.70771128
0.70728219
0.70689231
0.70649946
0.70618248
0.70596462
0.70541090
0.71651930
0.71680051
0.71663171
0.71658123
0.71654522
0.71633136
0.71619111
0.71598828
0.71586710
0.71570218
0.71560812
INFO - Training [41][  160/  196]   Loss 0.440915   Top1 85.725098   Top5 98.723145   BatchTime 0.379790   LR 0.001000
0.71567094
0.71552616
0.71538228
0.71537799
0.71521658
0.71507096
0.71488535
0.71442848
0.71394151
0.71345341
0.71323860
0.71293116
0.71249074
0.71203363
0.71146756
0.71111637
0.71069086
0.71025467
0.70998400
0.70984572
0.70972025
INFO - Training [41][  180/  196]   Loss 0.441212   Top1 85.683594   Top5 98.697917   BatchTime 0.379257   LR 0.000996
0.70984203
0.70964676
0.70967251
0.70977783
0.70972186
0.70971429
0.70978683
0.70984000
0.71002746
0.71013391
0.71041566
0.71049708
0.71052754
0.71048087
0.71062136
********************pre-trained*****************
INFO - ==> Top1: 85.722    Top5: 98.712    Loss: 0.441
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [41][   20/   40]   Loss 0.428238   Top1 86.621094   Top5 99.296875   BatchTime 0.133451
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.4746)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0764)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0816)
features.3.conv.0 tensor(0.0353)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0849)
features.4.conv.0 tensor(0.0697)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.0539)
features.5.conv.0 tensor(0.0490)
features.5.conv.3 tensor(0.0833)
features.5.conv.6 tensor(0.1396)
features.6.conv.0 tensor(0.0345)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0610)
features.7.conv.0 tensor(0.0962)
features.7.conv.3 tensor(0.1108)
features.7.conv.6 tensor(0.2591)
features.8.conv.0 tensor(0.0999)
features.8.conv.3 tensor(0.1004)
features.8.conv.6 tensor(0.3324)
features.9.conv.0 tensor(0.1296)
features.9.conv.3 tensor(0.1704)
features.9.conv.6 tensor(0.3830)
features.10.conv.0 tensor(0.0592)
features.10.conv.3 tensor(0.0880)
features.10.conv.6 tensor(0.0816)
features.11.conv.0 tensor(0.6174)
features.11.conv.3 tensor(0.1393)
features.11.conv.6 tensor(0.7177)
features.12.conv.0 tensor(0.7033)
features.12.conv.3 tensor(0.1127)
features.12.conv.6 tensor(0.8218)
features.13.conv.0 tensor(0.1973)
features.13.conv.3 tensor(0.1402)
features.13.conv.6 tensor(0.1124)
features.14.conv.0 tensor(0.9825)
features.14.conv.3 tensor(0.0951)
features.14.conv.6 tensor(0.9818)
features.15.conv.0 tensor(0.9745)
features.15.conv.3 tensor(0.0655)
features.15.conv.6 tensor(0.5143)
features.16.conv.0 tensor(0.2282)
features.16.conv.3 tensor(0.1130)
features.16.conv.6 tensor(0.5927)
conv.0 tensor(0.5367)
tensor(1194234.) 2188896.0
INFO - Validation [41][   40/   40]   Loss 0.409724   Top1 86.900000   Top5 99.380000   BatchTime 0.092712
INFO - ==> Top1: 86.900    Top5: 99.380    Loss: 0.410
INFO - ==> Sparsity : 0.546
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 88.170   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
0.71095514
0.71113306
0.71139908
0.71146530
0.71156281
0.71150619
0.71143454
0.71133018
0.71118087
0.71092653
0.71083039
0.71082813
0.71087641
0.71079659
0.71101338
0.71107060
0.71120179
0.71130991
0.71143776
INFO - Training [42][   20/  196]   Loss 0.443055   Top1 85.507812   Top5 98.359375   BatchTime 0.420375   LR 0.000988
0.71151459
0.71172833
0.71183383
0.71197575
0.71212411
0.71233726
0.71257436
0.71282142
0.71302944
0.71325827
0.71354306
0.71379101
0.71396840
0.71416920
0.71431714
0.71444649
0.71446949
0.71435028
0.71425742
0.71429580
0.71434212
0.71417314
0.71417552
0.71425718
INFO - Training [42][   40/  196]   Loss 0.446875   Top1 85.546875   Top5 98.466797   BatchTime 0.373843   LR 0.000984
0.71424675
0.71420068
0.71434504
0.71418273
0.71405971
0.71378070
0.71349466
0.71318412
0.71283251
0.71257716
0.71235585
0.71201092
0.71167487
0.71138936
0.71125114
0.71100962
0.71063089
0.71031624
INFO - Training [42][   60/  196]   Loss 0.448563   Top1 85.449219   Top5 98.496094   BatchTime 0.359547   LR 0.000980
0.70997655
0.70959419
0.70929486
0.70901626
0.70896637
0.70890671
0.70875728
0.70876509
0.70872927
0.70860463
0.70854235
0.70834029
0.70823044
0.70813394
0.70803428
0.70810050
0.70810467
0.70810610
INFO - Training [42][   80/  196]   Loss 0.444240   Top1 85.595703   Top5 98.583984   BatchTime 0.357525   LR 0.000976
0.70815164
0.70801175
0.70782155
0.71265256
0.71343607
0.71310997
0.71257997
0.71203381
0.71156830
0.71123952
0.71070266
0.71019632
0.70984215
0.70932436
0.70903844
0.70964533
0.71092582
0.71069962
0.71035820
0.70998716
0.70951980
INFO - Training [42][  100/  196]   Loss 0.435677   Top1 85.894531   Top5 98.656250   BatchTime 0.360074   LR 0.000972
0.70915663
0.70863134
0.70824349
0.70787829
0.70734239
0.70669472
0.70605302
0.70525235
0.70489764
0.70456797
0.70403165
0.70400482
0.70453030
0.70523715
0.70578021
0.70637858
0.70690244
0.70732808
0.70747948
0.70774996
0.71029407
0.71174073
INFO - Training [42][  120/  196]   Loss 0.431773   Top1 86.041667   Top5 98.720703   BatchTime 0.361144   LR 0.000968
0.71348172
0.71392035
0.71389121
0.71490157
0.71835554
0.72027272
0.72058034
0.72058856
0.72054970
0.72042769
0.72038221
0.72025526
0.72026378
0.72027314
0.72051138
0.72065657
0.72069031
0.72081286
0.72094226
0.72113496
0.72113377
INFO - Training [42][  140/  196]   Loss 0.430347   Top1 86.077009   Top5 98.766741   BatchTime 0.364230   LR 0.000964
0.72110295
0.72113925
0.72135329
0.72157359
0.72169417
0.72164750
0.72151661
0.72136253
0.72131997
0.72112554
0.72102433
0.72081715
0.72049958
0.72032154
0.72004282
0.71987951
INFO - Training [42][  160/  196]   Loss 0.436254   Top1 85.830078   Top5 98.715820   BatchTime 0.365687   LR 0.000959
0.71955705
0.71921790
0.71880358
0.71858460
0.71854818
0.71835530
0.71822548
0.71815997
0.71787000
0.71748734
0.71714950
0.71678388
0.71657038
0.71643984
0.71649516
0.71674550
0.71711177
0.71737915
0.71750957
0.71748084
0.71745926
INFO - Training [42][  180/  196]   Loss 0.436289   Top1 85.844184   Top5 98.697917   BatchTime 0.366429   LR 0.000955
0.71739072
0.71705472
0.71696216
0.71701908
0.71703792
0.71713775
0.71723813
0.71707606
0.71705192
0.71688515
0.71672732
0.71660399
0.71645898
0.71630734
0.71619010
0.71609986
INFO - ==> Top1: 85.900    Top5: 98.712    Loss: 0.436
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.4609)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0463)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0813)
features.3.conv.0 tensor(0.0295)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0844)
features.4.conv.0 tensor(0.0638)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.0521)
features.5.conv.0 tensor(0.0516)
features.5.conv.3 tensor(0.0799)
features.5.conv.6 tensor(0.1367)
features.6.conv.0 tensor(0.0299)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0602)
features.7.conv.0 tensor(0.0883)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.1047)
features.8.conv.0 tensor(0.1166)
features.8.conv.3 tensor(0.0969)
features.8.conv.6 tensor(0.3010)
features.9.conv.0 tensor(0.1327)
features.9.conv.3 tensor(0.1672)
features.9.conv.6 tensor(0.4003)
features.10.conv.0 tensor(0.0589)
features.10.conv.3 tensor(0.0848)
features.10.conv.6 tensor(0.1081)
features.11.conv.0 tensor(0.6251)
features.11.conv.3 tensor(0.1395)
features.11.conv.6 tensor(0.7141)
features.12.conv.0 tensor(0.6638)
features.12.conv.3 tensor(0.1134)
features.12.conv.6 tensor(0.8071)
features.13.conv.0 tensor(0.1912)
features.13.conv.3 tensor(0.1400)
features.13.conv.6 tensor(0.1016)
features.14.conv.0 tensor(0.9830)
features.14.conv.3 tensor(0.0929)
features.14.conv.6 tensor(0.9827)
features.15.conv.0 tensor(0.9756)
features.15.conv.3 tensor(0.0648)
features.15.conv.6 tensor(0.5328)
features.16.conv.0 tensor(0.2357)
features.16.conv.3 tensor(0.1126)
features.16.conv.6 tensor(0.6597)
conv.0 tensor(0.5210)
tensor(1205614.) 2188896.0
INFO - Validation [42][   20/   40]   Loss 0.448481   Top1 85.878906   Top5 99.140625   BatchTime 0.133791
INFO - Validation [42][   40/   40]   Loss 0.434177   Top1 86.150000   Top5 99.280000   BatchTime 0.093519
INFO - ==> Top1: 86.150    Top5: 99.280    Loss: 0.434
INFO - ==> Sparsity : 0.551
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 88.170   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
0.71657401
0.71690929
0.71722084
0.71751350
0.71791267
0.71827352
0.71891087
0.71932077
0.71973014
0.72006440
0.72023165
0.72043794
0.72061181
0.72075480
0.72083485
0.72103620
0.72131503
0.72177494
0.72225064
0.72263390
0.72299719
INFO - Training [43][   20/  196]   Loss 0.447014   Top1 85.703125   Top5 98.300781   BatchTime 0.479480   LR 0.000947
0.72366101
0.72440869
0.72497874
0.72565132
0.72630227
0.72653544
0.72938877
0.73192161
0.73310357
0.73282379
0.73263735
0.73251963
0.73231429
0.73207766
0.73178953
0.73162842
0.73155177
0.73135477
0.73119247
0.73106170
0.73088354
0.73080701
INFO - Training [43][   40/  196]   Loss 0.450171   Top1 85.488281   Top5 98.466797   BatchTime 0.420234   LR 0.000943
0.73070103
0.73057038
0.73009831
0.72972172
0.72942042
0.72924411
0.72907025
0.72892123
0.72881031
0.72866577
0.72845525
0.72828692
0.72810662
0.72798103
0.72779524
0.72755027
0.72737759
0.72725636
0.72720206
INFO - Training [43][   60/  196]   Loss 0.444229   Top1 85.657552   Top5 98.476562   BatchTime 0.386886   LR 0.000939
0.72692043
0.72643697
0.72622138
0.72597677
0.72574490
0.72539902
0.72495687
0.72462076
0.72428036
0.72393012
0.72347856
0.72256494
0.72176057
0.72137964
0.72099727
0.72059196
0.72006190
INFO - Training [43][   80/  196]   Loss 0.447550   Top1 85.468750   Top5 98.662109   BatchTime 0.379263   LR 0.000934
0.71909636
0.71808839
0.71753174
0.71712941
0.71713060
0.71704382
0.71663094
0.71639943
0.71652502
0.71641666
0.71626550
0.71592242
0.71562546
0.71577126
0.71581179
0.71597999
0.71626800
0.71624392
0.71632057
0.71631163
0.71615374
INFO - Training [43][  100/  196]   Loss 0.439461   Top1 85.722656   Top5 98.750000   BatchTime 0.378065   LR 0.000930
0.71594387
0.71566987
0.71576566
0.71593678
0.71588624
0.71584195
0.71566373
0.71545368
0.71514076
0.71501982
0.71498746
0.71478891
0.71453637
0.71428865
0.71430129
0.71418470
0.71412635
0.71420175
0.71423024
0.71417743
0.71432787
INFO - Training [43][  120/  196]   Loss 0.434002   Top1 85.901693   Top5 98.824870   BatchTime 0.379441   LR 0.000926
0.71435928
0.71455103
0.71464938
0.71461505
0.71467441
0.71480137
0.71476132
0.71475464
0.71492976
0.71514279
0.71525174
0.71530992
0.71509892
0.71470857
0.71441919
0.71502000
0.71501505
0.71512151
0.72220320
0.72193706
0.72167653
0.72127801
0.72092158
INFO - Training [43][  140/  196]   Loss 0.429585   Top1 86.090960   Top5 98.889509   BatchTime 0.376429   LR 0.000921
0.72051072
0.72008437
0.71957690
0.71899724
0.71862924
0.71824586
0.71786380
0.71740937
0.71680093
0.71618664
0.71572930
0.71509886
0.71469802
0.71430552
0.71397322
0.71371704
INFO - Training [43][  160/  196]   Loss 0.432412   Top1 85.939941   Top5 98.869629   BatchTime 0.374907   LR 0.000917
0.71344525
0.71312177
0.71278930
0.71230179
0.71154082
0.71093345
0.71027642
0.70941311
0.70879209
0.70820045
0.70759988
0.70730972
0.70714796
0.70690727
0.70668966
0.70659429
0.70648199
0.70630938
0.70608366
0.70589215
0.70569474
INFO - Training [43][  180/  196]   Loss 0.432272   Top1 85.917969   Top5 98.828125   BatchTime 0.374772   LR 0.000912
0.70548022
0.70533097
0.70518529
0.70496142
0.70470989
0.70445037
0.70412445
0.70374340
0.70341450
0.70329529
0.70329612
0.70330286
0.70329028
0.70343602
0.70359117
********************pre-trained*****************
INFO - ==> Top1: 86.046    Top5: 98.846    Loss: 0.430
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [43][   20/   40]   Loss 0.345103   Top1 88.964844   Top5 99.550781   BatchTime 0.197890
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.4805)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0747)
features.2.conv.0 tensor(0.0396)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0802)
features.3.conv.0 tensor(0.0359)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0831)
features.4.conv.0 tensor(0.0680)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0584)
features.5.conv.0 tensor(0.0529)
features.5.conv.3 tensor(0.0822)
features.5.conv.6 tensor(0.1582)
features.6.conv.0 tensor(0.0293)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0611)
features.7.conv.0 tensor(0.0811)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.1111)
features.8.conv.0 tensor(0.1147)
features.8.conv.3 tensor(0.1042)
features.8.conv.6 tensor(0.3134)
features.9.conv.0 tensor(0.1375)
features.9.conv.3 tensor(0.1655)
features.9.conv.6 tensor(0.3940)
features.10.conv.0 tensor(0.0540)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.0501)
features.11.conv.0 tensor(0.5864)
features.11.conv.3 tensor(0.1410)
features.11.conv.6 tensor(0.7178)
features.12.conv.0 tensor(0.6982)
features.12.conv.3 tensor(0.1130)
features.12.conv.6 tensor(0.8126)
features.13.conv.0 tensor(0.1847)
features.13.conv.3 tensor(0.1391)
features.13.conv.6 tensor(0.3594)
features.14.conv.0 tensor(0.9832)
features.14.conv.3 tensor(0.0946)
features.14.conv.6 tensor(0.9845)
features.15.conv.0 tensor(0.9766)
features.15.conv.3 tensor(0.0649)
features.15.conv.6 tensor(0.5272)
features.16.conv.0 tensor(0.2364)
features.16.conv.3 tensor(0.1135)
features.16.conv.6 tensor(0.6381)
conv.0 tensor(0.5400)
tensor(1228308.) 2188896.0
INFO - Validation [43][   40/   40]   Loss 0.333826   Top1 88.910000   Top5 99.600000   BatchTime 0.147575
INFO - ==> Top1: 88.910    Top5: 99.600    Loss: 0.334
INFO - ==> Sparsity : 0.561
INFO - Scoreboard best 1 ==> Epoch [43][Top1: 88.910   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
0.70384634
0.70411390
0.70417029
0.70425415
0.70427537
0.70430821
0.70436805
0.70431060
0.70429903
0.70435435
0.70443469
0.70460087
0.70467389
0.70493323
0.70508444
0.70516390
0.70529717
0.70548248
0.70558906
INFO - Training [44][   20/  196]   Loss 0.439560   Top1 85.527344   Top5 98.066406   BatchTime 0.447741   LR 0.000904
0.70565599
0.70572978
0.70566714
0.70562667
0.70556206
0.70536703
0.70521009
0.70496446
0.70480484
0.70463932
0.70444947
0.70459342
0.70461887
0.70446450
0.70432645
0.70411265
0.70388401
0.70364749
0.70346141
0.70315200
0.70288467
0.70267653
0.70248026
0.70227677
INFO - Training [44][   40/  196]   Loss 0.439684   Top1 85.761719   Top5 98.330078   BatchTime 0.386804   LR 0.000900
0.70207483
0.70188725
0.70161331
0.70157367
0.70152634
0.70120668
0.70087981
0.70052546
0.70015675
0.69980085
0.69946057
0.69920003
0.69918025
0.69908345
0.69888359
0.69903135
INFO - Training [44][   60/  196]   Loss 0.438931   Top1 85.865885   Top5 98.411458   BatchTime 0.383321   LR 0.000895
0.69894534
0.69865888
0.69840479
0.69837785
0.69813401
0.69786692
0.69780093
0.69759518
0.69750190
0.69743681
0.69744235
0.69741827
0.69743663
0.69735235
0.69704121
0.69676536
0.69646621
0.69622749
0.69622701
0.69648546
0.69664842
INFO - Training [44][   80/  196]   Loss 0.434093   Top1 86.064453   Top5 98.515625   BatchTime 0.384967   LR 0.000891
0.69671845
0.69681513
0.69692397
0.69684643
0.69683480
0.69677669
0.69690472
0.69703317
0.70019341
0.70040876
0.70022649
0.70012057
0.70018589
0.70029145
0.70047849
0.70049834
0.70058554
0.70080078
0.70106983
0.70123428
0.70143259
0.70157230
INFO - Training [44][  100/  196]   Loss 0.427876   Top1 86.253906   Top5 98.597656   BatchTime 0.380878   LR 0.000886
0.70175236
0.70192975
0.70204294
0.70200539
0.70202142
0.70189816
0.70179856
0.70175022
0.70178056
0.70178372
0.70192879
0.70213765
0.70217007
0.70206159
0.70197904
0.70207965
0.70213658
0.70222747
0.70230824
0.70241219
0.70245320
INFO - Training [44][  120/  196]   Loss 0.422365   Top1 86.448568   Top5 98.645833   BatchTime 0.378651   LR 0.000882
0.70247799
0.70266682
0.70289266
0.70289499
0.70290816
0.70299476
0.70322543
0.70349324
0.70380580
0.70402873
0.70422649
0.70446330
0.70441258
0.70431757
0.70413452
0.70403737
0.70401615
0.70378709
0.70355207
0.70321399
0.70297134
INFO - Training [44][  140/  196]   Loss 0.422705   Top1 86.537388   Top5 98.713728   BatchTime 0.381378   LR 0.000877
0.70279974
0.70258194
0.70241034
0.70224321
0.70189393
0.70172220
0.70154727
0.70131171
0.70109320
0.70100379
0.70105159
0.70105720
0.70114660
0.70102233
0.70086843
0.70055836
0.70028746
0.69987184
0.69974113
INFO - Training [44][  160/  196]   Loss 0.423165   Top1 86.525879   Top5 98.720703   BatchTime 0.384381   LR 0.000873
0.69937414
0.69909668
0.69865429
0.69842374
0.69821501
0.69797707
0.69755059
0.69716352
0.69706625
0.69686562
0.69684500
0.69705176
0.69717711
0.69752526
0.69773328
0.69765103
0.69775021
0.69775975
0.69786596
0.69797349
INFO - Training [44][  180/  196]   Loss 0.422013   Top1 86.493056   Top5 98.719618   BatchTime 0.385480   LR 0.000868
0.69801927
0.69803017
0.69806802
0.69792336
0.69774526
0.69753599
0.69747084
0.69739223
0.69743651
0.69739097
0.69704753
0.69676358
0.69662344
********************pre-trained*****************
INFO - ==> Top1: 86.556    Top5: 98.722    Loss: 0.420
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.4570)
features.1.conv.0 tensor(0.0371)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0747)
features.2.conv.0 tensor(0.0422)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0848)
features.3.conv.0 tensor(0.0324)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0814)
features.4.conv.0 tensor(0.0659)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0723)
features.5.conv.0 tensor(0.0493)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1353)
features.6.conv.0 tensor(0.0353)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0618)
features.7.conv.0 tensor(0.0831)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.1769)
features.8.conv.0 tensor(0.1293)
features.8.conv.3 tensor(0.1013)
features.8.conv.6 tensor(0.3211)
features.9.conv.0 tensor(0.3186)
features.9.conv.3 tensor(0.1620)
features.9.conv.6 tensor(0.4002)
features.10.conv.0 tensor(0.0634)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.0663)
features.11.conv.0 tensor(0.6095)
features.11.conv.3 tensor(0.1424)
features.11.conv.6 tensor(0.7168)
features.12.conv.0 tensor(0.6654)
features.12.conv.3 tensor(0.1150)
features.12.conv.6 tensor(0.8190)
features.13.conv.0 tensor(0.1960)
features.13.conv.3 tensor(0.1395)
features.13.conv.6 tensor(0.4620)
features.14.conv.0 tensor(0.9840)
features.14.conv.3 tensor(0.0932)
features.14.conv.6 tensor(0.9822)
features.15.conv.0 tensor(0.9775)
features.15.conv.3 tensor(0.0640)
features.15.conv.6 tensor(0.5235)
features.16.conv.0 tensor(0.2592)
features.16.conv.3 tensor(0.1140)
features.16.conv.6 tensor(0.6053)
conv.0 tensor(0.5462)
tensor(1241018.) 2188896.0
INFO - Validation [44][   20/   40]   Loss 0.382752   Top1 87.597656   Top5 99.433594   BatchTime 0.182024
INFO - Validation [44][   40/   40]   Loss 0.367920   Top1 87.880000   Top5 99.520000   BatchTime 0.115686
INFO - ==> Top1: 87.880    Top5: 99.520    Loss: 0.368
INFO - ==> Sparsity : 0.567
INFO - Scoreboard best 1 ==> Epoch [43][Top1: 88.910   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
0.69619727
0.69501382
0.69495898
0.69522160
0.69632006
0.69678080
0.69751263
0.69814491
0.69876438
0.69887322
0.69883955
0.69901490
0.69906652
0.69933087
0.69943011
0.69981176
0.69995201
0.70000744
0.70011950
INFO - Training [45][   20/  196]   Loss 0.429352   Top1 85.839844   Top5 98.437500   BatchTime 0.409331   LR 0.000860
0.70035565
0.70054525
0.70047110
0.70010859
0.70019013
0.70046020
0.70029235
0.70021778
0.70021015
0.70003521
0.69978547
0.70014966
0.69988590
0.69841486
0.69678038
0.69647557
0.69610763
0.69513053
0.69377899
0.69477004
0.69585270
0.69696289
0.69813925
INFO - Training [45][   40/  196]   Loss 0.432865   Top1 85.800781   Top5 98.583984   BatchTime 0.377292   LR 0.000855
0.69913709
0.69978142
0.70048654
0.70088536
0.70106226
0.70130253
0.70150548
0.70153117
0.70186591
0.70194113
0.70202678
0.70239806
0.70257872
0.70275337
0.70296609
0.70324594
0.70357794
0.70378608
0.70377171
0.70360202
0.70336723
0.70311505
INFO - Training [45][   60/  196]   Loss 0.433472   Top1 85.917969   Top5 98.658854   BatchTime 0.376722   LR 0.000850
0.70287961
0.70272100
0.70255262
0.70219499
0.70198041
0.70197296
0.70187289
0.70167077
0.70138049
0.70107144
0.70076483
0.70039010
0.69988739
0.69918519
0.69888103
INFO - Training [45][   80/  196]   Loss 0.433902   Top1 85.932617   Top5 98.808594   BatchTime 0.380232   LR 0.000846
0.69885844
0.69871718
0.69865841
0.69852740
0.69841117
0.69828415
0.69807392
0.69778848
0.69746435
0.69726998
0.69719958
0.69693911
0.69689566
0.69644791
0.69623369
0.69577801
0.69557357
0.69541782
0.69539297
0.69511628
0.69467008
0.69397402
INFO - Training [45][  100/  196]   Loss 0.423411   Top1 86.261719   Top5 98.882812   BatchTime 0.375278   LR 0.000841
0.69388235
0.69368488
0.69345284
0.69334656
0.69324350
0.69301426
0.69314224
0.69330001
0.69309068
0.69290006
0.69262022
0.69234532
0.69231999
0.69209313
0.69203007
0.69169945
0.69158822
0.69162685
0.69149423
0.69143301
0.69128925
0.69107729
0.69081366
INFO - Training [45][  120/  196]   Loss 0.414336   Top1 86.608073   Top5 98.912760   BatchTime 0.372134   LR 0.000836
0.69057673
0.69026768
0.69001883
0.69015771
0.68988186
0.68995160
0.69013393
0.69055980
0.69126087
0.69181240
0.69187015
0.69139725
0.69110334
0.69040179
0.69016212
0.68997711
INFO - Training [45][  140/  196]   Loss 0.412789   Top1 86.690848   Top5 98.959263   BatchTime 0.372659   LR 0.000832
0.68994015
0.68975103
0.69001615
0.69017231
0.69006473
0.69030148
0.69051266
0.69084990
0.69114637
0.69157499
0.69200361
0.69252026
0.69301081
0.69317412
0.69319379
0.69800669
0.69758862
0.69760066
0.69725496
0.69700563
0.69701624
0.69690722
INFO - Training [45][  160/  196]   Loss 0.416334   Top1 86.560059   Top5 98.935547   BatchTime 0.370397   LR 0.000827
0.69675916
0.69662249
0.69632822
0.69615960
0.69597065
0.69561613
0.69540197
0.69536346
0.69539356
0.69523430
0.69490570
0.69442767
0.69409013
0.69379681
0.69353324
0.69327825
0.69289827
0.69257402
0.69227844
0.69206083
0.69182730
INFO - Training [45][  180/  196]   Loss 0.417597   Top1 86.508247   Top5 98.901910   BatchTime 0.372359   LR 0.000822
0.69144237
0.69086838
0.69030213
0.69008845
0.68996024
0.68980396
0.69057351
0.69036460
0.68988156
0.69000614
0.69002306
0.68958658
0.68967706
********************pre-trained*****************
INFO - ==> Top1: 86.498    Top5: 98.874    Loss: 0.417
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.380386   Top1 87.636719   Top5 99.375000   BatchTime 0.134450
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.4863)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0480)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0848)
features.3.conv.0 tensor(0.0347)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0783)
features.4.conv.0 tensor(0.0684)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.0907)
features.5.conv.0 tensor(0.0467)
features.5.conv.3 tensor(0.0822)
features.5.conv.6 tensor(0.1514)
features.6.conv.0 tensor(0.0389)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0589)
features.7.conv.0 tensor(0.0937)
features.7.conv.3 tensor(0.1117)
features.7.conv.6 tensor(0.0862)
features.8.conv.0 tensor(0.0928)
features.8.conv.3 tensor(0.1027)
features.8.conv.6 tensor(0.3672)
features.9.conv.0 tensor(0.1230)
features.9.conv.3 tensor(0.1629)
features.9.conv.6 tensor(0.4034)
features.10.conv.0 tensor(0.0554)
features.10.conv.3 tensor(0.0828)
features.10.conv.6 tensor(0.0828)
features.11.conv.0 tensor(0.6330)
features.11.conv.3 tensor(0.1418)
features.11.conv.6 tensor(0.7168)
features.12.conv.0 tensor(0.6898)
features.12.conv.3 tensor(0.1152)
features.12.conv.6 tensor(0.8157)
features.13.conv.0 tensor(0.1856)
features.13.conv.3 tensor(0.1406)
features.13.conv.6 tensor(0.3992)
features.14.conv.0 tensor(0.9846)
features.14.conv.3 tensor(0.0932)
features.14.conv.6 tensor(0.9824)
features.15.conv.0 tensor(0.9774)
features.15.conv.3 tensor(0.0644)
features.15.conv.6 tensor(0.5373)
features.16.conv.0 tensor(0.2630)
features.16.conv.3 tensor(0.1123)
features.16.conv.6 tensor(0.8888)
conv.0 tensor(0.5440)
tensor(1320376.) 2188896.0
INFO - Validation [45][   40/   40]   Loss 0.372908   Top1 87.850000   Top5 99.410000   BatchTime 0.093795
INFO - ==> Top1: 87.850    Top5: 99.410    Loss: 0.373
INFO - ==> Sparsity : 0.603
INFO - Scoreboard best 1 ==> Epoch [43][Top1: 88.910   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
0.69005406
0.69027746
0.69066823
0.69106513
0.69126409
0.69157928
0.69156224
0.69145495
0.69143498
0.69149208
0.69193685
0.69194883
0.69230676
0.69300216
0.69333941
0.69352400
0.69369620
0.69395828
0.69404781
0.69409615
0.69405890
0.69393039
INFO - Training [46][   20/  196]   Loss 0.413989   Top1 86.523438   Top5 98.144531   BatchTime 0.399671   LR 0.000814
0.69387203
0.69394368
0.69423634
0.69437939
0.69456416
0.69466144
0.69484240
0.69512421
0.69543993
0.69576126
0.69612491
0.69630259
0.69642591
0.69641453
0.69629353
0.69612515
0.69600254
0.69592232
0.69585782
0.69597030
0.69613713
0.69603539
INFO - Training [46][   40/  196]   Loss 0.427381   Top1 86.308594   Top5 98.417969   BatchTime 0.387442   LR 0.000809
0.69614255
0.69612533
0.69613242
0.69633085
0.69640517
0.69675279
0.69718891
0.69782692
0.69859016
0.69910705
0.69953954
0.69989449
0.70015305
0.70040905
0.70069110
0.70083684
0.70100099
INFO - Training [46][   60/  196]   Loss 0.431049   Top1 86.367188   Top5 98.463542   BatchTime 0.376258   LR 0.000804
0.70102632
0.70117348
0.70140201
0.70165288
0.70190150
0.70197642
0.70214421
0.70224267
0.70235026
0.70245063
0.70236015
0.70222318
0.70217347
0.70190364
0.70152819
0.70151377
0.70134407
0.70127898
0.70126534
0.70111728
INFO - Training [46][   80/  196]   Loss 0.433181   Top1 86.308594   Top5 98.593750   BatchTime 0.380385   LR 0.000799
0.70098883
0.70088631
0.70076448
0.70062953
0.70054799
0.70048457
0.70043385
0.70038199
0.70033306
0.70035094
0.70022100
0.70013988
0.70001084
0.69982708
0.69956714
0.69949645
0.69922292
0.69897991
0.69883770
0.69863427
0.69852459
INFO - Training [46][  100/  196]   Loss 0.425454   Top1 86.468750   Top5 98.648438   BatchTime 0.378637   LR 0.000794
0.69839311
0.69840342
0.69846934
0.69843042
0.69841588
0.69837558
0.69831806
0.69813937
0.69807291
0.69774413
0.69734639
0.69708979
0.69678944
0.69638866
0.69617420
0.69577533
0.69569731
INFO - Training [46][  120/  196]   Loss 0.422055   Top1 86.549479   Top5 98.684896   BatchTime 0.376858   LR 0.000789
0.69539452
0.69514173
0.69498199
0.69478190
0.69448471
0.69409025
0.69385004
0.69354081
0.69354403
0.69336998
0.69310236
0.69285697
0.69253880
0.69230729
0.69192171
0.69150698
0.69119024
0.69089329
0.69060171
0.69058311
0.69046247
0.69018781
0.68993187
INFO - Training [46][  140/  196]   Loss 0.419784   Top1 86.685268   Top5 98.744420   BatchTime 0.373519   LR 0.000785
0.68996865
0.69015652
0.69018036
0.69167674
0.69217408
0.69238383
0.69247198
0.69239157
0.69238967
0.69228756
0.69210845
0.69152510
0.69125593
0.69104999
0.69054300
0.69010460
0.68985546
0.68952417
0.68931103
0.68920255
0.68912172
INFO - Training [46][  160/  196]   Loss 0.419530   Top1 86.604004   Top5 98.750000   BatchTime 0.373041   LR 0.000780
0.68927282
0.68932760
0.68959463
0.68941063
0.68944281
0.68967032
0.68989360
0.69005340
0.69006503
0.69032973
0.69074786
0.69111937
0.69150376
0.69139290
0.69164318
0.69205004
0.69225174
0.69241714
0.69262135
0.69301462
0.69358343
INFO - Training [46][  180/  196]   Loss 0.418431   Top1 86.592882   Top5 98.728299   BatchTime 0.375123   LR 0.000775
0.69633055
0.69656295
0.69685972
0.69723409
0.69786668
0.69872421
0.69972676
0.70107132
0.70232707
0.70275050
0.70247215
0.70236313
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 86.578    Top5: 98.724    Loss: 0.418
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [46][   20/   40]   Loss 0.379888   Top1 87.207031   Top5 99.492188   BatchTime 0.140481
INFO - Validation [46][   40/   40]   Loss 0.369980   Top1 87.550000   Top5 99.560000   BatchTime 0.096858
INFO - ==> Top1: 87.550    Top5: 99.560    Loss: 0.370
INFO - ==> Sparsity : 0.547
INFO - Scoreboard best 1 ==> Epoch [43][Top1: 88.910   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 88.610   Top5: 99.660]
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.4941)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.0486)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0854)
features.3.conv.0 tensor(0.0350)
features.3.conv.3 tensor(0.0432)
features.3.conv.6 tensor(0.0768)
features.4.conv.0 tensor(0.0640)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.0957)
features.5.conv.0 tensor(0.0503)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1603)
features.6.conv.0 tensor(0.0358)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0608)
features.7.conv.0 tensor(0.1031)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.2453)
features.8.conv.0 tensor(0.0951)
features.8.conv.3 tensor(0.1024)
features.8.conv.6 tensor(0.3317)
features.9.conv.0 tensor(0.1296)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.4093)
features.10.conv.0 tensor(0.0661)
features.10.conv.3 tensor(0.0839)
features.10.conv.6 tensor(0.0854)
features.11.conv.0 tensor(0.6192)
features.11.conv.3 tensor(0.1426)
features.11.conv.6 tensor(0.7212)
features.12.conv.0 tensor(0.6678)
features.12.conv.3 tensor(0.1161)
features.12.conv.6 tensor(0.8254)
features.13.conv.0 tensor(0.1945)
features.13.conv.3 tensor(0.1370)
features.13.conv.6 tensor(0.4172)
features.14.conv.0 tensor(0.9850)
features.14.conv.3 tensor(0.0913)
features.14.conv.6 tensor(0.9847)
features.15.conv.0 tensor(0.9778)
features.15.conv.3 tensor(0.0637)
features.15.conv.6 tensor(0.5374)
features.16.conv.0 tensor(0.2526)
features.16.conv.3 tensor(0.1113)
features.16.conv.6 tensor(0.4558)
conv.0 tensor(0.5607)
tensor(1198015.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
0.70213443
0.70186853
0.70160335
0.70144284
0.70137763
0.70101124
0.70068246
0.70035464
0.70005381
0.69990438
0.69946837
0.69908613
0.69894534
0.69872856
0.69843435
0.69791245
0.69745541
0.69697958
0.69666582
0.69637799
0.69615108
0.69585228
0.69572449
INFO - Training [47][   20/  196]   Loss 0.419689   Top1 86.464844   Top5 98.398438   BatchTime 0.424605   LR 0.000766
0.69554394
0.69512862
0.69499713
0.69503009
0.69511080
0.69507778
0.69519365
0.69546664
0.69549125
0.69546735
0.69562453
0.69560558
0.69545835
0.69543684
0.69536239
0.69534451
0.69539648
INFO - Training [47][   40/  196]   Loss 0.420262   Top1 86.650391   Top5 98.564453   BatchTime 0.388247   LR 0.000761
0.69531822
0.69529951
0.69540584
0.69545799
0.69550824
0.69566423
0.69578034
0.69595659
0.69562441
0.69538265
0.69501632
0.69483531
0.69441289
0.69415057
0.69385976
0.69348782
0.69321942
0.69292313
0.69245261
0.69193101
0.69159430
0.69135666
INFO - Training [47][   60/  196]   Loss 0.422889   Top1 86.640625   Top5 98.574219   BatchTime 0.380768   LR 0.000756
0.69114012
0.69098318
0.69069856
0.69061065
0.69050163
0.69031841
0.69017369
0.68999845
0.68967617
0.68930817
0.68894917
0.68862140
0.68831176
0.68804818
0.68790883
0.68771148
0.68743211
0.68716407
0.68691355
0.68667346
0.68645197
0.68615383
INFO - Training [47][   80/  196]   Loss 0.422861   Top1 86.513672   Top5 98.696289   BatchTime 0.376116   LR 0.000752
0.68593204
0.68578422
0.68562955
0.68565881
0.68578213
0.68575692
0.68565542
0.68543863
0.68539196
0.68498445
0.68480110
0.68461376
0.68460315
0.68445623
0.68406004
0.68393803
INFO - Training [47][  100/  196]   Loss 0.418322   Top1 86.605469   Top5 98.750000   BatchTime 0.374129   LR 0.000747
0.68386114
0.68394136
0.68417573
0.68387300
0.68364370
0.68350172
0.68338788
0.68333048
0.68316281
0.68307054
0.68298954
0.68302411
0.68298495
0.68287581
0.68274832
0.68269658
0.68267196
0.68278825
0.68303323
0.68318325
0.68325943
0.68345708
INFO - Training [47][  120/  196]   Loss 0.410712   Top1 86.871745   Top5 98.841146   BatchTime 0.374268   LR 0.000742
0.68373048
0.68403411
0.68427175
0.68453181
0.68485135
0.68515050
0.68557376
0.68582511
0.68610775
0.68639940
0.68663353
0.68679136
0.68713731
0.68932307
0.68939781
0.68942881
0.68954188
0.68943506
0.68922096
INFO - Training [47][  140/  196]   Loss 0.404945   Top1 87.047991   Top5 98.900670   BatchTime 0.378893   LR 0.000737
0.68895787
0.68897545
0.68913311
0.68924671
0.68921608
0.68928140
0.68925029
0.68927675
0.68929929
0.68935502
0.68936950
0.68939871
0.68952304
0.68967074
0.68976259
0.68984872
0.68995190
0.69003564
0.69010520
0.69015950
0.69001853
0.68997121
INFO - Training [47][  160/  196]   Loss 0.408528   Top1 86.904297   Top5 98.889160   BatchTime 0.377384   LR 0.000732
0.68993276
0.68981260
0.68973953
0.68970490
0.68958378
0.68946511
0.68941057
0.68951136
0.68934530
0.68921936
0.68908739
0.68899173
0.68892437
0.68881655
0.68875724
0.68879443
0.68884939
INFO - Training [47][  180/  196]   Loss 0.410085   Top1 86.807726   Top5 98.862847   BatchTime 0.375965   LR 0.000727
0.68887979
0.68881488
0.68873852
0.68865722
0.68853009
0.68831241
0.68822211
0.68821442
0.68812364
0.68805104
0.68796235
0.68777287
0.68765664
0.68739867
0.68722188
0.68694896
INFO - ==> Top1: 86.826    Top5: 98.866    Loss: 0.409
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [47][   20/   40]   Loss 0.319233   Top1 89.960938   Top5 99.492188   BatchTime 0.217301
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.4746)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0703)
features.2.conv.0 tensor(0.0498)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0845)
features.3.conv.0 tensor(0.0318)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0764)
features.4.conv.0 tensor(0.0645)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.1006)
features.5.conv.0 tensor(0.0563)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.1672)
features.6.conv.0 tensor(0.0350)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0608)
features.7.conv.0 tensor(0.0974)
features.7.conv.3 tensor(0.1068)
features.7.conv.6 tensor(0.2270)
features.8.conv.0 tensor(0.0985)
features.8.conv.3 tensor(0.1062)
features.8.conv.6 tensor(0.3597)
features.9.conv.0 tensor(0.1308)
features.9.conv.3 tensor(0.1623)
features.9.conv.6 tensor(0.4104)
features.10.conv.0 tensor(0.0677)
features.10.conv.3 tensor(0.0804)
features.10.conv.6 tensor(0.1187)
features.11.conv.0 tensor(0.6435)
features.11.conv.3 tensor(0.1397)
features.11.conv.6 tensor(0.7232)
features.12.conv.0 tensor(0.6708)
features.12.conv.3 tensor(0.1113)
features.12.conv.6 tensor(0.8202)
features.13.conv.0 tensor(0.1941)
features.13.conv.3 tensor(0.1373)
features.13.conv.6 tensor(0.5065)
features.14.conv.0 tensor(0.9854)
features.14.conv.3 tensor(0.0919)
features.14.conv.6 tensor(0.9857)
features.15.conv.0 tensor(0.9786)
features.15.conv.3 tensor(0.0640)
features.15.conv.6 tensor(0.5328)
features.16.conv.0 tensor(0.2673)
features.16.conv.3 tensor(0.1094)
features.16.conv.6 tensor(0.5837)
conv.0 tensor(0.5465)
tensor(1244440.) 2188896.0
INFO - Validation [47][   40/   40]   Loss 0.303738   Top1 89.900000   Top5 99.600000   BatchTime 0.142169
INFO - ==> Top1: 89.900    Top5: 99.600    Loss: 0.304
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [43][Top1: 88.910   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.660   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
0.68666917
0.68653738
0.68632811
0.68624365
0.68622011
0.68619126
0.68634021
0.68650216
0.68672359
0.68674934
0.68670934
0.68674010
0.68675542
0.68678755
0.68679327
0.68684453
0.68693900
0.68693691
0.68691999
0.68678558
INFO - Training [48][   20/  196]   Loss 0.413621   Top1 86.308594   Top5 98.437500   BatchTime 0.479410   LR 0.000718
0.68665057
0.68650132
0.68634045
0.68627024
0.68623579
0.68628925
0.68638951
0.68653882
0.68674594
0.68679672
0.68671280
0.68665761
0.68692333
0.68701100
0.68720144
0.68732411
0.68740237
0.68760222
0.68774259
0.68783528
0.68789375
INFO - Training [48][   40/  196]   Loss 0.421107   Top1 86.171875   Top5 98.515625   BatchTime 0.430369   LR 0.000713
0.68778229
0.68773305
0.68760157
0.68789744
0.68801665
0.68839693
0.68833429
0.68832445
0.68831646
0.68814856
0.68806863
0.68799073
0.68799740
0.68808568
0.68813515
0.68830985
0.68842620
0.68854809
0.68850809
0.68842345
0.68840599
INFO - Training [48][   60/  196]   Loss 0.416295   Top1 86.425781   Top5 98.548177   BatchTime 0.411558   LR 0.000708
0.68846136
0.68838853
0.68822616
0.68810958
0.68796629
0.68762511
0.68724638
0.68664789
0.68627030
0.68596727
0.68596166
0.68611538
0.68949956
0.68947786
0.68956107
0.68966824
0.68972850
0.68988103
0.68998581
0.69007695
0.69018555
0.69023752
INFO - Training [48][   80/  196]   Loss 0.410941   Top1 86.611328   Top5 98.686523   BatchTime 0.402866   LR 0.000703
0.69020170
0.69013762
0.69015759
0.69021857
0.69025791
0.69030225
0.69035763
0.69037133
0.69047028
0.69055420
0.69063824
0.69072282
0.69083613
0.69091660
0.69102508
0.69105262
INFO - Training [48][  100/  196]   Loss 0.407855   Top1 86.765625   Top5 98.734375   BatchTime 0.395255   LR 0.000698
0.69095701
0.69087762
0.69084901
0.69072062
0.69069380
0.69059116
0.69059277
0.69062489
0.69059438
0.69050288
0.69048184
0.69046944
0.69053447
0.69049174
0.69045031
0.69042951
0.69034356
0.69032413
0.69020444
0.69022268
0.69013685
0.68991035
INFO - Training [48][  120/  196]   Loss 0.401008   Top1 86.985677   Top5 98.811849   BatchTime 0.390827   LR 0.000693
0.68976247
0.68956578
0.68944460
0.68932158
0.68920541
0.68900204
0.68894434
0.68895483
0.68900746
0.68891251
0.68884331
0.68877131
0.68866521
0.68864208
0.68869108
0.68866146
0.68870467
0.68867773
0.68863052
0.68866032
0.68857205
INFO - Training [48][  140/  196]   Loss 0.394768   Top1 87.207031   Top5 98.917411   BatchTime 0.390097   LR 0.000688
0.68865222
0.68870950
0.68872029
0.68863791
0.68862355
0.68854290
0.68848461
0.68852931
0.68855238
0.68851638
0.68860018
0.68861806
0.68874663
0.68887174
0.68907773
0.68929237
0.68964612
0.68997127
0.69023550
0.69047749
0.69058138
INFO - Training [48][  160/  196]   Loss 0.399222   Top1 87.065430   Top5 98.889160   BatchTime 0.389020   LR 0.000683
0.69059968
0.69067973
0.69723636
0.69722736
0.69712389
0.69710982
0.69712055
0.69698906
0.69680279
0.69671148
0.69665450
0.69646591
0.69636917
0.69631249
0.69628233
INFO - Training [48][  180/  196]   Loss 0.398703   Top1 87.061632   Top5 98.869358   BatchTime 0.389070   LR 0.000678
0.69624293
0.69626343
0.69630581
0.69642168
0.69646126
0.69651818
0.69646925
0.69648576
0.69649321
0.69641006
0.69623095
0.69594544
0.69564492
0.69524169
0.69475418
0.69436699
0.69407618
********************pre-trained*****************
INFO - ==> Top1: 87.084    Top5: 98.872    Loss: 0.399
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.4824)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0686)
features.2.conv.0 tensor(0.0463)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0830)
features.3.conv.0 tensor(0.0379)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0731)
features.4.conv.0 tensor(0.0728)
features.4.conv.3 tensor(0.0816)
features.4.conv.6 tensor(0.1102)
features.5.conv.0 tensor(0.0651)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1808)
features.6.conv.0 tensor(0.0348)
features.6.conv.3 tensor(0.0521)
features.6.conv.6 tensor(0.0591)
features.7.conv.0 tensor(0.0942)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.1598)
features.8.conv.0 tensor(0.1008)
features.8.conv.3 tensor(0.1030)
features.8.conv.6 tensor(0.3235)
features.9.conv.0 tensor(0.1356)
features.9.conv.3 tensor(0.1629)
features.9.conv.6 tensor(0.4136)
features.10.conv.0 tensor(0.0709)
features.10.conv.3 tensor(0.0859)
features.10.conv.6 tensor(0.0854)
features.11.conv.0 tensor(0.6268)
features.11.conv.3 tensor(0.1406)
features.11.conv.6 tensor(0.7259)
features.12.conv.0 tensor(0.6766)
features.12.conv.3 tensor(0.1130)
features.12.conv.6 tensor(0.8306)
features.13.conv.0 tensor(0.2103)
features.13.conv.3 tensor(0.1364)
features.13.conv.6 tensor(0.4477)
features.14.conv.0 tensor(0.9855)
features.14.conv.3 tensor(0.0912)
features.14.conv.6 tensor(0.9865)
features.15.conv.0 tensor(0.9793)
features.15.conv.3 tensor(0.0635)
features.15.conv.6 tensor(0.5077)
features.16.conv.0 tensor(0.2635)
features.16.conv.3 tensor(0.1118)
features.16.conv.6 tensor(0.6264)
conv.0 tensor(0.5464)
tensor(1245646.) 2188896.0
INFO - Validation [48][   20/   40]   Loss 0.338376   Top1 89.003906   Top5 99.667969   BatchTime 0.157185
INFO - Validation [48][   40/   40]   Loss 0.323125   Top1 89.250000   Top5 99.680000   BatchTime 0.106458
INFO - ==> Top1: 89.250    Top5: 99.680    Loss: 0.323
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [48][Top1: 89.250   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 88.910   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
0.69364560
0.69313049
0.69273716
0.69449961
0.69414479
0.69379348
0.69355839
0.69336367
0.69313860
0.69294840
0.69272000
0.69241256
0.69213992
0.69180447
0.69145620
0.69111419
0.69094425
0.69073159
0.69061643
0.69036365
0.69020420
0.69004965
INFO - Training [49][   20/  196]   Loss 0.397864   Top1 86.757812   Top5 98.417969   BatchTime 0.461310   LR 0.000669
0.68988687
0.68955100
0.68939131
0.68917239
0.68881035
0.68853170
0.68838364
0.68811655
0.68785244
0.68773365
0.68756026
0.68762094
0.68801105
0.68803316
0.68802869
0.68823785
0.68838537
INFO - Training [49][   40/  196]   Loss 0.407870   Top1 86.816406   Top5 98.496094   BatchTime 0.409287   LR 0.000664
0.68853557
0.68875295
0.68874651
0.68890965
0.68909389
0.68949276
0.68984616
0.69014877
0.69016111
0.69012934
0.69010633
0.69014823
0.69003820
0.69005126
0.69010979
0.69017476
0.69031662
0.69049329
0.69076365
0.69107324
0.69128597
0.69138736
INFO - Training [49][   60/  196]   Loss 0.409429   Top1 86.705729   Top5 98.574219   BatchTime 0.394753   LR 0.000659
0.69148308
0.69152504
0.69144809
0.69139606
0.69128567
0.69118857
0.69103366
0.69098443
0.69086128
0.69073182
0.69072402
0.69073182
0.69075179
0.69090939
0.69096851
0.69086903
0.69080579
0.69074684
0.69067633
0.69069147
0.69074845
INFO - Training [49][   80/  196]   Loss 0.410394   Top1 86.694336   Top5 98.754883   BatchTime 0.392557   LR 0.000654
0.69084615
0.69092757
0.69097054
0.69085836
0.69076782
0.69062871
0.69052058
0.69052136
0.69051886
0.69042039
0.69016612
0.68982780
0.68955827
0.68942779
0.68938673
0.68941814
0.68956542
0.68960112
0.68939847
0.68931448
0.68922949
0.68920171
INFO - Training [49][  100/  196]   Loss 0.403421   Top1 86.941406   Top5 98.800781   BatchTime 0.386463   LR 0.000649
0.68905294
0.68892276
0.68883473
0.68881303
0.68874460
0.68865454
0.68838704
0.68818009
0.68797177
0.68765098
0.68730837
0.68705344
0.68680727
0.68650675
0.68627977
INFO - Training [49][  120/  196]   Loss 0.397383   Top1 87.207031   Top5 98.844401   BatchTime 0.385384   LR 0.000644
0.68598974
0.68570721
0.68547666
0.68520403
0.68502963
0.68495625
0.68495917
0.68486732
0.68479091
0.68459445
0.68439955
0.68418485
0.68393689
0.68357331
0.68323565
0.68279469
0.68238121
0.68181121
0.68124390
0.68084854
0.68048984
0.68008488
0.67984509
INFO - Training [49][  140/  196]   Loss 0.396978   Top1 87.223772   Top5 98.914621   BatchTime 0.380983   LR 0.000639
0.67954022
0.67935091
0.67895228
0.67854667
0.67807424
0.67779332
0.67745113
0.67700887
0.67655963
0.67627996
0.67653531
0.67676198
0.67683589
0.67720270
0.67753398
0.67791086
0.67817122
0.67834795
0.67847228
0.67853487
0.67858255
0.67858875
INFO - Training [49][  160/  196]   Loss 0.401179   Top1 87.048340   Top5 98.881836   BatchTime 0.379086   LR 0.000634
0.67866600
0.67878187
0.67884457
0.67895043
0.67907107
0.67921531
0.67934954
0.67961556
0.67992604
0.68021744
0.68060958
0.68098086
0.68123895
0.68155479
0.68181151
INFO - Training [49][  180/  196]   Loss 0.404194   Top1 86.946615   Top5 98.860677   BatchTime 0.380389   LR 0.000629
0.68212801
0.68246090
0.68269175
0.68294960
0.68315262
0.68331242
0.68335187
0.68336725
0.68335438
0.68338025
0.68334651
0.68330628
0.68334985
0.68331581
0.68327719
0.68331450
0.68324900
INFO - ==> Top1: 86.996    Top5: 98.850    Loss: 0.403
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [49][   20/   40]   Loss 0.343247   Top1 89.062500   Top5 99.550781   BatchTime 0.144060
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0830)
features.3.conv.0 tensor(0.0275)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0751)
features.4.conv.0 tensor(0.0695)
features.4.conv.3 tensor(0.0868)
features.4.conv.6 tensor(0.1261)
features.5.conv.0 tensor(0.0667)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1745)
features.6.conv.0 tensor(0.0361)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0615)
features.7.conv.0 tensor(0.1036)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.2058)
features.8.conv.0 tensor(0.0959)
features.8.conv.3 tensor(0.1030)
features.8.conv.6 tensor(0.3448)
features.9.conv.0 tensor(0.1393)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.4543)
features.10.conv.0 tensor(0.0638)
features.10.conv.3 tensor(0.0833)
features.10.conv.6 tensor(0.1423)
features.11.conv.0 tensor(0.6255)
features.11.conv.3 tensor(0.1412)
features.11.conv.6 tensor(0.7358)
features.12.conv.0 tensor(0.6688)
features.12.conv.3 tensor(0.1142)
features.12.conv.6 tensor(0.8281)
features.13.conv.0 tensor(0.2084)
features.13.conv.3 tensor(0.1335)
features.13.conv.6 tensor(0.4385)
features.14.conv.0 tensor(0.9860)
features.14.conv.3 tensor(0.0903)
features.14.conv.6 tensor(0.9854)
features.15.conv.0 tensor(0.9793)
features.15.conv.3 tensor(0.0653)
features.15.conv.6 tensor(0.4945)
features.16.conv.0 tensor(0.2781)
features.16.conv.3 tensor(0.1095)
features.16.conv.6 tensor(0.5798)
conv.0 tensor(0.5639)
tensor(1242348.) 2188896.0
INFO - Validation [49][   40/   40]   Loss 0.329176   Top1 89.370000   Top5 99.650000   BatchTime 0.110109
INFO - ==> Top1: 89.370    Top5: 99.650    Loss: 0.329
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 89.370   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 89.250   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
0.68318540
0.68316424
0.68325317
0.68332720
0.68330079
0.68330270
0.68336087
0.68341118
0.68347442
0.68356115
0.68353993
0.68345106
0.68337417
0.68325144
0.68304950
0.68279153
0.68260741
0.68241221
0.68234205
0.68237001
INFO - Training [50][   20/  196]   Loss 0.413559   Top1 86.679688   Top5 98.359375   BatchTime 0.445535   LR 0.000620
0.68237889
0.68242973
0.68236727
0.68234307
0.68231565
0.68222713
0.68218726
0.68217653
0.68213499
0.68189168
0.68165523
0.68162704
0.68161088
0.68160337
0.68160248
0.68152618
0.68143630
0.68137580
0.68148482
0.68155110
INFO - Training [50][   40/  196]   Loss 0.407981   Top1 87.167969   Top5 98.466797   BatchTime 0.420308   LR 0.000615
0.68167180
0.68175685
0.68173391
0.68172836
0.68172342
0.68172216
0.68166691
0.68152863
0.68145740
0.68136841
0.68121868
0.68113524
0.68115622
0.68116194
0.68119514
0.68132740
0.68139762
0.68157512
0.68183208
0.68203455
0.68220472
0.68235701
0.68252027
INFO - Training [50][   60/  196]   Loss 0.404914   Top1 87.194010   Top5 98.483073   BatchTime 0.396826   LR 0.000610
0.68251711
0.68259555
0.68262023
0.68255812
0.68245524
0.68240881
0.68230653
0.68222696
0.68215537
0.68201202
0.68176419
0.68154293
0.68131471
0.68110758
0.68080902
0.68055749
0.68035173
0.68005997
0.67983019
0.67974496
INFO - Training [50][   80/  196]   Loss 0.402078   Top1 87.177734   Top5 98.642578   BatchTime 0.396705   LR 0.000605
0.67958683
0.67945760
0.67938387
0.67924154
0.67912847
0.67919236
0.67915738
0.67927456
0.67940480
0.67950976
0.67969489
0.67991281
0.68008876
0.68033916
0.68046963
0.68062377
0.68071133
INFO - Training [50][  100/  196]   Loss 0.396174   Top1 87.343750   Top5 98.730469   BatchTime 0.389861   LR 0.000600
0.68081105
0.68096286
0.68109447
0.68117893
0.68122339
0.68136358
0.68158609
0.68187189
0.68267864
0.68490213
0.68498248
0.68482447
0.68452603
0.68412465
0.68372118
0.68343270
0.68321037
0.68304694
0.68301409
0.68299788
0.68290859
0.68283170
INFO - Training [50][  120/  196]   Loss 0.391371   Top1 87.539062   Top5 98.828125   BatchTime 0.385886   LR 0.000595
0.68268776
0.68248320
0.68228865
0.68217325
0.68207538
0.68191630
0.68175834
0.68149602
0.68125355
0.68109280
0.68093604
0.68072116
0.68054694
0.68023831
0.67989618
0.67957020
0.67905080
0.67875969
0.67835009
0.67800814
0.67775381
0.67777079
INFO - Training [50][  140/  196]   Loss 0.390990   Top1 87.533482   Top5 98.914621   BatchTime 0.381965   LR 0.000590
0.67771697
0.67770278
0.67762047
0.67759085
0.67768347
0.67782009
0.67791814
0.67802781
0.67816442
0.67812753
0.67804021
0.67795366
0.67801702
0.67808723
0.67815399
0.67838353
0.67856848
INFO - Training [50][  160/  196]   Loss 0.394326   Top1 87.412109   Top5 98.918457   BatchTime 0.379100   LR 0.000585
0.67864585
0.67875916
0.67882168
0.67890316
0.67900950
0.68073541
0.68164527
0.68184751
0.68205470
0.68236250
0.68261427
0.68291450
0.68309605
0.68328905
0.68351948
0.68378502
0.68398398
0.68409908
0.68424588
0.68438911
0.68446612
0.68444705
0.68439674
INFO - Training [50][  180/  196]   Loss 0.395666   Top1 87.382812   Top5 98.865017   BatchTime 0.375781   LR 0.000580
0.68440682
0.68437624
0.68438625
0.68441254
0.68449086
0.68457347
0.68453562
0.68443650
0.68437588
0.68437475
0.68442196
INFO - ==> Top1: 87.390    Top5: 98.854    Loss: 0.395
0.68443769
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [50][   20/   40]   Loss 0.334174   Top1 89.335938   Top5 99.531250   BatchTime 0.189136
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0286)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0681)
features.2.conv.0 tensor(0.0486)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0854)
features.3.conv.0 tensor(0.0292)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0753)
features.4.conv.0 tensor(0.0718)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.0549)
features.5.conv.0 tensor(0.0671)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1681)
features.6.conv.0 tensor(0.0376)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0601)
features.7.conv.0 tensor(0.1081)
features.7.conv.3 tensor(0.1071)
features.7.conv.6 tensor(0.2318)
features.8.conv.0 tensor(0.1038)
features.8.conv.3 tensor(0.1045)
features.8.conv.6 tensor(0.3421)
features.9.conv.0 tensor(0.1429)
features.9.conv.3 tensor(0.1623)
features.9.conv.6 tensor(0.4237)
features.10.conv.0 tensor(0.0670)
features.10.conv.3 tensor(0.0854)
features.10.conv.6 tensor(0.1765)
features.11.conv.0 tensor(0.6415)
features.11.conv.3 tensor(0.1406)
features.11.conv.6 tensor(0.7295)
features.12.conv.0 tensor(0.6632)
features.12.conv.3 tensor(0.1148)
features.12.conv.6 tensor(0.8328)
features.13.conv.0 tensor(0.2408)
features.13.conv.3 tensor(0.1337)
features.13.conv.6 tensor(0.4604)
features.14.conv.0 tensor(0.9860)
features.14.conv.3 tensor(0.0898)
features.14.conv.6 tensor(0.9867)
features.15.conv.0 tensor(0.9794)
features.15.conv.3 tensor(0.0652)
features.15.conv.6 tensor(0.4698)
features.16.conv.0 tensor(0.2758)
features.16.conv.3 tensor(0.1095)
features.16.conv.6 tensor(0.5785)
conv.0 tensor(0.5844)
tensor(1251864.) 2188896.0
INFO - Validation [50][   40/   40]   Loss 0.321010   Top1 89.620000   Top5 99.640000   BatchTime 0.121268
INFO - ==> Top1: 89.620    Top5: 99.640    Loss: 0.321
INFO - ==> Sparsity : 0.572
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [49][Top1: 89.370   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
0.68441445
0.68428153
0.68416440
0.68406343
0.68401301
0.68394136
0.68392426
0.68391228
0.68377036
0.68370819
0.68366188
0.68361616
0.68356603
0.68364960
0.68374026
0.68387717
0.68406731
0.68415254
0.68425792
0.68436062
0.68451333
0.68461680
INFO - Training [51][   20/  196]   Loss 0.401859   Top1 87.167969   Top5 98.125000   BatchTime 0.400728   LR 0.000571
0.68482167
0.68511039
0.68541390
0.68569058
0.68593925
0.68610764
0.68619639
0.68624169
0.68627697
0.68633068
0.68631256
0.68627948
0.68626058
0.68617779
0.68608385
0.68598151
0.68585223
0.68578213
0.68566477
0.68560511
0.68553239
0.68546140
INFO - Training [51][   40/  196]   Loss 0.405587   Top1 87.187500   Top5 98.203125   BatchTime 0.383093   LR 0.000566
0.68536431
0.68527848
0.68516958
0.68502307
0.68479824
0.68463975
0.68454897
0.68440574
0.68425739
0.68408632
0.68392509
0.68372726
0.68357301
0.68338037
0.68320036
0.68299598
INFO - Training [51][   60/  196]   Loss 0.398222   Top1 87.363281   Top5 98.463542   BatchTime 0.377877   LR 0.000561
0.68279397
0.68256891
0.68239081
0.68223226
0.68215686
0.68205887
0.68191946
0.68181306
0.68168908
0.68146974
0.68135220
0.68120706
0.68105900
0.68099070
0.68089122
0.68082780
0.68074566
0.68066698
0.68051922
0.68049365
0.68052948
0.68050814
INFO - Training [51][   80/  196]   Loss 0.398368   Top1 87.236328   Top5 98.608398   BatchTime 0.376649   LR 0.000556
0.68044347
0.68037719
0.68032569
0.68022877
0.68020302
0.68007177
0.68004477
0.67992508
0.67986119
0.67969549
0.67950970
0.67946202
0.67957652
0.67967916
0.67973971
0.67957973
0.67939597
0.67953390
0.67945492
0.67958206
0.67959863
0.67963368
INFO - Training [51][  100/  196]   Loss 0.393108   Top1 87.410156   Top5 98.695312   BatchTime 0.373044   LR 0.000551
0.67975074
0.67985773
0.67976445
0.67969263
0.67949778
0.67934215
0.67924082
0.67916667
0.67914581
0.67918533
0.67921209
0.67930633
0.67930865
0.67927182
0.67922854
0.67914683
INFO - Training [51][  120/  196]   Loss 0.384801   Top1 87.682292   Top5 98.785807   BatchTime 0.372990   LR 0.000546
0.67908746
0.67904568
0.67904168
0.67904609
0.67904025
0.67913967
0.67925894
0.67934000
0.67939860
0.67937076
0.67936879
0.67943919
0.67958140
0.67961568
0.67947435
0.67934626
0.67929924
0.67933935
0.67929369
0.67921537
0.67912990
0.67909342
INFO - Training [51][  140/  196]   Loss 0.386200   Top1 87.670201   Top5 98.872768   BatchTime 0.372678   LR 0.000541
0.67896748
0.67890996
0.67894483
0.67891532
0.67888790
0.67882895
0.67874062
0.67875427
0.67883247
0.67897129
0.67896980
0.67902565
0.67910147
0.67909151
0.67920917
0.67929244
0.67939991
INFO - Training [51][  160/  196]   Loss 0.387098   Top1 87.590332   Top5 98.872070   BatchTime 0.370226   LR 0.000536
0.67944622
0.67952579
0.67953610
0.67960745
0.67967057
0.67973942
0.67976254
0.67975777
0.67970866
0.67965019
0.67961508
0.67961991
0.67963827
0.67961222
0.67961156
0.67966515
0.67978263
0.67993158
0.68004525
0.68030906
0.68058115
0.68478572
0.68467915
INFO - Training [51][  180/  196]   Loss 0.388102   Top1 87.534722   Top5 98.823785   BatchTime 0.368491   LR 0.000531
0.68464053
0.68458962
0.68458241
0.68455100
0.68452656
0.68448484
0.68431431
0.68424094
0.68430954
0.68440521
0.68451792
0.68468666
0.68484968
0.68499988
********************pre-trained*****************
INFO - ==> Top1: 87.580    Top5: 98.842    Loss: 0.387
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [51][   20/   40]   Loss 0.685636   Top1 79.628906   Top5 98.730469   BatchTime 0.141607
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.4746)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0472)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0874)
features.3.conv.0 tensor(0.0269)
features.3.conv.3 tensor(0.0386)
features.3.conv.6 tensor(0.0766)
features.4.conv.0 tensor(0.0719)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.0635)
features.5.conv.0 tensor(0.0719)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.1748)
features.6.conv.0 tensor(0.0382)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0597)
features.7.conv.0 tensor(0.1123)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.2473)
features.8.conv.0 tensor(0.1071)
features.8.conv.3 tensor(0.1042)
features.8.conv.6 tensor(0.3394)
features.9.conv.0 tensor(0.1427)
features.9.conv.3 tensor(0.1635)
features.9.conv.6 tensor(0.4382)
features.10.conv.0 tensor(0.0729)
features.10.conv.3 tensor(0.0871)
features.10.conv.6 tensor(0.0845)
features.11.conv.0 tensor(0.6316)
features.11.conv.3 tensor(0.1397)
features.11.conv.6 tensor(0.7356)
features.12.conv.0 tensor(0.6721)
features.12.conv.3 tensor(0.1136)
features.12.conv.6 tensor(0.8307)
features.13.conv.0 tensor(0.2097)
features.13.conv.3 tensor(0.1341)
features.13.conv.6 tensor(0.4597)
features.14.conv.0 tensor(0.9853)
features.14.conv.3 tensor(0.0890)
features.14.conv.6 tensor(0.9877)
features.15.conv.0 tensor(0.9796)
features.15.conv.3 tensor(0.0664)
features.15.conv.6 tensor(0.4688)
features.16.conv.0 tensor(0.3456)
features.16.conv.3 tensor(0.1103)
features.16.conv.6 tensor(0.5890)
conv.0 tensor(0.5753)
tensor(1258091.) 2188896.0
INFO - Validation [51][   40/   40]   Loss 0.670042   Top1 80.010000   Top5 98.760000   BatchTime 0.100928
INFO - ==> Top1: 80.010    Top5: 98.760    Loss: 0.670
INFO - ==> Sparsity : 0.575
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [49][Top1: 89.370   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
0.68510640
0.68521047
0.68525910
0.68525261
0.68528825
0.68526000
0.68535876
0.68536717
0.68536246
0.68537879
0.68541527
0.68542773
0.68548775
0.68558013
0.68560326
0.68553787
0.68545300
0.68539059
0.68529099
0.68522936
INFO - Training [52][   20/  196]   Loss 0.380544   Top1 87.656250   Top5 98.574219   BatchTime 0.436652   LR 0.000523
0.68522370
0.68518764
0.68530273
0.68533826
0.68540674
0.68537641
0.68530899
0.68532628
0.68527359
0.68522424
0.68514401
0.68507069
0.68506694
0.68504828
0.68507010
0.68512267
0.68520951
0.68536514
0.68559241
0.68586224
0.68617302
INFO - Training [52][   40/  196]   Loss 0.394957   Top1 87.177734   Top5 98.779297   BatchTime 0.362864   LR 0.000518
0.68642259
0.68658417
0.68672585
0.68686843
0.68699706
0.68719256
0.68728989
0.68730086
0.68728733
0.68737459
0.68736905
0.68745118
0.68737477
0.68726969
0.68718767
0.68716854
0.68710423
0.68713152
0.68716598
0.68709648
0.68713528
0.68714738
0.68719822
INFO - Training [52][   60/  196]   Loss 0.392508   Top1 87.187500   Top5 98.795573   BatchTime 0.359659   LR 0.000513
0.68720925
0.68719471
0.68723637
0.68713623
0.68695968
0.68681878
0.68668050
0.68665797
0.68665135
0.68656135
0.68647259
0.68650025
0.68651569
0.68650991
0.68660593
0.68666285
INFO - Training [52][   80/  196]   Loss 0.392150   Top1 87.285156   Top5 98.823242   BatchTime 0.360504   LR 0.000508
0.68680596
0.68684506
0.68675971
0.68672383
0.68675578
0.68678045
0.68680054
0.68671709
0.68657333
0.68640560
0.68624383
0.68600005
0.68580079
0.68556809
0.68527544
0.68497807
0.68477345
0.68452740
0.68436188
0.68426275
0.68404919
INFO - Training [52][  100/  196]   Loss 0.385264   Top1 87.523438   Top5 98.898438   BatchTime 0.362246   LR 0.000503
0.68391448
0.68358839
0.68331867
0.68311113
0.68289709
0.68267781
0.68243343
0.68215436
0.68181878
0.68140799
0.68089873
0.68062449
0.68039113
0.68026972
0.68007475
0.67985237
0.67965573
0.67944962
0.67927891
0.67925268
0.67929125
0.67938137
0.67955536
INFO - Training [52][  120/  196]   Loss 0.377777   Top1 87.819010   Top5 98.984375   BatchTime 0.363465   LR 0.000498
0.67963481
0.67966330
0.67965180
0.67967963
0.67977309
0.67987871
0.67999333
0.68015760
0.68031520
0.68051732
0.68071991
0.68089586
0.68105012
0.68109125
0.68114859
0.68107909
INFO - Training [52][  140/  196]   Loss 0.375854   Top1 87.932478   Top5 99.009487   BatchTime 0.362757   LR 0.000493
0.68094492
0.68088633
0.68076247
0.68072611
0.68078357
0.68082172
0.68082619
0.68070418
0.68060344
0.68037194
0.68013078
0.67989385
0.67958158
0.67925453
0.67903060
0.67891353
0.67892432
0.67886621
0.67880154
0.67865282
0.67851132
0.67829764
0.67812145
INFO - Training [52][  160/  196]   Loss 0.381411   Top1 87.795410   Top5 98.967285   BatchTime 0.361923   LR 0.000488
0.67793971
0.67778361
0.67757589
0.67732066
0.67710859
0.67700380
0.67694497
0.67688143
0.67680460
0.67676300
0.67671639
0.67657870
0.67642659
0.67645496
0.67648506
0.67653614
0.67665648
0.67676675
0.67691612
0.67710310
0.67737615
INFO - Training [52][  180/  196]   Loss 0.382488   Top1 87.782118   Top5 98.921441   BatchTime 0.362995   LR 0.000483
0.67764336
0.67780191
0.67802846
0.67820352
0.67839772
0.67841846
0.67857587
0.67875266
0.67886603
0.67893595
0.67895746
0.67895550
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 87.866    Top5: 98.912    Loss: 0.381
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [52][   20/   40]   Loss 0.408127   Top1 87.246094   Top5 99.375000   BatchTime 0.137857
INFO - Validation [52][   40/   40]   Loss 0.390231   Top1 87.500000   Top5 99.440000   BatchTime 0.099500
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.4824)
features.1.conv.0 tensor(0.0345)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0616)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0813)
features.3.conv.0 tensor(0.0286)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0736)
features.4.conv.0 tensor(0.0729)
features.4.conv.3 tensor(0.0868)
features.4.conv.6 tensor(0.0718)
features.5.conv.0 tensor(0.0671)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.1781)
features.6.conv.0 tensor(0.0418)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0580)
features.7.conv.0 tensor(0.1133)
features.7.conv.3 tensor(0.1085)
features.7.conv.6 tensor(0.2338)
features.8.conv.0 tensor(0.1090)
features.8.conv.3 tensor(0.1047)
features.8.conv.6 tensor(0.3498)
features.9.conv.0 tensor(0.1538)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.4338)
features.10.conv.0 tensor(0.0718)
features.10.conv.3 tensor(0.0877)
features.10.conv.6 tensor(0.1305)
features.11.conv.0 tensor(0.6634)
features.11.conv.3 tensor(0.1400)
features.11.conv.6 tensor(0.7406)
features.12.conv.0 tensor(0.6936)
features.12.conv.3 tensor(0.1125)
features.12.conv.6 tensor(0.8295)
features.13.conv.0 tensor(0.2237)
features.13.conv.3 tensor(0.1341)
features.13.conv.6 tensor(0.4673)
features.14.conv.0 tensor(0.9850)
features.14.conv.3 tensor(0.0882)
features.14.conv.6 tensor(0.9884)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.0671)
features.15.conv.6 tensor(0.4674)
features.16.conv.0 tensor(0.3578)
features.16.conv.3 tensor(0.1102)
features.16.conv.6 tensor(0.6225)
conv.0 tensor(0.5597)
tensor(1270228.) 2188896.0
INFO - ==> Top1: 87.500    Top5: 99.440    Loss: 0.390
INFO - ==> Sparsity : 0.580
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [49][Top1: 89.370   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
0.67899108
0.67903000
0.67902058
0.67898256
0.67899591
0.67899442
0.67904764
0.67915195
0.67921156
0.67940688
0.67970383
0.67988676
0.68012756
0.68038446
0.68057293
0.68066633
0.68073606
0.68081981
0.68090802
INFO - Training [53][   20/  196]   Loss 0.409583   Top1 86.386719   Top5 98.085938   BatchTime 0.490657   LR 0.000474
0.68100792
0.68097585
0.68096071
0.68099076
0.68101805
0.68108898
0.68118906
0.68132597
0.68138272
0.68138289
0.68134117
0.68124115
0.68110013
0.68091446
0.68075126
0.68060523
0.68049234
0.68037540
0.68024266
0.68017596
0.68012762
0.68011850
0.68004078
INFO - Training [53][   40/  196]   Loss 0.399228   Top1 87.187500   Top5 98.515625   BatchTime 0.420830   LR 0.000470
0.68002731
0.68006641
0.68017334
0.68018228
0.68018538
0.68010074
0.68001342
0.67996764
0.68000746
0.67994136
0.67985791
0.67976755
0.67967814
0.67958564
0.67946637
0.67931539
0.67914617
0.67901343
0.67889309
0.67880201
0.67866910
INFO - Training [53][   60/  196]   Loss 0.388484   Top1 87.617188   Top5 98.704427   BatchTime 0.404483   LR 0.000465
0.67864174
0.67857742
0.67848217
0.67840093
0.67831522
0.67816609
0.67814964
0.67820913
0.67825836
0.67823750
0.67820317
0.67810112
0.67806154
0.67799330
0.67791557
0.67778039
0.67755467
INFO - Training [53][   80/  196]   Loss 0.386526   Top1 87.553711   Top5 98.818359   BatchTime 0.394166   LR 0.000460
0.67729390
0.67710251
0.67692566
0.67677546
0.67661399
0.67652029
0.67642695
0.67634481
0.67631137
0.67624694
0.67623967
0.67621762
0.67621565
0.67626524
0.67625272
0.67623210
0.67623430
0.67616993
0.67610776
0.67605573
0.67606086
0.67601424
INFO - Training [53][  100/  196]   Loss 0.379078   Top1 87.820312   Top5 98.855469   BatchTime 0.389488   LR 0.000455
0.67600250
0.67599189
0.67599070
0.67602211
0.67600697
0.67601079
0.67604023
0.67606759
0.67612565
0.67613834
0.67607063
0.67607504
0.67617291
0.67623234
0.67636245
0.67650145
0.67670238
0.67679834
0.67691541
0.67701536
0.67700958
0.67694157
INFO - Training [53][  120/  196]   Loss 0.375039   Top1 88.027344   Top5 98.893229   BatchTime 0.383853   LR 0.000450
0.67675066
0.67665821
0.67648637
0.67634225
0.67628658
0.67617196
0.67609739
0.67594683
0.67584443
0.67569137
0.67549759
0.67529482
0.67517292
0.67501950
0.67492950
0.67499703
INFO - Training [53][  140/  196]   Loss 0.372466   Top1 88.147321   Top5 98.936942   BatchTime 0.381161   LR 0.000445
0.67500663
0.67505354
0.67507058
0.67514151
0.67525530
0.67536467
0.67546654
0.67557931
0.67561352
0.67554170
0.67545599
0.67534912
0.67513174
0.67499226
0.67490494
0.67480958
0.67469424
0.67465425
0.67472214
0.67478901
0.67487997
0.67488497
0.67498267
INFO - Training [53][  160/  196]   Loss 0.374665   Top1 88.037109   Top5 98.930664   BatchTime 0.378187   LR 0.000441
0.67507541
0.67526299
0.67526919
0.67528456
0.67528909
0.67527467
0.67521340
0.67515349
0.67509133
0.67503464
0.67498779
0.67491156
0.67499846
0.67510480
0.67520577
0.67526227
0.67539978
0.67547280
0.67557514
0.67571068
INFO - Training [53][  180/  196]   Loss 0.374053   Top1 88.096788   Top5 98.908420   BatchTime 0.379698   LR 0.000436
0.67588353
0.67614162
0.67627615
0.67627805
0.67637902
0.67650354
0.67659968
0.67664170
0.67671239
0.67680317
0.67686701
0.67683530
0.67675728
********************pre-trained*****************
INFO - ==> Top1: 88.140    Top5: 98.890    Loss: 0.372
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [53][   20/   40]   Loss 0.343118   Top1 89.238281   Top5 99.511719   BatchTime 0.130958
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0495)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0807)
features.3.conv.0 tensor(0.0298)
features.3.conv.3 tensor(0.0363)
features.3.conv.6 tensor(0.0723)
features.4.conv.0 tensor(0.0662)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.0885)
features.5.conv.0 tensor(0.0659)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.1742)
features.6.conv.0 tensor(0.0435)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0561)
features.7.conv.0 tensor(0.1158)
features.7.conv.3 tensor(0.1114)
features.7.conv.6 tensor(0.2467)
features.8.conv.0 tensor(0.1051)
features.8.conv.3 tensor(0.1036)
features.8.conv.6 tensor(0.3494)
features.9.conv.0 tensor(0.1570)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.4422)
features.10.conv.0 tensor(0.0666)
features.10.conv.3 tensor(0.0891)
features.10.conv.6 tensor(0.1641)
features.11.conv.0 tensor(0.6541)
features.11.conv.3 tensor(0.1397)
features.11.conv.6 tensor(0.7492)
features.12.conv.0 tensor(0.6788)
features.12.conv.3 tensor(0.1136)
features.12.conv.6 tensor(0.8251)
features.13.conv.0 tensor(0.2299)
features.13.conv.3 tensor(0.1329)
features.13.conv.6 tensor(0.4748)
features.14.conv.0 tensor(0.9844)
features.14.conv.3 tensor(0.0878)
features.14.conv.6 tensor(0.9885)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.0657)
features.15.conv.6 tensor(0.4461)
features.16.conv.0 tensor(0.4378)
features.16.conv.3 tensor(0.1108)
features.16.conv.6 tensor(0.5908)
conv.0 tensor(0.5476)
tensor(1266138.) 2188896.0
INFO - Validation [53][   40/   40]   Loss 0.328857   Top1 89.470000   Top5 99.590000   BatchTime 0.094438
INFO - ==> Top1: 89.470    Top5: 99.590    Loss: 0.329
INFO - ==> Sparsity : 0.578
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 89.470   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
0.67640358
0.67608333
0.67582059
0.67554075
0.67516798
0.67518514
0.67523199
0.67522794
0.67503917
0.67470980
0.67447716
0.67441034
0.67441052
0.67421901
0.67389870
0.67357153
0.67383277
0.67397195
0.67410296
0.67391855
0.67354256
0.67363799
0.67367744
0.67371416
INFO - Training [54][   20/  196]   Loss 0.396307   Top1 87.285156   Top5 98.457031   BatchTime 0.493671   LR 0.000427
0.67357892
0.67322564
0.67311305
0.67233896
0.67183495
0.67048681
0.66969949
0.66919899
0.66888613
0.66840458
0.66830289
0.66821247
0.66825378
0.66859585
0.66886169
0.66914862
0.66878867
0.66832292
0.66772163
0.66739726
INFO - Training [54][   40/  196]   Loss 0.395704   Top1 87.294922   Top5 98.681641   BatchTime 0.390428   LR 0.000423
0.66698402
0.66628623
0.66540629
0.66461271
0.66452956
0.66391653
0.66305351
0.66268742
0.66203868
0.66176105
0.66129696
0.66086370
0.66032869
0.66033256
0.66034454
0.66036415
0.66024888
0.66021234
0.66009957
0.66004604
INFO - Training [54][   60/  196]   Loss 0.393205   Top1 87.467448   Top5 98.704427   BatchTime 0.360133   LR 0.000418
0.66010904
0.66000557
0.65983921
0.65968060
0.65948033
0.65924799
0.65911841
0.65900385
0.65900004
0.65892792
0.65883583
0.65873235
0.65865570
0.65861458
0.65855736
0.65850741
0.65845317
INFO - Training [54][   80/  196]   Loss 0.381745   Top1 87.836914   Top5 98.813477   BatchTime 0.359769   LR 0.000413
0.65836352
0.65825635
0.65811729
0.65807110
0.65796900
0.65789461
0.65786052
0.65774012
0.65761232
0.65752423
0.65743792
0.65734786
0.65726411
0.65724230
0.65720516
0.65713161
0.65708429
0.65707678
0.65708834
0.65709990
0.65710700
INFO - Training [54][  100/  196]   Loss 0.376250   Top1 87.964844   Top5 98.839844   BatchTime 0.361702   LR 0.000408
0.65707195
0.65702850
0.65701663
0.65704608
0.65710843
0.65720516
0.65719628
0.65712243
0.65704894
0.65710175
0.65713292
0.65720749
0.65726751
0.65734017
0.65738958
0.65737110
0.65733296
0.65729570
0.65728974
0.65728432
0.65726113
0.65721047
INFO - Training [54][  120/  196]   Loss 0.371047   Top1 88.082682   Top5 98.893229   BatchTime 0.363104   LR 0.000404
0.65722865
0.65725899
0.65739024
0.65747994
0.65766054
0.65771484
0.65774739
0.65773690
0.65766364
0.65750778
0.65749490
0.65756404
0.65765285
0.65762997
0.65771651
0.65780067
INFO - Training [54][  140/  196]   Loss 0.369837   Top1 88.169643   Top5 98.928571   BatchTime 0.364097   LR 0.000399
0.65776980
0.65744847
0.65690339
0.65623146
0.65540111
0.65447426
0.65362412
0.65309811
0.65267110
0.65244621
0.65250283
0.65298337
0.65348941
0.65358174
0.65408915
0.65449977
0.65474325
0.65541452
0.65613139
0.65658456
0.65703094
0.65744007
INFO - Training [54][  160/  196]   Loss 0.372919   Top1 88.037109   Top5 98.935547   BatchTime 0.364507   LR 0.000394
0.65783226
0.65817279
0.65856183
0.65880400
0.65892720
0.65920478
0.65947974
0.65968615
0.65978444
0.65975761
0.65981567
0.65981799
0.65982187
0.65975666
0.65974069
0.65970087
0.65970784
INFO - Training [54][  180/  196]   Loss 0.373305   Top1 88.033854   Top5 98.895399   BatchTime 0.362281   LR 0.000390
0.65971845
0.65964448
0.65957224
0.65957028
0.65963340
0.65975982
0.65988326
0.65994138
0.66003811
0.66010958
0.66021442
0.66023278
0.66018498
0.66022295
0.66025841
0.66020167
0.66021657
********************pre-trained*****************
INFO - ==> Top1: 88.078    Top5: 98.886    Loss: 0.372
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [54][   20/   40]   Loss 0.446952   Top1 86.503906   Top5 99.179688   BatchTime 0.135624
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0293)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0298)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0729)
features.4.conv.0 tensor(0.0641)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.0972)
features.5.conv.0 tensor(0.0695)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.1755)
features.6.conv.0 tensor(0.0430)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0582)
features.7.conv.0 tensor(0.1082)
features.7.conv.3 tensor(0.1088)
features.7.conv.6 tensor(0.2682)
features.8.conv.0 tensor(0.1075)
features.8.conv.3 tensor(0.1033)
features.8.conv.6 tensor(0.3520)
features.9.conv.0 tensor(0.1629)
features.9.conv.3 tensor(0.1655)
features.9.conv.6 tensor(0.4438)
features.10.conv.0 tensor(0.0702)
features.10.conv.3 tensor(0.0862)
features.10.conv.6 tensor(0.1618)
features.11.conv.0 tensor(0.6436)
features.11.conv.3 tensor(0.1385)
features.11.conv.6 tensor(0.7382)
features.12.conv.0 tensor(0.7078)
features.12.conv.3 tensor(0.1148)
features.12.conv.6 tensor(0.8335)
features.13.conv.0 tensor(0.2468)
features.13.conv.3 tensor(0.1308)
features.13.conv.6 tensor(0.4628)
features.14.conv.0 tensor(0.9845)
features.14.conv.3 tensor(0.0865)
features.14.conv.6 tensor(0.9884)
features.15.conv.0 tensor(0.9801)
features.15.conv.3 tensor(0.0664)
features.15.conv.6 tensor(0.4352)
features.16.conv.0 tensor(0.6832)
features.16.conv.3 tensor(0.1090)
features.16.conv.6 tensor(0.6708)
conv.0 tensor(0.5866)
tensor(1344103.) 2188896.0
INFO - Validation [54][   40/   40]   Loss 0.421995   Top1 86.850000   Top5 99.370000   BatchTime 0.094285
INFO - ==> Top1: 86.850    Top5: 99.370    Loss: 0.422
INFO - ==> Sparsity : 0.614
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 89.470   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
0.66017759
0.66015965
0.66017687
0.66022640
0.66027957
0.66037631
0.66043144
0.66055876
0.66067392
0.66085631
0.66094410
0.66100770
0.66103679
0.66108048
0.66109389
0.66109830
0.66113859
0.66108489
0.66105872
0.66105741
0.66109073
0.66109955
0.66113663
INFO - Training [55][   20/  196]   Loss 0.377608   Top1 88.007812   Top5 98.378906   BatchTime 0.472620   LR 0.000381
0.66115189
0.66116995
0.66123873
0.66129535
0.66139859
0.66148120
0.66156137
0.66162556
0.66164541
0.66172332
0.66177559
0.66179222
0.66178757
0.66178626
0.66178548
0.66184109
0.66181427
0.66174293
0.66169375
0.66167080
0.66161752
INFO - Training [55][   40/  196]   Loss 0.381148   Top1 87.656250   Top5 98.574219   BatchTime 0.431056   LR 0.000377
0.66161078
0.66157645
0.66159260
0.66162050
0.66161418
0.66159427
0.66161209
0.66156411
0.66142035
0.66131461
0.66121340
0.66117454
0.66115534
0.66105998
0.66096199
0.66087317
0.66076022
0.66063058
0.66051620
INFO - Training [55][   60/  196]   Loss 0.382637   Top1 87.513021   Top5 98.645833   BatchTime 0.393191   LR 0.000372
0.66036057
0.66023439
0.66018605
0.66008770
0.66006655
0.66011626
0.66020709
0.66032118
0.66032290
0.66017866
0.66022903
0.66025299
0.66025096
0.66027063
0.66039151
0.66044450
0.66045415
0.66052216
INFO - Training [55][   80/  196]   Loss 0.380467   Top1 87.636719   Top5 98.779297   BatchTime 0.379002   LR 0.000368
0.66062707
0.66070116
0.66072536
0.66074216
0.66073090
0.66065216
0.66065240
0.66068012
0.66068155
0.66072130
0.66071075
0.66070980
0.66080374
0.66072530
0.66071767
0.66059899
0.66047150
0.66036659
0.66028553
0.66020060
0.66020685
INFO - Training [55][  100/  196]   Loss 0.375548   Top1 87.843750   Top5 98.843750   BatchTime 0.377542   LR 0.000363
0.66024679
0.66028076
0.66022134
0.66016752
0.66011578
0.66008168
0.66003126
0.65994745
0.65981686
0.65971142
0.65963799
0.65957719
0.65951520
0.65949512
0.65947586
0.65933615
0.65912366
0.65879041
0.65865237
0.65861142
0.65857655
0.65851974
INFO - Training [55][  120/  196]   Loss 0.367634   Top1 88.095703   Top5 98.902995   BatchTime 0.375319   LR 0.000358
0.65844053
0.65839803
0.65841633
0.65839702
0.65836608
0.65827382
0.65821564
0.65819669
0.65817124
0.65821940
0.65815824
0.65805030
0.65796733
0.65787971
0.65777546
0.65759546
INFO - Training [55][  140/  196]   Loss 0.366048   Top1 88.191964   Top5 98.956473   BatchTime 0.375894   LR 0.000354
0.65742254
0.65728629
0.65712672
0.65699250
0.65696746
0.65697539
0.65704489
0.65709764
0.65712482
0.65712839
0.65716946
0.65716690
0.65719163
0.65722615
0.65723133
0.65723664
0.65721714
0.65727162
0.65734726
0.65737820
0.65737456
INFO - Training [55][  160/  196]   Loss 0.369272   Top1 88.024902   Top5 98.925781   BatchTime 0.375921   LR 0.000349
0.65735680
0.65735590
0.65730619
0.65724587
0.65725034
0.65720481
0.65712649
0.65706593
0.65697545
0.65693653
0.65682447
0.65673733
0.65660006
0.65647805
0.65630156
0.65618008
0.65607470
0.65598142
0.65592235
0.65590698
0.65588671
0.65583408
INFO - Training [55][  180/  196]   Loss 0.370989   Top1 87.992622   Top5 98.891059   BatchTime 0.374895   LR 0.000345
0.65581602
0.65585744
0.65590847
0.65595239
0.65596527
0.65597200
0.65604937
0.65609944
0.65606344
0.65601087
0.65588909
0.65587753
0.65589744
********************pre-trained*****************
INFO - ==> Top1: 87.994    Top5: 98.914    Loss: 0.370
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [55][   20/   40]   Loss 0.350328   Top1 89.160156   Top5 99.531250   BatchTime 0.139086
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0280)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0793)
features.3.conv.0 tensor(0.0336)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.0738)
features.4.conv.0 tensor(0.0675)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.1248)
features.5.conv.0 tensor(0.0700)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.1756)
features.6.conv.0 tensor(0.0417)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0588)
features.7.conv.0 tensor(0.1217)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.2758)
features.8.conv.0 tensor(0.1080)
features.8.conv.3 tensor(0.1036)
features.8.conv.6 tensor(0.3626)
features.9.conv.0 tensor(0.1674)
features.9.conv.3 tensor(0.1670)
features.9.conv.6 tensor(0.4565)
features.10.conv.0 tensor(0.0727)
features.10.conv.3 tensor(0.0856)
features.10.conv.6 tensor(0.1630)
features.11.conv.0 tensor(0.6413)
features.11.conv.3 tensor(0.1373)
features.11.conv.6 tensor(0.7458)
features.12.conv.0 tensor(0.6861)
features.12.conv.3 tensor(0.1140)
features.12.conv.6 tensor(0.8336)
features.13.conv.0 tensor(0.2478)
features.13.conv.3 tensor(0.1306)
features.13.conv.6 tensor(0.5036)
features.14.conv.0 tensor(0.9849)
features.14.conv.3 tensor(0.0883)
features.14.conv.6 tensor(0.9892)
features.15.conv.0 tensor(0.9804)
features.15.conv.3 tensor(0.0677)
features.15.conv.6 tensor(0.4414)
features.16.conv.0 tensor(0.6795)
features.16.conv.3 tensor(0.1073)
features.16.conv.6 tensor(0.6811)
conv.0 tensor(0.5698)
tensor(1345460.) 2188896.0
INFO - Validation [55][   40/   40]   Loss 0.321709   Top1 89.540000   Top5 99.620000   BatchTime 0.094849
INFO - ==> Top1: 89.540    Top5: 99.620    Loss: 0.322
INFO - ==> Sparsity : 0.615
INFO - Scoreboard best 1 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [55][Top1: 89.540   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
0.65593743
0.65597868
0.65595955
0.65593427
0.65587091
0.65583050
0.65575367
0.65568370
0.65559042
0.65546620
0.65538317
0.65530533
0.65526944
0.65523231
0.65523261
0.65527529
0.65524817
0.65533042
0.65536219
0.65545046
0.65551591
0.65560555
0.65568560
INFO - Training [56][   20/  196]   Loss 0.391060   Top1 87.578125   Top5 98.593750   BatchTime 0.493634   LR 0.000337
0.65574509
0.65574473
0.65581518
0.65585792
0.65588909
0.65589410
0.65592897
0.65597934
0.65601510
0.65607506
0.65614587
0.65617424
0.65614164
0.65608710
0.65606058
0.65604645
INFO - Training [56][   40/  196]   Loss 0.384311   Top1 87.783203   Top5 98.750000   BatchTime 0.433766   LR 0.000333
0.65602928
0.65597701
0.65598446
0.65594512
0.65586537
0.65585667
0.65584612
0.65584666
0.65585530
0.65587372
0.65591675
0.65596241
0.65600878
0.65600473
0.65602720
0.65605229
0.65603745
0.65600711
0.65605468
0.65608704
0.65617371
0.65618974
INFO - Training [56][   60/  196]   Loss 0.382467   Top1 87.825521   Top5 98.834635   BatchTime 0.412031   LR 0.000328
0.65617549
0.65618306
0.65617985
0.65624958
0.65631831
0.65635628
0.65637374
0.65638310
0.65633184
0.65626550
0.65622073
0.65620983
0.65622973
0.65623808
0.65622890
0.65623355
0.65627235
0.65630931
0.65631181
0.65632665
0.65631413
INFO - Training [56][   80/  196]   Loss 0.380185   Top1 87.807617   Top5 98.984375   BatchTime 0.381451   LR 0.000324
0.65629351
0.65628064
0.65627766
0.65626192
0.65616953
0.65608025
0.65595943
0.65583587
0.65577239
0.65567589
0.65555340
0.65543795
0.65531522
0.65522218
0.65510625
0.65497375
0.65489513
0.65482140
INFO - Training [56][  100/  196]   Loss 0.377556   Top1 87.925781   Top5 98.988281   BatchTime 0.371996   LR 0.000319
0.65474892
0.65476871
0.65479332
0.65478057
0.65487885
0.65487051
0.65496516
0.65505081
0.65507340
0.65504491
0.65498120
0.65489453
0.65476733
0.65468705
0.65456486
0.65458113
0.65455776
0.65456349
0.65463603
0.65478128
0.65532261
0.65687245
0.65690345
0.65692461
INFO - Training [56][  120/  196]   Loss 0.371280   Top1 88.147786   Top5 99.029948   BatchTime 0.364956   LR 0.000315
0.65687984
0.65689504
0.65698749
0.65707070
0.65712661
0.65715325
0.65717477
0.65717149
0.65723282
0.65724957
0.65724802
0.65724361
0.65725166
0.65730524
0.65737456
0.65738672
INFO - Training [56][  140/  196]   Loss 0.368559   Top1 88.270089   Top5 99.048549   BatchTime 0.366124   LR 0.000311
0.65737271
0.65736955
0.65734738
0.65732270
0.65728301
0.65728736
0.65725851
0.65727133
0.65727472
0.65729100
0.65735334
0.65741694
0.65746564
0.65755463
0.65760803
0.65763742
0.65766066
0.65774596
0.65770882
0.65767187
0.65759254
0.65756506
INFO - Training [56][  160/  196]   Loss 0.370541   Top1 88.220215   Top5 99.023438   BatchTime 0.365239   LR 0.000306
0.65760660
0.65747219
0.65738827
0.65728384
0.65728498
0.65723586
0.65710980
0.65711093
0.65719563
0.65717369
0.65708864
0.65703553
0.65703470
0.65698099
0.65690982
0.65689671
0.65665394
0.65663892
0.65660030
0.65658110
0.65653676
INFO - Training [56][  180/  196]   Loss 0.370207   Top1 88.250868   Top5 98.984375   BatchTime 0.366545   LR 0.000302
0.65647870
0.65652162
0.65655488
0.65647429
0.65635401
0.65632588
0.65631950
0.65632153
0.65636307
0.65636957
0.65643626
0.65654445
0.65665007
********************pre-trained*****************
INFO - ==> Top1: 88.320    Top5: 98.972    Loss: 0.368
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [56][   20/   40]   Loss 0.326570   Top1 90.078125   Top5 99.667969   BatchTime 0.145655
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.4824)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0501)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0310)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0697)
features.4.conv.0 tensor(0.0659)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.1016)
features.5.conv.0 tensor(0.0628)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.1906)
features.6.conv.0 tensor(0.0415)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0564)
features.7.conv.0 tensor(0.1301)
features.7.conv.3 tensor(0.1079)
features.7.conv.6 tensor(0.2691)
features.8.conv.0 tensor(0.1115)
features.8.conv.3 tensor(0.1016)
features.8.conv.6 tensor(0.3777)
features.9.conv.0 tensor(0.1699)
features.9.conv.3 tensor(0.1646)
features.9.conv.6 tensor(0.4598)
features.10.conv.0 tensor(0.0736)
features.10.conv.3 tensor(0.0830)
features.10.conv.6 tensor(0.1792)
features.11.conv.0 tensor(0.6511)
features.11.conv.3 tensor(0.1356)
features.11.conv.6 tensor(0.7482)
features.12.conv.0 tensor(0.7249)
features.12.conv.3 tensor(0.1140)
features.12.conv.6 tensor(0.8406)
features.13.conv.0 tensor(0.2525)
features.13.conv.3 tensor(0.1321)
features.13.conv.6 tensor(0.4789)
features.14.conv.0 tensor(0.9845)
features.14.conv.3 tensor(0.0873)
features.14.conv.6 tensor(0.9895)
features.15.conv.0 tensor(0.9807)
features.15.conv.3 tensor(0.0669)
INFO - Validation [56][   40/   40]   Loss 0.306005   Top1 90.270000   Top5 99.720000   BatchTime 0.105979
INFO - ==> Top1: 90.270    Top5: 99.720    Loss: 0.306
INFO - ==> Sparsity : 0.609
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
features.15.conv.6 tensor(0.4274)
features.16.conv.0 tensor(0.6684)
features.16.conv.3 tensor(0.1065)
features.16.conv.6 tensor(0.6258)
conv.0 tensor(0.5865)
tensor(1333740.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
0.65691143
0.65706164
0.65716362
0.65723419
0.65729547
0.65738714
0.65725267
0.65727293
0.65720379
0.65721065
0.65731972
0.65735692
0.65735644
0.65729791
0.65735012
0.65740001
0.65716767
0.65701944
0.65697956
0.65696949
0.65696681
0.65706527
INFO - Training [57][   20/  196]   Loss 0.385956   Top1 87.246094   Top5 98.515625   BatchTime 0.448067   LR 0.000294
0.65717107
0.65714860
0.65714240
0.65724832
0.65716988
0.65707660
0.65710217
0.65695149
0.65670812
0.65664995
0.65665984
0.65652424
0.65643489
0.65635258
0.65633053
0.65634030
0.65634704
0.65631521
INFO - Training [57][   40/  196]   Loss 0.379663   Top1 87.753906   Top5 98.642578   BatchTime 0.398410   LR 0.000290
0.65626675
0.65617198
0.65612280
0.65610301
0.65605253
0.65588993
0.65577006
0.65564591
0.65555811
0.65547460
0.65541357
0.65534031
0.65529728
0.65525067
0.65515858
0.65520489
0.65522003
0.65518928
0.65520626
0.65513319
0.65511870
0.65505743
INFO - Training [57][   60/  196]   Loss 0.378718   Top1 87.675781   Top5 98.743490   BatchTime 0.385212   LR 0.000286
0.65500283
0.65494186
0.65491670
0.65489662
0.65482157
0.65474272
0.65461576
0.65445524
0.65435976
0.65426755
0.65411711
0.65397048
0.65383935
0.65371180
0.65360397
0.65361398
0.65357727
0.65348113
0.65347266
0.65343559
INFO - Training [57][   80/  196]   Loss 0.375548   Top1 87.822266   Top5 98.862305   BatchTime 0.363953   LR 0.000282
0.65329868
0.65326691
0.65317351
0.65301269
0.65296900
0.65291721
0.65290803
0.65285540
0.65277255
0.65273905
0.65280408
0.65281010
0.65272766
0.65263283
0.65257490
0.65254766
0.65248573
0.65241009
0.65232033
0.65227181
0.65225041
0.65218633
INFO - Training [57][  100/  196]   Loss 0.371304   Top1 87.980469   Top5 98.929688   BatchTime 0.362627   LR 0.000277
0.65211183
0.65215087
0.65227014
0.65234655
0.65237427
0.65246302
0.65250194
0.65261239
0.65271264
0.65277654
0.65284103
0.65293831
0.65297091
0.65301120
0.65296751
0.65285796
0.65287262
INFO - Training [57][  120/  196]   Loss 0.363153   Top1 88.326823   Top5 98.974609   BatchTime 0.361401   LR 0.000273
0.65279567
0.65275586
0.65272397
0.65266007
0.65266025
0.65264946
0.65255815
0.65251833
0.65252179
0.65258920
0.65260929
0.65255177
0.65258729
0.65270692
0.65266824
0.65261698
0.65262908
0.65269154
0.65268660
0.65267128
0.65271246
0.65278822
0.65274978
INFO - Training [57][  140/  196]   Loss 0.359799   Top1 88.390067   Top5 99.034598   BatchTime 0.361064   LR 0.000269
0.65271848
0.65268010
0.65270817
0.65268672
0.65264171
0.65259296
0.65252322
0.65251285
0.65247107
0.65237367
0.65234554
0.65228295
0.65215862
0.65216976
0.65211052
0.65217334
INFO - Training [57][  160/  196]   Loss 0.360656   Top1 88.315430   Top5 99.020996   BatchTime 0.361385   LR 0.000265
0.65224200
0.65226793
0.65227473
0.65225691
0.65230763
0.65235966
0.65241724
0.65244091
0.65253335
0.65251070
0.65246528
0.65247267
0.65250558
0.65250397
0.65255177
0.65255255
0.65254694
0.65261829
0.65271133
0.65273440
INFO - Training [57][  180/  196]   Loss 0.361149   Top1 88.315972   Top5 98.953993   BatchTime 0.365700   LR 0.000261
0.65277302
0.65285271
0.65295547
0.65295547
0.65283883
0.65305144
0.65297455
0.65295976
0.65289420
0.65284044
0.65283257
0.65279871
0.65276015
0.65274787
0.65272212
0.65269017
********************pre-trained*****************
INFO - ==> Top1: 88.342    Top5: 98.958    Loss: 0.360
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [57][   20/   40]   Loss 0.421748   Top1 87.246094   Top5 99.355469   BatchTime 0.133355
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.4746)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0796)
features.3.conv.0 tensor(0.0321)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0707)
features.4.conv.0 tensor(0.0706)
features.4.conv.3 tensor(0.0828)
features.4.conv.6 tensor(0.1239)
features.5.conv.0 tensor(0.0635)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.1938)
features.6.conv.0 tensor(0.0409)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0575)
features.7.conv.0 tensor(0.1309)
features.7.conv.3 tensor(0.1094)
features.7.conv.6 tensor(0.2675)
features.8.conv.0 tensor(0.1189)
features.8.conv.3 tensor(0.1013)
features.8.conv.6 tensor(0.3652)
features.9.conv.0 tensor(0.1702)
features.9.conv.3 tensor(0.1620)
features.9.conv.6 tensor(0.4681)
features.10.conv.0 tensor(0.0756)
features.10.conv.3 tensor(0.0868)
features.10.conv.6 tensor(0.1669)
features.11.conv.0 tensor(0.6501)
features.11.conv.3 tensor(0.1364)
features.11.conv.6 tensor(0.7585)
features.12.conv.0 tensor(0.7061)
features.12.conv.3 tensor(0.1134)
features.12.conv.6 tensor(0.8456)
features.13.conv.0 tensor(0.2605)
features.13.conv.3 tensor(0.1316)
features.13.conv.6 tensor(0.4791)
features.14.conv.0 tensor(0.9847)
features.14.conv.3 tensor(0.0860)
features.14.conv.6 tensor(0.9894)
features.15.conv.0 tensor(0.9809)
features.15.conv.3 tensor(0.0676)
features.15.conv.6 tensor(0.4337)
features.16.conv.0 tensor(0.6815)
features.16.conv.3 tensor(0.1064)
features.16.conv.6 tensor(0.6705)
conv.0 tensor(0.6023)
tensor(1357075.) 2188896.0
INFO - Validation [57][   40/   40]   Loss 0.397188   Top1 87.550000   Top5 99.500000   BatchTime 0.094220
INFO - ==> Top1: 87.550    Top5: 99.500    Loss: 0.397
INFO - ==> Sparsity : 0.620
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 89.620   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
0.65267283
0.65267754
0.65269136
0.65268880
0.65267175
0.65262425
0.65258986
0.65253311
0.65250343
0.65247303
0.65244609
0.65241337
0.65242994
0.65245539
0.65245688
0.65244156
0.65241247
0.65240103
0.65240663
0.65245342
0.65251118
INFO - Training [58][   20/  196]   Loss 0.378878   Top1 87.363281   Top5 98.339844   BatchTime 0.449183   LR 0.000254
0.65254837
0.65257937
0.65257096
0.65249842
0.65243596
0.65239888
0.65232658
0.65224618
0.65214753
0.65206409
0.65200281
0.65190130
0.65183222
0.65174979
0.65169197
0.65163606
0.65157413
0.65149337
0.65145236
0.65145445
0.65145582
0.65145320
INFO - Training [58][   40/  196]   Loss 0.372648   Top1 87.880859   Top5 98.632812   BatchTime 0.404396   LR 0.000250
0.65150428
0.65149778
0.65149933
0.65149415
0.65150380
0.65153581
0.65152049
0.65151405
0.65148222
0.65147918
0.65152067
0.65153122
0.65153468
0.65152031
0.65149397
0.65146214
0.65142184
INFO - Training [58][   60/  196]   Loss 0.362397   Top1 88.268229   Top5 98.723958   BatchTime 0.391287   LR 0.000246
0.65136683
0.65133905
0.65128165
0.65121472
0.65115535
0.65109563
0.65102577
0.65098560
0.65097177
0.65099639
0.65092301
0.65093184
0.65090436
0.65088171
0.65082663
0.65078497
0.65067023
0.65062976
0.65051788
0.65056163
0.65056992
0.65056473
0.65045941
0.65030766
INFO - Training [58][   80/  196]   Loss 0.361129   Top1 88.344727   Top5 98.823242   BatchTime 0.376981   LR 0.000242
0.65010649
0.65001255
0.65008914
0.65014917
0.65034062
0.65041399
0.65040779
0.65046513
0.65051228
0.65041095
0.65039843
0.65037942
0.65034193
0.65044212
0.65045726
0.65046108
0.65052426
0.65062958
0.65073001
INFO - Training [58][  100/  196]   Loss 0.356168   Top1 88.566406   Top5 98.871094   BatchTime 0.364428   LR 0.000238
0.65084332
0.65093356
0.65102059
0.65118647
0.65132481
0.65140802
0.65145290
0.65146637
0.65146649
0.65157533
0.65152842
0.65152317
0.65155470
0.65156692
0.65149838
0.65153748
0.65142936
0.65138614
0.65133989
0.65131730
0.65123498
0.65113461
INFO - Training [58][  120/  196]   Loss 0.352240   Top1 88.688151   Top5 98.929036   BatchTime 0.364165   LR 0.000234
0.65114188
0.65112811
0.65108567
0.65099257
0.65091634
0.65082991
0.65077758
0.65073913
0.65075123
0.65072668
0.65067530
0.65060169
0.65051782
0.65044075
0.65040559
0.65039217
0.65037513
INFO - Training [58][  140/  196]   Loss 0.351088   Top1 88.769531   Top5 99.003906   BatchTime 0.363198   LR 0.000230
0.65042990
0.65044117
0.65043056
0.65045816
0.65045607
0.65037996
0.65040755
0.65031278
0.65024465
0.65015703
0.65018773
0.65020806
0.65035141
0.65043408
0.65052629
0.65059048
0.65070486
0.65072048
0.65073889
0.65081853
0.65074170
0.65068686
INFO - Training [58][  160/  196]   Loss 0.356908   Top1 88.620605   Top5 98.962402   BatchTime 0.362113   LR 0.000226
0.65059328
0.65046459
0.65052545
0.65050691
0.65053624
0.65057498
0.65064520
0.65069836
0.65069425
0.65072668
0.65077317
0.65076548
0.65080470
0.65086466
0.65089846
0.65098721
INFO - Training [58][  180/  196]   Loss 0.356666   Top1 88.643663   Top5 98.967014   BatchTime 0.362760   LR 0.000222
0.65106124
0.65114075
0.65125936
0.65126759
0.65132469
0.65136731
0.65133393
0.65138757
0.65141857
0.65140569
0.65149993
0.65149659
0.65140253
0.65137708
0.65126365
0.65123314
********************pre-trained*****************
INFO - ==> Top1: 88.640    Top5: 98.968    Loss: 0.356
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [58][   20/   40]   Loss 0.334467   Top1 89.238281   Top5 99.511719   BatchTime 0.135606
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.4844)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0790)
features.3.conv.0 tensor(0.0304)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0705)
features.4.conv.0 tensor(0.0768)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.1200)
features.5.conv.0 tensor(0.0649)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.1916)
features.6.conv.0 tensor(0.0400)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0580)
features.7.conv.0 tensor(0.1283)
features.7.conv.3 tensor(0.1076)
features.7.conv.6 tensor(0.2793)
features.8.conv.0 tensor(0.1226)
features.8.conv.3 tensor(0.0969)
features.8.conv.6 tensor(0.3720)
features.9.conv.0 tensor(0.1760)
features.9.conv.3 tensor(0.1606)
features.9.conv.6 tensor(0.4585)
features.10.conv.0 tensor(0.0767)
features.10.conv.3 tensor(0.0862)
features.10.conv.6 tensor(0.1631)
features.11.conv.0 tensor(0.6708)
features.11.conv.3 tensor(0.1368)
features.11.conv.6 tensor(0.7575)
features.12.conv.0 tensor(0.7018)
features.12.conv.3 tensor(0.1155)
features.12.conv.6 tensor(0.8462)
features.13.conv.0 tensor(0.2532)
features.13.conv.3 tensor(0.1308)
features.13.conv.6 tensor(0.4944)
features.14.conv.0 tensor(0.9845)
features.14.conv.3 tensor(0.0861)
features.14.conv.6 tensor(0.9897)
features.15.conv.0 tensor(0.9809)
features.15.conv.3 tensor(0.0659)
features.15.conv.6 tensor(0.4429)
features.16.conv.0 tensor(0.6800)
features.16.conv.3 tensor(0.1054)
features.16.conv.6 tensor(0.6827)
conv.0 tensor(0.6019)
tensor(1363952.) 2188896.0
INFO - Validation [58][   40/   40]   Loss 0.315377   Top1 89.710000   Top5 99.620000   BatchTime 0.095910
INFO - ==> Top1: 89.710    Top5: 99.620    Loss: 0.315
INFO - ==> Sparsity : 0.623
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 89.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
0.65122378
0.65116614
0.65114921
0.65115643
0.65113962
0.65117747
0.65118718
0.65115803
0.65123421
0.65127045
0.65131050
0.65129584
0.65128213
0.65129852
0.65127897
0.65120322
0.65116996
0.65115201
0.65102965
0.65091664
0.65079188
INFO - Training [59][   20/  196]   Loss 0.377868   Top1 87.382812   Top5 98.554688   BatchTime 0.450371   LR 0.000215
0.65069258
0.65056759
0.65054947
0.65049732
0.65041989
0.65032613
0.65026796
0.65018898
0.65009785
0.65005887
0.64997447
0.64983034
0.64975178
0.64975244
0.64965105
0.64954484
0.64949530
0.64948219
0.64943659
0.64942133
0.64949256
INFO - Training [59][   40/  196]   Loss 0.366024   Top1 87.900391   Top5 98.750000   BatchTime 0.413598   LR 0.000212
0.64943951
0.64945745
0.64953858
0.64948612
0.64944023
0.64938712
0.64931500
0.64924806
0.64924186
0.64921421
0.64921868
0.64921635
0.64919329
0.64920723
0.64923108
0.64923418
0.64918774
0.64922673
INFO - Training [59][   60/  196]   Loss 0.357050   Top1 88.509115   Top5 98.756510   BatchTime 0.391637   LR 0.000208
0.64927137
0.64927763
0.64918238
0.64899021
0.64887065
0.64871722
0.64854443
0.64842671
0.64833146
0.64825904
0.64821726
0.64820462
0.64831859
0.64827359
0.64828020
0.64835191
0.64836144
0.64842802
0.64848095
0.64864415
INFO - Training [59][   80/  196]   Loss 0.355229   Top1 88.569336   Top5 98.881836   BatchTime 0.391657   LR 0.000204
0.64878935
0.64891404
0.64896953
0.64914280
0.64912724
0.64922321
0.64927047
0.64930850
0.64933944
0.64937729
0.64942479
0.64947420
0.64945608
0.64946425
0.64952457
0.64956260
0.64954340
0.64955342
0.64961940
0.64966404
0.64969569
0.64972079
0.64971501
0.64969909
0.64971584
INFO - Training [59][  100/  196]   Loss 0.351857   Top1 88.660156   Top5 98.898438   BatchTime 0.379288   LR 0.000201
0.64974594
0.64973825
0.64976192
0.64976364
0.64977551
0.64982647
0.64980292
0.64982212
0.64981306
0.64983010
0.64984083
0.64986241
0.64977962
0.64980656
0.64985305
0.64986813
0.64990193
INFO - Training [59][  120/  196]   Loss 0.348707   Top1 88.860677   Top5 98.958333   BatchTime 0.371392   LR 0.000197
0.64994627
0.65000606
0.65006793
0.65010262
0.65013462
0.65016121
0.65015078
0.65021259
0.65018147
0.65021670
0.65020967
0.65024263
0.65021801
0.65028632
0.65032536
0.65031970
0.65039366
0.65042365
0.65044630
0.65049529
0.65053707
0.65060174
0.65064627
INFO - Training [59][  140/  196]   Loss 0.349577   Top1 88.856027   Top5 99.012277   BatchTime 0.368884   LR 0.000193
0.65067822
0.65075845
0.65080875
0.65088087
0.65093529
0.65100938
0.65101755
0.65108794
0.65116346
0.65110111
0.65106946
0.65100896
0.65095389
0.65089375
0.65091985
0.65093130
0.65090197
INFO - Training [59][  160/  196]   Loss 0.351482   Top1 88.781738   Top5 99.018555   BatchTime 0.366316   LR 0.000190
0.65091968
0.65088838
0.65093637
0.65091145
0.65093052
0.65091103
0.65092617
0.65090328
0.65092289
0.65092385
0.65090138
0.65093362
0.65095997
0.65093106
0.65093929
0.65092695
0.65087742
0.65088183
0.65089083
0.65087861
0.65089214
0.65089560
INFO - Training [59][  180/  196]   Loss 0.349113   Top1 88.875868   Top5 99.008247   BatchTime 0.364549   LR 0.000186
0.65089750
0.65092504
0.65093577
0.65091699
0.65098244
0.65098131
0.65095198
0.65088993
0.65087765
0.65082079
0.65075833
0.65074694
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 88.920    Top5: 99.012    Loss: 0.348
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [59][   20/   40]   Loss 0.471981   Top1 86.367188   Top5 99.082031   BatchTime 0.130998
INFO - Validation [59][   40/   40]   Loss 0.454212   Top1 86.400000   Top5 99.230000   BatchTime 0.091705
INFO - ==> Top1: 86.400    Top5: 99.230    Loss: 0.454
INFO - ==> Sparsity : 0.622
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 89.710   Top5: 99.620]
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.4824)
features.1.conv.0 tensor(0.0339)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0638)
features.2.conv.0 tensor(0.0469)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0784)
features.3.conv.0 tensor(0.0318)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.0707)
features.4.conv.0 tensor(0.0710)
features.4.conv.3 tensor(0.0851)
features.4.conv.6 tensor(0.1217)
features.5.conv.0 tensor(0.0716)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.2021)
features.6.conv.0 tensor(0.0412)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0573)
features.7.conv.0 tensor(0.1269)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.2765)
features.8.conv.0 tensor(0.1272)
features.8.conv.3 tensor(0.0995)
features.8.conv.6 tensor(0.3725)
features.9.conv.0 tensor(0.1788)
features.9.conv.3 tensor(0.1609)
features.9.conv.6 tensor(0.4558)
features.10.conv.0 tensor(0.0758)
features.10.conv.3 tensor(0.0871)
features.10.conv.6 tensor(0.1803)
features.11.conv.0 tensor(0.6604)
features.11.conv.3 tensor(0.1372)
features.11.conv.6 tensor(0.7565)
features.12.conv.0 tensor(0.7158)
features.12.conv.3 tensor(0.1138)
features.12.conv.6 tensor(0.8457)
features.13.conv.0 tensor(0.2759)
features.13.conv.3 tensor(0.1316)
features.13.conv.6 tensor(0.5038)
features.14.conv.0 tensor(0.9842)
features.14.conv.3 tensor(0.0854)
features.14.conv.6 tensor(0.9900)
features.15.conv.0 tensor(0.9812)
features.15.conv.3 tensor(0.0657)
features.15.conv.6 tensor(0.4430)
features.16.conv.0 tensor(0.6834)
features.16.conv.3 tensor(0.1061)
features.16.conv.6 tensor(0.6530)
conv.0 tensor(0.6087)
tensor(1361176.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
0.65082484
0.65087163
0.65086520
0.65086782
0.65088177
0.65084857
0.65083814
0.65080017
0.65075809
0.65070963
0.65072423
0.65070719
0.65063542
0.65067214
0.65070695
0.65062243
0.65057015
0.65052342
0.65049809
0.65045828
0.65042704
0.65039098
0.65036362
INFO - Training [60][   20/  196]   Loss 0.368860   Top1 87.812500   Top5 98.574219   BatchTime 0.437908   LR 0.000180
0.65035570
0.65036088
0.65033996
0.65039176
0.65037233
0.65034431
0.65034372
0.65035343
0.65037155
0.65038681
0.65034229
0.65032005
0.65025777
0.65024042
0.65017468
0.65016329
0.65017325
0.65015632
0.65013433
0.65018439
0.65023726
0.65027219
INFO - Training [60][   40/  196]   Loss 0.370607   Top1 88.066406   Top5 98.662109   BatchTime 0.401037   LR 0.000176
0.65019214
0.65021563
0.65017295
0.65015143
0.65011543
0.65006602
0.65001947
0.65000492
0.64996064
0.64989203
0.64986259
0.64979458
0.64968997
0.64966369
0.64962602
0.64958072
INFO - Training [60][   60/  196]   Loss 0.364803   Top1 88.320312   Top5 98.736979   BatchTime 0.387413   LR 0.000173
0.64951855
0.64944392
0.64940387
0.64933789
0.64927900
0.64914030
0.64903808
0.64895159
0.64889294
0.64886612
0.64876479
0.64875221
0.64868432
0.64863676
0.64854866
0.64855689
0.64854580
0.64860970
0.64865124
0.64867353
0.64868009
0.64866334
0.64867443
INFO - Training [60][   80/  196]   Loss 0.360022   Top1 88.452148   Top5 98.862305   BatchTime 0.378119   LR 0.000169
0.64872307
0.64869970
0.64870918
0.64875329
0.64876944
0.64872169
0.64869058
0.64870524
0.64877063
0.64879602
0.64881796
0.64883918
0.64891273
0.64893776
0.64891726
0.64888763
0.64893770
INFO - Training [60][  100/  196]   Loss 0.354954   Top1 88.703125   Top5 98.921875   BatchTime 0.372752   LR 0.000166
0.64891928
0.64893395
0.64893138
0.64891553
0.64889318
0.64888340
0.64881104
0.64876044
0.64871645
0.64869052
0.64864719
0.64861017
0.64857143
0.64851826
0.64848691
0.64842248
0.64841902
0.64840907
0.64836097
INFO - Training [60][  120/  196]   Loss 0.350074   Top1 88.880208   Top5 98.961589   BatchTime 0.363865   LR 0.000162
0.64834017
0.64826250
0.64825743
0.64819777
0.64811260
0.64807004
0.64805484
0.64797795
0.64792877
0.64786476
0.64780408
0.64780790
0.64779848
0.64777201
0.64782768
0.64781201
0.64769948
0.64778215
0.64782059
0.64781868
0.64782983
0.64781231
0.64788586
INFO - Training [60][  140/  196]   Loss 0.347337   Top1 88.909040   Top5 99.031808   BatchTime 0.360919   LR 0.000159
0.64790177
0.64792621
0.64790761
0.64784807
0.64778894
0.64777207
0.64775008
0.64773041
0.64770669
0.64768636
0.64765406
0.64761567
0.64763105
0.64764875
0.64768344
0.64770341
0.64770371
0.64768523
0.64761198
0.64757776
0.64746058
0.64760065
INFO - Training [60][  160/  196]   Loss 0.348751   Top1 88.869629   Top5 99.055176   BatchTime 0.362698   LR 0.000156
0.64749742
0.64744288
0.64737612
0.64736021
0.64731669
0.64725929
0.64720613
0.64718413
0.64710265
0.64706421
0.64703953
0.64702290
0.64700031
0.64698464
0.64703965
0.64705503
INFO - Training [60][  180/  196]   Loss 0.349806   Top1 88.854167   Top5 99.027778   BatchTime 0.363320   LR 0.000152
0.64703226
0.64701927
0.64701730
0.64699835
0.64695561
0.64691228
0.64686507
0.64689445
0.64690942
0.64680272
0.64673412
0.64671111
0.64666969
0.64662206
0.64656740
INFO - ==> Top1: 88.874    Top5: 99.030    Loss: 0.350
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.4805)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0472)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0775)
features.3.conv.0 tensor(0.0304)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0684)
features.4.conv.0 tensor(0.0724)
features.4.conv.3 tensor(0.0868)
features.4.conv.6 tensor(0.1117)
features.5.conv.0 tensor(0.0674)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.1989)
features.6.conv.0 tensor(0.0422)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0578)
features.7.conv.0 tensor(0.1323)
features.7.conv.3 tensor(0.1062)
features.7.conv.6 tensor(0.2940)
features.8.conv.0 tensor(0.1310)
features.8.conv.3 tensor(0.0987)
features.8.conv.6 tensor(0.3905)
features.9.conv.0 tensor(0.1786)
features.9.conv.3 tensor(0.1626)
features.9.conv.6 tensor(0.4541)
features.10.conv.0 tensor(0.0763)
features.10.conv.3 tensor(0.0874)
features.10.conv.6 tensor(0.1776)
features.11.conv.0 tensor(0.6814)
features.11.conv.3 tensor(0.1370)
features.11.conv.6 tensor(0.7595)
features.12.conv.0 tensor(0.7259)
features.12.conv.3 tensor(0.1127)
features.12.conv.6 tensor(0.8462)
features.13.conv.0 tensor(0.3101)
features.13.conv.3 tensor(0.1298)
features.13.conv.6 tensor(0.5046)
features.14.conv.0 tensor(0.9840)
features.14.conv.3 tensor(0.0851)
features.14.conv.6 tensor(0.9902)
features.15.conv.0 tensor(0.9812)
features.15.conv.3 tensor(0.0664)
features.15.conv.6 tensor(0.4436)
features.16.conv.0 tensor(0.6826)
features.16.conv.3 tensor(0.1063)
features.16.conv.6 tensor(0.6574)
conv.0 tensor(0.6305)
tensor(1376160.) 2188896.0
INFO - Validation [60][   20/   40]   Loss 0.385344   Top1 88.515625   Top5 99.355469   BatchTime 0.139028
INFO - Validation [60][   40/   40]   Loss 0.365289   Top1 88.520000   Top5 99.520000   BatchTime 0.097225
INFO - ==> Top1: 88.520    Top5: 99.520    Loss: 0.365
INFO - ==> Sparsity : 0.629
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 89.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
0.64652669
0.64651233
0.64645821
0.64642698
0.64633143
0.64629132
0.64627033
0.64623857
0.64621621
0.64623749
0.64624536
0.64628196
0.64626819
0.64643919
0.64650267
0.64653736
0.64649296
0.64647472
0.64645183
0.64646238
0.64650434
0.64655191
INFO - Training [61][   20/  196]   Loss 0.378197   Top1 87.792969   Top5 98.828125   BatchTime 0.459609   LR 0.000147
0.64653927
0.64656371
0.64662665
0.64664632
0.64665884
0.64667600
0.64668268
0.64666313
0.64659303
0.64652914
0.64646399
0.64642870
0.64634752
0.64626926
0.64622051
0.64617467
0.64614648
0.64612728
0.64609319
0.64606923
0.64605927
INFO - Training [61][   40/  196]   Loss 0.373088   Top1 87.958984   Top5 98.876953   BatchTime 0.424037   LR 0.000143
0.64599973
0.64591330
0.64586550
0.64580715
0.64575803
0.64572948
0.64566487
0.64560252
0.64549851
0.64542311
0.64533257
0.64523548
0.64519131
0.64512032
0.64512408
0.64516932
0.64525533
0.64529663
0.64535153
0.64539760
0.64542615
0.64544106
INFO - Training [61][   60/  196]   Loss 0.373191   Top1 88.007812   Top5 98.854167   BatchTime 0.404378   LR 0.000140
0.64545858
0.64546126
0.64546913
0.64549637
0.64551061
0.64549476
0.64554507
0.64556509
0.64559317
0.64561188
0.64560634
0.64562112
0.64562058
0.64566594
0.64568192
0.64570069
0.64575225
INFO - Training [61][   80/  196]   Loss 0.373109   Top1 88.041992   Top5 98.935547   BatchTime 0.393701   LR 0.000137
0.64577228
0.64578670
0.64575988
0.64573950
0.64576101
0.64575624
0.64581192
0.64584392
0.64593750
0.64594328
0.64596069
0.64596117
0.64595580
0.64593422
0.64591938
0.64595568
0.64594758
0.64591461
0.64587188
0.64584988
0.64579403
0.64578116
0.64576745
0.64574647
INFO - Training [61][  100/  196]   Loss 0.365477   Top1 88.328125   Top5 99.007812   BatchTime 0.384101   LR 0.000134
0.64573210
0.64570701
0.64565676
0.64562517
0.64562500
0.64560902
0.64557141
0.64551038
0.64551508
0.64546305
0.64541286
0.64538038
0.64538610
0.64534718
0.64537632
0.64539081
0.64538503
0.64538443
INFO - Training [61][  120/  196]   Loss 0.356587   Top1 88.639323   Top5 99.042969   BatchTime 0.373305   LR 0.000131
0.64543951
0.64537632
0.64538169
0.64537865
0.64538306
0.64535534
0.64537233
0.64539343
0.64542514
0.64542174
0.64542353
0.64552563
0.64558357
0.64561707
0.64562225
0.64565277
0.64568579
INFO - Training [61][  140/  196]   Loss 0.355720   Top1 88.730469   Top5 99.059710   BatchTime 0.370642   LR 0.000128
0.64572769
0.64575285
0.64577997
0.64584249
0.64586765
0.64592236
0.64596814
0.64598221
0.64600331
0.64596814
0.64593095
0.64594668
0.64598733
0.64597011
0.64597666
0.64602715
0.64600766
0.64604896
0.64594036
0.64591420
0.64589423
INFO - Training [61][  160/  196]   Loss 0.357812   Top1 88.601074   Top5 99.047852   BatchTime 0.370415   LR 0.000125
0.64588171
0.64591306
0.64591676
0.64590311
0.64593530
0.64594764
0.64594579
0.64594769
0.64598668
0.64596629
0.64597362
0.64596343
0.64594382
0.64593315
0.64591366
0.64590377
0.64587992
0.64585328
0.64593792
0.64589310
INFO - Training [61][  180/  196]   Loss 0.359719   Top1 88.476562   Top5 99.021267   BatchTime 0.374476   LR 0.000122
0.64590746
0.64592731
0.64592957
0.64594477
0.64600831
0.64600986
0.64603394
0.64607859
0.64607358
0.64607471
0.64611572
0.64612824
0.64610863
0.64612508
********************pre-trained*****************
INFO - ==> Top1: 88.556    Top5: 99.002    Loss: 0.357
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [61][   20/   40]   Loss 0.345366   Top1 89.550781   Top5 99.628906   BatchTime 0.133537
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0448)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0781)
features.3.conv.0 tensor(0.0312)
features.3.conv.3 tensor(0.0386)
features.3.conv.6 tensor(0.0690)
features.4.conv.0 tensor(0.0737)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.1234)
features.5.conv.0 tensor(0.0706)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.1938)
features.6.conv.0 tensor(0.0420)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0588)
features.7.conv.0 tensor(0.1331)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.2894)
features.8.conv.0 tensor(0.1368)
features.8.conv.3 tensor(0.0987)
features.8.conv.6 tensor(0.3790)
features.9.conv.0 tensor(0.1783)
features.9.conv.3 tensor(0.1617)
features.9.conv.6 tensor(0.4608)
features.10.conv.0 tensor(0.0753)
features.10.conv.3 tensor(0.0871)
features.10.conv.6 tensor(0.1775)
features.11.conv.0 tensor(0.6740)
features.11.conv.3 tensor(0.1375)
features.11.conv.6 tensor(0.7591)
features.12.conv.0 tensor(0.7087)
features.12.conv.3 tensor(0.1132)
features.12.conv.6 tensor(0.8490)
features.13.conv.0 tensor(0.3212)
features.13.conv.3 tensor(0.1312)
features.13.conv.6 tensor(0.4989)
features.14.conv.0 tensor(0.9838)
features.14.conv.3 tensor(0.0853)
features.14.conv.6 tensor(0.9904)
features.15.conv.0 tensor(0.9815)
features.15.conv.3 tensor(0.0670)
features.15.conv.6 tensor(0.4433)
features.16.conv.0 tensor(0.6863)
features.16.conv.3 tensor(0.1069)
features.16.conv.6 tensor(0.6860)
conv.0 tensor(0.6387)
tensor(1387688.) 2188896.0
INFO - Validation [61][   40/   40]   Loss 0.326486   Top1 89.520000   Top5 99.690000   BatchTime 0.092266
INFO - ==> Top1: 89.520    Top5: 99.690    Loss: 0.326
INFO - ==> Sparsity : 0.634
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 89.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
0.64609808
0.64617664
0.64622241
0.64615399
0.64615273
0.64615971
0.64615816
0.64610445
0.64605552
0.64604634
0.64600843
0.64595574
0.64589119
0.64585215
0.64581895
0.64574724
0.64568657
0.64566571
0.64569664
0.64567703
0.64566946
0.64561999
0.64559036
INFO - Training [62][   20/  196]   Loss 0.360119   Top1 88.457031   Top5 98.476562   BatchTime 0.492059   LR 0.000117
0.64563447
0.64563268
0.64552766
0.64544958
0.64536113
0.64533675
0.64535028
0.64533335
0.64530075
0.64525515
0.64524102
0.64524055
0.64525253
0.64523262
0.64522350
0.64523298
0.64522928
0.64519346
0.64517760
0.64519393
INFO - Training [62][   40/  196]   Loss 0.360851   Top1 88.349609   Top5 98.681641   BatchTime 0.441989   LR 0.000114
0.64522612
0.64527255
0.64529914
0.64529061
0.64523792
0.64516705
0.64512819
0.64516014
0.64513844
0.64512414
0.64516443
0.64521885
0.64520401
0.64517248
0.64513767
0.64507711
0.64501649
0.64495903
0.64486098
INFO - Training [62][   60/  196]   Loss 0.353917   Top1 88.815104   Top5 98.802083   BatchTime 0.433164   LR 0.000111
0.64485776
0.64484000
0.64487034
0.64479178
0.64475971
0.64466923
0.64465177
0.64457089
0.64453936
0.64452970
0.64459038
0.64464307
0.64472783
0.64473581
0.64475089
0.64477688
0.64481467
0.64476383
0.64479786
0.64481544
INFO - Training [62][   80/  196]   Loss 0.355067   Top1 88.808594   Top5 98.920898   BatchTime 0.427433   LR 0.000108
0.64476001
0.64473522
0.64470774
0.64464849
0.64463007
0.64459085
0.64451337
0.64447641
0.64439052
0.64434290
0.64438170
0.64438587
0.64436978
0.64435410
0.64437342
0.64435083
0.64434916
0.64435494
0.64435184
0.64432520
0.64431000
0.64428401
0.64431518
INFO - Training [62][  100/  196]   Loss 0.347704   Top1 89.035156   Top5 98.996094   BatchTime 0.417685   LR 0.000105
0.64431453
0.64433324
0.64434063
0.64443183
0.64437658
0.64435917
0.64440650
0.64441949
0.64436620
0.64433861
0.64422613
0.64423317
0.64409995
0.64422685
0.64422178
0.64421946
INFO - Training [62][  120/  196]   Loss 0.340776   Top1 89.303385   Top5 99.055990   BatchTime 0.404953   LR 0.000102
0.64418930
0.64419729
0.64415967
0.64420003
0.64421576
0.64423090
0.64421046
0.64424783
0.64424580
0.64416933
0.64410967
0.64406884
0.64403385
0.64398378
0.64392787
0.64400309
0.64404273
0.64406610
0.64403141
0.64401132
0.64399427
INFO - Training [62][  140/  196]   Loss 0.339887   Top1 89.377790   Top5 99.087612   BatchTime 0.402059   LR 0.000100
0.64407980
0.64404738
0.64401895
0.64396983
0.64392972
0.64390820
0.64391255
0.64390451
0.64391673
0.64397484
0.64400929
0.64396530
0.64394867
0.64387161
0.64387476
0.64387476
0.64384520
0.64381540
0.64385492
INFO - Training [62][  160/  196]   Loss 0.342958   Top1 89.199219   Top5 99.084473   BatchTime 0.402830   LR 0.000097
0.64387119
0.64379376
0.64377439
0.64373231
0.64375591
0.64373487
0.64372545
0.64365214
0.64362854
0.64362842
0.64357340
0.64356589
0.64352733
0.64342690
0.64339215
0.64336544
0.64337653
0.64335501
0.64330047
0.64324754
0.64318502
INFO - Training [62][  180/  196]   Loss 0.343679   Top1 89.184028   Top5 99.019097   BatchTime 0.401411   LR 0.000094
0.64311695
0.64306229
0.64304990
0.64297032
0.64290094
0.64292020
0.64291114
0.64284152
0.64274967
0.64270836
0.64263934
0.64266068
0.64262921
0.64259058
********************pre-trained*****************
INFO - ==> Top1: 89.224    Top5: 99.022    Loss: 0.342
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [62][   20/   40]   Loss 0.400745   Top1 87.949219   Top5 99.238281   BatchTime 0.136358
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0793)
features.3.conv.0 tensor(0.0304)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0692)
features.4.conv.0 tensor(0.0710)
features.4.conv.3 tensor(0.0868)
features.4.conv.6 tensor(0.1157)
features.5.conv.0 tensor(0.0711)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.1927)
features.6.conv.0 tensor(0.0412)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0579)
features.7.conv.0 tensor(0.1332)
features.7.conv.3 tensor(0.1062)
features.7.conv.6 tensor(0.2976)
features.8.conv.0 tensor(0.1352)
features.8.conv.3 tensor(0.0995)
features.8.conv.6 tensor(0.3803)
features.9.conv.0 tensor(0.1801)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.4618)
features.10.conv.0 tensor(0.0768)
features.10.conv.3 tensor(0.0880)
features.10.conv.6 tensor(0.1792)
features.11.conv.0 tensor(0.6838)
features.11.conv.3 tensor(0.1393)
features.11.conv.6 tensor(0.7602)
features.12.conv.0 tensor(0.7145)
features.12.conv.3 tensor(0.1138)
features.12.conv.6 tensor(0.8474)
features.13.conv.0 tensor(0.3251)
features.13.conv.3 tensor(0.1306)
features.13.conv.6 tensor(0.5206)
features.14.conv.0 tensor(0.9837)
features.14.conv.3 tensor(0.0853)
features.14.conv.6 tensor(0.9904)
features.15.conv.0 tensor(0.9815)
features.15.conv.3 tensor(0.0659)
features.15.conv.6 tensor(0.4449)
features.16.conv.0 tensor(0.6896)
features.16.conv.3 tensor(0.1063)
features.16.conv.6 tensor(0.7123)
conv.0 tensor(0.6598)
tensor(1408468.) 2188896.0
INFO - Validation [62][   40/   40]   Loss 0.387197   Top1 87.930000   Top5 99.430000   BatchTime 0.094210
INFO - ==> Top1: 87.930    Top5: 99.430    Loss: 0.387
INFO - ==> Sparsity : 0.643
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 89.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
0.64262706
0.64269161
0.64265680
0.64263028
0.64267009
0.64266878
0.64256412
0.64256126
0.64252472
0.64247745
0.64239126
0.64229721
0.64226693
0.64219642
0.64215130
0.64209086
0.64204645
0.64201874
0.64193219
0.64190900
0.64187282
0.64184242
0.64182270
INFO - Training [63][   20/  196]   Loss 0.370530   Top1 88.125000   Top5 98.496094   BatchTime 0.500588   LR 0.000090
0.64173788
0.64173365
0.64170343
0.64160830
0.64156586
0.64152932
0.64149857
0.64147907
0.64141423
0.64136237
0.64125085
0.64137191
0.64125699
0.64115047
0.64109069
0.64105392
0.64098752
0.64090127
0.64080912
0.64071745
0.64065796
0.64062983
INFO - Training [63][   40/  196]   Loss 0.377111   Top1 87.910156   Top5 98.623047   BatchTime 0.439009   LR 0.000087
0.64057148
0.64058381
0.64062077
0.64065510
0.64071321
0.64076126
0.64074022
0.64075357
0.64077377
0.64079511
0.64077210
0.64076626
0.64080620
0.64083594
0.64087462
0.64086378
INFO - Training [63][   60/  196]   Loss 0.372832   Top1 88.164062   Top5 98.769531   BatchTime 0.413077   LR 0.000085
0.64089543
0.64089525
0.64096266
0.64097172
0.64090550
0.64088386
0.64090538
0.64089161
0.64082986
0.64079475
0.64076781
0.64064741
0.64057755
0.64057469
0.64050305
0.64048421
0.64047807
0.64038175
0.64034629
0.64027202
0.64022654
0.64019436
0.64009517
0.64008486
INFO - Training [63][   80/  196]   Loss 0.370442   Top1 88.183594   Top5 98.920898   BatchTime 0.396342   LR 0.000082
0.64007980
0.64003664
0.63998395
0.63996339
0.63995594
0.63997608
0.64002162
0.64002281
0.64005095
0.64007682
0.64007521
0.64009595
0.64018542
0.64023000
0.64027572
0.64029843
0.64031816
0.64031011
0.64030129
INFO - Training [63][  100/  196]   Loss 0.361607   Top1 88.558594   Top5 98.933594   BatchTime 0.377483   LR 0.000080
0.64032036
0.64029104
0.64026833
0.64029109
0.64028698
0.64027679
0.64024711
0.64026755
0.64026320
0.64023411
0.64023685
0.64027560
0.64026004
0.64025295
0.64025819
0.64027232
INFO - Training [63][  120/  196]   Loss 0.354156   Top1 88.863932   Top5 98.974609   BatchTime 0.376877   LR 0.000077
0.64029634
0.64030969
0.64030588
0.64029974
0.64028496
0.64033884
0.64041895
0.64039904
0.64042771
0.64048058
0.64046693
0.64049447
0.64050561
0.64048928
0.64053345
0.64057493
0.64056849
0.64057404
0.64064735
0.64071047
0.64078063
0.64079660
INFO - Training [63][  140/  196]   Loss 0.353718   Top1 88.847656   Top5 99.037388   BatchTime 0.375742   LR 0.000075
0.64078885
0.64084280
0.64087981
0.64086449
0.64090371
0.64088017
0.64086425
0.64084774
0.64078844
0.64083779
0.64077276
0.64075625
0.64074981
0.64075714
0.64072746
0.64066762
0.64062363
0.64060134
0.64061481
0.64061773
0.64059889
INFO - Training [63][  160/  196]   Loss 0.353582   Top1 88.808594   Top5 99.018555   BatchTime 0.376789   LR 0.000072
0.64060992
0.64061040
0.64064699
0.64063889
0.64062399
0.64050746
0.64062065
0.64055443
0.64053994
0.64050889
0.64047962
0.64045089
0.64046168
0.64044923
0.64043432
0.64043099
0.64043635
0.64044929
0.64049125
0.64050037
0.64047009
0.64046860
INFO - Training [63][  180/  196]   Loss 0.353761   Top1 88.815104   Top5 98.993056   BatchTime 0.374811   LR 0.000070
0.64049065
0.64049220
0.64049315
0.64048463
0.64051384
0.64055735
0.64054132
0.64059615
0.64059788
0.64058357
0.64056736
INFO - ==> Top1: 88.838    Top5: 98.978    Loss: 0.352
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [63][   20/   40]   Loss 0.337431   Top1 89.550781   Top5 99.433594   BatchTime 0.138440
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.4805)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0660)
features.2.conv.0 tensor(0.0457)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0784)
features.3.conv.0 tensor(0.0298)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0694)
features.4.conv.0 tensor(0.0742)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.1247)
features.5.conv.0 tensor(0.0724)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.1960)
features.6.conv.0 tensor(0.0420)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0579)
features.7.conv.0 tensor(0.1348)
features.7.conv.3 tensor(0.1071)
features.7.conv.6 tensor(0.2976)
features.8.conv.0 tensor(0.1339)
features.8.conv.3 tensor(0.1004)
features.8.conv.6 tensor(0.3868)
features.9.conv.0 tensor(0.1825)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.4661)
features.10.conv.0 tensor(0.0761)
features.10.conv.3 tensor(0.0856)
features.10.conv.6 tensor(0.1880)
features.11.conv.0 tensor(0.6818)
features.11.conv.3 tensor(0.1385)
features.11.conv.6 tensor(0.7593)
features.12.conv.0 tensor(0.7235)
features.12.conv.3 tensor(0.1132)
features.12.conv.6 tensor(0.8477)
features.13.conv.0 tensor(0.3196)
features.13.conv.3 tensor(0.1294)
features.13.conv.6 tensor(0.5316)
features.14.conv.0 tensor(0.9835)
features.14.conv.3 tensor(0.0851)
features.14.conv.6 tensor(0.9902)
features.15.conv.0 tensor(0.9816)
features.15.conv.3 tensor(0.0660)
features.15.conv.6 tensor(0.4421)
features.16.conv.0 tensor(0.6897)
features.16.conv.3 tensor(0.1054)
features.16.conv.6 tensor(0.7061)
conv.0 tensor(0.6803)
tensor(1416314.) 2188896.0
INFO - Validation [63][   40/   40]   Loss 0.316069   Top1 89.920000   Top5 99.560000   BatchTime 0.097370
INFO - ==> Top1: 89.920    Top5: 99.560    Loss: 0.316
INFO - ==> Sparsity : 0.647
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [63][Top1: 89.920   Top5: 99.560]
INFO - Scoreboard best 3 ==> Epoch [47][Top1: 89.900   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
0.64058977
0.64058930
0.64061701
0.64060795
0.64064068
0.64068538
0.64071238
0.64072102
0.64075035
0.64072257
0.64072531
0.64072484
0.64071780
0.64069420
0.64063376
0.64063752
0.64062274
0.64062399
0.64059627
0.64059061
0.64058328
INFO - Training [64][   20/  196]   Loss 0.347274   Top1 88.671875   Top5 98.398438   BatchTime 0.458088   LR 0.000066
0.64049757
0.64050716
0.64051795
0.64051819
0.64049625
0.64048254
0.64047718
0.64050406
0.64053702
0.64053851
0.64052165
0.64053446
0.64050764
0.64053136
0.64051265
0.64051008
0.64049566
0.64053828
0.64052850
0.64054590
0.64056402
0.64057094
INFO - Training [64][   40/  196]   Loss 0.346150   Top1 88.808594   Top5 98.701172   BatchTime 0.411124   LR 0.000064
0.64055252
0.64054585
0.64052832
0.64050049
0.64043826
0.64048249
0.64046890
0.64041567
0.64039469
0.64038867
0.64038426
0.64039409
0.64037079
0.64033926
0.64037150
0.64036995
0.64033812
0.64036143
0.64036989
0.64033383
0.64032358
0.64030480
INFO - Training [64][   60/  196]   Loss 0.347141   Top1 88.886719   Top5 98.776042   BatchTime 0.397163   LR 0.000062
0.64027458
0.64027852
0.64025128
0.64022136
0.64017510
0.64014506
0.64009428
0.64003539
0.64005816
0.64007545
0.64004058
0.64005131
0.64009374
0.64011759
0.64007300
0.64007008
INFO - Training [64][   80/  196]   Loss 0.349195   Top1 88.676758   Top5 98.911133   BatchTime 0.390430   LR 0.000059
0.64006191
0.64006412
0.64002699
0.63996249
0.63996190
0.63997626
0.63995278
0.63995045
0.63990676
0.63995278
0.63997287
0.63995248
0.63995892
0.63995677
0.63996547
0.63999081
0.64001137
0.64007020
0.64009142
INFO - Training [64][  100/  196]   Loss 0.343649   Top1 88.917969   Top5 98.976562   BatchTime 0.374698   LR 0.000057
0.64008141
0.64010215
0.64007491
0.64011216
0.64012957
0.64011496
0.64009106
0.64008564
0.64012933
0.64013553
0.64009577
0.64008939
0.64008033
0.64008367
0.64009541
0.64001685
0.64004189
0.64003402
0.63996518
0.63992816
0.63989985
0.63990587
INFO - Training [64][  120/  196]   Loss 0.341900   Top1 89.016927   Top5 99.029948   BatchTime 0.371807   LR 0.000055
0.63985491
0.63983536
0.63984138
0.63983220
0.63980126
0.63978004
0.63974810
0.63974798
0.63978916
0.63978237
0.63977742
0.63979208
0.63975489
0.63975370
0.63973105
0.63968843
0.63968390
0.63968241
0.63965535
0.63974339
0.63974369
0.63973814
INFO - Training [64][  140/  196]   Loss 0.340521   Top1 89.082031   Top5 99.062500   BatchTime 0.372354   LR 0.000053
0.63970906
0.63964665
0.63962364
0.63962400
0.63958681
0.63955766
0.63952976
0.63956338
0.63957882
0.63954431
0.63957661
0.63965887
0.63960570
0.63963938
0.63961214
0.63956434
0.63962543
0.63957077
0.63957542
0.63960230
INFO - Training [64][  160/  196]   Loss 0.343512   Top1 88.972168   Top5 99.094238   BatchTime 0.374764   LR 0.000051
0.63960749
0.63958132
0.63954765
0.63955933
0.63954175
0.63954288
0.63949251
0.63945788
0.63951343
0.63949555
0.63949668
0.63949138
0.63950002
0.63949126
0.63946295
0.63949680
INFO - Training [64][  180/  196]   Loss 0.343614   Top1 88.967014   Top5 99.062500   BatchTime 0.374417   LR 0.000049
0.63948596
0.63952088
0.63949889
0.63950777
0.63949579
0.63948530
0.63945013
0.63940781
0.63944012
0.63942802
0.63939762
0.63936520
0.63930792
0.63923806
0.63924932
0.63922888
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 89.026    Top5: 99.058    Loss: 0.342
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [64][   20/   40]   Loss 0.325520   Top1 89.667969   Top5 99.453125   BatchTime 0.138754
INFO - Validation [64][   40/   40]   Loss 0.305595   Top1 90.120000   Top5 99.590000   BatchTime 0.097101
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.4805)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0793)
features.3.conv.0 tensor(0.0301)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0694)
features.4.conv.0 tensor(0.0747)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.1260)
features.5.conv.0 tensor(0.0736)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.1938)
features.6.conv.0 tensor(0.0422)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0579)
features.7.conv.0 tensor(0.1373)
features.7.conv.3 tensor(0.1073)
features.7.conv.6 tensor(0.2983)
features.8.conv.0 tensor(0.1350)
features.8.conv.3 tensor(0.1010)
features.8.conv.6 tensor(0.3833)
features.9.conv.0 tensor(0.1835)
features.9.conv.3 tensor(0.1626)
features.9.conv.6 tensor(0.4723)
features.10.conv.0 tensor(0.0777)
features.10.conv.3 tensor(0.0865)
features.10.conv.6 tensor(0.1852)
features.11.conv.0 tensor(0.6848)
features.11.conv.3 tensor(0.1373)
features.11.conv.6 tensor(0.7605)
features.12.conv.0 tensor(0.7254)
features.12.conv.3 tensor(0.1123)
features.12.conv.6 tensor(0.8485)
features.13.conv.0 tensor(0.3397)
features.13.conv.3 tensor(0.1289)
features.13.conv.6 tensor(0.5368)
features.14.conv.0 tensor(0.9834)
features.14.conv.3 tensor(0.0845)
features.14.conv.6 tensor(0.9904)
features.15.conv.0 tensor(0.9816)
features.15.conv.3 tensor(0.0663)
features.15.conv.6 tensor(0.4460)
features.16.conv.0 tensor(0.6873)
features.16.conv.3 tensor(0.1066)
features.16.conv.6 tensor(0.7119)
conv.0 tensor(0.6849)
tensor(1422391.) 2188896.0
INFO - ==> Top1: 90.120    Top5: 99.590    Loss: 0.306
INFO - ==> Sparsity : 0.650
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [64][Top1: 90.120   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [63][Top1: 89.920   Top5: 99.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
0.63921994
0.63923448
0.63925076
0.63928181
0.63927674
0.63926834
0.63929647
0.63931626
0.63924164
0.63930166
0.63932228
0.63932621
0.63929266
0.63930184
0.63929737
0.63928223
0.63930833
0.63929850
0.63929558
0.63929045
0.63929474
0.63928187
INFO - Training [65][   20/  196]   Loss 0.348132   Top1 88.457031   Top5 98.574219   BatchTime 0.441044   LR 0.000046
0.63927943
0.63925791
0.63927895
0.63927889
0.63925171
0.63924938
0.63925368
0.63925701
0.63925451
0.63926572
0.63929254
0.63925350
0.63924795
0.63925183
0.63923746
0.63921618
0.63919705
0.63921231
0.63917059
0.63918972
0.63908285
0.63908935
INFO - Training [65][   40/  196]   Loss 0.354627   Top1 88.349609   Top5 98.798828   BatchTime 0.407123   LR 0.000044
0.63908917
0.63909531
0.63907188
0.63903320
0.63899422
0.63900059
0.63896018
0.63893926
0.63892543
0.63891244
0.63883668
0.63885140
0.63888305
0.63888097
0.63885599
0.63882881
0.63881135
INFO - Training [65][   60/  196]   Loss 0.353407   Top1 88.541667   Top5 98.834635   BatchTime 0.391698   LR 0.000042
0.63879025
0.63877523
0.63876092
0.63871431
0.63864625
0.63868880
0.63870126
0.63862991
0.63866365
0.63866603
0.63866174
0.63865000
0.63866907
0.63866806
0.63866293
0.63867891
0.63865715
0.63867164
0.63867849
0.63866615
0.63871473
0.63872945
INFO - Training [65][   80/  196]   Loss 0.350723   Top1 88.696289   Top5 98.930664   BatchTime 0.384150   LR 0.000040
0.63870925
0.63874650
0.63878405
0.63879555
0.63876939
0.63876730
0.63879365
0.63880515
0.63876528
0.63880670
0.63880360
0.63873500
0.63873595
0.63870072
0.63869101
0.63872552
0.63872069
0.63871396
0.63870049
INFO - Training [65][  100/  196]   Loss 0.345148   Top1 88.992188   Top5 98.992188   BatchTime 0.369277   LR 0.000039
0.63867688
0.63862610
0.63868719
0.63866967
0.63866872
0.63866663
0.63862890
0.63860279
0.63862455
0.63863540
0.63862187
0.63864267
0.63862467
0.63860506
0.63859582
0.63860798
0.63860124
0.63855398
0.63860995
0.63861018
0.63861173
0.63862991
INFO - Training [65][  120/  196]   Loss 0.338778   Top1 89.248047   Top5 99.065755   BatchTime 0.370135   LR 0.000037
0.63862932
0.63863486
0.63862145
0.63861746
0.63859540
0.63854182
0.63854110
0.63857478
0.63857210
0.63854545
0.63856012
0.63852113
0.63854748
0.63854748
0.63853437
0.63854140
0.63852280
0.63848084
0.63847804
0.63851219
INFO - Training [65][  140/  196]   Loss 0.338704   Top1 89.274554   Top5 99.118304   BatchTime 0.370042   LR 0.000035
0.63848734
0.63846332
0.63843137
0.63841629
0.63844067
0.63846338
0.63842928
0.63843685
0.63844806
0.63844335
0.63845348
0.63845134
0.63849217
0.63847691
0.63848734
0.63850015
0.63852489
INFO - Training [65][  160/  196]   Loss 0.339451   Top1 89.245605   Top5 99.106445   BatchTime 0.371410   LR 0.000033
0.63851291
0.63851255
0.63853103
0.63855612
0.63857991
0.63855577
0.63854206
0.63854253
0.63851476
0.63851815
0.63848001
0.63851786
0.63855398
0.63856143
0.63851535
0.63850272
0.63854271
0.63850152
0.63846451
0.63849682
0.63849396
INFO - Training [65][  180/  196]   Loss 0.339453   Top1 89.238281   Top5 99.047309   BatchTime 0.372996   LR 0.000032
0.63851225
0.63851625
0.63852400
0.63851744
0.63851851
0.63848937
0.63846958
0.63848472
0.63846630
0.63844049
0.63841480
0.63843143
0.63843840
0.63844562
********************pre-trained*****************
INFO - ==> Top1: 89.258    Top5: 99.050    Loss: 0.339
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0466)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0790)
features.3.conv.0 tensor(0.0301)
features.3.conv.3 tensor(0.0386)
features.3.conv.6 tensor(0.0684)
features.4.conv.0 tensor(0.0776)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.1221)
features.5.conv.0 tensor(0.0723)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1943)
features.6.conv.0 tensor(0.0425)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0581)
features.7.conv.0 tensor(0.1379)
features.7.conv.3 tensor(0.1056)
features.7.conv.6 tensor(0.2901)
features.8.conv.0 tensor(0.1363)
features.8.conv.3 tensor(0.1001)
features.8.conv.6 tensor(0.3821)
features.9.conv.0 tensor(0.1849)
features.9.conv.3 tensor(0.1629)
features.9.conv.6 tensor(0.4714)
features.10.conv.0 tensor(0.0781)
features.10.conv.3 tensor(0.0854)
features.10.conv.6 tensor(0.1802)
features.11.conv.0 tensor(0.6855)
features.11.conv.3 tensor(0.1356)
features.11.conv.6 tensor(0.7605)
features.12.conv.0 tensor(0.7302)
features.12.conv.3 tensor(0.1134)
features.12.conv.6 tensor(0.8483)
features.13.conv.0 tensor(0.3514)
features.13.conv.3 tensor(0.1296)
features.13.conv.6 tensor(0.5449)
features.14.conv.0 tensor(0.9834)
features.14.conv.3 tensor(0.0848)
features.14.conv.6 tensor(0.9904)
features.15.conv.0 tensor(0.9817)
features.15.conv.3 tensor(0.0667)
features.15.conv.6 tensor(0.4459)
features.16.conv.0 tensor(0.6893)
features.16.conv.3 tensor(0.1065)
features.16.conv.6 tensor(0.7194)
conv.0 tensor(0.6891)
tensor(1427983.) 2188896.0
INFO - Validation [65][   20/   40]   Loss 0.342434   Top1 89.531250   Top5 99.550781   BatchTime 0.138120
INFO - Validation [65][   40/   40]   Loss 0.318328   Top1 90.010000   Top5 99.670000   BatchTime 0.096126
INFO - ==> Top1: 90.010    Top5: 99.670    Loss: 0.318
INFO - ==> Sparsity : 0.652
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [64][Top1: 90.120   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [65][Top1: 90.010   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
0.63843948
0.63843817
0.63839167
0.63837379
0.63836408
0.63833964
0.63835752
0.63834739
0.63840902
0.63839990
0.63843113
0.63846523
0.63844454
0.63845307
0.63844204
0.63846672
0.63848609
0.63845599
0.63843942
0.63843197
0.63845456
0.63843805
INFO - Training [66][   20/  196]   Loss 0.347351   Top1 88.847656   Top5 98.496094   BatchTime 0.470425   LR 0.000029
0.63841218
0.63842237
0.63839298
0.63842893
0.63839209
0.63838065
0.63838410
0.63841909
0.63841236
0.63843715
0.63844222
0.63848042
0.63848865
0.63847017
0.63846874
0.63847607
0.63848960
0.63847548
0.63847327
0.63848358
0.63842738
0.63847899
INFO - Training [66][   40/  196]   Loss 0.354778   Top1 88.632812   Top5 98.720703   BatchTime 0.422657   LR 0.000028
0.63848114
0.63846165
0.63846433
0.63844705
0.63843316
0.63838321
0.63840097
0.63838935
0.63834465
0.63833982
0.63830894
0.63831311
0.63829285
0.63831055
0.63826668
0.63825178
0.63822985
INFO - Training [66][   60/  196]   Loss 0.359171   Top1 88.339844   Top5 98.678385   BatchTime 0.398937   LR 0.000026
0.63819891
0.63822418
0.63821340
0.63821572
0.63819480
0.63822800
0.63826334
0.63823622
0.63821584
0.63821876
0.63820034
0.63815540
0.63817352
0.63815331
0.63814688
0.63819391
0.63819724
0.63819224
0.63819212
0.63819438
0.63820136
0.63820231
0.63818473
0.63816190
0.63813281
INFO - Training [66][   80/  196]   Loss 0.354494   Top1 88.588867   Top5 98.833008   BatchTime 0.384832   LR 0.000025
0.63811707
0.63808894
0.63810802
0.63808411
0.63807255
0.63808489
0.63809663
0.63810831
0.63812226
0.63811469
0.63811070
0.63812321
0.63809979
0.63816226
0.63816184
0.63816506
0.63814461
INFO - Training [66][  100/  196]   Loss 0.347604   Top1 88.871094   Top5 98.843750   BatchTime 0.371800   LR 0.000023
0.63812310
0.63814813
0.63814592
0.63808274
0.63813132
0.63816762
0.63815951
0.63809025
0.63803279
0.63802767
0.63802701
0.63802147
0.63803208
0.63803571
0.63804567
0.63806808
0.63805324
0.63807398
0.63811964
0.63813865
0.63811618
INFO - Training [66][  120/  196]   Loss 0.341069   Top1 89.101562   Top5 98.951823   BatchTime 0.375517   LR 0.000022
0.63811749
0.63814676
0.63813424
0.63818944
0.63818765
0.63816446
0.63816994
0.63819879
0.63819951
0.63817585
0.63816994
0.63815862
0.63812947
0.63811481
0.63809621
0.63807499
0.63806254
0.63804364
0.63806456
0.63808638
INFO - Training [66][  140/  196]   Loss 0.338749   Top1 89.218750   Top5 99.026228   BatchTime 0.377856   LR 0.000021
0.63805234
0.63796300
0.63795924
0.63794118
0.63789332
0.63784647
0.63781148
0.63775659
0.63772404
0.63772380
0.63770181
0.63768905
0.63763750
0.63756275
0.63753432
0.63752609
0.63751101
INFO - Training [66][  160/  196]   Loss 0.340588   Top1 89.152832   Top5 99.033203   BatchTime 0.375904   LR 0.000019
0.63746798
0.63737923
0.63742536
0.63745868
0.63744819
0.63741022
0.63744664
0.63747430
0.63748449
0.63750637
0.63755643
0.63756806
0.63756204
0.63757831
0.63761282
0.63765526
0.63765442
0.63764733
0.63765484
0.63764989
INFO - Training [66][  180/  196]   Loss 0.342669   Top1 89.095052   Top5 98.997396   BatchTime 0.377847   LR 0.000018
0.63764149
0.63760835
0.63760400
0.63756788
0.63758415
0.63758039
0.63754773
0.63752472
0.63755250
0.63758105
0.63760459
0.63761228
0.63761777
0.63759190
0.63763595
********************pre-trained*****************
INFO - ==> Top1: 89.172    Top5: 98.998    Loss: 0.341
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0457)
features.2.conv.3 tensor(0.0517)
features.2.conv.6 tensor(0.0796)
features.3.conv.0 tensor(0.0307)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0690)
features.4.conv.0 tensor(0.0776)
features.4.conv.3 tensor(0.0897)
features.4.conv.6 tensor(0.1230)
features.5.conv.0 tensor(0.0731)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1948)
features.6.conv.0 tensor(0.0431)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0579)
features.7.conv.0 tensor(0.1375)
features.7.conv.3 tensor(0.1059)
features.7.conv.6 tensor(0.2911)
features.8.conv.0 tensor(0.1369)
features.8.conv.3 tensor(0.0987)
features.8.conv.6 tensor(0.3819)
features.9.conv.0 tensor(0.2048)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.4709)
features.10.conv.0 tensor(0.0776)
features.10.conv.3 tensor(0.0836)
features.10.conv.6 tensor(0.1818)
features.11.conv.0 tensor(0.6842)
features.11.conv.3 tensor(0.1370)
features.11.conv.6 tensor(0.7615)
features.12.conv.0 tensor(0.7293)
features.12.conv.3 tensor(0.1132)
features.12.conv.6 tensor(0.8483)
features.13.conv.0 tensor(0.3527)
features.13.conv.3 tensor(0.1298)
features.13.conv.6 tensor(0.5455)
features.14.conv.0 tensor(0.9833)
features.14.conv.3 tensor(0.0843)
features.14.conv.6 tensor(0.9905)
features.15.conv.0 tensor(0.9818)
features.15.conv.3 tensor(0.0666)
features.15.conv.6 tensor(0.4457)
features.16.conv.0 tensor(0.6893)
features.16.conv.3 tensor(0.1066)
features.16.conv.6 tensor(0.7253)
conv.0 tensor(0.6937)
tensor(1432312.) 2188896.0
INFO - Validation [66][   20/   40]   Loss 0.343005   Top1 89.707031   Top5 99.472656   BatchTime 0.139347
INFO - Validation [66][   40/   40]   Loss 0.319012   Top1 90.060000   Top5 99.630000   BatchTime 0.100265
INFO - ==> Top1: 90.060    Top5: 99.630    Loss: 0.319
INFO - ==> Sparsity : 0.654
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [64][Top1: 90.120   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [66][Top1: 90.060   Top5: 99.630]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
0.63764155
0.63762146
0.63760281
0.63757420
0.63754857
0.63755298
0.63752621
0.63750827
0.63750893
0.63749933
0.63747001
0.63746822
0.63746917
0.63748235
0.63750988
0.63747329
0.63750452
0.63752568
0.63751066
0.63749528
0.63752824
0.63751459
0.63752949
INFO - Training [67][   20/  196]   Loss 0.366236   Top1 88.496094   Top5 98.652344   BatchTime 0.445146   LR 0.000016
0.63748968
0.63746917
0.63750929
0.63749331
0.63745475
0.63743365
0.63742733
0.63747495
0.63749403
0.63751966
0.63750696
0.63751704
0.63751239
0.63748705
0.63746434
0.63751745
0.63752329
INFO - Training [67][   40/  196]   Loss 0.365087   Top1 88.720703   Top5 98.652344   BatchTime 0.398555   LR 0.000015
0.63750678
0.63748831
0.63748628
0.63748169
0.63745552
0.63745487
0.63746041
0.63744342
0.63745254
0.63743293
0.63740754
0.63738960
0.63736951
0.63735849
0.63737524
0.63737738
0.63737917
0.63738161
0.63735795
0.63735282
0.63733035
INFO - Training [67][   60/  196]   Loss 0.354994   Top1 88.919271   Top5 98.750000   BatchTime 0.393422   LR 0.000014
0.63733727
0.63731521
0.63728052
0.63727415
0.63724589
0.63722229
0.63722968
0.63719153
0.63715017
0.63711405
0.63707489
0.63707662
0.63708466
0.63708103
0.63705701
0.63702548
0.63699621
0.63698655
INFO - Training [67][   80/  196]   Loss 0.354321   Top1 88.837891   Top5 98.881836   BatchTime 0.376970   LR 0.000013
0.63696533
0.63693935
0.63694948
0.63694042
0.63693470
0.63693839
0.63690925
0.63689184
0.63688469
0.63684654
0.63683027
0.63681370
0.63679111
0.63675117
0.63672882
0.63671994
0.63670421
0.63669634
0.63666677
0.63666886
0.63666445
0.63663918
0.63663292
0.63662493
INFO - Training [67][  100/  196]   Loss 0.349843   Top1 88.960938   Top5 98.921875   BatchTime 0.367176   LR 0.000012
0.63661349
0.63659328
0.63657999
0.63657206
0.63656068
0.63656449
0.63651699
0.63649374
0.63649112
0.63647842
0.63648826
0.63648337
0.63649291
0.63648820
0.63647586
0.63646239
INFO - Training [67][  120/  196]   Loss 0.346515   Top1 89.049479   Top5 98.987630   BatchTime 0.368957   LR 0.000011
0.63643777
0.63644588
0.63648140
0.63647759
0.63647813
0.63647294
0.63646072
0.63646865
0.63647330
0.63646239
0.63646454
0.63646370
0.63643891
0.63646692
0.63647801
0.63647312
0.63648593
0.63650018
0.63649964
0.63651896
INFO - Training [67][  140/  196]   Loss 0.342524   Top1 89.165737   Top5 99.059710   BatchTime 0.373074   LR 0.000010
0.63653439
0.63652569
0.63654220
0.63654333
0.63653231
0.63652956
0.63654059
0.63653761
0.63653094
0.63654190
0.63654035
0.63652337
0.63650626
0.63649988
0.63648862
0.63649029
0.63647336
0.63648373
0.63650078
0.63649023
0.63649285
INFO - Training [67][  160/  196]   Loss 0.345148   Top1 89.101562   Top5 99.038086   BatchTime 0.373400   LR 0.000009
0.63649696
0.63647950
0.63649917
0.63648021
0.63646173
0.63645327
0.63645476
0.63644522
0.63645285
0.63645512
0.63646483
0.63648057
0.63647842
0.63647932
0.63648176
0.63648260
0.63647199
0.63646066
0.63646036
0.63644940
0.63643748
0.63644016
INFO - Training [67][  180/  196]   Loss 0.343989   Top1 89.116753   Top5 99.036458   BatchTime 0.373927   LR 0.000008
0.63646442
0.63645506
0.63645667
0.63647097
0.63647681
0.63649136
0.63647723
0.63645309
0.63645422
0.63644207
0.63644892
0.63645297
0.63647538
0.63647944
********************pre-trained*****************
INFO - ==> Top1: 89.160    Top5: 99.030    Loss: 0.343
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [67][   20/   40]   Loss 0.327016   Top1 89.667969   Top5 99.707031   BatchTime 0.136647
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0469)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0796)
features.3.conv.0 tensor(0.0310)
features.3.conv.3 tensor(0.0378)
features.3.conv.6 tensor(0.0688)
features.4.conv.0 tensor(0.0793)
features.4.conv.3 tensor(0.0897)
features.4.conv.6 tensor(0.1222)
features.5.conv.0 tensor(0.0736)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1956)
features.6.conv.0 tensor(0.0418)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0579)
features.7.conv.0 tensor(0.1383)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.2912)
features.8.conv.0 tensor(0.1373)
features.8.conv.3 tensor(0.0998)
features.8.conv.6 tensor(0.3813)
features.9.conv.0 tensor(0.2421)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.4711)
features.10.conv.0 tensor(0.0778)
features.10.conv.3 tensor(0.0854)
features.10.conv.6 tensor(0.1828)
features.11.conv.0 tensor(0.6850)
features.11.conv.3 tensor(0.1360)
features.11.conv.6 tensor(0.7620)
features.12.conv.0 tensor(0.7297)
features.12.conv.3 tensor(0.1128)
features.12.conv.6 tensor(0.8484)
features.13.conv.0 tensor(0.3529)
features.13.conv.3 tensor(0.1287)
features.13.conv.6 tensor(0.5460)
features.14.conv.0 tensor(0.9833)
features.14.conv.3 tensor(0.0843)
features.14.conv.6 tensor(0.9907)
features.15.conv.0 tensor(0.9818)
features.15.conv.3 tensor(0.0667)
features.15.conv.6 tensor(0.4461)
features.16.conv.0 tensor(0.6909)
features.16.conv.3 tensor(0.1053)
features.16.conv.6 tensor(0.7267)
conv.0 tensor(0.6939)
tensor(1434294.) 2188896.0
INFO - Validation [67][   40/   40]   Loss 0.309624   Top1 90.040000   Top5 99.700000   BatchTime 0.095109
INFO - ==> Top1: 90.040    Top5: 99.700    Loss: 0.310
INFO - ==> Sparsity : 0.655
INFO - Scoreboard best 1 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [64][Top1: 90.120   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [66][Top1: 90.060   Top5: 99.630]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
0.63650429
0.63651490
0.63653338
0.63655663
0.63656133
0.63659745
0.63660347
0.63659841
0.63662046
0.63663989
0.63665533
0.63665271
0.63666624
0.63665640
0.63664931
0.63665926
0.63665539
0.63665915
0.63668215
INFO - Training [68][   20/  196]   Loss 0.361809   Top1 88.398438   Top5 98.515625   BatchTime 0.458094   LR 0.000007
0.63666314
0.63666588
0.63665938
0.63663369
0.63662273
0.63662720
0.63662392
0.63662469
0.63662392
0.63659984
0.63662016
0.63662148
0.63661242
0.63661104
0.63660324
0.63658762
0.63657808
0.63658309
0.63658893
0.63658369
0.63657314
0.63659811
INFO - Training [68][   40/  196]   Loss 0.359559   Top1 88.457031   Top5 98.701172   BatchTime 0.413807   LR 0.000006
0.63659751
0.63659424
0.63658404
0.63658005
0.63657242
0.63656771
0.63657618
0.63657862
0.63658923
0.63657850
0.63656181
0.63654870
0.63653976
0.63654387
0.63655299
0.63655519
0.63656807
0.63657886
0.63657457
0.63656408
0.63656205
0.63655365
0.63657022
INFO - Training [68][   60/  196]   Loss 0.354916   Top1 88.593750   Top5 98.743490   BatchTime 0.391954   LR 0.000006
0.63654548
0.63654912
0.63655394
0.63655323
0.63652378
0.63653159
0.63650960
0.63650823
0.63651711
0.63655370
0.63654977
0.63655704
0.63653922
0.63652611
0.63651949
0.63652968
0.63653082
INFO - Training [68][   80/  196]   Loss 0.354581   Top1 88.608398   Top5 98.833008   BatchTime 0.383277   LR 0.000005
0.63653266
0.63652587
0.63652009
0.63649642
0.63651502
0.63653135
0.63654047
0.63653284
0.63651925
0.63650090
0.63649631
0.63647461
0.63647407
0.63646770
0.63647354
0.63648564
0.63650322
0.63649267
INFO - Training [68][  100/  196]   Loss 0.343497   Top1 88.996094   Top5 98.894531   BatchTime 0.371726   LR 0.000004
0.63649786
0.63648951
0.63648850
0.63648683
0.63646400
0.63646519
0.63644904
0.63646919
0.63646179
0.63647735
0.63646942
0.63647991
0.63650161
0.63650310
0.63650829
0.63651139
0.63649940
0.63649434
0.63649702
0.63649368
0.63650703
0.63651538
0.63651162
INFO - Training [68][  120/  196]   Loss 0.338947   Top1 89.137370   Top5 98.948568   BatchTime 0.368235   LR 0.000004
0.63650244
0.63649541
0.63647789
0.63649118
0.63648415
0.63645279
0.63645983
0.63645512
0.63647085
0.63647920
0.63646412
0.63646305
0.63646591
0.63645399
0.63644534
0.63644063
0.63644207
0.63642579
0.63642675
0.63640821
INFO - Training [68][  140/  196]   Loss 0.336406   Top1 89.235491   Top5 99.029018   BatchTime 0.371590   LR 0.000003
0.63643807
0.63644385
0.63644791
0.63645053
0.63644564
0.63645291
0.63646179
0.63645494
0.63643706
0.63642114
0.63640511
0.63643223
0.63644552
0.63642967
0.63644600
0.63644654
0.63642758
0.63642770
0.63644105
0.63643950
0.63644624
0.63644081
INFO - Training [68][  160/  196]   Loss 0.339094   Top1 89.130859   Top5 98.981934   BatchTime 0.371014   LR 0.000003
0.63645387
0.63645351
0.63644212
0.63642728
0.63642246
0.63641644
0.63638484
0.63641077
0.63641071
0.63641506
0.63640594
0.63640350
0.63642591
0.63640296
0.63641423
0.63640940
INFO - Training [68][  180/  196]   Loss 0.340705   Top1 89.053819   Top5 98.964844   BatchTime 0.370877   LR 0.000002
0.63639504
0.63636953
0.63636613
0.63636267
0.63635266
0.63636118
0.63637924
0.63637495
0.63637733
0.63639015
0.63638091
0.63636172
0.63634396
0.63632739
0.63633853
0.63635963
********************pre-trained*****************
INFO - ==> Top1: 89.068    Top5: 98.978    Loss: 0.341
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0660)
features.2.conv.0 tensor(0.0472)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0799)
features.3.conv.0 tensor(0.0307)
features.3.conv.3 tensor(0.0378)
features.3.conv.6 tensor(0.0684)
features.4.conv.0 tensor(0.0796)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.1224)
features.5.conv.0 tensor(0.0736)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.1956)
features.6.conv.0 tensor(0.0423)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0580)
features.7.conv.0 tensor(0.1381)
features.7.conv.3 tensor(0.1053)
features.7.conv.6 tensor(0.2911)
features.8.conv.0 tensor(0.1374)
features.8.conv.3 tensor(0.0990)
features.8.conv.6 tensor(0.3816)
features.9.conv.0 tensor(0.2450)
features.9.conv.3 tensor(0.1617)
features.9.conv.6 tensor(0.4712)
features.10.conv.0 tensor(0.0776)
features.10.conv.3 tensor(0.0859)
features.10.conv.6 tensor(0.1826)
features.11.conv.0 tensor(0.6855)
features.11.conv.3 tensor(0.1364)
features.11.conv.6 tensor(0.7620)
features.12.conv.0 tensor(0.7303)
features.12.conv.3 tensor(0.1128)
features.12.conv.6 tensor(0.8485)
features.13.conv.0 tensor(0.3524)
features.13.conv.3 tensor(0.1298)
features.13.conv.6 tensor(0.5459)
features.14.conv.0 tensor(0.9832)
features.14.conv.3 tensor(0.0843)
features.14.conv.6 tensor(0.9908)
features.15.conv.0 tensor(0.9818)
features.15.conv.3 tensor(0.0667)
features.15.conv.6 tensor(0.4475)
features.16.conv.0 tensor(0.6914)
features.16.conv.3 tensor(0.1058)
features.16.conv.6 tensor(0.7258)
conv.0 tensor(0.6941)
tensor(1434487.) 2188896.0
INFO - Validation [68][   20/   40]   Loss 0.324552   Top1 90.195312   Top5 99.589844   BatchTime 0.137467
INFO - Validation [68][   40/   40]   Loss 0.306479   Top1 90.370000   Top5 99.670000   BatchTime 0.096405
INFO - ==> Top1: 90.370    Top5: 99.670    Loss: 0.306
INFO - ==> Sparsity : 0.655
INFO - Scoreboard best 1 ==> Epoch [68][Top1: 90.370   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [64][Top1: 90.120   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
0.63637519
0.63638777
0.63639712
0.63641107
0.63641298
0.63642925
0.63642645
0.63642555
0.63641208
0.63641560
0.63641179
0.63641882
0.63642180
0.63641983
0.63641125
0.63643599
0.63642192
0.63642615
0.63641614
0.63642716
INFO - Training [69][   20/  196]   Loss 0.353509   Top1 88.652344   Top5 98.574219   BatchTime 0.442558   LR 0.000002
0.63642120
0.63641381
0.63642758
0.63643873
0.63644284
0.63644385
0.63643593
0.63644493
0.63643390
0.63643849
0.63643694
0.63643509
0.63644964
0.63646424
0.63646173
0.63645554
0.63645154
0.63643557
0.63643247
0.63642699
0.63641280
0.63642162
INFO - Training [69][   40/  196]   Loss 0.352358   Top1 88.603516   Top5 98.681641   BatchTime 0.401776   LR 0.000001
0.63642758
0.63643730
0.63642651
0.63640517
0.63638771
0.63639653
0.63637859
0.63637888
0.63638777
0.63641113
0.63641584
0.63643581
0.63641983
0.63640749
0.63640314
0.63639688
0.63639569
0.63637739
0.63638198
0.63638836
0.63640398
0.63641113
INFO - Training [69][   60/  196]   Loss 0.352010   Top1 88.691406   Top5 98.710938   BatchTime 0.394392   LR 0.000001
0.63641983
0.63641828
0.63640130
0.63639456
0.63639528
0.63639337
0.63639176
0.63638127
0.63636303
0.63634592
0.63634217
0.63636488
0.63637334
0.63637209
0.63638473
0.63641322
0.63641918
0.63641661
INFO - Training [69][   80/  196]   Loss 0.346814   Top1 88.925781   Top5 98.862305   BatchTime 0.378663   LR 0.000001
0.63641471
0.63641220
0.63639069
0.63640219
0.63639528
0.63639235
0.63639760
0.63640171
0.63639957
0.63640302
0.63640201
0.63638413
0.63638753
0.63636464
0.63636130
0.63633305
0.63631588
0.63632840
0.63631767
0.63631558
0.63632083
INFO - Training [69][  100/  196]   Loss 0.344286   Top1 89.160156   Top5 98.921875   BatchTime 0.376369   LR 0.000000
0.63633090
0.63632637
0.63632548
0.63631487
0.63632351
0.63631880
0.63629764
0.63629705
0.63629961
0.63632363
0.63632756
0.63632834
0.63633639
0.63634312
0.63633722
0.63632941
0.63634837
0.63633281
0.63634920
0.63635457
0.63635606
0.63635141
0.63637489
0.63635612
0.63635951
0.63636571
0.63635927
0.63633883
0.63635337
0.63634700
0.63636577
0.63635480
0.63636416
0.63635212
0.63636881
0.63635498
0.63637680
0.63638318
0.63637882
0.63636512
0.63637650
0.63637590
INFO - Training [69][  120/  196]   Loss 0.341227   Top1 89.225260   Top5 99.003906   BatchTime 0.379063   LR 0.000000
INFO - Training [69][  140/  196]   Loss 0.339741   Top1 89.257812   Top5 99.079241   BatchTime 0.380533   LR 0.000000
0.63637429
0.63638806
0.63638330
0.63638169
0.63638425
0.63637483
0.63636935
0.63635582
0.63635409
0.63636678
0.63639039
0.63638830
0.63640887
0.63641578
0.63641399
INFO - Training [69][  160/  196]   Loss 0.344203   Top1 89.067383   Top5 99.045410   BatchTime 0.380699   LR 0.000000
0.63641065
0.63638961
0.63638270
0.63637447
0.63637334
0.63635319
0.63634771
0.63636905
0.63637096
0.63637149
0.63637209
0.63638335
0.63639492
0.63642061
0.63640928
0.63636339
0.63635838
0.63636106
0.63637823
0.63635850
0.63635343
0.63636261
INFO - Training [69][  180/  196]   Loss 0.344066   Top1 89.027778   Top5 99.001736   BatchTime 0.378422   LR 0.000000
0.63636601
0.63637865
0.63637912
0.63638836
0.63637960
0.63635927
0.63634908
0.63635433
0.63634050
0.63633853
0.63633674
0.63635874
0.63636476
0.63637048
********************pre-trained*****************
INFO - ==> Top1: 89.008    Top5: 98.974    Loss: 0.344
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [69][   20/   40]   Loss 0.329207   Top1 90.273438   Top5 99.472656   BatchTime 0.140022
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.4785)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0660)
features.2.conv.0 tensor(0.0472)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0799)
features.3.conv.0 tensor(0.0310)
features.3.conv.3 tensor(0.0378)
features.3.conv.6 tensor(0.0681)
features.4.conv.0 tensor(0.0793)
features.4.conv.3 tensor(0.0897)
features.4.conv.6 tensor(0.1222)
features.5.conv.0 tensor(0.0729)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.1958)
features.6.conv.0 tensor(0.0423)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0579)
features.7.conv.0 tensor(0.1386)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.2910)
features.8.conv.0 tensor(0.1373)
features.8.conv.3 tensor(0.0990)
features.8.conv.6 tensor(0.3816)
features.9.conv.0 tensor(0.2448)
features.9.conv.3 tensor(0.1606)
features.9.conv.6 tensor(0.4714)
features.10.conv.0 tensor(0.0782)
features.10.conv.3 tensor(0.0851)
features.10.conv.6 tensor(0.1826)
features.11.conv.0 tensor(0.6853)
features.11.conv.3 tensor(0.1366)
features.11.conv.6 tensor(0.7619)
features.12.conv.0 tensor(0.7304)
features.12.conv.3 tensor(0.1127)
features.12.conv.6 tensor(0.8484)
features.13.conv.0 tensor(0.3524)
features.13.conv.3 tensor(0.1291)
features.13.conv.6 tensor(0.5459)
features.14.conv.0 tensor(0.9832)
features.14.conv.3 tensor(0.0844)
features.14.conv.6 tensor(0.9907)
features.15.conv.0 tensor(0.9817)
features.15.conv.3 tensor(0.0666)
features.15.conv.6 tensor(0.4464)
features.16.conv.0 tensor(0.6912)
features.16.conv.3 tensor(0.1060)
features.16.conv.6 tensor(0.7259)
conv.0 tensor(0.6941)
tensor(1434304.) 2188896.0
INFO - Validation [69][   40/   40]   Loss 0.310160   Top1 90.350000   Top5 99.590000   BatchTime 0.097405
INFO - ==> Top1: 90.350    Top5: 99.590    Loss: 0.310
INFO - ==> Sparsity : 0.655
INFO - Scoreboard best 1 ==> Epoch [68][Top1: 90.370   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [69][Top1: 90.350   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 90.270   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/_checkpoint.pth.tar
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
*************hard_pruning_mode*******************
INFO - Validation [   20/   40]   Loss 0.329207   Top1 90.273438   Top5 99.472656   BatchTime 0.137179
INFO - Validation [   40/   40]   Loss 0.310160   Top1 90.350000   Top5 99.590000   BatchTime 0.093950
INFO - ==> Top1: 90.350    Top5: 99.590    Loss: 0.310
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [0][   20/  196]   Loss 0.588464   Top1 79.687500   Top5 97.617188   BatchTime 0.429759   LR 0.004999
INFO - Training [0][   40/  196]   Loss 0.574153   Top1 80.224609   Top5 97.890625   BatchTime 0.386407   LR 0.004995
INFO - Training [0][   60/  196]   Loss 0.569526   Top1 80.247396   Top5 97.877604   BatchTime 0.365582   LR 0.004989
INFO - Training [0][   80/  196]   Loss 0.566016   Top1 80.400391   Top5 98.061523   BatchTime 0.354240   LR 0.004980
INFO - Training [0][  100/  196]   Loss 0.560746   Top1 80.492188   Top5 98.167969   BatchTime 0.356457   LR 0.004968
INFO - Training [0][  120/  196]   Loss 0.551908   Top1 80.719401   Top5 98.268229   BatchTime 0.358680   LR 0.004954
INFO - Training [0][  140/  196]   Loss 0.550495   Top1 80.809152   Top5 98.320312   BatchTime 0.358555   LR 0.004938
INFO - Training [0][  160/  196]   Loss 0.552037   Top1 80.778809   Top5 98.251953   BatchTime 0.356498   LR 0.004919
INFO - Training [0][  180/  196]   Loss 0.554797   Top1 80.690104   Top5 98.211806   BatchTime 0.356486   LR 0.004897
********************pre-trained*****************
INFO - ==> Top1: 80.760    Top5: 98.196    Loss: 0.554
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.535097   Top1 82.636719   Top5 98.925781   BatchTime 0.132210
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.4883)
features.1.conv.0 tensor(0.0286)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0359)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0799)
features.3.conv.0 tensor(0.0341)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0549)
features.4.conv.0 tensor(0.0975)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.1974)
features.5.conv.0 tensor(0.0684)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.2729)
features.6.conv.0 tensor(0.0423)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0558)
features.7.conv.0 tensor(0.1544)
features.7.conv.3 tensor(0.1091)
features.7.conv.6 tensor(0.3879)
features.8.conv.0 tensor(0.1533)
features.8.conv.3 tensor(0.1021)
features.8.conv.6 tensor(0.4611)
features.9.conv.0 tensor(0.3225)
features.9.conv.3 tensor(0.1701)
features.9.conv.6 tensor(0.5392)
features.10.conv.0 tensor(0.0874)
features.10.conv.3 tensor(0.0851)
features.10.conv.6 tensor(0.2706)
features.11.conv.0 tensor(0.7266)
features.11.conv.3 tensor(0.1530)
features.11.conv.6 tensor(0.8016)
features.12.conv.0 tensor(0.7652)
features.12.conv.3 tensor(0.1233)
features.12.conv.6 tensor(0.8783)
features.13.conv.0 tensor(0.4406)
features.13.conv.3 tensor(0.1395)
features.13.conv.6 tensor(0.5374)
features.14.conv.0 tensor(0.9799)
features.14.conv.3 tensor(0.0796)
features.14.conv.6 tensor(0.9936)
features.15.conv.0 tensor(0.9844)
features.15.conv.3 tensor(0.0787)
features.15.conv.6 tensor(0.4884)
features.16.conv.0 tensor(0.7572)
features.16.conv.3 tensor(0.1309)
features.16.conv.6 tensor(0.7800)
conv.0 tensor(0.8186)
tensor(1544569.) 2188896.0
INFO - Validation [0][   40/   40]   Loss 0.527173   Top1 82.560000   Top5 99.060000   BatchTime 0.093112
INFO - ==> Top1: 82.560    Top5: 99.060    Loss: 0.527
INFO - ==> Sparsity : 0.706
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.560   Top5: 99.060]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [1][   20/  196]   Loss 0.582532   Top1 79.433594   Top5 97.695312   BatchTime 0.435258   LR 0.004853
INFO - Training [1][   40/  196]   Loss 0.583104   Top1 79.628906   Top5 97.812500   BatchTime 0.384065   LR 0.004825
INFO - Training [1][   60/  196]   Loss 0.570378   Top1 80.000000   Top5 97.858073   BatchTime 0.354611   LR 0.004794
INFO - Training [1][   80/  196]   Loss 0.570413   Top1 80.039062   Top5 97.924805   BatchTime 0.349152   LR 0.004761
INFO - Training [1][  100/  196]   Loss 0.565324   Top1 80.292969   Top5 97.988281   BatchTime 0.355666   LR 0.004725
INFO - Training [1][  120/  196]   Loss 0.557119   Top1 80.611979   Top5 98.076172   BatchTime 0.352301   LR 0.004687
INFO - Training [1][  140/  196]   Loss 0.560132   Top1 80.613839   Top5 98.099888   BatchTime 0.350254   LR 0.004647
INFO - Training [1][  160/  196]   Loss 0.560746   Top1 80.578613   Top5 98.120117   BatchTime 0.349928   LR 0.004605
INFO - Training [1][  180/  196]   Loss 0.560214   Top1 80.529514   Top5 98.070747   BatchTime 0.349652   LR 0.004560
********************pre-trained*****************
INFO - ==> Top1: 79.414    Top5: 97.278    Loss: 0.588
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 129.338466   Top1 10.253906   Top5 50.214844   BatchTime 0.141923
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.6512)
features.16.conv.3 tensor(0.1157)
features.16.conv.6 tensor(0.7749)
conv.0 tensor(0.8474)
tensor(686150.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 129.811661   Top1 10.000000   Top5 50.000000   BatchTime 0.099822
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 129.812
INFO - ==> Sparsity : 0.313
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.560   Top5: 99.060]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 10.000   Top5: 50.000]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [2][   20/  196]   Loss 2.347130   Top1 9.414062   Top5 49.667969   BatchTime 0.424212   LR 0.004477
INFO - Training [2][   40/  196]   Loss 2.349835   Top1 9.580078   Top5 49.492188   BatchTime 0.385031   LR 0.004426
INFO - Training [2][   60/  196]   Loss 2.343809   Top1 9.524740   Top5 49.648438   BatchTime 0.371284   LR 0.004374
INFO - Training [2][   80/  196]   Loss 2.340528   Top1 9.448242   Top5 49.565430   BatchTime 0.351044   LR 0.004320
INFO - Training [2][  100/  196]   Loss 2.337079   Top1 9.617188   Top5 49.914062   BatchTime 0.351657   LR 0.004264
INFO - Training [2][  120/  196]   Loss 2.332750   Top1 9.746094   Top5 50.182292   BatchTime 0.354422   LR 0.004206
INFO - Training [2][  140/  196]   Loss 2.330411   Top1 9.665179   Top5 50.066964   BatchTime 0.352977   LR 0.004146
INFO - Training [2][  160/  196]   Loss 2.328286   Top1 9.746094   Top5 50.026855   BatchTime 0.356560   LR 0.004085
INFO - Training [2][  180/  196]   Loss 2.326547   Top1 9.756944   Top5 50.093316   BatchTime 0.359739   LR 0.004022
********************pre-trained*****************
INFO - ==> Top1: 9.718    Top5: 49.888    Loss: 2.326
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [2][   20/   40]   Loss 88.262582   Top1 10.253906   Top5 50.468750   BatchTime 0.139825
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.1293)
features.16.conv.3 tensor(0.0488)
features.16.conv.6 tensor(0.7719)
conv.0 tensor(0.8810)
tensor(618260.) 2188896.0
INFO - Validation [2][   40/   40]   Loss 88.634898   Top1 10.000000   Top5 50.000000   BatchTime 0.097530
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 88.635
INFO - ==> Sparsity : 0.282
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.560   Top5: 99.060]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 10.000   Top5: 50.000]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 10.000   Top5: 50.000]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [3][   20/  196]   Loss 2.318510   Top1 8.867188   Top5 49.433594   BatchTime 0.444349   LR 0.003907
INFO - Training [3][   40/  196]   Loss 2.313197   Top1 9.365234   Top5 50.000000   BatchTime 0.399767   LR 0.003840
INFO - Training [3][   60/  196]   Loss 2.311741   Top1 9.746094   Top5 49.882812   BatchTime 0.378194   LR 0.003771
INFO - Training [3][   80/  196]   Loss 2.310970   Top1 9.814453   Top5 49.848633   BatchTime 0.353755   LR 0.003701
INFO - Training [3][  100/  196]   Loss 2.310525   Top1 9.937500   Top5 49.882812   BatchTime 0.349386   LR 0.003630
INFO - Training [3][  120/  196]   Loss 2.310022   Top1 9.876302   Top5 49.902344   BatchTime 0.355947   LR 0.003558
INFO - Training [3][  140/  196]   Loss 2.309443   Top1 9.882812   Top5 49.997210   BatchTime 0.353760   LR 0.003484
INFO - Training [3][  160/  196]   Loss 2.308920   Top1 10.048828   Top5 50.095215   BatchTime 0.352098   LR 0.003410
INFO - Training [3][  180/  196]   Loss 2.308550   Top1 10.108507   Top5 50.188802   BatchTime 0.350601   LR 0.003335
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 10.102    Top5: 50.094    Loss: 2.308
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.1293)
features.16.conv.3 tensor(0.0502)
features.16.conv.6 tensor(0.7765)
conv.0 tensor(0.8878)
tensor(622495.) 2188896.0
INFO - Validation [3][   20/   40]   Loss 99.394653   Top1 10.253906   Top5 50.214844   BatchTime 0.146132
INFO - Validation [3][   40/   40]   Loss 99.808261   Top1 10.000000   Top5 50.000000   BatchTime 0.098995
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 99.808
INFO - ==> Sparsity : 0.284
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.560   Top5: 99.060]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 10.000   Top5: 50.000]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 10.000   Top5: 50.000]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [4][   20/  196]   Loss 2.305957   Top1 10.703125   Top5 49.121094   BatchTime 0.434053   LR 0.003200
INFO - Training [4][   40/  196]   Loss 2.306490   Top1 10.146484   Top5 49.726562   BatchTime 0.389627   LR 0.003122
INFO - Training [4][   60/  196]   Loss 2.305708   Top1 10.026042   Top5 49.876302   BatchTime 0.373845   LR 0.003044
INFO - Training [4][   80/  196]   Loss 2.305504   Top1 10.004883   Top5 49.921875   BatchTime 0.359688   LR 0.002965
INFO - Training [4][  100/  196]   Loss 2.305389   Top1 10.007812   Top5 49.816406   BatchTime 0.345156   LR 0.002886
INFO - Training [4][  120/  196]   Loss 2.305311   Top1 10.019531   Top5 49.837240   BatchTime 0.347002   LR 0.002806
INFO - Training [4][  140/  196]   Loss 2.305398   Top1 9.988839   Top5 49.796317   BatchTime 0.348370   LR 0.002726
INFO - Training [4][  160/  196]   Loss 2.305252   Top1 9.990234   Top5 49.855957   BatchTime 0.348699   LR 0.002646
INFO - Training [4][  180/  196]   Loss 2.305266   Top1 9.937066   Top5 49.841580   BatchTime 0.347826   LR 0.002566
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 9.934    Top5: 49.938    Loss: 2.305
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 85.017147   Top1 10.253906   Top5 50.214844   BatchTime 0.137099
INFO - Validation [4][   40/   40]   Loss 85.379832   Top1 10.000000   Top5 50.000000   BatchTime 0.096686
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.1293)
features.16.conv.3 tensor(0.0497)
features.16.conv.6 tensor(0.7773)
conv.0 tensor(0.8902)
tensor(623684.) 2188896.0
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 85.380
INFO - ==> Sparsity : 0.285
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.560   Top5: 99.060]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 10.000   Top5: 50.000]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 10.000   Top5: 50.000]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [5][   20/  196]   Loss 2.303798   Top1 9.921875   Top5 50.429688   BatchTime 0.421362   LR 0.002424
INFO - Training [5][   40/  196]   Loss 2.304775   Top1 9.423828   Top5 49.726562   BatchTime 0.381748   LR 0.002343
INFO - Training [5][   60/  196]   Loss 2.304328   Top1 9.720052   Top5 49.804688   BatchTime 0.368584   LR 0.002263
INFO - Training [5][   80/  196]   Loss 2.303952   Top1 9.960938   Top5 50.058594   BatchTime 0.359972   LR 0.002183
INFO - Training [5][  100/  196]   Loss 2.304436   Top1 9.832031   Top5 49.750000   BatchTime 0.348868   LR 0.002104
INFO - Training [5][  120/  196]   Loss 2.304633   Top1 9.889323   Top5 49.628906   BatchTime 0.346531   LR 0.002024
INFO - Training [5][  140/  196]   Loss 2.304491   Top1 9.907924   Top5 49.743304   BatchTime 0.345783   LR 0.001946
INFO - Training [5][  160/  196]   Loss 2.304476   Top1 9.794922   Top5 49.641113   BatchTime 0.347160   LR 0.001868
INFO - Training [5][  180/  196]   Loss 2.304421   Top1 9.819878   Top5 49.737413   BatchTime 0.347703   LR 0.001790
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 9.814    Top5: 49.648    Loss: 2.304
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [5][   20/   40]   Loss 79.298018   Top1 10.253906   Top5 50.214844   BatchTime 0.137076
INFO - Validation [5][   40/   40]   Loss 79.649358   Top1 10.000000   Top5 50.000000   BatchTime 0.095544
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.1293)
features.16.conv.3 tensor(0.0491)
features.16.conv.6 tensor(0.7780)
conv.0 tensor(0.8907)
tensor(624128.) 2188896.0
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 79.649
INFO - ==> Sparsity : 0.285
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.560   Top5: 99.060]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 10.000   Top5: 50.000]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 10.000   Top5: 50.000]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [6][   20/  196]   Loss 2.304530   Top1 9.765625   Top5 49.960938   BatchTime 0.444959   LR 0.001655
INFO - Training [6][   40/  196]   Loss 2.303916   Top1 10.156250   Top5 50.400391   BatchTime 0.396076   LR 0.001580
INFO - Training [6][   60/  196]   Loss 2.303623   Top1 9.986979   Top5 50.462240   BatchTime 0.376024   LR 0.001506
INFO - Training [6][   80/  196]   Loss 2.303325   Top1 10.136719   Top5 50.708008   BatchTime 0.364929   LR 0.001432
INFO - Training [6][  100/  196]   Loss 2.303240   Top1 10.121094   Top5 50.636719   BatchTime 0.358472   LR 0.001360
INFO - Training [6][  120/  196]   Loss 2.303246   Top1 10.208333   Top5 50.439453   BatchTime 0.342443   LR 0.001289
INFO - Training [6][  140/  196]   Loss 2.303210   Top1 10.164621   Top5 50.571987   BatchTime 0.338152   LR 0.001220
INFO - Training [6][  160/  196]   Loss 2.303300   Top1 10.109863   Top5 50.563965   BatchTime 0.338930   LR 0.001151
INFO - Training [6][  180/  196]   Loss 2.303230   Top1 10.112847   Top5 50.523003   BatchTime 0.338136   LR 0.001084
********************pre-trained*****************
INFO - ==> Top1: 10.124    Top5: 50.532    Loss: 2.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [6][   20/   40]   Loss 76.739457   Top1 10.253906   Top5 50.214844   BatchTime 0.132859
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.1293)
features.16.conv.3 tensor(0.0498)
features.16.conv.6 tensor(0.7781)
conv.0 tensor(0.8914)
tensor(624438.) 2188896.0
INFO - Validation [6][   40/   40]   Loss 77.073659   Top1 10.000000   Top5 50.000000   BatchTime 0.093398
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 77.074
INFO - ==> Sparsity : 0.285
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 82.560   Top5: 99.060]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 10.000   Top5: 50.000]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 10.000   Top5: 50.000]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-092958/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 87, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader, qat_model, teacher_model, criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 154, in train_one_epoch_slsq
    outputs = qat_model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 140, in forward
    x = self.features(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 95, in forward
    return self.conv(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1208, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 584, in forward
    return F.relu(ConvBn2d._forward(self, input))
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 101, in _forward
    return self._forward_approximate(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 114, in _forward_approximate
    scaled_weight = self.weight_fake_quant(self.weight * scale_factor.reshape(weight_shape))
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/quan/observer.py", line 529, in forward
    if self.observer_enabled[0] == 1:
KeyboardInterrupt