Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.0005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.0005
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.96300584
0.96269518
0.96095669
0.96116918
INFO - Training [0][   20/  196]   Loss 1.752371   Top1 38.867188   Top5 84.003906   BatchTime 0.329879   LR 0.000500
0.92139637
0.92970812
0.93482959
0.93042272
0.93134511
0.93032771
0.92781615
0.92464489
0.92297429
0.92184436
0.92019826
0.91879922
0.91758013
0.91604221
0.91468912
0.91329920
0.91225058
0.91103417
0.90987247
0.90916657
0.90861917
0.90803939
INFO - Training [0][   40/  196]   Loss 1.683268   Top1 41.367188   Top5 85.673828   BatchTime 0.296958   LR 0.000500
0.90743732
0.90630394
0.90525490
0.90498567
0.90430415
0.90404838
0.90368527
0.90362161
0.90306890
0.90293020
0.90243226
0.90228373
0.90206796
0.90131587
0.90073913
0.89982271
0.89988911
0.89964992
0.89971048
0.89940685
0.89937073
0.89920598
0.89928591
0.89940846
INFO - Training [0][   60/  196]   Loss 1.600473   Top1 44.127604   Top5 87.213542   BatchTime 0.281461   LR 0.000499
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 77, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader,qat_model, teacher_model,criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 186, in train_one_epoch_slsq
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
0.89956212
0.89934325