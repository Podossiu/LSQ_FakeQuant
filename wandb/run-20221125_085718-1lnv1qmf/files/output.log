Files already downloaded and verified
Files already downloaded and verified
********************pre-trained*****************
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
*************soft_pruning_mode*******************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.95438832
0.95021045
0.93092614
0.92863518
0.93068796
0.93493474
0.93088126
0.91980386
0.91907549
0.91931826
INFO - Training [0][   20/  196]   Loss 1.590196   Top1 53.359375   Top5 89.160156   BatchTime 0.376594   LR 0.004999
0.91866934
0.91784608
0.91654843
0.91783321
0.91957074
0.92144090
0.92248350
0.92269307
0.92263943
0.92221308
0.92138356
0.91729826
0.92170447
0.92326331
0.92329776
0.92332017
0.92354935
INFO - Training [0][   40/  196]   Loss 1.507331   Top1 52.158203   Top5 89.609375   BatchTime 0.370209   LR 0.004995
0.92382658
0.92373538
0.92382771
0.92542726
0.92575705
0.92521101
0.92513210
0.92526346
0.92534834
0.92507792
0.92506939
0.92487335
0.92497557
0.92506236
0.92546457
0.92545849
0.92580628
0.92572755
0.92609090
0.92638195
0.92679155
0.92706347
0.92709982
INFO - Training [0][   60/  196]   Loss 1.405974   Top1 54.576823   Top5 90.800781   BatchTime 0.389844   LR 0.004989
0.92716360
0.92715657
0.92710227
0.92698991
0.92748708
0.92777109
0.92833030
0.92849922
0.92873633
0.92869240
0.92911357
0.92936611
0.92913771
0.92920548
0.92902744
0.92900038
0.92923832
0.92932343
0.92949051
INFO - Training [0][   80/  196]   Loss 1.332692   Top1 56.708984   Top5 91.733398   BatchTime 0.397770   LR 0.004980
0.92928916
0.92920828
0.92898315
0.92905521
0.92906606
0.92902106
0.92905724
0.92911202
0.92945701
0.92910492
0.91614914
0.90539050
0.91590077
0.92196369
0.92141485
0.91784400
0.91619545
0.91009742
0.90246356
0.88403416
INFO - Training [0][  100/  196]   Loss 1.276072   Top1 58.265625   Top5 92.335938   BatchTime 0.399370   LR 0.004968
0.87567335
0.87622529
0.87607557
0.87671208
0.87735993
0.87726897
0.87794113
0.87830359
0.87835449
0.87864363
0.87850052
0.87826747
0.87826282
0.87739044
0.87669736
0.87738043
0.87947685
0.87949675
0.87982023
INFO - Training [0][  120/  196]   Loss 1.227787   Top1 59.941406   Top5 92.861328   BatchTime 0.401242   LR 0.004954
0.87957019
0.87946904
0.87957394
0.87939739
0.87968165
0.87972981
0.87995571
0.87891006
0.87705219
0.87489492
0.87060714
0.86436146
0.86086357
0.86326510
0.86485195
0.86791664
0.87194723
0.87545598
0.87931317
INFO - Training [0][  140/  196]   Loss 1.196027   Top1 60.987723   Top5 93.178013   BatchTime 0.405070   LR 0.004938
0.88197118
0.88229626
0.88294685
0.88364130
0.88398510
0.88465512
0.88523418
0.88537943
0.88530880
0.88547945
0.88549006
0.88565385
0.88614708
0.88624710
0.88651770
0.88688308
0.88689077
0.88720763
0.88714916
0.88747776
INFO - Training [0][  160/  196]   Loss 1.173873   Top1 61.623535   Top5 93.461914   BatchTime 0.405704   LR 0.004919
0.88742405
0.88782358
0.88816184
0.88790470
0.88741511
0.88718450
0.88704211
0.88720667
0.88695544
0.88674349
0.88670605
0.88696718
0.88681853
0.88683838
0.88699657
0.88678396
0.88631451
0.88605720
0.88598448
INFO - Training [0][  180/  196]   Loss 1.151029   Top1 62.211372   Top5 93.595920   BatchTime 0.406425   LR 0.004897
0.88560861
0.88528496
0.88506085
0.88470876
0.88448060
0.88435459
0.88431394
0.88373446
0.88317782
0.88273060
0.88284731
0.88257784
0.88228983
0.88201469
0.88164747
0.88143975
0.88151401
0.88138473
0.88116181
********************pre-trained*****************
INFO - ==> Top1: 62.820    Top5: 93.758    Loss: 1.132
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [0][   20/   40]   Loss 0.802677   Top1 74.101562   Top5 98.222656   BatchTime 0.151722
INFO - Validation [0][   40/   40]   Loss 0.798051   Top1 74.010000   Top5 98.280000   BatchTime 0.141863
INFO - ==> Top1: 74.010    Top5: 98.280    Loss: 0.798
INFO - ==> Sparsity : 0.256
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 74.010   Top5: 98.280]
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0751)
features.2.conv.0 tensor(0.0405)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0833)
features.3.conv.0 tensor(0.0460)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0751)
features.4.conv.0 tensor(0.0638)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.1045)
features.5.conv.0 tensor(0.0570)
features.5.conv.3 tensor(0.0619)
features.5.conv.6 tensor(0.1086)
features.6.conv.0 tensor(0.0462)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0932)
features.7.conv.0 tensor(0.0633)
features.7.conv.3 tensor(0.0903)
features.7.conv.6 tensor(0.1283)
features.8.conv.0 tensor(0.1182)
features.8.conv.3 tensor(0.0995)
features.8.conv.6 tensor(0.1326)
features.9.conv.0 tensor(0.1420)
features.9.conv.3 tensor(0.1152)
features.9.conv.6 tensor(0.1357)
features.10.conv.0 tensor(0.0777)
features.10.conv.3 tensor(0.0885)
features.10.conv.6 tensor(0.1110)
features.11.conv.0 tensor(0.1510)
features.11.conv.3 tensor(0.0847)
features.11.conv.6 tensor(0.1776)
features.12.conv.0 tensor(0.1307)
features.12.conv.3 tensor(0.0976)
features.12.conv.6 tensor(0.1646)
features.13.conv.0 tensor(0.0763)
features.13.conv.3 tensor(0.1235)
features.13.conv.6 tensor(0.1146)
features.14.conv.0 tensor(0.0808)
features.14.conv.3 tensor(0.0782)
features.14.conv.6 tensor(0.7733)
features.15.conv.0 tensor(0.8690)
features.15.conv.3 tensor(0.0777)
features.15.conv.6 tensor(0.8443)
features.16.conv.0 tensor(0.0783)
features.16.conv.3 tensor(0.0818)
features.16.conv.6 tensor(0.1400)
conv.0 tensor(0.0660)
tensor(559902.) 2188896.0
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.88129425
0.88140291
0.88130736
0.88132304
0.88151264
0.88160092
0.88140529
0.88183343
0.88185501
0.88190103
0.88195521
0.88201028
0.88218510
0.88189775
0.88181037
0.88186586
0.88163978
0.88130188
INFO - Training [1][   20/  196]   Loss 0.948655   Top1 68.554688   Top5 94.960938   BatchTime 0.431916   LR 0.004853
0.88113672
0.88111985
0.88101566
0.88117915
0.88107991
0.88098156
0.88085419
0.88060629
0.88059205
0.88086474
0.88114244
0.88033462
0.87966394
0.87897497
0.87756860
0.87428606
0.87108546
0.86343825
0.85906297
0.86988646
0.87518209
0.87670302
0.87705266
INFO - Training [1][   40/  196]   Loss 0.930417   Top1 69.541016   Top5 95.468750   BatchTime 0.398862   LR 0.004825
0.87646598
0.87659174
0.87740320
0.87738734
0.87694865
0.87672532
0.87625730
0.87532592
0.87363243
0.87014949
0.86380351
0.85588247
0.84926659
0.84167498
0.83558708
0.84991789
0.85167289
INFO - Training [1][   60/  196]   Loss 0.919951   Top1 69.778646   Top5 95.670573   BatchTime 0.383492   LR 0.004794
0.85200179
0.85198170
0.84762341
0.84247631
0.83435196
0.84398848
0.84989774
0.84916002
0.85407883
0.85421890
0.85451239
0.85456383
0.85441130
0.85412097
0.85378021
0.85313845
0.85311657
0.85323697
0.85286790
0.85250723
0.85224468
0.85220718
INFO - Training [1][   80/  196]   Loss 0.908856   Top1 69.956055   Top5 95.859375   BatchTime 0.378108   LR 0.004761
0.85222894
0.85222906
0.85212761
0.85222745
0.85233945
0.85248184
0.85256767
0.85241532
0.85311598
0.85290515
0.85269332
0.85270232
0.85248458
0.85200906
0.85168415
0.85128152
0.85138929
INFO - Training [1][  100/  196]   Loss 0.892778   Top1 70.589844   Top5 96.023438   BatchTime 0.373331   LR 0.004725
0.85131311
0.85169417
0.85199547
0.85178596
0.85168421
0.85153627
0.85176975
0.85195959
0.85234660
0.85236889
0.85195017
0.85204750
0.85202688
0.85209101
0.85199785
0.85225600
0.85225660
0.85224682
0.85245848
0.85221416
0.85216552
0.85223138
INFO - Training [1][  120/  196]   Loss 0.887223   Top1 70.817057   Top5 96.149089   BatchTime 0.371418   LR 0.004687
0.85225606
0.85220587
0.85225129
0.85261112
0.85294563
0.85309321
0.85269099
0.85258430
0.85269791
0.85264552
0.85257745
0.85240155
0.85259068
0.85233390
0.85211927
0.85182959
0.85153872
0.85131502
0.85044235
0.85045815
0.85060501
0.85038161
0.85018742
INFO - Training [1][  140/  196]   Loss 0.879315   Top1 71.063058   Top5 96.238839   BatchTime 0.369047   LR 0.004647
0.85020500
0.84994793
0.85000700
0.84990990
0.85114735
0.85086310
0.85080576
0.85081065
0.85039461
0.85039306
0.85059994
0.85065514
0.85040218
0.85042405
0.85003608
INFO - Training [1][  160/  196]   Loss 0.877277   Top1 71.069336   Top5 96.230469   BatchTime 0.369638   LR 0.004605
0.85029626
0.85076761
0.84999382
0.84989458
0.84987009
0.84965807
0.84909022
0.84914172
0.84939641
0.84859407
0.84885079
0.84885472
0.84854788
0.84815919
0.84786761
0.84757334
0.84743357
0.84742212
0.84767163
0.84839916
0.84766704
0.84711242
0.84717178
INFO - Training [1][  180/  196]   Loss 0.864759   Top1 71.467014   Top5 96.284722   BatchTime 0.368921   LR 0.004560
0.84692591
0.84650499
0.84666747
0.84580553
0.84528762
0.84446102
0.84493661
0.84499615
0.84676498
0.84623855
0.84394741
0.84120423
0.84481531
0.84816921
0.84825838
0.84821373
********************pre-trained*****************
INFO - ==> Top1: 71.614    Top5: 96.340    Loss: 0.859
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [1][   20/   40]   Loss 0.681107   Top1 78.515625   Top5 98.125000   BatchTime 0.114139
INFO - Validation [1][   40/   40]   Loss 0.690189   Top1 77.850000   Top5 98.260000   BatchTime 0.082762
INFO - ==> Top1: 77.850    Top5: 98.260    Loss: 0.690
INFO - ==> Sparsity : 0.345
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 77.850   Top5: 98.260]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 74.010   Top5: 98.280]
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0703)
features.2.conv.0 tensor(0.0422)
features.2.conv.3 tensor(0.0694)
features.2.conv.6 tensor(0.0897)
features.3.conv.0 tensor(0.0315)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0762)
features.4.conv.0 tensor(0.0648)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.1040)
features.5.conv.0 tensor(0.0479)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.1071)
features.6.conv.0 tensor(0.0407)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0900)
features.7.conv.0 tensor(0.0690)
features.7.conv.3 tensor(0.0952)
features.7.conv.6 tensor(0.1302)
features.8.conv.0 tensor(0.0928)
features.8.conv.3 tensor(0.1233)
features.8.conv.6 tensor(0.1313)
features.9.conv.0 tensor(0.1481)
features.9.conv.3 tensor(0.1215)
features.9.conv.6 tensor(0.1375)
features.10.conv.0 tensor(0.0774)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.1151)
features.11.conv.0 tensor(0.1806)
features.11.conv.3 tensor(0.0990)
features.11.conv.6 tensor(0.2368)
features.12.conv.0 tensor(0.1597)
features.12.conv.3 tensor(0.1028)
features.12.conv.6 tensor(0.1793)
features.13.conv.0 tensor(0.0840)
features.13.conv.3 tensor(0.1410)
features.13.conv.6 tensor(0.1128)
features.14.conv.0 tensor(0.8459)
features.14.conv.3 tensor(0.0826)
features.14.conv.6 tensor(0.8885)
features.15.conv.0 tensor(0.9046)
features.15.conv.3 tensor(0.0766)
features.15.conv.6 tensor(0.8923)
features.16.conv.0 tensor(0.0949)
features.16.conv.3 tensor(0.0858)
features.16.conv.6 tensor(0.2020)
conv.0 tensor(0.1075)
tensor(754230.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.84841448
0.84872121
0.84916723
0.84939450
0.84977871
0.85029429
0.85052389
0.85051221
0.85048175
0.85012102
0.84993798
0.85009944
0.84978968
0.84997803
0.85009146
0.85009974
0.85021311
0.85059708
INFO - Training [2][   20/  196]   Loss 0.847415   Top1 70.839844   Top5 95.683594   BatchTime 0.412683   LR 0.004477
0.85076690
0.85083586
0.85059983
0.85059106
0.85052365
0.85061586
0.85067785
0.85073417
0.85070735
0.85079855
0.85040146
0.84997803
0.84917325
0.84797746
0.84234297
0.83188301
0.82460904
0.81953019
0.81503665
0.82471836
0.83599007
0.82958752
INFO - Training [2][   40/  196]   Loss 0.829671   Top1 72.021484   Top5 96.093750   BatchTime 0.386869   LR 0.004426
0.83676785
0.84260172
0.84618396
0.84853226
0.85005146
0.85030782
0.84994149
0.85004079
0.85007846
0.85001361
0.84990871
0.84992331
0.85024697
0.85024172
0.85027647
0.84994823
0.84984142
INFO - Training [2][   60/  196]   Loss 0.814412   Top1 72.792969   Top5 96.334635   BatchTime 0.377694   LR 0.004374
0.84989411
0.84999776
0.84993780
0.84955907
0.84922588
0.84820491
0.84959555
0.84975576
0.84975487
0.84977746
0.84968042
0.84968352
0.84948766
0.84948140
0.84964591
0.84982359
0.84997344
0.85000575
0.84960961
0.84986788
0.84969431
0.84970206
INFO - Training [2][   80/  196]   Loss 0.793629   Top1 73.442383   Top5 96.582031   BatchTime 0.374129   LR 0.004320
0.84979552
0.84980130
0.84961277
0.84941566
0.84954149
0.84939587
0.84946346
0.84955555
0.84946948
0.84951508
0.84919792
0.84921402
0.84939700
0.84944916
0.84952509
0.84954542
0.84942615
0.84960282
0.84941447
0.84918374
0.84920281
0.84929198
INFO - Training [2][  100/  196]   Loss 0.782242   Top1 73.882812   Top5 96.671875   BatchTime 0.371470   LR 0.004264
0.84937584
0.84928966
0.84958994
0.84949619
0.84913951
0.84927571
0.84899336
0.84901565
0.84892899
0.84910554
0.84945130
0.84946376
0.84931779
0.84936804
0.84920377
0.84924740
0.84929889
0.84922796
0.84920335
0.84945315
0.84945095
INFO - Training [2][  120/  196]   Loss 0.774214   Top1 74.296875   Top5 96.770833   BatchTime 0.375500   LR 0.004206
0.84964144
0.84972864
0.84969926
0.84955096
0.84956986
0.84962767
0.84943521
0.84919727
0.84912670
0.84882128
0.84864199
0.84845406
0.84830546
0.84815347
0.84777755
0.84813029
INFO - Training [2][  140/  196]   Loss 0.770458   Top1 74.453125   Top5 96.872210   BatchTime 0.374135   LR 0.004146
0.84769946
0.84729993
0.84727055
0.84724057
0.84703773
0.84665018
0.84664303
0.84681374
0.84687215
0.84872836
0.84830540
0.84835392
0.84823376
0.84807372
0.84815472
0.84820163
0.84817970
0.84807128
0.84844726
0.84862459
0.84867263
0.84855926
INFO - Training [2][  160/  196]   Loss 0.772830   Top1 74.343262   Top5 96.850586   BatchTime 0.373051   LR 0.004085
0.84796715
0.84766018
0.84760725
0.84741443
0.84689462
0.84656227
0.84541202
0.84520364
0.84293574
0.83817977
0.83258682
0.82838488
0.82473499
0.82622790
0.82821137
0.82916057
0.82937407
INFO - Training [2][  180/  196]   Loss 0.769665   Top1 74.437934   Top5 96.803385   BatchTime 0.371452   LR 0.004022
0.82908565
0.82828748
0.82766682
0.82599282
0.82525951
0.82449257
0.82298952
0.82131314
0.82133061
0.82269090
0.82329768
0.82363927
0.82417870
0.82434434
0.82463521
0.82518470
0.82602429
0.82639664
INFO - ==> Top1: 74.568    Top5: 96.806    Loss: 0.766
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82695687
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.544404   Top1 81.464844   Top5 99.062500   BatchTime 0.130385
INFO - Validation [2][   40/   40]   Loss 0.543652   Top1 81.420000   Top5 99.140000   BatchTime 0.089956
INFO - ==> Top1: 81.420    Top5: 99.140    Loss: 0.544
INFO - ==> Sparsity : 0.359
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 81.420   Top5: 99.140]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 77.850   Top5: 98.260]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 74.010   Top5: 98.280]
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.2012)
features.1.conv.0 tensor(0.0312)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0425)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0900)
features.3.conv.0 tensor(0.0333)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0734)
features.4.conv.0 tensor(0.0558)
features.4.conv.3 tensor(0.1019)
features.4.conv.6 tensor(0.1073)
features.5.conv.0 tensor(0.0469)
features.5.conv.3 tensor(0.0561)
features.5.conv.6 tensor(0.1053)
features.6.conv.0 tensor(0.0179)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0904)
features.7.conv.0 tensor(0.0710)
features.7.conv.3 tensor(0.0978)
features.7.conv.6 tensor(0.1299)
features.8.conv.0 tensor(0.1046)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.1282)
features.9.conv.0 tensor(0.1394)
features.9.conv.3 tensor(0.1389)
features.9.conv.6 tensor(0.6086)
features.10.conv.0 tensor(0.0732)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.1015)
features.11.conv.0 tensor(0.1862)
features.11.conv.3 tensor(0.0993)
features.11.conv.6 tensor(0.2589)
features.12.conv.0 tensor(0.1464)
features.12.conv.3 tensor(0.1505)
features.12.conv.6 tensor(0.1900)
features.13.conv.0 tensor(0.0906)
features.13.conv.3 tensor(0.1468)
features.13.conv.6 tensor(0.0889)
features.14.conv.0 tensor(0.8884)
features.14.conv.3 tensor(0.0836)
features.14.conv.6 tensor(0.9050)
features.15.conv.0 tensor(0.9220)
features.15.conv.3 tensor(0.0747)
features.15.conv.6 tensor(0.9162)
features.16.conv.0 tensor(0.1093)
features.16.conv.3 tensor(0.0854)
features.16.conv.6 tensor(0.1959)
conv.0 tensor(0.1195)
tensor(785641.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.82769066
0.82850981
0.82991534
0.83114934
0.83198816
0.83273768
0.83336526
0.83393347
0.83413571
0.83420789
0.83467352
0.83507842
0.83524758
0.83559185
0.83583891
0.83579803
0.83588749
0.83619648
0.83603507
0.83593202
INFO - Training [3][   20/  196]   Loss 0.738320   Top1 75.703125   Top5 96.894531   BatchTime 0.422665   LR 0.003907
0.83601105
0.83590430
0.83574271
0.83579493
0.83570206
0.83586937
0.83576751
0.83590579
0.83596098
0.83560073
0.83541781
0.83523554
0.83502591
0.83483416
0.83468449
0.83447623
0.83444494
0.83439553
0.83436036
0.83445507
0.83440769
INFO - Training [3][   40/  196]   Loss 0.734428   Top1 76.035156   Top5 96.923828   BatchTime 0.401732   LR 0.003840
0.83453751
0.83487463
0.83511335
0.83495528
0.83511406
0.83485073
0.83387834
0.83356827
0.83359694
0.83365065
0.83351123
0.83318508
0.83306646
0.83301574
0.83277512
0.83259189
0.83337897
0.83407468
0.83371246
0.83424717
0.83437258
INFO - Training [3][   60/  196]   Loss 0.731420   Top1 75.872396   Top5 97.128906   BatchTime 0.394544   LR 0.003771
0.83430380
0.83396387
0.83238268
0.83165169
0.83027887
0.82986230
0.83161509
0.83155608
0.83126384
0.83056593
0.82975364
0.82795554
0.82743359
0.82827795
0.82993561
0.83304721
INFO - Training [3][   80/  196]   Loss 0.721601   Top1 76.235352   Top5 97.270508   BatchTime 0.388231   LR 0.003701
0.83333439
0.83339369
0.83344162
0.83326435
0.83279985
0.83293188
0.83258712
0.83259833
0.83259952
0.83259267
0.83218712
0.83207327
0.83202565
0.83199960
0.83237922
0.83277106
0.83280385
0.83293080
0.83299965
0.83273077
0.83282316
0.83274180
INFO - Training [3][  100/  196]   Loss 0.709377   Top1 76.667969   Top5 97.339844   BatchTime 0.382389   LR 0.003630
0.83244854
0.83217418
0.83183253
0.83155894
0.83125031
0.83084106
0.83052307
0.83021039
0.82976156
0.82959861
0.82943743
0.82931429
0.82897174
0.82874691
0.82834691
0.82786709
0.82729858
INFO - Training [3][  120/  196]   Loss 0.704055   Top1 76.868490   Top5 97.376302   BatchTime 0.378766   LR 0.003558
0.82704020
0.82641500
0.82599026
0.82567078
0.82501918
0.82430273
0.82373810
0.82331580
0.82378769
0.82389069
0.82414734
0.82445681
0.82483268
0.82516766
0.82540578
0.82597929
0.82608974
0.82624626
0.82610893
0.83185107
0.83062935
0.82928860
INFO - Training [3][  140/  196]   Loss 0.701286   Top1 77.003348   Top5 97.405134   BatchTime 0.376132   LR 0.003484
0.82716244
0.82474631
0.82078564
0.82799149
0.83141184
0.83202827
0.83294827
0.83373570
0.83442777
0.83460480
0.83481365
0.83540690
0.83581018
0.83611465
0.83754748
0.83876568
0.84066159
0.84030199
0.84034520
0.84030199
0.84005618
0.83978564
0.83951747
INFO - Training [3][  160/  196]   Loss 0.701514   Top1 76.972656   Top5 97.380371   BatchTime 0.373526   LR 0.003410
0.83893865
0.83814913
0.83801293
0.83778787
0.83753133
0.83650851
0.83449107
0.83382493
0.83309197
0.83333528
0.83301139
0.83224779
0.82973599
0.83099949
INFO - Training [3][  180/  196]   Loss 0.698833   Top1 77.020399   Top5 97.326389   BatchTime 0.364793   LR 0.003335
0.83163470
0.83182979
0.82860237
0.82021576
0.82112461
0.81774092
0.81132704
0.80780208
0.80903441
0.81374699
0.81845689
0.82579327
0.83007705
0.83315426
0.83666474
0.83699501
0.83726805
0.83724982
INFO - ==> Top1: 77.074    Top5: 97.302    Loss: 0.696
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83735043
0.83705616
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.587950   Top1 81.386719   Top5 98.144531   BatchTime 0.125254
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.2051)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0677)
features.2.conv.0 tensor(0.0411)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0909)
features.3.conv.0 tensor(0.0295)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0686)
features.4.conv.0
INFO - Validation [3][   40/   40]   Loss 0.579470   Top1 81.620000   Top5 98.390000   BatchTime 0.089567
INFO - ==> Top1: 81.620    Top5: 98.390    Loss: 0.579
INFO - ==> Sparsity : 0.368
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 81.620   Top5: 98.390]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 81.420   Top5: 99.140]
features.4.conv.0 tensor(0.0516)
features.4.conv.3 tensor(0.1013)
features.4.conv.6 tensor(0.1040)
features.5.conv.0 tensor(0.0469)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.1058)
features.6.conv.0 tensor(0.0335)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0893)
features.7.conv.0 tensor(0.0664)
features.7.conv.3 tensor(0.0938)
features.7.conv.6 tensor(0.1387)
features.8.conv.0 tensor(0.1157)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.1295)
features.9.conv.0 tensor(0.1132)
features.9.conv.3 tensor(0.1383)
features.9.conv.6 tensor(0.3081)
features.10.conv.0 tensor(0.0711)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.0929)
features.11.conv.0 tensor(0.1652)
features.11.conv.3 tensor(0.1009)
features.11.conv.6 tensor(0.2163)
features.12.conv.0 tensor(0.3411)
features.12.conv.3 tensor(0.1532)
features.12.conv.6 tensor(0.2095)
features.13.conv.0 tensor(0.1032)
features.13.conv.3 tensor(0.1518)
features.13.conv.6 tensor(0.0645)
features.14.conv.0 tensor(0.9105)
features.14.conv.3 tensor(0.0801)
features.14.conv.6 tensor(0.9060)
features.15.conv.0 tensor(0.9335)
features.15.conv.3 tensor(0.0753)
features.15.conv.6 tensor(0.9179)
features.16.conv.0 tensor(0.0989)
features.16.conv.3 tensor(0.0896)
features.16.conv.6 tensor(0.2538)
conv.0 tensor(0.1167)
tensor(805067.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.83577216
0.83623016
0.83509600
0.83396119
0.83219063
0.82424736
0.83268547
0.83478516
0.83643442
0.83627152
0.83625120
0.83632797
0.83690792
0.83691555
0.83702576
0.83734471
0.83760047
0.83776349
0.83827668
INFO - Training [4][   20/  196]   Loss 0.668816   Top1 78.183594   Top5 96.484375   BatchTime 0.421849   LR 0.003200
0.83835930
0.83831304
0.83831871
0.83889443
0.83892536
0.83921731
0.83938807
0.83959603
0.83936054
0.83961511
0.83992523
0.83997178
0.84000814
0.84005761
0.83999211
0.84003270
0.83987081
INFO - Training [4][   40/  196]   Loss 0.669701   Top1 78.183594   Top5 96.972656   BatchTime 0.394399   LR 0.003122
0.83985847
0.83991104
0.83964562
0.83990985
0.83980834
0.83999014
0.84037995
0.84054631
0.84036905
0.84033209
0.84013706
0.84002501
0.84001237
0.84006876
0.83973593
0.83940691
0.83914292
0.83853579
0.83818841
0.83791935
0.83738703
0.83724159
INFO - Training [4][   60/  196]   Loss 0.696636   Top1 77.265625   Top5 97.011719   BatchTime 0.384071   LR 0.003044
0.83716637
0.83718604
0.83709109
0.83725905
0.83707803
0.83780479
0.83801228
0.83739412
0.83744907
0.83748084
0.83717138
0.83723742
0.83730549
0.83779293
0.83800393
0.83834469
0.83844298
0.83846503
0.83852261
0.83866131
0.83851433
0.83850592
INFO - Training [4][   80/  196]   Loss 0.700616   Top1 77.045898   Top5 97.148438   BatchTime 0.376442   LR 0.002965
0.83843976
0.83834296
0.83830857
0.83819968
0.83804941
0.83796144
0.83808011
0.83795559
0.83771497
0.83757985
0.83712918
0.83673126
0.83641422
0.83596569
0.83534431
0.83447409
0.83341175
INFO - Training [4][  100/  196]   Loss 0.691519   Top1 77.359375   Top5 97.246094   BatchTime 0.372294   LR 0.002886
0.83083081
0.83148074
0.83233732
0.83475596
0.83460927
0.83528680
0.83546132
0.83549386
0.83524215
0.83514076
0.83486682
0.83460373
0.83430469
0.83419579
0.83398181
0.83386266
0.83365571
0.83337075
0.83342975
0.83291394
0.83271021
0.83245510
INFO - Training [4][  120/  196]   Loss 0.684515   Top1 77.558594   Top5 97.360026   BatchTime 0.370920   LR 0.002806
0.83214056
0.83163726
0.83140278
0.83095324
0.83057910
0.83019978
0.82957375
0.82885236
0.82842904
0.82789332
0.82720882
0.82615268
0.82553267
0.82510692
0.82402259
0.82324308
INFO - Training [4][  140/  196]   Loss 0.681591   Top1 77.698103   Top5 97.399554   BatchTime 0.370195   LR 0.002726
0.82048720
0.82047653
0.81998175
0.82068139
0.82070214
0.82052809
0.81976461
0.81917816
0.81866509
0.81864089
0.81792885
0.81713712
0.81654215
0.81527609
0.81395340
0.81295669
0.81298113
0.81211227
0.80950129
0.80701911
INFO - Training [4][  160/  196]   Loss 0.681651   Top1 77.690430   Top5 97.395020   BatchTime 0.361177   LR 0.002646
0.80631834
0.80581152
0.80670512
0.80763346
0.80999947
0.81202507
0.81349945
0.81474721
0.81603175
0.81599957
0.81301725
0.80194396
0.79577780
0.79482520
0.79575473
0.80113679
0.81271285
0.82163936
0.82645977
0.82888055
0.82943010
0.82940102
INFO - Training [4][  180/  196]   Loss 0.676157   Top1 77.840712   Top5 97.358941   BatchTime 0.364001   LR 0.002566
0.82907289
0.82879168
0.82928884
0.82981646
0.82994556
0.82940537
0.82870024
0.82898974
0.82852137
0.82738239
0.82764560
0.82908261
0.83055496
0.83199990
0.83216029
0.83221281
INFO - ==> Top1: 77.916    Top5: 97.364    Loss: 0.673
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83179140
0.83136338
0.83106858
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.514025   Top1 82.265625   Top5 99.023438   BatchTime 0.123871
INFO - Validation [4][   40/   40]   Loss 0.512515   Top1 82.350000   Top5 99.150000   BatchTime 0.087677
INFO - ==> Top1: 82.350    Top5: 99.150    Loss: 0.513
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 82.350   Top5: 99.150]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 81.620   Top5: 98.390]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 81.420   Top5: 99.140]
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0293)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0729)
features.2.conv.0 tensor(0.0454)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0938)
features.3.conv.0 tensor(0.0310)
features.3.conv.3 tensor(0.0563)
features.3.conv.6 tensor(0.0710)
features.4.conv.0 tensor(0.0477)
features.4.conv.3 tensor(0.1059)
features.4.conv.6 tensor(0.0985)
features.5.conv.0 tensor(0.0347)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.1086)
features.6.conv.0 tensor(0.0286)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0872)
features.7.conv.0 tensor(0.0693)
features.7.conv.3 tensor(0.1024)
features.7.conv.6 tensor(0.1292)
features.8.conv.0 tensor(0.1130)
features.8.conv.3 tensor(0.1296)
features.8.conv.6 tensor(0.1249)
features.9.conv.0 tensor(0.1259)
features.9.conv.3 tensor(0.1337)
features.9.conv.6 tensor(0.3507)
features.10.conv.0 tensor(0.0682)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.2289)
features.11.conv.0 tensor(0.1907)
features.11.conv.3 tensor(0.1061)
features.11.conv.6 tensor(0.2237)
features.12.conv.0 tensor(0.2205)
features.12.conv.3 tensor(0.1570)
features.12.conv.6 tensor(0.3258)
features.13.conv.0 tensor(0.1077)
features.13.conv.3 tensor(0.1489)
features.13.conv.6 tensor(0.0797)
features.14.conv.0 tensor(0.9155)
features.14.conv.3 tensor(0.0807)
features.14.conv.6 tensor(0.9158)
features.15.conv.0 tensor(0.9434)
features.15.conv.3 tensor(0.0784)
features.15.conv.6 tensor(0.9292)
features.16.conv.0 tensor(0.1035)
features.16.conv.3 tensor(0.0891)
features.16.conv.6 tensor(0.2374)
conv.0 tensor(0.1186)
tensor(816172.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.83070368
0.83060914
0.83031011
0.83031178
0.83053416
0.83069450
0.83076328
0.83029723
0.83046138
0.83053643
0.83095104
0.83202159
0.83202529
0.83180130
0.83143640
0.83116758
0.83097643
0.83066261
INFO - Training [5][   20/  196]   Loss 0.626128   Top1 79.296875   Top5 97.128906   BatchTime 0.431445   LR 0.002424
0.83036983
0.83016127
0.82966679
0.82970601
0.82916242
0.82855308
0.82845509
0.82845330
0.82858711
0.82841533
0.82832569
0.82804018
0.82782620
0.82796907
0.82812470
0.82952321
0.82968104
0.82939368
0.82904208
0.82899052
0.82849467
0.82798892
INFO - Training [5][   40/  196]   Loss 0.646881   Top1 78.261719   Top5 97.187500   BatchTime 0.397391   LR 0.002343
0.82760942
0.82737774
0.82647222
0.82608134
0.82583809
0.82573843
0.82542282
0.82554626
0.82488495
0.82444543
0.82418758
0.82378721
0.82303137
0.82206738
0.82036144
0.81769180
INFO - Training [5][   60/  196]   Loss 0.638664   Top1 78.704427   Top5 97.311198   BatchTime 0.389018   LR 0.002263
0.81551927
0.81492972
0.81473261
0.81369495
0.81238919
0.81172639
0.81097192
0.81136978
0.81212294
0.81237882
0.81336415
0.81321180
0.81216115
0.81109148
0.81038177
0.80996525
0.80992228
0.80991459
0.81007195
0.81065750
0.81140190
0.81270152
INFO - Training [5][   80/  196]   Loss 0.625063   Top1 79.121094   Top5 97.412109   BatchTime 0.382235   LR 0.002183
0.81381917
0.81583405
0.81691819
0.81779712
0.81828904
0.81916416
0.81999749
0.82030779
0.82042390
0.82019174
0.81990206
0.81910574
0.81865293
0.81879514
0.81869328
0.81839287
0.81811857
0.81806159
0.81807750
0.81781822
0.81726068
INFO - Training [5][  100/  196]   Loss 0.614634   Top1 79.492188   Top5 97.550781   BatchTime 0.382134   LR 0.002104
0.81744140
0.81742197
0.81644785
0.81482786
0.81309623
0.81337720
0.81422895
0.81385601
0.81343156
0.81325257
0.81269240
0.81196249
0.81167835
0.81111896
0.81134725
0.81420159
0.81892264
0.81899178
INFO - Training [5][  120/  196]   Loss 0.608783   Top1 79.726562   Top5 97.630208   BatchTime 0.373692   LR 0.002024
0.81893677
0.81919509
0.81913722
0.81881446
0.81872404
0.81857514
0.81866324
0.81851554
0.81805378
0.81790239
0.81746089
0.81724977
0.81676519
0.81647515
0.81627250
0.81577426
0.81536788
0.81444585
0.81328923
0.81261927
INFO - Training [5][  140/  196]   Loss 0.607897   Top1 79.765625   Top5 97.689732   BatchTime 0.365040   LR 0.001946
0.81200695
0.81152934
0.81074333
0.80996031
0.80931944
0.80883688
0.80834514
0.80849266
0.80888480
0.80909336
0.80954027
0.80967075
0.80971694
0.80966055
0.80953038
0.80952996
0.80930555
0.80915785
0.80869901
0.80772376
0.80743271
0.80714834
INFO - Training [5][  160/  196]   Loss 0.609740   Top1 79.699707   Top5 97.690430   BatchTime 0.364982   LR 0.001868
0.80804992
0.80850577
0.80762208
0.80155349
0.79455984
0.79696858
0.80751157
0.81233293
0.81506741
0.81566679
0.81629014
0.81651294
0.81670177
0.81670427
0.81645465
0.81605500
0.81552178
INFO - Training [5][  180/  196]   Loss 0.609051   Top1 79.691840   Top5 97.649740   BatchTime 0.363567   LR 0.001790
0.81535900
0.81543696
0.81585920
0.81605738
0.81632513
0.81791699
0.81782240
0.81743264
0.81725186
0.81726730
0.81700641
0.81685638
0.81680608
0.81645966
0.81653619
0.81622529
INFO - ==> Top1: 79.772    Top5: 97.662    Loss: 0.607
0.81631982
0.81579548
0.81560147
0.81533825
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.439598   Top1 85.175781   Top5 99.238281   BatchTime 0.116503
INFO - Validation [5][   40/   40]   Loss 0.431002   Top1 85.280000   Top5 99.300000   BatchTime 0.085195
INFO - ==> Top1: 85.280    Top5: 99.300    Loss: 0.431
INFO - ==> Sparsity : 0.386
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 85.280   Top5: 99.300]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 82.350   Top5: 99.150]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 81.620   Top5: 98.390]
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.2188)
features.1.conv.0 tensor(0.0326)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0720)
features.2.conv.0 tensor(0.0405)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0880)
features.3.conv.0 tensor(0.0312)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0692)
features.4.conv.0 tensor(0.0573)
features.4.conv.3 tensor(0.1047)
features.4.conv.6 tensor(0.1128)
features.5.conv.0 tensor(0.0433)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.1029)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0844)
features.7.conv.0 tensor(0.0649)
features.7.conv.3 tensor(0.1019)
features.7.conv.6 tensor(0.1367)
features.8.conv.0 tensor(0.1093)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.1239)
features.9.conv.0 tensor(0.1160)
features.9.conv.3 tensor(0.1369)
features.9.conv.6 tensor(0.3786)
features.10.conv.0 tensor(0.0630)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.0986)
features.11.conv.0 tensor(0.2063)
features.11.conv.3 tensor(0.1042)
features.11.conv.6 tensor(0.2219)
features.12.conv.0 tensor(0.4079)
features.12.conv.3 tensor(0.1559)
features.12.conv.6 tensor(0.5367)
features.13.conv.0 tensor(0.1092)
features.13.conv.3 tensor(0.1439)
features.13.conv.6 tensor(0.1065)
features.14.conv.0 tensor(0.9234)
features.14.conv.3 tensor(0.0803)
features.14.conv.6 tensor(0.9269)
features.15.conv.0 tensor(0.9492)
features.15.conv.3 tensor(0.0781)
features.15.conv.6 tensor(0.9370)
features.16.conv.0 tensor(0.1186)
features.16.conv.3 tensor(0.0975)
features.16.conv.6 tensor(0.2425)
conv.0 tensor(0.1148)
tensor(844413.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.81490892
0.81444871
0.81371731
0.81335348
0.81309187
0.81258810
0.81151199
0.80889404
0.80659211
0.80783606
0.80690396
0.80525225
0.80470806
0.80714154
0.80690515
0.80545962
0.80421078
0.80395991
0.80241281
INFO - Training [6][   20/  196]   Loss 0.613820   Top1 79.335938   Top5 97.539062   BatchTime 0.426904   LR 0.001655
0.79800951
0.79636401
0.79546225
0.79686338
0.79925299
0.80019361
0.80113083
0.80224186
0.80188936
0.80082792
0.79985750
0.79815555
0.79734933
0.79546601
0.79285532
0.78812844
0.78967655
INFO - Training [6][   40/  196]   Loss 0.602554   Top1 79.541016   Top5 97.636719   BatchTime 0.395902   LR 0.001580
0.79708380
0.80156291
0.80428511
0.80647713
0.80834109
0.80985564
0.81100386
0.81133354
0.81141889
0.81162471
0.81166303
0.81156206
0.81134504
0.81128806
0.81118846
0.81135142
0.81114399
0.81132162
0.81139332
0.81151187
INFO - Training [6][   60/  196]   Loss 0.586726   Top1 80.260417   Top5 97.786458   BatchTime 0.391386   LR 0.001506
0.81267762
0.81346947
0.81362426
0.81382883
0.81381762
0.81396329
0.81401694
0.81334710
0.81346142
0.81309563
0.81249797
0.81210113
0.81178039
0.81137788
0.81108713
0.81090534
0.81073815
0.81060207
0.81051028
0.81044126
0.81046718
0.81048656
INFO - Training [6][   80/  196]   Loss 0.576367   Top1 80.830078   Top5 97.958984   BatchTime 0.385831   LR 0.001432
0.81046706
0.81041932
0.81033611
0.81034273
0.81034517
0.81031686
0.81004608
0.80974072
0.80908608
0.80880082
0.80846226
0.80812430
0.80779773
0.80759072
0.80757570
0.80758625
0.80741537
0.80715972
INFO - Training [6][  100/  196]   Loss 0.566202   Top1 81.191406   Top5 97.972656   BatchTime 0.375241   LR 0.001360
0.80701661
0.80674881
0.80678785
0.80672789
0.80655521
0.80635953
0.80631196
0.80616719
0.80755818
0.80728561
0.80666047
0.80639052
0.80611140
0.80598348
0.80571121
0.80548668
0.80533725
0.80551970
0.80687726
0.80692518
INFO - Training [6][  120/  196]   Loss 0.561859   Top1 81.344401   Top5 98.037109   BatchTime 0.361766   LR 0.001289
0.80682522
0.80661619
0.80629271
0.80611831
0.80597281
0.80577338
0.80556899
0.80505389
0.80458593
0.80449599
0.80617040
0.80578130
0.80534691
0.80522335
0.80517346
0.80509567
0.80503982
0.80536646
0.80505472
0.80506873
INFO - Training [6][  140/  196]   Loss 0.562024   Top1 81.336496   Top5 98.069196   BatchTime 0.355109   LR 0.001220
0.80500782
0.80501956
0.80524510
0.80532473
0.80519956
0.80526626
0.80499715
0.80498898
0.80515230
0.80496985
0.80503619
0.80508888
0.80625695
0.80648887
0.80642164
0.80620909
0.80600470
0.80596799
0.80594826
0.80604845
0.80607450
0.80577904
INFO - Training [6][  160/  196]   Loss 0.562091   Top1 81.293945   Top5 98.056641   BatchTime 0.356786   LR 0.001151
0.80583167
0.80596858
0.80580378
0.80570853
0.80573028
0.80595917
0.80582130
0.80560070
0.80554157
0.80525124
0.80518520
0.80500543
0.80457932
0.80430853
0.80412072
0.80389267
0.80395013
0.80375075
0.80350965
0.80280739
0.80217892
0.80223876
INFO - Training [6][  180/  196]   Loss 0.560433   Top1 81.284722   Top5 97.979601   BatchTime 0.357478   LR 0.001084
0.80220163
0.80209374
0.80199885
0.80174512
0.80159354
0.80123496
0.80059838
0.79993922
0.79928744
0.79793745
0.79697090
0.79665047
0.79563320
0.79412401
0.79122245
0.78947914
INFO - ==> Top1: 81.290    Top5: 97.994    Loss: 0.561
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.496248   Top1 83.261719   Top5 98.652344   BatchTime 0.121102
INFO - Validation [6][   40/   40]   Loss 0.490721   Top1 83.420000   Top5 98.780000   BatchTime 0.088067
INFO - ==> Top1: 83.420    Top5: 98.780    Loss: 0.491
INFO - ==> Sparsity : 0.402
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 85.280   Top5: 99.300]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 83.420   Top5: 98.780]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 82.350   Top5: 99.150]
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.2695)
features.1.conv.0 tensor(0.0267)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0694)
features.2.conv.0 tensor(0.0399)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0888)
features.3.conv.0 tensor(0.0292)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0710)
features.4.conv.0 tensor(0.0658)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.1022)
features.5.conv.0 tensor(0.0438)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0921)
features.6.conv.0 tensor(0.0288)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0838)
features.7.conv.0 tensor(0.0621)
features.7.conv.3 tensor(0.1016)
features.7.conv.6 tensor(0.1320)
features.8.conv.0 tensor(0.1037)
features.8.conv.3 tensor(0.1409)
features.8.conv.6 tensor(0.1210)
features.9.conv.0 tensor(0.1208)
features.9.conv.3 tensor(0.1366)
features.9.conv.6 tensor(0.3933)
features.10.conv.0 tensor(0.0567)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.0980)
features.11.conv.0 tensor(0.6304)
features.11.conv.3 tensor(0.1055)
features.11.conv.6 tensor(0.4433)
features.12.conv.0 tensor(0.3356)
features.12.conv.3 tensor(0.1555)
features.12.conv.6 tensor(0.5557)
features.13.conv.0 tensor(0.1261)
features.13.conv.3 tensor(0.1456)
features.13.conv.6 tensor(0.1230)
features.14.conv.0 tensor(0.9273)
features.14.conv.3 tensor(0.0796)
features.14.conv.6 tensor(0.9338)
features.15.conv.0 tensor(0.9513)
features.15.conv.3 tensor(0.0766)
features.15.conv.6 tensor(0.9371)
features.16.conv.0 tensor(0.1006)
features.16.conv.3 tensor(0.0938)
features.16.conv.6 tensor(0.2475)
conv.0 tensor(0.1122)
tensor(879215.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.78519094
0.78101593
0.77885932
0.77841043
0.78136498
0.78502417
0.78653187
0.78792721
0.78874314
0.78861862
0.78823447
0.78754097
0.78910553
0.79000908
0.79248399
0.79429001
0.79578477
0.79719466
INFO - Training [7][   20/  196]   Loss 0.558127   Top1 81.601562   Top5 97.500000   BatchTime 0.455752   LR 0.000969
0.79843456
0.79939318
0.79994112
0.80063516
0.80100322
0.80142760
0.80179316
0.80218428
0.80257469
0.80254281
0.80252802
0.80248368
0.80223781
0.80184633
0.80166495
0.80167454
0.80161232
0.80157828
0.80152422
0.80128813
0.80119741
0.80125934
INFO - Training [7][   40/  196]   Loss 0.553668   Top1 81.650391   Top5 97.871094   BatchTime 0.408589   LR 0.000907
0.80154461
0.80180997
0.80180317
0.80158615
0.80122477
0.80089480
0.80055040
0.80006176
0.79960674
0.79957765
0.79913956
0.79873365
0.79795265
0.79789019
0.79796267
0.79795432
INFO - Training [7][   60/  196]   Loss 0.547414   Top1 81.881510   Top5 97.916667   BatchTime 0.396430   LR 0.000845
0.79806298
0.79801691
0.79818672
0.79783797
0.79693151
0.79705822
0.79699844
0.79672587
0.79643667
0.79625243
0.79630768
0.79751658
0.79755992
0.79741013
0.79718667
0.79699379
0.79684991
0.79656237
0.79663634
0.79648894
0.79643375
0.79627085
INFO - Training [7][   80/  196]   Loss 0.544133   Top1 81.982422   Top5 97.998047   BatchTime 0.389874   LR 0.000786
0.79611808
0.79577941
0.79594344
0.79698479
0.79646033
0.79637146
0.79624361
0.79593098
0.79618669
0.79629570
0.79626513
0.79617649
0.79617274
0.79616791
0.79772955
0.79760563
0.79743141
0.79745620
0.79735845
0.79709345
0.79682511
0.79634464
INFO - Training [7][  100/  196]   Loss 0.537765   Top1 82.148438   Top5 98.082031   BatchTime 0.383076   LR 0.000728
0.79574800
0.79507482
0.79412115
0.79413378
0.79473460
0.79529643
0.79616761
0.79650402
0.79694313
0.79744673
0.79721409
0.79718482
0.79720527
0.79719770
0.79716098
0.79697436
0.79692256
0.79678738
0.79675317
INFO - Training [7][  120/  196]   Loss 0.532709   Top1 82.265625   Top5 98.173828   BatchTime 0.372509   LR 0.000673
0.79680198
0.79690599
0.79646826
0.79651082
0.79655707
0.79657102
0.79677880
0.79668564
0.79648453
0.79630953
0.79639792
0.79623425
0.79591888
0.79607749
0.79603070
0.79564804
0.79517686
0.79498369
0.79515630
0.79516882
0.79496145
0.79491079
INFO - Training [7][  140/  196]   Loss 0.529653   Top1 82.385603   Top5 98.233817   BatchTime 0.357670   LR 0.000619
0.79475921
0.79478884
0.79475158
0.79461557
0.79471290
0.79463619
0.79462433
0.79455310
0.79452717
0.79463404
0.79458833
0.79462868
0.79468733
0.79463989
0.79470855
0.79470986
0.79475212
0.79471976
0.79456258
INFO - Training [7][  160/  196]   Loss 0.532502   Top1 82.277832   Top5 98.210449   BatchTime 0.353444   LR 0.000567
0.79423749
0.79401344
0.79367572
0.79342550
0.79289490
0.79264295
0.79238468
0.79226977
0.79220378
0.79205227
0.79193944
0.79184592
0.79171616
0.79163581
0.79139698
0.79103351
INFO - Training [7][  180/  196]   Loss 0.530421   Top1 82.345920   Top5 98.148872   BatchTime 0.354149   LR 0.000517
0.79075485
0.79050797
0.79026127
0.78997505
0.78980100
0.78947502
0.78927827
0.78888506
0.78838611
0.78786099
0.78673607
0.78600568
0.78565454
0.78498751
0.78448039
0.78438783
INFO - ==> Top1: 82.396    Top5: 98.158    Loss: 0.527
0.78412455
0.78397804
0.78405464
0.78413123
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.550708   Top1 81.152344   Top5 98.730469   BatchTime 0.113566
INFO - Validation [7][   40/   40]   Loss 0.537328   Top1 81.530000   Top5 98.910000   BatchTime 0.081751
INFO - ==> Top1: 81.530    Top5: 98.910    Loss: 0.537
INFO - ==> Sparsity : 0.402
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 85.280   Top5: 99.300]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 83.420   Top5: 98.780]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 82.350   Top5: 99.150]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.3086)
features.1.conv.0 tensor(0.0280)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0420)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0891)
features.3.conv.0 tensor(0.0269)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.0712)
features.4.conv.0 tensor(0.0628)
features.4.conv.3 tensor(0.1007)
features.4.conv.6 tensor(0.1242)
features.5.conv.0 tensor(0.0417)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1069)
features.6.conv.0 tensor(0.0260)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0832)
features.7.conv.0 tensor(0.0703)
features.7.conv.3 tensor(0.0975)
features.7.conv.6 tensor(0.1378)
features.8.conv.0 tensor(0.1070)
features.8.conv.3 tensor(0.1354)
features.8.conv.6 tensor(0.1309)
features.9.conv.0 tensor(0.1289)
features.9.conv.3 tensor(0.1337)
features.9.conv.6 tensor(0.3993)
features.10.conv.0 tensor(0.0545)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.0988)
features.11.conv.0 tensor(0.2826)
features.11.conv.3 tensor(0.1086)
features.11.conv.6 tensor(0.4174)
features.12.conv.0 tensor(0.4329)
features.12.conv.3 tensor(0.1566)
features.12.conv.6 tensor(0.6123)
features.13.conv.0 tensor(0.1375)
features.13.conv.3 tensor(0.1468)
features.13.conv.6 tensor(0.1389)
features.14.conv.0 tensor(0.9341)
features.14.conv.3 tensor(0.0782)
features.14.conv.6 tensor(0.9465)
features.15.conv.0 tensor(0.9526)
features.15.conv.3 tensor(0.0759)
features.15.conv.6 tensor(0.9477)
features.16.conv.0 tensor(0.1047)
features.16.conv.3 tensor(0.0942)
features.16.conv.6 tensor(0.2501)
conv.0 tensor(0.1183)
tensor(879032.) 2188896.0
0.78425193
0.78433126
0.78440279
0.78466684
0.78464246
0.78470778
0.78489226
0.78513199
0.78545088
0.78559929
0.78565001
0.78577691
0.78564560
0.78569049
0.78563929
INFO - Training [8][   20/  196]   Loss 0.532862   Top1 81.738281   Top5 97.500000   BatchTime 0.424111   LR 0.000434
0.78610379
0.78649765
0.78775078
0.79016727
0.79152119
0.79103172
0.79073870
0.79045558
0.79021186
0.78988302
0.78967154
0.78953260
0.78939474
0.78914416
0.78904903
0.78870088
0.78830379
0.78820980
0.78823829
0.78824359
0.78816056
0.78807122
0.78795588
INFO - Training [8][   40/  196]   Loss 0.520371   Top1 82.402344   Top5 97.744141   BatchTime 0.392335   LR 0.000389
0.78774017
0.78761506
0.78744471
0.78726536
0.78688139
0.78662425
0.78641826
0.78646517
0.78632975
0.78608030
0.78578097
0.78547400
0.78535974
0.78532499
0.78525162
0.78530747
0.78525543
0.78515857
0.78495395
0.78488082
0.78461957
0.78436130
INFO - Training [8][   60/  196]   Loss 0.524941   Top1 82.480469   Top5 97.897135   BatchTime 0.382391   LR 0.000347
0.78416747
0.78396314
0.78367478
0.78354383
0.78347939
0.78330618
0.78307801
0.78306597
0.78299475
0.78282166
0.78264004
0.78264213
0.78242260
0.78229052
0.78226900
0.78224128
INFO - Training [8][   80/  196]   Loss 0.515686   Top1 82.910156   Top5 98.002930   BatchTime 0.378652   LR 0.000308
0.78228480
0.78231329
0.78232545
0.78247976
0.78226358
0.78209567
0.78195661
0.78188431
0.78182083
0.78178459
0.78177017
0.78182805
0.78168613
0.78169852
0.78183722
0.78166097
0.78162342
0.78141463
0.78120571
0.78103071
0.78095490
0.78078038
INFO - Training [8][  100/  196]   Loss 0.507588   Top1 83.195312   Top5 98.085938   BatchTime 0.376725   LR 0.000270
0.78073686
0.78068703
0.78058964
0.78046381
0.78042752
0.78036886
0.78035176
0.78045326
0.78062385
0.78042185
0.78032172
0.78037429
0.78021407
0.77998370
0.77985883
0.77981532
0.77971482
0.77956134
0.77956897
0.77970862
0.77962101
0.77963930
INFO - Training [8][  120/  196]   Loss 0.500211   Top1 83.447266   Top5 98.193359   BatchTime 0.374450   LR 0.000235
0.77942497
0.77944499
0.77952206
0.77945304
0.77940685
0.77936673
0.77928656
0.77922630
0.77922058
0.77924925
0.77921683
0.77918762
0.77909821
0.77909023
0.77904981
0.77893716
0.77848172
0.77825356
INFO - Training [8][  140/  196]   Loss 0.499609   Top1 83.484933   Top5 98.261719   BatchTime 0.368404   LR 0.000202
0.77805293
0.77789652
0.77781767
0.77770925
0.77775437
0.77778512
0.77784705
0.77787405
0.77796179
0.77797347
0.77787167
0.77800936
0.77782887
0.77759802
0.77739412
0.77710712
0.77687812
0.77668786
INFO - Training [8][  160/  196]   Loss 0.501559   Top1 83.437500   Top5 98.269043   BatchTime 0.364018   LR 0.000172
0.77634484
0.77638727
0.77661043
0.77668720
0.77644086
0.77611583
0.77610123
0.77610511
0.77607095
0.77595848
0.77599895
0.77606022
0.77622104
0.77637774
0.77645224
0.77669150
0.77673411
0.77700454
0.77718443
0.77729875
0.77741545
0.77755272
0.77752584
0.77741259
0.77686989
INFO - Training [8][  180/  196]   Loss 0.500603   Top1 83.437500   Top5 98.220486   BatchTime 0.359386   LR 0.000143
0.77755207
0.77756977
0.77750522
0.77740383
0.77731180
0.77722526
0.77715743
0.77702838
0.77691507
0.77675337
0.77658677
0.77638108
0.77619463
0.77604383
0.77587622
********************pre-trained*****************
INFO - ==> Top1: 83.444    Top5: 98.206    Loss: 0.500
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.365173   Top1 87.441406   Top5 99.492188   BatchTime 0.120134
INFO - Validation [8][   40/   40]   Loss 0.351449   Top1 87.750000   Top5 99.640000   BatchTime 0.088346
INFO - ==> Top1: 87.750    Top5: 99.640    Loss: 0.351
INFO - ==> Sparsity : 0.423
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.750   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 85.280   Top5: 99.300]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 83.420   Top5: 98.780]
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.2734)
features.1.conv.0 tensor(0.0267)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0734)
features.2.conv.0 tensor(0.0411)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0914)
features.3.conv.0 tensor(0.0260)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0707)
features.4.conv.0 tensor(0.0617)
features.4.conv.3 tensor(0.1007)
features.4.conv.6 tensor(0.1206)
features.5.conv.0 tensor(0.0417)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.1128)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0824)
features.7.conv.0 tensor(0.0675)
features.7.conv.3 tensor(0.0984)
features.7.conv.6 tensor(0.1449)
features.8.conv.0 tensor(0.1082)
features.8.conv.3 tensor(0.1337)
features.8.conv.6 tensor(0.1759)
features.9.conv.0 tensor(0.1335)
features.9.conv.3 tensor(0.1351)
features.9.conv.6 tensor(0.4038)
features.10.conv.0 tensor(0.0550)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.1241)
features.11.conv.0 tensor(0.3494)
features.11.conv.3 tensor(0.1069)
features.11.conv.6 tensor(0.4655)
features.12.conv.0 tensor(0.4405)
features.12.conv.3 tensor(0.1574)
features.12.conv.6 tensor(0.6963)
features.13.conv.0 tensor(0.1425)
features.13.conv.3 tensor(0.1454)
features.13.conv.6 tensor(0.1529)
features.14.conv.0 tensor(0.9344)
features.14.conv.3 tensor(0.0784)
features.14.conv.6 tensor(0.9479)
features.15.conv.0 tensor(0.9526)
features.15.conv.3 tensor(0.0753)
features.15.conv.6 tensor(0.9515)
features.16.conv.0 tensor(0.1052)
features.16.conv.3 tensor(0.0948)
features.16.conv.6 tensor(0.3477)
conv.0 tensor(0.1205)
tensor(926189.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.77589226
0.77588856
0.77587050
0.77578211
0.77573830
0.77569455
0.77561659
0.77557462
0.77550292
0.77545738
0.77539039
0.77527112
0.77525973
0.77525115
0.77523410
0.77517980
INFO - Training [9][   20/  196]   Loss 0.512809   Top1 82.539062   Top5 97.656250   BatchTime 0.436046   LR 0.000100
0.77509642
0.77492833
0.77477008
0.77465296
0.77454013
0.77445132
0.77440315
0.77431262
0.77424443
0.77416801
0.77411765
0.77397799
0.77383286
0.77375740
0.77377242
0.77375203
0.77373147
0.77370650
0.77368009
0.77367097
0.77364880
0.77361387
INFO - Training [9][   40/  196]   Loss 0.513250   Top1 82.734375   Top5 97.939453   BatchTime 0.401675   LR 0.000079
0.77359271
0.77350384
0.77343214
0.77339578
0.77333641
0.77325094
0.77314562
0.77304357
0.77294689
0.77283347
0.77270454
0.77258325
0.77240229
0.77226079
0.77216810
0.77208561
0.77198261
0.77188832
0.77178466
0.77170813
0.77164936
0.77160692
INFO - Training [9][   60/  196]   Loss 0.502422   Top1 83.190104   Top5 98.020833   BatchTime 0.391736   LR 0.000060
0.77159065
0.77158588
0.77159369
0.77155453
0.77153224
0.77155608
0.77156126
0.77156919
0.77156049
0.77153689
0.77152479
0.77151233
0.77152294
0.77152717
0.77153385
0.77153546
INFO - Training [9][   80/  196]   Loss 0.503467   Top1 83.159180   Top5 98.120117   BatchTime 0.384183   LR 0.000044
0.77157199
0.77160215
0.77161252
0.77164882
0.77164274
0.77165163
0.77163619
0.77157074
0.77152532
0.77151161
0.77152973
0.77153951
0.77151334
0.77151078
0.77151519
0.77154034
0.77155852
0.77155167
0.77158350
0.77159786
0.77157605
0.77154154
INFO - Training [9][  100/  196]   Loss 0.497380   Top1 83.406250   Top5 98.210938   BatchTime 0.384729   LR 0.000030
0.77152592
0.77148426
0.77144295
0.77140743
0.77136600
0.77132440
0.77129084
0.77123541
0.77118266
0.77111715
0.77105027
0.77098566
0.77094078
0.77086627
0.77083039
0.77077806
0.77074611
0.77071166
0.77068597
0.77063102
INFO - Training [9][  120/  196]   Loss 0.491755   Top1 83.522135   Top5 98.284505   BatchTime 0.384689   LR 0.000019
0.77061898
0.77058083
0.77054763
0.77050674
0.77047020
0.77043712
0.77040339
0.77040631
0.77038163
0.77035105
0.77031189
0.77026588
0.77023399
0.77020603
0.77015966
0.77011657
0.77010691
0.77007800
0.77006316
INFO - Training [9][  140/  196]   Loss 0.489468   Top1 83.571429   Top5 98.353795   BatchTime 0.373583   LR 0.000010
0.77004176
0.77002823
0.77000695
0.76998436
0.76997203
0.76995778
0.76994121
0.76992631
0.76992244
0.76992762
0.76991701
0.76992899
0.76993954
0.76996344
0.76995426
0.76995242
0.76992840
0.76990765
0.76989722
INFO - Training [9][  160/  196]   Loss 0.491489   Top1 83.486328   Top5 98.364258   BatchTime 0.368563   LR 0.000004
0.76990795
0.76990235
0.76989228
0.76988018
0.76986718
0.76987076
0.76987010
0.76985824
0.76985711
0.76985073
0.76984745
0.76986927
0.76986045
0.76986736
0.76986843
0.76985604
0.76984298
0.76985168
0.76984441
0.76984227
0.76982933
0.76982808
INFO - Training [9][  180/  196]   Loss 0.491754   Top1 83.450521   Top5 98.313802   BatchTime 0.367851   LR 0.000001
0.76982051
0.76983422
0.76983815
0.76982754
0.76983690
0.76983678
0.76984346
0.76982814
0.76983833
0.76984304
0.76983601
0.76983601
0.76982534
0.76981771
0.76983213
0.76982337
0.76984763
INFO - ==> Top1: 83.400    Top5: 98.298    Loss: 0.492
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.76986086
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.584609   Top1 80.488281   Top5 98.632812   BatchTime 0.111606
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.2754)
features.1.conv.0 tensor(0.0280)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0411)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0920)
features.3.conv.0 tensor(0.0260)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0707)
features.4.conv.0 tensor(0.0627)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.1221)
features.5.conv.0 tensor(0.0412)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.1157)
features.6.conv.0 tensor(0.0244)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0828)
features.7.conv.0 tensor(0.0677)
features.7.conv.3 tensor(0.0992)
features.7.conv.6 tensor(0.1523)
features.8.conv.0 tensor(0.1088)
features.8.conv.3 tensor(0.1345)
features.8.conv.6 tensor(0.1890)
features.9.conv.0 tensor(0.1363)
features.9.conv.3 tensor(0.1354)
features.9.conv.6 tensor(0.4043)
features.10.conv.0 tensor(0.0554)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.1235)
features.11.conv.0 tensor(0.3585)
features.11.conv.3 tensor(0.1061)
features.11.conv.6 tensor(0.4671)
features.12.conv.0 tensor(0.4517)
features.12.conv.3 tensor(0.1570)
features.12.conv.6 tensor(0.6994)
features.13.conv.0 tensor(0.1440)
features.13.conv.3 tensor(0.1453)
features.13.conv.6 tensor(0.1575)
features.14.conv.0 tensor(0.9348)
features.14.conv.3 tensor(0.0786)
features.14.conv.6 tensor(0.9491)
features.15.conv.0 tensor(0.9529)
features.15.conv.3 tensor(0.0750)
features.15.conv.6 tensor(0.9521)
features.16.conv.0 tensor(0.1056)
features.16.conv.3 tensor(0.0944)
features.16.conv.6 tensor(0.4059)
conv.0 tensor(0.1207)
tensor(947099.) 2188896.0
INFO - Validation [9][   40/   40]   Loss 0.573662   Top1 80.570000   Top5 98.760000   BatchTime 0.081740
INFO - ==> Top1: 80.570    Top5: 98.760    Loss: 0.574
INFO - ==> Sparsity : 0.433
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.750   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 85.280   Top5: 99.300]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 83.420   Top5: 98.780]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.76985914
0.76245737
0.76837099
0.77064544
0.77193815
0.77180320
0.77170026
0.77158576
0.77339590
0.77292305
0.77270460
0.77201259
0.77148074
0.77139753
0.77164245
0.77102810
0.77063406
0.77024162
INFO - Training [10][   20/  196]   Loss 0.566244   Top1 80.878906   Top5 97.636719   BatchTime 0.437108   LR 0.002500
0.76891881
0.76765019
0.76716620
0.76754177
0.76629269
0.77388215
0.78001392
0.78408462
0.78670090
0.78866160
0.79048783
0.79243457
0.79265475
0.79290289
0.79350561
0.79399401
0.79415697
0.79416555
0.79437160
0.79480338
0.79470998
0.79520100
INFO - Training [10][   40/  196]   Loss 0.574008   Top1 80.585938   Top5 97.734375   BatchTime 0.398774   LR 0.002499
0.79577273
0.79614717
0.79683512
0.79762465
0.79835147
0.79900205
0.79962802
0.80007321
0.80028629
0.80049652
0.80073237
0.80106491
0.80085868
0.80110776
0.80121964
0.80114371
0.80117565
INFO - Training [10][   60/  196]   Loss 0.577840   Top1 80.540365   Top5 97.773438   BatchTime 0.388045   LR 0.002499
0.80143166
0.80159485
0.80178708
0.80167693
0.80180538
0.80201405
0.80210733
0.80225343
0.80210638
0.80223441
0.80233473
0.80256397
0.80274439
0.80300456
0.80333036
0.80637687
0.80655390
0.80655313
0.80864370
0.80787611
INFO - Training [10][   80/  196]   Loss 0.583412   Top1 80.566406   Top5 97.875977   BatchTime 0.391263   LR 0.002497
0.80786163
0.80764145
0.80759054
0.80748874
0.80732399
0.80698800
0.80691582
0.80648649
0.80635870
0.80622244
0.80593860
0.80553854
0.80533481
0.80501676
0.80470359
0.80462420
0.80415970
0.80380434
0.80326337
0.80271161
0.80205995
INFO - Training [10][  100/  196]   Loss 0.578342   Top1 80.617188   Top5 97.871094   BatchTime 0.387381   LR 0.002496
0.80172330
0.80130780
0.80066299
0.79982668
0.79908544
0.79859316
0.79766285
0.79772502
0.79724932
0.79704994
0.79667348
0.79651642
0.79626042
0.79657972
0.79745257
0.79774410
0.79803604
0.79804319
0.79817605
0.79771513
0.79790223
0.79844320
INFO - Training [10][  120/  196]   Loss 0.574263   Top1 80.735677   Top5 97.952474   BatchTime 0.385296   LR 0.002494
0.79895145
0.79949951
0.79995704
0.80121613
0.80173343
0.80181044
0.80168045
0.80192751
0.80180562
0.80293131
0.80345339
0.80368954
0.80678862
0.81175524
0.81142044
0.81089908
0.81105506
0.81133133
0.81130999
INFO - Training [10][  140/  196]   Loss 0.574495   Top1 80.756138   Top5 97.996652   BatchTime 0.373539   LR 0.002492
0.81130171
0.81161809
0.81363958
0.81462771
0.81527847
0.81543231
0.81535065
0.81533009
0.81520277
0.81488001
0.81481224
0.81482452
0.81491214
0.81507921
0.81500095
0.81505048
0.81536627
0.81564993
0.81570995
0.81583154
0.81611413
INFO - Training [10][  160/  196]   Loss 0.577088   Top1 80.676270   Top5 98.005371   BatchTime 0.363063   LR 0.002490
0.81586510
0.81590199
0.81578076
0.81582320
0.81586701
0.81582022
0.81571066
0.81582767
0.81580073
0.81555223
0.81547701
0.81520373
0.81505579
0.81480014
0.81472164
0.81476569
0.81462795
INFO - Training [10][  180/  196]   Loss 0.578813   Top1 80.605469   Top5 97.931858   BatchTime 0.362928   LR 0.002487
0.81472778
0.81477994
0.81462407
0.81438208
0.81427753
0.81420964
0.81411451
0.81413448
0.81415039
0.81424689
0.81436217
0.81438595
0.81432176
0.81447607
0.81445771
0.81459820
0.81457835
INFO - ==> Top1: 80.670    Top5: 97.934    Loss: 0.578
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.81461936
0.81487161
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.468065   Top1 83.769531   Top5 99.257812   BatchTime 0.114096
INFO - Validation [10][   40/   40]   Loss 0.458659   Top1 84.070000   Top5 99.320000   BatchTime 0.082261
INFO - ==> Top1: 84.070    Top5: 99.320    Loss: 0.459
INFO - ==> Sparsity : 0.387
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.750   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 85.280   Top5: 99.300]
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 84.070   Top5: 99.320]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0267)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0677)
features.2.conv.0 tensor(0.0312)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0946)
features.3.conv.0 tensor(0.0284)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0660)
features.4.conv.0 tensor(0.0508)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0981)
features.5.conv.0 tensor(0.0407)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.1071)
features.6.conv.0 tensor(0.0296)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0767)
features.7.conv.0 tensor(0.0704)
features.7.conv.3 tensor(0.1019)
features.7.conv.6 tensor(0.1300)
features.8.conv.0 tensor(0.0986)
features.8.conv.3 tensor(0.1398)
features.8.conv.6 tensor(0.1150)
features.9.conv.0 tensor(0.1191)
features.9.conv.3 tensor(0.1351)
features.9.conv.6 tensor(0.3961)
features.10.conv.0 tensor(0.0659)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.0895)
features.11.conv.0 tensor(0.1350)
features.11.conv.3 tensor(0.1171)
features.11.conv.6 tensor(0.2414)
features.12.conv.0 tensor(0.2915)
features.12.conv.3 tensor(0.1607)
features.12.conv.6 tensor(0.5467)
features.13.conv.0 tensor(0.1168)
features.13.conv.3 tensor(0.1466)
features.13.conv.6 tensor(0.1324)
features.14.conv.0 tensor(0.9362)
features.14.conv.3 tensor(0.0818)
features.14.conv.6 tensor(0.9273)
features.15.conv.0 tensor(0.9585)
features.15.conv.3 tensor(0.0705)
features.15.conv.6 tensor(0.9386)
features.16.conv.0 tensor(0.0906)
features.16.conv.3 tensor(0.0946)
features.16.conv.6 tensor(0.2491)
conv.0 tensor(0.1340)
tensor(847317.) 2188896.0
0.81485814
0.81492358
0.81524378
0.81547040
0.81564319
0.81561190
0.81540108
0.81531250
0.81538093
0.81525004
0.81526834
0.81537396
0.81532294
0.81525546
0.81512928
0.81527293
0.81501353
INFO - Training [11][   20/  196]   Loss 0.590138   Top1 80.585938   Top5 97.539062   BatchTime 0.426606   LR 0.002481
0.81473845
0.81459934
0.81444997
0.81419694
0.81386906
0.81386238
0.81385571
0.81338626
0.81317735
0.81326634
0.81327796
0.81333309
0.81345850
0.81324923
0.81328464
0.81326532
0.81323200
0.81329858
0.81321967
0.81330222
0.81342894
INFO - Training [11][   40/  196]   Loss 0.586316   Top1 80.849609   Top5 97.597656   BatchTime 0.413308   LR 0.002478
0.81320453
0.81291801
0.81253117
0.81205553
0.81139767
0.81083566
0.81025749
0.80940425
0.80870903
0.80863434
0.80836922
0.80793053
0.80724502
0.80649531
0.80566967
0.80511165
0.80474550
0.80358738
0.80197763
0.80431318
0.80283737
INFO - Training [11][   60/  196]   Loss 0.586681   Top1 80.820312   Top5 97.662760   BatchTime 0.396266   LR 0.002474
0.80522358
0.80687678
0.80771255
0.80885124
0.81154734
0.81157762
0.81144655
0.81143349
0.81122094
0.81069094
0.81089449
0.81104153
0.81100184
0.81113356
0.81113011
0.81109494
0.81071800
INFO - Training [11][   80/  196]   Loss 0.583808   Top1 80.966797   Top5 97.690430   BatchTime 0.386270   LR 0.002470
0.81024700
0.80946869
0.80882341
0.80890918
0.80892080
0.80924100
0.81216395
0.81173646
0.81192845
0.81196207
0.81234223
0.81233954
0.81201965
0.81208831
0.81221133
0.81219971
0.81238031
0.81262600
0.81377965
0.81360245
0.81352609
0.81344402
0.81342113
INFO - Training [11][  100/  196]   Loss 0.585883   Top1 80.886719   Top5 97.734375   BatchTime 0.381178   LR 0.002465
0.81349581
0.81311786
0.81306446
0.81285983
0.81316406
0.81248140
0.81236243
0.81250000
0.81237727
0.81226492
0.81230164
0.81216067
0.81207556
0.81220996
0.81205177
0.81177121
0.81169647
0.81185555
0.81288356
0.81283879
0.81274056
INFO - Training [11][  120/  196]   Loss 0.581865   Top1 80.953776   Top5 97.848307   BatchTime 0.378916   LR 0.002460
0.81287360
0.81295085
0.81301528
0.81282288
0.81276423
0.81263316
0.81232554
0.81212908
0.81224477
0.81227148
0.81211334
0.81152868
0.81120795
0.81107497
0.81082475
0.81061924
INFO - Training [11][  140/  196]   Loss 0.581455   Top1 80.959821   Top5 97.901786   BatchTime 0.378024   LR 0.002455
0.81018722
0.80983722
0.80943924
0.80898732
0.80868870
0.80828714
0.80799335
0.80778593
0.80755591
0.80734289
0.80721074
0.80681676
0.80684519
0.80672973
0.80660474
0.80644476
0.80662173
0.80684519
0.80664355
0.80655599
INFO - Training [11][  160/  196]   Loss 0.583997   Top1 80.874023   Top5 97.900391   BatchTime 0.368242   LR 0.002450
0.80630767
0.80611193
0.80578327
0.80580497
0.80553013
0.80537522
0.80547678
0.80576569
0.80742007
0.80733389
0.80713952
0.80711514
0.80688584
0.80615854
0.80563307
0.80559176
0.80576563
0.80551946
0.80512971
0.80487204
0.80439222
0.80399984
0.80371052
0.80343628
INFO - Training [11][  180/  196]   Loss 0.583160   Top1 80.818142   Top5 97.881944   BatchTime 0.365207   LR 0.002444
0.80306584
0.80232167
0.80193371
0.80169398
0.80094570
0.80016500
0.79979455
0.80125618
0.80012578
0.79864019
0.79716337
0.79513222
0.79611564
0.79629081
0.79636282
0.79627317
INFO - ==> Top1: 80.818    Top5: 97.882    Loss: 0.583
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.402665   Top1 86.289062   Top5 99.433594   BatchTime 0.126459
INFO - Validation [11][   40/   40]   Loss 0.393981   Top1 86.360000   Top5 99.530000   BatchTime 0.089951
INFO - ==> Top1: 86.360    Top5: 99.530    Loss: 0.394
INFO - ==> Sparsity : 0.401
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.750   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 86.360   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 85.280   Top5: 99.300]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.2305)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0263)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0935)
features.3.conv.0 tensor(0.0281)
features.3.conv.3 tensor(0.0556)
features.3.conv.6 tensor(0.0642)
features.4.conv.0 tensor(0.0501)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.0986)
features.5.conv.0 tensor(0.0392)
features.5.conv.3 tensor(0.0648)
features.5.conv.6 tensor(0.1115)
features.6.conv.0 tensor(0.0254)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0802)
features.7.conv.0 tensor(0.0698)
features.7.conv.3 tensor(0.1013)
features.7.conv.6 tensor(0.1302)
features.8.conv.0 tensor(0.0858)
features.8.conv.3 tensor(0.1389)
features.8.conv.6 tensor(0.1160)
features.9.conv.0 tensor(0.1117)
features.9.conv.3 tensor(0.1345)
features.9.conv.6 tensor(0.4172)
features.10.conv.0 tensor(0.0516)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0931)
features.11.conv.0 tensor(0.1479)
features.11.conv.3 tensor(0.1229)
features.11.conv.6 tensor(0.6263)
features.12.conv.0 tensor(0.3660)
features.12.conv.3 tensor(0.1632)
features.12.conv.6 tensor(0.5608)
features.13.conv.0 tensor(0.1194)
features.13.conv.3 tensor(0.1507)
features.13.conv.6 tensor(0.0795)
features.14.conv.0 tensor(0.9418)
features.14.conv.3 tensor(0.0803)
features.14.conv.6 tensor(0.9353)
features.15.conv.0 tensor(0.9606)
features.15.conv.3 tensor(0.0740)
features.15.conv.6 tensor(0.9440)
features.16.conv.0 tensor(0.0993)
features.16.conv.3 tensor(0.0972)
features.16.conv.6 tensor(0.2659)
conv.0 tensor(0.1286)
tensor(876922.) 2188896.0
0.79674131
0.79760879
0.79797632
0.79888761
0.79957670
0.79990715
0.80019528
0.80063242
0.80119526
0.80165613
0.80182987
0.80202574
0.80249250
0.80252385
0.80251056
0.80256444
0.80240762
INFO - Training [12][   20/  196]   Loss 0.590185   Top1 80.488281   Top5 97.441406   BatchTime 0.472922   LR 0.002433
0.80211538
0.80160815
0.80099910
0.80015677
0.79903257
0.79812425
0.79726118
0.79572296
0.79315412
0.79194891
0.79129297
0.79047275
0.79006702
0.79023904
0.79031080
0.79015321
0.78989094
0.78984600
0.79027522
0.79071277
0.79124385
0.79168469
INFO - Training [12][   40/  196]   Loss 0.597502   Top1 80.019531   Top5 97.744141   BatchTime 0.422424   LR 0.002426
0.79214346
0.79237074
0.79227245
0.79209775
0.79222554
0.79237515
0.79232562
0.79265684
0.79277235
0.79306000
0.79374129
0.79679394
0.79951185
0.80536258
0.80541182
0.80536759
0.80522591
0.80508459
0.80523115
0.80481201
0.80471790
0.80457550
INFO - Training [12][   60/  196]   Loss 0.582959   Top1 80.494792   Top5 97.799479   BatchTime 0.402827   LR 0.002419
0.80357349
0.80308610
0.80244040
0.80168504
0.80198133
0.80228001
0.80169135
0.80132103
0.80023789
0.79989296
0.80006754
0.80043042
0.80103654
0.80110568
0.80006325
0.79954678
INFO - Training [12][   80/  196]   Loss 0.577155   Top1 80.800781   Top5 97.939453   BatchTime 0.395211   LR 0.002412
0.79868048
0.79767865
0.79884952
0.79916096
0.79946607
0.79920471
0.79863387
0.79900700
0.80078727
0.80069196
0.80074275
0.80088389
0.80075961
0.80078602
0.80090255
0.80099243
0.80101907
0.80086696
0.80071610
0.80046743
0.80030286
0.80027312
INFO - Training [12][  100/  196]   Loss 0.573713   Top1 80.953125   Top5 98.039062   BatchTime 0.388488   LR 0.002404
0.80007529
0.79999852
0.79987955
0.79973286
0.79968047
0.79974031
0.79949123
0.79939508
0.79932499
0.79912239
0.79911399
0.79910791
0.79898214
0.79874450
0.79826212
0.79783237
0.79736656
0.79629493
0.79565096
0.79574293
0.79583102
0.79568499
INFO - Training [12][  120/  196]   Loss 0.566102   Top1 81.263021   Top5 98.108724   BatchTime 0.384414   LR 0.002396
0.79588443
0.79624832
0.79652148
0.79643166
0.79639882
0.79626632
0.79600388
0.79589719
0.79587221
0.79559904
0.79541069
0.79577124
0.79543978
0.79516286
0.79462892
0.79391444
0.79341084
INFO - Training [12][  140/  196]   Loss 0.563644   Top1 81.356027   Top5 98.113839   BatchTime 0.381807   LR 0.002388
0.79347467
0.79389000
0.79447663
0.79516333
0.79808259
0.79765582
0.79745120
0.79751778
0.79735273
0.79743868
0.79727721
0.79729307
0.79712933
0.79722279
0.79720849
0.79743701
0.79776537
0.79805458
0.79827988
INFO - Training [12][  160/  196]   Loss 0.569083   Top1 81.164551   Top5 98.081055   BatchTime 0.371434   LR 0.002380
0.79877162
0.79900146
0.79909474
0.79928082
0.79932708
0.79909736
0.79897165
0.79891092
0.79889071
0.79867947
0.79874665
0.79856765
0.79846293
0.79828465
0.79806727
0.79824680
0.79756534
0.79727268
0.79708278
0.79689306
0.79648823
0.79650038
INFO - Training [12][  180/  196]   Loss 0.570431   Top1 81.076389   Top5 98.001302   BatchTime 0.360412   LR 0.002371
0.79683548
0.79723716
0.79753751
0.79793280
0.79826933
0.79841888
0.79854596
0.79838336
0.79783159
0.79700238
0.79664850
0.79659611
0.79685414
0.79734743
0.79742265
0.79749829
0.79783607
INFO - ==> Top1: 81.138    Top5: 98.002    Loss: 0.569
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [12][   20/   40]   Loss 0.418608   Top1 85.605469   Top5 99.277344   BatchTime 0.141982
INFO - Validation [12][   40/   40]   Loss 0.414783   Top1 85.740000   Top5 99.410000   BatchTime 0.096701
INFO - ==> Top1: 85.740    Top5: 99.410    Loss: 0.415
INFO - ==> Sparsity : 0.405
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.750   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 86.360   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 85.740   Top5: 99.410]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.2734)
features.1.conv.0 tensor(0.0319)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0673)
features.2.conv.0 tensor(0.0333)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0929)
features.3.conv.0 tensor(0.0292)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0690)
features.4.conv.0 tensor(0.0439)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.0994)
features.5.conv.0 tensor(0.0381)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.1100)
features.6.conv.0 tensor(0.0335)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0758)
features.7.conv.0 tensor(0.0614)
features.7.conv.3 tensor(0.1050)
features.7.conv.6 tensor(0.1192)
features.8.conv.0 tensor(0.0848)
features.8.conv.3 tensor(0.1363)
features.8.conv.6 tensor(0.1185)
features.9.conv.0 tensor(0.1112)
features.9.conv.3 tensor(0.1325)
features.9.conv.6 tensor(0.4325)
features.10.conv.0 tensor(0.0532)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.1255)
features.11.conv.0 tensor(0.1813)
features.11.conv.3 tensor(0.1291)
features.11.conv.6 tensor(0.4227)
features.12.conv.0 tensor(0.3866)
features.12.conv.3 tensor(0.1642)
features.12.conv.6 tensor(0.5793)
features.13.conv.0 tensor(0.1118)
features.13.conv.3 tensor(0.1454)
features.13.conv.6 tensor(0.1037)
features.14.conv.0 tensor(0.9513)
features.14.conv.3 tensor(0.0803)
features.14.conv.6 tensor(0.9371)
features.15.conv.0 tensor(0.9489)
features.15.conv.3 tensor(0.0745)
features.15.conv.6 tensor(0.9533)
features.16.conv.0 tensor(0.1054)
features.16.conv.3 tensor(0.0929)
features.16.conv.6 tensor(0.3086)
conv.0 tensor(0.1241)
tensor(886251.) 2188896.0
0.79789203
0.79807001
0.79906094
0.79851967
0.79697883
0.79497772
0.79360348
0.78740257
0.77766484
0.77501947
0.77655083
0.79242414
0.80254614
0.80237091
0.80196911
0.80144548
0.80104774
INFO - Training [13][   20/  196]   Loss 0.563390   Top1 81.386719   Top5 97.753906   BatchTime 0.422045   LR 0.002355
0.80063206
0.80018133
0.79978144
0.79969114
0.79949057
0.79926836
0.79908329
0.79911768
0.79929072
0.79929072
0.79954708
0.79961300
0.79937518
0.79933876
0.79892129
0.79866779
0.79825199
0.79794997
0.79761493
0.79731095
0.79691279
0.79660267
0.79649627
INFO - Training [13][   40/  196]   Loss 0.569902   Top1 81.035156   Top5 97.714844   BatchTime 0.394631   LR 0.002345
0.79618335
0.79623896
0.79612339
0.79571879
0.79549098
0.79505396
0.79478312
0.79458094
0.79422110
0.79398423
0.79337311
0.79196179
0.79073465
0.79072493
0.79071629
0.79305345
nan
nan
nan
nan
nan
INFO - Training [13][   60/  196]   Loss nan   Top1 77.415365   Top5 95.546875   BatchTime 0.386748   LR 0.002336
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [13][   80/  196]   Loss nan   Top1 60.561523   Top5 83.750000   BatchTime 0.383083   LR 0.002325
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [13][  100/  196]   Loss nan   Top1 50.546875   Top5 77.042969   BatchTime 0.380658   LR 0.002315
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [13][  120/  196]   Loss nan   Top1 43.795573   Top5 72.288411   BatchTime 0.377453   LR 0.002304
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [13][  140/  196]   Loss nan   Top1 38.811384   Top5 69.012277   BatchTime 0.374936   LR 0.002293
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [13][  160/  196]   Loss nan   Top1 35.202637   Top5 66.594238   BatchTime 0.376525   LR 0.002282
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [13][  180/  196]   Loss nan   Top1 32.500000   Top5 64.850260   BatchTime 0.367702   LR 0.002271
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - ==> Top1: 30.754    Top5: 63.756    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [13][   20/   40]   Loss 92.984592   Top1 10.253906   Top5 49.960938   BatchTime 0.116021
INFO - Validation [13][   40/   40]   Loss 93.272967   Top1 10.000000   Top5 50.000000   BatchTime 0.083736
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0364)
features.16.conv.3 tensor(0.0299)
features.16.conv.6 tensor(0.2157)
conv.0 tensor(0.1549)
tensor(135529.) 2188896.0
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 93.273
INFO - ==> Sparsity : 0.062
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.750   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 86.360   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 85.740   Top5: 99.410]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][   20/  196]   Loss nan   Top1 10.253906   Top5 50.449219   BatchTime 0.428937   LR 0.002250
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][   40/  196]   Loss nan   Top1 9.843750   Top5 50.185547   BatchTime 0.398282   LR 0.002238
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][   60/  196]   Loss nan   Top1 9.837240   Top5 49.804688   BatchTime 0.383807   LR 0.002225
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][   80/  196]   Loss nan   Top1 9.819336   Top5 49.682617   BatchTime 0.380368   LR 0.002213
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][  100/  196]   Loss nan   Top1 9.941406   Top5 49.617188   BatchTime 0.380298   LR 0.002200
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][  120/  196]   Loss nan   Top1 9.980469   Top5 49.690755   BatchTime 0.382110   LR 0.002186
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][  140/  196]   Loss nan   Top1 9.893973   Top5 49.681920   BatchTime 0.379857   LR 0.002173
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][  160/  196]   Loss nan   Top1 9.865723   Top5 49.570312   BatchTime 0.376120   LR 0.002159
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [14][  180/  196]   Loss nan   Top1 9.824219   Top5 49.437934   BatchTime 0.364690   LR 0.002145
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - ==> Top1: 9.856    Top5: 49.496    Loss: nan
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
nan
nan
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 73.248895   Top1 10.253906   Top5 50.117188   BatchTime 0.116657
INFO - Validation [14][   40/   40]   Loss 73.502876   Top1 10.000000   Top5 50.000000   BatchTime 0.084152
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 73.503
INFO - ==> Sparsity : 0.069
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.750   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 86.360   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 85.740   Top5: 99.410]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-085712/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.)
features.0.conv.3 tensor(0.)
features.1.conv.0 tensor(0.)
features.1.conv.3 tensor(0.)
features.1.conv.6 tensor(0.)
features.2.conv.0 tensor(0.)
features.2.conv.3 tensor(0.)
features.2.conv.6 tensor(0.)
features.3.conv.0 tensor(0.)
features.3.conv.3 tensor(0.)
features.3.conv.6 tensor(0.)
features.4.conv.0 tensor(0.)
features.4.conv.3 tensor(0.)
features.4.conv.6 tensor(0.)
features.5.conv.0 tensor(0.)
features.5.conv.3 tensor(0.)
features.5.conv.6 tensor(0.)
features.6.conv.0 tensor(0.)
features.6.conv.3 tensor(0.)
features.6.conv.6 tensor(0.)
features.7.conv.0 tensor(0.)
features.7.conv.3 tensor(0.)
features.7.conv.6 tensor(0.)
features.8.conv.0 tensor(0.)
features.8.conv.3 tensor(0.)
features.8.conv.6 tensor(0.)
features.9.conv.0 tensor(0.)
features.9.conv.3 tensor(0.)
features.9.conv.6 tensor(0.)
features.10.conv.0 tensor(0.)
features.10.conv.3 tensor(0.)
features.10.conv.6 tensor(0.)
features.11.conv.0 tensor(0.)
features.11.conv.3 tensor(0.)
features.11.conv.6 tensor(0.)
features.12.conv.0 tensor(0.)
features.12.conv.3 tensor(0.)
features.12.conv.6 tensor(0.)
features.13.conv.0 tensor(0.)
features.13.conv.3 tensor(0.)
features.13.conv.6 tensor(0.)
features.14.conv.0 tensor(0.)
features.14.conv.3 tensor(0.)
features.14.conv.6 tensor(0.)
features.15.conv.0 tensor(0.)
features.15.conv.3 tensor(0.)
features.15.conv.6 tensor(0.)
features.16.conv.0 tensor(0.0516)
features.16.conv.3 tensor(0.0278)
features.16.conv.6 tensor(0.2871)
conv.0 tensor(0.1347)
tensor(151541.) 2188896.0
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][   20/  196]   Loss nan   Top1 9.746094   Top5 49.707031   BatchTime 0.422517   LR 0.002120
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][   40/  196]   Loss nan   Top1 9.755859   Top5 49.960938   BatchTime 0.394765   LR 0.002106
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][   60/  196]   Loss nan   Top1 9.772135   Top5 49.720052   BatchTime 0.385232   LR 0.002091
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][   80/  196]   Loss nan   Top1 9.741211   Top5 49.697266   BatchTime 0.385353   LR 0.002076
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][  100/  196]   Loss nan   Top1 9.816406   Top5 49.679688   BatchTime 0.383279   LR 0.002061
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][  120/  196]   Loss nan   Top1 9.807943   Top5 49.723307   BatchTime 0.379759   LR 0.002045
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][  140/  196]   Loss nan   Top1 9.854911   Top5 49.693080   BatchTime 0.377910   LR 0.002030
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
INFO - Training [15][  160/  196]   Loss nan   Top1 9.890137   Top5 49.741211   BatchTime 0.376399   LR 0.002014
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7efa908a0c10>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1466, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1430, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 77, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader,qat_model, teacher_model,criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 154, in train_one_epoch_slsq
    outputs = qat_model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 140, in forward
    x = self.features(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 93, in forward
    return self.skip_add.add(x, self.conv(x))
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1208, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 224, in forward
    return self._forward(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 101, in _forward
    return self._forward_approximate(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 114, in _forward_approximate
    scaled_weight = self.weight_fake_quant(self.weight * scale_factor.reshape(weight_shape))
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/quan/observer.py", line 549, in forward
KeyboardInterrupt