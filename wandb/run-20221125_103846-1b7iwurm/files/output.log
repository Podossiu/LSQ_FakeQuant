Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
*************soft_pruning_mode*******************
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.95438832
0.95023167
0.92816657
0.91748524
0.91271400
0.90288162
0.90580291
0.90504885
0.90447956
INFO - Training [0][   20/  196]   Loss 1.577210   Top1 53.574219   Top5 89.453125   BatchTime 0.479493   LR 0.004999
0.90222001
0.89886230
0.89874381
0.90146768
0.90739059
0.91356581
0.91524166
0.91620809
0.91726750
0.91771656
0.91802901
0.91799301
0.91806799
0.91709101
0.91791642
0.91824061
0.91806716
INFO - Training [0][   40/  196]   Loss 1.482339   Top1 52.714844   Top5 89.892578   BatchTime 0.403854   LR 0.004995
0.91796046
0.91749233
0.91571933
0.91511023
0.91519558
0.91598856
0.91754144
0.91728640
0.91715902
0.91724843
0.91699022
0.91679275
0.91674107
0.91688246
0.91677219
0.91666871
0.91647005
0.91645747
0.91638935
0.91651535
0.91647857
0.91645372
INFO - Training [0][   60/  196]   Loss 1.381680   Top1 54.739583   Top5 90.989583   BatchTime 0.400370   LR 0.004989
0.91646731
0.91650581
0.91626823
0.91619420
0.91632980
0.91645598
0.91631877
0.91642481
0.91656929
0.91662169
0.91651875
0.91629469
0.91626507
0.91615140
0.91623735
0.91631842
0.91605622
0.91605288
0.91590488
0.91573852
INFO - Training [0][   80/  196]   Loss 1.310006   Top1 56.733398   Top5 91.875000   BatchTime 0.400276   LR 0.004980
0.91558051
0.91548967
0.91550118
0.91562134
0.91606241
0.91634214
0.91662967
0.91699588
0.91752952
0.91889381
0.92020017
0.92155933
0.92402214
0.92693847
0.93115646
0.93554693
0.94054371
0.94383866
0.94431430
0.94411862
INFO - Training [0][  100/  196]   Loss 1.249231   Top1 58.441406   Top5 92.589844   BatchTime 0.401104   LR 0.004968
0.94414675
0.94419253
0.94402212
0.94401103
0.94399923
0.94414598
0.94426358
0.94419909
0.94391453
0.94381559
0.94399196
0.94395626
0.94401002
0.94416672
0.94392359
0.94404846
0.94407654
0.94414902
0.94423765
INFO - Training [0][  120/  196]   Loss 1.201458   Top1 60.022786   Top5 93.050130   BatchTime 0.402742   LR 0.004954
0.94403893
0.94389421
0.94395775
0.94411492
0.94406104
0.94398463
0.94396937
0.94411594
0.94396639
0.94404268
0.94257241
0.94394058
0.94407970
0.94401509
0.94413060
0.94407898
0.94421631
0.94421905
0.94397557
0.94384831
INFO - Training [0][  140/  196]   Loss 1.167408   Top1 61.057478   Top5 93.434710   BatchTime 0.402552   LR 0.004938
0.94376463
0.94282734
0.94309741
0.94296074
0.94345361
0.94373691
0.94406241
0.94399035
0.94390064
0.94395256
0.94403189
0.94377744
0.94377518
0.94386315
0.94390690
0.94378775
0.94350773
0.94353533
0.94369709
0.94373280
0.94389963
0.94391835
0.94378597
0.94374239
INFO - Training [0][  160/  196]   Loss 1.144688   Top1 61.733398   Top5 93.618164   BatchTime 0.404096   LR 0.004919
0.94367433
0.94384331
0.94380885
0.94375765
0.94315964
0.94229263
0.94118220
0.94191182
0.94273657
0.94328344
0.94396579
0.94403672
0.94393551
0.94385093
0.94389409
INFO - Training [0][  180/  196]   Loss 1.121922   Top1 62.378472   Top5 93.808594   BatchTime 0.403060   LR 0.004897
0.94409764
0.94415909
0.94413805
0.94402605
0.94348335
0.94274861
0.94191688
0.94050884
0.93879020
0.93845695
0.94012529
0.94147730
0.94334078
0.94414812
0.94238639
0.94379807
0.93448693
0.94365376
0.94396222
********************pre-trained*****************
INFO - ==> Top1: 62.978    Top5: 93.954    Loss: 1.104
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [0][   20/   40]   Loss 0.830979   Top1 73.652344   Top5 97.382812   BatchTime 0.114022
INFO - Validation [0][   40/   40]   Loss 0.836001   Top1 73.270000   Top5 97.400000   BatchTime 0.086958
INFO - ==> Top1: 73.270    Top5: 97.400    Loss: 0.836
INFO - ==> Sparsity : 0.118
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 73.270   Top5: 97.400]
features.0.conv.0 tensor(0.5521)
features.0.conv.3 tensor(0.3535)
features.1.conv.0 tensor(0.0410)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0681)
features.2.conv.0 tensor(0.0405)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0865)
features.3.conv.0 tensor(0.0382)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0755)
features.4.conv.0 tensor(0.0542)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.0968)
features.5.conv.0 tensor(0.0661)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.1063)
features.6.conv.0 tensor(0.0492)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0806)
features.7.conv.0 tensor(0.0690)
features.7.conv.3 tensor(0.0958)
features.7.conv.6 tensor(0.1250)
features.8.conv.0 tensor(0.0994)
features.8.conv.3 tensor(0.1108)
features.8.conv.6 tensor(0.1287)
features.9.conv.0 tensor(0.1165)
features.9.conv.3 tensor(0.1212)
features.9.conv.6 tensor(0.1307)
features.10.conv.0 tensor(0.0611)
features.10.conv.3 tensor(0.0877)
features.10.conv.6 tensor(0.1024)
features.11.conv.0 tensor(0.1190)
features.11.conv.3 tensor(0.0963)
features.11.conv.6 tensor(0.1727)
features.12.conv.0 tensor(0.0991)
features.12.conv.3 tensor(0.0916)
features.12.conv.6 tensor(0.1686)
features.13.conv.0 tensor(0.0754)
features.13.conv.3 tensor(0.1281)
features.13.conv.6 tensor(0.1047)
features.14.conv.0 tensor(0.0689)
features.14.conv.3 tensor(0.0763)
features.14.conv.6 tensor(0.3046)
features.15.conv.0 tensor(0.0549)
features.15.conv.3 tensor(0.0735)
features.15.conv.6 tensor(0.2644)
features.16.conv.0 tensor(0.0454)
features.16.conv.3 tensor(0.0762)
features.16.conv.6 tensor(0.1122)
conv.0 tensor(0.0819)
tensor(258211.) 2188896.0
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.94406193
0.94409025
0.94422847
0.94410616
0.94412929
0.94433302
0.94418675
0.94400078
0.94406497
0.94407803
0.94430238
0.94391990
0.94378334
0.94375473
0.94371122
0.94354898
0.94362706
0.94362122
0.94384563
0.94393629
0.94375306
INFO - Training [1][   20/  196]   Loss 0.930904   Top1 68.359375   Top5 95.605469   BatchTime 0.458939   LR 0.004853
0.94379669
0.94370419
0.94361901
0.94368464
0.94372225
0.94371819
0.94369417
0.94364792
0.94201797
0.94090182
0.93663108
0.92855370
0.92214125
0.92684788
0.92829233
0.92806256
INFO - Training [1][   40/  196]   Loss 0.923264   Top1 68.359375   Top5 95.732422   BatchTime 0.413581   LR 0.004825
0.92727089
0.92603582
0.92620569
0.92659968
0.92650783
0.92699569
0.92773300
0.93164879
0.93525022
0.93753421
0.93913233
0.94025469
0.94254327
0.94412881
0.94403213
0.94412053
0.94423854
0.94413048
0.94419020
0.94431561
0.94443268
0.94444591
0.94438308
INFO - Training [1][   60/  196]   Loss 0.913641   Top1 68.548177   Top5 95.794271   BatchTime 0.393899   LR 0.004794
0.94425827
0.94431865
0.94432944
0.94424146
0.94437993
0.94459689
0.94464022
0.94455868
0.94441271
0.94432741
0.94436491
0.94438589
0.94442838
0.94402081
0.94159561
0.93595749
0.92655867
0.92490792
0.92079848
0.92058569
0.91861135
0.91834033
INFO - Training [1][   80/  196]   Loss 0.898872   Top1 69.155273   Top5 96.044922   BatchTime 0.387329   LR 0.004761
0.92011285
0.92356694
0.92760479
0.93470341
0.93949497
0.94347930
0.94366086
0.94364637
0.94373065
0.94380760
0.94382209
0.94381601
0.94385773
0.94398385
0.94396645
0.94391924
0.94414431
INFO - Training [1][  100/  196]   Loss 0.884910   Top1 69.695312   Top5 96.113281   BatchTime 0.380284   LR 0.004725
0.94404209
0.94402081
0.94415194
0.94399667
0.94393450
0.94355369
0.94546407
0.94867593
0.95330495
0.95606118
0.95623958
0.95659727
0.95660549
0.95660949
0.95637918
0.95641214
0.95669311
0.95655376
0.95631003
0.95660478
0.95638567
0.95634753
INFO - Training [1][  120/  196]   Loss 0.875385   Top1 70.107422   Top5 96.207682   BatchTime 0.377045   LR 0.004687
0.95628101
0.95635617
0.95620573
0.95635235
0.95603448
0.95608455
0.95615333
0.95640248
0.95631790
0.95629781
0.95648754
0.95645535
0.95631737
0.95635951
0.95461220
0.95645100
INFO - Training [1][  140/  196]   Loss 0.864513   Top1 70.571987   Top5 96.350446   BatchTime 0.375431   LR 0.004647
0.95633519
0.95647466
0.95645750
0.95632911
0.95603609
0.95631164
0.95613927
0.95622009
0.95635289
0.95644593
0.95652884
0.95655149
0.95668668
0.95644444
0.95637929
0.95627570
0.95659029
0.95679349
0.95711493
0.95705181
0.95717454
0.95715815
INFO - Training [1][  160/  196]   Loss 0.861409   Top1 70.617676   Top5 96.354980   BatchTime 0.374263   LR 0.004605
0.95703954
0.95702207
0.95724398
0.95718551
0.95696145
0.95687699
0.95707870
0.95696932
0.95697695
0.95708901
0.95728034
0.95715117
0.95695114
0.95667964
0.95673305
0.95666772
0.95670062
0.95654434
0.95653510
0.95640612
0.95638984
0.95627671
0.95638311
INFO - Training [1][  180/  196]   Loss 0.852758   Top1 70.898438   Top5 96.356337   BatchTime 0.371566   LR 0.004560
0.95649821
0.95668548
0.95636326
0.95619494
0.95608294
0.95589381
0.95592636
0.95601439
0.95603710
0.95594049
0.95602101
0.95610046
INFO - ==> Top1: 71.036    Top5: 96.364    Loss: 0.849
0.95257032
0.95626324
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [1][   20/   40]   Loss 0.614453   Top1 79.355469   Top5 98.496094   BatchTime 0.138294
INFO - Validation [1][   40/   40]   Loss 0.620314   Top1 78.990000   Top5 98.510000   BatchTime 0.100725
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.1484)
features.1.conv.0 tensor(0.0391)
features.1.conv.3 tensor(0.0984)
features.1.conv.6 tensor(0.0734)
features.2.conv.0 tensor(0.0443)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0891)
features.3.conv.0 tensor(0.0330)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0632)
features.4.conv.0 tensor(0.0480)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0998)
features.5.conv.0 tensor(0.0640)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.1099)
features.6.conv.0 tensor(0.0472)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0802)
features.7.conv.0 tensor(0.0739)
features.7.conv.3 tensor(0.1082)
features.7.conv.6 tensor(0.1206)
features.8.conv.0 tensor(0.0867)
features.8.conv.3 tensor(0.1244)
features.8.conv.6 tensor(0.1313)
features.9.conv.0 tensor(0.0885)
features.9.conv.3 tensor(0.1325)
features.9.conv.6 tensor(0.1210)
features.10.conv.0 tensor(0.0535)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0924)
features.11.conv.0 tensor(0.0899)
features.11.conv.3 tensor(0.1015)
features.11.conv.6 tensor(0.1902)
features.12.conv.0 tensor(0.1285)
features.12.conv.3 tensor(0.1092)
features.12.conv.6 tensor(0.1803)
features.13.conv.0 tensor(0.0664)
features.13.conv.3 tensor(0.1449)
features.13.conv.6 tensor(0.0931)
features.14.conv.0 tensor(0.0826)
features.14.conv.3 tensor(0.0819)
features.14.conv.6 tensor(0.3752)
features.15.conv.0 tensor(0.0552)
features.15.conv.3 tensor(0.0758)
features.15.conv.6 tensor(0.2956)
features.16.conv.0 tensor(0.0716)
features.16.conv.3 tensor(0.0873)
features.16.conv.6 tensor(0.0744)
conv.0 tensor(0.1043)
tensor(276350.) 2188896.0
INFO - ==> Top1: 78.990    Top5: 98.510    Loss: 0.620
INFO - ==> Sparsity : 0.126
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 78.990   Top5: 98.510]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 73.270   Top5: 97.400]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.95617139
0.95634437
0.95641643
0.95646733
0.95647556
0.95659626
0.95647532
0.95620030
0.95620805
0.95631254
0.95633686
0.95618957
0.95643198
0.95650047
0.95684636
0.95676106
0.95664126
0.95637017
INFO - Training [2][   20/  196]   Loss 0.837033   Top1 71.972656   Top5 95.664062   BatchTime 0.442204   LR 0.004477
0.95494616
0.95149219
0.93509823
0.92858142
0.93421674
0.95559043
0.95662463
0.95639026
0.95644784
0.95629656
0.95631474
0.95615399
0.95622659
0.95604324
0.95624119
0.95641124
0.95615441
0.95626032
0.95662564
0.95669019
0.95651966
0.95647287
INFO - Training [2][   40/  196]   Loss 0.819110   Top1 72.773438   Top5 95.947266   BatchTime 0.404531   LR 0.004426
0.95664036
0.95655543
0.95647496
0.95626003
0.95639282
0.95622915
0.95602262
0.95605123
0.95597816
0.95605022
0.95602554
0.95606679
0.95626122
0.95668983
0.95632732
0.95650458
0.95631766
0.95621264
0.95621258
0.95626134
0.95660615
0.95643055
INFO - Training [2][   60/  196]   Loss 0.805805   Top1 72.962240   Top5 96.217448   BatchTime 0.388331   LR 0.004374
0.95642179
0.95637804
0.95615703
0.94411808
0.95633608
0.95640653
0.95632601
0.95608586
0.95608675
0.95599312
0.95589626
0.95596582
0.95625645
0.95665056
0.95641494
0.95637906
0.95606995
INFO - Training [2][   80/  196]   Loss 0.793803   Top1 73.364258   Top5 96.459961   BatchTime 0.382892   LR 0.004320
0.95613974
0.95633686
0.95614290
0.95589715
0.95573133
0.95573002
0.95587820
0.95595348
0.95613796
0.95629132
0.95634609
0.95644468
0.95636344
0.95654553
0.95671237
0.95676285
0.95655912
0.95632601
0.95417404
0.95628965
0.95671898
0.95657593
INFO - Training [2][  100/  196]   Loss 0.778424   Top1 73.726562   Top5 96.550781   BatchTime 0.379046   LR 0.004264
0.95681810
0.95672542
0.95665967
0.95651448
0.95657372
0.95682693
0.95679522
0.95686442
0.95660537
0.95619315
0.95533270
0.95559996
0.95548183
0.95514071
0.95643193
0.95662957
0.95644277
0.95635426
0.95652670
0.95670813
0.95681077
0.95703173
INFO - Training [2][  120/  196]   Loss 0.771053   Top1 73.945312   Top5 96.676432   BatchTime 0.377387   LR 0.004206
0.95702410
0.95686543
0.95669109
0.95664215
0.95679832
0.95711541
0.95703113
0.95712698
0.95709217
0.95663768
0.95656204
0.95653939
0.95648056
0.95649594
0.95640618
0.95666742
INFO - Training [2][  140/  196]   Loss 0.769257   Top1 73.998326   Top5 96.721540   BatchTime 0.375588   LR 0.004146
0.95700783
0.95692408
0.95666134
0.95689416
0.95684659
0.95678174
0.95665985
0.95702654
0.95697111
0.95678598
0.95685333
0.95676196
0.95672655
0.95710641
0.95717049
0.95692050
0.95695454
0.95655125
0.95654726
0.95667213
0.95672137
0.95683438
INFO - Training [2][  160/  196]   Loss 0.769434   Top1 74.038086   Top5 96.716309   BatchTime 0.373971   LR 0.004085
0.95665741
0.95669872
0.95686519
0.95702630
0.95691717
0.95690495
0.95651895
0.95655948
0.95681000
0.95678025
0.95702475
0.95701063
0.95700681
0.95683974
0.95713198
0.95747823
0.95728201
INFO - Training [2][  180/  196]   Loss 0.765029   Top1 74.160156   Top5 96.673177   BatchTime 0.372913   LR 0.004022
0.95718920
0.95703351
0.95712036
0.95690000
0.95718384
0.95690775
0.95707297
0.95719212
0.95720345
0.95708847
0.95716542
0.95669824
0.95699716
0.95689505
0.95696294
0.95703214
0.95700139
INFO - ==> Top1: 74.288    Top5: 96.680    Loss: 0.761
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.95696795
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [2][   20/   40]   Loss 0.631960   Top1 79.687500   Top5 98.652344   BatchTime 0.148175
INFO - Validation [2][   40/   40]   Loss 0.623956   Top1 79.370000   Top5 98.660000   BatchTime 0.103097
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.1543)
features.1.conv.0 tensor(0.0352)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0399)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0755)
features.3.conv.0 tensor(0.0286)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0614)
features.4.conv.0 tensor(0.0506)
features.4.conv.3 tensor(0.1036)
features.4.conv.6 tensor(0.1045)
features.5.conv.0 tensor(0.0495)
features.5.conv.3 tensor(0.0567)
features.5.conv.6 tensor(0.0998)
features.6.conv.0 tensor(0.0454)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0728)
features.7.conv.0 tensor(0.0704)
features.7.conv.3 tensor(0.1204)
features.7.conv.6 tensor(0.1227)
features.8.conv.0 tensor(0.0910)
features.8.conv.3 tensor(0.1374)
features.8.conv.6 tensor(0.1319)
features.9.conv.0 tensor(0.0799)
features.9.conv.3 tensor(0.1476)
features.9.conv.6 tensor(0.1212)
features.10.conv.0 tensor(0.0621)
features.10.conv.3 tensor(0.1088)
features.10.conv.6 tensor(0.0904)
features.11.conv.0 tensor(0.1085)
features.11.conv.3 tensor(0.0980)
features.11.conv.6 tensor(0.1903)
features.12.conv.0 tensor(0.1479)
features.12.conv.3 tensor(0.1117)
features.12.conv.6 tensor(0.1993)
features.13.conv.0 tensor(0.0718)
features.13.conv.3 tensor(0.1564)
features.13.conv.6 tensor(0.0918)
features.14.conv.0 tensor(0.0912)
features.14.conv.3 tensor(0.0830)
features.14.conv.6 tensor(0.3734)
features.15.conv.0 tensor(0.0711)
features.15.conv.3 tensor(0.0752)
features.15.conv.6 tensor(0.3256)
features.16.conv.0 tensor(0.0844)
features.16.conv.3 tensor(0.0909)
features.16.conv.6 tensor(0.1051)
conv.0 tensor(0.1412)
tensor(314276.) 2188896.0
INFO - ==> Top1: 79.370    Top5: 98.660    Loss: 0.624
INFO - ==> Sparsity : 0.144
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 79.370   Top5: 98.660]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 78.990   Top5: 98.510]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 73.270   Top5: 97.400]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.95688987
0.95708132
0.95721412
0.95710438
0.95715332
0.95724261
0.95751148
0.95704395
0.95498520
0.95209479
0.95742214
0.95691276
0.95706290
0.95728642
0.95735461
0.95721143
0.95709217
0.95718855
0.95733756
0.95732701
0.95747173
0.95731723
INFO - Training [3][   20/  196]   Loss 0.721567   Top1 75.136719   Top5 96.796875   BatchTime 0.442659   LR 0.003907
0.95741016
0.95724308
0.95706421
0.95711583
0.95727313
0.95710421
0.95697582
0.95702320
0.95711720
0.95726258
0.95744640
0.95762229
0.95740736
0.95752603
0.95749003
0.95707297
0.95723319
INFO - Training [3][   40/  196]   Loss 0.725366   Top1 75.068359   Top5 96.894531   BatchTime 0.398245   LR 0.003840
0.95727628
0.95715088
0.95735908
0.95750833
0.95762062
0.95764387
0.95768851
0.95717788
0.95729041
0.95745075
0.95732474
0.95714998
0.95679241
0.95653683
0.95636433
0.95614982
0.95553118
0.95529264
0.95696777
0.95723099
0.95727694
0.95721996
INFO - Training [3][   60/  196]   Loss 0.721218   Top1 75.136719   Top5 97.011719   BatchTime 0.390208   LR 0.003771
0.95731378
0.95731288
0.95710236
0.95700711
0.95703655
0.95705372
0.95690930
0.95667315
0.95642549
0.95636255
0.95642048
0.95651984
0.95639241
0.95646644
0.95650059
0.95661902
INFO - Training [3][   80/  196]   Loss 0.709942   Top1 75.717773   Top5 97.114258   BatchTime 0.382606   LR 0.003701
0.95646554
0.95648432
0.95652682
0.95670903
0.95616937
0.95560032
0.95564789
0.95538533
0.95440584
0.95351297
0.95352083
0.95332539
0.95293719
0.95351791
0.95385867
0.95441616
0.95580786
0.95643055
0.95787710
0.95783311
0.95785713
0.95760000
0.95765239
INFO - Training [3][  100/  196]   Loss 0.699132   Top1 76.144531   Top5 97.175781   BatchTime 0.377601   LR 0.003630
0.95740420
0.95722175
0.95723301
0.95704305
0.95708621
0.95706803
0.95720661
0.95687979
0.95685917
0.95436972
0.95292038
0.95589042
0.95715559
0.95718521
0.95741361
0.95725900
0.95710027
0.95711845
0.95744944
0.95749789
0.95739722
0.95733452
INFO - Training [3][  120/  196]   Loss 0.695024   Top1 76.344401   Top5 97.226562   BatchTime 0.373911   LR 0.003558
0.95718813
0.95700312
0.95713019
0.95718521
0.95701844
0.95699537
0.95662272
0.95667696
0.95658493
0.95378608
0.95678097
0.95684254
0.95668870
0.95674187
0.95680147
0.95684814
0.95668679
INFO - Training [3][  140/  196]   Loss 0.689027   Top1 76.515067   Top5 97.307478   BatchTime 0.372252   LR 0.003484
0.95655757
0.95638072
0.95632130
0.95635426
0.95644343
0.95662218
0.95703304
0.95708567
0.95695359
0.95708299
0.95732486
0.95736378
0.95705187
0.95734811
0.95741385
0.95727003
0.95732903
0.95735157
0.95751685
0.95764017
0.95771587
0.95771003
INFO - Training [3][  160/  196]   Loss 0.689855   Top1 76.425781   Top5 97.319336   BatchTime 0.371699   LR 0.003410
0.95764428
0.95758158
0.95744270
0.95738769
0.95731491
0.95723724
0.95732051
0.95706367
0.95693868
0.95720279
0.95756555
0.95694846
0.95665246
0.95638007
0.95609170
0.95591581
0.95554858
0.95524865
INFO - Training [3][  180/  196]   Loss 0.687436   Top1 76.438802   Top5 97.261285   BatchTime 0.366048   LR 0.003335
0.95478797
0.95431250
0.95390904
0.95317304
0.95255774
0.95135576
0.95000291
0.94764060
0.94543213
0.94125229
0.93803656
0.93651366
0.93407160
0.93269354
0.93120259
0.93263584
0.93215901
********************pre-trained*****************
INFO - ==> Top1: 76.570    Top5: 97.250    Loss: 0.684
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.468931   Top1 84.082031   Top5 99.101562   BatchTime 0.130585
INFO - Validation [3][   40/   40]   Loss 0.460253   Top1 84.290000   Top5 99.290000   BatchTime 0.094297
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0760)
features.2.conv.0 tensor(0.0376)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0888)
features.3.conv.0 tensor(0.0258)
features.3.conv.3 tensor(0.0617)
features.3.conv.6 tensor(0.0566)
features.4.conv.0 tensor(0.0428)
features.4.conv.3 tensor(0.1047)
features.4.conv.6 tensor(0.0996)
features.5.conv.0 tensor(0.0452)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.1051)
features.6.conv.0 tensor(0.0420)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0773)
features.7.conv.3 tensor(0.1241)
features.7.conv.6 tensor(0.1191)
features.8.conv.0 tensor(0.0762)
features.8.conv.3 tensor(0.1421)
features.8.conv.6 tensor(0.1299)
features.9.conv.0 tensor(0.0517)
features.9.conv.3 tensor(0.1562)
features.9.conv.6 tensor(0.1196)
features.10.conv.0 tensor(0.0486)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.0898)
features.11.conv.0 tensor(0.1187)
features.11.conv.3 tensor(0.1046)
features.11.conv.6 tensor(0.1883)
features.12.conv.0 tensor(0.1407)
features.12.conv.3 tensor(0.1177)
features.12.conv.6 tensor(0.1917)
features.13.conv.0 tensor(0.0760)
features.13.conv.3 tensor(0.1674)
features.13.conv.6 tensor(0.0855)
features.14.conv.0 tensor(0.1029)
features.14.conv.3 tensor(0.0816)
features.14.conv.6 tensor(0.3966)
features.15.conv.0 tensor(0.8992)
features.15.conv.3 tensor(0.0779)
features.15.conv.6 tensor(0.3395)
features.16.conv.0 tensor(0.0767)
features.16.conv.3 tensor(0.1001)
features.16.conv.6 tensor(0.1090)
conv.0 tensor(0.1606)
tensor(455009.) 2188896.0
INFO - ==> Top1: 84.290    Top5: 99.290    Loss: 0.460
INFO - ==> Sparsity : 0.208
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 84.290   Top5: 99.290]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 79.370   Top5: 98.660]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 78.990   Top5: 98.510]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.93027091
0.93040740
0.93035340
0.93052483
0.93157613
0.93297279
0.93596244
0.93846685
0.94048637
0.94044161
0.94051600
0.94105214
0.94111323
0.94139481
0.94127768
0.94100505
0.94069242
INFO - Training [4][   20/  196]   Loss 0.678999   Top1 76.699219   Top5 96.738281   BatchTime 0.440634   LR 0.003200
0.94059616
0.94037217
0.93992424
0.93974340
0.93961924
0.93901038
0.93802577
0.93816525
0.93767923
0.93660671
0.93384314
0.93209994
0.93122762
0.93073159
0.93049437
0.93021768
0.92996204
0.92982876
0.93026692
0.93050015
0.93100929
0.93151104
INFO - Training [4][   40/  196]   Loss 0.658223   Top1 77.695312   Top5 97.197266   BatchTime 0.403893   LR 0.003122
0.93152034
0.93154448
0.93199706
0.93261868
0.93290192
0.93290669
0.93279552
0.93247503
0.93259543
0.93272161
0.93295783
0.93314070
0.93347311
0.93368071
0.93387181
0.93380058
0.93358421
0.93340772
0.93302041
0.93295479
0.93284988
INFO - Training [4][   60/  196]   Loss 0.657304   Top1 77.591146   Top5 97.298177   BatchTime 0.393397   LR 0.003044
0.93300819
0.93291330
0.93304944
0.93285346
0.93273658
0.93256825
0.93240726
0.93239099
0.93187016
0.93161786
0.93141609
0.93131310
0.93129122
0.93145311
0.93156272
0.93161684
0.93152350
INFO - Training [4][   80/  196]   Loss 0.656273   Top1 77.495117   Top5 97.377930   BatchTime 0.384802   LR 0.002965
0.93157929
0.93138015
0.93132424
0.93102497
0.93109912
0.93087947
0.93084675
0.93088520
0.93103409
0.93127400
0.93157411
0.93143523
0.93125057
0.93130040
0.93133980
0.93125683
0.93127471
0.93142390
0.93178290
0.93170279
0.93170488
0.93180430
INFO - Training [4][  100/  196]   Loss 0.645450   Top1 77.964844   Top5 97.468750   BatchTime 0.381899   LR 0.002886
0.93200547
0.93226320
0.93246877
0.93246889
0.93259531
0.93262434
0.93264681
0.93275064
0.93272245
0.93269312
0.93248069
0.93230379
0.93248427
0.93247527
0.93251055
0.93257058
0.93256015
0.93264693
0.93290693
0.93284982
0.93244404
0.93214595
INFO - Training [4][  120/  196]   Loss 0.636178   Top1 78.294271   Top5 97.565104   BatchTime 0.379455   LR 0.002806
0.93197364
0.93184865
0.93174171
0.93171477
0.93177611
0.93173903
0.93168992
0.93159878
0.93154538
0.93160176
0.93170685
0.93159395
0.93163633
0.93162954
0.93164116
0.93164456
INFO - Training [4][  140/  196]   Loss 0.633765   Top1 78.417969   Top5 97.617188   BatchTime 0.376543   LR 0.002726
0.93174601
0.93168741
0.93203521
0.93201524
0.93206090
0.93223017
0.93216234
0.93212956
0.93201816
0.93182844
0.93192619
0.93161064
0.92938262
0.92045021
0.92476976
0.91541749
0.90429991
0.90341675
0.90336895
INFO - Training [4][  160/  196]   Loss 0.634230   Top1 78.430176   Top5 97.634277   BatchTime 0.369556   LR 0.002646
0.90301931
0.90326589
0.90353757
0.90346628
0.90317589
0.90336508
0.90328699
0.90350759
0.90330660
0.90303200
0.90289325
0.90280235
0.90304953
0.90346694
0.90402007
0.90454090
0.90455914
0.90492833
0.90549332
0.90554386
0.90552634
0.90537310
0.90492374
0.90469056
INFO - Training [4][  180/  196]   Loss 0.631057   Top1 78.532986   Top5 97.586806   BatchTime 0.364989   LR 0.002566
0.90460187
0.90454865
0.90469366
0.90463507
0.90483111
0.90511411
0.90537620
0.90528518
0.90508270
0.90491599
0.90481967
0.90481389
0.90480798
0.90475482
0.90477020
0.90503842
INFO - ==> Top1: 78.636    Top5: 97.586    Loss: 0.628
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 0.476248   Top1 84.355469   Top5 99.199219   BatchTime 0.132013
INFO - Validation [4][   40/   40]   Loss 0.469229   Top1 84.300000   Top5 99.250000   BatchTime 0.093329
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.1543)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0344)
features.2.conv.3 tensor(0.0679)
features.2.conv.6 tensor(0.0822)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0617)
features.3.conv.6 tensor(0.0525)
features.4.conv.0 tensor(0.0436)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.1048)
features.5.conv.0 tensor(0.0477)
features.5.conv.3 tensor(0.0579)
features.5.conv.6 tensor(0.1047)
features.6.conv.0 tensor(0.0339)
features.6.conv.3 tensor(0.0631)
features.6.conv.6 tensor(0.0736)
features.7.conv.0 tensor(0.0669)
features.7.conv.3 tensor(0.1221)
features.7.conv.6 tensor(0.1145)
features.8.conv.0 tensor(0.0741)
features.8.conv.3 tensor(0.1418)
features.8.conv.6 tensor(0.1216)
features.9.conv.0 tensor(0.0729)
features.9.conv.3 tensor(0.1513)
features.9.conv.6 tensor(0.1165)
features.10.conv.0 tensor(0.0531)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.0898)
features.11.conv.0 tensor(0.1288)
features.11.conv.3 tensor(0.1030)
features.11.conv.6 tensor(0.2030)
features.12.conv.0 tensor(0.1362)
features.12.conv.3 tensor(0.1198)
features.12.conv.6 tensor(0.1937)
features.13.conv.0 tensor(0.0475)
features.13.conv.3 tensor(0.1653)
features.13.conv.6 tensor(0.0822)
features.14.conv.0 tensor(0.8176)
features.14.conv.3 tensor(0.0890)
features.14.conv.6 tensor(0.3352)
features.15.conv.0 tensor(0.8355)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.3645)
features.16.conv.0 tensor(0.0763)
features.16.conv.3 tensor(0.0970)
features.16.conv.6 tensor(0.1087)
conv.0 tensor(0.1412)
tensor(540600.) 2188896.0
INFO - ==> Top1: 84.300    Top5: 99.250    Loss: 0.469
INFO - ==> Sparsity : 0.247
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 84.300   Top5: 99.250]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 84.290   Top5: 99.290]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 79.370   Top5: 98.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.90493566
0.90505928
0.90539575
0.90529519
0.90522319
0.90512973
0.90490431
0.90480417
0.90502065
0.90489680
0.90481883
0.90483880
0.90516734
0.90508717
0.90503061
0.90497589
0.90482873
0.90468270
0.90463442
INFO - Training [5][   20/  196]   Loss 0.609030   Top1 79.082031   Top5 97.402344   BatchTime 0.427857   LR 0.002424
0.90466177
0.90473992
0.90466994
0.90465796
0.90478945
0.90481114
0.90463406
0.90469372
0.90428197
0.90398824
0.90395141
0.90379310
0.90386480
0.90376663
0.90354294
0.90350205
0.90367752
0.90362775
0.90369529
0.90364945
0.90339309
0.90340370
INFO - Training [5][   40/  196]   Loss 0.626178   Top1 78.408203   Top5 97.460938   BatchTime 0.393448   LR 0.002343
0.90354311
0.90356505
0.90371895
0.90391588
0.90412271
0.90400350
0.90365255
0.90358692
0.90360808
0.90365690
0.90360707
0.90359569
0.90361488
0.90379876
0.90401345
0.90396518
0.90378976
INFO - Training [5][   60/  196]   Loss 0.614611   Top1 78.906250   Top5 97.428385   BatchTime 0.383825   LR 0.002263
0.90374708
0.90378749
0.90386337
0.90386063
0.90386516
0.90407956
0.90392238
0.90421396
0.90443218
0.90471590
0.90470237
0.90503335
0.90535200
0.90548176
0.90545887
0.90556270
0.90570837
0.90576804
0.90578336
0.90575480
0.90555745
0.90535772
INFO - Training [5][   80/  196]   Loss 0.599136   Top1 79.345703   Top5 97.578125   BatchTime 0.379119   LR 0.002183
0.90524954
0.90537846
0.90536004
0.90530235
0.90534699
0.90534192
0.90555781
0.90585738
0.90574968
0.90566558
0.90552330
0.90558493
0.90573603
0.90566689
0.90578896
0.90570414
0.90574914
INFO - Training [5][  100/  196]   Loss 0.588920   Top1 79.652344   Top5 97.703125   BatchTime 0.373474   LR 0.002104
0.90571165
0.90566897
0.90567809
0.90559864
0.90564370
0.90578133
0.90570885
0.90561903
0.90549660
0.90539068
0.90528744
0.90539330
0.90546751
0.90552694
0.90550613
0.90562111
0.90543807
0.90551460
0.90553337
0.90582734
0.90617961
0.90587795
INFO - Training [5][  120/  196]   Loss 0.582263   Top1 79.905599   Top5 97.760417   BatchTime 0.371305   LR 0.002024
0.90570951
0.90579224
0.90572447
0.90567100
0.90561330
0.90581930
0.90578437
0.90597534
0.90586174
0.90571249
0.90576369
0.90543461
0.90539569
0.90515667
0.90479928
0.90450531
0.90421081
0.90389687
0.90384614
0.90401620
INFO - Training [5][  140/  196]   Loss 0.579856   Top1 79.969308   Top5 97.815290   BatchTime 0.362962   LR 0.001946
0.90316021
0.90320301
0.90274477
0.90287954
0.90268463
0.90201735
0.90124720
0.90127563
0.89924419
0.89823800
0.89919239
0.89908534
0.89963812
0.89904541
0.89730883
0.89541972
0.89216769
INFO - Training [5][  160/  196]   Loss 0.582266   Top1 79.873047   Top5 97.800293   BatchTime 0.359957   LR 0.001868
0.89109248
0.88944077
0.89439815
0.89809251
0.90098035
0.90255952
0.90362388
0.90449846
0.90467083
0.90464067
0.90489060
0.90475380
0.90482205
0.90483230
0.90467906
0.90460622
0.90454561
0.90459621
0.90445381
0.90439582
0.90445936
0.90438986
INFO - Training [5][  180/  196]   Loss 0.581190   Top1 79.943576   Top5 97.773438   BatchTime 0.362096   LR 0.001790
0.90421128
0.90409261
0.90414667
0.90405434
0.90390396
0.90401536
0.90434468
0.90441155
0.90404660
0.90398318
0.90393066
0.90352082
0.90414691
0.90418017
0.90405500
0.90392381
INFO - ==> Top1: 80.056    Top5: 97.782    Loss: 0.578
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.90391630
0.90389329
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.431183   Top1 85.839844   Top5 99.277344   BatchTime 0.129994
INFO - Validation [5][   40/   40]   Loss 0.418805   Top1 85.960000   Top5 99.410000   BatchTime 0.095139
INFO - ==> Top1: 85.960    Top5: 99.410    Loss: 0.419
INFO - ==> Sparsity : 0.258
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 85.960   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 84.300   Top5: 99.250]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 84.290   Top5: 99.290]
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0332)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0781)
features.2.conv.0 tensor(0.0240)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0822)
features.3.conv.0 tensor(0.0234)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0549)
features.4.conv.0 tensor(0.0448)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.1055)
features.5.conv.0 tensor(0.0522)
features.5.conv.3 tensor(0.0619)
features.5.conv.6 tensor(0.1027)
features.6.conv.0 tensor(0.0345)
features.6.conv.3 tensor(0.0590)
features.6.conv.6 tensor(0.0707)
features.7.conv.0 tensor(0.0692)
features.7.conv.3 tensor(0.1181)
features.7.conv.6 tensor(0.1091)
features.8.conv.0 tensor(0.0790)
features.8.conv.3 tensor(0.1450)
features.8.conv.6 tensor(0.1207)
features.9.conv.0 tensor(0.0781)
features.9.conv.3 tensor(0.1528)
features.9.conv.6 tensor(0.1147)
features.10.conv.0 tensor(0.0419)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0898)
features.11.conv.0 tensor(0.1328)
features.11.conv.3 tensor(0.1042)
features.11.conv.6 tensor(0.2110)
features.12.conv.0 tensor(0.1273)
features.12.conv.3 tensor(0.1238)
features.12.conv.6 tensor(0.1868)
features.13.conv.0 tensor(0.0629)
features.13.conv.3 tensor(0.1618)
features.13.conv.6 tensor(0.0844)
features.14.conv.0 tensor(0.8215)
features.14.conv.3 tensor(0.0946)
features.14.conv.6 tensor(0.3865)
features.15.conv.0 tensor(0.8557)
features.15.conv.3 tensor(0.0881)
features.15.conv.6 tensor(0.3896)
features.16.conv.0 tensor(0.0824)
features.16.conv.3 tensor(0.1000)
features.16.conv.6 tensor(0.1310)
conv.0 tensor(0.1397)
tensor(563955.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.90380150
0.90392119
0.90415961
0.90410715
0.90404141
0.90407014
0.90386003
0.90376276
0.90368581
0.90364277
0.90345067
0.90339237
0.90343821
0.90341038
0.90343714
0.90351510
0.90342343
0.90344685
0.90285164
INFO - Training [6][   20/  196]   Loss 0.575530   Top1 79.941406   Top5 97.734375   BatchTime 0.446368   LR 0.001655
0.90372688
0.90367866
0.90371537
0.90355796
0.90357709
0.90361089
0.90369821
0.90362823
0.90352046
0.90337247
0.90321881
0.90312564
0.90315950
0.90322083
0.90303952
0.90295881
0.90304005
0.90306652
0.90317941
0.90353954
0.90339619
0.90332389
INFO - Training [6][   40/  196]   Loss 0.565765   Top1 80.283203   Top5 97.705078   BatchTime 0.407334   LR 0.001580
0.90336472
0.90349376
0.90346456
0.90326512
0.90309370
0.90307134
0.90302682
0.90308201
0.90304625
0.90307236
0.90311396
0.90313190
0.90333432
0.90330058
0.90308815
0.90303296
INFO - Training [6][   60/  196]   Loss 0.549803   Top1 81.041667   Top5 97.819010   BatchTime 0.393933   LR 0.001506
0.90301502
0.90295339
0.90306181
0.90324110
0.90327138
0.90338445
0.90364182
0.90399259
0.90421647
0.90417147
0.90433729
0.90398747
0.90374011
0.90382594
0.90380859
0.90360045
0.90345573
0.90348065
0.90363091
0.90372491
0.90377855
0.90357274
INFO - Training [6][   80/  196]   Loss 0.541856   Top1 81.313477   Top5 97.949219   BatchTime 0.384068   LR 0.001432
0.90360767
0.90384561
0.90360868
0.90329528
0.90318096
0.90299982
0.90273911
0.90271693
0.90234911
0.90222633
0.90209192
0.90194011
0.90196025
0.90188116
0.90180546
0.90168905
0.90161055
INFO - Training [6][  100/  196]   Loss 0.533934   Top1 81.507812   Top5 97.996094   BatchTime 0.378692   LR 0.001360
0.90124810
0.90122020
0.90080535
0.90052521
0.90033329
0.90001881
0.89960313
0.89940989
0.89940965
0.89900464
0.89852524
0.89804417
0.89744675
0.89701599
0.89689183
0.89611334
0.89606625
0.89608705
0.89573002
0.89536005
0.89494997
0.89441097
0.89361674
0.89326262
0.89286017
INFO - Training [6][  120/  196]   Loss 0.529074   Top1 81.653646   Top5 98.095703   BatchTime 0.368911   LR 0.001289
0.89218193
0.89155054
0.89086545
0.89022261
0.88941860
0.88847613
0.88774383
0.88697320
0.88627201
0.88527501
0.88460594
0.88438755
0.88397783
0.88350260
0.88290995
0.88219929
0.88133866
INFO - Training [6][  140/  196]   Loss 0.528702   Top1 81.690848   Top5 98.113839   BatchTime 0.367241   LR 0.001220
0.88060498
0.88025534
0.87968403
0.87944990
0.87912405
0.87916845
0.87937218
0.87929583
0.87933606
0.87942094
0.87945431
0.87948567
0.87920827
0.87884951
0.87853187
0.87819266
0.87774920
0.87742323
0.87712824
0.87695032
0.87664968
INFO - Training [6][  160/  196]   Loss 0.530189   Top1 81.628418   Top5 98.093262   BatchTime 0.368014   LR 0.001151
0.87644708
0.87617928
0.87581807
0.87555772
0.87535065
0.87504137
0.87479025
0.87467200
0.87481868
0.87483472
0.87469369
0.87484819
0.87496561
0.87481284
0.87463075
0.87471235
0.87452090
INFO - Training [6][  180/  196]   Loss 0.527854   Top1 81.684028   Top5 98.029514   BatchTime 0.367846   LR 0.001084
0.87450749
0.87422782
0.87423348
0.87428439
0.87401605
0.87376881
0.87359530
0.87356758
0.87353981
0.87357944
0.87351793
0.87344605
0.87313795
0.87259591
0.87210613
0.87167811
INFO - ==> Top1: 81.682    Top5: 98.048    Loss: 0.527
0.87138915
0.87142408
0.87137157
0.87105113
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.427126   Top1 85.761719   Top5 99.335938   BatchTime 0.135945
INFO - Validation [6][   40/   40]   Loss 0.409635   Top1 86.090000   Top5 99.520000   BatchTime 0.095558
INFO - ==> Top1: 86.090    Top5: 99.520    Loss: 0.410
INFO - ==> Sparsity : 0.299
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 86.090   Top5: 99.520]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 85.960   Top5: 99.410]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 84.300   Top5: 99.250]
features.0.conv.0 tensor(0.5417)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0299)
features.1.conv.3 tensor(0.0961)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0263)
features.2.conv.3 tensor(0.0694)
features.2.conv.6 tensor(0.0787)
features.3.conv.0 tensor(0.0220)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0521)
features.4.conv.0 tensor(0.0452)
features.4.conv.3 tensor(0.1024)
features.4.conv.6 tensor(0.1095)
features.5.conv.0 tensor(0.0461)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.0999)
features.6.conv.0 tensor(0.0312)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0704)
features.7.conv.0 tensor(0.0597)
features.7.conv.3 tensor(0.1181)
features.7.conv.6 tensor(0.1093)
features.8.conv.0 tensor(0.0692)
features.8.conv.3 tensor(0.1438)
features.8.conv.6 tensor(0.1163)
features.9.conv.0 tensor(0.0769)
features.9.conv.3 tensor(0.1551)
features.9.conv.6 tensor(0.1146)
features.10.conv.0 tensor(0.0441)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.0891)
features.11.conv.0 tensor(0.1371)
features.11.conv.3 tensor(0.0999)
features.11.conv.6 tensor(0.2016)
features.12.conv.0 tensor(0.1401)
features.12.conv.3 tensor(0.1260)
features.12.conv.6 tensor(0.1831)
features.13.conv.0 tensor(0.0645)
features.13.conv.3 tensor(0.1634)
features.13.conv.6 tensor(0.0857)
features.14.conv.0 tensor(0.8278)
features.14.conv.3 tensor(0.0961)
features.14.conv.6 tensor(0.4828)
features.15.conv.0 tensor(0.8675)
features.15.conv.3 tensor(0.0868)
features.15.conv.6 tensor(0.9128)
features.16.conv.0 tensor(0.0899)
features.16.conv.3 tensor(0.0987)
features.16.conv.6 tensor(0.1170)
conv.0 tensor(0.1320)
tensor(655366.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.87082249
0.87071091
0.87077397
0.87086350
0.87105417
0.87074190
0.87017834
0.86921781
0.86820388
0.86580139
0.86392349
0.86327499
0.86288309
0.86270308
0.86303008
0.86523783
0.86703598
0.86813945
0.86911839
0.86970228
INFO - Training [7][   20/  196]   Loss 0.520515   Top1 81.992188   Top5 97.441406   BatchTime 0.458327   LR 0.000969
0.86999035
0.87041491
0.87103420
0.87231463
0.87243378
0.87266856
0.87283236
0.87292862
0.87302876
0.87286818
0.87291706
0.87282401
0.87268996
0.87308890
0.87339234
0.87355965
0.87353462
INFO - Training [7][   40/  196]   Loss 0.515368   Top1 82.236328   Top5 97.939453   BatchTime 0.407723   LR 0.000907
0.87343752
0.87335783
0.87342072
0.87353337
0.87340724
0.87346905
0.87360078
0.87363619
0.87366670
0.87360048
0.87351751
0.87336272
0.87323964
0.87314099
0.87310827
0.87297016
0.87290138
0.87285143
0.87281090
0.87281567
0.87272543
0.87276328
INFO - Training [7][   60/  196]   Loss 0.506499   Top1 82.714844   Top5 97.897135   BatchTime 0.393617   LR 0.000845
0.87271792
0.87267315
0.87252784
0.87248975
0.87237626
0.87221831
0.87198788
0.87181109
0.87155271
0.87127674
0.87110555
0.87099093
0.87106103
0.87077433
0.87093252
0.87090302
0.87051487
INFO - Training [7][   80/  196]   Loss 0.506454   Top1 82.558594   Top5 98.022461   BatchTime 0.384369   LR 0.000786
0.87021101
0.86978197
0.86925292
0.86901963
0.86890709
0.86880141
0.86819774
0.86763632
0.86751437
0.86770403
0.86753464
0.86735916
0.86695784
0.86678708
0.86671466
0.86657667
0.86685276
0.86663491
0.86580139
0.86518139
0.86408728
0.86327386
0.86206985
0.86038727
0.85752052
INFO - Training [7][  100/  196]   Loss 0.496697   Top1 82.835938   Top5 98.125000   BatchTime 0.369521   LR 0.000728
0.85524261
0.85359901
0.85291928
0.85083163
0.84955841
0.85178041
0.85537475
0.85864896
0.86197448
0.86365050
0.86420947
0.86477244
0.86505395
0.86491734
0.86504149
0.86523455
0.86527556
0.86536449
INFO - Training [7][  120/  196]   Loss 0.494644   Top1 82.910156   Top5 98.212891   BatchTime 0.363096   LR 0.000673
0.86575937
0.86617243
0.86639363
0.86679536
0.86729586
0.86787689
0.86809319
0.86835688
0.86895072
0.86931282
0.86945993
0.86962724
0.86964470
0.86977196
0.86979961
0.86995810
0.87014288
0.87042785
0.87050217
0.87026370
0.87005740
0.87006897
INFO - Training [7][  140/  196]   Loss 0.495345   Top1 82.918527   Top5 98.253348   BatchTime 0.363230   LR 0.000619
0.87007308
0.87002683
0.86985648
0.86977488
0.86972344
0.86955196
0.86955798
0.86937267
0.86920196
0.86877823
0.86835480
0.86770248
0.86725432
0.86672056
0.86624908
0.86567515
INFO - Training [7][  160/  196]   Loss 0.496424   Top1 82.856445   Top5 98.261719   BatchTime 0.364042   LR 0.000567
0.86515254
0.86411119
0.86269993
0.86172700
0.86124885
0.86092770
0.86171186
0.86165005
0.86083019
0.85996193
0.85841513
0.85740262
0.85529304
0.85380399
0.85266864
0.85123098
0.84968948
0.84851038
0.84693199
0.84637618
0.84573209
0.84515566
0.84487361
INFO - Training [7][  180/  196]   Loss 0.497916   Top1 82.825521   Top5 98.200955   BatchTime 0.363757   LR 0.000517
0.84460980
0.84483027
0.84532613
0.84553856
0.84608531
0.84674793
0.84735352
0.84754711
0.84773088
0.84826088
0.84848142
0.84842354
0.84845734
0.84863418
0.84857243
0.84908420
INFO - ==> Top1: 82.904    Top5: 98.204    Loss: 0.495
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 0.403423   Top1 86.640625   Top5 99.355469   BatchTime 0.119604
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.1738)
features.1.conv.0 tensor(0.0306)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0747)
features.2.conv.0 tensor(0.0284)
features.2.conv.3 tensor(0.0679)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0514)
features.4.conv.0 tensor(0.0378)
features.4.conv.3 tensor(0.1047)
features.4.conv.6 tensor(0.1069)
features.5.conv.0 tensor(0.0446)
features.5.conv.3 tensor(0.0602)
features.5.conv.6 tensor(0.1012)
features.6.conv.0 tensor(0.0288)
features.6.conv.3 tensor(0.0584)
features.6.conv.6 tensor(0.0706)
features.7.conv.0 tensor(0.0608)
features.7.conv.3 tensor(0.1204)
features.7.conv.6 tensor(0.1075)
features.8.conv.0 tensor(0.0678)
features.8.conv.3 tensor(0.1429)
features.8.conv.6 tensor(0.1165)
features.9.conv.0 tensor(0.0753)
features.9.conv.3 tensor(0.1545)
features.9.conv.6 tensor(0.1141)
features.10.conv.0 tensor(0.0454)
features.10.conv.3 tensor(0.1091)
features.10.conv.6 tensor(0.0879)
features.11.conv.0 tensor(0.1365)
features.11.conv.3 tensor(0.0992)
features.11.conv.6 tensor(0.2057)
features.12.conv.0 tensor(0.1445)
features.12.conv.3 tensor(0.1244)
features.12.conv.6 tensor(0.1894)
features.13.conv.0 tensor(0.0662)
features.13.conv.3 tensor(0.1620)
features.13.conv.6 tensor(0.0916)
features.14.conv.0 tensor(0.8350)
features.14.conv.3 tensor(0.0966)
features.14.conv.6 tensor(0.7567)
features.15.conv.0 tensor(0.8736)
features.15.conv.3 tensor(0.0855)
features.15.conv.6 tensor(0.9000)
features.16.conv.0 tensor(0.0895)
features.16.conv.3 tensor(0.0957)
features.16.conv.6 tensor(0.1193)
conv.0 tensor(0.1299)
tensor(698500.) 2188896.0
INFO - Validation [7][   40/   40]   Loss 0.391681   Top1 86.700000   Top5 99.540000   BatchTime 0.087482
INFO - ==> Top1: 86.700    Top5: 99.540    Loss: 0.392
INFO - ==> Sparsity : 0.319
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 86.700   Top5: 99.540]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 86.090   Top5: 99.520]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 85.960   Top5: 99.410]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.84974599
0.85008967
0.85050642
0.85076386
0.85093564
0.85092407
0.85097009
0.85091209
0.85050327
0.85038489
0.85048074
0.85022902
0.84971303
0.84899294
0.84833723
0.84806150
0.84779549
0.84742105
0.84725815
0.84680045
0.84686369
INFO - Training [8][   20/  196]   Loss 0.448987   Top1 84.199219   Top5 97.675781   BatchTime 0.430127   LR 0.000434
0.84698981
0.84682006
0.84702659
0.84710056
0.84728462
0.84737921
0.84751183
0.84750366
0.84737420
0.84740764
0.84745997
0.84780151
0.84776503
0.84794003
0.84821856
0.84848708
INFO - Training [8][   40/  196]   Loss 0.482763   Top1 82.910156   Top5 97.773438   BatchTime 0.397357   LR 0.000389
0.84889096
0.84934157
0.84964097
0.84970057
0.84982920
0.85015923
0.85012627
0.85008138
0.84996289
0.84973615
0.84935343
0.84934080
0.84889185
0.84869725
0.84799755
0.84722382
0.84677595
0.84681654
0.84676135
0.84669858
0.84647840
0.84640694
0.84644073
INFO - Training [8][   60/  196]   Loss 0.480544   Top1 83.118490   Top5 97.890625   BatchTime 0.383256   LR 0.000347
0.84621131
0.84621733
0.84591746
0.84572268
0.84640807
0.84631675
0.84613490
0.84596884
0.84562528
0.84570771
0.84585077
0.84576637
0.84580261
0.84622139
0.84673589
0.84899586
0.84888256
0.84871250
0.84856462
INFO - Training [8][   80/  196]   Loss 0.484665   Top1 83.125000   Top5 97.983398   BatchTime 0.364838   LR 0.000308
0.84836662
0.84808242
0.84794283
0.84771997
0.84753102
0.84751850
0.84740424
0.84714127
0.84714365
0.84721398
0.84726602
0.84727925
0.84701282
0.84654671
0.84634769
0.84611344
0.84604359
0.84611589
0.84629595
0.84612161
0.84605837
0.84588087
INFO - Training [8][  100/  196]   Loss 0.480768   Top1 83.242188   Top5 98.085938   BatchTime 0.366050   LR 0.000270
0.84607118
0.84619051
0.84605956
0.84592110
0.84593779
0.84588778
0.84580892
0.84560555
0.84538335
0.84534997
0.84529269
0.84512919
0.84501874
0.84480208
0.84453839
0.84429014
INFO - Training [8][  120/  196]   Loss 0.472707   Top1 83.548177   Top5 98.186849   BatchTime 0.366297   LR 0.000235
0.84470612
0.84452796
0.84449828
0.84443426
0.84429502
0.84427851
0.84403330
0.84400064
0.84393495
0.84392172
0.84381294
0.84383124
0.84377569
0.84371257
0.84358072
0.84342170
0.84348702
0.84356499
0.84370953
0.84380680
0.84365201
0.84355354
INFO - Training [8][  140/  196]   Loss 0.469353   Top1 83.643973   Top5 98.219866   BatchTime 0.366173   LR 0.000202
0.84334487
0.84325695
0.84318775
0.84308004
0.84299201
0.84296197
0.84284896
0.84291995
0.84291834
0.84278953
0.84279388
0.84278309
0.84272856
0.84263325
0.84271556
0.84274340
0.84282929
0.84278262
0.84278816
0.84277773
0.84276158
0.84277695
INFO - Training [8][  160/  196]   Loss 0.470575   Top1 83.615723   Top5 98.244629   BatchTime 0.365872   LR 0.000172
0.84279376
0.84281379
0.84286183
0.84281588
0.84272957
0.84277600
0.84284329
0.84288561
0.84288323
0.84297848
0.84293616
0.84294468
0.84291929
0.84286690
0.84286588
0.84285182
0.84278065
INFO - Training [8][  180/  196]   Loss 0.468093   Top1 83.665365   Top5 98.203125   BatchTime 0.364996   LR 0.000143
0.84243733
0.84211534
0.84209275
0.84194112
0.84178811
0.84173054
0.84161496
0.84137207
0.84133035
0.84120548
0.84115267
0.84114701
0.84104604
0.84089923
0.84076691
0.84042501
0.84014064
INFO - ==> Top1: 83.718    Top5: 98.220    Loss: 0.466
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84004366
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.377211   Top1 87.421875   Top5 99.414062   BatchTime 0.124248
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0286)
features.1.conv.3 tensor(0.0984)
features.1.conv.6 tensor(0.0720)
features.2.conv.0 tensor(0.0272)
features.2.conv.3 tensor(0.0679)
features.2.conv.6 tensor(0.0804)
features.3.conv.0 tensor(0.0211)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0499)
features.4.conv.0 tensor(0.0378)
features.4.conv.3 tensor(0.1042)
features.4.conv.6 tensor(0.1056)
features.5.conv.0 tensor(0.0456)
features.5.conv.3 tensor(0.0619)
features.5.conv.6 tensor(0.1034)
features.6.conv.0 tensor(0.0303)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0689)
features.7.conv.0 tensor(0.0590)
features.7.conv.3 tensor(0.1204)
features.7.conv.6 tensor(0.1093)
features.8.conv.0 tensor(0.0664)
features.8.conv.3 tensor(0.1398)
features.8.conv.6 tensor(0.1252)
features.9.conv.0 tensor(0.0754)
features.9.conv.3
INFO - Validation [8][   40/   40]   Loss 0.367104   Top1 87.680000   Top5 99.530000   BatchTime 0.091538
INFO - ==> Top1: 87.680    Top5: 99.530    Loss: 0.367
INFO - ==> Sparsity : 0.328
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 86.700   Top5: 99.540]
features.9.conv.3 tensor(0.1539)
features.9.conv.6 tensor(0.1187)
features.10.conv.0 tensor(0.0416)
features.10.conv.3 tensor(0.1056)
features.10.conv.6 tensor(0.0887)
features.11.conv.0 tensor(0.1371)
features.11.conv.3 tensor(0.0986)
features.11.conv.6 tensor(0.2131)
features.12.conv.0 tensor(0.1492)
features.12.conv.3 tensor(0.1240)
features.12.conv.6 tensor(0.2004)
features.13.conv.0 tensor(0.0693)
features.13.conv.3 tensor(0.1618)
features.13.conv.6 tensor(0.0880)
features.14.conv.0 tensor(0.8390)
features.14.conv.3 tensor(0.0985)
features.14.conv.6 tensor(0.8628)
features.15.conv.0 tensor(0.8808)
features.15.conv.3 tensor(0.0847)
features.15.conv.6 tensor(0.9064)
features.16.conv.0 tensor(0.0907)
features.16.conv.3 tensor(0.0961)
features.16.conv.6 tensor(0.1200)
conv.0 tensor(0.1293)
tensor(718971.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.84002340
0.83999962
0.83992398
0.83986050
0.83983952
0.83978891
0.83977419
0.83971500
0.83975095
0.83978742
0.83985341
0.83992344
0.83992589
0.83994186
0.83995652
0.83994722
0.83992916
INFO - Training [9][   20/  196]   Loss 0.465137   Top1 83.632812   Top5 97.753906   BatchTime 0.423940   LR 0.000100
0.83993852
0.83981633
0.83979481
0.83977848
0.83978009
0.83971268
0.83967239
0.83962876
0.83954632
0.83948940
0.83937913
0.83935773
0.83922964
0.83911067
0.83890206
0.83868879
0.83861107
0.83860880
0.83853769
0.83843040
0.83842349
0.83840090
INFO - Training [9][   40/  196]   Loss 0.477232   Top1 83.447266   Top5 97.998047   BatchTime 0.387144   LR 0.000079
0.83834380
0.83830786
0.83828074
0.83828825
0.83831429
0.83829117
0.83827859
0.83825570
0.83821732
0.83817989
0.83820158
0.83822781
0.83822602
0.83821243
0.83821172
0.83820128
0.83814347
0.83811504
0.83811718
INFO - Training [9][   60/  196]   Loss 0.465247   Top1 84.010417   Top5 98.151042   BatchTime 0.361305   LR 0.000060
0.83810341
0.83813441
0.83809733
0.83808261
0.83806258
0.83806980
0.83808452
0.83810210
0.83810288
0.83808810
0.83810347
0.83809066
0.83813173
0.83814818
0.83813441
0.83813494
0.83813423
0.83814353
0.83812004
0.83816981
0.83814633
0.83811766
INFO - Training [9][   80/  196]   Loss 0.467496   Top1 83.867188   Top5 98.251953   BatchTime 0.363065   LR 0.000044
0.83812362
0.83811986
0.83812094
0.83815068
0.83817899
0.83815259
0.83813012
0.83813179
0.83815610
0.83815676
0.83817834
0.83817506
0.83812863
0.83811927
0.83812833
0.83816469
INFO - Training [9][  100/  196]   Loss 0.460076   Top1 84.117188   Top5 98.300781   BatchTime 0.365851   LR 0.000030
0.83821255
0.83821815
0.83823222
0.83821309
0.83820719
0.83818263
0.83818316
0.83819669
0.83816683
0.83816940
0.83820432
0.83818555
0.83817530
0.83818179
0.83814961
0.83813184
0.83812255
0.83812463
0.83811021
0.83807808
0.83810031
0.83810437
0.83809239
INFO - Training [9][  120/  196]   Loss 0.453955   Top1 84.348958   Top5 98.346354   BatchTime 0.363681   LR 0.000019
0.83809566
0.83806485
0.83805412
0.83809286
0.83808470
0.83806860
0.83811814
0.83810121
0.83809882
0.83810896
0.83811015
0.83811826
0.83811814
0.83804941
0.83807617
0.83808589
0.83810157
0.83807117
0.83808559
0.83808869
0.83810383
0.83808827
INFO - Training [9][  140/  196]   Loss 0.454214   Top1 84.355469   Top5 98.384487   BatchTime 0.363654   LR 0.000010
0.83807534
0.83806378
0.83810210
0.83807945
0.83807129
0.83804959
0.83807802
0.83803928
0.83807170
0.83803636
0.83806318
0.83804500
0.83808988
0.83804905
0.83802640
0.83804852
0.83804190
INFO - Training [9][  160/  196]   Loss 0.459892   Top1 84.155273   Top5 98.378906   BatchTime 0.363410   LR 0.000004
0.83802378
0.83803552
0.83804208
0.83807653
0.83806741
0.83803153
0.83807147
0.83803064
0.83803564
0.83804184
0.83807367
0.83805442
0.83806473
0.83806187
0.83803087
0.83807617
0.83807713
0.83806902
0.83801889
0.83800089
0.83797884
0.83797771
INFO - Training [9][  180/  196]   Loss 0.457644   Top1 84.216580   Top5 98.339844   BatchTime 0.362851   LR 0.000001
0.83803356
0.83804756
0.83807510
0.83807170
0.83807057
0.83805436
0.83807093
0.83806956
0.83804548
0.83805805
0.83804566
0.83804870
0.83807838
0.83807945
0.83806938
0.83803493
INFO - ==> Top1: 84.258    Top5: 98.330    Loss: 0.457
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [9][   20/   40]   Loss 0.358934   Top1 87.578125   Top5 99.550781   BatchTime 0.190842
INFO - Validation [9][   40/   40]   Loss 0.348532   Top1 88.080000   Top5 99.600000   BatchTime 0.122031
INFO - ==> Top1: 88.080    Top5: 99.600    Loss: 0.349
INFO - ==> Sparsity : 0.329
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.700   Top5: 99.540]
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.1758)
features.1.conv.0 tensor(0.0286)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0725)
features.2.conv.0 tensor(0.0278)
features.2.conv.3 tensor(0.0671)
features.2.conv.6 tensor(0.0810)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0510)
features.4.conv.0 tensor(0.0374)
features.4.conv.3 tensor(0.1065)
features.4.conv.6 tensor(0.1060)
features.5.conv.0 tensor(0.0451)
features.5.conv.3 tensor(0.0613)
features.5.conv.6 tensor(0.1051)
features.6.conv.0 tensor(0.0298)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0692)
features.7.conv.0 tensor(0.0590)
features.7.conv.3 tensor(0.1189)
features.7.conv.6 tensor(0.1117)
features.8.conv.0 tensor(0.0658)
features.8.conv.3 tensor(0.1406)
features.8.conv.6 tensor(0.1224)
features.9.conv.0 tensor(0.0758)
features.9.conv.3 tensor(0.1539)
features.9.conv.6 tensor(0.1213)
features.10.conv.0 tensor(0.0417)
features.10.conv.3 tensor(0.1073)
features.10.conv.6 tensor(0.0888)
features.11.conv.0 tensor(0.1370)
features.11.conv.3 tensor(0.0984)
features.11.conv.6 tensor(0.2111)
features.12.conv.0 tensor(0.1502)
features.12.conv.3 tensor(0.1236)
features.12.conv.6 tensor(0.2005)
features.13.conv.0 tensor(0.0692)
features.13.conv.3 tensor(0.1624)
features.13.conv.6 tensor(0.0940)
features.14.conv.0 tensor(0.8396)
features.14.conv.3 tensor(0.0975)
features.14.conv.6 tensor(0.8676)
features.15.conv.0 tensor(0.8837)
features.15.conv.3 tensor(0.0851)
features.15.conv.6 tensor(0.9069)
features.16.conv.0 tensor(0.0911)
features.16.conv.3 tensor(0.0968)
features.16.conv.6 tensor(0.1203)
conv.0 tensor(0.1287)
tensor(720812.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.83801806
0.83985668
0.84221166
0.84464252
0.84533626
0.84790510
0.84957772
0.85134059
0.85252881
0.85356671
0.85292011
0.85284412
0.85427970
0.85524303
0.85555798
INFO - Training [10][   20/  196]   Loss 0.538582   Top1 81.308594   Top5 97.500000   BatchTime 0.405743   LR 0.002500
0.85551566
0.85644126
0.85661197
0.85720646
0.85679400
0.84961605
0.83439833
0.85672039
0.86002839
0.86034286
0.86065924
0.86078537
0.86096150
0.86169738
0.86215931
0.86249804
0.86296409
0.86303180
0.86416972
0.86504787
0.86577523
0.86638755
0.86584401
0.86562949
0.86581594
0.86664647
0.86699170
INFO - Training [10][   40/  196]   Loss 0.550342   Top1 81.015625   Top5 97.578125   BatchTime 0.356839   LR 0.002499
0.86796999
0.86868322
0.87004179
0.87091273
0.87193203
0.87278956
0.87367970
0.87514025
0.87629181
0.87806153
0.87966061
0.88087392
0.88162839
0.88178849
0.88210547
0.88192117
INFO - Training [10][   60/  196]   Loss 0.551809   Top1 80.904948   Top5 97.819010   BatchTime 0.359121   LR 0.002499
0.88193512
0.88209486
0.88208961
0.88203257
0.88146824
0.88101345
0.88046813
0.88042629
0.88262051
0.88250488
0.88205677
0.88175040
0.88096404
0.88021827
0.87960613
0.87902784
0.87844557
0.87814754
0.87754148
0.87711400
0.87671465
INFO - Training [10][   80/  196]   Loss 0.555540   Top1 80.820312   Top5 97.944336   BatchTime 0.365498   LR 0.002497
0.87622511
0.87579089
0.87521070
0.87462950
0.87407249
0.87169838
0.85838771
0.86830968
0.87207288
0.87157887
0.87146616
0.87080342
0.87017077
0.86950356
0.86883235
0.86815566
0.86717874
0.86490870
0.86307627
0.86156124
0.85944200
0.85794604
INFO - Training [10][  100/  196]   Loss 0.552760   Top1 80.945312   Top5 97.988281   BatchTime 0.366079   LR 0.002496
0.86005455
0.86035907
0.86087328
0.86258084
0.86562586
0.86727196
0.86740661
0.86795044
0.86714453
0.86566353
0.86473197
0.86352336
0.86342251
0.86236876
0.86110979
0.85969454
INFO - Training [10][  120/  196]   Loss 0.542319   Top1 81.285807   Top5 98.095703   BatchTime 0.365748   LR 0.002494
0.85819286
0.85793173
0.85773236
0.85761070
0.85716462
0.85667384
0.85615897
0.85522836
0.85390955
0.85259938
0.85247004
0.85189825
0.85158867
0.85136235
0.85038054
0.85084099
0.85122734
0.85115051
0.85094166
0.85031945
0.84954739
0.84883910
INFO - Training [10][  140/  196]   Loss 0.542556   Top1 81.392299   Top5 98.158482   BatchTime 0.365851   LR 0.002492
0.84828895
0.84784520
0.84759367
0.84719050
0.84766161
0.84918708
0.85042304
0.85222042
0.85402733
0.85533166
0.85676342
0.85780942
0.86037719
0.86331862
0.86523968
0.86742711
0.86987025
0.87121671
0.86897236
0.85180664
0.84907997
0.85054725
0.85042369
INFO - Training [10][  160/  196]   Loss 0.546699   Top1 81.252441   Top5 98.098145   BatchTime 0.364737   LR 0.002490
0.85257822
0.86730134
0.87872863
0.87904561
0.87806588
0.87764305
0.87760413
0.87768435
0.87792277
0.87805271
0.87822652
0.87847966
0.87892151
0.87910324
0.87898695
0.87874311
INFO - Training [10][  180/  196]   Loss 0.547316   Top1 81.221788   Top5 98.023003   BatchTime 0.365048   LR 0.002487
0.87848115
0.87846565
0.87866759
0.87891030
0.87936896
0.87992990
0.88022035
0.88029671
0.88066459
0.88049370
0.88082099
0.88119942
0.88135511
0.88066787
0.88020986
0.87989438
INFO - ==> Top1: 81.326    Top5: 98.042    Loss: 0.546
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87941551
0.87898999
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.597752   Top1 81.269531   Top5 98.281250   BatchTime 0.115859
INFO - Validation [10][   40/   40]   Loss 0.593479   Top1 81.170000   Top5 98.490000   BatchTime 0.083197
INFO - ==> Top1: 81.170    Top5: 98.490    Loss: 0.593
INFO - ==> Sparsity : 0.291
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.700   Top5: 99.540]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.1738)
features.1.conv.0 tensor(0.0286)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0751)
features.2.conv.0 tensor(0.0231)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0802)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0617)
features.3.conv.6 tensor(0.0493)
features.4.conv.0 tensor(0.0400)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.1012)
features.5.conv.0 tensor(0.0417)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.1021)
features.6.conv.0 tensor(0.0282)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0679)
features.7.conv.0 tensor(0.0600)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.1116)
features.8.conv.0 tensor(0.0546)
features.8.conv.3 tensor(0.1403)
features.8.conv.6 tensor(0.1179)
features.9.conv.0 tensor(0.0685)
features.9.conv.3 tensor(0.1534)
features.9.conv.6 tensor(0.1153)
features.10.conv.0 tensor(0.0511)
features.10.conv.3 tensor(0.1102)
features.10.conv.6 tensor(0.0861)
features.11.conv.0 tensor(0.1477)
features.11.conv.3 tensor(0.1044)
features.11.conv.6 tensor(0.1796)
features.12.conv.0 tensor(0.1193)
features.12.conv.3 tensor(0.1495)
features.12.conv.6 tensor(0.1823)
features.13.conv.0 tensor(0.0763)
features.13.conv.3 tensor(0.1682)
features.13.conv.6 tensor(0.0808)
features.14.conv.0 tensor(0.8338)
features.14.conv.3 tensor(0.0998)
features.14.conv.6 tensor(0.4041)
features.15.conv.0 tensor(0.8415)
features.15.conv.3 tensor(0.0855)
features.15.conv.6 tensor(0.8466)
features.16.conv.0 tensor(0.0916)
features.16.conv.3 tensor(0.0972)
features.16.conv.6 tensor(0.1381)
conv.0 tensor(0.1377)
tensor(637078.) 2188896.0
0.87887591
0.87887061
0.87884086
0.87906092
0.87936711
0.87926829
0.87919080
0.87914169
0.87896144
0.87866819
0.87865281
0.87875986
0.87871730
0.87849718
0.87829643
0.87819541
0.87814415
0.87811536
INFO - Training [11][   20/  196]   Loss 0.557013   Top1 80.605469   Top5 97.578125   BatchTime 0.414028   LR 0.002481
0.87827539
0.87830365
0.87822860
0.87810940
0.87791646
0.87797660
0.87789112
0.87795907
0.87822545
0.87828243
0.87856573
0.87906653
0.87926304
0.87910050
0.87911439
0.87963253
0.88081127
0.88157606
0.88100713
INFO - Training [11][   40/  196]   Loss 0.562924   Top1 80.400391   Top5 97.675781   BatchTime 0.371959   LR 0.002478
0.88085455
0.88059515
0.88044524
0.88008958
0.87970811
0.87925786
0.87928057
0.87892580
0.87884390
0.87867105
0.87845564
0.87815720
0.87780744
0.87750727
0.87740535
0.87736309
0.87746674
0.87704617
0.87690383
0.87684685
0.87687021
0.87671572
INFO - Training [11][   60/  196]   Loss 0.557830   Top1 80.820312   Top5 97.786458   BatchTime 0.366508   LR 0.002474
0.87668735
0.87683350
0.87697190
0.87713307
0.87702584
0.87696946
0.87695467
0.87681049
0.87689590
0.87697178
0.87701094
0.87677819
0.87683117
0.87668848
0.87657475
0.87633753
0.87642229
0.87620687
0.87608689
0.87630963
0.87640244
0.87659353
0.87659836
INFO - Training [11][   80/  196]   Loss 0.553715   Top1 80.864258   Top5 97.861328   BatchTime 0.364496   LR 0.002470
0.87656742
0.87669206
0.87695223
0.87700307
0.87714881
0.87718135
0.87712002
0.87727129
0.87760973
0.87800628
0.87816668
0.87802356
0.87796187
0.87783712
0.87758690
0.87735832
INFO - Training [11][  100/  196]   Loss 0.545515   Top1 81.234375   Top5 97.929688   BatchTime 0.366441   LR 0.002465
0.87743187
0.87760222
0.87752903
0.87746561
0.87711388
0.87655157
0.87648350
0.87647039
0.87627578
0.87575775
0.87559241
0.87579316
0.87602037
0.87607843
0.87631923
0.87650335
0.87680513
0.87665111
0.87685633
0.87738943
0.87892151
0.87961793
INFO - Training [11][  120/  196]   Loss 0.540499   Top1 81.396484   Top5 98.059896   BatchTime 0.366086   LR 0.002460
0.87985212
0.87977660
0.87965065
0.87947136
0.87939966
0.87919575
0.87889594
0.87875801
0.87867075
0.87882030
0.87880045
0.87853104
0.87829930
0.87812591
0.87796360
0.87790644
0.87783200
0.87755823
0.87745506
0.87759978
0.87739229
0.87737972
INFO - Training [11][  140/  196]   Loss 0.542250   Top1 81.330915   Top5 98.080357   BatchTime 0.365659   LR 0.002455
0.87702018
0.87686205
0.87700307
0.87707537
0.87769026
0.87786752
0.87795508
0.87793571
0.87796742
0.87787801
0.87769163
0.87763733
0.87783939
0.87805635
0.87808657
0.87795216
INFO - Training [11][  160/  196]   Loss 0.547651   Top1 81.191406   Top5 98.034668   BatchTime 0.365114   LR 0.002450
0.87796980
0.87799984
0.87787098
0.87782121
0.87765259
0.87754852
0.87781298
0.87747383
0.87708676
0.87698174
0.87722582
0.87706590
0.87702829
0.87688744
0.87692398
0.87668753
0.87708360
0.87686998
0.87703401
0.87662488
0.87640232
0.87652826
0.87653124
INFO - Training [11][  180/  196]   Loss 0.548108   Top1 81.132812   Top5 97.970920   BatchTime 0.363739   LR 0.002444
0.87658030
0.87661010
0.87671053
0.87688190
0.87674779
0.87682641
0.87698805
0.87738132
0.87760568
0.87775522
0.87816769
0.87804973
0.87778121
0.87778080
0.87764728
INFO - ==> Top1: 81.154    Top5: 97.950    Loss: 0.547
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.803923   Top1 72.792969   Top5 98.183594   BatchTime 0.123624
INFO - Validation [11][   40/   40]   Loss 0.802323   Top1 72.820000   Top5 98.240000   BatchTime 0.089227
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0778)
features.3.conv.0 tensor(0.0260)
features.3.conv.3 tensor(0.0532)
features.3.conv.6 tensor(0.0503)
features.4.conv.0 tensor(0.0404)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.1040)
features.5.conv.0 tensor(0.0459)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.1027)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0596)
features.6.conv.6 tensor(0.0624)
features.7.conv.0 tensor(0.0723)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.1067)
features.8.conv.0 tensor(0.0507)
features.8.conv.3 tensor(0.1337)
features.8.conv.6 tensor(0.1139)
features.9.conv.0 tensor(0.0740)
features.9.conv.3 tensor(0.1531)
features.9.conv.6 tensor(0.1154)
features.10.conv.0 tensor(0.0446)
features.10.conv.3 tensor(0.1114)
features.10.conv.6 tensor(0.0802)
features.11.conv.0 tensor(0.1435)
features.11.conv.3 tensor(0.1086)
features.11.conv.6 tensor(0.1867)
features.12.conv.0 tensor(0.1177)
features.12.conv.3 tensor(0.1545)
features.12.conv.6 tensor(0.1933)
features.13.conv.0 tensor(0.0742)
features.13.conv.3 tensor(0.1651)
features.13.conv.6 tensor(0.0787)
features.14.conv.0 tensor(0.8175)
features.14.conv.3 tensor(0.0999)
features.14.conv.6 tensor(0.4517)
features.15.conv.0 tensor(0.8795)
features.15.conv.3 tensor(0.0863)
features.15.conv.6 tensor(0.8539)
features.16.conv.0 tensor(0.0859)
features.16.conv.3 tensor(0.0991)
features.16.conv.6 tensor(0.1639)
conv.0 tensor(0.1569)
tensor(663854.) 2188896.0
INFO - ==> Top1: 72.820    Top5: 98.240    Loss: 0.802
INFO - ==> Sparsity : 0.303
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.700   Top5: 99.540]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
0.87754053
0.87748641
0.87746108
0.87743783
0.87720287
0.87705261
0.87673897
0.87647396
0.87656832
0.87648177
0.87600696
0.87577260
0.87560338
0.87565297
0.87562168
0.87569070
0.87553674
0.87554055
0.87562639
0.87571299
0.87592679
INFO - Training [12][   20/  196]   Loss 0.565908   Top1 80.996094   Top5 97.558594   BatchTime 0.418313   LR 0.002433
0.87590080
0.87589735
0.87590867
0.87577915
0.87574375
0.87566298
0.87533152
0.87517202
0.87509209
0.87505925
0.87511611
0.87533259
0.87485558
0.87484443
0.87475359
0.87473273
0.87439466
0.87419790
0.87405688
INFO - Training [12][   40/  196]   Loss 0.564411   Top1 80.888672   Top5 97.607422   BatchTime 0.362650   LR 0.002426
0.87382782
0.87373781
0.87336922
0.87315404
0.87284058
0.87273735
0.87270272
0.87240756
0.87237281
0.87230366
0.87204641
0.87172425
0.87168080
0.87144762
0.87105101
0.87097198
0.87074274
INFO - Training [12][   60/  196]   Loss 0.553201   Top1 81.152344   Top5 97.662760   BatchTime 0.359067   LR 0.002419
0.87074208
0.87055385
0.86977839
0.86885875
0.86786962
0.86677164
0.86561280
0.86376053
0.86218846
0.85991508
0.85773110
0.85659140
0.85503536
0.85368282
0.85170060
0.85082281
0.85016567
0.84969634
0.84944469
0.84928191
0.84946477
0.84917361
0.84894657
INFO - Training [12][   80/  196]   Loss 0.555958   Top1 81.171875   Top5 97.778320   BatchTime 0.358417   LR 0.002412
0.84878379
0.84888458
0.84881258
0.84954280
0.85172004
0.85496920
0.86013621
0.86667174
0.87230337
0.87606633
0.87667614
0.87649304
0.87647861
0.87660205
0.87663227
0.87689787
0.87708646
0.87718356
0.87731326
0.87725240
0.87727380
INFO - Training [12][  100/  196]   Loss 0.548522   Top1 81.433594   Top5 97.781250   BatchTime 0.360763   LR 0.002404
0.87720412
0.87718415
0.87730771
0.87698096
0.87638271
0.87619770
0.87635630
0.87650299
0.87657380
0.87651032
0.87659532
0.87689716
0.87671596
0.87682652
0.87696087
0.87694448
INFO - Training [12][  120/  196]   Loss 0.542688   Top1 81.624349   Top5 97.903646   BatchTime 0.363054   LR 0.002396
0.87764525
0.87816590
0.87805855
0.87717795
0.87760264
0.87607843
0.87587112
0.87767357
0.87759191
0.87749493
0.87760276
0.87726808
0.87703556
0.87698561
0.87703949
0.87678313
0.87664479
0.87623113
0.87591577
0.87559813
0.87517834
0.87481087
INFO - Training [12][  140/  196]   Loss 0.538637   Top1 81.771763   Top5 97.946429   BatchTime 0.363695   LR 0.002388
0.87482035
0.87489921
0.87468177
0.87454438
0.87464613
0.87510830
0.87544161
0.87585056
0.87640429
0.87697363
0.87749678
0.87796891
0.87878013
0.87988907
0.88031960
0.88011891
0.87980229
INFO - Training [12][  160/  196]   Loss 0.543543   Top1 81.599121   Top5 97.919922   BatchTime 0.363629   LR 0.002380
0.87960154
0.87920064
0.87868708
0.87829208
0.87796646
0.87767661
0.87761062
0.87745857
0.87729770
0.87731612
0.87732655
0.87717396
0.87675333
0.87646157
0.87635952
0.87645143
0.87639099
0.87642533
0.87610376
0.87599099
0.87540698
0.87533361
INFO - Training [12][  180/  196]   Loss 0.540740   Top1 81.673177   Top5 97.903646   BatchTime 0.363293   LR 0.002371
0.87682688
0.87684971
0.87690669
0.87684399
0.87700230
0.87712002
0.87748694
0.87793177
0.87800378
0.87805223
0.87806916
0.87786293
0.87759984
0.87736237
0.87730062
0.87700295
INFO - ==> Top1: 81.732    Top5: 97.924    Loss: 0.538
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87680793
0.87647331
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.412507   Top1 86.523438   Top5 99.335938   BatchTime 0.137081
INFO - Validation [12][   40/   40]   Loss 0.408460   Top1 86.540000   Top5 99.360000   BatchTime 0.096481
INFO - ==> Top1: 86.540    Top5: 99.360    Loss: 0.408
INFO - ==> Sparsity : 0.303
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 86.700   Top5: 99.540]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0686)
features.2.conv.0 tensor(0.0246)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0764)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0508)
features.4.conv.0 tensor(0.0330)
features.4.conv.3 tensor(0.1076)
features.4.conv.6 tensor(0.1016)
features.5.conv.0 tensor(0.0462)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.1006)
features.6.conv.0 tensor(0.0252)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0613)
features.7.conv.0 tensor(0.0623)
features.7.conv.3 tensor(0.1209)
features.7.conv.6 tensor(0.1076)
features.8.conv.0 tensor(0.0562)
features.8.conv.3 tensor(0.1383)
features.8.conv.6 tensor(0.1145)
features.9.conv.0 tensor(0.0826)
features.9.conv.3 tensor(0.1591)
features.9.conv.6 tensor(0.1093)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.1062)
features.10.conv.6 tensor(0.0809)
features.11.conv.0 tensor(0.1580)
features.11.conv.3 tensor(0.1136)
features.11.conv.6 tensor(0.1908)
features.12.conv.0 tensor(0.1470)
features.12.conv.3 tensor(0.1578)
features.12.conv.6 tensor(0.1909)
features.13.conv.0 tensor(0.0687)
features.13.conv.3 tensor(0.1663)
features.13.conv.6 tensor(0.0799)
features.14.conv.0 tensor(0.8572)
features.14.conv.3 tensor(0.1053)
features.14.conv.6 tensor(0.4385)
features.15.conv.0 tensor(0.8883)
features.15.conv.3 tensor(0.0852)
features.15.conv.6 tensor(0.8462)
features.16.conv.0 tensor(0.0833)
features.16.conv.3 tensor(0.1016)
features.16.conv.6 tensor(0.1599)
conv.0 tensor(0.1440)
tensor(663501.) 2188896.0
0.87627321
0.87596732
0.87579000
0.87561494
0.87554497
0.87543947
0.87526518
0.87529302
0.87527096
0.87536085
0.87529403
0.87542582
0.87526232
0.87502766
0.87494504
0.87478638
0.87454242
0.87430483
0.87429833
0.87406069
0.87384498
INFO - Training [13][   20/  196]   Loss 0.544387   Top1 81.445312   Top5 97.421875   BatchTime 0.436887   LR 0.002355
0.87370324
0.87374341
0.87388569
0.87432605
0.87426287
0.87589163
0.87638038
0.87618405
0.87589478
0.87559921
0.87547034
0.87543207
0.87535530
0.87529129
0.87530762
0.87547570
0.87549073
0.87538624
INFO - Training [13][   40/  196]   Loss 0.550529   Top1 81.318359   Top5 97.597656   BatchTime 0.379754   LR 0.002345
0.87533897
0.87516719
0.87511522
0.87512755
0.87515515
0.87510258
0.87515056
0.87518692
0.87519509
0.87541550
0.87540710
0.87538403
0.87545073
0.87520456
0.87519652
0.87522173
0.87490755
0.87470257
0.87452024
0.87442344
0.87442619
0.87459761
INFO - Training [13][   60/  196]   Loss 0.538209   Top1 81.679688   Top5 97.786458   BatchTime 0.376753   LR 0.002336
0.87477291
0.87491661
0.87509269
0.87524849
0.87544745
0.87529868
0.87532175
0.87556773
0.87586629
0.87597954
0.87598187
0.87629598
0.87682593
0.87731904
0.87843132
0.88026428
INFO - Training [13][   80/  196]   Loss 0.535909   Top1 81.621094   Top5 97.934570   BatchTime 0.371899   LR 0.002325
0.88118327
0.88155121
0.88153744
0.88037908
0.87927675
0.87820852
0.87774253
0.87699229
0.87676293
0.87654787
0.87583953
0.87524873
0.87500852
0.87450063
0.87428206
0.87319064
0.87446809
0.87465602
0.87510085
0.87541986
0.87554318
0.87596375
0.87689018
INFO - Training [13][  100/  196]   Loss 0.538374   Top1 81.519531   Top5 97.906250   BatchTime 0.368349   LR 0.002315
0.87727606
0.87745881
0.87766999
0.87809289
0.87784368
0.87791222
0.87761837
0.87753814
0.87775534
0.87771875
0.87759006
0.87749821
0.87752771
0.87730461
0.87723041
0.87752861
0.87771720
INFO - Training [13][  120/  196]   Loss 0.543524   Top1 81.383464   Top5 97.952474   BatchTime 0.367945   LR 0.002304
0.87729800
0.87703848
0.87705022
0.87695485
0.87689334
0.87668943
0.87549078
0.87247461
0.87192649
0.86470199
0.86258000
0.87252277
0.87548333
0.87567538
0.87557536
0.87562937
0.87558091
0.87573063
0.87589365
0.87599200
0.87611264
0.87639695
INFO - Training [13][  140/  196]   Loss 0.543708   Top1 81.328125   Top5 97.996652   BatchTime 0.367327   LR 0.002293
0.87675041
0.87688446
0.87696522
0.87683576
0.87673658
0.87679482
0.87675005
0.87679672
0.87670034
0.87677997
0.87676829
0.87683123
0.87655878
0.87645900
0.87636405
0.87634212
0.87626100
0.87630683
0.87623155
0.87618130
0.87606764
INFO - Training [13][  160/  196]   Loss 0.547117   Top1 81.215820   Top5 97.988281   BatchTime 0.368507   LR 0.002282
0.87579370
0.87577611
0.87548989
0.87543303
0.87532192
0.87543446
0.87517977
0.87493873
0.87483859
0.87466246
0.87457943
0.87447369
0.87486339
0.87500405
0.87486184
0.87475061
0.87480164
INFO - Training [13][  180/  196]   Loss 0.546308   Top1 81.230469   Top5 97.940538   BatchTime 0.367146   LR 0.002271
0.87510192
0.87511075
0.87508291
0.87525022
0.87543786
0.87561965
0.87595314
0.87597185
0.87605989
0.87612623
0.87636572
0.87660259
0.87701070
0.87735671
0.87759262
0.87810624
0.87879521
INFO - ==> Top1: 81.340    Top5: 97.928    Loss: 0.544
0.87967932
0.88021141
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.387133   Top1 86.992188   Top5 99.296875   BatchTime 0.125594
INFO - Validation [13][   40/   40]   Loss 0.378183   Top1 86.980000   Top5 99.480000   BatchTime 0.090011
INFO - ==> Top1: 86.980    Top5: 99.480    Loss: 0.378
INFO - ==> Sparsity : 0.308
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 86.980   Top5: 99.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1543)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0694)
features.2.conv.0 tensor(0.0272)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0796)
features.3.conv.0 tensor(0.0156)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.1788)
features.4.conv.0 tensor(0.0381)
features.4.conv.3 tensor(0.1013)
features.4.conv.6 tensor(0.0998)
features.5.conv.0 tensor(0.0402)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0996)
features.6.conv.0 tensor(0.0233)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0592)
features.7.conv.0 tensor(0.0561)
features.7.conv.3 tensor(0.1238)
features.7.conv.6 tensor(0.1031)
features.8.conv.0 tensor(0.0498)
features.8.conv.3 tensor(0.1377)
features.8.conv.6 tensor(0.1111)
features.9.conv.0 tensor(0.0733)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.1113)
features.10.conv.0 tensor(0.0338)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.0814)
features.11.conv.0 tensor(0.1585)
features.11.conv.3 tensor(0.1146)
features.11.conv.6 tensor(0.1779)
features.12.conv.0 tensor(0.1309)
features.12.conv.3 tensor(0.1615)
features.12.conv.6 tensor(0.1980)
features.13.conv.0 tensor(0.0695)
features.13.conv.3 tensor(0.1703)
features.13.conv.6 tensor(0.0828)
features.14.conv.0 tensor(0.8672)
features.14.conv.3 tensor(0.1066)
features.14.conv.6 tensor(0.4604)
features.15.conv.0 tensor(0.9089)
features.15.conv.3 tensor(0.0840)
features.15.conv.6 tensor(0.7445)
features.16.conv.0 tensor(0.0717)
features.16.conv.3 tensor(0.1084)
features.16.conv.6 tensor(0.1828)
conv.0 tensor(0.1763)
tensor(673338.) 2188896.0
0.88177776
0.88227266
0.88187754
0.88130927
0.88098180
0.88057011
0.88011134
0.87930107
0.87844288
0.87793106
0.87697458
0.87655663
0.87604648
0.87588388
0.87573248
0.87578601
0.87581390
INFO - Training [14][   20/  196]   Loss 0.534391   Top1 81.230469   Top5 97.773438   BatchTime 0.420871   LR 0.002250
0.87558240
0.87548661
0.87549990
0.87535721
0.87538612
0.87561411
0.87499434
0.87485474
0.87488687
0.87492156
0.87479556
0.87468803
0.87474889
0.87475783
0.87465209
0.87450916
0.87447673
0.87430269
0.87407333
0.87398857
0.87393469
0.87401199
0.87399364
0.87403542
0.87411344
INFO - Training [14][   40/  196]   Loss 0.546893   Top1 80.966797   Top5 97.812500   BatchTime 0.368842   LR 0.002238
0.87426621
0.87422335
0.87403977
0.87411696
0.87418115
0.87420708
0.87410569
0.87408763
0.87425029
0.87383473
0.87333900
0.87295216
0.87257117
0.87247998
0.87192512
0.87148070
0.87038571
INFO - Training [14][   60/  196]   Loss 0.541016   Top1 81.126302   Top5 97.851562   BatchTime 0.361949   LR 0.002225
0.87127417
0.87146938
0.87119824
0.87127662
0.87134528
0.87154251
0.87197876
0.87259966
0.87375605
0.87613833
0.87626988
0.87613010
0.87585592
0.87584358
0.87589669
0.87582636
0.87571406
0.87562150
0.87562388
0.87549412
0.87549716
0.87543476
0.87563086
INFO - Training [14][   80/  196]   Loss 0.532643   Top1 81.333008   Top5 97.993164   BatchTime 0.358106   LR 0.002213
0.87591690
0.87600946
0.87606061
0.87598521
0.87588966
0.87601548
0.87609345
0.87595338
0.87615663
0.87615323
0.87625206
0.87613094
0.87616551
0.87622148
0.87635094
0.87647003
INFO - Training [14][  100/  196]   Loss 0.524292   Top1 81.578125   Top5 98.062500   BatchTime 0.361015   LR 0.002200
0.87637937
0.87650752
0.87655747
0.87668574
0.87652493
0.87627012
0.87619692
0.87634146
0.87619692
0.87612838
0.87616271
0.87626165
0.87631088
0.87649280
0.87663215
0.87685001
0.87707943
0.87722999
0.87703627
0.87693477
0.87700939
INFO - Training [14][  120/  196]   Loss 0.520157   Top1 81.878255   Top5 98.125000   BatchTime 0.364249   LR 0.002186
0.87680489
0.87661791
0.87656939
0.87656087
0.87642270
0.87623537
0.87616402
0.87599778
0.87569350
0.87551546
0.87571961
0.87549180
0.87538856
0.87552869
0.87520534
0.87498081
0.87483138
0.87482083
0.87471306
0.87429351
0.87417269
0.87420923
INFO - Training [14][  140/  196]   Loss 0.516011   Top1 82.092634   Top5 98.208705   BatchTime 0.364242   LR 0.002173
0.87416846
0.87393874
0.87389535
0.87407464
0.87395006
0.87391931
0.87406546
0.87404656
0.87398154
0.87384605
0.87390035
0.87407935
0.87384796
0.87365240
0.87351596
0.87315470
INFO - Training [14][  160/  196]   Loss 0.515371   Top1 82.058105   Top5 98.217773   BatchTime 0.364907   LR 0.002159
0.87240738
0.87319779
0.87381601
0.87378669
0.87370729
0.87385052
0.87389374
0.87380177
0.87386960
0.87398261
0.87398112
0.87385011
0.87401330
0.87426418
0.87442565
0.87463582
0.87495613
0.87541896
0.87555045
0.87590152
0.87618881
0.87664503
INFO - Training [14][  180/  196]   Loss 0.513777   Top1 82.087674   Top5 98.148872   BatchTime 0.364288   LR 0.002145
0.87663943
0.87644744
0.87589008
0.87510830
0.87483478
0.87472075
0.87468183
0.87458557
0.87458277
0.87459773
0.87475574
0.87447876
0.87414527
0.87411624
0.87410432
0.87392294
0.87386918
INFO - ==> Top1: 82.190    Top5: 98.176    Loss: 0.511
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [14][   20/   40]   Loss 0.431327   Top1 85.781250   Top5 99.335938   BatchTime 0.149309
INFO - Validation [14][   40/   40]   Loss 0.417994   Top1 86.200000   Top5 99.530000   BatchTime 0.100096
INFO - ==> Top1: 86.200    Top5: 99.530    Loss: 0.418
INFO - ==> Sparsity : 0.322
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 86.980   Top5: 99.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.1738)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0773)
features.2.conv.0 tensor(0.0312)
features.2.conv.3 tensor(0.0687)
features.2.conv.6 tensor(0.0784)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0562)
features.4.conv.0 tensor(0.0365)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.0933)
features.5.conv.0 tensor(0.0431)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0972)
features.6.conv.0 tensor(0.0254)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0626)
features.7.conv.0 tensor(0.0490)
features.7.conv.3 tensor(0.1270)
features.7.conv.6 tensor(0.0983)
features.8.conv.0 tensor(0.0542)
features.8.conv.3 tensor(0.1461)
features.8.conv.6 tensor(0.1081)
features.9.conv.0 tensor(0.0751)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.1110)
features.10.conv.0 tensor(0.0387)
features.10.conv.3 tensor(0.1079)
features.10.conv.6 tensor(0.0788)
features.11.conv.0 tensor(0.1612)
features.11.conv.3 tensor(0.1138)
features.11.conv.6 tensor(0.1720)
features.12.conv.0 tensor(0.1451)
features.12.conv.3 tensor(0.1613)
features.12.conv.6 tensor(0.2024)
features.13.conv.0 tensor(0.0688)
features.13.conv.3 tensor(0.1730)
features.13.conv.6 tensor(0.0801)
features.14.conv.0 tensor(0.8706)
features.14.conv.3 tensor(0.1095)
features.14.conv.6 tensor(0.5127)
features.15.conv.0 tensor(0.9144)
features.15.conv.3 tensor(0.0839)
features.15.conv.6 tensor(0.8826)
features.16.conv.0 tensor(0.0590)
features.16.conv.3 tensor(0.1101)
features.16.conv.6 tensor(0.1868)
conv.0 tensor(0.1828)
tensor(705848.) 2188896.0
0.87378019
0.87322587
0.87297326
0.87308186
0.87316781
0.87323850
0.87356949
0.87424183
0.87405425
0.87390089
0.87397438
0.87388945
0.87365252
0.87357074
0.87330776
0.87296122
0.87254763
0.87264341
INFO - Training [15][   20/  196]   Loss 0.536312   Top1 81.699219   Top5 97.382812   BatchTime 0.397853   LR 0.002120
0.87253660
0.87252092
0.87259686
0.87243038
0.87255961
0.87254876
0.87224555
0.87212837
0.87210411
0.87181926
0.87128174
0.87093693
0.87068367
0.86962140
0.86939085
0.86917609
0.86923569
0.86916643
0.86927146
INFO - Training [15][   40/  196]   Loss 0.517680   Top1 82.197266   Top5 97.666016   BatchTime 0.359290   LR 0.002106
0.86916810
0.86897784
0.86879236
0.86849803
0.86944908
0.86927879
0.86911118
0.86911952
0.86912030
0.86877853
0.86868310
0.86868840
0.86835337
0.86815912
0.86783266
0.86743826
0.86704701
0.86662388
0.86623615
0.86576551
0.86507905
0.86412293
0.86344105
INFO - Training [15][   60/  196]   Loss 0.511565   Top1 82.506510   Top5 97.825521   BatchTime 0.356207   LR 0.002091
0.86284977
0.86245662
0.86150962
0.86071545
0.86009365
0.85959905
0.85969979
0.85984659
0.85936266
0.85863227
0.85829580
0.85804588
0.85795873
0.85801178
0.85797340
0.85784024
0.85761267
0.85711581
INFO - Training [15][   80/  196]   Loss 0.508394   Top1 82.587891   Top5 97.978516   BatchTime 0.351662   LR 0.002076
0.85650128
0.85604924
0.85552800
0.85518545
0.85479397
0.85469311
0.85439146
0.85439646
0.85392004
0.85351670
0.85326767
0.85289025
0.85243732
0.85220224
0.85202581
0.85171098
0.85110480
0.85017860
0.84953338
0.84873211
0.84838849
0.84805912
0.84785599
INFO - Training [15][  100/  196]   Loss 0.501694   Top1 82.824219   Top5 98.054688   BatchTime 0.350391   LR 0.002061
0.84749943
0.84722793
0.84691668
0.84675425
0.84660739
0.84665579
0.84662110
0.84665728
0.84696043
0.84713715
0.84710675
0.84697515
0.84686738
0.84673494
0.84652048
0.84662288
0.84691215
0.84741789
0.84827602
0.84912872
0.85032690
INFO - Training [15][  120/  196]   Loss 0.491975   Top1 83.134766   Top5 98.193359   BatchTime 0.357015   LR 0.002045
0.85162538
0.85274774
0.85403043
0.85528308
0.85688704
0.85943836
0.86025012
0.86101931
0.86155134
0.86192161
0.86225456
0.86269337
0.86305302
0.86310637
0.86317497
0.86307937
INFO - Training [15][  140/  196]   Loss 0.493701   Top1 83.113839   Top5 98.239397   BatchTime 0.359490   LR 0.002030
0.86303890
0.86324859
0.86329317
0.86332279
0.86312151
0.86308265
0.86256838
0.86246926
0.86212498
0.86136895
0.86132360
0.86102396
0.86077791
0.86058861
0.86016005
0.85978323
0.85920787
0.85860670
0.85818595
0.85765505
0.85708719
INFO - Training [15][  160/  196]   Loss 0.495259   Top1 83.100586   Top5 98.208008   BatchTime 0.361188   LR 0.002014
0.85664421
0.85630822
0.85622293
0.85589367
0.85544342
0.85513669
0.85482079
0.85474151
0.85418838
0.85396290
0.85427713
0.85418969
0.85395515
0.85389906
0.85347438
0.85341269
0.85314566
0.85245985
0.85203093
0.85175115
0.85148013
0.85094738
INFO - Training [15][  180/  196]   Loss 0.496148   Top1 83.012153   Top5 98.155382   BatchTime 0.362045   LR 0.001998
0.85079765
0.85070515
0.84978902
0.84938091
0.84917092
0.84906906
0.84906209
0.84926379
0.84930879
0.84924394
0.84946209
0.84935182
0.84940171
0.84958225
0.84960490
********************pre-trained*****************
INFO - ==> Top1: 83.058    Top5: 98.168    Loss: 0.494
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [15][   20/   40]   Loss 0.400690   Top1 86.484375   Top5 99.296875   BatchTime 0.147950
INFO - Validation [15][   40/   40]   Loss 0.396730   Top1 86.530000   Top5 99.460000   BatchTime 0.112657
INFO - ==> Top1: 86.530    Top5: 99.460    Loss: 0.397
INFO - ==> Sparsity : 0.356
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 86.980   Top5: 99.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0699)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0793)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0571)
features.3.conv.6 tensor(0.0573)
features.4.conv.0 tensor(0.0316)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0923)
features.5.conv.0 tensor(0.0303)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.0978)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0613)
features.7.conv.0 tensor(0.0533)
features.7.conv.3 tensor(0.1273)
features.7.conv.6 tensor(0.0973)
features.8.conv.0 tensor(0.0581)
features.8.conv.3 tensor(0.1403)
features.8.conv.6 tensor(0.1050)
features.9.conv.0 tensor(0.0715)
features.9.conv.3 tensor(0.1583)
features.9.conv.6 tensor(0.1107)
features.10.conv.0 tensor(0.0451)
features.10.conv.3 tensor(0.1094)
features.10.conv.6 tensor(0.0775)
features.11.conv.0 tensor(0.1480)
features.11.conv.3 tensor(0.1115)
features.11.conv.6 tensor(0.1790)
features.12.conv.0 tensor(0.1533)
features.12.conv.3 tensor(0.1680)
features.12.conv.6 tensor(0.2193)
features.13.conv.0 tensor(0.0677)
features.13.conv.3 tensor(0.1723)
features.13.conv.6 tensor(0.0785)
features.14.conv.0 tensor(0.8814)
features.14.conv.3 tensor(0.1083)
features.14.conv.6 tensor(0.8545)
features.15.conv.0 tensor(0.9153)
features.15.conv.3 tensor(0.0839)
features.15.conv.6 tensor(0.8705)
features.16.conv.0 tensor(0.0811)
features.16.conv.3 tensor(0.1093)
features.16.conv.6 tensor(0.1991)
conv.0 tensor(0.2151)
tensor(779441.) 2188896.0
0.84954482
0.84935802
0.84938073
0.84929633
0.84914416
0.84913498
0.84922761
0.84923834
0.84894377
0.84835249
0.84804791
0.84820980
0.84840345
0.84895051
0.84976196
0.84959656
0.84973735
0.84980446
INFO - Training [16][   20/  196]   Loss 0.487281   Top1 82.792969   Top5 97.929688   BatchTime 0.400006   LR 0.001969
0.84957021
0.84915227
0.84916580
0.84937668
0.84924346
0.84921134
0.84915715
0.84917361
0.84895921
0.84878021
0.84868860
0.84871417
0.84872365
0.84870940
0.84860420
0.84865755
0.84847486
0.84837669
0.84835142
INFO - Training [16][   40/  196]   Loss 0.489877   Top1 82.919922   Top5 98.017578   BatchTime 0.358788   LR 0.001953
0.84809774
0.84782392
0.84770828
0.84765357
0.84739876
0.84702909
0.84691608
0.84664941
0.84643668
0.84638470
0.84637678
0.84602553
0.84577650
0.84599435
0.84590596
0.84551316
0.84552997
0.84552753
0.84590954
0.84599543
0.84625781
0.84661210
0.84695870
INFO - Training [16][   60/  196]   Loss 0.481508   Top1 83.346354   Top5 98.105469   BatchTime 0.357030   LR 0.001936
0.84883368
0.84920943
0.84917098
0.84940153
0.84984428
0.85011345
0.85047698
0.85057116
0.85026121
0.85002065
0.85009122
0.85022074
0.85029197
0.85018957
0.85015422
0.84993982
INFO - Training [16][   80/  196]   Loss 0.480596   Top1 83.540039   Top5 98.203125   BatchTime 0.357727   LR 0.001919
0.84977365
0.84988922
0.84990376
0.84954518
0.84936106
0.84908921
0.84868127
0.84856743
0.84868413
0.84856069
0.84822708
0.84725672
0.84742683
0.84749085
0.84750348
0.84855193
0.84830081
0.84807533
0.84796935
0.84804052
0.84751135
0.84743732
0.84804308
INFO - Training [16][  100/  196]   Loss 0.477263   Top1 83.652344   Top5 98.187500   BatchTime 0.358285   LR 0.001902
0.84836012
0.84813589
0.84823835
0.84846056
0.84856278
0.84875578
0.85067898
0.85107911
0.85108769
0.85113132
0.85119998
0.85126191
0.85076725
0.85071349
0.85088617
0.85074514
0.85071105
0.85068220
0.85077047
0.85073060
0.85037625
0.85029757
INFO - Training [16][  120/  196]   Loss 0.476021   Top1 83.740234   Top5 98.255208   BatchTime 0.357797   LR 0.001885
0.85080856
0.85211235
0.85266525
0.85302931
0.85390037
0.85415828
0.85391980
0.85374504
0.85372365
0.85380960
0.85412610
0.85443616
0.85472161
0.85466695
0.85449362
0.85463697
0.85625589
INFO - Training [16][  140/  196]   Loss 0.473126   Top1 83.864397   Top5 98.311942   BatchTime 0.358787   LR 0.001867
0.85611755
0.85611528
0.85639977
0.85616362
0.85612315
0.85587865
0.85526448
0.85477310
0.85440153
0.85410410
0.85382921
0.85351777
0.85299879
0.85239106
0.85208213
0.85171956
0.85114771
0.85078877
0.85034931
0.85015070
0.84968835
0.84906685
INFO - Training [16][  160/  196]   Loss 0.477531   Top1 83.696289   Top5 98.254395   BatchTime 0.358934   LR 0.001850
0.84862250
0.84858376
0.84854138
0.84834951
0.84835982
0.84833562
0.84800416
0.84766442
0.84765154
0.84765905
0.84765220
0.84770787
0.84802526
0.84853929
0.84858716
0.84851176
0.84863997
0.84836030
0.84832096
0.84849131
0.84880120
0.84905118
INFO - Training [16][  180/  196]   Loss 0.477336   Top1 83.691406   Top5 98.209635   BatchTime 0.360094   LR 0.001832
0.84915918
0.84908533
0.84912354
0.84946454
0.84944421
0.84912109
0.84910285
0.84905714
0.84913802
0.84899575
0.84909201
0.84924591
0.84931874
0.84864527
********************pre-trained*****************
INFO - ==> Top1: 83.760    Top5: 98.228    Loss: 0.474
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.433025   Top1 85.195312   Top5 99.218750   BatchTime 0.135339
features.0.conv.0 tensor(0.5208)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0677)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0787)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0586)
features.3.conv.6 tensor(0.0540)
features.4.conv.0 tensor(0.0324)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0920)
features.5.conv.0 tensor(0.0353)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0970)
features.6.conv.0 tensor(0.0296)
features.6.conv.3 tensor(0.0475)
features.6.conv.6
INFO - Validation [16][   40/   40]   Loss 0.422035   Top1 85.460000   Top5 99.340000   BatchTime 0.118411
INFO - ==> Top1: 85.460    Top5: 99.340    Loss: 0.422
INFO - ==> Sparsity : 0.366
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 86.980   Top5: 99.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
features.6.conv.6 tensor(0.0596)
features.7.conv.0 tensor(0.0621)
features.7.conv.3 tensor(0.1253)
features.7.conv.6 tensor(0.0966)
features.8.conv.0 tensor(0.0568)
features.8.conv.3 tensor(0.1415)
features.8.conv.6 tensor(0.1056)
features.9.conv.0 tensor(0.0817)
features.9.conv.3 tensor(0.1554)
features.9.conv.6 tensor(0.1095)
features.10.conv.0 tensor(0.0393)
features.10.conv.3 tensor(0.1102)
features.10.conv.6 tensor(0.0791)
features.11.conv.0 tensor(0.1401)
features.11.conv.3 tensor(0.1150)
features.11.conv.6 tensor(0.1703)
features.12.conv.0 tensor(0.1574)
features.12.conv.3 tensor(0.1651)
features.12.conv.6 tensor(0.2183)
features.13.conv.0 tensor(0.0711)
features.13.conv.3 tensor(0.1740)
features.13.conv.6 tensor(0.0803)
features.14.conv.0 tensor(0.8899)
features.14.conv.3 tensor(0.1104)
features.14.conv.6 tensor(0.8602)
features.15.conv.0 tensor(0.9253)
features.15.conv.3 tensor(0.0841)
features.15.conv.6 tensor(0.9521)
features.16.conv.0 tensor(0.0799)
features.16.conv.3 tensor(0.1105)
features.16.conv.6 tensor(0.2022)
conv.0 tensor(0.2281)
tensor(801768.) 2188896.0
0.84847003
0.84861803
0.84875339
0.84861255
0.84847248
0.84834808
0.84843314
0.84837967
0.84836984
0.84828436
0.84853983
0.84872979
0.84885687
0.84904820
0.84915328
0.84913963
0.84917736
INFO - Training [17][   20/  196]   Loss 0.481383   Top1 83.359375   Top5 97.832031   BatchTime 0.427642   LR 0.001800
0.84933996
0.84964496
0.84998006
0.85010183
0.85000443
0.84976536
0.84964877
0.84978610
0.84972399
0.84972763
0.84965658
0.84971726
0.84969491
0.84976226
0.84998488
0.84989524
0.84971786
0.84939897
0.84934282
0.84915882
INFO - Training [17][   40/  196]   Loss 0.478360   Top1 83.769531   Top5 98.017578   BatchTime 0.363652   LR 0.001782
0.84917110
0.84932190
0.84960222
0.84920609
0.84935355
0.84956282
0.84971696
0.84997314
0.85004717
0.84996557
0.84971762
0.84920663
0.84887582
0.84857768
0.84816086
0.84794843
0.84765142
0.84733957
0.84716564
0.84694362
0.84620720
0.84582937
0.84518677
INFO - Training [17][   60/  196]   Loss 0.476554   Top1 83.730469   Top5 98.059896   BatchTime 0.362426   LR 0.001764
0.84517616
0.84486252
0.84468091
0.84512287
0.84564352
0.84619862
0.84648079
0.84666234
0.84653735
0.84599805
0.84561765
0.84535915
0.84615624
0.84738618
0.84730518
0.84731728
0.84710562
0.84682035
0.84649950
0.84621567
0.84610480
0.84610289
INFO - Training [17][   80/  196]   Loss 0.469652   Top1 83.828125   Top5 98.232422   BatchTime 0.362619   LR 0.001746
0.84612972
0.84595609
0.84557176
0.84535360
0.84527767
0.84506184
0.84499991
0.84480613
0.84477431
0.84493524
0.84646815
0.84637737
0.84657162
0.84707677
0.84715557
0.84744775
INFO - Training [17][  100/  196]   Loss 0.466944   Top1 83.937500   Top5 98.261719   BatchTime 0.362904   LR 0.001727
0.84753174
0.84733039
0.84735709
0.84766054
0.84760767
0.84781802
0.84834015
0.84876543
0.84905714
0.84917444
0.84927100
0.84920788
0.84910107
0.84893876
0.84881896
0.84866405
0.84812635
0.84803671
0.84787130
0.84781444
0.84785026
INFO - Training [17][  120/  196]   Loss 0.461034   Top1 84.088542   Top5 98.369141   BatchTime 0.366748   LR 0.001708
0.84789103
0.84799737
0.84830242
0.84826916
0.84845406
0.84867173
0.84864819
0.84842044
0.84831464
0.84828436
0.84834355
0.84858567
0.84885800
0.84915638
0.84919506
0.84953058
0.84956026
0.84983599
0.85000879
0.84995514
0.84997529
0.84987223
INFO - Training [17][  140/  196]   Loss 0.462921   Top1 83.992746   Top5 98.381696   BatchTime 0.367484   LR 0.001690
0.84984660
0.84986502
0.84987319
0.84987968
0.84963852
0.84923875
0.84907198
0.84909886
0.84915531
0.84914136
0.84885502
0.84887350
0.84893626
0.84897232
0.84908020
0.84867877
INFO - Training [17][  160/  196]   Loss 0.465081   Top1 83.945312   Top5 98.342285   BatchTime 0.367651   LR 0.001671
0.84855705
0.84833425
0.84816277
0.84802902
0.84808230
0.84800607
0.84781069
0.84768540
0.84765601
0.84765154
0.84751803
0.84752488
0.84738964
0.84736949
0.84738779
0.84753078
0.84772527
0.84684765
0.84648252
0.84587669
0.84574330
0.84541881
INFO - Training [17][  180/  196]   Loss 0.463513   Top1 83.973524   Top5 98.279080   BatchTime 0.366768   LR 0.001652
0.84516948
0.84467465
0.84452206
0.84380692
0.84267133
0.84060246
0.83860910
0.83749938
0.83958423
0.84011459
0.83940887
0.83939832
0.83898050
0.83976144
0.83635223
0.83158416
0.83011204
INFO - ==> Top1: 84.104    Top5: 98.288    Loss: 0.460
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [17][   20/   40]   Loss 0.368755   Top1 87.851562   Top5 99.433594   BatchTime 0.158627
INFO - Validation [17][   40/   40]   Loss 0.361171   Top1 88.160000   Top5 99.550000   BatchTime 0.124747
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0629)
features.2.conv.0 tensor(0.0234)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0755)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0553)
features.4.conv.0 tensor(0.0231)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0946)
features.5.conv.0 tensor(0.0373)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0934)
features.6.conv.0 tensor(0.0228)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0566)
features.7.conv.0 tensor(0.0599)
features.7.conv.3 tensor(0.1256)
features.7.conv.6 tensor(0.0962)
features.8.conv.0 tensor(0.0420)
features.8.conv.3 tensor(0.1441)
features.8.conv.6 tensor(0.1003)
features.9.conv.0 tensor(0.0779)
features.9.conv.3 tensor(0.1548)
features.9.conv.6 tensor(0.1072)
features.10.conv.0 tensor(0.0389)
features.10.conv.3 tensor(0.1091)
features.10.conv.6 tensor(0.0803)
features.11.conv.0 tensor(0.1518)
features.11.conv.3 tensor(0.1161)
features.11.conv.6 tensor(0.1678)
features.12.conv.0 tensor(0.1479)
features.12.conv.3 tensor(0.1659)
features.12.conv.6 tensor(0.2210)
features.13.conv.0 tensor(0.0716)
features.13.conv.3 tensor(0.1763)
features.13.conv.6 tensor(0.0875)
features.14.conv.0 tensor(0.8926)
features.14.conv.3 tensor(0.1148)
features.14.conv.6 tensor(0.8695)
features.15.conv.0 tensor(0.9237)
features.15.conv.3 tensor(0.0868)
features.15.conv.6 tensor(0.9122)
features.16.conv.0 tensor(0.0736)
features.16.conv.3 tensor(0.1115)
features.16.conv.6 tensor(0.6504)
conv.0 tensor(0.2293)
tensor(934572.) 2188896.0
INFO - ==> Top1: 88.160    Top5: 99.550    Loss: 0.361
INFO - ==> Sparsity : 0.427
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 88.160   Top5: 99.550]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.83030093
0.82416105
0.83201039
0.84248209
0.84910440
0.85062408
0.85121965
0.85184896
0.85249388
0.85303873
0.85351706
0.85391873
0.85440457
0.85485125
0.85482937
0.85501111
0.85512704
0.85483718
INFO - Training [18][   20/  196]   Loss 0.468149   Top1 83.203125   Top5 97.968750   BatchTime 0.394667   LR 0.001618
0.85363066
0.85308295
0.85289216
0.85276586
0.85365444
0.85397643
0.85395485
0.85391939
0.85383368
0.85368145
0.85338402
0.85295409
0.85290474
0.85247880
0.85203153
0.85148388
0.85107547
0.85051513
0.85029519
0.85002291
0.84980839
0.84963590
INFO - Training [18][   40/  196]   Loss 0.447201   Top1 84.179688   Top5 98.242188   BatchTime 0.376462   LR 0.001599
0.84947681
0.84951270
0.84946048
0.84942818
0.84907007
0.84905678
0.84892738
0.84874421
0.84869158
0.84848326
0.84800589
0.84805220
0.84806049
0.84791869
0.84836847
0.84787190
0.84785956
INFO - Training [18][   60/  196]   Loss 0.451095   Top1 84.166667   Top5 98.281250   BatchTime 0.370488   LR 0.001579
0.84793812
0.84797937
0.84799057
0.84806556
0.84795427
0.84809428
0.84811896
0.84810120
0.84791213
0.84795064
0.84870696
0.84914231
0.84946394
0.84930819
0.84938836
0.84914351
0.84918177
0.84913045
0.84908062
0.84920502
0.84895092
0.84904248
INFO - Training [18][   80/  196]   Loss 0.449842   Top1 84.243164   Top5 98.339844   BatchTime 0.370072   LR 0.001560
0.84904778
0.84923065
0.84946191
0.84963882
0.84953922
0.84964460
0.84987909
0.85013372
0.85022545
0.85006583
0.84975809
0.84911352
0.85018504
0.84998298
0.85001391
0.85010105
0.84973991
INFO - Training [18][  100/  196]   Loss 0.445164   Top1 84.464844   Top5 98.351562   BatchTime 0.365200   LR 0.001540
0.84963191
0.84945834
0.84924138
0.84905511
0.84902984
0.84906179
0.84906197
0.84905028
0.84924495
0.84927708
0.84958267
0.84956843
0.84929568
0.84919626
0.84927571
0.84942681
0.84971964
0.84997255
0.85003817
0.85008007
0.85002869
0.85030025
0.85032707
INFO - Training [18][  120/  196]   Loss 0.440509   Top1 84.654948   Top5 98.421224   BatchTime 0.363659   LR 0.001521
0.85039747
0.85037577
0.85055882
0.85057473
0.85064095
0.85056466
0.85058641
0.85044336
0.85028702
0.85022736
0.85026574
0.85024959
0.85016507
0.84995651
0.84959006
0.84973639
0.84985608
0.84962761
0.84940875
0.84936422
0.84920561
0.84917760
INFO - Training [18][  140/  196]   Loss 0.437943   Top1 84.790737   Top5 98.496094   BatchTime 0.364430   LR 0.001501
0.84910482
0.84891927
0.84892768
0.84909809
0.84910434
0.84920299
0.84932339
0.84919649
0.84927553
0.84966224
0.84945107
0.84929794
0.84897596
0.84834123
0.84816104
0.84811741
0.84780318
0.84766495
INFO - Training [18][  160/  196]   Loss 0.442738   Top1 84.572754   Top5 98.486328   BatchTime 0.360820   LR 0.001482
0.84735948
0.84721667
0.84724116
0.84737432
0.84699446
0.84706318
0.84710985
0.84716839
0.84708154
0.84738022
0.84725279
0.84706664
0.84709650
0.84718078
0.84729636
0.84730709
0.84712851
INFO - Training [18][  180/  196]   Loss 0.444502   Top1 84.492188   Top5 98.409288   BatchTime 0.358384   LR 0.001462
0.84705412
0.84756392
0.84744340
0.84750462
0.84737474
0.84741831
0.84762001
0.84718889
0.84732485
0.84736067
0.84709585
0.84708339
0.84692788
0.84674126
0.84666938
0.84703976
0.84752584
INFO - ==> Top1: 84.458    Top5: 98.398    Loss: 0.444
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84737825
0.84709507
0.84674555
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 0.387756   Top1 87.343750   Top5 99.394531   BatchTime 0.146078
INFO - Validation [18][   40/   40]   Loss 0.382255   Top1 87.190000   Top5 99.550000   BatchTime 0.099177
INFO - ==> Top1: 87.190    Top5: 99.550    Loss: 0.382
INFO - ==> Sparsity : 0.371
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 88.160   Top5: 99.550]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0188)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0712)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0545)
features.4.conv.0 tensor(0.0293)
features.4.conv.3 tensor(0.1001)
features.4.conv.6 tensor(0.0920)
features.5.conv.0 tensor(0.0347)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.0942)
features.6.conv.0 tensor(0.0262)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0569)
features.7.conv.0 tensor(0.0541)
features.7.conv.3 tensor(0.1311)
features.7.conv.6 tensor(0.0920)
features.8.conv.0 tensor(0.0592)
features.8.conv.3 tensor(0.1505)
features.8.conv.6 tensor(0.1016)
features.9.conv.0 tensor(0.0793)
features.9.conv.3 tensor(0.1562)
features.9.conv.6 tensor(0.1029)
features.10.conv.0 tensor(0.0331)
features.10.conv.3 tensor(0.1131)
features.10.conv.6 tensor(0.0797)
features.11.conv.0 tensor(0.1418)
features.11.conv.3 tensor(0.1204)
features.11.conv.6 tensor(0.1891)
features.12.conv.0 tensor(0.1529)
features.12.conv.3 tensor(0.1647)
features.12.conv.6 tensor(0.2112)
features.13.conv.0 tensor(0.0691)
features.13.conv.3 tensor(0.1775)
features.13.conv.6 tensor(0.0807)
features.14.conv.0 tensor(0.9009)
features.14.conv.3 tensor(0.1146)
features.14.conv.6 tensor(0.8596)
features.15.conv.0 tensor(0.9335)
features.15.conv.3 tensor(0.0872)
features.15.conv.6 tensor(0.9534)
features.16.conv.0 tensor(0.0788)
features.16.conv.3 tensor(0.1093)
features.16.conv.6 tensor(0.2064)
conv.0 tensor(0.2452)
tensor(812691.) 2188896.0
0.84667820
0.84670085
0.84667790
0.84686750
0.84667373
0.84676129
0.84658682
0.84692341
0.84667832
0.84668112
0.84655762
0.84644467
0.84607381
0.84536588
0.84496170
0.84478813
0.84424770
0.84414726
0.84420985
INFO - Training [19][   20/  196]   Loss 0.453529   Top1 84.082031   Top5 97.929688   BatchTime 0.393550   LR 0.001427
0.84442967
0.84436274
0.84467220
0.84467685
0.84478712
0.84509939
0.84527028
0.84523565
0.84551245
0.84578359
0.84586614
0.84572464
0.84627962
0.84705609
0.84714168
0.84703040
0.84686136
0.84721452
0.84706563
0.84689850
0.84691310
0.84692150
INFO - Training [19][   40/  196]   Loss 0.449876   Top1 84.355469   Top5 98.046875   BatchTime 0.377175   LR 0.001407
0.84706938
0.84707707
0.84716386
0.84754306
0.84797400
0.84798849
0.84832942
0.84839272
0.84769619
0.84725207
0.84637791
0.84645957
0.84724373
0.84662676
0.84596395
0.84500593
0.84437138
INFO - Training [19][   60/  196]   Loss 0.441564   Top1 84.576823   Top5 98.216146   BatchTime 0.366403   LR 0.001387
0.84411544
0.84340030
0.84245163
0.84011930
0.83599550
0.84006399
0.84152925
0.84282500
0.84274822
0.84307283
0.84325677
0.84302264
0.84253246
0.84221268
0.84136939
0.84037870
0.84011853
0.83982342
0.83927268
0.83923107
0.83807421
0.83561689
0.83382833
INFO - Training [19][   80/  196]   Loss 0.439745   Top1 84.663086   Top5 98.334961   BatchTime 0.363525   LR 0.001367
0.83242500
0.83191705
0.83057839
0.82827747
0.82795984
0.82702607
0.82637060
0.82792896
0.82984740
0.83143741
0.83334887
0.83552939
0.83682144
0.83761668
0.83882308
0.84016418
0.84158516
INFO - Training [19][  100/  196]   Loss 0.434966   Top1 84.777344   Top5 98.394531   BatchTime 0.363010   LR 0.001347
0.84283412
0.84381974
0.84478682
0.84549075
0.84670162
0.84806764
0.84984970
0.85015637
0.85005665
0.85005474
0.85028005
0.85080832
0.85099858
0.85086077
0.85067511
0.85065335
0.85048473
0.85026318
0.85039294
0.85018551
0.84985918
0.84967154
INFO - Training [19][  120/  196]   Loss 0.430702   Top1 85.009766   Top5 98.486328   BatchTime 0.363024   LR 0.001327
0.84943336
0.84955293
0.84973013
0.84981531
0.84975284
0.84965760
0.84973681
0.84984756
0.84991920
0.84976077
0.84951293
0.84937751
0.84922433
0.84916663
0.84904563
0.84867287
INFO - Training [19][  140/  196]   Loss 0.428410   Top1 85.128348   Top5 98.526786   BatchTime 0.364036   LR 0.001307
0.84836411
0.84820592
0.84814554
0.84787983
0.84752250
0.84753782
0.84762657
0.84764445
0.84746146
0.84751737
0.84738523
0.84752935
0.84749711
0.84755337
0.84751356
0.84733593
0.84771997
0.84795415
0.84754455
0.84749782
0.84704500
0.84697622
INFO - Training [19][  160/  196]   Loss 0.431624   Top1 85.097656   Top5 98.491211   BatchTime 0.364109   LR 0.001287
0.84708583
0.84714282
0.84709102
0.84707868
0.84719163
0.84753394
0.84722292
0.84722614
0.84720683
0.84724981
0.84721690
0.84721357
0.84730667
0.84706593
0.84678233
0.84683698
0.84685171
0.84682775
0.84668696
0.84651613
0.84655595
0.84658343
INFO - Training [19][  180/  196]   Loss 0.432241   Top1 85.084635   Top5 98.465712   BatchTime 0.364011   LR 0.001266
0.84641725
0.84656107
0.84659708
0.84651613
0.84660727
0.84684336
0.84698641
0.84684616
0.84672409
0.84700274
0.84694958
0.84683329
0.84651995
0.84661508
0.84675163
0.84690005
INFO - ==> Top1: 85.106    Top5: 98.464    Loss: 0.432
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [19][   20/   40]   Loss 0.729318   Top1 76.171875   Top5 98.105469   BatchTime 0.130958
INFO - Validation [19][   40/   40]   Loss 0.723332   Top1 76.170000   Top5 98.260000   BatchTime 0.090826
INFO - ==> Top1: 76.170    Top5: 98.260    Loss: 0.723
INFO - ==> Sparsity : 0.370
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 88.160   Top5: 99.550]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 87.680   Top5: 99.530]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4826)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0273)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0595)
features.2.conv.0 tensor(0.0203)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0790)
features.3.conv.0 tensor(0.0246)
features.3.conv.3 tensor(0.0548)
features.3.conv.6 tensor(0.0551)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.0990)
features.4.conv.6 tensor(0.0920)
features.5.conv.0 tensor(0.0286)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.0934)
features.6.conv.0 tensor(0.0286)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0576)
features.7.conv.0 tensor(0.0584)
features.7.conv.3 tensor(0.1293)
features.7.conv.6 tensor(0.0923)
features.8.conv.0 tensor(0.0529)
features.8.conv.3 tensor(0.1432)
features.8.conv.6 tensor(0.0970)
features.9.conv.0 tensor(0.0820)
features.9.conv.3 tensor(0.1560)
features.9.conv.6 tensor(0.0993)
features.10.conv.0 tensor(0.0334)
features.10.conv.3 tensor(0.1088)
features.10.conv.6 tensor(0.0769)
features.11.conv.0 tensor(0.1590)
features.11.conv.3 tensor(0.1181)
features.11.conv.6 tensor(0.1836)
features.12.conv.0 tensor(0.1570)
features.12.conv.3 tensor(0.1632)
features.12.conv.6 tensor(0.2264)
features.13.conv.0 tensor(0.0715)
features.13.conv.3 tensor(0.1744)
features.13.conv.6 tensor(0.0840)
features.14.conv.0 tensor(0.9013)
features.14.conv.3 tensor(0.1111)
features.14.conv.6 tensor(0.8648)
features.15.conv.0 tensor(0.9348)
features.15.conv.3 tensor(0.0865)
features.15.conv.6 tensor(0.9502)
features.16.conv.0 tensor(0.0789)
features.16.conv.3 tensor(0.1105)
features.16.conv.6 tensor(0.2096)
conv.0 tensor(0.2324)
tensor(810837.) 2188896.0
0.84671479
0.84655529
0.84627861
0.84629303
0.84654450
0.84647214
0.84629506
0.84625953
0.84620744
0.84620368
0.84623176
0.84612018
0.84634584
0.84615314
0.84629679
0.84612030
0.84636170
0.84616059
0.84609687
0.84583765
0.84553850
INFO - Training [20][   20/  196]   Loss 0.419560   Top1 84.960938   Top5 97.871094   BatchTime 0.379197   LR 0.001231
0.84533733
0.84523255
0.84511864
0.84493250
0.84500366
0.84533679
0.84553236
0.84540325
0.84538704
0.84534049
0.84525895
0.84535366
0.84542108
0.84534514
0.84558994
0.84586734
INFO - Training [20][   40/  196]   Loss 0.430816   Top1 84.794922   Top5 98.222656   BatchTime 0.375649   LR 0.001211
0.84592849
0.84618342
0.84683836
0.84754807
0.84855098
0.84873807
0.84876400
0.84896010
0.84911495
0.84881341
0.84910035
0.84904826
0.84910131
0.84918034
0.84933811
0.84935886
0.84946436
0.84952682
0.84934604
0.84928465
0.84940398
INFO - Training [20][   60/  196]   Loss 0.426828   Top1 85.006510   Top5 98.287760   BatchTime 0.372301   LR 0.001191
0.84945476
0.84959924
0.84928983
0.84911883
0.84920764
0.84934330
0.84930778
0.84921139
0.84919298
0.84919912
0.84892279
0.84878206
0.84864444
0.84845847
0.84827060
0.84788209
0.84741098
0.84744108
0.84712803
0.84687650
0.84672022
0.84587973
0.84612823
INFO - Training [20][   80/  196]   Loss 0.422515   Top1 85.224609   Top5 98.378906   BatchTime 0.369786   LR 0.001171
0.84622520
0.84584087
0.84499943
0.84481651
0.84545821
0.84603947
0.84795213
0.84780121
0.84748828
0.84739649
0.84741884
0.84736496
0.84728467
0.84739262
0.84740955
0.84739697
0.84764731
INFO - Training [20][  100/  196]   Loss 0.417485   Top1 85.386719   Top5 98.453125   BatchTime 0.364071   LR 0.001151
0.84801048
0.84802204
0.84806401
0.84785110
0.84764248
0.84756017
0.84740692
0.84731793
0.84720922
0.84717739
0.84712332
0.84721226
0.84754896
0.84723127
0.84688485
0.84678173
0.84697610
0.84690243
0.84715092
0.84749967
0.84769708
0.84805357
INFO - Training [20][  120/  196]   Loss 0.412537   Top1 85.566406   Top5 98.535156   BatchTime 0.365399   LR 0.001131
0.84750390
0.84849805
0.84890956
0.84898031
0.84901261
0.84916836
0.84923971
0.84938961
0.84941262
0.84959430
0.84956890
0.84952658
0.84928805
0.84928930
0.84934562
0.84937543
0.84938389
0.84912324
0.84910500
0.84893805
0.84895486
INFO - Training [20][  140/  196]   Loss 0.409740   Top1 85.728237   Top5 98.579799   BatchTime 0.368342   LR 0.001111
0.84915978
0.84913850
0.84889656
0.84889424
0.84889299
0.84891778
0.84897017
0.84875643
0.84852242
0.84812427
0.84787661
0.84759235
0.84733665
0.84666187
0.84610301
0.84524405
0.84578627
INFO - Training [20][  160/  196]   Loss 0.413436   Top1 85.615234   Top5 98.571777   BatchTime 0.366556   LR 0.001091
0.84735554
0.84691596
0.84687674
0.84673405
0.84662277
0.84661877
0.84610403
0.84605026
0.84734577
0.84684652
0.84649456
0.84622127
0.84611636
0.84535682
0.84490883
0.84432441
0.84400278
0.84378940
0.84337646
0.84278613
0.84253067
0.84206641
INFO - Training [20][  180/  196]   Loss 0.415315   Top1 85.603299   Top5 98.504774   BatchTime 0.365629   LR 0.001071
0.84162074
0.84123510
0.84050316
0.84062159
0.84062648
0.84045893
0.84042716
0.84023881
0.83999979
0.84179956
0.84147644
0.84136724
0.84079480
0.84051710
0.83995992
0.83919513
INFO - ==> Top1: 85.652    Top5: 98.514    Loss: 0.414
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 0.360866   Top1 88.144531   Top5 99.433594   BatchTime 0.129250
INFO - Validation [20][   40/   40]   Loss 0.343296   Top1 88.330000   Top5 99.600000   BatchTime 0.091836
INFO - ==> Top1: 88.330    Top5: 99.600    Loss: 0.343
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 88.330   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 88.160   Top5: 99.550]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 88.080   Top5: 99.600]
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0234)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0778)
features.3.conv.0 tensor(0.0237)
features.3.conv.3 tensor(0.0525)
features.3.conv.6 tensor(0.0538)
features.4.conv.0 tensor(0.0272)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0951)
features.5.conv.0 tensor(0.0321)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.0959)
features.6.conv.0 tensor(0.0247)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0577)
features.7.conv.0 tensor(0.0640)
features.7.conv.3 tensor(0.1264)
features.7.conv.6 tensor(0.0923)
features.8.conv.0 tensor(0.0608)
features.8.conv.3 tensor(0.1470)
features.8.conv.6 tensor(0.0976)
features.9.conv.0 tensor(0.0763)
features.9.conv.3 tensor(0.1542)
features.9.conv.6 tensor(0.1008)
features.10.conv.0 tensor(0.0349)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0770)
features.11.conv.0 tensor(0.1576)
features.11.conv.3 tensor(0.1173)
features.11.conv.6 tensor(0.4460)
features.12.conv.0 tensor(0.1518)
features.12.conv.3 tensor(0.1607)
features.12.conv.6 tensor(0.2360)
features.13.conv.0 tensor(0.0724)
features.13.conv.3 tensor(0.1771)
features.13.conv.6 tensor(0.0800)
features.14.conv.0 tensor(0.9085)
features.14.conv.3 tensor(0.1113)
features.14.conv.6 tensor(0.8666)
features.15.conv.0 tensor(0.9420)
features.15.conv.3 tensor(0.0847)
features.15.conv.6 tensor(0.9435)
features.16.conv.0 tensor(0.0804)
features.16.conv.3 tensor(0.1141)
features.16.conv.6 tensor(0.2156)
conv.0 tensor(0.2198)
tensor(823797.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
0.83838642
0.83608246
0.83370513
0.83390969
0.83426553
0.83454138
0.83512551
0.83632356
0.83804911
0.83926487
0.84038240
0.84092891
0.84177250
0.84230900
0.84337068
0.84488541
0.84746319
0.84750611
0.84736162
INFO - Training [21][   20/  196]   Loss 0.439609   Top1 84.882812   Top5 97.558594   BatchTime 0.451613   LR 0.001036
0.84761560
0.84732074
0.84697205
0.84678894
0.84702623
0.84690487
0.84700990
0.84690279
0.84696811
0.84716612
0.84707075
0.84698081
0.84682131
0.84674454
0.84693199
0.84697950
0.84652305
0.84623462
0.84641415
0.84642386
0.84630901
0.84640270
INFO - Training [21][   40/  196]   Loss 0.433289   Top1 84.970703   Top5 97.949219   BatchTime 0.408190   LR 0.001016
0.84643376
0.84649855
0.84632450
0.84653294
0.84655243
0.84641635
0.84619433
0.84605128
0.84582287
0.84574318
0.84549463
0.84531426
0.84526342
0.84539342
0.84533995
0.84533656
INFO - Training [21][   60/  196]   Loss 0.424481   Top1 85.240885   Top5 98.170573   BatchTime 0.391711   LR 0.000996
0.84526342
0.84520543
0.84542447
0.84525448
0.84523588
0.84631592
0.84658009
0.84649462
0.84640789
0.84628773
0.84613431
0.84605825
0.84608561
0.84594572
0.84578001
0.84562612
0.84540349
0.84542459
0.84549516
0.84546220
0.84549028
0.84554470
INFO - Training [21][   80/  196]   Loss 0.423387   Top1 85.112305   Top5 98.305664   BatchTime 0.385904   LR 0.000976
0.84573174
0.84577245
0.84628278
0.84636253
0.84620845
0.84587133
0.84560722
0.84551841
0.84537673
0.84519297
0.84457457
0.84444690
0.84419364
0.84442747
0.84519333
0.84578782
0.84581786
0.84550154
0.84542269
0.84540182
0.84538853
INFO - Training [21][  100/  196]   Loss 0.419813   Top1 85.257812   Top5 98.359375   BatchTime 0.386330   LR 0.000957
0.84550011
0.84589690
0.84575307
0.84535760
0.84546429
0.84564298
0.84547651
0.84564549
0.84556633
0.84512085
0.84450066
0.84448278
0.84439176
0.84447473
0.84524173
0.84514922
INFO - Training [21][  120/  196]   Loss 0.412930   Top1 85.576172   Top5 98.447266   BatchTime 0.383605   LR 0.000937
0.84506184
0.84484899
0.84486628
0.84548974
0.84558296
0.84540445
0.84554052
0.84545481
0.84564525
0.84563583
0.84535271
0.84516805
0.84528971
0.84516525
0.84493250
0.84439743
0.84381443
0.84372073
0.84375036
0.84372431
0.84395367
0.84384704
INFO - Training [21][  140/  196]   Loss 0.410670   Top1 85.672433   Top5 98.498884   BatchTime 0.380393   LR 0.000918
0.84370357
0.84353882
0.84320372
0.84297103
0.84334618
0.84326375
0.84458870
0.84467787
0.84451860
0.84454274
0.84451383
0.84446841
0.84470880
0.84493852
0.84515566
0.84528774
0.84535837
0.84511429
0.84493870
0.84481961
0.84481251
0.84474856
INFO - Training [21][  160/  196]   Loss 0.412544   Top1 85.686035   Top5 98.479004   BatchTime 0.378425   LR 0.000899
0.84478420
0.84483653
0.84478116
0.84470689
0.84455705
0.84442669
0.84427404
0.84455621
0.84417444
0.84382397
0.84388161
0.84391469
0.84396875
0.84394288
0.84390014
0.84396183
0.84384567
INFO - Training [21][  180/  196]   Loss 0.413713   Top1 85.627170   Top5 98.396267   BatchTime 0.377721   LR 0.000879
0.84383923
0.84369618
0.84353632
0.84340316
0.84346074
0.84387207
0.84410495
0.84444052
0.84434140
0.84418565
0.84428281
0.84435129
0.84418315
0.84400105
0.84367490
0.84335035
INFO - ==> Top1: 85.682    Top5: 98.408    Loss: 0.411
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84338844
0.84341085
0.84322512
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 0.355128   Top1 88.222656   Top5 99.511719   BatchTime 0.151849
INFO - Validation [21][   40/   40]   Loss 0.339659   Top1 88.440000   Top5 99.650000   BatchTime 0.136682
INFO - ==> Top1: 88.440    Top5: 99.650    Loss: 0.340
INFO - ==> Sparsity : 0.370
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 88.440   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 88.330   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 88.160   Top5: 99.550]
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0246)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0770)
features.3.conv.0 tensor(0.0249)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0527)
features.4.conv.0 tensor(0.0293)
features.4.conv.3 tensor(0.0966)
features.4.conv.6 tensor(0.0964)
features.5.conv.0 tensor(0.0340)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0923)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0565)
features.7.conv.0 tensor(0.0565)
features.7.conv.3 tensor(0.1250)
features.7.conv.6 tensor(0.0903)
features.8.conv.0 tensor(0.0557)
features.8.conv.3 tensor(0.1447)
features.8.conv.6 tensor(0.0977)
features.9.conv.0 tensor(0.0762)
features.9.conv.3 tensor(0.1562)
features.9.conv.6 tensor(0.1024)
features.10.conv.0 tensor(0.0323)
features.10.conv.3 tensor(0.1027)
features.10.conv.6 tensor(0.0766)
features.11.conv.0 tensor(0.1618)
features.11.conv.3 tensor(0.1179)
features.11.conv.6 tensor(0.1718)
features.12.conv.0 tensor(0.1585)
features.12.conv.3 tensor(0.1617)
features.12.conv.6 tensor(0.2723)
features.13.conv.0 tensor(0.0761)
features.13.conv.3 tensor(0.1784)
features.13.conv.6 tensor(0.1010)
features.14.conv.0 tensor(0.9107)
features.14.conv.3 tensor(0.1119)
features.14.conv.6 tensor(0.8682)
features.15.conv.0 tensor(0.9420)
features.15.conv.3 tensor(0.0858)
features.15.conv.6 tensor(0.9509)
features.16.conv.0 tensor(0.0981)
features.16.conv.3 tensor(0.1131)
features.16.conv.6 tensor(0.2033)
conv.0 tensor(0.2132)
tensor(810975.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.84309620
0.84337384
0.84339231
0.84329873
0.84341741
0.84363472
0.84338963
0.84344548
0.84343988
0.84359604
0.84379071
0.84375608
0.84350431
0.84343940
0.84359348
0.84314120
0.84221190
INFO - Training [22][   20/  196]   Loss 0.386756   Top1 86.523438   Top5 98.046875   BatchTime 0.438824   LR 0.000846
0.84122467
0.84099215
0.84089035
0.84093171
0.84047812
0.83980054
0.83960342
0.84024078
0.84061009
0.84095687
0.84179819
0.84229690
0.84207118
0.84172970
0.84184033
0.84212238
0.84354234
0.84368664
0.84449172
0.84486109
0.84460998
INFO - Training [22][   40/  196]   Loss 0.403310   Top1 86.054688   Top5 98.281250   BatchTime 0.406636   LR 0.000827
0.84467047
0.84433162
0.84435427
0.84435588
0.84391046
0.84371942
0.84378052
0.84363097
0.84352577
0.84388512
0.84366989
0.84359699
0.84501535
0.84514391
0.84503204
0.84503758
0.84504467
0.84499460
0.84504783
0.84487683
0.84464926
0.84477764
0.84471971
INFO - Training [22][   60/  196]   Loss 0.404460   Top1 86.093750   Top5 98.359375   BatchTime 0.390520   LR 0.000808
0.84477997
0.84488052
0.84503663
0.84521967
0.84521693
0.84497142
0.84496355
0.84463936
0.84473509
0.84470952
0.84472668
0.84470969
0.84472519
0.84466207
0.84464175
0.84454131
INFO - Training [22][   80/  196]   Loss 0.399819   Top1 86.049805   Top5 98.437500   BatchTime 0.383836   LR 0.000789
0.84457487
0.84467494
0.84460765
0.84477252
0.84459049
0.84440207
0.84404600
0.84362060
0.84370697
0.84361845
0.84345412
0.84323794
0.84325981
0.84314311
0.84309793
0.84316546
0.84309882
0.84302604
0.84298378
0.84301037
0.84292382
0.84274703
INFO - Training [22][  100/  196]   Loss 0.391904   Top1 86.281250   Top5 98.441406   BatchTime 0.380829   LR 0.000770
0.84257543
0.84261179
0.84293526
0.84267229
0.84238726
0.84221315
0.84200752
0.84193367
0.84185559
0.84173167
0.84178954
0.84193689
0.84206772
0.84213918
0.84194970
0.84200501
0.84211165
0.84212995
0.84202534
0.84201509
0.84148926
0.84226596
INFO - Training [22][  120/  196]   Loss 0.389092   Top1 86.468099   Top5 98.502604   BatchTime 0.378012   LR 0.000752
0.84261179
0.84259653
0.84260029
0.84251887
0.84240532
0.84221208
0.84209251
0.84176236
0.84156936
0.84150225
0.84129274
0.84118384
0.84092230
0.84094858
0.84083354
0.84060085
0.84077150
INFO - Training [22][  140/  196]   Loss 0.389828   Top1 86.411830   Top5 98.554688   BatchTime 0.375915   LR 0.000734
0.84079522
0.84187776
0.84253490
0.84271204
0.84260011
0.84262645
0.84247547
0.84251642
0.84291840
0.84300333
0.84324026
0.84342068
0.84463400
0.84519511
0.84509647
0.84504080
0.84504431
0.84485966
0.84482437
0.84491158
0.84482342
INFO - Training [22][  160/  196]   Loss 0.392752   Top1 86.291504   Top5 98.535156   BatchTime 0.375610   LR 0.000715
0.84479266
0.84458870
0.84447378
0.84500760
0.84472567
0.84423375
0.84381038
0.84392446
0.84395874
0.84376276
0.84379882
0.84386176
0.84387374
0.84373581
0.84345853
0.84349430
0.84319162
INFO - Training [22][  180/  196]   Loss 0.393992   Top1 86.197917   Top5 98.487413   BatchTime 0.374036   LR 0.000697
0.84297842
0.84280932
0.84294546
0.84320462
0.84312475
0.84290463
0.84293830
0.84295452
0.84299117
0.84277302
0.84253097
0.84234554
0.84230959
0.84238392
0.84237057
0.84258592
0.84251136
0.84239793
0.84242839
0.84261620
INFO - ==> Top1: 86.254    Top5: 98.494    Loss: 0.392
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [22][   20/   40]   Loss 0.341324   Top1 88.593750   Top5 99.472656   BatchTime 0.133099
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0629)
features.2.conv.0 tensor(0.0266)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0784)
features.3.conv.0 tensor(0.0255)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0506)
features.4.conv.0 tensor(0.0257)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0964)
features.5.conv.0 tensor(0.0312)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.0923)
features.6.conv.0 tensor(0.0176)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0552)
features.7.conv.0 tensor(0.0608)
features.7.conv.3 tensor(0.1230)
features.7.conv.6 tensor(0.0900)
features.8.conv.0 tensor(0.0551)
features.8.conv.3 tensor(0.1450)
features.8.conv.6 tensor(0.0991)
features.9.conv.0 tensor(0.0775)
features.9.conv.3 tensor(0.1554)
features.9.conv.6 tensor(0.1025)
features.10.conv.0 tensor(0.0327)
features.10.conv.3 tensor(0.1045)
features.10.conv.6 tensor(0.0765)
features.11.conv.0 tensor(0.1641)
features.11.conv.3 tensor(0.1128)
features.11.conv.6 tensor(0.1641)
features.12.conv.0 tensor(0.1482)
features.12.conv.3 tensor(0.1611)
features.12.conv.6 tensor(0.2752)
features.13.conv.0 tensor(0.0742)
features.13.conv.3 tensor(0.1769)
features.13.conv.6 tensor(0.1012)
features.14.conv.0 tensor(0.9177)
features.14.conv.3 tensor(0.1095)
features.14.conv.6 tensor(0.8891)
features.15.conv.0 tensor(0.9439)
features.15.conv.3 tensor(0.0855)
features.15.conv.6 tensor(0.9464)
features.16.conv.0 tensor(0.0773)
features.16.conv.3 tensor(0.1139)
features.16.conv.6 tensor(0.2046)
conv.0 tensor(0.2186)
tensor(813520.) 2188896.0
INFO - Validation [22][   40/   40]   Loss 0.337688   Top1 88.530000   Top5 99.610000   BatchTime 0.094312
INFO - ==> Top1: 88.530    Top5: 99.610    Loss: 0.338
INFO - ==> Sparsity : 0.372
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 88.440   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 88.330   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
0.84239918
0.84216619
0.84193128
0.84206492
0.84216750
0.84220976
0.84212339
0.84315568
0.84399897
0.84385633
0.84371465
0.84328651
0.84293514
0.84284359
0.84291798
0.84277689
0.84254098
0.84254968
0.84273428
0.84254140
0.84260511
0.84258068
0.84254646
0.84272856
0.84269047
0.84251273
0.84231961
0.84239990
0.84251249
0.84255046
0.84357589
INFO - Training [23][   20/  196]   Loss 0.388090   Top1 86.152344   Top5 98.085938   BatchTime 0.426394   LR 0.000666
0.84341675
0.84352416
0.84326541
0.84324312
0.84335643
0.84307742
INFO - Training [23][   40/  196]   Loss 0.396151   Top1 85.957031   Top5 98.251953   BatchTime 0.393405   LR 0.000648
0.84326786
0.84288937
0.84281540
0.84254986
0.84217298
0.84185648
0.84168410
0.84163529
0.84159714
0.84127337
0.84138566
0.84159476
0.84150100
0.84150028
0.84123898
0.84079778
0.84041584
0.84006983
0.84009385
0.84124339
0.84118485
INFO - Training [23][   60/  196]   Loss 0.395622   Top1 85.944010   Top5 98.326823   BatchTime 0.386079   LR 0.000630
0.84144938
0.84137458
0.84130490
0.84163564
0.84152871
0.84150559
0.84122175
0.84105831
0.84107161
0.84098786
0.84092981
0.84087193
0.84077233
0.84077382
0.84084958
0.84085423
0.84049863
0.84038407
0.84029144
0.84008306
0.83996212
0.83956933
INFO - Training [23][   80/  196]   Loss 0.396576   Top1 85.976562   Top5 98.461914   BatchTime 0.382601   LR 0.000613
0.83916545
0.83891988
0.83870572
0.83854866
0.83838052
0.83810461
0.83787984
0.83768404
0.83750689
0.83740598
0.83730620
0.83722448
0.83710355
0.83672816
0.83640057
0.83620936
INFO - Training [23][  100/  196]   Loss 0.388403   Top1 86.347656   Top5 98.484375   BatchTime 0.379439   LR 0.000596
0.83602077
0.83602619
0.83586860
0.83583200
0.83539337
0.83533573
0.83526117
0.83492941
0.83485216
0.83465016
0.83460361
0.83461905
0.83454168
0.83437198
0.83435887
0.83432591
0.83620143
0.83619231
0.83604819
0.83578664
0.83560574
0.83557284
INFO - Training [23][  120/  196]   Loss 0.381170   Top1 86.585286   Top5 98.554688   BatchTime 0.378980   LR 0.000579
0.83523470
0.83479589
0.83444822
0.83445197
0.83643746
0.83606368
0.83569539
0.83556795
0.83546120
0.83518690
0.83484751
0.83477515
0.83472520
0.83480120
0.83470625
0.83457792
0.83424157
0.83406264
0.83409637
0.83381563
0.83359069
INFO - Training [23][  140/  196]   Loss 0.379496   Top1 86.668527   Top5 98.590960   BatchTime 0.379885   LR 0.000562
0.83354813
0.83358854
0.83354098
0.83371139
0.83362925
0.83352989
0.83360046
0.83354062
0.83339947
0.83316439
0.83321536
0.83333844
0.83336753
0.83334529
0.83324146
0.83332068
0.83326292
0.83312136
0.83294374
0.83268458
0.83255947
INFO - Training [23][  160/  196]   Loss 0.381140   Top1 86.643066   Top5 98.588867   BatchTime 0.378612   LR 0.000545
0.83264309
0.83274281
0.83234787
0.83198768
0.83172560
0.83169591
0.83166397
0.83171624
0.83173788
0.83188272
0.83170849
0.83166134
0.83177662
0.83212155
0.83294731
0.83285117
0.83257473
0.83250517
0.83236021
INFO - Training [23][  180/  196]   Loss 0.380262   Top1 86.642795   Top5 98.522135   BatchTime 0.372211   LR 0.000529
0.83219349
0.83226764
0.83238900
0.83238310
0.83223444
0.83218449
0.83215219
0.83218944
0.83200926
0.83183938
0.83178329
0.83177322
0.83190042
0.83356726
0.83311051
0.83304727
0.83314639
INFO - ==> Top1: 86.648    Top5: 98.544    Loss: 0.380
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [23][   20/   40]   Loss 0.354676   Top1 88.242188   Top5 99.355469   BatchTime 0.125198
INFO - Validation [23][   40/   40]   Loss 0.345709   Top1 88.310000   Top5 99.540000   BatchTime 0.090846
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0616)
features.2.conv.0 tensor(0.0243)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0778)
features.3.conv.0 tensor(0.0255)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0525)
features.4.conv.0 tensor(0.0270)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0978)
features.5.conv.0 tensor(0.0259)
features.5.conv.3 tensor(0.0787)
features.5.conv.6 tensor(0.0911)
features.6.conv.0 tensor(0.0197)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0544)
features.7.conv.0 tensor(0.0596)
features.7.conv.3 tensor(0.1218)
features.7.conv.6 tensor(0.0902)
features.8.conv.0 tensor(0.0553)
features.8.conv.3 tensor(0.1409)
features.8.conv.6 tensor(0.0980)
features.9.conv.0 tensor(0.1380)
features.9.conv.3 tensor(0.1542)
features.9.conv.6 tensor(0.1088)
features.10.conv.0 tensor(0.0328)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0745)
features.11.conv.0 tensor(0.1674)
features.11.conv.3 tensor(0.1101)
features.11.conv.6 tensor(0.3893)
features.12.conv.0 tensor(0.1629)
features.12.conv.3 tensor(0.1566)
features.12.conv.6 tensor(0.2934)
features.13.conv.0 tensor(0.0758)
features.13.conv.3 tensor(0.1771)
features.13.conv.6 tensor(0.0877)
features.14.conv.0 tensor(0.9187)
features.14.conv.3 tensor(0.1091)
features.14.conv.6 tensor(0.8755)
features.15.conv.0 tensor(0.9443)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.9507)
features.16.conv.0 tensor(0.0818)
features.16.conv.3 tensor(0.1150)
features.16.conv.6 tensor(0.2039)
conv.0 tensor(0.2320)
tensor(832972.) 2188896.0
INFO - ==> Top1: 88.310    Top5: 99.540    Loss: 0.346
INFO - ==> Sparsity : 0.381
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 88.440   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 88.330   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.83252150
0.83213598
0.83198404
0.83172786
0.83135039
0.83095628
0.83057386
0.83030027
0.83000827
0.82970363
0.82947612
0.82912022
0.82915777
0.82913882
0.82916981
0.82905990
0.82928544
0.82963616
0.82981163
INFO - Training [24][   20/  196]   Loss 0.405363   Top1 85.039062   Top5 98.183594   BatchTime 0.430996   LR 0.000500
0.82987934
0.82978958
0.83006656
0.83029169
0.83021402
0.83020031
0.83011901
0.82995653
0.82990175
0.82996649
0.83018434
0.83035749
0.83045685
0.83078355
0.83110815
0.83197349
0.83293051
INFO - Training [24][   40/  196]   Loss 0.392016   Top1 86.005859   Top5 98.369141   BatchTime 0.391070   LR 0.000484
0.83440202
0.83526558
0.83539647
0.83550465
0.83551079
0.83541983
0.83492517
0.83475965
0.83455390
0.83454770
0.83595997
0.83583111
0.83581692
0.83575964
0.83582443
0.83594155
0.83608603
0.83591080
0.83543551
0.83527684
0.83768421
0.84044182
INFO - Training [24][   60/  196]   Loss 0.386965   Top1 86.315104   Top5 98.483073   BatchTime 0.382178   LR 0.000468
0.84266412
0.84194642
0.84165716
0.84180796
0.84122002
0.84155691
0.84124810
0.84116793
0.84124488
0.84120178
0.84094888
0.84103453
0.84094477
0.84105760
0.84108657
0.84090412
0.84084225
0.84088355
0.84089142
0.84073693
0.84079808
INFO - Training [24][   80/  196]   Loss 0.381138   Top1 86.630859   Top5 98.603516   BatchTime 0.355791   LR 0.000453
0.84071922
0.84080815
0.84075791
0.84076053
0.84069401
0.84046018
0.84039271
0.84034580
0.84015572
0.84007007
0.83999598
0.83983350
0.83957964
0.83932102
0.83930635
0.83941478
0.83935922
0.84043938
0.84096158
0.84086877
0.84113950
0.84218639
0.84216738
INFO - Training [24][  100/  196]   Loss 0.373592   Top1 86.886719   Top5 98.601562   BatchTime 0.337578   LR 0.000437
0.84216994
0.84222490
0.84217411
0.84215379
0.84195113
0.84176320
0.84159642
0.84151953
0.84151393
0.84142655
0.84134156
0.84128052
0.84130740
0.84126347
0.84140283
INFO - Training [24][  120/  196]   Loss 0.369690   Top1 87.021484   Top5 98.681641   BatchTime 0.324394   LR 0.000422
0.84153086
0.84162885
0.84163004
0.84157562
0.84144747
0.84134573
0.84132600
0.84123474
0.84127599
0.84140563
0.84155148
0.84141159
0.84148484
0.84152639
0.84145802
0.84139299
0.84152848
0.84130764
0.84150410
0.84137201
0.84119159
0.84114408
0.84104395
0.84061015
0.84052044
INFO - Training [24][  140/  196]   Loss 0.368419   Top1 87.039621   Top5 98.727679   BatchTime 0.313055   LR 0.000407
0.84041786
0.84029174
0.84032279
0.84039903
0.84022337
0.83973634
0.83893597
0.83838540
0.83763987
0.83746505
0.83730543
0.83741540
0.83738858
0.83737159
0.83730453
INFO - Training [24][  160/  196]   Loss 0.371188   Top1 87.006836   Top5 98.720703   BatchTime 0.306416   LR 0.000392
0.83703125
0.83665043
0.83644092
0.83723485
0.83725452
0.83784920
0.83819062
0.83813608
0.83824646
0.83831072
0.83822107
0.83813012
0.83814025
0.83819908
0.83826995
0.83821553
0.83821678
0.83858073
0.83971012
0.83958572
0.83955282
0.83945954
INFO - Training [24][  180/  196]   Loss 0.371741   Top1 86.957465   Top5 98.663194   BatchTime 0.302640   LR 0.000378
0.83941311
0.83938825
0.83943516
0.83952457
0.83968514
0.83965242
0.83964455
0.83951080
0.83938140
0.83936971
0.83951265
0.83945483
0.83919102
0.83922321
0.83939767
INFO - ==> Top1: 87.028    Top5: 98.646    Loss: 0.370
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83957005
0.83935773
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 0.320141   Top1 89.765625   Top5 99.550781   BatchTime 0.119954
INFO - Validation [24][   40/   40]   Loss 0.308053   Top1 89.680000   Top5 99.670000   BatchTime 0.087255
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0255)
features.2.conv.3 tensor(0.0556)
features.2.conv.6 tensor(0.0773)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0525)
features.4.conv.0 tensor(0.0285)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.1037)
features.5.conv.0 tensor(0.0296)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.0920)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0540)
features.7.conv.0 tensor(0.0619)
features.7.conv.3 tensor(0.1247)
features.7.conv.6 tensor(0.0907)
features.8.conv.0 tensor(0.0571)
features.8.conv.3 tensor(0.1377)
features.8.conv.6 tensor(0.0975)
features.9.conv.0 tensor(0.0813)
features.9.conv.3 tensor(0.1528)
features.9.conv.6 tensor(0.1074)
features.10.conv.0 tensor(0.0323)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.0753)
features.11.conv.0 tensor(0.1691)
features.11.conv.3 tensor(0.1107)
features.11.conv.6 tensor(0.1856)
features.12.conv.0 tensor(0.1630)
features.12.conv.3 tensor(0.1574)
features.12.conv.6 tensor(0.2821)
features.13.conv.0 tensor(0.0788)
features.13.conv.3 tensor(0.1765)
features.13.conv.6 tensor(0.0972)
features.14.conv.0 tensor(0.9193)
features.14.conv.3 tensor(0.1098)
features.14.conv.6 tensor(0.9070)
features.15.conv.0 tensor(0.9474)
features.15.conv.3 tensor(0.0838)
features.15.conv.6 tensor(0.9545)
features.16.conv.0 tensor(0.0837)
features.16.conv.3 tensor(0.1131)
features.16.conv.6 tensor(0.2012)
conv.0 tensor(0.2185)
tensor(820914.) 2188896.0
INFO - ==> Top1: 89.680    Top5: 99.670    Loss: 0.308
INFO - ==> Sparsity : 0.375
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 88.440   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.83856279
0.83803642
0.83746362
0.83704007
0.83689344
0.83691746
0.83697289
0.83762652
0.83846539
0.83832169
0.83823580
0.83813149
0.83821589
0.83804679
0.83804637
0.83805251
INFO - Training [25][   20/  196]   Loss 0.377966   Top1 86.660156   Top5 98.339844   BatchTime 0.332189   LR 0.000353
0.83789569
0.83771068
0.83721215
0.83705235
0.83713150
0.83717346
0.83724666
0.83703762
0.83693987
0.83681613
0.83671045
0.83668095
0.83670449
0.83677900
0.83689266
0.83700037
0.83717251
0.83795857
0.83818257
0.83800280
0.83803713
0.83802563
0.83797479
INFO - Training [25][   40/  196]   Loss 0.379185   Top1 86.826172   Top5 98.447266   BatchTime 0.293038   LR 0.000339
0.83783036
0.83806682
0.83765054
0.83761644
0.83746678
0.83731103
0.83720368
0.83716351
0.83678806
0.83671290
0.83811975
0.84036034
0.84007287
0.83983177
0.83929712
0.83861619
0.83827513
0.83794796
0.83768934
0.83745080
0.83736295
INFO - Training [25][   60/  196]   Loss 0.378469   Top1 86.842448   Top5 98.515625   BatchTime 0.292491   LR 0.000325
0.83744687
0.83761495
0.83761835
0.83756757
0.83742410
0.83758289
0.83760780
0.83881491
0.83892262
0.83898133
0.83886707
0.83872086
0.83871603
0.83879590
0.83860636
INFO - Training [25][   80/  196]   Loss 0.376691   Top1 86.982422   Top5 98.535156   BatchTime 0.288250   LR 0.000312
0.83855474
0.83833325
0.83832473
0.83865792
0.83857119
0.83875811
0.83888692
0.83905149
0.83910757
0.83871853
0.83880121
0.83863986
0.83856094
0.83856899
0.83857065
0.83852208
0.83875954
0.83871955
0.83862722
0.83826268
0.83860260
0.83874387
INFO - Training [25][  100/  196]   Loss 0.372639   Top1 87.046875   Top5 98.582031   BatchTime 0.284688   LR 0.000299
0.83863121
0.83847135
0.83834046
0.83847100
0.83824414
0.83789980
0.83799678
0.83760679
0.83773196
0.83753437
0.83744377
0.83709311
0.83694392
0.83686411
0.83674216
0.83650798
0.83615261
0.83634841
0.83672452
0.83672333
0.83662158
0.83665174
0.83673382
INFO - Training [25][  120/  196]   Loss 0.368548   Top1 87.174479   Top5 98.645833   BatchTime 0.280581   LR 0.000286
0.83630395
0.83606911
0.83604455
0.83600605
0.83602387
0.83592677
0.83578765
0.83568275
0.83558297
0.83574373
0.83572525
0.83578491
0.83581018
0.83572590
0.83553010
INFO - Training [25][  140/  196]   Loss 0.366172   Top1 87.282366   Top5 98.710938   BatchTime 0.278552   LR 0.000273
0.83534849
0.83522034
0.83517188
0.83507931
0.83507776
0.83506519
0.83501941
0.83494806
0.83474129
0.83459622
0.83468652
0.83454663
0.83443731
0.83442181
0.83445919
0.83459699
0.83477491
0.83499825
0.83481693
0.83461267
0.83451617
0.83415604
INFO - Training [25][  160/  196]   Loss 0.371358   Top1 87.163086   Top5 98.674316   BatchTime 0.278629   LR 0.000261
0.83385187
0.83343762
0.83327812
0.83306581
0.83290040
0.83258420
0.83236343
0.83224410
0.83207399
0.83200020
0.83249664
0.83413255
0.83404475
0.83382481
0.83374482
0.83368641
0.83378977
0.83536875
0.83534950
0.83552063
0.83545196
INFO - Training [25][  180/  196]   Loss 0.370649   Top1 87.198351   Top5 98.606771   BatchTime 0.278886   LR 0.000248
0.83536762
0.83532792
0.83534867
0.83534747
0.83510327
0.83525580
0.83485848
0.83459550
0.83427507
0.83385688
0.83375931
0.83378416
0.83375156
0.83362442
INFO - ==> Top1: 87.286    Top5: 98.618    Loss: 0.369
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83359665
0.83357275
0.83343124
0.83329570
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 0.342894   Top1 88.339844   Top5 99.492188   BatchTime 0.122769
INFO - Validation [25][   40/   40]   Loss 0.331209   Top1 88.470000   Top5 99.640000   BatchTime 0.086645
INFO - ==> Top1: 88.470    Top5: 99.640    Loss: 0.331
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.470   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0616)
features.2.conv.0 tensor(0.0240)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0741)
features.3.conv.0 tensor(0.0246)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0519)
features.4.conv.0 tensor(0.0278)
features.4.conv.3 tensor(0.0897)
features.4.conv.6 tensor(0.0959)
features.5.conv.0 tensor(0.0298)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0913)
features.6.conv.0 tensor(0.0200)
features.6.conv.3 tensor(0.0521)
features.6.conv.6 tensor(0.0551)
features.7.conv.0 tensor(0.0640)
features.7.conv.3 tensor(0.1241)
features.7.conv.6 tensor(0.1169)
features.8.conv.0 tensor(0.0570)
features.8.conv.3 tensor(0.1389)
features.8.conv.6 tensor(0.0970)
features.9.conv.0 tensor(0.0793)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.1085)
features.10.conv.0 tensor(0.0328)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0827)
features.11.conv.0 tensor(0.1706)
features.11.conv.3 tensor(0.1103)
features.11.conv.6 tensor(0.2280)
features.12.conv.0 tensor(0.1694)
features.12.conv.3 tensor(0.1570)
features.12.conv.6 tensor(0.2915)
features.13.conv.0 tensor(0.0784)
features.13.conv.3 tensor(0.1767)
features.13.conv.6 tensor(0.0875)
features.14.conv.0 tensor(0.9212)
features.14.conv.3 tensor(0.1091)
features.14.conv.6 tensor(0.9030)
features.15.conv.0 tensor(0.9495)
features.15.conv.3 tensor(0.0830)
features.15.conv.6 tensor(0.9550)
features.16.conv.0 tensor(0.0847)
features.16.conv.3 tensor(0.1142)
features.16.conv.6 tensor(0.1997)
conv.0 tensor(0.2167)
tensor(823175.) 2188896.0
0.83306557
0.83291483
0.83300018
0.83332354
0.83319068
0.83306062
0.83323145
0.83322704
0.83325261
0.83330131
0.83307070
0.83284652
0.83260053
0.83228862
0.83202928
INFO - Training [26][   20/  196]   Loss 0.386907   Top1 86.210938   Top5 97.929688   BatchTime 0.318265   LR 0.000228
0.83173043
0.83158416
0.83153623
0.83154261
0.83149672
0.83146060
0.83243656
0.83297360
0.83313596
0.83299392
0.83303708
0.83288306
0.83293658
0.83267212
0.83258325
0.83251196
0.83239812
0.83241999
0.83242351
0.83226544
0.83223736
0.83233285
INFO - Training [26][   40/  196]   Loss 0.387868   Top1 86.337891   Top5 98.144531   BatchTime 0.293993   LR 0.000216
0.83252209
0.83251762
0.83265513
0.83258045
0.83238244
0.83247298
0.83242160
0.83241922
0.83220470
0.83222002
0.83196533
0.83198023
0.83201241
0.83188778
0.83190376
0.83184636
0.83144164
0.83127898
0.83120614
0.83112258
0.83108193
0.83113122
0.83123839
INFO - Training [26][   60/  196]   Loss 0.373112   Top1 87.083333   Top5 98.255208   BatchTime 0.285036   LR 0.000205
0.83131880
0.83132148
0.83120894
0.83123338
0.83142966
0.83157951
0.83137214
0.83121043
0.83114535
0.83114153
0.83119023
0.83118904
0.83111858
0.83110416
0.83114707
0.83128434
INFO - Training [26][   80/  196]   Loss 0.371600   Top1 87.138672   Top5 98.422852   BatchTime 0.277598   LR 0.000194
0.83108848
0.83095318
0.83089411
0.83077335
0.83070964
0.83074707
0.83068955
0.83066666
0.83060533
0.83058763
0.83053243
0.83046710
0.83035886
0.83035141
0.83234650
0.83224654
0.83217329
0.83205497
0.83186108
0.83168811
0.83169532
INFO - Training [26][  100/  196]   Loss 0.366254   Top1 87.328125   Top5 98.472656   BatchTime 0.278180   LR 0.000183
0.83164948
0.83167869
0.83201855
0.83244985
0.83205181
0.83194798
0.83185273
0.83183461
0.83175850
0.83161271
0.83144325
0.83136708
0.83122063
0.83110172
0.83117169
0.83121407
0.83096325
0.83062041
0.83047158
0.83043230
0.83042580
0.83033139
0.83027571
0.83023709
INFO - Training [26][  120/  196]   Loss 0.362351   Top1 87.460938   Top5 98.561198   BatchTime 0.273841   LR 0.000173
0.83019859
0.83021170
0.83029735
0.83026093
0.83016092
0.83009022
0.83029598
0.83027214
0.83008170
0.83000857
0.83005744
0.82982832
0.82980257
0.82976824
0.82964545
0.82964909
INFO - Training [26][  140/  196]   Loss 0.361438   Top1 87.497210   Top5 98.607701   BatchTime 0.270407   LR 0.000163
0.82975280
0.82967085
0.82954317
0.82944745
0.82933289
0.82919085
0.82909089
0.82907403
0.82909852
0.82906449
0.82911730
0.82877970
0.82881063
0.82874542
0.82866937
0.82838655
0.82807106
0.82799810
0.82797104
0.82827997
0.82924843
0.82875669
0.82858354
0.82845843
INFO - Training [26][  160/  196]   Loss 0.361595   Top1 87.500000   Top5 98.608398   BatchTime 0.267855   LR 0.000153
0.82830667
0.82814300
0.82797772
0.82793194
0.82785058
0.82777363
0.82790333
0.82776892
0.82779670
0.82785457
0.82793981
0.82785743
0.82785779
0.82797843
0.82798600
INFO - Training [26][  180/  196]   Loss 0.359704   Top1 87.532552   Top5 98.556858   BatchTime 0.268539   LR 0.000144
0.82803482
0.82790333
0.82787764
0.82774407
0.82770568
0.82759613
0.82755005
0.82760882
0.82781667
0.82771116
0.82757950
0.82752538
0.82754904
0.82744002
0.82735711
INFO - ==> Top1: 87.500    Top5: 98.584    Loss: 0.360
0.82734543
0.82733357
0.82726848
0.82727444
0.82723993
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [26][   20/   40]   Loss 0.377604   Top1 88.417969   Top5 99.433594   BatchTime 0.125322
INFO - Validation [26][   40/   40]   Loss 0.363915   Top1 88.240000   Top5 99.570000   BatchTime 0.089973
INFO - ==> Top1: 88.240    Top5: 99.570    Loss: 0.364
INFO - ==> Sparsity : 0.378
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.470   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0647)
features.2.conv.0 tensor(0.0214)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0747)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0502)
features.3.conv.6 tensor(0.0508)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.0994)
features.5.conv.0 tensor(0.0311)
features.5.conv.3 tensor(0.0781)
features.5.conv.6 tensor(0.0908)
features.6.conv.0 tensor(0.0187)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0554)
features.7.conv.0 tensor(0.0628)
features.7.conv.3 tensor(0.1256)
features.7.conv.6 tensor(0.0972)
features.8.conv.0 tensor(0.0592)
features.8.conv.3 tensor(0.1389)
features.8.conv.6 tensor(0.0968)
features.9.conv.0 tensor(0.0804)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.1135)
features.10.conv.0 tensor(0.0328)
features.10.conv.3 tensor(0.1059)
features.10.conv.6 tensor(0.0792)
features.11.conv.0 tensor(0.1721)
features.11.conv.3 tensor(0.1080)
features.11.conv.6 tensor(0.2710)
features.12.conv.0 tensor(0.1680)
features.12.conv.3 tensor(0.1584)
features.12.conv.6 tensor(0.2975)
features.13.conv.0 tensor(0.0790)
features.13.conv.3 tensor(0.1767)
features.13.conv.6 tensor(0.0921)
features.14.conv.0 tensor(0.9230)
features.14.conv.3 tensor(0.1102)
features.14.conv.6 tensor(0.9012)
features.15.conv.0 tensor(0.9501)
features.15.conv.3 tensor(0.0821)
features.15.conv.6 tensor(0.9590)
features.16.conv.0 tensor(0.0850)
features.16.conv.3 tensor(0.1130)
features.16.conv.6 tensor(0.1998)
conv.0 tensor(0.2161)
tensor(826486.) 2188896.0
0.82714629
0.82678485
0.82663435
0.82658792
0.82673246
0.82667303
0.82630342
0.82605177
0.82602102
0.82610488
0.82614988
0.82597017
0.82590258
0.82578605
0.82569641
0.82570601
0.82553029
0.82525212
0.82521367
0.82528597
0.82537109
0.82522106
INFO - Training [27][   20/  196]   Loss 0.375832   Top1 87.187500   Top5 98.203125   BatchTime 0.317379   LR 0.000128
0.82525086
0.82528800
0.82537305
0.82571274
0.82607704
0.82615358
0.82603920
0.82595730
0.82585353
0.82569897
0.82550341
0.82523984
0.82500535
0.82467687
0.82440847
0.82439190
0.82460445
INFO - Training [27][   40/  196]   Loss 0.363379   Top1 87.548828   Top5 98.427734   BatchTime 0.277078   LR 0.000119
0.82440013
0.82408941
0.82386398
0.82371318
0.82358998
0.82351792
0.82345796
0.82341152
0.82332587
0.82328695
0.82329738
0.82324505
0.82321382
0.82320505
0.82320845
0.82316691
INFO - Training [27][   60/  196]   Loss 0.365244   Top1 87.428385   Top5 98.470052   BatchTime 0.268002   LR 0.000111
0.82313615
0.82308656
0.82303125
0.82300431
0.82301283
0.82299715
0.82299823
0.82298142
0.82298213
0.82296914
0.82296640
0.82295060
0.82293409
0.82292336
0.82293433
0.82295656
0.82299858
0.82309580
0.82340491
0.82317424
0.82307124
0.82299793
0.82294488
0.82291901
0.82288587
INFO - Training [27][   80/  196]   Loss 0.364601   Top1 87.465820   Top5 98.535156   BatchTime 0.259543   LR 0.000102
0.82287645
0.82279617
0.82276326
0.82271552
0.82270724
0.82266885
0.82264531
0.82263136
0.82260555
0.82260156
0.82256871
0.82255900
0.82254839
0.82249796
0.82247448
0.82244539
0.82242578
INFO - Training [27][  100/  196]   Loss 0.357178   Top1 87.640625   Top5 98.562500   BatchTime 0.256040   LR 0.000095
0.82241148
0.82240915
0.82238883
0.82239980
0.82239038
0.82236707
0.82234722
0.82232076
0.82231432
0.82229036
0.82226056
0.82222742
0.82220787
0.82220596
0.82218552
0.82214570
0.82210910
0.82207215
0.82204270
0.82201833
0.82199335
0.82199240
0.82198054
0.82197022
INFO - Training [27][  120/  196]   Loss 0.349300   Top1 87.958984   Top5 98.681641   BatchTime 0.254755   LR 0.000087
0.82197273
0.82195878
0.82197458
0.82249498
0.82248074
0.82240641
0.82232314
0.82227010
0.82224756
0.82220566
0.82215232
0.82209116
0.82204181
0.82198131
0.82194740
0.82191938
0.82189727
INFO - Training [27][  140/  196]   Loss 0.347277   Top1 88.027344   Top5 98.744420   BatchTime 0.252525   LR 0.000080
0.82183379
0.82180697
0.82180172
0.82174110
0.82175744
0.82169700
0.82170814
0.82167470
0.82165354
0.82163250
0.82163954
0.82163131
0.82159716
0.82157111
0.82154906
0.82152313
0.82150322
0.82145298
0.82146615
0.82145566
0.82140833
0.82135850
0.82132912
0.82123137
INFO - Training [27][  160/  196]   Loss 0.350465   Top1 87.905273   Top5 98.713379   BatchTime 0.252075   LR 0.000073
0.82115507
0.82105213
0.82084668
0.82063550
0.82056165
0.82056248
0.82056141
0.82058245
0.82058376
0.82054049
0.82052857
0.82049787
0.82046229
0.82047164
0.82047176
0.82045072
INFO - Training [27][  180/  196]   Loss 0.351468   Top1 87.797309   Top5 98.658854   BatchTime 0.251513   LR 0.000066
0.82044482
0.82043815
0.82040429
0.82038581
0.82040304
0.82038307
0.82034439
0.82034719
0.82035989
0.82033736
0.82036036
0.82035220
0.82033473
0.82033592
0.82032925
0.82031107
0.82033390
0.82032305
INFO - ==> Top1: 87.814    Top5: 98.668    Loss: 0.351
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [27][   20/   40]   Loss 0.373578   Top1 87.890625   Top5 99.511719   BatchTime 0.131551
INFO - Validation [27][   40/   40]   Loss 0.363984   Top1 87.830000   Top5 99.570000   BatchTime 0.093539
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0226)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0741)
features.3.conv.0 tensor(0.0231)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0514)
features.4.conv.0 tensor(0.0286)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.1084)
features.5.conv.0 tensor(0.0317)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0915)
features.6.conv.0 tensor(0.0187)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0553)
features.7.conv.0 tensor(0.0616)
features.7.conv.3 tensor(0.1218)
features.7.conv.6 tensor(0.1008)
features.8.conv.0 tensor(0.0589)
features.8.conv.3 tensor(0.1383)
features.8.conv.6 tensor(0.1002)
features.9.conv.0 tensor(0.0813)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.1201)
features.10.conv.0 tensor(0.0332)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0796)
features.11.conv.0 tensor(0.1746)
features.11.conv.3 tensor(0.1084)
features.11.conv.6 tensor(0.3220)
features.12.conv.0 tensor(0.1720)
features.12.conv.3 tensor(0.1578)
features.12.conv.6 tensor(0.2989)
features.13.conv.0 tensor(0.0792)
features.13.conv.3 tensor(0.1738)
features.13.conv.6 tensor(0.0973)
features.14.conv.0 tensor(0.9245)
features.14.conv.3 tensor(0.1096)
features.14.conv.6 tensor(0.9087)
features.15.conv.0 tensor(0.9507)
features.15.conv.3 tensor(0.0824)
features.15.conv.6 tensor(0.9635)
features.16.conv.0 tensor(0.0855)
features.16.conv.3 tensor(0.1128)
features.16.conv.6 tensor(0.1996)
conv.0 tensor(0.2179)
tensor(833519.) 2188896.0
INFO - ==> Top1: 87.830    Top5: 99.570    Loss: 0.364
INFO - ==> Sparsity : 0.381
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.470   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
0.82033676
0.82027727
0.82029098
0.82028466
0.82028598
0.82030320
0.82030779
0.82028013
0.82028824
0.82027483
0.82029015
0.82025659
0.82022876
0.82024258
0.82025260
0.82021856
0.82020527
0.82020366
INFO - Training [28][   20/  196]   Loss 0.381211   Top1 86.933594   Top5 97.968750   BatchTime 0.349424   LR 0.000055
0.82020253
0.82019871
0.82020414
0.82019705
0.82020426
0.82021999
0.82022154
0.82018250
0.82018918
0.82021379
0.82023430
0.82020855
0.82020199
0.82020140
0.82021958
0.82021749
0.82022047
0.82023644
0.82026172
0.82024872
0.82025868
0.82023174
0.82026368
INFO - Training [28][   40/  196]   Loss 0.377512   Top1 87.167969   Top5 98.310547   BatchTime 0.302516   LR 0.000050
0.82024413
0.82027584
0.82023638
0.82025468
0.82026029
0.82025516
0.82023174
0.82024306
0.82023013
0.82023889
0.82023770
0.82023287
0.82022601
0.82022589
0.82022023
0.82019579
INFO - Training [28][   60/  196]   Loss 0.367982   Top1 87.337240   Top5 98.411458   BatchTime 0.289118   LR 0.000044
0.82021391
0.82023007
0.82021540
0.82021427
0.82019043
0.82019264
0.82018626
0.82016975
0.82016373
0.82014900
0.82013220
0.82014298
0.82013470
0.82015955
0.82014877
0.82013166
0.82013983
0.82012397
0.82009763
0.82009381
0.82008761
0.82006639
0.82007861
INFO - Training [28][   80/  196]   Loss 0.367087   Top1 87.333984   Top5 98.481445   BatchTime 0.281976   LR 0.000039
0.82007396
0.82007748
0.82005388
0.82002759
0.82001072
0.82001978
0.82002413
0.82000721
0.82001823
0.82004684
0.82002819
0.81999850
0.81998461
0.81997478
0.81997544
INFO - Training [28][  100/  196]   Loss 0.357831   Top1 87.636719   Top5 98.531250   BatchTime 0.276929   LR 0.000034
0.81995046
0.81994379
0.81995380
0.81995243
0.81995720
0.81993794
0.81994575
0.81994462
0.81994259
0.81994152
0.81993157
0.81994325
0.81994385
0.81993240
0.81993854
0.81992614
0.81993145
0.81993556
0.81992531
0.81990391
0.81988370
0.81989431
0.81991273
INFO - Training [28][  120/  196]   Loss 0.352515   Top1 87.809245   Top5 98.649089   BatchTime 0.275767   LR 0.000030
0.81988347
0.81988585
0.81989288
0.81990588
0.81987166
0.81984866
0.81984288
0.81985086
0.81982636
0.81982106
0.81983393
0.81982714
0.81982756
0.81981087
0.81981635
0.81981194
0.81980020
0.81979609
0.81981325
0.81981945
0.81979805
0.81981128
INFO - Training [28][  140/  196]   Loss 0.350665   Top1 87.929688   Top5 98.716518   BatchTime 0.274752   LR 0.000026
0.81982511
0.81982684
0.81982106
0.81983030
0.81984687
0.81985909
0.81986773
0.81983310
0.81984681
0.81984335
0.81981331
0.81983519
0.81981707
0.81981045
0.81981468
0.81979853
INFO - Training [28][  160/  196]   Loss 0.353362   Top1 87.807617   Top5 98.713379   BatchTime 0.271484   LR 0.000022
0.81979507
0.81978470
0.81977326
0.81976676
0.81977206
0.81976658
0.81977600
0.81978309
0.81976748
0.81974578
0.81975061
0.81973445
0.81974548
0.81974328
0.81973338
0.81971264
0.81971830
0.81970394
0.81966805
0.81966209
0.81964397
0.81966549
0.81966984
0.81962222
INFO - Training [28][  180/  196]   Loss 0.352864   Top1 87.799479   Top5 98.697917   BatchTime 0.269249   LR 0.000018
0.81960160
0.81959468
0.81959569
0.81960022
0.81960666
0.81961858
0.81961948
0.81957954
0.81959671
0.81961048
0.81959343
0.81956547
0.81958479
0.81956875
0.81959146
0.81957638
INFO - ==> Top1: 87.930    Top5: 98.712    Loss: 0.350
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.329045   Top1 89.140625   Top5 99.550781   BatchTime 0.121142
INFO - Validation [28][   40/   40]   Loss 0.317728   Top1 89.200000   Top5 99.620000   BatchTime 0.084655
INFO - ==> Top1: 89.200    Top5: 99.620    Loss: 0.318
INFO - ==> Sparsity : 0.382
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0638)
features.2.conv.0 tensor(0.0214)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0747)
features.3.conv.0 tensor(0.0237)
features.3.conv.3 tensor(0.0478)
features.3.conv.6 tensor(0.0521)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.1029)
features.5.conv.0 tensor(0.0316)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.0954)
features.6.conv.0 tensor(0.0179)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0557)
features.7.conv.0 tensor(0.0619)
features.7.conv.3 tensor(0.1218)
features.7.conv.6 tensor(0.1039)
features.8.conv.0 tensor(0.0588)
features.8.conv.3 tensor(0.1389)
features.8.conv.6 tensor(0.1005)
features.9.conv.0 tensor(0.0802)
features.9.conv.3 tensor(0.1522)
features.9.conv.6 tensor(0.1206)
features.10.conv.0 tensor(0.0332)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0780)
features.11.conv.0 tensor(0.1755)
features.11.conv.3 tensor(0.1086)
features.11.conv.6 tensor(0.3289)
features.12.conv.0 tensor(0.1787)
features.12.conv.3 tensor(0.1588)
features.12.conv.6 tensor(0.2999)
features.13.conv.0 tensor(0.0791)
features.13.conv.3 tensor(0.1757)
features.13.conv.6 tensor(0.1152)
features.14.conv.0 tensor(0.9244)
features.14.conv.3 tensor(0.1095)
features.14.conv.6 tensor(0.9087)
features.15.conv.0 tensor(0.9509)
features.15.conv.3 tensor(0.0825)
features.15.conv.6 tensor(0.9637)
features.16.conv.0 tensor(0.0857)
features.16.conv.3 tensor(0.1130)
features.16.conv.6 tensor(0.1995)
conv.0 tensor(0.2183)
tensor(836235.) 2188896.0
0.81952268
0.81952477
0.81951916
0.81949693
0.81948781
0.81949043
0.81946433
0.81945074
0.81943566
0.81941831
0.81941444
0.81939560
0.81938416
0.81938046
0.81937164
0.81936097
0.81933737
0.81931913
0.81930554
0.81934321
INFO - Training [29][   20/  196]   Loss 0.367271   Top1 86.933594   Top5 98.476562   BatchTime 0.351765   LR 0.000013
0.81933808
0.81930572
0.81929445
0.81926936
0.81923097
0.81922430
0.81920123
0.81917179
0.81915039
0.81915754
0.81909740
0.81904930
0.81901586
0.81895339
0.81895977
0.81895238
0.81891048
0.81888038
0.81885588
0.81880569
0.81880981
0.81881028
INFO - Training [29][   40/  196]   Loss 0.366435   Top1 86.972656   Top5 98.623047   BatchTime 0.314235   LR 0.000010
0.81878620
0.81881374
0.81881690
0.81876618
0.81873661
0.81873906
0.81870711
0.81870639
0.81868970
0.81866318
0.81865889
0.81866068
0.81865257
0.81862885
0.81864607
0.81864142
INFO - Training [29][   60/  196]   Loss 0.367362   Top1 87.005208   Top5 98.593750   BatchTime 0.296716   LR 0.000008
0.81864506
0.81866366
0.81865132
0.81865418
0.81864291
0.81865793
0.81866038
0.81867105
0.81866151
0.81865805
0.81864411
0.81865066
0.81866938
0.81868476
0.81868607
0.81867856
0.81867003
0.81867784
0.81867284
0.81867939
0.81868464
0.81867075
0.81865591
INFO - Training [29][   80/  196]   Loss 0.361715   Top1 87.333984   Top5 98.696289   BatchTime 0.288308   LR 0.000005
0.81863552
0.81864750
0.81866127
0.81866610
0.81866086
0.81867123
0.81870192
0.81868839
0.81869721
0.81870103
0.81870556
0.81868893
0.81866807
0.81869483
0.81871021
0.81869102
INFO - Training [29][  100/  196]   Loss 0.356399   Top1 87.570312   Top5 98.671875   BatchTime 0.280436   LR 0.000004
0.81869584
0.81869334
0.81868166
0.81868488
0.81866699
0.81867844
0.81864768
0.81865644
0.81864840
0.81866759
0.81867170
0.81865162
0.81866682
0.81865692
0.81865865
0.81865573
0.81865448
0.81863725
0.81865025
0.81866062
0.81864917
0.81864953
0.81865829
0.81866252
INFO - Training [29][  120/  196]   Loss 0.350776   Top1 87.783203   Top5 98.727214   BatchTime 0.275098   LR 0.000002
0.81865239
0.81864357
0.81865168
0.81866235
0.81864619
0.81865311
0.81865317
0.81865513
0.81865257
0.81864357
0.81863773
0.81863374
0.81864321
0.81865871
0.81868505
0.81866986
INFO - Training [29][  140/  196]   Loss 0.349660   Top1 87.865513   Top5 98.777902   BatchTime 0.271600   LR 0.000001
0.81866205
0.81867433
0.81865668
0.81865650
0.81864476
0.81864214
0.81864631
0.81864536
0.81866294
0.81864583
0.81865460
0.81863570
0.81863159
0.81863046
0.81861722
0.81863904
0.81864190
0.81865132
0.81864870
0.81865221
0.81864101
0.81863838
0.81861812
INFO - Training [29][  160/  196]   Loss 0.351604   Top1 87.854004   Top5 98.769531   BatchTime 0.270595   LR 0.000001
0.81865466
0.81865788
0.81864774
0.81863815
0.81862974
0.81864506
0.81863314
0.81863546
0.81864560
0.81862557
0.81863350
0.81864315
0.81864488
0.81864834
0.81863749
0.81864458
0.81864381
0.81864482
0.81863868
0.81863552
0.81862974
INFO - Training [29][  180/  196]   Loss 0.350809   Top1 87.877604   Top5 98.719618   BatchTime 0.271572   LR 0.000000
0.81863761
0.81865311
0.81865293
0.81866467
0.81865549
0.81866330
0.81864566
0.81864476
0.81865096
0.81864065
0.81864697
0.81864059
0.81865472
0.81866330
0.81866199
INFO - ==> Top1: 87.966    Top5: 98.730    Loss: 0.348
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [29][   20/   40]   Loss 0.367573   Top1 88.261719   Top5 99.414062   BatchTime 0.123070
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.1953)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0634)
features.2.conv.0 tensor(0.0220)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0755)
features.3.conv.0 tensor(0.0240)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.0525)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.1037)
features.5.conv.0 tensor(0.0312)
features.5.conv.3 tensor(0.0770)
features.5.conv.6 tensor(0.0964)
features.6.conv.0 tensor(0.0179)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0556)
features.7.conv.0 tensor(0.0619)
features.7.conv.3 tensor(0.1218)
features.7.conv.6 tensor(0.1051)
features.8.conv.0 tensor(0.0587)
features.8.conv.3 tensor(0.1386)
features.8.conv.6 tensor(0.1008)
features.9.conv.0 tensor(0.0803)
features.9.conv.3 tensor(0.1539)
features.9.conv.6 tensor(0.1200)
features.10.conv.0 tensor(0.0332)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0779)
features.11.conv.0 tensor(0.1756)
features.11.conv.3 tensor(0.1084)
features.11.conv.6 tensor(0.3316)
features.12.conv.0 tensor(0.1891)
features.12.conv.3 tensor(0.1588)
features.12.conv.6 tensor(0.3009)
features.13.conv.0 tensor(0.0794)
features.13.conv.3 tensor(0.1769)
features.13.conv.6 tensor(0.1126)
features.14.conv.0 tensor(0.9246)
features.14.conv.3 tensor(0.1093)
features.14.conv.6 tensor(0.9092)
features.15.conv.0 tensor(0.9507)
features.15.conv.3 tensor(0.0816)
features.15.conv.6 tensor(0.9627)
features.16.conv.0 tensor(0.0857)
features.16.conv.3 tensor(0.1127)
features.16.conv.6 tensor(0.1996)
conv.0 tensor(0.2189)
tensor(837032.) 2188896.0
INFO - Validation [29][   40/   40]   Loss 0.352828   Top1 88.450000   Top5 99.520000   BatchTime 0.088480
INFO - ==> Top1: 88.450    Top5: 99.520    Loss: 0.353
INFO - ==> Sparsity : 0.382
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
0.81864583
0.81999028
0.82168818
0.82277530
0.82363492
0.82556319
0.82625777
0.82696944
0.82689148
0.82694077
0.82958531
0.83228493
0.83171844
0.83138222
0.83139253
0.83137983
0.83142549
0.83145076
0.83136648
0.83148849
INFO - Training [30][   20/  196]   Loss 0.369553   Top1 87.050781   Top5 98.125000   BatchTime 0.317113   LR 0.001250
0.83155608
0.83158833
0.83188635
0.83241439
0.83447856
0.83511215
0.83735788
0.84604990
0.84627861
0.84661341
0.84675777
0.84672546
0.84669304
0.84688187
0.84714204
INFO - Training [30][   40/  196]   Loss 0.392826   Top1 86.250000   Top5 98.339844   BatchTime 0.286467   LR 0.001250
0.84760153
0.84777623
0.84767348
0.85173565
0.85198092
0.85153365
0.85107625
0.85093421
0.85031360
0.84965706
0.84942871
0.84888738
0.84835494
0.84832817
0.84850848
0.84839743
0.84829682
0.84804034
0.84796327
0.84796327
0.84796453
0.84790969
INFO - Training [30][   60/  196]   Loss 0.397574   Top1 86.028646   Top5 98.359375   BatchTime 0.285404   LR 0.001250
0.84758312
0.84765399
0.84822947
0.84810787
0.84803581
0.84783292
0.84773099
0.84778911
0.84786057
0.84793305
0.84809977
0.84862238
0.84890389
0.84930265
0.84959078
0.84952593
0.84949380
0.84947664
0.84949481
0.84951365
0.84965414
0.84976834
INFO - Training [30][   80/  196]   Loss 0.401936   Top1 85.947266   Top5 98.432617   BatchTime 0.280752   LR 0.001250
0.84994316
0.85007375
0.85001248
0.85006905
0.85011441
0.85003620
0.84999144
0.85002518
0.85011059
0.85015935
0.85019255
0.84999549
0.84978712
0.84969538
0.84969008
0.84960359
INFO - Training [30][  100/  196]   Loss 0.397512   Top1 86.136719   Top5 98.472656   BatchTime 0.274725   LR 0.001250
0.84943753
0.84934193
0.84937257
0.84961194
0.84962207
0.84953094
0.84943730
0.84937656
0.84913129
0.84905761
0.84921449
0.84911650
0.84910798
0.84898531
0.84889179
0.84887362
0.84874904
0.84859598
0.84839386
0.84825897
0.84831232
0.84864408
0.84847426
0.84841293
0.84857321
INFO - Training [30][  120/  196]   Loss 0.391793   Top1 86.396484   Top5 98.554688   BatchTime 0.270087   LR 0.001249
0.84867358
0.84874547
0.84879500
0.84873223
0.84887916
0.84885710
0.84865832
0.84868646
0.84866631
0.84853226
0.84850436
0.84863037
0.84866196
0.84846556
0.84836298
0.84822005
INFO - Training [30][  140/  196]   Loss 0.390486   Top1 86.523438   Top5 98.610491   BatchTime 0.268231   LR 0.001249
0.84793258
0.84666449
0.84477401
0.84701937
0.84785396
0.84781861
0.84772110
0.84774846
0.84732056
0.84693146
0.84697872
0.84687233
0.84704643
0.84714162
0.84781486
0.84802967
0.84803891
0.84818280
0.84818214
0.84818423
0.84809673
0.84799987
INFO - Training [30][  160/  196]   Loss 0.393091   Top1 86.423340   Top5 98.603516   BatchTime 0.268523   LR 0.001249
0.84805560
0.84799689
0.84800875
0.84782624
0.84785897
0.84785998
0.84781325
0.84775770
0.84765536
0.84756750
0.84753531
0.84739500
0.84726548
0.84740573
0.84766138
0.84876072
0.84871185
0.84884149
0.84870631
0.84880531
0.84861183
0.84855622
INFO - Training [30][  180/  196]   Loss 0.395780   Top1 86.308594   Top5 98.528646   BatchTime 0.268526   LR 0.001248
0.84868890
0.84898543
0.84822232
0.84798747
0.84822387
0.84843940
0.84849304
0.84867430
0.84890801
0.84898812
0.84875917
0.84837544
0.84810424
0.84780008
0.84823644
INFO - ==> Top1: 86.376    Top5: 98.540    Loss: 0.396
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84849095
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.429918   Top1 85.859375   Top5 99.160156   BatchTime 0.126243
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0590)
features.2.conv.0 tensor(0.0133)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0729)
features.3.conv.0 tensor(0.0226)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0460)
features.4.conv.0 tensor(0.0293)
features.4.conv.3 tensor(0.0961)
features.4.conv.6 tensor(0.0938)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.0920)
features.6.conv.0 tensor(0.0184)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0493)
features.7.conv.3 tensor(0.1204)
features.7.conv.6 tensor(0.0886)
features.8.conv.0 tensor(0.0555)
features.8.conv.3 tensor(0.1415)
features.8.conv.6 tensor(0.0962)
features.9.conv.0 tensor(0.0770)
features.9.conv.3 tensor(0.1476)
features.9.conv.6 tensor(0.1000)
features.10.conv.0 tensor(0.0359)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0740)
features.11.conv.0 tensor(0.1387)
features.11.conv.3 tensor(0.1121)
features.11.conv.6 tensor(0.1415)
features.12.conv.0 tensor(0.1203)
features.12.conv.3 tensor(0.1545)
features.12.conv.6 tensor(0.1980)
features.13.conv.0 tensor(0.0663)
features.13.conv.3 tensor(0.1767)
features.13.conv.6 tensor(0.0785)
features.14.conv.0 tensor(0.9163)
features.14.conv.3 tensor(0.1106)
features.14.conv.6 tensor(0.9140)
features.15.conv.0 tensor(0.9485)
features.15.conv.3 tensor(0.0852)
features.15.conv.6 tensor(0.9049)
features.16.conv.0 tensor(0.0797)
features.16.conv.3 tensor(0.1179)
features.16.conv.6 tensor(0.2121)
conv.0 tensor(0.2553)
tensor(817470.) 2188896.0
INFO - Validation [30][   40/   40]   Loss 0.422768   Top1 85.810000   Top5 99.370000   BatchTime 0.093259
INFO - ==> Top1: 85.810    Top5: 99.370    Loss: 0.423
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
0.84800631
0.84764636
0.84740978
0.84756905
0.84779972
0.84782308
0.84788704
0.84780139
0.84761494
0.84742308
0.84750760
0.84759563
0.84781533
0.84787768
0.84810680
0.84719533
0.84694088
INFO - Training [31][   20/  196]   Loss 0.401685   Top1 85.839844   Top5 98.300781   BatchTime 0.350638   LR 0.001248
0.84667480
0.84649187
0.84633046
0.84615552
0.84633940
0.84659940
0.84689713
0.84793520
0.84796137
0.84774745
0.84786803
0.84796065
0.84796834
0.84808856
0.84802806
0.84801161
0.84834200
0.84833950
0.84844732
0.84881073
0.84902942
INFO - Training [31][   40/  196]   Loss 0.409970   Top1 85.605469   Top5 98.320312   BatchTime 0.319085   LR 0.001247
0.84917682
0.84947675
0.84990531
0.84959871
0.84926760
0.84906143
0.84873414
0.84837139
0.84806001
0.84773266
0.84741986
0.84715706
0.84711784
0.84695947
0.84689319
0.84703958
0.84702426
0.84680647
0.84671837
0.84673131
0.84673673
0.84662443
INFO - Training [31][   60/  196]   Loss 0.408807   Top1 85.709635   Top5 98.378906   BatchTime 0.301528   LR 0.001247
0.84660065
0.84685349
0.84725434
0.84737927
0.84823585
0.84828478
0.84781533
0.84697193
0.84652668
0.84602410
0.84563583
0.84525013
0.84488863
0.84453714
0.84431738
0.84395987
0.84366435
0.84326530
0.84312159
0.84290218
0.84261024
0.84253818
INFO - Training [31][   80/  196]   Loss 0.407884   Top1 85.820312   Top5 98.466797   BatchTime 0.292886   LR 0.001246
0.84233385
0.84251386
0.84191674
0.84168166
0.84174472
0.84177881
0.84162545
0.84160477
0.84182262
0.84190649
0.84169656
0.84226835
0.84389013
0.84377259
0.84368187
INFO - Training [31][  100/  196]   Loss 0.407557   Top1 85.781250   Top5 98.511719   BatchTime 0.287927   LR 0.001246
0.84359944
0.84357071
0.84353054
0.84333861
0.84303242
0.84287775
0.84277260
0.84267443
0.84265566
0.84236592
0.84205526
0.84173787
0.84200531
0.84243828
0.84289032
0.84275132
0.84278232
0.84278196
0.84254950
0.84234971
0.84224588
0.84217483
INFO - Training [31][  120/  196]   Loss 0.402004   Top1 85.963542   Top5 98.541667   BatchTime 0.286471   LR 0.001245
0.84210020
0.84188592
0.84173638
0.84143138
0.84127921
0.84075695
0.83970761
0.83939153
0.83938539
0.83917141
0.83902234
0.83926952
0.83985013
0.83995974
0.84004325
0.84032553
0.84023708
0.84021455
0.84029478
0.84140623
INFO - Training [31][  140/  196]   Loss 0.399657   Top1 86.032366   Top5 98.613281   BatchTime 0.288287   LR 0.001244
0.84323204
0.84321940
0.84319532
0.84371740
0.84396458
0.84418064
0.84433663
0.84542078
0.84748191
0.84820443
0.84856743
0.84877551
0.84899229
0.84953690
0.85023540
0.85161233
0.85578346
0.86165959
0.86765152
0.87215108
0.87608004
INFO - Training [31][  160/  196]   Loss 0.403894   Top1 85.981445   Top5 98.593750   BatchTime 0.288545   LR 0.001244
0.87692016
0.87695199
0.87683994
0.87678540
0.87677687
0.87667823
0.87686628
0.87718022
0.87722307
0.87724388
0.87663668
0.87633717
0.87612885
0.87583411
0.87585002
0.87585139
0.87573761
0.87583029
0.87587351
0.87591660
INFO - Training [31][  180/  196]   Loss 0.404658   Top1 85.996094   Top5 98.561198   BatchTime 0.288890   LR 0.001243
0.87615484
0.87596375
0.87601918
0.87603951
0.87566143
0.87601376
0.87601995
0.87603438
0.87611645
0.87620342
0.87601364
0.87623328
0.87650234
0.87643445
INFO - ==> Top1: 86.038    Top5: 98.556    Loss: 0.404
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.87638813
0.87629062
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [31][   20/   40]   Loss 0.368368   Top1 88.359375   Top5 99.589844   BatchTime 0.128537
INFO - Validation [31][   40/   40]   Loss 0.356529   Top1 88.280000   Top5 99.630000   BatchTime 0.089829
INFO - ==> Top1: 88.280    Top5: 99.630    Loss: 0.357
INFO - ==> Sparsity : 0.338
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.530   Top5: 99.610]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0556)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0579)
features.2.conv.6 tensor(0.0732)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0494)
features.3.conv.6 tensor(0.0428)
features.4.conv.0 tensor(0.0249)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0941)
features.5.conv.0 tensor(0.0273)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.0921)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0532)
features.7.conv.0 tensor(0.0472)
features.7.conv.3 tensor(0.1212)
features.7.conv.6 tensor(0.0913)
features.8.conv.0 tensor(0.0550)
features.8.conv.3 tensor(0.1400)
features.8.conv.6 tensor(0.0954)
features.9.conv.0 tensor(0.0787)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.0999)
features.10.conv.0 tensor(0.0391)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0751)
features.11.conv.0 tensor(0.1155)
features.11.conv.3 tensor(0.1198)
features.11.conv.6 tensor(0.1516)
features.12.conv.0 tensor(0.1364)
features.12.conv.3 tensor(0.1570)
features.12.conv.6 tensor(0.2391)
features.13.conv.0 tensor(0.0660)
features.13.conv.3 tensor(0.1767)
features.13.conv.6 tensor(0.0773)
features.14.conv.0 tensor(0.9062)
features.14.conv.3 tensor(0.1091)
features.14.conv.6 tensor(0.8449)
features.15.conv.0 tensor(0.9438)
features.15.conv.3 tensor(0.0875)
features.15.conv.6 tensor(0.6055)
features.16.conv.0 tensor(0.0770)
features.16.conv.3 tensor(0.1169)
features.16.conv.6 tensor(0.2143)
conv.0 tensor(0.2045)
tensor(740565.) 2188896.0
0.87644798
0.87617797
0.87615818
0.87577969
0.87528247
0.87520510
0.87477368
0.87467164
0.87455231
0.87459475
0.87460846
0.87444609
0.87448150
0.87456799
0.87458366
0.87484866
0.87470865
INFO - Training [32][   20/  196]   Loss 0.413571   Top1 85.371094   Top5 97.949219   BatchTime 0.423497   LR 0.001242
0.87526846
0.87732857
0.87739772
0.87740815
0.87731910
0.87721235
0.87703365
0.87722450
0.87715024
0.87692350
0.87646335
0.87650627
0.87645262
0.87663752
0.87676418
0.87676078
0.87680650
0.87658298
0.87632281
0.87608993
0.87591845
0.87590140
INFO - Training [32][   40/  196]   Loss 0.422680   Top1 85.273438   Top5 98.105469   BatchTime 0.392177   LR 0.001241
0.87579840
0.87561429
0.87562865
0.87624347
0.87636477
0.87646496
0.87647712
0.87672180
0.87659246
0.87651461
0.87633145
0.87617224
0.87607270
0.87583417
0.87578946
0.87559849
0.87512052
0.87554109
0.87524182
0.87509483
0.87503064
0.87446493
0.87352276
INFO - Training [32][   60/  196]   Loss 0.414855   Top1 85.514323   Top5 98.170573   BatchTime 0.380780   LR 0.001240
0.87297034
0.87255675
0.87212610
0.87167603
0.87123626
0.87114936
0.87089902
0.87067425
0.87098843
0.87077737
0.87080389
0.87083721
0.87103426
0.87108445
0.87106043
0.87142462
0.87252557
INFO - Training [32][   80/  196]   Loss 0.413071   Top1 85.595703   Top5 98.310547   BatchTime 0.374663   LR 0.001239
0.87456727
0.87578630
0.87587208
0.87554437
0.87607628
0.87606394
0.87628198
0.87651211
0.87649637
0.87590271
0.87526488
0.87502730
0.87497973
0.87523347
0.87537575
0.87502688
0.87487465
0.87499821
0.87529951
0.87535870
0.87561148
0.87543237
INFO - Training [32][  100/  196]   Loss 0.403117   Top1 86.011719   Top5 98.355469   BatchTime 0.372096   LR 0.001238
0.87497711
0.87454021
0.87441748
0.87443662
0.87456059
0.87565976
0.87578988
0.87575424
0.87559962
0.87551028
0.87560111
0.87553024
0.87527466
0.87503219
0.87496901
0.87489408
INFO - Training [32][  120/  196]   Loss 0.397157   Top1 86.302083   Top5 98.434245   BatchTime 0.370875   LR 0.001237
0.87449223
0.87417877
0.87383962
0.87326699
0.87292528
0.87290275
0.87306762
0.87362796
0.87378651
0.87342858
0.87465149
0.87414354
0.87346029
0.87330854
0.87280136
0.87244457
0.87137437
0.86974549
0.86796725
0.86654991
0.86497825
0.86216736
0.85905176
INFO - Training [32][  140/  196]   Loss 0.398047   Top1 86.266741   Top5 98.515625   BatchTime 0.368347   LR 0.001236
0.85607642
0.85334396
0.84867156
0.84412503
0.83747625
0.83614093
0.83269399
0.82306111
0.82219523
0.82044369
0.82041985
0.82065248
0.82141137
0.82353348
0.82802069
0.83355492
0.83901936
0.84329551
0.84541160
0.84671110
0.84664917
INFO - Training [32][  160/  196]   Loss 0.398288   Top1 86.225586   Top5 98.520508   BatchTime 0.368971   LR 0.001235
0.84667325
0.84748852
0.84721184
0.84703958
0.84702176
0.84705096
0.84670550
0.84630620
0.84657091
0.84617895
0.84572446
0.84561920
0.84547901
0.84529126
0.84522742
0.84405214
0.84375769
0.84377617
INFO - Training [32][  180/  196]   Loss 0.397173   Top1 86.276042   Top5 98.465712   BatchTime 0.365552   LR 0.001234
0.84338570
0.84296197
0.84236640
0.84180528
0.84163386
0.84150243
0.84269851
0.84266454
0.84210920
0.84158874
0.84162688
0.84194189
0.84136796
0.84060425
0.83964342
0.83871323
0.83707958
INFO - ==> Top1: 86.268    Top5: 98.460    Loss: 0.397
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.364934   Top1 88.691406   Top5 99.511719   BatchTime 0.135919
INFO - Validation [32][   40/   40]   Loss 0.346096   Top1 88.660000   Top5 99.600000   BatchTime 0.097341
INFO - ==> Top1: 88.660    Top5: 99.600    Loss: 0.346
INFO - ==> Sparsity : 0.372
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 88.660   Top5: 99.600]
features.0.conv.0 tensor(0.4931)
features.0.conv.3 tensor(0.1758)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0582)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0755)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0486)
features.3.conv.6 tensor(0.0447)
features.4.conv.0 tensor(0.0247)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.0850)
features.5.conv.0 tensor(0.0327)
features.5.conv.3 tensor(0.0775)
features.5.conv.6 tensor(0.0924)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0446)
features.7.conv.3 tensor(0.1282)
features.7.conv.6 tensor(0.0917)
features.8.conv.0 tensor(0.0628)
features.8.conv.3 tensor(0.1374)
features.8.conv.6 tensor(0.0950)
features.9.conv.0 tensor(0.0710)
features.9.conv.3 tensor(0.1479)
features.9.conv.6 tensor(0.1372)
features.10.conv.0 tensor(0.0397)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0738)
features.11.conv.0 tensor(0.1270)
features.11.conv.3 tensor(0.1181)
features.11.conv.6 tensor(0.3972)
features.12.conv.0 tensor(0.1431)
features.12.conv.3 tensor(0.1537)
features.12.conv.6 tensor(0.2059)
features.13.conv.0 tensor(0.0752)
features.13.conv.3 tensor(0.1771)
features.13.conv.6 tensor(0.0815)
features.14.conv.0 tensor(0.9204)
features.14.conv.3 tensor(0.1127)
features.14.conv.6 tensor(0.8770)
features.15.conv.0 tensor(0.9481)
features.15.conv.3 tensor(0.0921)
features.15.conv.6 tensor(0.9542)
features.16.conv.0 tensor(0.0816)
features.16.conv.3 tensor(0.1179)
features.16.conv.6 tensor(0.2156)
conv.0 tensor(0.1974)
tensor(814585.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
0.83758235
0.83796066
0.83794707
0.83776081
0.83710456
0.83596116
0.83496195
0.83372641
0.83343995
0.83492643
0.83597285
0.83770090
0.83991665
0.84142518
0.84232175
0.84335262
0.84553558
INFO - Training [33][   20/  196]   Loss 0.402046   Top1 85.898438   Top5 98.046875   BatchTime 0.395102   LR 0.001232
0.84816337
0.84830350
0.84802121
0.84789228
0.84773296
0.84757537
0.84741247
0.84724963
0.84707856
0.84689558
0.84673804
0.84675997
0.84672505
0.84645319
0.84660393
0.84634876
0.84645718
0.84640366
0.84641176
0.84643710
0.84626871
0.84608549
INFO - Training [33][   40/  196]   Loss 0.407398   Top1 85.830078   Top5 98.076172   BatchTime 0.376114   LR 0.001230
0.84590346
0.84599370
0.84608090
0.84628141
0.84615564
0.84622204
0.84600449
0.84578502
0.84580612
0.84608245
0.84625506
0.84618962
0.84650153
0.84667844
0.84674186
0.84680969
0.84689069
0.84696960
0.84655488
0.84636229
0.84632474
0.84628779
INFO - Training [33][   60/  196]   Loss 0.403045   Top1 86.100260   Top5 98.177083   BatchTime 0.373272   LR 0.001229
0.84634143
0.84629953
0.84622484
0.84609699
0.84613764
0.84632838
0.84641224
0.84649277
0.84644789
0.84657705
0.84653306
0.84656286
0.84637994
0.84635454
0.84611702
0.84600091
0.84585214
INFO - Training [33][   80/  196]   Loss 0.402587   Top1 86.088867   Top5 98.300781   BatchTime 0.366119   LR 0.001228
0.84579265
0.84592754
0.84527010
0.84465134
0.84452713
0.84432268
0.84407669
0.84435230
0.84438491
0.84431112
0.84438705
0.84478539
0.84477419
0.84464735
0.84436983
0.84394222
0.84327996
0.84249169
0.84138274
0.84072703
0.83945644
0.84079516
0.84183782
INFO - Training [33][  100/  196]   Loss 0.399713   Top1 86.210938   Top5 98.332031   BatchTime 0.363015   LR 0.001226
0.84299523
0.84583080
0.84627050
0.84779215
0.84824806
0.84823275
0.84785825
0.84773052
0.84914565
0.84875607
0.84870547
0.84886843
0.84870988
0.84847128
0.84797502
0.84760457
INFO - Training [33][  120/  196]   Loss 0.393446   Top1 86.529948   Top5 98.460286   BatchTime 0.364374   LR 0.001225
0.84745371
0.84733033
0.84693754
0.84637475
0.84623182
0.84614438
0.84625238
0.84614521
0.84592891
0.84578985
0.84594089
0.84574562
0.84576195
0.84585637
0.84580535
0.84562808
0.84553927
0.84551871
0.84551722
0.84581429
0.84588391
0.84593225
0.84618962
INFO - Training [33][  140/  196]   Loss 0.391049   Top1 86.612723   Top5 98.532366   BatchTime 0.364055   LR 0.001224
0.84630448
0.84790349
0.84842807
0.84844905
0.84851992
0.84830636
0.84801549
0.84825188
0.84767330
0.84734392
0.84645569
0.84590530
0.84595037
0.84592587
0.84670287
0.84713566
0.84707218
0.84692305
0.84690851
0.84697700
0.84703422
INFO - Training [33][  160/  196]   Loss 0.393424   Top1 86.530762   Top5 98.530273   BatchTime 0.364899   LR 0.001222
0.84686601
0.84658217
0.84614396
0.84597731
0.84634137
0.84588689
0.84579206
0.84559900
0.84550363
0.84438866
0.84150314
0.84325349
0.84401649
0.84332401
0.84281451
0.84241241
INFO - Training [33][  180/  196]   Loss 0.394453   Top1 86.451823   Top5 98.480903   BatchTime 0.365978   LR 0.001221
0.84231049
0.84302908
0.84397310
0.84442228
0.84573466
0.84811276
0.84778458
0.84752136
0.84748334
0.84758359
0.84781224
0.84786874
0.84786999
0.84813654
0.84832770
0.84843910
0.84834921
INFO - ==> Top1: 86.462    Top5: 98.496    Loss: 0.395
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84837925
0.84848630
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 0.348119   Top1 88.750000   Top5 99.609375   BatchTime 0.139078
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0538)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0752)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0438)
features.4.conv.0 tensor(0.0260)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0811)
features.5.conv.0 tensor(0.0339)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.0897)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0524)
features.7.conv.0 tensor(0.0453)
features.7.conv.3 tensor(0.1296)
features.7.conv.6 tensor(0.0894)
features.8.conv.0 tensor(0.0518)
features.8.conv.3 tensor(0.1395)
features.8.conv.6 tensor(0.0958)
features.9.conv.0 tensor(0.0818)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.1040)
features.10.conv.0 tensor(0.0350)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0750)
features.11.conv.0 tensor(0.1346)
features.11.conv.3 tensor(0.1223)
features.11.conv.6 tensor(0.1580)
features.12.conv.0 tensor(0.1317)
features.12.conv.3 tensor(0.1547)
features.12.conv.6 tensor(0.2142)
features.13.conv.0 tensor(0.0705)
features.13.conv.3 tensor(0.1750)
features.13.conv.6 tensor(0.0851)
features.14.conv.0 tensor(0.9392)
features.14.conv.3 tensor(0.1150)
features.14.conv.6 tensor(0.8900)
features.15.conv.0 tensor(0.9518)
features.15.conv.3 tensor(0.0905)
features.15.conv.6 tensor(0.9483)
features.16.conv.0 tensor(0.0786)
features.16.conv.3 tensor(0.1223)
features.16.conv.6 tensor(0.2096)
conv.0 tensor(0.2033)
tensor(805426.) 2188896.0
INFO - Validation [33][   40/   40]   Loss 0.338657   Top1 88.590000   Top5 99.640000   BatchTime 0.118526
INFO - ==> Top1: 88.590    Top5: 99.640    Loss: 0.339
INFO - ==> Sparsity : 0.368
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 88.660   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
0.84836566
0.84822971
0.84834439
0.84837174
0.84823203
0.84842098
0.84854472
0.84865534
0.84891045
0.84918481
0.84955978
0.84967810
0.84946841
0.84879833
0.84873962
0.84844232
0.84804785
0.84791881
0.84794897
0.84762305
0.84765428
INFO - Training [34][   20/  196]   Loss 0.399549   Top1 85.390625   Top5 98.183594   BatchTime 0.388171   LR 0.001218
0.84700340
0.84668255
0.84629488
0.84630239
0.84567916
0.84555894
0.84515721
0.84577239
0.84593117
0.84564465
0.84559220
0.84556353
0.84524906
0.84482145
0.84471005
0.84501195
0.84525269
0.84556997
0.84573275
INFO - Training [34][   40/  196]   Loss 0.403616   Top1 85.556641   Top5 98.417969   BatchTime 0.349022   LR 0.001216
0.84597701
0.84601617
0.84630799
0.84633994
0.84632331
0.84622121
0.84618545
0.84663302
0.84698278
0.84732586
0.84750986
0.84784228
0.84723634
0.84686643
0.84652299
0.84617859
0.84565324
INFO - Training [34][   60/  196]   Loss 0.402575   Top1 85.833333   Top5 98.496094   BatchTime 0.353814   LR 0.001215
0.84536362
0.84521616
0.84524727
0.84491086
0.84403443
0.84307981
0.84183079
0.84023499
0.83662593
0.83828121
0.84155452
0.84326655
0.84315997
0.84311587
0.84249675
0.84188747
0.84085894
0.83945018
0.83901876
0.83824617
0.83768708
0.83747435
INFO - Training [34][   80/  196]   Loss 0.406363   Top1 85.830078   Top5 98.525391   BatchTime 0.356832   LR 0.001213
0.83813578
0.83874178
0.83973831
0.83935809
0.83894026
0.83871919
0.83848494
0.83827573
0.83789253
0.83802247
0.83786535
0.83741963
0.83683503
0.83639038
0.83601344
0.83568299
0.83551282
0.83537787
0.83565217
0.83539176
0.83504063
0.83488727
INFO - Training [34][  100/  196]   Loss 0.411210   Top1 85.625000   Top5 98.496094   BatchTime 0.358826   LR 0.001211
0.83460438
0.83374739
0.83313560
0.83251476
0.83258408
0.83265978
0.83301437
0.83309120
0.83362794
0.83499354
0.83549315
0.83566928
0.83558744
0.83560479
0.83554965
0.83593035
INFO - Training [34][  120/  196]   Loss 0.411381   Top1 85.703125   Top5 98.538411   BatchTime 0.358610   LR 0.001209
0.83655250
0.83715677
0.83781224
0.83900768
0.84118301
0.84111631
0.84111434
0.84136695
0.84175527
0.84178060
0.84179658
0.84172553
0.84180754
0.84185243
0.84177357
0.84190840
0.84190786
0.84177881
0.84191936
0.84152544
0.84126657
0.84162867
0.84171408
INFO - Training [34][  140/  196]   Loss 0.412245   Top1 85.658482   Top5 98.588170   BatchTime 0.359204   LR 0.001208
0.84176886
0.84186423
0.84178406
0.84194183
0.84188873
0.84174067
0.84194738
0.84304076
0.84443462
0.84451991
0.84459496
0.84507626
0.84552050
0.84569657
0.84586716
0.84569603
0.84591538
0.84534979
0.84498262
0.84495443
0.84490800
0.84492588
INFO - Training [34][  160/  196]   Loss 0.413464   Top1 85.656738   Top5 98.566895   BatchTime 0.359590   LR 0.001206
0.84518218
0.84527826
0.84532404
0.84499002
0.84616166
0.84597415
0.84585696
0.84568042
0.84569496
0.84530234
0.84516960
0.84492326
0.84477437
0.84457362
0.84429568
0.84380919
INFO - Training [34][  180/  196]   Loss 0.413330   Top1 85.611979   Top5 98.493924   BatchTime 0.359752   LR 0.001204
0.84346288
0.84288520
0.84276479
0.84254009
0.84239960
0.84257412
0.84249204
0.84318709
0.84334838
0.84332556
0.84336305
0.84327483
0.84348315
0.84367371
0.84384471
0.84382087
0.84384674
0.84355730
INFO - ==> Top1: 85.680    Top5: 98.480    Loss: 0.412
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.372942   Top1 87.734375   Top5 99.375000   BatchTime 0.120474
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0972)
features.1.conv.6 tensor(0.0560)
features.2.conv.0 tensor(0.0191)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0692)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0509)
features.3.conv.6 tensor(0.0436)
features.4.conv.0 tensor(0.0329)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0827)
features.5.conv.0 tensor(0.0363)
features.5.conv.3 tensor(0.0810)
features.5.conv.6 tensor(0.0885)
features.6.conv.0 tensor(0.0200)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0505)
features.7.conv.0 tensor(0.0405)
features.7.conv.3 tensor(0.1322)
features.7.conv.6 tensor(0.0904)
features.8.conv.0 tensor(0.0506)
features.8.conv.3 tensor(0.1380)
features.8.conv.6 tensor(0.0963)
features.9.conv.0 tensor(0.0809)
features.9.conv.3 tensor(0.1548)
features.9.conv.6 tensor(0.1106)
features.10.conv.0 tensor(0.0394)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0731)
features.11.conv.0 tensor(0.1416)
features.11.conv.3 tensor(0.1186)
features.11.conv.6 tensor(0.1777)
features.12.conv.0 tensor(0.1395)
features.12.conv.3 tensor(0.1613)
features.12.conv.6 tensor(0.2123)
features.13.conv.0 tensor(0.0694)
features.13.conv.3 tensor(0.1779)
features.13.conv.6 tensor(0.0836)
features.14.conv.0 tensor(0.9276)
features.14.conv.3 tensor(0.1164)
features.14.conv.6 tensor(0.9009)
features.15.conv.0 tensor(0.9511)
features.15.conv.3 tensor(0.0913)
features.15.conv.6 tensor(0.9563)
features.16.conv.0 tensor(0.0799)
features.16.conv.3 tensor(0.1236)
features.16.conv.6 tensor(0.2159)
conv.0 tensor(0.2510)
tensor(829959.) 2188896.0
INFO - Validation [34][   40/   40]   Loss 0.360539   Top1 87.810000   Top5 99.560000   BatchTime 0.087519
INFO - ==> Top1: 87.810    Top5: 99.560    Loss: 0.361
INFO - ==> Sparsity : 0.379
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 88.660   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
0.84389931
0.84393895
0.84415239
0.84438950
0.84497172
0.84621018
0.84619176
0.84608310
0.84607577
0.84605461
0.84614670
0.84634411
0.84635645
0.84637409
0.84659195
0.84679908
0.84665287
0.84676933
0.84691757
0.84685552
0.84691340
INFO - Training [35][   20/  196]   Loss 0.417832   Top1 85.332031   Top5 98.242188   BatchTime 0.399951   LR 0.001201
0.84686023
0.84685439
0.84669393
0.84675771
0.84703767
0.84664750
0.84637916
0.84631014
0.84618747
0.84542745
0.84480357
0.84455657
0.84464699
0.84474319
0.84460723
0.84454590
0.84545529
0.84674740
0.84683239
0.84706843
INFO - Training [35][   40/  196]   Loss 0.420306   Top1 85.390625   Top5 98.291016   BatchTime 0.351661   LR 0.001199
0.84701389
0.84691101
0.84684736
0.84702235
0.84696519
0.84702200
0.84697205
0.84689426
0.84682977
0.84669518
0.84650409
0.84647918
0.84628230
0.84622055
0.84576797
0.84566903
0.84568322
0.84551501
0.84525353
INFO - Training [35][   60/  196]   Loss 0.416171   Top1 85.390625   Top5 98.378906   BatchTime 0.337717   LR 0.001197
0.84523052
0.84523225
0.84509718
0.84492558
0.84477043
0.84471756
0.84422052
0.84365249
0.84337240
0.84330970
0.84327191
0.84339523
0.84342754
0.84365326
0.84358746
0.84357303
0.84343761
INFO - Training [35][   80/  196]   Loss 0.409423   Top1 85.810547   Top5 98.457031   BatchTime 0.341387   LR 0.001195
0.84338266
0.84307081
0.84336501
0.84292865
0.84260511
0.84254450
0.84248590
0.84242636
0.84245247
0.84260541
0.84276617
0.84348655
0.84500885
0.84541953
0.84506238
0.84509647
0.84509951
0.84495002
0.84469926
0.84474266
0.84481484
0.84488249
INFO - Training [35][  100/  196]   Loss 0.401049   Top1 86.089844   Top5 98.531250   BatchTime 0.345470   LR 0.001192
0.84509951
0.84511358
0.84528184
0.84533376
0.84549576
0.84674925
0.84711128
0.84708440
0.84693146
0.84674966
0.84658164
0.84655672
0.84629494
0.84608775
0.84639788
0.84643275
0.84624815
0.84643751
0.84644371
0.84618276
0.84601521
0.84609860
0.84597653
INFO - Training [35][  120/  196]   Loss 0.395299   Top1 86.279297   Top5 98.606771   BatchTime 0.348637   LR 0.001190
0.84592372
0.84591168
0.84588182
0.84570068
0.84548330
0.84525782
0.84516525
0.84521538
0.84519726
0.84500796
0.84482384
0.84473765
0.84469378
0.84403646
0.84395200
INFO - Training [35][  140/  196]   Loss 0.393022   Top1 86.445312   Top5 98.660714   BatchTime 0.352025   LR 0.001188
0.84366530
0.84272373
0.84248126
0.84216028
0.84267193
0.84347463
0.84324652
0.84323299
0.84288788
0.84258842
0.84263968
0.84236091
0.84212238
0.84176165
0.84160519
0.84187275
0.84203303
0.84169936
0.84128088
0.84093648
0.84039420
0.84000289
0.83984172
INFO - Training [35][  160/  196]   Loss 0.396165   Top1 86.352539   Top5 98.623047   BatchTime 0.353214   LR 0.001186
0.83971733
0.83950973
0.83955640
0.83951104
0.83941895
0.83929485
0.83939213
0.83948427
0.83985013
0.84071338
0.84024537
0.83991408
0.83992577
0.84006548
0.84038609
0.84069508
0.84086251
0.84091687
0.84108657
0.84111911
0.84136850
0.84139019
INFO - Training [35][  180/  196]   Loss 0.395087   Top1 86.365017   Top5 98.569878   BatchTime 0.354406   LR 0.001184
0.84119391
0.84124726
0.84121466
0.84107471
0.84098637
0.84095836
0.84092194
0.84091437
0.84109914
0.84100366
0.84075660
INFO - ==> Top1: 86.398    Top5: 98.588    Loss: 0.394
0.84001875
0.83944857
0.83883458
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.377389   Top1 87.675781   Top5 99.296875   BatchTime 0.124645
INFO - Validation [35][   40/   40]   Loss 0.375293   Top1 87.540000   Top5 99.420000   BatchTime 0.087930
INFO - ==> Top1: 87.540    Top5: 99.420    Loss: 0.375
INFO - ==> Sparsity : 0.372
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 88.660   Top5: 99.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.1914)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0551)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0709)
features.3.conv.0 tensor(0.0159)
features.3.conv.3 tensor(0.0517)
features.3.conv.6 tensor(0.0414)
features.4.conv.0 tensor(0.0334)
features.4.conv.3 tensor(0.0978)
features.4.conv.6 tensor(0.0853)
features.5.conv.0 tensor(0.0309)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0887)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0453)
features.7.conv.0 tensor(0.0361)
features.7.conv.3 tensor(0.1311)
features.7.conv.6 tensor(0.0897)
features.8.conv.0 tensor(0.0540)
features.8.conv.3 tensor(0.1377)
features.8.conv.6 tensor(0.0933)
features.9.conv.0 tensor(0.0817)
features.9.conv.3 tensor(0.1528)
features.9.conv.6 tensor(0.1099)
features.10.conv.0 tensor(0.0385)
features.10.conv.3 tensor(0.1062)
features.10.conv.6 tensor(0.0730)
features.11.conv.0 tensor(0.1441)
features.11.conv.3 tensor(0.1206)
features.11.conv.6 tensor(0.2411)
features.12.conv.0 tensor(0.1663)
features.12.conv.3 tensor(0.1626)
features.12.conv.6 tensor(0.2069)
features.13.conv.0 tensor(0.0679)
features.13.conv.3 tensor(0.1784)
features.13.conv.6 tensor(0.0835)
features.14.conv.0 tensor(0.9330)
features.14.conv.3 tensor(0.1159)
features.14.conv.6 tensor(0.9077)
features.15.conv.0 tensor(0.9558)
features.15.conv.3 tensor(0.0909)
features.15.conv.6 tensor(0.9521)
features.16.conv.0 tensor(0.0873)
features.16.conv.3 tensor(0.1230)
features.16.conv.6 tensor(0.1976)
conv.0 tensor(0.2099)
tensor(815099.) 2188896.0
0.83982521
0.84130162
0.84124243
0.84129351
0.84132713
0.84100354
0.84065509
0.83978134
0.83945239
0.83918279
0.83893543
0.83888912
0.83875126
0.83876848
0.83863705
0.83903134
0.83920836
0.83924115
0.83933115
0.83919424
0.83921021
0.83906549
INFO - Training [36][   20/  196]   Loss 0.420979   Top1 85.468750   Top5 97.851562   BatchTime 0.424563   LR 0.001180
0.83901536
0.83911318
0.83897161
0.83895087
0.83902365
0.83896923
0.83891147
0.83885342
0.83876318
0.83875328
0.83824158
0.83790028
0.83801013
0.83833325
0.83854979
0.83852094
0.83909720
INFO - Training [36][   40/  196]   Loss 0.407000   Top1 86.113281   Top5 98.076172   BatchTime 0.388575   LR 0.001177
0.84078974
0.84119618
0.84126049
0.84125292
0.84131223
0.84141999
0.84140611
0.84141439
0.84144235
0.84136927
0.84128958
0.84055847
0.84031129
0.83962822
0.83915496
0.83872563
0.84011501
0.84415239
0.84443021
0.84434974
0.84421480
0.84401077
0.84390831
0.84401411
INFO - Training [36][   60/  196]   Loss 0.397099   Top1 86.425781   Top5 98.216146   BatchTime 0.374303   LR 0.001175
0.84413642
0.84410369
0.84444070
0.84523952
0.84520173
0.84524810
0.84509897
0.84494948
0.84483474
0.84473169
0.84471345
0.84487689
0.84481329
0.84474933
0.84466475
INFO - Training [36][   80/  196]   Loss 0.393759   Top1 86.523438   Top5 98.349609   BatchTime 0.349729   LR 0.001173
0.84467816
0.84464604
0.84455663
0.84442937
0.84444875
0.84448224
0.84425390
0.84402370
0.84392321
0.84369057
0.84373158
0.84376281
0.84351134
0.84387326
0.84379673
0.84370315
0.84353679
0.84331226
0.84295541
0.84264964
0.84275585
0.84261751
0.84247971
INFO - Training [36][  100/  196]   Loss 0.385271   Top1 86.832031   Top5 98.398438   BatchTime 0.351310   LR 0.001170
0.84266973
0.84305918
0.84282458
0.84261888
0.84279561
0.84308404
0.84312570
0.84320408
0.84284550
0.84272993
0.84285355
0.84272146
0.84263635
0.84255809
0.84237087
0.84231108
0.84259307
INFO - Training [36][  120/  196]   Loss 0.380487   Top1 86.998698   Top5 98.525391   BatchTime 0.351330   LR 0.001168
0.84283429
0.84289181
0.84300756
0.84296060
0.84267032
0.84240431
0.84267861
0.84279579
0.84325838
0.84577423
0.84579343
0.84503996
0.84469539
0.84375024
0.84269559
0.84034419
0.84047729
0.84182638
0.84325635
0.84407151
INFO - Training [36][  140/  196]   Loss 0.377623   Top1 87.064732   Top5 98.571429   BatchTime 0.357267   LR 0.001165
0.84475273
0.84421730
0.84361482
0.84225529
0.84119755
0.83996165
0.83550870
0.83633476
0.83721852
0.83690184
0.83811766
0.83983588
0.84172875
0.84304786
0.84417641
0.84454203
0.84411734
0.84390855
0.84403092
0.84429306
0.84380496
0.84335566
INFO - Training [36][  160/  196]   Loss 0.377935   Top1 86.994629   Top5 98.576660   BatchTime 0.358650   LR 0.001163
0.84366935
0.84334648
0.84372848
0.84423071
0.84439123
0.84587455
0.84569573
0.84520012
0.84470814
0.84424973
0.84424204
0.84391987
0.84325892
0.84299463
0.84355050
0.84451711
0.84483528
0.84530032
0.84737062
0.84703130
0.84691387
INFO - Training [36][  180/  196]   Loss 0.380090   Top1 86.896701   Top5 98.509115   BatchTime 0.360628   LR 0.001160
0.84716713
0.84713221
0.84762627
0.84777206
0.84746945
0.84735554
0.84755075
0.84796143
0.84776825
0.84761506
0.84757066
0.84758031
0.84754568
0.84778875
0.84785974
********************pre-trained*****************
INFO - ==> Top1: 86.980    Top5: 98.536    Loss: 0.378
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 0.353628   Top1 88.320312   Top5 99.472656   BatchTime 0.128746
INFO - Validation [36][   40/   40]   Loss 0.340029   Top1 88.830000   Top5 99.570000   BatchTime 0.090906
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0586)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0540)
features.2.conv.6 tensor(0.0666)
features.3.conv.0 tensor(0.0156)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0430)
features.4.conv.0 tensor(0.0322)
features.4.conv.3 tensor(0.1030)
features.4.conv.6 tensor(0.0881)
features.5.conv.0 tensor(0.0295)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.0853)
features.6.conv.0 tensor(0.0260)
features.6.conv.3 tensor(0.0509)
features.6.conv.6 tensor(0.0453)
features.7.conv.0 tensor(0.0416)
features.7.conv.3 tensor(0.1293)
features.7.conv.6 tensor(0.0890)
features.8.conv.0 tensor(0.0504)
features.8.conv.3 tensor(0.1418)
features.8.conv.6 tensor(0.0940)
features.9.conv.0 tensor(0.0827)
features.9.conv.3 tensor(0.1534)
features.9.conv.6 tensor(0.1010)
features.10.conv.0 tensor(0.0354)
features.10.conv.3 tensor(0.1091)
features.10.conv.6 tensor(0.0720)
features.11.conv.0 tensor(0.1328)
features.11.conv.3 tensor(0.1229)
features.11.conv.6 tensor(0.1190)
features.12.conv.0 tensor(0.1226)
features.12.conv.3 tensor(0.1628)
features.12.conv.6 tensor(0.2173)
features.13.conv.0 tensor(0.0646)
features.13.conv.3 tensor(0.1763)
features.13.conv.6 tensor(0.0929)
features.14.conv.0 tensor(0.9358)
features.14.conv.3 tensor(0.1125)
features.14.conv.6 tensor(0.8960)
features.15.conv.0 tensor(0.9562)
features.15.conv.3 tensor(0.0951)
features.15.conv.6 tensor(0.9573)
features.16.conv.0 tensor(0.0865)
features.16.conv.3 tensor(0.1231)
features.16.conv.6 tensor(0.1806)
conv.0 tensor(0.2023)
tensor(797352.) 2188896.0
INFO - ==> Top1: 88.830    Top5: 99.570    Loss: 0.340
INFO - ==> Sparsity : 0.364
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [36][Top1: 88.830   Top5: 99.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
0.84775072
0.84765106
0.84716672
0.84722823
0.84728849
0.84717953
0.84707427
0.84717137
0.84712040
0.84712106
0.84719312
0.84690416
0.84692103
0.84678304
0.84661543
0.84620553
0.84615797
0.84614682
0.84582579
0.84557641
0.84491605
INFO - Training [37][   20/  196]   Loss 0.395506   Top1 86.191406   Top5 97.910156   BatchTime 0.419303   LR 0.001155
0.84465706
0.84346771
0.84184736
0.84095603
0.83636463
0.83898759
0.84236288
0.84487009
0.84703481
0.84681857
0.84660912
0.84654051
0.84638876
0.84639996
0.84624344
0.84590137
0.84567636
INFO - Training [37][   40/  196]   Loss 0.396470   Top1 86.328125   Top5 98.164062   BatchTime 0.386099   LR 0.001153
0.84523886
0.84527069
0.84568137
0.84564716
0.84552521
0.84561151
0.84550053
0.84537506
0.84533840
0.84531325
0.84605587
0.84613806
0.84598696
0.84578252
0.84560138
0.84554487
0.84518468
0.84500402
0.84492296
0.84497786
0.84484273
INFO - Training [37][   60/  196]   Loss 0.390601   Top1 86.406250   Top5 98.281250   BatchTime 0.381318   LR 0.001150
0.84455246
0.84445626
0.84473717
0.84431326
0.84360808
0.84350735
0.84335792
0.84336245
0.84314775
0.84323877
0.84302133
0.84267002
0.84237397
0.84244472
0.84255803
0.84235114
0.84202999
0.84187943
0.84193921
INFO - Training [37][   80/  196]   Loss 0.386073   Top1 86.577148   Top5 98.408203   BatchTime 0.364896   LR 0.001147
0.84203815
0.84179413
0.84179330
0.84164453
0.84151202
0.84156412
0.84140974
0.84114152
0.84112209
0.84114569
0.84172970
0.84337980
0.84294045
0.84271020
0.84245133
0.84223127
0.84209782
0.84232813
0.84249526
0.84235841
0.84214479
0.84221107
INFO - Training [37][  100/  196]   Loss 0.382257   Top1 86.628906   Top5 98.472656   BatchTime 0.345771   LR 0.001144
0.84248012
0.84267503
0.84231555
0.84215873
0.84211969
0.84134239
0.84010845
0.84113842
0.84097993
0.83967620
0.84096187
0.84158731
0.84136850
0.84117919
0.84084755
0.84091240
0.84067750
0.84101695
INFO - Training [37][  120/  196]   Loss 0.380017   Top1 86.806641   Top5 98.538411   BatchTime 0.345954   LR 0.001142
0.84104592
0.84111750
0.84122878
0.84130079
0.84097379
0.84075689
0.84079039
0.84074008
0.84079581
0.84098202
0.84084904
0.84092534
0.84119260
0.84157091
0.84309161
0.84614474
0.84583890
0.84619904
0.84613383
0.84622234
0.84612375
0.84672177
INFO - Training [37][  140/  196]   Loss 0.378233   Top1 86.947545   Top5 98.593750   BatchTime 0.348379   LR 0.001139
0.84730577
0.84863549
0.84883481
0.84862363
0.84839499
0.84834814
0.84843642
0.84869659
0.84851032
0.84852809
0.84829873
0.84813416
0.84813231
0.84813267
0.84801191
0.84796870
0.84806430
0.84820437
0.84839422
0.84836322
0.84803659
0.84787762
INFO - Training [37][  160/  196]   Loss 0.378473   Top1 86.896973   Top5 98.610840   BatchTime 0.350512   LR 0.001136
0.84771395
0.84792024
0.84776121
0.84767556
0.84771824
0.84780270
0.84785724
0.84807277
0.84785980
0.84761465
0.84746033
0.84754092
0.84727901
0.84712726
0.84721446
0.84728080
INFO - Training [37][  180/  196]   Loss 0.379548   Top1 86.805556   Top5 98.559028   BatchTime 0.352269   LR 0.001133
0.84739798
0.84707791
0.84700394
0.84696001
0.84694368
0.84697676
0.84705383
0.84696847
0.84708613
0.84715271
0.84711885
0.84710622
0.84712398
0.84745497
0.84721148
0.84690762
0.84689289
0.84649110
********************pre-trained*****************
INFO - ==> Top1: 86.836    Top5: 98.568    Loss: 0.380
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [37][   20/   40]   Loss 0.348568   Top1 88.515625   Top5 99.511719   BatchTime 0.129474
INFO - Validation [37][   40/   40]   Loss 0.334193   Top1 88.830000   Top5 99.640000   BatchTime 0.093071
INFO - ==> Top1: 88.830    Top5: 99.640    Loss: 0.334
INFO - ==> Sparsity : 0.364
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 88.830   Top5: 99.640]
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1855)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0525)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0674)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0393)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.1053)
features.4.conv.6 tensor(0.0845)
features.5.conv.0 tensor(0.0347)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.0866)
features.6.conv.0 tensor(0.0187)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0460)
features.7.conv.0 tensor(0.0538)
features.7.conv.3 tensor(0.1270)
features.7.conv.6 tensor(0.0917)
features.8.conv.0 tensor(0.0528)
features.8.conv.3 tensor(0.1395)
features.8.conv.6 tensor(0.0921)
features.9.conv.0 tensor(0.0785)
features.9.conv.3 tensor(0.1522)
features.9.conv.6 tensor(0.0998)
features.10.conv.0 tensor(0.0369)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0697)
features.11.conv.0 tensor(0.1401)
features.11.conv.3 tensor(0.1209)
features.11.conv.6 tensor(0.1219)
features.12.conv.0 tensor(0.1193)
features.12.conv.3 tensor(0.1680)
features.12.conv.6 tensor(0.2142)
features.13.conv.0 tensor(0.0664)
features.13.conv.3 tensor(0.1738)
features.13.conv.6 tensor(0.0886)
features.14.conv.0 tensor(0.9372)
features.14.conv.3 tensor(0.1133)
features.14.conv.6 tensor(0.9062)
features.15.conv.0 tensor(0.9549)
features.15.conv.3 tensor(0.0944)
features.15.conv.6 tensor(0.9393)
features.16.conv.0 tensor(0.0886)
features.16.conv.3 tensor(0.1241)
features.16.conv.6 tensor(0.1731)
conv.0 tensor(0.2069)
tensor(796125.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
0.84651798
0.84679461
0.84685093
0.84685904
0.84668303
0.84669036
0.84656674
0.84651196
0.84666181
0.84671360
0.84639347
0.84627759
0.84627616
0.84585786
0.84571749
0.84568101
0.84565252
0.84560752
0.84555030
INFO - Training [38][   20/  196]   Loss 0.390046   Top1 86.035156   Top5 98.242188   BatchTime 0.437433   LR 0.001128
0.84552395
0.84540367
0.84530979
0.84521240
0.84494543
0.84494460
0.84519118
0.84508818
0.84497380
0.84518933
0.84513170
0.84488922
0.84475017
0.84465420
0.84460634
0.84450847
0.84437943
0.84441209
0.84464258
0.84415740
0.84453255
INFO - Training [38][   40/  196]   Loss 0.387382   Top1 86.533203   Top5 98.447266   BatchTime 0.404651   LR 0.001125
0.84430391
0.84397203
0.84392613
0.84368569
0.84324634
0.84275824
0.84242064
0.84257245
0.84288800
0.84262323
0.84241986
0.84185970
0.84102774
0.84049809
0.84025121
0.84012705
0.83933336
INFO - Training [38][   60/  196]   Loss 0.383548   Top1 86.582031   Top5 98.561198   BatchTime 0.391159   LR 0.001122
0.83909017
0.83862060
0.83839184
0.83805519
0.83693302
0.83639705
0.83558118
0.83280218
0.83236843
0.82824397
0.82306200
0.81792533
0.81708616
0.81479597
0.81492257
0.81384343
0.81486923
0.81618404
0.81586230
0.81525528
0.81547081
0.81571716
INFO - Training [38][   80/  196]   Loss 0.383493   Top1 86.591797   Top5 98.613281   BatchTime 0.381842   LR 0.001119
0.81674838
0.81542981
0.81452137
0.81429040
0.81497377
0.81596613
0.81632614
0.81656742
0.81718427
0.81989068
0.81989944
0.82007408
0.82018620
0.82038981
0.82067025
0.82088208
0.82111269
0.82115132
0.82129800
INFO - Training [38][  100/  196]   Loss 0.378842   Top1 86.824219   Top5 98.671875   BatchTime 0.369640   LR 0.001116
0.82119709
0.82096040
0.82114184
0.82113504
0.82098991
0.82100898
0.82127756
0.82183605
0.82216090
0.82251155
0.82293087
0.82299346
0.82317811
0.82428890
0.82633787
0.82702309
0.82798612
0.82903874
0.83108145
0.83139527
0.83137959
INFO - Training [38][  120/  196]   Loss 0.372640   Top1 87.067057   Top5 98.750000   BatchTime 0.355099   LR 0.001112
0.83175331
0.83225471
0.83261180
0.83288854
0.83296406
0.83339357
0.83385253
0.83416110
0.83432251
0.83425224
0.83440763
0.83444804
0.83475518
0.83459681
0.83406353
0.83448577
0.83480418
INFO - Training [38][  140/  196]   Loss 0.368530   Top1 87.223772   Top5 98.814174   BatchTime 0.355553   LR 0.001109
0.83489496
0.83509761
0.83501101
0.83415425
0.83452445
0.83466542
0.83502728
0.83619106
0.83605653
0.83604431
0.83628154
0.83638847
0.83768243
0.83798003
0.83807802
0.83824807
0.83798748
0.83792126
0.83813626
0.83842677
0.83874983
0.83840132
INFO - Training [38][  160/  196]   Loss 0.372796   Top1 87.036133   Top5 98.769531   BatchTime 0.356657   LR 0.001106
0.83862281
0.83874059
0.83879280
0.83848518
0.83804852
0.83777416
0.83759075
0.83749402
0.83759856
0.83742994
0.83722144
0.83719438
0.83696109
0.83655375
0.83634341
0.83615160
0.83594257
0.83533055
0.83467203
0.83463591
0.83510095
0.83574909
INFO - Training [38][  180/  196]   Loss 0.375821   Top1 86.946615   Top5 98.704427   BatchTime 0.357391   LR 0.001103
0.83610684
0.83616155
0.83621043
0.83620399
0.83626902
0.83616430
0.83618230
0.83619171
0.83621264
0.83591318
0.83567214
0.83546805
0.83521038
0.83518678
0.83538395
0.83504039
INFO - ==> Top1: 86.940    Top5: 98.706    Loss: 0.375
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [38][   20/   40]   Loss 0.334242   Top1 89.570312   Top5 99.589844   BatchTime 0.131847
INFO - Validation [38][   40/   40]   Loss 0.316463   Top1 89.780000   Top5 99.690000   BatchTime 0.093076
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0534)
features.2.conv.0 tensor(0.0197)
features.2.conv.3 tensor(0.0525)
features.2.conv.6 tensor(0.0689)
features.3.conv.0 tensor(0.0139)
features.3.conv.3 tensor(0.0463)
features.3.conv.6 tensor(0.0443)
features.4.conv.0 tensor(0.0256)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.0832)
features.5.conv.0 tensor(0.0355)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0864)
features.6.conv.0 tensor(0.0177)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0470)
features.7.conv.0 tensor(0.0435)
features.7.conv.3 tensor(0.1230)
features.7.conv.6 tensor(0.1038)
features.8.conv.0 tensor(0.0538)
features.8.conv.3 tensor(0.1374)
features.8.conv.6 tensor(0.0949)
features.9.conv.0 tensor(0.0739)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.1056)
features.10.conv.0 tensor(0.0373)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.0706)
features.11.conv.0 tensor(0.1340)
features.11.conv.3 tensor(0.1208)
features.11.conv.6 tensor(0.1238)
features.12.conv.0 tensor(0.3658)
features.12.conv.3 tensor(0.1800)
features.12.conv.6 tensor(0.2069)
features.13.conv.0 tensor(0.0688)
features.13.conv.3 tensor(0.1721)
features.13.conv.6 tensor(0.0916)
features.14.conv.0 tensor(0.9348)
features.14.conv.3 tensor(0.1127)
features.14.conv.6 tensor(0.9390)
features.15.conv.0 tensor(0.9583)
features.15.conv.3 tensor(0.0927)
features.15.conv.6 tensor(0.9558)
features.16.conv.0 tensor(0.0899)
features.16.conv.3 tensor(0.1263)
features.16.conv.6 tensor(0.1669)
conv.0 tensor(0.1998)
tensor(812868.) 2188896.0
INFO - ==> Top1: 89.780    Top5: 99.690    Loss: 0.316
INFO - ==> Sparsity : 0.371
INFO - Scoreboard best 1 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
0.83496600
0.83503091
0.83471954
0.83441025
0.83409637
0.83360755
0.83350348
0.83382159
0.83366203
0.83350599
0.83374286
0.83474755
0.83440155
0.83423674
0.83428782
0.83437383
0.83461034
0.83486241
INFO - Training [39][   20/  196]   Loss 0.391395   Top1 86.367188   Top5 98.183594   BatchTime 0.439408   LR 0.001097
0.83475691
0.83471388
0.83451450
0.83437973
0.83429819
0.83410192
0.83402085
0.83382058
0.83408129
0.83379197
0.83367342
0.83318955
0.83283323
0.83215278
0.83105767
0.83040720
0.82956958
0.83135408
0.83281565
0.83448547
0.83588576
0.83822328
0.83819187
INFO - Training [39][   40/  196]   Loss 0.389032   Top1 86.542969   Top5 98.437500   BatchTime 0.399623   LR 0.001094
0.83847147
0.83882117
0.83865416
0.83856493
0.83887976
0.83881998
0.83863765
0.83842522
0.83844858
0.83846051
0.83825839
0.83794868
0.83787066
0.83800024
0.83797687
0.83832496
0.83847260
INFO - Training [39][   60/  196]   Loss 0.381958   Top1 86.783854   Top5 98.496094   BatchTime 0.379521   LR 0.001090
0.83820456
0.83783013
0.83791864
0.83759362
0.83737743
0.83722597
0.83715820
0.83708048
0.83688366
0.83690119
0.83693409
0.83707923
0.83739370
0.83751893
0.83754033
0.83788192
0.83772111
0.83788019
INFO - Training [39][   80/  196]   Loss 0.381906   Top1 86.699219   Top5 98.569336   BatchTime 0.368509   LR 0.001087
0.83785695
0.83814746
0.83863646
0.83926171
0.83963114
0.83970720
0.83990961
0.83977026
0.83943850
0.83931923
0.83881748
0.83855295
0.83862543
0.83848435
0.83813173
0.83764988
0.83751351
0.83740795
0.83693379
0.83686829
0.83715731
0.83690286
0.83668470
0.83675361
INFO - Training [39][  100/  196]   Loss 0.378683   Top1 86.832031   Top5 98.535156   BatchTime 0.361441   LR 0.001084
0.83661044
0.83632177
0.83619416
0.83620429
0.83628005
0.83668286
0.83612221
0.83601630
0.83616787
0.83616436
0.83583796
0.83448654
0.83498156
0.83596659
0.83608299
0.83629417
0.83663791
0.83661628
0.83681619
0.83682960
0.83660430
INFO - Training [39][  120/  196]   Loss 0.371183   Top1 87.089844   Top5 98.678385   BatchTime 0.348574   LR 0.001080
0.83647048
0.83640480
0.83664143
0.83658743
0.83698010
0.83738506
0.83775395
0.83807415
0.83847356
0.83863080
0.83863693
0.83850163
0.83840621
0.83813661
0.83823162
0.83843201
0.83829051
0.83827019
INFO - Training [39][  140/  196]   Loss 0.371620   Top1 87.042411   Top5 98.730469   BatchTime 0.347471   LR 0.001077
0.83820266
0.83808047
0.83782715
0.83751535
0.83766323
0.83740145
0.83695883
0.83614045
0.83597434
0.83588511
0.83567607
0.83555979
0.83553380
0.83533227
0.83547169
0.83511293
0.83493346
0.83493805
0.83457893
0.83445603
0.83442795
0.83423710
INFO - Training [39][  160/  196]   Loss 0.371322   Top1 87.031250   Top5 98.737793   BatchTime 0.349724   LR 0.001073
0.83418614
0.83415484
0.83490270
0.83520061
0.83521599
0.83516270
0.83501750
0.83509487
0.83505410
0.83485210
0.83468825
0.83474982
0.83461618
0.83460504
0.83456749
0.83452410
INFO - Training [39][  180/  196]   Loss 0.373330   Top1 87.000868   Top5 98.682726   BatchTime 0.351426   LR 0.001070
0.83421749
0.83402967
0.83402020
0.83383584
0.83410227
0.83421201
0.83423078
0.83406347
0.83438987
0.83705419
0.83685434
0.83679867
0.83626288
0.83497924
0.83420539
0.83412981
0.83395678
INFO - ==> Top1: 87.070    Top5: 98.686    Loss: 0.372
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83374578
0.83406883
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [39][   20/   40]   Loss 0.344059   Top1 88.886719   Top5 99.550781   BatchTime 0.125909
INFO - Validation [39][   40/   40]   Loss 0.345242   Top1 88.820000   Top5 99.630000   BatchTime 0.090836
INFO - ==> Top1: 88.820    Top5: 99.630    Loss: 0.345
INFO - ==> Sparsity : 0.365
INFO - Scoreboard best 1 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1875)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0590)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0686)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0395)
features.4.conv.0 tensor(0.0264)
features.4.conv.3 tensor(0.0990)
features.4.conv.6 tensor(0.0846)
features.5.conv.0 tensor(0.0290)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0863)
features.6.conv.0 tensor(0.0186)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0480)
features.7.conv.0 tensor(0.0455)
features.7.conv.3 tensor(0.1224)
features.7.conv.6 tensor(0.0876)
features.8.conv.0 tensor(0.0512)
features.8.conv.3 tensor(0.1389)
features.8.conv.6 tensor(0.0935)
features.9.conv.0 tensor(0.0727)
features.9.conv.3 tensor(0.1528)
features.9.conv.6 tensor(0.0965)
features.10.conv.0 tensor(0.0306)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0700)
features.11.conv.0 tensor(0.1460)
features.11.conv.3 tensor(0.1240)
features.11.conv.6 tensor(0.1103)
features.12.conv.0 tensor(0.3108)
features.12.conv.3 tensor(0.1871)
features.12.conv.6 tensor(0.2293)
features.13.conv.0 tensor(0.0736)
features.13.conv.3 tensor(0.1719)
features.13.conv.6 tensor(0.0992)
features.14.conv.0 tensor(0.9352)
features.14.conv.3 tensor(0.1126)
features.14.conv.6 tensor(0.8996)
features.15.conv.0 tensor(0.9611)
features.15.conv.3 tensor(0.0949)
features.15.conv.6 tensor(0.9431)
features.16.conv.0 tensor(0.0903)
features.16.conv.3 tensor(0.1278)
features.16.conv.6 tensor(0.1571)
conv.0 tensor(0.1952)
tensor(798784.) 2188896.0
0.83430815
0.83674395
0.83686799
0.83869791
0.83890110
0.83884037
0.83876902
0.83871698
0.83872050
0.83881688
0.83886158
0.83880293
0.83858567
0.83845466
0.83847064
0.83834785
0.83806282
0.83778697
0.83768541
0.83752936
0.83735585
INFO - Training [40][   20/  196]   Loss 0.390836   Top1 86.347656   Top5 98.300781   BatchTime 0.451397   LR 0.001064
0.83732063
0.83711994
0.83697045
0.83695638
0.83687961
0.83681607
0.83680338
0.83657432
0.83663863
0.83670247
0.83676714
0.83679956
0.83683586
0.83678401
0.83672661
0.83664870
0.83657038
INFO - Training [40][   40/  196]   Loss 0.380096   Top1 86.787109   Top5 98.251953   BatchTime 0.404233   LR 0.001060
0.83688301
0.83709037
0.83717203
0.83692247
0.83635598
0.83562064
0.83552909
0.83558947
0.83530897
0.83513629
0.83507574
0.83518583
0.83557230
0.83565634
0.83605486
0.83728814
0.83723676
0.83695352
0.83736187
0.83895713
0.83853924
0.83824837
INFO - Training [40][   60/  196]   Loss 0.380988   Top1 86.634115   Top5 98.417969   BatchTime 0.391169   LR 0.001056
0.83824003
0.83854789
0.83888942
0.83951938
0.83927447
0.83936912
0.83963579
0.83971339
0.83974952
0.84000164
0.84017891
0.84027445
0.83997917
0.83975005
0.83985806
0.83983988
0.83995944
0.83985484
INFO - Training [40][   80/  196]   Loss 0.376293   Top1 86.840820   Top5 98.544922   BatchTime 0.380202   LR 0.001053
0.83974904
0.83993900
0.83968675
0.83959776
0.83950955
0.83966017
0.83973247
0.83990532
0.83993149
0.83987159
0.84039831
0.84034210
0.84011650
0.83998245
0.83992600
0.83988267
0.83982223
0.83984482
0.83990502
0.83949363
0.83965015
0.83950740
INFO - Training [40][  100/  196]   Loss 0.370669   Top1 86.941406   Top5 98.597656   BatchTime 0.377208   LR 0.001049
0.83977526
0.84001780
0.83941847
0.83813334
0.83772182
0.83775634
0.83861953
0.83848536
0.83812338
0.83793128
0.83807576
0.83666843
0.83423978
0.83192062
0.82779503
0.82942474
0.83299208
INFO - Training [40][  120/  196]   Loss 0.364251   Top1 87.190755   Top5 98.701172   BatchTime 0.370368   LR 0.001045
0.83577919
0.83738583
0.83846128
0.83866554
0.83857030
0.83859372
0.83858597
0.83850312
0.83861405
0.83914530
0.83893675
0.83882612
0.83869106
0.83852541
0.83841032
0.83841157
0.83812779
0.83800304
0.83782113
INFO - Training [40][  140/  196]   Loss 0.363173   Top1 87.282366   Top5 98.744420   BatchTime 0.363756   LR 0.001042
0.83803779
0.83829993
0.83818930
0.83797288
0.83758509
0.83746642
0.83729315
0.83711183
0.83713418
0.83690101
0.83657521
0.83638257
0.83629233
0.83586735
0.83583850
0.83571750
0.83556592
0.83542085
0.83549106
0.83534652
0.83511621
0.83523905
0.83552814
INFO - Training [40][  160/  196]   Loss 0.365617   Top1 87.216797   Top5 98.723145   BatchTime 0.362033   LR 0.001038
0.83551776
0.83575207
0.83736169
0.83778292
0.83789951
0.83765990
0.83731127
0.83699697
0.83675367
0.83691567
0.83684063
0.83636725
0.83522755
0.83449304
0.83376503
0.83342320
0.83338761
INFO - Training [40][  180/  196]   Loss 0.367189   Top1 87.157118   Top5 98.667535   BatchTime 0.361422   LR 0.001034
0.83407873
0.83304775
0.83242393
0.83110464
0.83101326
0.83076054
0.82708585
0.82654381
0.82699841
0.82492298
0.82028061
0.81989974
0.81507176
0.81210059
0.81197315
0.81564075
0.82374668
INFO - ==> Top1: 87.164    Top5: 98.666    Loss: 0.367
0.83026731
0.83339554
0.83547151
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [40][   20/   40]   Loss 0.470795   Top1 86.933594   Top5 99.472656   BatchTime 0.126046
INFO - Validation [40][   40/   40]   Loss 0.468884   Top1 86.980000   Top5 99.530000   BatchTime 0.091265
INFO - ==> Top1: 86.980    Top5: 99.530    Loss: 0.469
INFO - ==> Sparsity : 0.356
INFO - Scoreboard best 1 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 89.200   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1836)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0547)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0571)
features.2.conv.6 tensor(0.0654)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0440)
features.3.conv.6 tensor(0.0378)
features.4.conv.0 tensor(0.0262)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0863)
features.5.conv.0 tensor(0.0332)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.0876)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0471)
features.7.conv.0 tensor(0.0492)
features.7.conv.3 tensor(0.1215)
features.7.conv.6 tensor(0.0874)
features.8.conv.0 tensor(0.0534)
features.8.conv.3 tensor(0.1360)
features.8.conv.6 tensor(0.0932)
features.9.conv.0 tensor(0.0648)
features.9.conv.3 tensor(0.1542)
features.9.conv.6 tensor(0.0932)
features.10.conv.0 tensor(0.0403)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.0685)
features.11.conv.0 tensor(0.1506)
features.11.conv.3 tensor(0.1231)
features.11.conv.6 tensor(0.1401)
features.12.conv.0 tensor(0.2632)
features.12.conv.3 tensor(0.1887)
features.12.conv.6 tensor(0.2129)
features.13.conv.0 tensor(0.0723)
features.13.conv.3 tensor(0.1723)
features.13.conv.6 tensor(0.1494)
features.14.conv.0 tensor(0.9341)
features.14.conv.3 tensor(0.1150)
features.14.conv.6 tensor(0.9119)
features.15.conv.0 tensor(0.9655)
features.15.conv.3 tensor(0.0932)
features.15.conv.6 tensor(0.9496)
features.16.conv.0 tensor(0.0977)
features.16.conv.3 tensor(0.1275)
features.16.conv.6 tensor(0.1534)
conv.0 tensor(0.1339)
tensor(780062.) 2188896.0
0.83579433
0.83572209
0.83591813
0.83585125
0.83580106
0.83590543
0.83624953
0.83636230
0.83659220
0.83695954
0.83738893
0.83869946
0.83924675
0.83923680
0.83913827
0.83896726
0.83891279
0.83893430
0.83889347
0.83914924
0.84043109
INFO - Training [41][   20/  196]   Loss 0.380695   Top1 86.914062   Top5 98.125000   BatchTime 0.429111   LR 0.001027
0.84099936
0.84129131
0.84169787
0.84155846
0.84184140
0.84197205
0.84199101
0.84191835
0.84198171
0.84198588
0.84176528
0.84176379
0.84187746
0.84157419
0.84125441
0.84095138
0.84092230
INFO - Training [41][   40/  196]   Loss 0.376210   Top1 86.894531   Top5 98.173828   BatchTime 0.394360   LR 0.001023
0.84091252
0.84074593
0.84092522
0.84105694
0.84085107
0.84102249
0.84088683
0.84079307
0.84076762
0.84058177
0.84071982
0.84071809
0.84069115
0.84041452
0.84044218
0.84053904
0.84065998
0.84074920
0.84040213
0.84023309
0.84021795
0.84030515
INFO - Training [41][   60/  196]   Loss 0.368071   Top1 87.154948   Top5 98.307292   BatchTime 0.379883   LR 0.001020
0.84031183
0.84013635
0.84037334
0.84086442
0.84091479
0.84105897
0.84085190
0.84100401
0.84114659
0.84085292
0.84079337
0.84087974
0.84107655
0.84123665
0.84128541
0.84136969
0.84198070
INFO - Training [41][   80/  196]   Loss 0.366549   Top1 87.148438   Top5 98.486328   BatchTime 0.374128   LR 0.001016
0.84224236
0.84211046
0.84188187
0.84160846
0.84143662
0.84126484
0.84090370
0.84084547
0.84036350
0.84034914
0.84056401
0.84033209
0.84019184
0.84001780
0.83992058
0.84007764
0.83977163
0.83945614
0.83927190
0.83880258
0.83854312
0.83835816
INFO - Training [41][  100/  196]   Loss 0.360218   Top1 87.441406   Top5 98.566406   BatchTime 0.371196   LR 0.001012
0.83859658
0.83851951
0.83834141
0.83842808
0.83850217
0.83856076
0.83872116
0.83793551
0.83739036
0.83736700
0.83722895
0.83712250
0.83709919
0.83715105
0.83706146
0.83690542
0.83682299
0.83786738
0.83787090
0.83772379
0.83775520
0.83785397
INFO - Training [41][  120/  196]   Loss 0.357596   Top1 87.633464   Top5 98.652344   BatchTime 0.372756   LR 0.001008
0.83747226
0.83745635
0.83731741
0.83730525
0.83752686
0.83751476
0.83836299
0.83814353
0.83805573
0.83775610
0.83714664
0.83714151
0.83720738
0.83712524
0.83684975
0.83680511
0.83642668
0.83613682
0.83596247
INFO - Training [41][  140/  196]   Loss 0.354609   Top1 87.734375   Top5 98.713728   BatchTime 0.363358   LR 0.001004
0.83626807
0.83664215
0.83683288
0.83860403
0.83871317
0.83909720
0.83907109
0.83893758
0.83874446
0.83877522
0.83860451
0.83836782
0.83853531
0.83849144
0.83836907
0.83824730
0.83815569
0.83794630
0.83791864
0.83737713
0.83677626
INFO - Training [41][  160/  196]   Loss 0.358218   Top1 87.617188   Top5 98.688965   BatchTime 0.353186   LR 0.001000
0.83671117
0.83680242
0.83807331
0.83811879
0.83824831
0.83823127
0.83802623
0.83809698
0.83795029
0.83786058
0.83803612
0.83789569
0.83754528
0.83753395
0.83741468
0.83728242
INFO - Training [41][  180/  196]   Loss 0.358373   Top1 87.580295   Top5 98.619792   BatchTime 0.354295   LR 0.000996
0.83731496
0.83753693
0.83765042
0.83805525
0.84020883
0.84051085
0.84042287
0.84033632
0.84028989
0.84013581
0.84010822
0.84006190
0.84023148
0.84031934
0.84038872
0.84039646
0.84032238
INFO - ==> Top1: 87.674    Top5: 98.608    Loss: 0.356
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84030968
0.84022194
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [41][   20/   40]   Loss 0.326311   Top1 90.039062   Top5 99.609375   BatchTime 0.123342
INFO - Validation [41][   40/   40]   Loss 0.311341   Top1 90.320000   Top5 99.670000   BatchTime 0.088780
INFO - ==> Top1: 90.320    Top5: 99.670    Loss: 0.311
INFO - ==> Sparsity : 0.363
INFO - Scoreboard best 1 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 89.680   Top5: 99.670]
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0538)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0666)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0397)
features.4.conv.0 tensor(0.0239)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.0815)
features.5.conv.0 tensor(0.0303)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.0854)
features.6.conv.0 tensor(0.0207)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0467)
features.7.conv.0 tensor(0.0416)
features.7.conv.3 tensor(0.1227)
features.7.conv.6 tensor(0.0848)
features.8.conv.0 tensor(0.0611)
features.8.conv.3 tensor(0.1340)
features.8.conv.6 tensor(0.0926)
features.9.conv.0 tensor(0.0623)
features.9.conv.3 tensor(0.1531)
features.9.conv.6 tensor(0.0934)
features.10.conv.0 tensor(0.0390)
features.10.conv.3 tensor(0.1056)
features.10.conv.6 tensor(0.0665)
features.11.conv.0 tensor(0.1443)
features.11.conv.3 tensor(0.1225)
features.11.conv.6 tensor(0.1288)
features.12.conv.0 tensor(0.2525)
features.12.conv.3 tensor(0.1861)
features.12.conv.6 tensor(0.2121)
features.13.conv.0 tensor(0.0712)
features.13.conv.3 tensor(0.1744)
features.13.conv.6 tensor(0.0767)
features.14.conv.0 tensor(0.9396)
features.14.conv.3 tensor(0.1119)
features.14.conv.6 tensor(0.8976)
features.15.conv.0 tensor(0.9666)
features.15.conv.3 tensor(0.0949)
features.15.conv.6 tensor(0.9567)
features.16.conv.0 tensor(0.0946)
features.16.conv.3 tensor(0.1293)
features.16.conv.6 tensor(0.1463)
conv.0 tensor(0.1975)
tensor(794634.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
0.84199691
0.84158564
0.84123886
0.84136891
0.84156978
0.84158939
0.84150732
0.84185916
0.84206176
0.84182370
0.84180170
0.84151173
0.84142488
0.84137475
0.84128469
0.84122926
0.84139669
INFO - Training [42][   20/  196]   Loss 0.362366   Top1 87.167969   Top5 97.890625   BatchTime 0.459864   LR 0.000988
0.84165847
0.84180206
0.84186989
0.84206444
0.84200615
0.84210336
0.84208637
0.84212595
0.84211278
0.84186929
0.84176999
0.84190929
0.84186703
0.84196305
0.84176290
0.84192306
0.84190804
0.84180027
0.84177250
0.84178358
0.84195417
0.84198636
INFO - Training [42][   40/  196]   Loss 0.363698   Top1 87.314453   Top5 98.222656   BatchTime 0.407442   LR 0.000984
0.84222245
0.84219527
0.84188235
0.84183651
0.84161615
0.84139037
0.84113473
0.84110308
0.84122390
0.84144515
0.84135330
0.84110898
0.84112167
0.84078652
0.84049773
0.84035927
0.84040231
INFO - Training [42][   60/  196]   Loss 0.361471   Top1 87.473958   Top5 98.398438   BatchTime 0.389790   LR 0.000980
0.84045136
0.84033424
0.84018952
0.84004754
0.83992130
0.84003013
0.83990401
0.83875066
0.83745927
0.83790702
0.83786446
0.83703381
0.83661473
0.83617622
0.83629113
0.83633322
0.83644587
0.83595526
0.83612871
0.83635378
0.83663279
0.83952630
INFO - Training [42][   80/  196]   Loss 0.362119   Top1 87.500000   Top5 98.486328   BatchTime 0.381776   LR 0.000976
0.84145194
0.84175336
0.84163409
0.84169197
0.84140408
0.84127933
0.84105456
0.84088331
0.84084857
0.84073794
0.84090841
0.84098053
0.84083021
0.84096897
0.84092396
0.84064597
0.84052086
0.84054768
0.84070319
0.84062070
0.84074938
0.84102786
0.84105897
INFO - Training [42][  100/  196]   Loss 0.357129   Top1 87.691406   Top5 98.566406   BatchTime 0.377049   LR 0.000972
0.84116763
0.84090680
0.84087044
0.84078217
0.84070003
0.84060937
0.84064668
0.84081244
0.84062570
0.84075165
0.84079540
0.84069067
0.84089738
0.84067953
0.84062821
0.84020382
INFO - Training [42][  120/  196]   Loss 0.351823   Top1 87.792969   Top5 98.652344   BatchTime 0.376618   LR 0.000968
0.84030777
0.84048599
0.84037268
0.84024465
0.84027970
0.84035796
0.84065610
0.84057021
0.84080595
0.84102225
0.84110183
0.84089142
0.84067994
0.84062827
0.84056664
0.84061623
0.84085077
0.84030312
0.84043270
INFO - Training [42][  140/  196]   Loss 0.355164   Top1 87.703683   Top5 98.738839   BatchTime 0.366866   LR 0.000964
0.84055483
0.84073734
0.84079754
0.84090072
0.84096640
0.84117371
0.84111363
0.84153420
0.84165353
0.84121740
0.84120995
0.84108740
0.84098154
0.84075850
0.84077770
0.84084767
0.84067333
0.84084779
0.84121197
0.84142351
0.84134334
0.84113544
0.84119660
0.84109378
INFO - Training [42][  160/  196]   Loss 0.355964   Top1 87.597656   Top5 98.713379   BatchTime 0.361805   LR 0.000959
0.84095609
0.84104985
0.84099609
0.84076089
0.84040838
0.84035069
0.84055865
0.84082615
0.84108877
0.84115613
0.84119070
0.84140468
0.84111649
0.84093106
0.84085095
0.84083050
0.84055394
INFO - Training [42][  180/  196]   Loss 0.356554   Top1 87.586806   Top5 98.691406   BatchTime 0.361815   LR 0.000955
0.84051406
0.84037602
0.83999598
0.84018815
0.84020990
0.84038734
0.83997715
0.83933318
0.83874816
0.83850861
0.83818310
0.83789355
0.83778751
0.83779782
0.83791023
0.83742970
INFO - ==> Top1: 87.604    Top5: 98.700    Loss: 0.356
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83688581
0.83649909
0.83643967
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [42][   20/   40]   Loss 0.302757   Top1 90.136719   Top5 99.550781   BatchTime 0.125819
INFO - Validation [42][   40/   40]   Loss 0.293029   Top1 90.480000   Top5 99.650000   BatchTime 0.089924
INFO - ==> Top1: 90.480    Top5: 99.650    Loss: 0.293
INFO - ==> Sparsity : 0.367
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.1738)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0516)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0677)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0455)
features.3.conv.6 tensor(0.0393)
features.4.conv.0 tensor(0.0241)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.0845)
features.5.conv.0 tensor(0.0326)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0841)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0466)
features.7.conv.0 tensor(0.0390)
features.7.conv.3 tensor(0.1186)
features.7.conv.6 tensor(0.0844)
features.8.conv.0 tensor(0.0507)
features.8.conv.3 tensor(0.1322)
features.8.conv.6 tensor(0.0942)
features.9.conv.0 tensor(0.0698)
features.9.conv.3 tensor(0.1536)
features.9.conv.6 tensor(0.0942)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0670)
features.11.conv.0 tensor(0.1460)
features.11.conv.3 tensor(0.1248)
features.11.conv.6 tensor(0.1427)
features.12.conv.0 tensor(0.2338)
features.12.conv.3 tensor(0.1829)
features.12.conv.6 tensor(0.2167)
features.13.conv.0 tensor(0.0631)
features.13.conv.3 tensor(0.1732)
features.13.conv.6 tensor(0.0960)
features.14.conv.0 tensor(0.9347)
features.14.conv.3 tensor(0.1162)
features.14.conv.6 tensor(0.9028)
features.15.conv.0 tensor(0.9637)
features.15.conv.3 tensor(0.0956)
features.15.conv.6 tensor(0.9488)
features.16.conv.0 tensor(0.0900)
features.16.conv.3 tensor(0.1315)
features.16.conv.6 tensor(0.1659)
conv.0 tensor(0.2072)
tensor(803633.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
0.83733088
0.83707279
0.83660048
0.83656710
0.83665848
0.83759147
0.83794975
0.83823264
0.83836645
0.83821958
0.83799481
0.83790386
0.83799869
0.83802217
0.83768857
0.83773524
0.83784157
0.83791077
INFO - Training [43][   20/  196]   Loss 0.375865   Top1 86.816406   Top5 98.281250   BatchTime 0.441715   LR 0.000947
0.83807808
0.83808708
0.83822185
0.83813864
0.83802801
0.83837962
0.83830655
0.83827949
0.83824229
0.83810723
0.83811736
0.83803499
0.83806646
0.83802408
0.83789486
0.83802271
0.83787775
0.83762205
0.83757067
0.83764380
0.83751351
0.83724862
0.83714533
INFO - Training [43][   40/  196]   Loss 0.373912   Top1 86.748047   Top5 98.486328   BatchTime 0.398961   LR 0.000943
0.83725685
0.83712983
0.83722317
0.83703071
0.83657330
0.83496118
0.83451116
0.83506727
0.83531326
0.83513069
0.83500075
0.83504307
0.83722335
0.83706218
0.83696377
0.83676666
INFO - Training [43][   60/  196]   Loss 0.364100   Top1 87.070312   Top5 98.600260   BatchTime 0.388968   LR 0.000939
0.83671480
0.83640003
0.83628511
0.83633810
0.83653730
0.83654791
0.83650458
0.83661997
0.83672017
0.83656681
0.83632833
0.83617419
0.83667588
0.83809072
0.83780414
0.83729255
0.83716977
0.83712989
0.83684534
0.83678734
0.83706003
0.83693886
0.83691317
INFO - Training [43][   80/  196]   Loss 0.361068   Top1 87.314453   Top5 98.706055   BatchTime 0.379961   LR 0.000934
0.83701694
0.83660227
0.83650166
0.83638191
0.83633620
0.83626944
0.83651847
0.83669382
0.83685410
0.83829606
0.83821934
0.83829433
0.83853287
0.83850271
0.83841300
0.83823472
0.83799499
INFO - Training [43][  100/  196]   Loss 0.357343   Top1 87.390625   Top5 98.734375   BatchTime 0.375931   LR 0.000930
0.83794421
0.83805835
0.83800519
0.83794224
0.83783442
0.83781672
0.83786726
0.83798748
0.83840185
0.84055489
0.84016442
0.83985734
0.83991408
0.83964211
0.83947539
0.83923548
0.83915365
0.83914119
0.83904344
0.83890206
0.83889741
0.83891433
INFO - Training [43][  120/  196]   Loss 0.351338   Top1 87.652995   Top5 98.789062   BatchTime 0.371865   LR 0.000926
0.83872640
0.83878493
0.83868545
0.83861101
0.83886182
0.83890003
0.83892363
0.83898526
0.83914882
0.83905071
0.83877414
0.83853394
0.83828890
0.83797848
0.83765167
0.83734292
0.83734357
0.83643574
INFO - Training [43][  140/  196]   Loss 0.351770   Top1 87.636719   Top5 98.861607   BatchTime 0.368321   LR 0.000921
0.83422744
0.83218056
0.82971418
0.82682776
0.82623780
0.82504600
0.82403868
0.82321894
0.82160562
0.82064188
0.82167983
0.82353252
0.82506436
0.82611525
0.82679230
0.82717884
0.82799202
0.82862824
0.82971728
0.83057594
0.83162379
0.83270729
INFO - Training [43][  160/  196]   Loss 0.355469   Top1 87.509766   Top5 98.842773   BatchTime 0.367854   LR 0.000917
0.83351976
0.83424836
0.83498406
0.83557498
0.83614975
0.83671242
0.83709383
0.83740491
0.83786327
0.83810425
0.83829170
0.83865505
0.83903319
0.83923602
0.84117699
0.84133995
0.84115607
0.84115535
0.84110504
0.84086758
0.84072775
0.84038913
INFO - Training [43][  180/  196]   Loss 0.354962   Top1 87.578125   Top5 98.786892   BatchTime 0.366584   LR 0.000912
0.84045702
0.84049785
0.84051311
0.84049106
0.84073514
0.84074372
0.84071392
0.84048706
0.84049493
0.84022307
0.83999705
0.83968258
0.83880162
0.83830136
0.83829582
********************pre-trained*****************
INFO - ==> Top1: 87.604    Top5: 98.788    Loss: 0.354
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [43][   20/   40]   Loss 0.342044   Top1 89.687500   Top5 99.550781   BatchTime 0.128875
INFO - Validation [43][   40/   40]   Loss 0.332322   Top1 89.620000   Top5 99.640000   BatchTime 0.090854
INFO - ==> Top1: 89.620    Top5: 99.640    Loss: 0.332
INFO - ==> Sparsity : 0.371
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.1699)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0486)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0674)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0417)
features.3.conv.6 tensor(0.0419)
features.4.conv.0 tensor(0.0254)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0833)
features.5.conv.0 tensor(0.0304)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0832)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0474)
features.7.conv.0 tensor(0.0389)
features.7.conv.3 tensor(0.1215)
features.7.conv.6 tensor(0.0834)
features.8.conv.0 tensor(0.0514)
features.8.conv.3 tensor(0.1322)
features.8.conv.6 tensor(0.0882)
features.9.conv.0 tensor(0.0706)
features.9.conv.3 tensor(0.1516)
features.9.conv.6 tensor(0.0921)
features.10.conv.0 tensor(0.0436)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0678)
features.11.conv.0 tensor(0.1518)
features.11.conv.3 tensor(0.1260)
features.11.conv.6 tensor(0.1277)
features.12.conv.0 tensor(0.2583)
features.12.conv.3 tensor(0.1846)
features.12.conv.6 tensor(0.2115)
features.13.conv.0 tensor(0.0749)
features.13.conv.3 tensor(0.1692)
features.13.conv.6 tensor(0.0974)
features.14.conv.0 tensor(0.9404)
features.14.conv.3 tensor(0.1138)
features.14.conv.6 tensor(0.9168)
features.15.conv.0 tensor(0.9609)
features.15.conv.3 tensor(0.0943)
features.15.conv.6 tensor(0.9523)
features.16.conv.0 tensor(0.0962)
features.16.conv.3 tensor(0.1296)
features.16.conv.6 tensor(0.1803)
conv.0 tensor(0.2057)
tensor(812864.) 2188896.0
0.83791709
0.83786190
0.83794707
0.83791208
0.83757359
0.83735836
0.83717513
0.83707952
0.83663839
0.83653307
0.83646595
0.83637190
0.83616257
0.83611155
0.83581418
0.83585596
0.83506048
0.83390278
0.83424950
0.83352143
INFO - Training [44][   20/  196]   Loss 0.360992   Top1 87.109375   Top5 98.242188   BatchTime 0.429090   LR 0.000904
0.83262813
0.83199048
0.83072680
0.82883006
0.82572824
0.82268649
0.82201737
0.82029122
0.81800103
0.81775880
0.81776637
0.81911862
0.82040876
0.82135749
0.82224792
0.82354581
0.82541412
INFO - Training [44][   40/  196]   Loss 0.358162   Top1 87.382812   Top5 98.388672   BatchTime 0.393523   LR 0.000900
0.82739979
0.82868719
0.82930803
0.82990932
0.83039588
0.83102971
0.83308780
0.83361024
0.83332354
0.83338219
0.83312744
0.83319038
0.83327007
0.83329076
0.83321768
0.83285153
0.83274966
0.83310336
0.83326459
0.83310723
0.83309591
0.83291775
INFO - Training [44][   60/  196]   Loss 0.356625   Top1 87.532552   Top5 98.476562   BatchTime 0.384069   LR 0.000895
0.83307421
0.83315563
0.83315116
0.83296549
0.83284289
0.83298308
0.83311671
0.83289582
0.83291787
0.83258587
0.83263797
0.83275431
0.83256400
0.83243769
0.83238095
0.83219439
0.83238029
0.83228403
0.83243507
0.83269083
0.83297920
0.83287334
INFO - Training [44][   80/  196]   Loss 0.359286   Top1 87.490234   Top5 98.603516   BatchTime 0.377986   LR 0.000891
0.83277476
0.83283323
0.83283973
0.83285582
0.83303750
0.83302766
0.83312577
0.83353686
0.83369875
0.83387488
0.83393210
0.83366054
0.83362716
0.83371967
0.83397233
0.83375454
0.83369982
INFO - Training [44][  100/  196]   Loss 0.352947   Top1 87.816406   Top5 98.621094   BatchTime 0.374127   LR 0.000886
0.83369547
0.83368963
0.83375204
0.83376509
0.83363342
0.83361059
0.83372217
0.83378720
0.83362436
0.83356792
0.83368808
0.83371145
0.83364481
0.83344865
0.83336538
0.83299881
0.83278042
0.83232045
0.83187354
0.83223873
0.83299130
0.83352149
0.83345479
INFO - Training [44][  120/  196]   Loss 0.348082   Top1 87.975260   Top5 98.688151   BatchTime 0.371116   LR 0.000882
0.83322483
0.83317280
0.83319026
0.83315140
0.83338827
0.83351147
0.83357054
0.83329684
0.83311307
0.83305931
0.83295214
0.83287901
0.83283675
0.83279717
0.83283108
0.83308500
0.83352345
0.83338135
0.83332878
INFO - Training [44][  140/  196]   Loss 0.347025   Top1 88.041295   Top5 98.741629   BatchTime 0.362436   LR 0.000877
0.83340251
0.83337480
0.83345950
0.83331275
0.83322781
0.83312488
0.83306336
0.83305907
0.83316869
0.83265632
0.83249497
0.83247244
0.83245242
0.83248764
0.83214855
0.83071244
INFO - Training [44][  160/  196]   Loss 0.349616   Top1 87.951660   Top5 98.723145   BatchTime 0.362775   LR 0.000873
0.83054537
0.83066791
0.83065057
0.83066684
0.83037299
0.83200085
0.83240330
0.83238274
0.83245897
0.83236271
0.83221686
0.83205360
0.83196098
0.83175898
0.83166897
0.83152187
0.83107960
0.83101273
0.83063471
0.83023471
0.83006656
0.82991272
0.83001459
INFO - Training [44][  180/  196]   Loss 0.351428   Top1 87.905816   Top5 98.639323   BatchTime 0.361935   LR 0.000868
0.83030367
0.83064717
0.83092958
0.83133930
0.83188158
0.83250660
0.83310485
0.83380175
0.83459651
0.83644944
0.83703500
0.83727759
0.83752537
0.83775800
0.83777732
0.83841938
INFO - ==> Top1: 87.916    Top5: 98.648    Loss: 0.351
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83966291
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [44][   20/   40]   Loss 0.345861   Top1 89.218750   Top5 99.453125   BatchTime 0.139908
INFO - Validation [44][   40/   40]   Loss 0.339046   Top1 89.200000   Top5 99.560000   BatchTime 0.109848
INFO - ==> Top1: 89.200    Top5: 99.560    Loss: 0.339
INFO - ==> Sparsity : 0.368
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.1719)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0495)
features.2.conv.0 tensor(0.0188)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0642)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0448)
features.3.conv.6 tensor(0.0371)
features.4.conv.0 tensor(0.0272)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0814)
features.5.conv.0 tensor(0.0330)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.0822)
features.6.conv.0 tensor(0.0194)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0465)
features.7.conv.0 tensor(0.0415)
features.7.conv.3 tensor(0.1224)
features.7.conv.6 tensor(0.0844)
features.8.conv.0 tensor(0.0594)
features.8.conv.3 tensor(0.1311)
features.8.conv.6 tensor(0.0866)
features.9.conv.0 tensor(0.0691)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.0913)
features.10.conv.0 tensor(0.0347)
features.10.conv.3 tensor(0.1068)
features.10.conv.6 tensor(0.0684)
features.11.conv.0 tensor(0.1484)
features.11.conv.3 tensor(0.1262)
features.11.conv.6 tensor(0.1308)
features.12.conv.0 tensor(0.2466)
features.12.conv.3 tensor(0.1854)
features.12.conv.6 tensor(0.2762)
features.13.conv.0 tensor(0.0707)
features.13.conv.3 tensor(0.1651)
features.13.conv.6 tensor(0.1026)
features.14.conv.0 tensor(0.9445)
features.14.conv.3 tensor(0.1142)
features.14.conv.6 tensor(0.9014)
features.15.conv.0 tensor(0.9630)
features.15.conv.3 tensor(0.0936)
features.15.conv.6 tensor(0.9633)
features.16.conv.0 tensor(0.0943)
features.16.conv.3 tensor(0.1287)
features.16.conv.6 tensor(0.1618)
conv.0 tensor(0.1932)
tensor(805108.) 2188896.0
0.83949244
0.83960509
0.83938330
0.83902729
0.83888167
0.83885252
0.83880365
0.83893418
0.83909839
0.83916074
0.83895594
0.83905470
0.83886021
0.83872449
0.83822519
0.83819568
0.83770537
0.83755696
0.83792007
0.83804178
INFO - Training [45][   20/  196]   Loss 0.351594   Top1 87.968750   Top5 98.242188   BatchTime 0.430497   LR 0.000860
0.83765268
0.83748120
0.83745784
0.83727658
0.83713025
0.83643687
0.83616483
0.83594894
0.83597904
0.83690441
0.83703494
0.83669668
0.83640236
0.83623445
0.83624357
0.83608472
0.83604485
INFO - Training [45][   40/  196]   Loss 0.362762   Top1 87.705078   Top5 98.398438   BatchTime 0.395624   LR 0.000855
0.83596617
0.83559233
0.83551538
0.83533329
0.83498901
0.83494020
0.83499002
0.83499986
0.83489424
0.83436984
0.83420813
0.83407122
0.83384550
0.83365303
0.83343244
0.83360523
0.83364254
0.83361894
0.83355159
0.83362073
0.83344787
0.83319753
INFO - Training [45][   60/  196]   Loss 0.360474   Top1 87.558594   Top5 98.476562   BatchTime 0.383188   LR 0.000850
0.83299541
0.83294761
0.83278614
0.83256710
0.83234251
0.83209419
0.83215290
0.83227396
0.83248293
0.83214897
0.83190215
0.83177692
0.83164871
0.83140379
0.83142287
0.83129525
0.83102965
INFO - Training [45][   80/  196]   Loss 0.357601   Top1 87.680664   Top5 98.637695   BatchTime 0.375408   LR 0.000846
0.83079773
0.83073580
0.83055371
0.83034348
0.82998008
0.82952929
0.82912076
0.82882637
0.82877564
0.82862431
0.82858211
0.82852948
0.82857931
0.82862723
0.82955676
0.82957035
0.82918811
0.82858592
0.82802147
0.82777071
0.82747149
0.82764572
0.82771057
INFO - Training [45][  100/  196]   Loss 0.349870   Top1 87.968750   Top5 98.707031   BatchTime 0.370496   LR 0.000841
0.82798994
0.82975411
0.82860965
0.82830960
0.82888460
0.83046222
0.83125687
0.83174103
0.83190244
0.83192444
0.83168697
0.83175504
0.83192760
0.83200973
0.83195537
0.83188564
0.83200502
0.83212894
0.83204538
0.83185852
0.83171475
0.83144414
0.83119363
INFO - Training [45][  120/  196]   Loss 0.346620   Top1 88.066406   Top5 98.769531   BatchTime 0.369337   LR 0.000836
0.83083868
0.83029443
0.82993388
0.82992631
0.82995385
0.82974964
0.82965934
0.82957661
0.82944256
0.82936245
0.82927406
0.82919008
0.82913411
0.83029205
0.83005357
0.83015186
0.83057380
0.83079040
0.83115691
INFO - Training [45][  140/  196]   Loss 0.345968   Top1 88.099888   Top5 98.819754   BatchTime 0.361133   LR 0.000832
0.83277565
0.83263195
0.83220255
0.83202362
0.83207554
0.83216339
0.83218729
0.83234346
0.83236492
0.83247161
0.83230436
0.83192110
0.83185536
0.83173835
0.83164066
0.83155924
INFO - Training [45][  160/  196]   Loss 0.349496   Top1 87.900391   Top5 98.811035   BatchTime 0.360915   LR 0.000827
0.83149594
0.83151919
0.83144593
0.83132410
0.83138245
0.83125603
0.83117068
0.83098638
0.83096415
0.83127898
0.83131671
0.83139664
0.83133936
0.83111995
0.83108354
0.83092499
0.83070576
0.83066595
0.83056176
0.83061653
0.83038014
0.82990855
0.82983273
INFO - Training [45][  180/  196]   Loss 0.349525   Top1 87.914497   Top5 98.747830   BatchTime 0.361095   LR 0.000822
0.83012980
0.82992923
0.82992852
0.82994610
0.82977825
0.82953954
0.82923400
0.82903904
0.82911891
0.82888770
0.82895160
0.82856268
0.82866043
0.82874525
0.82860333
0.82839215
INFO - ==> Top1: 87.928    Top5: 98.756    Loss: 0.350
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [45][   20/   40]   Loss 0.338131   Top1 89.433594   Top5 99.589844   BatchTime 0.147749
INFO - Validation [45][   40/   40]   Loss 0.325086   Top1 89.480000   Top5 99.660000   BatchTime 0.119098
INFO - ==> Top1: 89.480    Top5: 99.660    Loss: 0.325
INFO - ==> Sparsity : 0.382
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0477)
features.2.conv.0 tensor(0.0200)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0628)
features.3.conv.0 tensor(0.0148)
features.3.conv.3 tensor(0.0424)
features.3.conv.6 tensor(0.0395)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.0819)
features.5.conv.0 tensor(0.0382)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.0856)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0474)
features.7.conv.0 tensor(0.0480)
features.7.conv.3 tensor(0.1218)
features.7.conv.6 tensor(0.0839)
features.8.conv.0 tensor(0.0629)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.0876)
features.9.conv.0 tensor(0.0713)
features.9.conv.3 tensor(0.1508)
features.9.conv.6 tensor(0.0956)
features.10.conv.0 tensor(0.0323)
features.10.conv.3 tensor(0.1100)
features.10.conv.6 tensor(0.0875)
features.11.conv.0 tensor(0.1454)
features.11.conv.3 tensor(0.1264)
features.11.conv.6 tensor(0.1507)
features.12.conv.0 tensor(0.2490)
features.12.conv.3 tensor(0.1827)
features.12.conv.6 tensor(0.3659)
features.13.conv.0 tensor(0.0699)
features.13.conv.3 tensor(0.1696)
features.13.conv.6 tensor(0.0952)
features.14.conv.0 tensor(0.9394)
features.14.conv.3 tensor(0.1178)
features.14.conv.6 tensor(0.9098)
features.15.conv.0 tensor(0.9667)
features.15.conv.3 tensor(0.0925)
features.15.conv.6 tensor(0.9576)
features.16.conv.0 tensor(0.0954)
features.16.conv.3 tensor(0.1326)
features.16.conv.6 tensor(0.1757)
conv.0 tensor(0.2405)
tensor(835649.) 2188896.0
0.82804257
0.82773066
0.82768548
0.82900238
0.82895726
0.82868266
0.82886487
0.82878333
0.82850057
0.82832092
0.82829010
0.82841450
0.82829261
0.82805848
0.82798576
0.82781655
0.82794237
0.82791209
0.82776529
0.82777739
INFO - Training [46][   20/  196]   Loss 0.347049   Top1 87.421875   Top5 98.300781   BatchTime 0.449466   LR 0.000814
0.82777423
0.82787150
0.82788253
0.82793516
0.82796872
0.82828975
0.82850045
0.82839620
0.82819736
0.82821620
0.82824194
0.82832104
0.82850075
0.83069956
0.83090413
0.83079380
0.83068824
0.83064449
0.83017236
0.82954699
0.82947308
0.82956088
INFO - Training [46][   40/  196]   Loss 0.347273   Top1 87.519531   Top5 98.515625   BatchTime 0.408355   LR 0.000809
0.82965255
0.82971597
0.82976198
0.82976180
0.83103049
0.83090997
0.83087176
0.83098465
0.83071375
0.83090532
0.83089423
0.83073646
0.83089864
0.83066851
0.83045560
0.83049059
INFO - Training [46][   60/  196]   Loss 0.353941   Top1 87.434896   Top5 98.613281   BatchTime 0.394326   LR 0.000804
0.83050716
0.83043301
0.83036536
0.83052623
0.83077514
0.83081251
0.83052337
0.83049560
0.83053839
0.83054310
0.83077389
0.83076578
0.83044291
0.83040184
0.83036488
0.83021140
0.83014113
0.83000749
0.82984096
0.83005893
0.83002943
0.83013880
0.83004922
INFO - Training [46][   80/  196]   Loss 0.351276   Top1 87.656250   Top5 98.715820   BatchTime 0.383128   LR 0.000799
0.82991558
0.82987112
0.82963610
0.82935834
0.82937151
0.82893103
0.82871705
0.82830131
0.82802421
0.82797813
0.82786578
0.82757205
0.82740897
0.82719558
0.82720447
0.82701737
0.82691151
INFO - Training [46][  100/  196]   Loss 0.343557   Top1 88.011719   Top5 98.753906   BatchTime 0.376584   LR 0.000794
0.82665896
0.82629234
0.82602334
0.82584965
0.82550198
0.82535613
0.82534778
0.82548207
0.82525039
0.82518554
0.82517952
0.82523471
0.82534564
0.82487541
0.82472152
0.82456613
0.82441533
0.82413697
0.82384324
0.82363206
0.82324940
0.82282805
0.82260913
INFO - Training [46][  120/  196]   Loss 0.338765   Top1 88.193359   Top5 98.802083   BatchTime 0.372534   LR 0.000789
0.82295471
0.82291216
0.82277715
0.82284236
0.82427746
0.82419050
0.82422149
0.82443392
0.82471746
0.82515770
0.82546455
0.82600236
0.82720685
0.82741457
0.82741028
0.82715279
0.82721949
0.82650912
0.82677197
INFO - Training [46][  140/  196]   Loss 0.338311   Top1 88.250558   Top5 98.878348   BatchTime 0.364798   LR 0.000785
0.82732284
0.82722986
0.82729405
0.82743227
0.82740796
0.82703233
0.82690936
0.82653290
0.82612526
0.82604182
0.82586789
0.82593817
0.82606804
0.82605428
0.82586676
0.82580537
0.82579356
INFO - Training [46][  160/  196]   Loss 0.339652   Top1 88.168945   Top5 98.854980   BatchTime 0.362810   LR 0.000780
0.82554537
0.82531750
0.82492369
0.82469320
0.82427764
0.82405496
0.82411551
0.82404238
0.82401347
0.82410842
0.82440233
0.82418185
0.82410008
0.82392120
0.82364488
0.82376558
0.82373863
0.82358354
0.82345378
0.82337624
0.82327670
0.82339460
INFO - Training [46][  180/  196]   Loss 0.339186   Top1 88.192274   Top5 98.780382   BatchTime 0.362974   LR 0.000775
0.82331455
0.82335305
0.82337803
0.82321131
0.82311106
0.82303715
0.82282674
0.82266635
0.82242805
0.82208532
0.82157189
0.82129437
0.82071537
0.82003605
0.81940341
0.81990016
0.82128233
INFO - ==> Top1: 88.226    Top5: 98.776    Loss: 0.338
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [46][   20/   40]   Loss 0.339279   Top1 89.335938   Top5 99.453125   BatchTime 0.127166
INFO - Validation [46][   40/   40]   Loss 0.335033   Top1 89.210000   Top5 99.550000   BatchTime 0.091085
INFO - ==> Top1: 89.210    Top5: 99.550    Loss: 0.335
INFO - ==> Sparsity : 0.379
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0188)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0680)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0367)
features.4.conv.0 tensor(0.0285)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0817)
features.5.conv.0 tensor(0.0283)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.0812)
features.6.conv.0 tensor(0.0249)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0504)
features.7.conv.0 tensor(0.0424)
features.7.conv.3 tensor(0.1221)
features.7.conv.6 tensor(0.0842)
features.8.conv.0 tensor(0.0651)
features.8.conv.3 tensor(0.1288)
features.8.conv.6 tensor(0.0866)
features.9.conv.0 tensor(0.0659)
features.9.conv.3 tensor(0.1502)
features.9.conv.6 tensor(0.0925)
features.10.conv.0 tensor(0.0322)
features.10.conv.3 tensor(0.1091)
features.10.conv.6 tensor(0.0665)
features.11.conv.0 tensor(0.1477)
features.11.conv.3 tensor(0.1277)
features.11.conv.6 tensor(0.1885)
features.12.conv.0 tensor(0.2570)
features.12.conv.3 tensor(0.1869)
features.12.conv.6 tensor(0.4181)
features.13.conv.0 tensor(0.0675)
features.13.conv.3 tensor(0.1698)
features.13.conv.6 tensor(0.1322)
features.14.conv.0 tensor(0.9376)
features.14.conv.3 tensor(0.1167)
features.14.conv.6 tensor(0.9062)
features.15.conv.0 tensor(0.9666)
features.15.conv.3 tensor(0.0925)
features.15.conv.6 tensor(0.9623)
features.16.conv.0 tensor(0.0981)
features.16.conv.3 tensor(0.1303)
features.16.conv.6 tensor(0.1921)
conv.0 tensor(0.1937)
tensor(829486.) 2188896.0
0.82237339
0.82251424
0.82504350
0.82509071
0.82478297
0.82470280
0.82442212
0.82419240
0.82401657
0.82388204
0.82394701
0.82365316
0.82360458
0.82356447
0.82352692
0.82354909
0.82356614
INFO - Training [47][   20/  196]   Loss 0.359875   Top1 87.382812   Top5 98.125000   BatchTime 0.439835   LR 0.000766
0.82378459
0.82369816
0.82371986
0.82377684
0.82388473
0.82377362
0.82378364
0.82390326
0.82418334
0.82411879
0.82410139
0.82401240
0.82403404
0.82394755
0.82411289
0.82419646
0.82425761
0.82427222
0.82451504
0.82446104
0.82463127
0.82464623
0.82459402
INFO - Training [47][   40/  196]   Loss 0.351884   Top1 87.685547   Top5 98.300781   BatchTime 0.396818   LR 0.000761
0.82453042
0.82437700
0.82429874
0.82424396
0.82406861
0.82403111
0.82410061
0.82429481
0.82441741
0.82444406
0.82467192
0.82490194
0.82506627
0.82539034
0.82519859
0.82544214
0.82538134
INFO - Training [47][   60/  196]   Loss 0.342886   Top1 88.138021   Top5 98.541667   BatchTime 0.384407   LR 0.000756
0.82586932
0.82629651
0.82646012
0.82635081
0.82614583
0.82609606
0.82636541
0.82664704
0.82657748
0.82675797
0.82659096
0.82635283
0.82655078
0.82690758
0.82673198
0.82666969
0.82661819
0.82647598
0.82651204
0.82659584
0.82675254
INFO - Training [47][   80/  196]   Loss 0.345562   Top1 88.076172   Top5 98.632812   BatchTime 0.380730   LR 0.000752
0.82694489
0.82705021
0.82704318
0.82729501
0.82734537
0.82733238
0.82731766
0.82762426
0.82731706
0.82738602
0.82732368
0.82730192
0.82757884
0.82771093
0.82775158
0.82764488
0.82758868
0.82771242
0.82749432
0.82738101
0.82718796
0.82624978
0.82595652
INFO - Training [47][  100/  196]   Loss 0.339511   Top1 88.273438   Top5 98.675781   BatchTime 0.376725   LR 0.000747
0.82615584
0.82601804
0.82606912
0.82604277
0.82567149
0.82549262
0.82550007
0.82533288
0.82520932
0.82483828
0.82471633
0.82470006
0.82515281
0.82631326
0.82602596
0.82587659
INFO - Training [47][  120/  196]   Loss 0.334341   Top1 88.457031   Top5 98.743490   BatchTime 0.372447   LR 0.000742
0.82598138
0.82603478
0.82596958
0.82601815
0.82588184
0.82551485
0.82523155
0.82463306
0.82454872
0.82455164
0.82450038
0.82449496
0.82430512
0.82420582
0.82415074
0.82405639
0.82386702
0.82389659
0.82392377
0.82490778
INFO - Training [47][  140/  196]   Loss 0.332239   Top1 88.512835   Top5 98.800223   BatchTime 0.364003   LR 0.000737
0.82586026
0.82593203
0.82583243
0.82618499
0.82600582
0.82584953
0.82582182
0.82554960
0.82539356
0.82544571
0.82552630
0.82503915
0.82484972
0.82468730
0.82455844
0.82413268
0.82343370
0.82291573
0.82302159
0.82256520
0.82099205
0.82082731
0.82073563
INFO - Training [47][  160/  196]   Loss 0.335688   Top1 88.369141   Top5 98.806152   BatchTime 0.362610   LR 0.000732
0.82015049
0.81999743
0.82056129
0.82013750
0.82052016
0.82127345
0.82135504
0.82131171
0.82116419
0.82111424
0.82116973
0.82123476
0.82134217
0.82140815
0.82144213
0.82157820
0.82199985
0.82351732
0.82354492
0.82360882
0.82365394
0.82343739
INFO - Training [47][  180/  196]   Loss 0.334817   Top1 88.368056   Top5 98.747830   BatchTime 0.362476   LR 0.000727
0.82323611
0.82321405
0.82336664
0.82343823
0.82331860
0.82317513
0.82305348
0.82311946
0.82493263
0.82513958
0.82499146
INFO - ==> Top1: 88.414    Top5: 98.748    Loss: 0.333
0.82494915
0.82478380
0.82471633
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [47][   20/   40]   Loss 0.336039   Top1 89.218750   Top5 99.492188   BatchTime 0.127279
INFO - Validation [47][   40/   40]   Loss 0.326589   Top1 89.540000   Top5 99.610000   BatchTime 0.092059
INFO - ==> Top1: 89.540    Top5: 99.610    Loss: 0.327
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4479)
features.0.conv.3 tensor(0.1719)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0508)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0586)
features.2.conv.6 tensor(0.0639)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0355)
features.3.conv.6 tensor(0.0336)
features.4.conv.0 tensor(0.0301)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.0812)
features.5.conv.0 tensor(0.0277)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.0820)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0490)
features.7.conv.0 tensor(0.0408)
features.7.conv.3 tensor(0.1186)
features.7.conv.6 tensor(0.0851)
features.8.conv.0 tensor(0.0624)
features.8.conv.3 tensor(0.1291)
features.8.conv.6 tensor(0.1144)
features.9.conv.0 tensor(0.0748)
features.9.conv.3 tensor(0.1534)
features.9.conv.6 tensor(0.0938)
features.10.conv.0 tensor(0.0350)
features.10.conv.3 tensor(0.1073)
features.10.conv.6 tensor(0.0674)
features.11.conv.0 tensor(0.1361)
features.11.conv.3 tensor(0.1244)
features.11.conv.6 tensor(0.2091)
features.12.conv.0 tensor(0.2677)
features.12.conv.3 tensor(0.1867)
features.12.conv.6 tensor(0.3821)
features.13.conv.0 tensor(0.0786)
features.13.conv.3 tensor(0.1696)
features.13.conv.6 tensor(0.0962)
features.14.conv.0 tensor(0.9392)
features.14.conv.3 tensor(0.1161)
features.14.conv.6 tensor(0.9055)
features.15.conv.0 tensor(0.9673)
features.15.conv.3 tensor(0.0919)
features.15.conv.6 tensor(0.9579)
features.16.conv.0 tensor(0.1009)
features.16.conv.3 tensor(0.1295)
features.16.conv.6 tensor(0.1765)
conv.0 tensor(0.1944)
tensor(822235.) 2188896.0
0.82469493
0.82457900
0.82446408
0.82435519
0.82417196
0.82411593
0.82431030
0.82423532
0.82413381
0.82416242
0.82402247
0.82393688
0.82400918
0.82576662
0.82582188
0.82612544
0.82608658
0.82598406
0.82602489
0.82587343
0.82564437
INFO - Training [48][   20/  196]   Loss 0.348828   Top1 88.066406   Top5 98.242188   BatchTime 0.433631   LR 0.000718
0.82516938
0.82483786
0.82526225
0.82511973
0.82357091
0.82355458
0.82357746
0.82365638
0.82489944
0.82534194
0.82503945
0.82482374
0.82473999
0.82458228
0.82357574
0.82364815
INFO - Training [48][   40/  196]   Loss 0.347693   Top1 87.900391   Top5 98.320312   BatchTime 0.399963   LR 0.000713
0.82347649
0.82361037
0.82392925
0.82466322
0.82459104
0.82388544
0.82387513
0.82352579
0.82348561
0.82442909
0.82494229
0.82489145
0.82498491
0.82512212
0.82517916
0.82525659
0.82542658
0.82570153
0.82546413
0.82569021
0.82560921
0.82550198
INFO - Training [48][   60/  196]   Loss 0.349903   Top1 87.734375   Top5 98.437500   BatchTime 0.386032   LR 0.000708
0.82539797
0.82541615
0.82522541
0.82535207
0.82552224
0.82544231
0.82538974
0.82571745
0.82571155
0.82551348
0.82555014
0.82522237
0.82501704
0.82508385
0.82538438
0.82544464
0.82569122
0.82547671
0.82543194
0.82535601
0.82518262
0.82511479
0.82502288
INFO - Training [48][   80/  196]   Loss 0.345456   Top1 88.022461   Top5 98.608398   BatchTime 0.380197   LR 0.000703
0.82512373
0.82525533
0.82491314
0.82466060
0.82468849
0.82490426
0.82467663
0.82486588
0.82437670
0.82436550
0.82424748
0.82369477
0.82291919
0.82270634
0.82243109
0.82240409
0.82278466
INFO - Training [48][  100/  196]   Loss 0.334215   Top1 88.351562   Top5 98.730469   BatchTime 0.374527   LR 0.000698
0.82338786
0.82338536
0.82333845
0.82329041
0.82297987
0.82262474
0.82254833
0.82225311
0.82173753
0.82152522
0.82096857
0.82120371
0.82104051
0.82068861
0.82068342
0.82069218
0.82058406
0.82039398
0.82023931
0.82002866
0.81994969
0.81995344
INFO - Training [48][  120/  196]   Loss 0.331251   Top1 88.476562   Top5 98.785807   BatchTime 0.371114   LR 0.000693
0.81992149
0.81970268
0.81973904
0.81993622
0.82058138
0.82122058
0.82271308
0.82288301
0.82281208
0.82289845
0.82280880
0.82274425
0.82255036
0.82247609
0.82226753
0.82206130
0.82191950
0.82187480
0.82181245
0.82177281
0.82167107
INFO - Training [48][  140/  196]   Loss 0.328178   Top1 88.579799   Top5 98.842076   BatchTime 0.360108   LR 0.000688
0.82150310
0.82123995
0.82154232
0.82117510
0.82071990
0.82040852
0.81996018
0.81965297
0.81908137
0.81858480
0.81855798
0.81848085
0.81853336
0.81986177
0.82125205
0.82030308
INFO - Training [48][  160/  196]   Loss 0.332622   Top1 88.400879   Top5 98.840332   BatchTime 0.360660   LR 0.000683
0.81921744
0.81858391
0.81852704
0.82053804
0.82026476
0.82018811
0.82041091
0.82044518
0.82054883
0.82023877
0.82030410
0.82040304
0.82016355
0.82003498
0.82004887
0.82023770
0.82023579
0.82294762
0.82312524
0.82310957
0.82313526
0.82302654
INFO - Training [48][  180/  196]   Loss 0.332224   Top1 88.372396   Top5 98.791233   BatchTime 0.361561   LR 0.000678
0.82306701
0.82298332
0.82301420
0.82305998
0.82316905
0.82273740
0.82308078
0.82307178
0.82307678
0.82307547
0.82297611
0.82301128
0.82368356
0.82496274
0.82506329
0.82502711
INFO - ==> Top1: 88.330    Top5: 98.788    Loss: 0.333
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [48][   20/   40]   Loss 0.422451   Top1 86.738281   Top5 99.277344   BatchTime 0.140934
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.1641)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0521)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0664)
features.2.conv.6 tensor(0.0639)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0340)
features.3.conv.6 tensor(0.0369)
features.4.conv.0 tensor(0.0316)
features.4.conv.3 tensor(0.0949)
features.4.conv.6 tensor(0.0804)
features.5.conv.0 tensor(0.0269)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.0859)
features.6.conv.0 tensor(0.0262)
features.6.conv.3 tensor(0.0480)
features.6.conv.6 tensor(0.0496)
features.7.conv.0 tensor(0.0419)
features.7.conv.3 tensor(0.1192)
features.7.conv.6 tensor(0.0846)
features.8.conv.0 tensor(0.0590)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.0885)
features.9.conv.0 tensor(0.0758)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.0930)
features.10.conv.0 tensor(0.0335)
features.10.conv.3 tensor(0.1085)
features.10.conv.6 tensor(0.0685)
features.11.conv.0 tensor(0.1315)
features.11.conv.3 tensor(0.1242)
features.11.conv.6 tensor(0.2235)
features.12.conv.0 tensor(0.2611)
features.12.conv.3 tensor(0.1875)
features.12.conv.6 tensor(0.3866)
features.13.conv.0 tensor(0.0699)
features.13.conv.3 tensor(0.1682)
features.13.conv.6 tensor(0.1008)
features.14.conv.0 tensor(0.9439)
features.14.conv.3 tensor(0.1181)
features.14.conv.6 tensor(0.9263)
features.15.conv.0 tensor(0.9681)
features.15.conv.3 tensor(0.0936)
features.15.conv.6 tensor(0.9623)
features.16.conv.0 tensor(0.1046)
features.16.conv.3 tensor(0.1279)
features.16.conv.6 tensor(0.1812)
conv.0 tensor(0.1875)
tensor(825910.) 2188896.0
INFO - Validation [48][   40/   40]   Loss 0.415302   Top1 86.950000   Top5 99.380000   BatchTime 0.098147
INFO - ==> Top1: 86.950    Top5: 99.380    Loss: 0.415
INFO - ==> Sparsity : 0.377
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
0.82523000
0.82501602
0.82529706
0.82511789
0.82504135
0.82503533
0.82498598
0.82488924
0.82516319
0.82497746
0.82473528
0.82476819
0.82500887
0.82503361
0.82489985
0.82494086
0.82499653
INFO - Training [49][   20/  196]   Loss 0.330176   Top1 88.359375   Top5 98.085938   BatchTime 0.448927   LR 0.000669
0.82467401
0.82454288
0.82450652
0.82457334
0.82463342
0.82474953
0.82482326
0.82465458
0.82419235
0.82416064
0.82403350
0.82367682
0.82344949
0.82195526
0.82192677
0.82213300
0.82378548
0.82357663
0.82345569
0.82313383
0.82305884
INFO - Training [49][   40/  196]   Loss 0.329747   Top1 88.330078   Top5 98.388672   BatchTime 0.416303   LR 0.000664
0.82308382
0.82299662
0.82292390
0.82307184
0.82308996
0.82303709
0.82292652
0.82240546
0.82239097
0.82258755
0.82251590
0.82270771
0.82295018
0.82288164
0.82277185
0.82266438
0.82249057
0.82238668
0.82228392
0.82211590
0.82182652
0.82157671
INFO - Training [49][   60/  196]   Loss 0.323964   Top1 88.600260   Top5 98.561198   BatchTime 0.398589   LR 0.000659
0.82146943
0.82134944
0.82143909
0.82118672
0.82082343
0.82053125
0.81989568
0.82031137
0.82088459
0.82123208
0.82401139
0.82435906
0.82410067
0.82381743
0.82364959
0.82351863
0.82364649
0.82343662
0.82328367
0.82325143
0.82317358
0.82323885
INFO - Training [49][   80/  196]   Loss 0.326773   Top1 88.525391   Top5 98.647461   BatchTime 0.392010   LR 0.000654
0.82303554
0.82278061
0.82300144
0.82332724
0.82333541
0.82322037
0.82370073
0.82351577
0.82367539
0.82363921
0.82322371
0.82299954
0.82252151
0.82168037
0.82090712
0.82126671
0.82108843
INFO - Training [49][  100/  196]   Loss 0.326616   Top1 88.531250   Top5 98.699219   BatchTime 0.382799   LR 0.000649
0.82206762
0.82187462
0.82089883
0.82017356
0.81987607
0.81985599
0.82038856
0.82047540
0.82042992
0.82045960
0.82026684
0.82012975
0.81992275
0.81959760
0.81920016
0.81906193
0.81883484
0.81857264
0.81843835
0.81843907
0.81878364
0.81976801
0.81951547
INFO - Training [49][  120/  196]   Loss 0.323824   Top1 88.606771   Top5 98.798828   BatchTime 0.377330   LR 0.000644
0.81938106
0.81937397
0.81922001
0.81918538
0.81878203
0.81853223
0.81827593
0.81818575
0.81784159
0.81770217
0.81744045
0.81739670
0.81739450
0.81740922
INFO - Training [49][  140/  196]   Loss 0.321378   Top1 88.738839   Top5 98.853237   BatchTime 0.363847   LR 0.000639
0.81738055
0.81741154
0.81745797
0.81846672
0.81869119
0.81888211
0.81889874
0.82015592
0.82036626
0.82018667
0.82002980
0.81994492
0.81975508
0.81967121
0.81975442
0.81971276
0.82100320
0.82082206
0.82074696
0.82041371
0.81964588
0.81899416
0.82077307
INFO - Training [49][  160/  196]   Loss 0.322883   Top1 88.693848   Top5 98.828125   BatchTime 0.363570   LR 0.000634
0.82061487
0.82087630
0.82095617
0.82084918
0.82097453
0.82105613
0.82096785
0.82088315
0.82084399
0.82084870
0.82072496
0.82075071
0.82080984
0.82090920
0.82078797
0.82070947
0.82045978
0.82005376
0.81953466
0.81921554
0.81922162
0.81915307
0.81922907
INFO - Training [49][  180/  196]   Loss 0.322361   Top1 88.689236   Top5 98.782552   BatchTime 0.362608   LR 0.000629
0.81917667
0.81906718
0.81886923
0.81886166
0.81880486
0.81871629
0.81875104
0.81893092
0.81905669
0.81932724
0.81934625
INFO - ==> Top1: 88.734    Top5: 98.770    Loss: 0.322
0.81912255
0.81892735
0.81885952
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [49][   20/   40]   Loss 0.370886   Top1 88.476562   Top5 99.355469   BatchTime 0.135255
INFO - Validation [49][   40/   40]   Loss 0.360953   Top1 88.400000   Top5 99.480000   BatchTime 0.094058
INFO - ==> Top1: 88.400    Top5: 99.480    Loss: 0.361
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4479)
features.0.conv.3 tensor(0.1582)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0530)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0674)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0386)
features.3.conv.6 tensor(0.0334)
features.4.conv.0 tensor(0.0301)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.0781)
features.5.conv.0 tensor(0.0386)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.0820)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0521)
features.7.conv.0 tensor(0.0402)
features.7.conv.3 tensor(0.1183)
features.7.conv.6 tensor(0.0855)
features.8.conv.0 tensor(0.0570)
features.8.conv.3 tensor(0.1285)
features.8.conv.6 tensor(0.0870)
features.9.conv.0 tensor(0.0743)
features.9.conv.3 tensor(0.1528)
features.9.conv.6 tensor(0.0955)
features.10.conv.0 tensor(0.0378)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.0680)
features.11.conv.0 tensor(0.1325)
features.11.conv.3 tensor(0.1267)
features.11.conv.6 tensor(0.3022)
features.12.conv.0 tensor(0.2510)
features.12.conv.3 tensor(0.1844)
features.12.conv.6 tensor(0.4005)
features.13.conv.0 tensor(0.0690)
features.13.conv.3 tensor(0.1680)
features.13.conv.6 tensor(0.1235)
features.14.conv.0 tensor(0.9431)
features.14.conv.3 tensor(0.1181)
features.14.conv.6 tensor(0.9242)
features.15.conv.0 tensor(0.9666)
features.15.conv.3 tensor(0.0956)
features.15.conv.6 tensor(0.9530)
features.16.conv.0 tensor(0.1004)
features.16.conv.3 tensor(0.1278)
features.16.conv.6 tensor(0.1677)
conv.0 tensor(0.1829)
tensor(823788.) 2188896.0
0.81889689
0.81889081
0.81894368
0.81905633
0.81921566
0.81896168
0.81895363
0.81910568
0.81901145
0.81898516
0.81912869
0.81956750
0.81960160
0.81938082
0.81929463
0.81903750
0.81873602
0.81839621
0.81822056
0.81795764
INFO - Training [50][   20/  196]   Loss 0.322344   Top1 88.417969   Top5 98.417969   BatchTime 0.446810   LR 0.000620
0.81757557
0.81720090
0.81661338
0.81647402
0.81635129
0.81635219
0.81648469
0.81633270
0.81627476
0.81603605
0.81710857
0.81783158
0.81787008
0.81789702
0.81781912
0.81788647
0.81797546
0.81779498
0.81773520
0.81741631
0.81739730
INFO - Training [50][   40/  196]   Loss 0.328150   Top1 88.281250   Top5 98.486328   BatchTime 0.412531   LR 0.000615
0.81731451
0.81708419
0.81694871
0.81692034
0.81687802
0.81674677
0.81685996
0.81692064
0.81693244
0.81695378
0.81760752
0.81962121
0.81937003
0.81900263
0.81876463
0.81857753
INFO - Training [50][   60/  196]   Loss 0.328015   Top1 88.424479   Top5 98.515625   BatchTime 0.403464   LR 0.000610
0.81850672
0.81855214
0.81857383
0.81860566
0.81866151
0.81851315
0.81851161
0.81845415
0.81836641
0.81837511
0.81823003
0.81817889
0.81815481
0.81847280
0.81869125
0.81873369
0.82030326
0.82039315
0.82049799
0.82070309
0.82070613
INFO - Training [50][   80/  196]   Loss 0.327807   Top1 88.491211   Top5 98.666992   BatchTime 0.395010   LR 0.000605
0.82066739
0.82057202
0.82037443
0.82078183
0.82074940
0.82065743
0.82070577
0.82069069
0.82082152
0.82099998
0.82118350
0.82107848
0.82095528
0.82078254
0.82072616
0.82080740
0.82078826
0.82045811
0.81965905
0.81883454
0.81827372
0.81853902
INFO - Training [50][  100/  196]   Loss 0.323600   Top1 88.613281   Top5 98.730469   BatchTime 0.388634   LR 0.000600
0.81890327
0.81922626
0.81916481
0.81904137
0.81897873
0.81913245
0.81912494
0.81916261
0.81944531
0.82049620
0.82121557
0.82126683
0.82091665
0.82106066
0.82089669
0.82053369
0.82043654
0.81989503
INFO - Training [50][  120/  196]   Loss 0.319216   Top1 88.753255   Top5 98.828125   BatchTime 0.380559   LR 0.000595
0.81893796
0.81885672
0.81881672
0.82055742
0.82056510
0.82060277
0.82064486
0.82048696
0.82018882
0.81989253
0.81943816
0.81950659
0.82020819
0.82069016
0.82097346
0.82094198
0.82100725
0.82094592
0.82075387
INFO - Training [50][  140/  196]   Loss 0.315718   Top1 88.844866   Top5 98.909040   BatchTime 0.371874   LR 0.000590
0.82057685
0.82042581
0.82056922
0.82070041
0.82054812
0.81995338
0.81956190
0.81956059
0.81939673
0.81921089
0.81907475
0.81923729
0.81926423
0.81919599
0.81922746
0.81900716
0.81901085
0.81928211
0.81946653
0.81943721
0.81908935
INFO - Training [50][  160/  196]   Loss 0.317948   Top1 88.774414   Top5 98.886719   BatchTime 0.373346   LR 0.000585
0.81902087
0.81891972
0.81882203
0.81902659
0.81914485
0.81915551
0.81939560
0.81906956
0.81901485
0.81891769
0.81894875
0.81929052
0.82105684
0.82130772
0.82132846
0.82106328
0.82053626
0.82025677
0.82007462
0.81996399
0.81967723
0.81943768
INFO - Training [50][  180/  196]   Loss 0.319743   Top1 88.756510   Top5 98.808594   BatchTime 0.371651   LR 0.000580
0.81924468
0.81882697
0.81836665
0.81827122
0.81837279
0.81845081
0.81841785
0.81834805
0.81838560
0.81850344
0.81918538
0.81971747
0.82049191
0.82081074
0.82078224
0.82074457
********************pre-trained*****************
INFO - ==> Top1: 88.798    Top5: 98.828    Loss: 0.318
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [50][   20/   40]   Loss 0.547320   Top1 83.261719   Top5 98.964844   BatchTime 0.130113
INFO - Validation [50][   40/   40]   Loss 0.539249   Top1 83.390000   Top5 99.040000   BatchTime 0.089922
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.1602)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0551)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0654)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0343)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.0955)
features.4.conv.6 tensor(0.0799)
features.5.conv.0 tensor(0.0363)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.0801)
features.6.conv.0 tensor(0.0202)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0533)
features.7.conv.0 tensor(0.0434)
features.7.conv.3 tensor(0.1166)
features.7.conv.6 tensor(0.0826)
features.8.conv.0 tensor(0.0552)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.0865)
features.9.conv.0 tensor(0.0750)
features.9.conv.3 tensor(0.1508)
features.9.conv.6 tensor(0.0994)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.0678)
features.11.conv.0 tensor(0.1326)
features.11.conv.3 tensor(0.1250)
features.11.conv.6 tensor(0.3081)
features.12.conv.0 tensor(0.2414)
features.12.conv.3 tensor(0.1833)
features.12.conv.6 tensor(0.3936)
features.13.conv.0 tensor(0.0755)
features.13.conv.3 tensor(0.1680)
features.13.conv.6 tensor(0.0982)
features.14.conv.0 tensor(0.9438)
features.14.conv.3 tensor(0.1183)
features.14.conv.6 tensor(0.9000)
features.15.conv.0 tensor(0.9677)
features.15.conv.3 tensor(0.0944)
features.15.conv.6 tensor(0.9767)
features.16.conv.0 tensor(0.1004)
features.16.conv.3 tensor(0.1256)
features.16.conv.6 tensor(0.1898)
conv.0 tensor(0.2130)
tensor(840556.) 2188896.0
INFO - ==> Top1: 83.390    Top5: 99.040    Loss: 0.539
INFO - ==> Sparsity : 0.384
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
0.82081997
0.82077539
0.82070136
0.82070488
0.82085484
0.82088763
0.82057345
0.82066458
0.82075387
0.82083839
0.82095683
0.82107091
0.82143641
0.82168353
0.82187158
0.82207787
0.82218057
0.82215589
0.82203203
0.82188076
INFO - Training [51][   20/  196]   Loss 0.320512   Top1 88.671875   Top5 98.261719   BatchTime 0.433961   LR 0.000571
0.82181919
0.82145399
0.82132274
0.82094204
0.82092589
0.82078296
0.82049066
0.82030994
0.82013065
0.81996113
0.81979358
0.81963867
0.81953919
0.81970543
0.81999713
0.81986779
0.81981635
0.81977671
0.81948471
0.81941080
0.81936187
INFO - Training [51][   40/  196]   Loss 0.329012   Top1 88.544922   Top5 98.457031   BatchTime 0.402053   LR 0.000566
0.81926888
0.81920582
0.81904638
0.81888884
0.81892240
0.81884915
0.81858569
0.81838608
0.81837845
0.81836325
0.81830567
0.81830704
0.81863505
0.81880337
0.81921363
0.82133210
INFO - Training [51][   60/  196]   Loss 0.325733   Top1 88.756510   Top5 98.580729   BatchTime 0.392590   LR 0.000561
0.82139748
0.82125390
0.82133353
0.82119733
0.82106322
0.82097203
0.82104421
0.82047319
0.81961793
0.81888402
0.81867993
0.81824362
0.81769300
0.81934553
0.81885397
0.81845826
0.81834275
0.81819785
0.81815583
0.81794226
0.81768292
0.81733263
INFO - Training [51][   80/  196]   Loss 0.323941   Top1 88.828125   Top5 98.701172   BatchTime 0.385273   LR 0.000556
0.81735212
0.81715393
0.81700158
0.81710076
0.81732351
0.81740361
0.81728774
0.81709069
0.81716335
0.81739247
0.81745642
0.81746346
0.81735915
0.81711143
0.81676954
0.81675512
0.81655306
0.81654978
0.81634670
0.81611580
0.81634980
0.81646067
INFO - Training [51][  100/  196]   Loss 0.318773   Top1 89.003906   Top5 98.742188   BatchTime 0.382782   LR 0.000551
0.81638145
0.81635845
0.81626737
0.81626177
0.81607300
0.81608284
0.81601113
0.81590474
0.81579703
0.81557739
0.81539834
0.81488091
0.81482494
0.81501997
0.81490672
0.81532812
0.81545067
0.81548494
0.81549579
INFO - Training [51][  120/  196]   Loss 0.312361   Top1 89.176432   Top5 98.831380   BatchTime 0.372016   LR 0.000546
0.81574076
0.81642944
0.81651330
0.81643015
0.81609076
0.81570965
0.81519926
0.81412661
0.81370199
0.81380016
0.81334549
0.81461841
0.81464881
0.81500548
0.81530452
0.81572038
0.81545407
0.81481427
0.81427270
INFO - Training [51][  140/  196]   Loss 0.310231   Top1 89.257812   Top5 98.900670   BatchTime 0.363050   LR 0.000541
0.81393623
0.81330502
0.81292814
0.81240094
0.81231350
0.81190586
0.81193227
0.81148762
0.81098717
0.81094712
0.81158644
0.81205571
0.81215489
0.81221181
0.81226325
0.81244034
0.81302291
0.81507057
0.81557029
0.81559825
0.81611657
0.81661212
INFO - Training [51][  160/  196]   Loss 0.312965   Top1 89.135742   Top5 98.911133   BatchTime 0.364210   LR 0.000536
0.81675977
0.81673610
0.81670606
0.81693739
0.81976104
0.82004040
0.81992203
0.81987554
0.81971246
0.81970280
0.81966227
0.81968457
0.81998187
0.82017118
0.81988961
0.81979585
0.81981367
INFO - Training [51][  180/  196]   Loss 0.312370   Top1 89.140625   Top5 98.845486   BatchTime 0.362830   LR 0.000531
0.81966138
0.81960958
0.81955582
0.81943858
0.81943029
0.81951028
0.81967294
0.81964922
0.81965762
0.81977212
0.81976241
0.81955957
0.81999868
0.81963021
0.81939816
0.81940871
0.81926864
INFO - ==> Top1: 89.172    Top5: 98.854    Loss: 0.312
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.81911504
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [51][   20/   40]   Loss 0.573324   Top1 82.871094   Top5 99.121094   BatchTime 0.122681
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.1660)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0521)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0645)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0330)
features.4.conv.0 tensor(0.0298)
features.4.conv.3
INFO - Validation [51][   40/   40]   Loss 0.567064   Top1 82.490000   Top5 99.140000   BatchTime 0.089166
INFO - ==> Top1: 82.490    Top5: 99.140    Loss: 0.567
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
features.4.conv.3 tensor(0.0862)
features.4.conv.6 tensor(0.0767)
features.5.conv.0 tensor(0.0360)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.0798)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0500)
features.7.conv.0 tensor(0.0421)
features.7.conv.3 tensor(0.1163)
features.7.conv.6 tensor(0.0865)
features.8.conv.0 tensor(0.0549)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.0871)
features.9.conv.0 tensor(0.0786)
features.9.conv.3 tensor(0.1519)
features.9.conv.6 tensor(0.1021)
features.10.conv.0 tensor(0.0383)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0665)
features.11.conv.0 tensor(0.1375)
features.11.conv.3 tensor(0.1235)
features.11.conv.6 tensor(0.3176)
features.12.conv.0 tensor(0.2452)
features.12.conv.3 tensor(0.1846)
features.12.conv.6 tensor(0.4096)
features.13.conv.0 tensor(0.0712)
features.13.conv.3 tensor(0.1682)
features.13.conv.6 tensor(0.0727)
features.14.conv.0 tensor(0.9430)
features.14.conv.3 tensor(0.1175)
features.14.conv.6 tensor(0.9252)
features.15.conv.0 tensor(0.9674)
features.15.conv.3 tensor(0.0953)
features.15.conv.6 tensor(0.9519)
features.16.conv.0 tensor(0.1037)
features.16.conv.3 tensor(0.1248)
features.16.conv.6 tensor(0.1739)
conv.0 tensor(0.1803)
tensor(822100.) 2188896.0
0.81938881
0.81919253
0.81918961
0.81919038
0.81919384
0.81876832
0.81804198
0.81775576
0.81762308
0.81760395
0.81715286
0.81594580
0.81683594
0.81738567
0.81962156
0.81950516
0.81957871
0.81952125
0.81951916
0.81943184
0.81944257
INFO - Training [52][   20/  196]   Loss 0.305939   Top1 89.062500   Top5 98.593750   BatchTime 0.461357   LR 0.000523
0.81918955
0.81868416
0.81828296
0.81801349
0.81801063
0.81794232
0.81784528
0.81810337
0.81799847
0.81793070
0.81811672
0.81814331
0.81939465
0.81960881
0.82092100
0.82103246
INFO - Training [52][   40/  196]   Loss 0.308330   Top1 89.287109   Top5 98.574219   BatchTime 0.419586   LR 0.000518
0.82119310
0.82126677
0.82120138
0.82104558
0.82100093
0.82090300
0.82083321
0.82074606
0.82084918
0.82090700
0.82102102
0.82100374
0.82097512
0.82125700
0.82098079
0.82092369
0.82105309
0.82111979
0.82094300
0.82078093
0.82074261
INFO - Training [52][   60/  196]   Loss 0.311165   Top1 89.114583   Top5 98.645833   BatchTime 0.404509   LR 0.000513
0.82081217
0.82092321
0.82062250
0.82074410
0.82060498
0.82045966
0.82063419
0.82071048
0.82084692
0.82109541
0.82120782
0.82122648
0.82131344
0.82136983
0.82093519
0.82055324
0.82016909
0.81999832
0.81984836
0.81995529
0.82009530
0.82008380
INFO - Training [52][   80/  196]   Loss 0.312828   Top1 88.974609   Top5 98.813477   BatchTime 0.393655   LR 0.000508
0.82025009
0.81992543
0.81953037
0.81949097
0.81958890
0.81976914
0.81978101
0.81964660
0.81959099
0.81954855
0.81942201
0.81945968
0.81955290
0.81966799
0.81944746
0.81938988
0.81999439
0.82096380
0.82096845
0.82077181
0.82017249
0.81998813
INFO - Training [52][  100/  196]   Loss 0.311184   Top1 89.027344   Top5 98.839844   BatchTime 0.388753   LR 0.000503
0.82005483
0.82058477
0.82127482
0.82115275
0.82089579
0.82080936
0.82074940
0.82054794
0.82041764
0.82024509
0.81946003
0.81908011
0.81877291
0.81867337
0.81851441
0.81844556
0.81853932
0.81847167
0.81897914
INFO - Training [52][  120/  196]   Loss 0.310417   Top1 89.124349   Top5 98.867188   BatchTime 0.376412   LR 0.000498
0.82107407
0.82131481
0.82279509
0.82292694
0.82273889
0.82287556
0.82288063
0.82277435
0.82250673
0.82249588
0.82221180
0.82225072
0.82207119
0.82184321
0.82200032
0.82200706
INFO - Training [52][  140/  196]   Loss 0.307934   Top1 89.190848   Top5 98.936942   BatchTime 0.360492   LR 0.000493
0.82201946
0.82221913
0.82210380
0.82166910
0.82168198
0.82138938
0.82114136
0.82111156
0.82093930
0.82084537
0.82083374
0.82071608
0.82023162
0.82049721
0.81995136
0.81958133
0.81905144
0.81873697
0.81860429
0.81829596
0.81804210
INFO - Training [52][  160/  196]   Loss 0.309018   Top1 89.160156   Top5 98.940430   BatchTime 0.352713   LR 0.000488
0.81832206
0.81849301
0.81858689
0.81864703
0.81871420
0.81867319
0.81865823
0.81860781
0.81878573
0.81879264
0.81967103
0.81989413
0.82001019
0.82004660
0.82042295
0.82020330
0.82010508
0.81988984
0.81989998
0.81986797
0.81989020
INFO - Training [52][  180/  196]   Loss 0.310318   Top1 89.116753   Top5 98.878038   BatchTime 0.354915   LR 0.000483
0.82016993
0.82077891
0.82204819
0.82228076
0.82206571
0.82207274
0.82199025
0.82178444
0.82154310
0.82132989
0.82089728
0.82052910
0.82072258
0.82022160
0.82004851
0.81977260
0.81967360
INFO - ==> Top1: 89.086    Top5: 98.884    Loss: 0.311
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [52][   20/   40]   Loss 0.447171   Top1 86.367188   Top5 99.238281   BatchTime 0.128059
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.1738)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0148)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0671)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0349)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.0845)
features.4.conv.6 tensor(0.0858)
features.5.conv.0 tensor(0.0319)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.0794)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0519)
features.7.conv.0 tensor(0.0403)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.0845)
features.8.conv.0 tensor(0.0535)
features.8.conv.3 tensor(0.1314)
features.8.conv.6 tensor(0.0898)
features.9.conv.0 tensor(0.0750)
features.9.conv.3 tensor(0.1531)
features.9.conv.6 tensor(0.1032)
features.10.conv.0 tensor(0.0377)
features.10.conv.3 tensor(0.1056)
features.10.conv.6 tensor(0.0671)
features.11.conv.0 tensor(0.1381)
features.11.conv.3 tensor(0.1188)
features.11.conv.6 tensor(0.3299)
features.12.conv.0 tensor(0.2467)
features.12.conv.3 tensor(0.1842)
features.12.conv.6 tensor(0.3950)
features.13.conv.0 tensor(0.0754)
features.13.conv.3 tensor(0.1684)
features.13.conv.6 tensor(0.0832)
features.14.conv.0 tensor(0.9422)
features.14.conv.3 tensor(0.1154)
features.14.conv.6 tensor(0.9107)
features.15.conv.0 tensor(0.9685)
features.15.conv.3 tensor(0.0950)
features.15.conv.6 tensor(0.9698)
features.16.conv.0 tensor(0.1085)
features.16.conv.3 tensor(0.1228)
features.16.conv.6 tensor(0.1726)
conv.0 tensor(0.1850)
tensor(826030.) 2188896.0
INFO - Validation [52][   40/   40]   Loss 0.451661   Top1 85.950000   Top5 99.330000   BatchTime 0.091605
INFO - ==> Top1: 85.950    Top5: 99.330    Loss: 0.452
INFO - ==> Sparsity : 0.377
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 89.780   Top5: 99.690]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
0.81944370
0.81945509
0.81955570
0.81952566
0.81946522
0.81929308
0.81941426
0.81967568
0.82076359
0.82035500
0.81997317
0.81978023
0.81966990
0.81960350
0.81957549
0.81946069
0.81938422
0.81910390
0.81915373
INFO - Training [53][   20/  196]   Loss 0.324713   Top1 88.261719   Top5 98.144531   BatchTime 0.446271   LR 0.000474
0.81905288
0.81872869
0.81851500
0.81849831
0.81831306
0.81869507
0.82020885
0.81965023
0.81873548
0.81833577
0.81814748
0.81832635
0.81816781
0.81884277
0.81938475
0.81939960
0.81930542
0.81912231
0.81894237
0.81873870
0.81845480
INFO - Training [53][   40/  196]   Loss 0.319823   Top1 88.789062   Top5 98.281250   BatchTime 0.407499   LR 0.000470
0.81819206
0.81783450
0.81791139
0.81817359
0.81838399
0.81829119
0.81836754
0.81823093
0.81780493
0.81758749
0.81710154
0.81648594
0.81645596
0.81643361
0.81669104
0.81678873
0.81663829
0.81665969
0.81634992
0.81624669
0.81597638
0.81549001
INFO - Training [53][   60/  196]   Loss 0.316371   Top1 89.010417   Top5 98.476562   BatchTime 0.393852   LR 0.000465
0.81532621
0.81528485
0.81513458
0.81503218
0.81515425
0.81617296
0.81598920
0.81563216
0.81556273
0.81548142
0.81541115
0.81535947
0.81520629
0.81512624
0.81516218
0.81514698
INFO - Training [53][   80/  196]   Loss 0.311329   Top1 89.140625   Top5 98.691406   BatchTime 0.388114   LR 0.000460
0.81495821
0.81509191
0.81664121
0.81700438
0.81822616
0.81840444
0.81842208
0.81840438
0.81813496
0.81816143
0.81799370
0.81788802
0.81767136
0.81746888
0.81742811
0.81735855
0.81740373
0.81749839
0.81758511
0.81769639
0.81779593
0.81775147
0.81807697
INFO - Training [53][  100/  196]   Loss 0.303768   Top1 89.386719   Top5 98.789062   BatchTime 0.381964   LR 0.000455
0.81820452
0.81793684
0.81809008
0.81796640
0.81784338
0.81780750
0.81774753
0.81783873
0.81766689
0.81754112
0.81748801
0.81726277
0.81677568
0.81647652
0.81663650
0.81653583
INFO - Training [53][  120/  196]   Loss 0.297105   Top1 89.635417   Top5 98.902995   BatchTime 0.380001   LR 0.000450
0.81653762
0.81642467
0.81631207
0.81623131
0.81615758
0.81632763
0.81635749
0.81628871
0.81635255
0.81627131
0.81822801
0.81848109
0.81848621
0.81810862
0.81799316
0.81742913
0.81708056
0.81659299
0.81647301
0.81613743
0.81579065
0.81547469
0.81533676
0.81538010
INFO - Training [53][  140/  196]   Loss 0.297641   Top1 89.659598   Top5 98.942522   BatchTime 0.373418   LR 0.000445
0.81527251
0.81499517
0.81468111
0.81468445
0.81464106
0.81437647
0.81444210
0.81590766
0.81648445
0.81628329
0.81625670
0.81582487
0.81572878
0.81567961
0.81562430
0.81556267
0.81507689
0.81519645
0.81521326
INFO - Training [53][  160/  196]   Loss 0.300576   Top1 89.558105   Top5 98.906250   BatchTime 0.366082   LR 0.000441
0.81535345
0.81680411
0.81699306
0.81704694
0.81705719
0.81721854
0.81713152
0.81719416
0.81755954
0.81790656
0.81898081
0.81922525
0.81900531
0.81883162
0.81889266
0.81886518
0.81871188
0.81863540
0.81900775
0.81905675
0.81891978
INFO - Training [53][  180/  196]   Loss 0.302919   Top1 89.505208   Top5 98.849826   BatchTime 0.367198   LR 0.000436
0.81873792
0.81867695
0.81854576
0.81846166
0.81844318
0.81843555
0.81845826
0.81850702
0.81844288
0.81844556
0.81829327
0.81792700
0.81749374
0.81702459
0.81670547
********************pre-trained*****************
INFO - ==> Top1: 89.520    Top5: 98.844    Loss: 0.303
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [53][   20/   40]   Loss 0.341248   Top1 89.589844   Top5 99.453125   BatchTime 0.133562
INFO - Validation [53][   40/   40]   Loss 0.325677   Top1 89.850000   Top5 99.570000   BatchTime 0.095123
INFO - ==> Top1: 89.850    Top5: 99.570    Loss: 0.326
INFO - ==> Sparsity : 0.383
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 89.850   Top5: 99.570]
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.1719)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0499)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0674)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0370)
features.3.conv.6 tensor(0.0365)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.0770)
features.5.conv.0 tensor(0.0334)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.1167)
features.6.conv.0 tensor(0.0200)
features.6.conv.3 tensor(0.0417)
features.6.conv.6 tensor(0.0541)
features.7.conv.0 tensor(0.0400)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.0448)
features.8.conv.0 tensor(0.0553)
features.8.conv.3 tensor(0.1285)
features.8.conv.6 tensor(0.0874)
features.9.conv.0 tensor(0.0724)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.0997)
features.10.conv.0 tensor(0.0380)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.0669)
features.11.conv.0 tensor(0.1346)
features.11.conv.3 tensor(0.1190)
features.11.conv.6 tensor(0.3343)
features.12.conv.0 tensor(0.2498)
features.12.conv.3 tensor(0.1842)
features.12.conv.6 tensor(0.4063)
features.13.conv.0 tensor(0.0742)
features.13.conv.3 tensor(0.1713)
features.13.conv.6 tensor(0.0833)
features.14.conv.0 tensor(0.9433)
features.14.conv.3 tensor(0.1150)
features.14.conv.6 tensor(0.9192)
features.15.conv.0 tensor(0.9671)
features.15.conv.3 tensor(0.0947)
features.15.conv.6 tensor(0.9646)
features.16.conv.0 tensor(0.1064)
features.16.conv.3 tensor(0.1226)
features.16.conv.6 tensor(0.2221)
conv.0 tensor(0.1761)
tensor(837541.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
0.81592852
0.81563187
0.81550914
0.81539804
0.81536859
0.81551069
0.81562310
0.81545466
0.81537896
0.81500399
0.81480652
0.81464392
0.81433803
0.81408650
0.81408054
0.81409329
0.81419015
0.81415558
0.81501335
INFO - Training [54][   20/  196]   Loss 0.328287   Top1 88.417969   Top5 98.300781   BatchTime 0.450914   LR 0.000427
0.81696826
0.81721622
0.81668174
0.81671697
0.81661481
0.81657970
0.81665158
0.81670237
0.81663471
0.81770504
0.81787640
0.81802678
0.81824887
0.81833804
0.81857753
0.81813371
0.81815869
0.81826878
0.81823224
0.81816798
0.81812781
0.81820965
INFO - Training [54][   40/  196]   Loss 0.318954   Top1 88.798828   Top5 98.564453   BatchTime 0.407645   LR 0.000423
0.82022893
0.82020932
0.81993955
0.81972516
0.81975490
0.82005632
0.82016081
0.81989288
0.81986898
0.81974143
0.81983501
0.81952477
0.81907523
0.81897777
0.81887501
0.81866252
0.81850779
0.81840372
0.81827432
0.81834489
0.81806761
INFO - Training [54][   60/  196]   Loss 0.310836   Top1 89.042969   Top5 98.645833   BatchTime 0.399591   LR 0.000418
0.81785887
0.81736261
0.81680846
0.81688637
0.81793249
0.81856120
0.81859684
0.81873399
0.81897557
0.81910503
0.81890285
0.81910950
0.82087427
0.82044011
0.82032752
0.82054478
INFO - Training [54][   80/  196]   Loss 0.308883   Top1 89.189453   Top5 98.759766   BatchTime 0.390667   LR 0.000413
0.82061297
0.82185185
0.82142562
0.82115859
0.82091153
0.82057786
0.82041538
0.82054645
0.82059747
0.82061625
0.82063460
0.82055503
0.82075232
0.82079244
0.82116455
0.82105637
0.82097703
0.82070059
0.82051253
0.82054001
0.82057470
0.82057506
INFO - Training [54][  100/  196]   Loss 0.301859   Top1 89.410156   Top5 98.773438   BatchTime 0.385766   LR 0.000408
0.82066953
0.82056302
0.82057440
0.82042706
0.82045275
0.82047814
0.82029921
0.82042700
0.82043904
0.82048655
0.82046455
0.82042778
0.82040143
0.82042927
0.82047719
0.82053173
0.82082880
INFO - Training [54][  120/  196]   Loss 0.295986   Top1 89.664714   Top5 98.873698   BatchTime 0.380782   LR 0.000404
0.82076013
0.82048380
0.82033563
0.82032657
0.82037789
0.82037121
0.82064664
0.82235628
0.82224137
0.82221013
0.82222122
0.82222641
0.82217324
0.82219654
0.82236481
0.82213420
0.82190078
0.82184654
0.82180721
0.82137018
0.82092392
0.82055831
0.82058525
0.82033843
0.82034272
INFO - Training [54][  140/  196]   Loss 0.297056   Top1 89.665179   Top5 98.928571   BatchTime 0.371671   LR 0.000399
0.82054162
0.82077813
0.82089221
0.82098728
0.82035917
0.82025462
0.81986904
0.81958061
0.81931955
0.81918794
0.81885833
0.81836230
0.81830853
0.81840044
0.81810313
0.81784052
0.81792361
0.81790590
0.81749988
0.81699109
INFO - Training [54][  160/  196]   Loss 0.299739   Top1 89.572754   Top5 98.955078   BatchTime 0.362182   LR 0.000394
0.81681401
0.81654787
0.81647903
0.81636012
0.81628430
0.81636995
0.81627941
0.81699842
0.81818676
0.81773514
0.81746578
0.81722295
0.81708121
0.81691372
0.81689876
0.81782693
0.81741810
0.81715190
INFO - Training [54][  180/  196]   Loss 0.300935   Top1 89.537760   Top5 98.908420   BatchTime 0.358373   LR 0.000390
0.81684560
0.81656224
0.81641245
0.81579846
0.81537354
0.81527901
0.81524020
0.81511664
0.81494141
0.81485230
0.81469584
0.81457293
0.81450474
0.81441903
0.81437671
0.81429201
********************pre-trained*****************
INFO - ==> Top1: 89.500    Top5: 98.912    Loss: 0.301
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [54][   20/   40]   Loss 0.321526   Top1 90.097656   Top5 99.570312   BatchTime 0.128814
INFO - Validation [54][   40/   40]   Loss 0.307577   Top1 90.430000   Top5 99.640000   BatchTime 0.092971
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.1680)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0477)
features.2.conv.0 tensor(0.0150)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0660)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0324)
features.3.conv.6 tensor(0.0356)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.0903)
features.4.conv.6 tensor(0.0820)
features.5.conv.0 tensor(0.0330)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.0830)
features.6.conv.0 tensor(0.0192)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0534)
features.7.conv.0 tensor(0.0401)
features.7.conv.3 tensor(0.1160)
features.7.conv.6 tensor(0.0467)
features.8.conv.0 tensor(0.0517)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.0871)
features.9.conv.0 tensor(0.0725)
features.9.conv.3 tensor(0.1531)
features.9.conv.6 tensor(0.1332)
features.10.conv.0 tensor(0.0384)
features.10.conv.3 tensor(0.1056)
features.10.conv.6 tensor(0.0656)
features.11.conv.0 tensor(0.1390)
features.11.conv.3 tensor(0.1167)
features.11.conv.6 tensor(0.3374)
features.12.conv.0 tensor(0.2510)
features.12.conv.3 tensor(0.1829)
features.12.conv.6 tensor(0.4031)
features.13.conv.0 tensor(0.0736)
features.13.conv.3 tensor(0.1696)
features.13.conv.6 tensor(0.0900)
features.14.conv.0 tensor(0.9474)
features.14.conv.3 tensor(0.1148)
features.14.conv.6 tensor(0.9202)
features.15.conv.0 tensor(0.9667)
features.15.conv.3 tensor(0.0959)
features.15.conv.6 tensor(0.9651)
features.16.conv.0 tensor(0.1061)
features.16.conv.3 tensor(0.1207)
features.16.conv.6 tensor(0.1934)
conv.0 tensor(0.1782)
tensor(831709.) 2188896.0
INFO - ==> Top1: 90.430    Top5: 99.640    Loss: 0.308
INFO - ==> Sparsity : 0.380
INFO - Scoreboard best 1 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [54][Top1: 90.430   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [41][Top1: 90.320   Top5: 99.670]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
0.81423223
0.81424016
0.81404519
0.81422740
0.81401062
0.81404948
0.81409919
0.81377548
0.81342423
0.81312841
0.81308126
0.81279236
0.81263614
0.81252986
0.81248474
0.81236702
0.81209433
0.81182218
0.81165802
INFO - Training [55][   20/  196]   Loss 0.327173   Top1 88.613281   Top5 98.417969   BatchTime 0.450575   LR 0.000381
0.81153935
0.81163383
0.81149906
0.81170440
0.81196648
0.81354958
0.81268871
0.81249630
0.81239408
0.81256509
0.81280327
0.81339830
0.81535572
0.81589395
0.81597650
0.81578118
0.81568450
0.81563234
0.81567729
0.81537694
0.81529784
INFO - Training [55][   40/  196]   Loss 0.329051   Top1 88.505859   Top5 98.486328   BatchTime 0.407996   LR 0.000377
0.81517869
0.81509858
0.81518620
0.81530356
0.81530672
0.81509995
0.81526059
0.81540602
0.81536126
0.81528598
0.81521899
0.81527162
0.81521642
0.81520629
0.81518018
0.81509209
0.81507379
0.81502891
0.81557059
0.81535494
0.81498450
0.81493860
INFO - Training [55][   60/  196]   Loss 0.319185   Top1 88.828125   Top5 98.684896   BatchTime 0.398312   LR 0.000372
0.81473976
0.81470364
0.81459779
0.81440151
0.81438065
0.81416363
0.81376767
0.81358808
0.81316859
0.81298310
0.81283456
0.81277996
0.81249416
0.81210941
0.81165010
0.81138664
INFO - Training [55][   80/  196]   Loss 0.313953   Top1 88.994141   Top5 98.808594   BatchTime 0.390784   LR 0.000368
0.81137770
0.81123120
0.81133908
0.81172335
0.81242752
0.81380826
0.81644040
0.81651682
0.81624341
0.81601489
0.81587839
0.81589222
0.81583047
0.81577808
0.81581867
0.81605023
0.81608337
0.81601053
0.81613314
0.81592280
0.81585324
0.81569076
INFO - Training [55][  100/  196]   Loss 0.304911   Top1 89.343750   Top5 98.867188   BatchTime 0.384852   LR 0.000363
0.81552869
0.81555587
0.81570107
0.81573671
0.81595623
0.81607592
0.81626844
0.81650734
0.81625903
0.81617993
0.81606203
0.81603038
0.81592673
0.81577164
0.81610435
0.81782115
0.81805384
0.81799424
0.81798548
0.81802994
0.81824255
0.81862170
INFO - Training [55][  120/  196]   Loss 0.299918   Top1 89.459635   Top5 98.955078   BatchTime 0.381732   LR 0.000358
0.81890357
0.81914300
0.81905162
0.81905895
0.81881297
0.81845492
0.81790859
0.81760818
0.81777543
0.81780469
0.81791312
0.81789762
0.81789756
0.81981671
0.81966108
0.81966263
0.81958771
INFO - Training [55][  140/  196]   Loss 0.295585   Top1 89.662388   Top5 99.026228   BatchTime 0.377575   LR 0.000354
0.81972367
0.81976575
0.81953561
0.81931549
0.81929284
0.81927717
0.81966740
0.82006508
0.82006812
0.82017082
0.82019830
0.81987911
0.82003456
0.81994087
0.81979543
0.81973630
0.81955057
0.81947112
0.81926346
0.81949961
0.82150000
0.82328302
0.82433701
INFO - Training [55][  160/  196]   Loss 0.297570   Top1 89.628906   Top5 98.991699   BatchTime 0.373402   LR 0.000349
0.82527405
0.82565403
0.82590687
0.82628590
0.82652622
0.82655042
0.82604837
0.82595390
0.82595623
0.82577413
0.82556230
0.82540089
0.82526499
0.82500547
0.82460833
0.82463086
0.82444620
0.82402754
0.82349896
INFO - Training [55][  180/  196]   Loss 0.299146   Top1 89.613715   Top5 98.865017   BatchTime 0.368104   LR 0.000345
0.82298315
0.82267517
0.82249206
0.82231539
0.82220834
0.82210237
0.82193923
0.82191622
0.82199806
0.82186955
0.82173473
0.82183099
0.82159323
0.82175815
0.82158536
********************pre-trained*****************
INFO - ==> Top1: 89.662    Top5: 98.858    Loss: 0.297
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [55][   20/   40]   Loss 0.301556   Top1 90.703125   Top5 99.531250   BatchTime 0.138569
INFO - Validation [55][   40/   40]   Loss 0.288396   Top1 91.080000   Top5 99.720000   BatchTime 0.098204
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0486)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0674)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0352)
features.4.conv.0 tensor(0.0277)
features.4.conv.3 tensor(0.0874)
features.4.conv.6 tensor(0.0926)
features.5.conv.0 tensor(0.0308)
features.5.conv.3 tensor(0.0637)
features.5.conv.6 tensor(0.0863)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0531)
features.7.conv.0 tensor(0.0405)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.0485)
features.8.conv.0 tensor(0.0508)
features.8.conv.3 tensor(0.1250)
features.8.conv.6 tensor(0.0943)
features.9.conv.0 tensor(0.0699)
features.9.conv.3 tensor(0.1502)
features.9.conv.6 tensor(0.1051)
features.10.conv.0 tensor(0.0398)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0656)
features.11.conv.0 tensor(0.1393)
features.11.conv.3 tensor(0.1186)
features.11.conv.6 tensor(0.3344)
features.12.conv.0 tensor(0.2497)
features.12.conv.3 tensor(0.1825)
features.12.conv.6 tensor(0.3628)
features.13.conv.0 tensor(0.0714)
features.13.conv.3 tensor(0.1692)
features.13.conv.6 tensor(0.0906)
features.14.conv.0 tensor(0.9479)
features.14.conv.3 tensor(0.1144)
features.14.conv.6 tensor(0.9190)
features.15.conv.0 tensor(0.9675)
features.15.conv.3 tensor(0.0966)
features.15.conv.6 tensor(0.9639)
features.16.conv.0 tensor(0.1078)
features.16.conv.3 tensor(0.1226)
features.16.conv.6 tensor(0.1738)
conv.0 tensor(0.1763)
tensor(822053.) 2188896.0
INFO - ==> Top1: 91.080    Top5: 99.720    Loss: 0.288
INFO - ==> Sparsity : 0.376
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 91.080   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [54][Top1: 90.430   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  56
INFO - Training: 50000 samples (256 per mini-batch)
0.82146764
0.82134706
0.82121021
0.82081872
0.82040977
0.82066512
0.82016373
0.81990200
0.81955427
0.81933761
0.81920457
0.81909901
0.81925267
0.81932652
0.81901801
0.81867969
0.81844860
0.81848752
0.81963694
0.81905353
0.81861246
INFO - Training [56][   20/  196]   Loss 0.321561   Top1 88.496094   Top5 98.476562   BatchTime 0.441555   LR 0.000337
0.81805086
0.81729209
0.81647718
0.81605232
0.81579262
0.81589955
0.81607866
0.81712836
0.82051033
0.82011908
0.81985241
0.82007635
0.81974685
0.81936413
0.81897599
0.81861758
INFO - Training [56][   40/  196]   Loss 0.318746   Top1 88.535156   Top5 98.574219   BatchTime 0.406769   LR 0.000333
0.81834823
0.81824148
0.81807405
0.81803334
0.81823021
0.81788683
0.81799221
0.81793541
0.81777370
0.81762898
0.81743312
0.81734359
0.81721097
0.81679779
0.81643409
0.81610477
0.81603676
0.81594747
0.81571698
0.81564176
0.81541365
0.81540769
INFO - Training [56][   60/  196]   Loss 0.311371   Top1 89.036458   Top5 98.828125   BatchTime 0.395471   LR 0.000328
0.81525046
0.81500798
0.81493849
0.81464082
0.81468660
0.81459290
0.81433266
0.81425714
0.81419677
0.81424677
0.81408900
0.81403208
0.81386513
0.81387097
0.81380469
0.81366730
0.81374222
0.81387377
0.81375974
0.81361514
0.81363916
0.81377929
INFO - Training [56][   80/  196]   Loss 0.306706   Top1 89.287109   Top5 98.950195   BatchTime 0.388884   LR 0.000324
0.81377190
0.81355017
0.81351525
0.81356120
0.81351185
0.81356722
0.81327975
0.81309325
0.81322229
0.81515479
0.81527668
0.81523490
0.81516689
0.81504107
0.81472170
0.81446916
INFO - Training [56][  100/  196]   Loss 0.301265   Top1 89.441406   Top5 98.953125   BatchTime 0.383395   LR 0.000319
0.81420511
0.81368232
0.81404299
0.81404322
0.81402653
0.81381416
0.81365615
0.81372696
0.81390369
0.81437922
0.81458932
0.81483954
0.81488585
0.81463027
0.81480384
0.81491864
0.81482190
0.81454527
0.81442422
0.81450021
0.81434393
0.81425804
INFO - Training [56][  120/  196]   Loss 0.296632   Top1 89.563802   Top5 99.013672   BatchTime 0.379872   LR 0.000315
0.81403184
0.81342953
0.81326103
0.81314522
0.81300104
0.81293684
0.81283081
0.81267434
0.81253546
0.81274861
0.81281215
0.81312788
0.81301039
0.81311393
0.81276423
0.81273371
0.81269282
0.81271917
0.81287533
0.81449211
0.81581765
0.81578100
0.81573683
INFO - Training [56][  140/  196]   Loss 0.295293   Top1 89.598214   Top5 99.045759   BatchTime 0.376686   LR 0.000311
0.81566179
0.81552911
0.81517309
0.81474698
0.81448990
0.81447047
0.81569201
0.81499046
0.81461740
0.81462699
0.81460738
0.81447917
0.81430691
0.81411022
0.81400073
INFO - Training [56][  160/  196]   Loss 0.297714   Top1 89.567871   Top5 99.055176   BatchTime 0.377083   LR 0.000306
0.81404841
0.81408054
0.81405884
0.81367952
0.81344140
0.81381184
0.81556052
0.81763244
0.81848741
0.81845021
0.81831801
0.81864494
0.81829017
0.81810308
0.81790000
0.81785810
0.81769174
0.81764305
0.81785393
0.81802088
0.81792295
0.81768280
0.81759208
0.81757140
0.81752372
0.81752050
INFO - Training [56][  180/  196]   Loss 0.298135   Top1 89.583333   Top5 98.995226   BatchTime 0.370061   LR 0.000302
0.81743616
0.81750977
0.81744611
0.81734210
0.81724280
0.81716353
0.81708425
0.81693512
0.81691909
0.81690651
0.81681782
0.81685817
INFO - ==> Top1: 89.630    Top5: 98.980    Loss: 0.297
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.81696111
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [56][   20/   40]   Loss 0.315003   Top1 90.253906   Top5 99.570312   BatchTime 0.129299
INFO - Validation [56][   40/   40]   Loss 0.294919   Top1 90.800000   Top5 99.710000   BatchTime 0.090995
INFO - ==> Top1: 90.800    Top5: 99.710    Loss: 0.295
INFO - ==> Sparsity : 0.378
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 91.080   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [56][Top1: 90.800   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.1777)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0668)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0355)
features.3.conv.6 tensor(0.0369)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.0778)
features.5.conv.0 tensor(0.0342)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.0828)
features.6.conv.0 tensor(0.0220)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0539)
features.7.conv.0 tensor(0.0411)
features.7.conv.3 tensor(0.1157)
features.7.conv.6 tensor(0.0526)
features.8.conv.0 tensor(0.0505)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.0878)
features.9.conv.0 tensor(0.0692)
features.9.conv.3 tensor(0.1499)
features.9.conv.6 tensor(0.0989)
features.10.conv.0 tensor(0.0395)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0662)
features.11.conv.0 tensor(0.1455)
features.11.conv.3 tensor(0.1169)
features.11.conv.6 tensor(0.3438)
features.12.conv.0 tensor(0.2591)
features.12.conv.3 tensor(0.1819)
features.12.conv.6 tensor(0.3978)
features.13.conv.0 tensor(0.0743)
features.13.conv.3 tensor(0.1680)
features.13.conv.6 tensor(0.0893)
features.14.conv.0 tensor(0.9490)
features.14.conv.3 tensor(0.1141)
features.14.conv.6 tensor(0.9251)
features.15.conv.0 tensor(0.9681)
features.15.conv.3 tensor(0.0966)
features.15.conv.6 tensor(0.9765)
features.16.conv.0 tensor(0.1107)
features.16.conv.3 tensor(0.1218)
features.16.conv.6 tensor(0.1725)
conv.0 tensor(0.1741)
tensor(827436.) 2188896.0
0.81688929
0.81732440
0.81860024
0.81878418
0.81889820
0.81899631
0.81908023
0.81904310
0.81901842
0.81890869
0.81880605
0.81883126
0.81895781
0.81897426
0.81854361
0.81880307
0.81857938
0.81858081
0.81864917
0.81874323
0.81835133
INFO - Training [57][   20/  196]   Loss 0.325340   Top1 88.808594   Top5 98.339844   BatchTime 0.452686   LR 0.000294
0.81846303
0.81836563
0.81854343
0.81850725
0.81830996
0.81826991
0.81843638
0.81843632
0.81824899
0.81834388
0.81841707
0.81816292
0.81788945
0.81769913
0.81767607
0.81776887
INFO - Training [57][   40/  196]   Loss 0.307926   Top1 89.570312   Top5 98.593750   BatchTime 0.409285   LR 0.000290
0.81760997
0.81751645
0.81742054
0.81730795
0.81726289
0.81737250
0.81737405
0.81747568
0.81717628
0.81695861
0.81670266
0.81658745
0.81630981
0.81624424
0.81613266
0.81595284
0.81574684
0.81546384
0.81484085
0.81418502
0.81347781
0.81294596
INFO - Training [57][   60/  196]   Loss 0.300689   Top1 89.824219   Top5 98.710938   BatchTime 0.396279   LR 0.000286
0.81248593
0.81214118
0.81168252
0.81127769
0.81082892
0.81039870
0.81028187
0.81010181
0.80989569
0.80980068
0.80986643
0.80980527
0.80986458
0.81020176
0.81109244
0.81144756
0.81171262
0.81170171
0.81170046
0.81180519
0.81191230
0.81199974
INFO - Training [57][   80/  196]   Loss 0.296510   Top1 89.765625   Top5 98.852539   BatchTime 0.389255   LR 0.000282
0.81210166
0.81222653
0.81234616
0.81253868
0.81285346
0.81281626
0.81266505
0.81262130
0.81262475
0.81271601
0.81270844
0.81263632
0.81257784
0.81236905
0.81215107
0.81201398
0.81201577
0.81214970
0.81554306
0.81578642
0.81566834
0.81543207
INFO - Training [57][  100/  196]   Loss 0.290771   Top1 89.914062   Top5 98.910156   BatchTime 0.385663   LR 0.000277
0.81537980
0.81555587
0.81556344
0.81599206
0.81595713
0.81600404
0.81599540
0.81592661
0.81592268
0.81612849
0.81619000
0.81607705
0.81610924
0.81628478
0.81642950
0.81769425
INFO - Training [57][  120/  196]   Loss 0.287171   Top1 90.032552   Top5 98.961589   BatchTime 0.381705   LR 0.000273
0.81784666
0.81798220
0.81803387
0.81790024
0.81773365
0.81756741
0.81741458
0.81724697
0.81702828
0.81713730
0.81712109
0.81707549
0.81690127
0.81691635
0.81689698
0.81690264
0.81647319
0.81641757
0.81633633
0.81616050
0.81608975
0.81612903
0.81608427
INFO - Training [57][  140/  196]   Loss 0.287986   Top1 90.058594   Top5 99.009487   BatchTime 0.378074   LR 0.000269
0.81595212
0.81582135
0.81590259
0.81574845
0.81557119
0.81536716
0.81511384
0.81558853
0.81555080
0.81539035
0.81524235
0.81512994
0.81486976
0.81444705
0.81431651
0.81416404
INFO - Training [57][  160/  196]   Loss 0.289587   Top1 90.039062   Top5 98.972168   BatchTime 0.376980   LR 0.000265
0.81399560
0.81387198
0.81366539
0.81350845
0.81352156
0.81339538
0.81333488
0.81330955
0.81320840
0.81294417
0.81257093
0.81213838
0.81214923
0.81294137
0.81309479
0.81308359
0.81268042
0.81238991
0.81221193
0.81199545
0.81219190
0.81230021
0.81233674
0.81228602
0.81211621
0.81177616
INFO - Training [57][  180/  196]   Loss 0.291689   Top1 89.976128   Top5 98.917101   BatchTime 0.370512   LR 0.000261
0.81152076
0.81114596
0.81075424
0.81055802
0.81033504
0.81024140
0.81008059
0.81001240
0.80990535
0.80989593
0.80981231
INFO - ==> Top1: 90.022    Top5: 98.920    Loss: 0.291
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80990624
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [57][   20/   40]   Loss 0.308139   Top1 90.351562   Top5 99.550781   BatchTime 0.129975
INFO - Validation [57][   40/   40]   Loss 0.309631   Top1 90.120000   Top5 99.650000   BatchTime 0.094161
INFO - ==> Top1: 90.120    Top5: 99.650    Loss: 0.310
INFO - ==> Sparsity : 0.407
INFO - Scoreboard best 1 ==> Epoch [55][Top1: 91.080   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [56][Top1: 90.800   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [42][Top1: 90.480   Top5: 99.650]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4340)
features.0.conv.3 tensor(0.2070)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0477)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0668)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0347)
features.4.conv.0 tensor(0.0304)
features.4.conv.3 tensor(0.0874)
features.4.conv.6 tensor(0.0757)
features.5.conv.0 tensor(0.0342)
features.5.conv.3 tensor(0.0642)
features.5.conv.6 tensor(0.0845)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0545)
features.7.conv.0 tensor(0.0441)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.0596)
features.8.conv.0 tensor(0.0504)
features.8.conv.3 tensor(0.1256)
features.8.conv.6 tensor(0.0893)
features.9.conv.0 tensor(0.0723)
features.9.conv.3 tensor(0.1508)
features.9.conv.6 tensor(0.1040)
features.10.conv.0 tensor(0.0359)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.0658)
features.11.conv.0 tensor(0.1419)
features.11.conv.3 tensor(0.1179)
features.11.conv.6 tensor(0.3449)
features.12.conv.0 tensor(0.2652)
features.12.conv.3 tensor(0.1833)
features.12.conv.6 tensor(0.4087)
features.13.conv.0 tensor(0.0732)
features.13.conv.3 tensor(0.1686)
features.13.conv.6 tensor(0.0905)
features.14.conv.0 tensor(0.9514)
features.14.conv.3 tensor(0.1153)
features.14.conv.6 tensor(0.9184)
features.15.conv.0 tensor(0.9686)
features.15.conv.3 tensor(0.0943)
features.15.conv.6 tensor(0.9640)
features.16.conv.0 tensor(0.1106)
features.16.conv.3 tensor(0.1230)
features.16.conv.6 tensor(0.2833)
conv.0 tensor(0.2472)
tensor(890101.) 2188896.0
0.81009614
0.81012207
0.81023353
0.81008792
0.81013709
0.81010711
0.81004620
0.80989349
0.80983496
0.80984271
0.80995607
0.80958593
0.80944681
0.81218719
0.81367856
0.81358123
0.81334722
0.81317276
0.81297582
0.81272525
0.81250829
0.81264371
INFO - Training [58][   20/  196]   Loss 0.309365   Top1 88.828125   Top5 98.515625   BatchTime 0.433089   LR 0.000254
0.81256270
0.81210035
0.81185287
0.81163943
0.81159955
0.81141686
0.81140023
0.81140584
0.81137484
0.81120360
0.81121647
0.81139177
0.81151396
0.81146914
0.81169820
0.81149983
INFO - Training [58][   40/  196]   Loss 0.307951   Top1 88.876953   Top5 98.652344   BatchTime 0.404109   LR 0.000250
0.81149459
0.81144679
0.81138688
0.81130427
0.81119841
0.81092072
0.81081802
0.81079966
0.81053859
0.81042737
0.81031281
0.81000859
0.80972183
0.80935693
0.80900985
0.80859834
0.80902648
0.80973202
0.80963802
0.80933243
0.80904549
0.80876678
INFO - Training [58][   60/  196]   Loss 0.301898   Top1 89.173177   Top5 98.763021   BatchTime 0.391671   LR 0.000246
0.80853552
0.80842340
0.80846715
0.80813968
0.80801827
0.80742449
0.80725342
0.80711514
0.80683035
0.80680275
0.80680424
0.80652857
0.80634075
0.80592942
0.80570257
0.80575812
0.80584753
0.80605227
0.80784738
0.80873132
0.80912781
0.80926967
INFO - Training [58][   80/  196]   Loss 0.299035   Top1 89.365234   Top5 98.901367   BatchTime 0.385098   LR 0.000242
0.80925137
0.80895644
0.80882657
0.80859464
0.80838156
0.80835623
0.80838335
0.80834758
0.80831677
0.80834639
0.80778211
0.80779356
0.80772591
0.80767834
0.80764645
0.80762005
INFO - Training [58][  100/  196]   Loss 0.294336   Top1 89.578125   Top5 98.933594   BatchTime 0.384075   LR 0.000238
0.80762136
0.80782419
0.80815130
0.80848277
0.80881923
0.80913681
0.80953282
0.80988145
0.81006765
0.81016034
0.81059682
0.81297302
0.81299359
0.81302118
0.81382728
0.81364191
0.81329983
0.81306684
0.81292969
0.81309098
0.81322861
0.81305844
INFO - Training [58][  120/  196]   Loss 0.290246   Top1 89.703776   Top5 98.997396   BatchTime 0.381176   LR 0.000234
0.81270128
0.81260568
0.81262779
0.81268245
0.81266147
0.81256711
0.81264991
0.81267476
0.81276238
0.81281525
0.81285220
0.81281441
0.81284010
0.81276268
0.81246489
0.81212670
0.81174028
0.81144828
0.81115288
0.81084895
0.81063592
0.81052172
INFO - Training [58][  140/  196]   Loss 0.287991   Top1 89.785156   Top5 99.029018   BatchTime 0.377038   LR 0.000230
0.81038374
0.81019855
0.81002438
0.80990231
0.80982411
0.80976152
0.80959886
0.80936736
0.80910385
0.80898046
0.80882412
0.80885100
0.80887860
0.80887717
0.81071496
0.81135231
0.81213647
INFO - Training [58][  160/  196]   Loss 0.290442   Top1 89.768066   Top5 98.989258   BatchTime 0.375654   LR 0.000226
0.81330377
0.81318980
0.81318903
0.81322342
0.81313157
0.81300670
0.81290966
0.81284916
0.81274581
0.81268996
0.81261820
0.81264788
0.81272042
0.81276852
0.81258541
0.81261754
0.81251943
0.81237179
INFO - Training [58][  180/  196]   Loss 0.291189   Top1 89.726562   Top5 98.938802   BatchTime 0.369781   LR 0.000222
0.81219304
0.81201172
0.81198746
0.81165367
0.81118429
0.81096625
0.81081700
0.81075996
0.81030309
0.81017786
0.80993319
0.80974132
0.80963653
0.80970842
0.81003207
0.81001765
0.80978423
0.80958831
0.80958521
********************pre-trained*****************
INFO - ==> Top1: 89.790    Top5: 98.958    Loss: 0.289
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [58][   20/   40]   Loss 0.292050   Top1 91.074219   Top5 99.609375   BatchTime 0.165531
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0464)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0656)
features.2.conv.6 tensor(0.0689)
features.3.conv.0 tensor(0.0142)
features.3.conv.3 tensor(0.0324)
features.3.conv.6 tensor(0.0365)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.0862)
features.4.conv.6 tensor(0.0767)
features.5.conv.0 tensor(0.0334)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.1006)
features.6.conv.0 tensor(0.0200)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0553)
features.7.conv.0 tensor(0.0446)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.0652)
features.8.conv.0 tensor(0.0536)
features.8.conv.3 tensor(0.1273)
features.8.conv.6 tensor(0.0897)
features.9.conv.0 tensor(0.0708)
features.9.conv.3 tensor(0.1502)
features.9.conv.6 tensor(0.1104)
features.10.conv.0 tensor(0.0379)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0671)
features.11.conv.0 tensor(0.1501)
features.11.conv.3 tensor(0.1161)
features.11.conv.6 tensor(0.3619)
features.12.conv.0 tensor(0.2765)
features.12.conv.3 tensor(0.1809)
features.12.conv.6 tensor(0.4020)
features.13.conv.0 tensor(0.0743)
features.13.conv.3 tensor(0.1678)
features.13.conv.6 tensor(0.0489)
features.14.conv.0 tensor(0.9524)
features.14.conv.3 tensor(0.1154)
features.14.conv.6 tensor(0.9355)
features.15.conv.0 tensor(0.9697)
features.15.conv.3 tensor(0.0927)
features.15.conv.6 tensor(0.9716)
features.16.conv.0 tensor(0.1105)
features.16.conv.3 tensor(0.1219)
features.16.conv.6 tensor(0.2023)
conv.0 tensor(0.4123)
tensor(935310.) 2188896.0
INFO - Validation [58][   40/   40]   Loss 0.277944   Top1 91.280000   Top5 99.660000   BatchTime 0.107721
INFO - ==> Top1: 91.280    Top5: 99.660    Loss: 0.278
INFO - ==> Sparsity : 0.427
INFO - Scoreboard best 1 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 91.080   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 90.800   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
0.80945903
0.80949235
0.80983025
0.81200308
0.81203067
0.81237334
0.81255192
0.81259596
0.81252688
0.81267649
0.81314707
0.81342483
0.81346458
0.81353515
0.81361699
0.81345665
0.81341356
0.81350124
0.81328911
0.81315321
INFO - Training [59][   20/  196]   Loss 0.302464   Top1 89.511719   Top5 98.554688   BatchTime 0.453525   LR 0.000215
0.81327903
0.81311202
0.81296253
0.81276089
0.81255418
0.81237519
0.81228954
0.81204689
0.81199300
0.81187987
0.81166846
0.81167710
0.81170541
0.81190169
0.81203818
0.81231397
0.81224984
0.81208986
0.81204849
0.81227380
0.81254673
INFO - Training [59][   40/  196]   Loss 0.301784   Top1 89.628906   Top5 98.593750   BatchTime 0.409803   LR 0.000212
0.81227779
0.81197000
0.81179619
0.81119078
0.81117505
0.81072134
0.81038457
0.80990392
0.80944115
0.80902803
0.80874580
0.80850559
0.80823404
0.80783397
0.80747426
0.80726063
0.80735517
INFO - Training [59][   60/  196]   Loss 0.301642   Top1 89.726562   Top5 98.691406   BatchTime 0.397205   LR 0.000208
0.80740714
0.80706942
0.80663615
0.80603331
0.80553997
0.80522019
0.80506551
0.80711758
0.80686551
0.80658734
0.80642182
0.80623013
0.80631655
0.80631077
0.80751443
0.80927253
0.80902821
0.81032848
0.80996090
0.80943960
0.80934334
INFO - Training [59][   80/  196]   Loss 0.298159   Top1 89.819336   Top5 98.818359   BatchTime 0.391265   LR 0.000204
0.80898613
0.80864739
0.80851030
0.80860454
0.80838305
0.80824560
0.80796331
0.80803424
0.80787885
0.80784500
0.80821151
0.80855823
0.80830765
0.80842984
0.80857033
0.80885977
0.80884826
0.80890232
0.80896777
0.80931342
0.80980682
0.81020838
INFO - Training [59][  100/  196]   Loss 0.291154   Top1 90.015625   Top5 98.878906   BatchTime 0.386742   LR 0.000201
0.81031251
0.81028885
0.81022137
0.81030315
0.81009716
0.81000811
0.81005538
0.81009471
0.81005085
0.81010592
0.81025249
0.81035984
0.81046379
0.81074607
0.81084341
0.81116498
0.81192863
INFO - Training [59][  120/  196]   Loss 0.284680   Top1 90.208333   Top5 98.964844   BatchTime 0.381533   LR 0.000197
0.81210417
0.81178170
0.81191468
0.81204104
0.81207711
0.81218523
0.81206644
0.81204581
0.81211257
0.81228417
0.81247860
0.81229973
0.81223369
0.81235594
0.81219047
0.81222206
0.81227487
0.81238031
0.81244826
0.81239855
0.81217664
0.81204385
INFO - Training [59][  140/  196]   Loss 0.285767   Top1 90.234375   Top5 99.001116   BatchTime 0.377661   LR 0.000193
0.81192362
0.81165701
0.81119090
0.81089908
0.81086403
0.81086057
0.81084305
0.81097490
0.81168926
0.81166440
0.81156379
0.81128305
0.81108391
0.81110436
0.81102401
0.81095517
0.81106079
0.81080896
0.81024742
0.81000340
0.80959016
0.80910939
0.80845147
INFO - Training [59][  160/  196]   Loss 0.289663   Top1 90.065918   Top5 98.986816   BatchTime 0.375268   LR 0.000190
0.80810130
0.80798024
0.80787343
0.80772907
0.80772114
0.80775303
0.80882186
0.80846477
0.80827206
0.80827940
0.80827284
0.80828100
0.80825281
0.80827343
0.80827671
0.80823284
0.80824053
0.80828917
INFO - Training [59][  180/  196]   Loss 0.291688   Top1 90.015191   Top5 98.919271   BatchTime 0.371535   LR 0.000186
0.80838889
0.80843914
0.80859452
0.80872208
0.80855900
0.80838257
0.80827767
0.80821824
0.80810308
0.80798757
0.80793089
0.80800450
0.80800587
0.80815965
INFO - ==> Top1: 89.968    Top5: 98.906    Loss: 0.292
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80823696
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [59][   20/   40]   Loss 0.380791   Top1 88.300781   Top5 99.375000   BatchTime 0.150775
INFO - Validation [59][   40/   40]   Loss 0.373716   Top1 88.620000   Top5 99.540000   BatchTime 0.103175
INFO - ==> Top1: 88.620    Top5: 99.540    Loss: 0.374
INFO - ==> Sparsity : 0.410
INFO - Scoreboard best 1 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [55][Top1: 91.080   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [56][Top1: 90.800   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.1797)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0460)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0668)
features.3.conv.0 tensor(0.0145)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0354)
features.4.conv.0 tensor(0.0260)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.0820)
features.5.conv.0 tensor(0.0326)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.0856)
features.6.conv.0 tensor(0.0179)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0541)
features.7.conv.0 tensor(0.0425)
features.7.conv.3 tensor(0.1163)
features.7.conv.6 tensor(0.0711)
features.8.conv.0 tensor(0.0566)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.0889)
features.9.conv.0 tensor(0.0704)
features.9.conv.3 tensor(0.1508)
features.9.conv.6 tensor(0.1090)
features.10.conv.0 tensor(0.0387)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0664)
features.11.conv.0 tensor(0.1465)
features.11.conv.3 tensor(0.1144)
features.11.conv.6 tensor(0.3579)
features.12.conv.0 tensor(0.2919)
features.12.conv.3 tensor(0.1817)
features.12.conv.6 tensor(0.4371)
features.13.conv.0 tensor(0.0739)
features.13.conv.3 tensor(0.1672)
features.13.conv.6 tensor(0.0657)
features.14.conv.0 tensor(0.9509)
features.14.conv.3 tensor(0.1159)
features.14.conv.6 tensor(0.9206)
features.15.conv.0 tensor(0.9702)
features.15.conv.3 tensor(0.0931)
features.15.conv.6 tensor(0.9734)
features.16.conv.0 tensor(0.1115)
features.16.conv.3 tensor(0.1197)
features.16.conv.6 tensor(0.2059)
conv.0 tensor(0.3128)
tensor(897546.) 2188896.0
0.80825078
0.80834794
0.80830252
0.80818254
0.80789083
0.80754745
0.80719215
0.80689126
0.80681944
0.80670863
0.80645597
0.80632782
0.80621463
0.80615479
0.80604881
0.80552155
0.80509824
0.80490482
0.80462199
0.80449080
INFO - Training [60][   20/  196]   Loss 0.294379   Top1 89.609375   Top5 98.281250   BatchTime 0.449397   LR 0.000180
0.80426633
0.80388176
0.80361623
0.80334365
0.80318666
0.80297828
0.80282986
0.80275589
0.80258989
0.80251896
0.80241621
0.80222142
0.80215532
0.80196708
0.80183059
0.80176681
0.80173373
0.80155635
0.80138981
0.80139768
0.80157155
INFO - Training [60][   40/  196]   Loss 0.310859   Top1 89.296875   Top5 98.408203   BatchTime 0.412018   LR 0.000176
0.80162799
0.80156922
0.80158675
0.80154127
0.80141407
0.80129433
0.80134839
0.80134010
0.80134261
0.80135930
0.80149156
0.80154806
0.80145562
0.80150044
0.80157077
0.80179089
0.80198389
0.80219841
0.80284268
0.80755949
0.80753314
0.80766553
INFO - Training [60][   60/  196]   Loss 0.306068   Top1 89.394531   Top5 98.606771   BatchTime 0.402408   LR 0.000173
0.80745488
0.80729640
0.80727071
0.80727381
0.80701739
0.80649060
0.80646956
0.80643046
0.80623353
0.80610824
0.80593729
0.80594659
0.80602366
0.80593884
0.80585301
0.80576158
0.80566645
INFO - Training [60][   80/  196]   Loss 0.298114   Top1 89.711914   Top5 98.852539   BatchTime 0.390756   LR 0.000169
0.80562413
0.80562246
0.80561882
0.80559313
0.80553329
0.80550724
0.80547094
0.80536318
0.80532259
0.80533612
0.80530578
0.80526000
0.80520386
0.80518579
0.80518240
0.80516660
0.80517054
0.80512047
0.80513543
0.80512124
0.80510175
0.80509561
INFO - Training [60][  100/  196]   Loss 0.293406   Top1 89.898438   Top5 98.906250   BatchTime 0.384382   LR 0.000166
0.80511194
0.80512762
0.80512273
0.80510199
0.80510002
0.80507177
0.80505198
0.80501765
0.80496556
0.80485600
0.80480260
0.80471408
0.80456507
0.80445898
0.80440295
0.80431467
0.80423522
INFO - Training [60][  120/  196]   Loss 0.285248   Top1 90.185547   Top5 98.964844   BatchTime 0.380213   LR 0.000162
0.80419093
0.80424339
0.80415672
0.80414838
0.80422556
0.80436742
0.80441612
0.80441564
0.80452257
0.80439389
0.80432576
0.80423635
0.80416864
0.80409390
0.80400234
0.80399436
0.80399758
0.80398321
0.80397469
0.80397373
0.80420482
INFO - Training [60][  140/  196]   Loss 0.283100   Top1 90.273438   Top5 99.009487   BatchTime 0.378415   LR 0.000159
0.80420899
0.80428731
0.80421484
0.80432326
0.80417711
0.80407256
0.80396414
0.80391395
0.80398202
0.80416155
0.80423164
0.80426079
0.80403626
0.80398798
0.80412370
0.80410147
0.80407780
0.80407852
0.80419403
0.80418307
0.80406761
0.80409425
0.80420011
INFO - Training [60][  160/  196]   Loss 0.286525   Top1 90.114746   Top5 98.996582   BatchTime 0.375937   LR 0.000156
0.80433017
0.80438590
0.80414569
0.80407757
0.80400777
0.80400687
0.80406743
0.80398679
0.80397594
0.80411774
0.80424827
0.80410194
0.80417228
0.80412990
0.80422759
0.80408734
INFO - Training [60][  180/  196]   Loss 0.286147   Top1 90.147569   Top5 98.956163   BatchTime 0.376098   LR 0.000152
0.80395794
0.80397868
0.80400550
0.80401230
0.80396664
0.80395710
0.80384499
0.80384123
0.80381000
0.80360025
0.80335480
0.80322021
0.80321670
0.80320108
0.80320513
0.80314976
0.80302954
********************pre-trained*****************
INFO - ==> Top1: 90.168    Top5: 98.938    Loss: 0.286
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [60][   20/   40]   Loss 0.291558   Top1 91.230469   Top5 99.628906   BatchTime 0.135400
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.2012)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0464)
features.2.conv.0 tensor(0.0150)
features.2.conv.3 tensor(0.0648)
features.2.conv.6 tensor(0.0663)
features.3.conv.0 tensor(0.0148)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0356)
features.4.conv.0 tensor(0.0277)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.0765)
features.5.conv.0 tensor(0.0309)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.0851)
features.6.conv.0 tensor(0.0202)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0545)
features.7.conv.0 tensor(0.0415)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.0773)
features.8.conv.0 tensor(0.0558)
features.8.conv.3 tensor(0.1299)
features.8.conv.6 tensor(0.0929)
features.9.conv.0 tensor(0.0740)
features.9.conv.3 tensor(0.1510)
features.9.conv.6 tensor(0.1966)
features.10.conv.0 tensor(0.0368)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.0910)
features.11.conv.0 tensor(0.1452)
features.11.conv.3 tensor(0.1136)
features.11.conv.6 tensor(0.3512)
features.12.conv.0 tensor(0.2933)
features.12.conv.3 tensor(0.1811)
features.12.conv.6 tensor(0.4819)
features.13.conv.0 tensor(0.0750)
features.13.conv.3 tensor(0.1649)
features.13.conv.6 tensor(0.0840)
features.14.conv.0 tensor(0.9513)
features.14.conv.3 tensor(0.1156)
features.14.conv.6 tensor(0.9227)
features.15.conv.0 tensor(0.9705)
features.15.conv.3 tensor(0.0917)
features.15.conv.6 tensor(0.9754)
features.16.conv.0 tensor(0.1130)
features.16.conv.3 tensor(0.1200)
features.16.conv.6 tensor(0.1956)
conv.0 tensor(0.2353)
tensor(870764.) 2188896.0
INFO - Validation [60][   40/   40]   Loss 0.280580   Top1 91.500000   Top5 99.760000   BatchTime 0.096499
INFO - ==> Top1: 91.500    Top5: 99.760    Loss: 0.281
INFO - ==> Sparsity : 0.398
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [55][Top1: 91.080   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
0.80302101
0.80316615
0.80311233
0.80322737
0.80359334
0.80539930
0.80559140
0.80543530
0.80537236
0.80553156
0.80560660
0.80559760
0.80556893
0.80556947
0.80558664
0.80560195
0.80561006
0.80556417
0.80552346
INFO - Training [61][   20/  196]   Loss 0.284840   Top1 89.804688   Top5 98.457031   BatchTime 0.448960   LR 0.000147
0.80547512
0.80542630
0.80536234
0.80512363
0.80498922
0.80489910
0.80488282
0.80486649
0.80486369
0.80487227
0.80493927
0.80499220
0.80502957
0.80505383
0.80505532
0.80506206
0.80499488
0.80485207
0.80468911
0.80461705
0.80453300
0.80443752
INFO - Training [61][   40/  196]   Loss 0.302066   Top1 89.394531   Top5 98.564453   BatchTime 0.407201   LR 0.000143
0.80429256
0.80412918
0.80390424
0.80364275
0.80327642
0.80306613
0.80300194
0.80305845
0.80320698
0.80334550
0.80350184
0.80352694
0.80358934
0.80366755
0.80379134
0.80390602
0.80403596
0.80412626
0.80413425
0.80415219
0.80416387
0.80419570
INFO - Training [61][   60/  196]   Loss 0.294844   Top1 89.772135   Top5 98.717448   BatchTime 0.394235   LR 0.000140
0.80419374
0.80406082
0.80393672
0.80394167
0.80385196
0.80376893
0.80366760
0.80354410
0.80344290
0.80328673
0.80311698
0.80301374
0.80295920
0.80285758
0.80274528
0.80258876
INFO - Training [61][   80/  196]   Loss 0.293033   Top1 89.936523   Top5 98.828125   BatchTime 0.387281   LR 0.000137
0.80251104
0.80244344
0.80237061
0.80230874
0.80226111
0.80227762
0.80228740
0.80225426
0.80218571
0.80220115
0.80214310
0.80204117
0.80198222
0.80192018
0.80186850
0.80186707
0.80190402
0.80188608
0.80186361
0.80177915
0.80171144
0.80169028
INFO - Training [61][  100/  196]   Loss 0.286403   Top1 90.117188   Top5 98.882812   BatchTime 0.380524   LR 0.000134
0.80165792
0.80157942
0.80154520
0.80152535
0.80151057
0.80147254
0.80141294
0.80138230
0.80131394
0.80112225
0.80097145
0.80083847
0.80074412
0.80068529
0.80062532
0.80059922
0.80055702
0.80054158
0.80051529
0.80049616
0.80045676
0.80043656
INFO - Training [61][  120/  196]   Loss 0.282070   Top1 90.292969   Top5 98.948568   BatchTime 0.379041   LR 0.000131
0.80041742
0.80041718
0.80043310
0.80042827
0.80042106
0.80045271
0.80047512
0.80050802
0.80051792
0.80051476
0.80051357
0.80052513
0.80050951
0.80048651
0.80047411
0.80052984
0.80058211
INFO - Training [61][  140/  196]   Loss 0.284062   Top1 90.276228   Top5 98.978795   BatchTime 0.375677   LR 0.000128
0.80060154
0.80063242
0.80063385
0.80064166
0.80063987
0.80065125
0.80068272
0.80070978
0.80074173
0.80076504
0.80079842
0.80085909
0.80090737
0.80092740
0.80096960
0.80099589
0.80107075
0.80106860
0.80112469
0.80112743
0.80114895
0.80118638
INFO - Training [61][  160/  196]   Loss 0.284514   Top1 90.263672   Top5 98.942871   BatchTime 0.374878   LR 0.000125
0.80109686
0.80104077
0.80107129
0.80090356
0.80086708
0.80075592
0.80072016
0.80062824
0.80061692
0.80059844
0.80051100
0.80053502
0.80054891
0.80052644
0.80058187
0.80052733
0.80053174
0.80048984
0.80050188
INFO - Training [61][  180/  196]   Loss 0.284241   Top1 90.271267   Top5 98.867188   BatchTime 0.378257   LR 0.000122
0.80041063
0.80040717
0.80035478
0.80040509
0.80039585
0.80040652
0.80038708
0.80041546
0.80045658
0.80048740
0.80051219
0.80044490
0.80046326
0.80047709
INFO - ==> Top1: 90.326    Top5: 98.874    Loss: 0.283
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80048776
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [61][   20/   40]   Loss 0.296369   Top1 91.132812   Top5 99.667969   BatchTime 0.151731
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.2051)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0469)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0633)
features.2.conv.6 tensor(0.0660)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0349)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.0778)
features.5.conv.0 tensor(0.0295)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.0863)
features.6.conv.0 tensor(0.0205)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0540)
features.7.conv.0 tensor(0.0419)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.0798)
features.8.conv.0 tensor(0.0576)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.0894)
features.9.conv.0 tensor(0.0739)
features.9.conv.3 tensor(0.1502)
features.9.conv.6 tensor(0.1451)
features.10.conv.0 tensor(0.0376)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0686)
features.11.conv.0 tensor(0.1462)
features.11.conv.3 tensor(0.1130)
features.11.conv.6 tensor(0.3565)
features.12.conv.0 tensor(0.2881)
features.12.conv.3 tensor(0.1813)
features.12.conv.6 tensor(0.4469)
features.13.conv.0 tensor(0.0734)
features.13.conv.3 tensor(0.1657)
features.13.conv.6 tensor(0.0952)
features.14.conv.0 tensor(0.9520)
features.14.conv.3 tensor(0.1155)
features.14.conv.6 tensor(0.9292)
features.15.conv.0 tensor(0.9705)
features.15.conv.3 tensor(0.0928)
features.15.conv.6 tensor(0.9770)
features.16.conv.0 tensor(0.1148)
features.16.conv.3 tensor(0.1209)
features.16.conv.6 tensor(0.1872)
conv.0 tensor(0.2425)
tensor(869786.) 2188896.0
INFO - Validation [61][   40/   40]   Loss 0.283709   Top1 91.280000   Top5 99.780000   BatchTime 0.103394
INFO - ==> Top1: 91.280    Top5: 99.780    Loss: 0.284
INFO - ==> Sparsity : 0.397
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
0.80046374
0.80050331
0.80048251
0.80047011
0.80047143
0.80047750
0.80041713
0.80047303
0.80040795
0.80044514
0.80046195
0.80047882
0.80047613
0.80049044
0.80046391
0.80047029
0.80047888
0.80042732
INFO - Training [62][   20/  196]   Loss 0.298284   Top1 89.609375   Top5 98.554688   BatchTime 0.465341   LR 0.000117
0.80041456
0.80040777
0.80039519
0.80036390
0.80032867
0.80031621
0.80030507
0.80028993
0.80026168
0.80024755
0.80023807
0.80022758
0.80023068
0.80023575
0.80023801
0.80025208
0.80025101
0.80025458
0.80026668
0.80026674
0.80029184
0.80027014
INFO - Training [62][   40/  196]   Loss 0.298298   Top1 89.580078   Top5 98.701172   BatchTime 0.415069   LR 0.000114
0.80027950
0.80025870
0.80015230
0.80016786
0.80013531
0.80010682
0.80006623
0.80000138
0.79995793
0.79982072
0.79976380
0.79962540
0.79928720
0.79933697
0.79959202
0.79957372
0.79951298
0.79939389
0.79917866
0.79887646
0.79883307
0.79879582
INFO - Training [62][   60/  196]   Loss 0.290873   Top1 89.895833   Top5 98.821615   BatchTime 0.397881   LR 0.000111
0.79880863
0.79876733
0.79871988
0.79870629
0.79867482
0.79865074
0.79860765
0.79856485
0.79851955
0.79851836
0.79848051
0.79844826
0.79839212
0.79835981
0.79831624
0.79826617
INFO - Training [62][   80/  196]   Loss 0.288105   Top1 89.975586   Top5 98.979492   BatchTime 0.391254   LR 0.000108
0.79823411
0.79817379
0.79813254
0.79809183
0.79806036
0.79801840
0.79796869
0.79793000
0.79789621
0.79784459
0.79778594
0.79773504
0.79766887
0.79760790
0.79755765
0.79747504
0.79735881
0.79719824
0.79704386
0.79704410
0.79701388
0.79698509
INFO - Training [62][  100/  196]   Loss 0.280588   Top1 90.234375   Top5 99.027344   BatchTime 0.385504   LR 0.000105
0.79694730
0.79693055
0.79689360
0.79686123
0.79686022
0.79682922
0.79680395
0.79676926
0.79674220
0.79674631
0.79675126
0.79673302
0.79666609
0.79662067
0.79661161
0.79658645
0.79658073
0.79659134
0.79657161
0.79657066
0.79654253
0.79651046
INFO - Training [62][  120/  196]   Loss 0.277021   Top1 90.416667   Top5 99.065755   BatchTime 0.383090   LR 0.000102
0.79649198
0.79649121
0.79645026
0.79642218
0.79641372
0.79639471
0.79638785
0.79637039
0.79636419
0.79637748
0.79636443
0.79632998
0.79632217
0.79631042
0.79629475
0.79629064
INFO - Training [62][  140/  196]   Loss 0.275724   Top1 90.532924   Top5 99.098772   BatchTime 0.380592   LR 0.000100
0.79627812
0.79628891
0.79628003
0.79625738
0.79624027
0.79624254
0.79623312
0.79622513
0.79621452
0.79620707
0.79618835
0.79615742
0.79612541
0.79609698
0.79607767
0.79602933
0.79600716
0.79598075
0.79598624
0.79599541
0.79598856
0.79598898
INFO - Training [62][  160/  196]   Loss 0.277692   Top1 90.437012   Top5 99.084473   BatchTime 0.378872   LR 0.000097
0.79596949
0.79594219
0.79591787
0.79590237
0.79586858
0.79583961
0.79578257
0.79575670
0.79574269
0.79570365
0.79566258
0.79565388
0.79564208
0.79562217
0.79561085
0.79560786
0.79559571
0.79557866
0.79555130
0.79556108
0.79553336
0.79553252
0.79550296
INFO - Training [62][  180/  196]   Loss 0.277535   Top1 90.427517   Top5 99.012587   BatchTime 0.377330   LR 0.000094
0.79549938
0.79549384
0.79547030
0.79547286
0.79547220
0.79547441
0.79548550
0.79548579
0.79548341
0.79547602
0.79547274
0.79545373
0.79542172
INFO - ==> Top1: 90.464    Top5: 99.008    Loss: 0.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [62][   20/   40]   Loss 1.169516   Top1 67.460938   Top5 96.054688   BatchTime 0.198886
features.0.conv.0 tensor(0.4757)
features.0.conv.3 tensor(0.2148)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0473)
features.2.conv.0 tensor(0.0145)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0663)
features.3.conv.0 tensor(0.0142)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0349)
features.4.conv.0 tensor(0.0273)
features.4.conv.3 tensor(0.0914)
features.4.conv.6 tensor(0.0798)
features.5.conv.0 tensor(0.0308)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.0894)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0538)
features.7.conv.0 tensor(0.0428)
features.7.conv.3 tensor(0.1143)
features.7.conv.6 tensor(0.0810)
features.8.conv.0 tensor(0.0565)
features.8.conv.3 tensor(0.1296)
features.8.conv.6 tensor(0.0935)
features.9.conv.0 tensor(0.0740)
features.9.conv.3 tensor(0.1513)
features.9.conv.6 tensor(0.1505)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.1039)
features.10.conv.6 tensor(0.0689)
features.11.conv.0 tensor(0.1457)
features.11.conv.3 tensor(0.1134)
features.11.conv.6 tensor(0.3739)
features.12.conv.0 tensor(0.2919)
features.12.conv.3 tensor(0.1815)
features.12.conv.6 tensor(0.4499)
features.13.conv.0 tensor(0.0730)
features.13.conv.3 tensor(0.1663)
features.13.conv.6 tensor(0.1298)
features.14.conv.0 tensor(0.9522)
features.14.conv.3 tensor(0.1135)
features.14.conv.6 tensor(0.9304)
features.15.conv.0 tensor(0.9707)
features.15.conv.3 tensor(0.0918)
features.15.conv.6 tensor(0.9798)
features.16.conv.0 tensor(0.1153)
features.16.conv.3 tensor(0.1205)
features.16.conv.6 tensor(0.1971)
conv.0 tensor(0.2669)
tensor(888325.) 2188896.0
INFO - Validation [62][   40/   40]   Loss 1.150657   Top1 67.500000   Top5 96.430000   BatchTime 0.127141
INFO - ==> Top1: 67.500    Top5: 96.430    Loss: 1.151
INFO - ==> Sparsity : 0.406
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
0.79537892
0.79532737
0.79528546
0.79526317
0.79524320
0.79524040
0.79522932
0.79518229
0.79513323
0.79508698
0.79501969
0.79498386
0.79495585
0.79495311
0.79495859
0.79495788
0.79495484
0.79495656
0.79497576
0.79501677
0.79506046
0.79503560
0.79502320
INFO - Training [63][   20/  196]   Loss 0.301117   Top1 89.433594   Top5 98.242188   BatchTime 0.475964   LR 0.000090
0.79503024
0.79513615
0.79560953
0.79679805
0.79675746
0.79672050
0.79667455
0.79663086
0.79656005
0.79652369
0.79647422
0.79641634
0.79643506
0.79646605
0.79650557
0.79650742
INFO - Training [63][   40/  196]   Loss 0.301221   Top1 89.404297   Top5 98.417969   BatchTime 0.415952   LR 0.000087
0.79645592
0.79641932
0.79638314
0.79636055
0.79631132
0.79624128
0.79618979
0.79612470
0.79606414
0.79602528
0.79596859
0.79595214
0.79593831
0.79591501
0.79587799
0.79584014
0.79577327
0.79568744
0.79561502
0.79552448
0.79541516
0.79532230
0.79519933
INFO - Training [63][   60/  196]   Loss 0.294820   Top1 89.641927   Top5 98.645833   BatchTime 0.398004   LR 0.000085
0.79507911
0.79497987
0.79493278
0.79487365
0.79482865
0.79478943
0.79471064
0.79471004
0.79473090
0.79476994
0.79469520
0.79464841
0.79463261
0.79460090
0.79462534
0.79468632
INFO - Training [63][   80/  196]   Loss 0.290892   Top1 89.809570   Top5 98.823242   BatchTime 0.388072   LR 0.000082
0.79476452
0.79474163
0.79464775
0.79466015
0.79465550
0.79462355
0.79459047
0.79461884
0.79464567
0.79462504
0.79461467
0.79465294
0.79465932
0.79471856
0.79468328
0.79465193
0.79452193
0.79449797
0.79441768
0.79428190
0.79418278
0.79409587
INFO - Training [63][  100/  196]   Loss 0.284325   Top1 90.042969   Top5 98.878906   BatchTime 0.384226   LR 0.000080
0.79382807
0.79353005
0.79335022
0.79328305
0.79315358
0.79299253
0.79285336
0.79279190
0.79268557
0.79263884
0.79255038
0.79248071
0.79240578
0.79234558
0.79228276
0.79223788
0.79218984
0.79224533
0.79229492
0.79235166
0.79237235
0.79235834
INFO - Training [63][  120/  196]   Loss 0.276433   Top1 90.361328   Top5 98.961589   BatchTime 0.381177   LR 0.000077
0.79234886
0.79236084
0.79233581
0.79230487
0.79227418
0.79223812
0.79223531
0.79223281
0.79224586
0.79229873
0.79232895
0.79235893
0.79238838
0.79243749
0.79248530
0.79252011
0.79250431
INFO - Training [63][  140/  196]   Loss 0.274788   Top1 90.379464   Top5 99.009487   BatchTime 0.378270   LR 0.000075
0.79252154
0.79254687
0.79254949
0.79255235
0.79255092
0.79253983
0.79254472
0.79258209
0.79257929
0.79257882
0.79256779
0.79258347
0.79256195
0.79255509
0.79254490
0.79256970
0.79258937
0.79260516
0.79263353
0.79270792
0.79276216
0.79278970
INFO - Training [63][  160/  196]   Loss 0.278146   Top1 90.244141   Top5 98.994141   BatchTime 0.376244   LR 0.000072
0.79279828
0.79278630
0.79277647
0.79277408
0.79275376
0.79274231
0.79275042
0.79278195
0.79279709
0.79278141
0.79278791
0.79278207
0.79276896
0.79273939
0.79277557
0.79275018
0.79271317
0.79267740
0.79263413
0.79260117
0.79256648
0.79257351
INFO - Training [63][  180/  196]   Loss 0.279963   Top1 90.162760   Top5 98.921441   BatchTime 0.374588   LR 0.000070
0.79256576
0.79260284
0.79267454
0.79273182
0.79282480
0.79292023
0.79290056
0.79288322
0.79287302
0.79284173
0.79282922
0.79278541
0.79282302
********************pre-trained*****************
INFO - ==> Top1: 90.192    Top5: 98.922    Loss: 0.280
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [63][   20/   40]   Loss 0.310123   Top1 90.390625   Top5 99.472656   BatchTime 0.194173
INFO - Validation [63][   40/   40]   Loss 0.310589   Top1 90.400000   Top5 99.620000   BatchTime 0.131066
INFO - ==> Top1: 90.400    Top5: 99.620    Loss: 0.311
INFO - ==> Sparsity : 0.415
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.2109)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0516)
features.2.conv.0 tensor(0.0148)
features.2.conv.3 tensor(0.0625)
features.2.conv.6 tensor(0.0660)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0343)
features.4.conv.0 tensor(0.0275)
features.4.conv.3 tensor(0.0891)
features.4.conv.6 tensor(0.0762)
features.5.conv.0 tensor(0.0314)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.0863)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0534)
features.7.conv.0 tensor(0.0441)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.0813)
features.8.conv.0 tensor(0.0557)
features.8.conv.3 tensor(0.1259)
features.8.conv.6 tensor(0.1236)
features.9.conv.0 tensor(0.0737)
features.9.conv.3 tensor(0.1493)
features.9.conv.6 tensor(0.1408)
features.10.conv.0 tensor(0.0367)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0697)
features.11.conv.0 tensor(0.1466)
features.11.conv.3 tensor(0.1140)
features.11.conv.6 tensor(0.3895)
features.12.conv.0 tensor(0.2921)
features.12.conv.3 tensor(0.1813)
features.12.conv.6 tensor(0.4527)
features.13.conv.0 tensor(0.0738)
features.13.conv.3 tensor(0.1649)
features.13.conv.6 tensor(0.1585)
features.14.conv.0 tensor(0.9523)
features.14.conv.3 tensor(0.1131)
features.14.conv.6 tensor(0.9307)
features.15.conv.0 tensor(0.9706)
features.15.conv.3 tensor(0.0920)
features.15.conv.6 tensor(0.9769)
features.16.conv.0 tensor(0.1158)
features.16.conv.3 tensor(0.1203)
features.16.conv.6 tensor(0.2136)
conv.0 tensor(0.2962)
tensor(909284.) 2188896.0
0.79283923
0.79283100
0.79285622
0.79290116
0.79288363
0.79291803
0.79296100
0.79299790
0.79303539
0.79302853
0.79299784
0.79296410
0.79293430
0.79289228
0.79288089
0.79288685
0.79288673
0.79288018
0.79280531
0.79277271
0.79276663
0.79272956
0.79272777
INFO - Training [64][   20/  196]   Loss 0.290699   Top1 89.550781   Top5 98.378906   BatchTime 0.428646   LR 0.000066
0.79271477
0.79272676
0.79274893
0.79277694
0.79268116
0.79264343
0.79263580
0.79258305
0.79255348
0.79252970
0.79250515
0.79247743
0.79248965
0.79254073
0.79251063
0.79251814
INFO - Training [64][   40/  196]   Loss 0.291675   Top1 89.541016   Top5 98.593750   BatchTime 0.392423   LR 0.000064
0.79248679
0.79239792
0.79233944
0.79224521
0.79213309
0.79191941
0.79156131
0.79129344
0.79123437
0.79119176
0.79112631
0.79106367
0.79099303
0.79093748
0.79085666
0.79078281
0.79069287
0.79061133
0.79054451
0.79046059
0.79038167
0.79030049
INFO - Training [64][   60/  196]   Loss 0.290992   Top1 89.622396   Top5 98.671875   BatchTime 0.383682   LR 0.000062
0.79022807
0.79017121
0.79013491
0.79009491
0.79007554
0.79005134
0.79004145
0.79003626
0.79002726
0.79001188
0.78999126
0.78998280
0.78996569
0.78993791
0.78991926
0.78992075
0.78990632
INFO - Training [64][   80/  196]   Loss 0.287825   Top1 89.824219   Top5 98.837891   BatchTime 0.376498   LR 0.000059
0.78987777
0.78985268
0.78979737
0.78974110
0.78971475
0.78969383
0.78966457
0.78963208
0.78961790
0.78961223
0.78959960
0.78957111
0.78953117
0.78953898
0.78952712
0.78948885
0.78946823
0.78945041
0.78945047
0.78940266
0.78939724
0.78939182
INFO - Training [64][  100/  196]   Loss 0.280045   Top1 90.125000   Top5 98.910156   BatchTime 0.374378   LR 0.000057
0.78939259
0.78939295
0.78939205
0.78939885
0.78939873
0.78940636
0.78942007
0.78940570
0.78940809
0.78940159
0.78941739
0.78944683
0.78948766
0.78950959
0.78950787
0.78949612
0.78949511
0.78947866
0.78947842
0.78946394
0.78945804
0.78946263
INFO - Training [64][  120/  196]   Loss 0.276864   Top1 90.263672   Top5 98.974609   BatchTime 0.372396   LR 0.000055
0.78946096
0.78946900
0.78948027
0.78949827
0.78952509
0.78954160
0.78956771
0.78961468
0.78964001
0.78964889
0.78965718
0.78967017
0.78967029
0.78967607
0.78968865
0.78970593
0.78971106
INFO - Training [64][  140/  196]   Loss 0.274456   Top1 90.357143   Top5 99.029018   BatchTime 0.370555   LR 0.000053
0.78972447
0.78973329
0.78972167
0.78970182
0.78968668
0.78965467
0.78964311
0.78963661
0.78963524
0.78961396
0.78961158
0.78961855
0.78963953
0.78963226
0.78966266
0.78967148
0.78964782
0.78960955
0.78957611
0.78953117
0.78948349
INFO - Training [64][  160/  196]   Loss 0.275937   Top1 90.310059   Top5 99.045410   BatchTime 0.372012   LR 0.000051
0.78944725
0.78942299
0.78938597
0.78937215
0.78935879
0.78935033
0.78931099
0.78927583
0.78926361
0.78924543
0.78925222
0.78924227
0.78924412
0.78923261
0.78921413
0.78920984
0.78919971
0.78917545
0.78913724
0.78911173
0.78909278
0.78907114
INFO - Training [64][  180/  196]   Loss 0.275981   Top1 90.319010   Top5 98.999566   BatchTime 0.371932   LR 0.000049
0.78904700
0.78903240
0.78905445
0.78905517
0.78905731
0.78905326
0.78907371
0.78905666
0.78905511
0.78906405
0.78904289
0.78899026
0.78895742
0.78891844
********************pre-trained*****************
INFO - ==> Top1: 90.372    Top5: 99.012    Loss: 0.274
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [64][   20/   40]   Loss 0.333263   Top1 89.765625   Top5 99.570312   BatchTime 0.138728
features.0.conv.0 tensor(0.4861)
features.0.conv.3 tensor(0.2168)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0464)
features.2.conv.0 tensor(0.0150)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0654)
features.3.conv.0 tensor(0.0139)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0347)
features.4.conv.0 tensor(0.0278)
features.4.conv.3 tensor(0.0932)
features.4.conv.6 tensor(0.0771)
features.5.conv.0 tensor(0.0311)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.0913)
features.6.conv.0 tensor(0.0221)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0537)
features.7.conv.0 tensor(0.0453)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.0876)
features.8.conv.0 tensor(0.0558)
features.8.conv.3 tensor(0.1264)
features.8.conv.6 tensor(0.1141)
features.9.conv.0 tensor(0.0738)
features.9.conv.3 tensor(0.1510)
features.9.conv.6 tensor(0.1443)
features.10.conv.0 tensor(0.0373)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0697)
features.11.conv.0 tensor(0.1473)
features.11.conv.3 tensor(0.1130)
features.11.conv.6 tensor(0.4115)
features.12.conv.0 tensor(0.2921)
features.12.conv.3 tensor(0.1819)
features.12.conv.6 tensor(0.4612)
features.13.conv.0 tensor(0.0747)
features.13.conv.3 tensor(0.1634)
features.13.conv.6 tensor(0.1656)
features.14.conv.0 tensor(0.9524)
features.14.conv.3 tensor(0.1133)
features.14.conv.6 tensor(0.9335)
features.15.conv.0 tensor(0.9706)
features.15.conv.3 tensor(0.0917)
features.15.conv.6 tensor(0.9790)
features.16.conv.0 tensor(0.1165)
features.16.conv.3 tensor(0.1205)
features.16.conv.6 tensor(0.2061)
conv.0 tensor(0.3093)
tensor(915788.) 2188896.0
INFO - Validation [64][   40/   40]   Loss 0.336191   Top1 89.700000   Top5 99.620000   BatchTime 0.118269
INFO - ==> Top1: 89.700    Top5: 99.620    Loss: 0.336
INFO - ==> Sparsity : 0.418
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
0.78891236
0.78892910
0.78893000
0.78894299
0.78896093
0.78898841
0.78902191
0.78904998
0.78909725
0.78909159
0.78909910
0.78911263
0.78913957
0.78915393
0.78915948
0.78918582
0.78920472
0.78920251
0.78917795
0.78919679
0.78921038
0.78922039
0.78922552
INFO - Training [65][   20/  196]   Loss 0.292446   Top1 90.273438   Top5 98.203125   BatchTime 0.439345   LR 0.000046
0.78923845
0.78925091
0.78927344
0.78927088
0.78928244
0.78929228
0.78930330
0.78931373
0.78933817
0.78934366
0.78935343
0.78936899
0.78938472
0.78938639
0.78941017
0.78941995
INFO - Training [65][   40/  196]   Loss 0.278789   Top1 90.468750   Top5 98.613281   BatchTime 0.402473   LR 0.000044
0.78942508
0.78942126
0.78941101
0.78943288
0.78943914
0.78944212
0.78945905
0.78947294
0.78949016
0.78950977
0.78952485
0.78952742
0.78955811
0.78957474
0.78958064
0.78959781
0.78962350
0.78964180
0.78965014
0.78967798
0.78969282
0.78968126
INFO - Training [65][   60/  196]   Loss 0.280846   Top1 90.436198   Top5 98.678385   BatchTime 0.388084   LR 0.000042
0.78969318
0.78969443
0.78970647
0.78970593
0.78971404
0.78971249
0.78972226
0.78970987
0.78969562
0.78968269
0.78968179
0.78966767
0.78963333
0.78959203
0.78957754
0.78953820
0.78950882
INFO - Training [65][   80/  196]   Loss 0.277214   Top1 90.590820   Top5 98.842773   BatchTime 0.381650   LR 0.000040
0.78949422
0.78945988
0.78945351
0.78945327
0.78943509
0.78943306
0.78942603
0.78941405
0.78941900
0.78941011
0.78939283
0.78940529
0.78939682
0.78939146
0.78940648
0.78940856
0.78940505
0.78942001
0.78943759
0.78942412
0.78941214
0.78941745
INFO - Training [65][  100/  196]   Loss 0.271268   Top1 90.730469   Top5 98.914062   BatchTime 0.378752   LR 0.000039
0.78939903
0.78940177
0.78937823
0.78936309
0.78937292
0.78933841
0.78929168
0.78929150
0.78928030
0.78926277
0.78924108
0.78922576
0.78922945
0.78924131
0.78920245
0.78918636
0.78915793
0.78914309
0.78911114
0.78910679
0.78910041
0.78908968
INFO - Training [65][  120/  196]   Loss 0.270602   Top1 90.742188   Top5 98.958333   BatchTime 0.376120   LR 0.000037
0.78910065
0.78909987
0.78907448
0.78907150
0.78907663
0.78904653
0.78902537
0.78901887
0.78899407
0.78896356
0.78895378
0.78892809
0.78889644
0.78888512
0.78885674
0.78884423
0.78880948
INFO - Training [65][  140/  196]   Loss 0.267917   Top1 90.831473   Top5 99.020647   BatchTime 0.373100   LR 0.000035
0.78880727
0.78878963
0.78878641
0.78876823
0.78875345
0.78875518
0.78873211
0.78872114
0.78870636
0.78870159
0.78870785
0.78869122
0.78868532
0.78869182
0.78868181
0.78868181
0.78869963
0.78870493
0.78869539
0.78868890
0.78870088
INFO - Training [65][  160/  196]   Loss 0.271440   Top1 90.717773   Top5 99.028320   BatchTime 0.374334   LR 0.000033
0.78870845
0.78872263
0.78870481
0.78870064
0.78870875
0.78871220
0.78873134
0.78872591
0.78871953
0.78870547
0.78869170
0.78869426
0.78870404
0.78870291
0.78869939
0.78866923
0.78868520
0.78868723
0.78866249
0.78865319
0.78865463
0.78864986
INFO - Training [65][  180/  196]   Loss 0.271253   Top1 90.670573   Top5 98.995226   BatchTime 0.374219   LR 0.000032
0.78862500
0.78860247
0.78860271
0.78860343
0.78858298
0.78857434
0.78856826
0.78855950
0.78856128
0.78856224
0.78858560
0.78859442
0.78859311
0.78860128
********************pre-trained*****************
INFO - ==> Top1: 90.610    Top5: 98.986    Loss: 0.272
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [65][   20/   40]   Loss 0.342043   Top1 90.097656   Top5 99.414062   BatchTime 0.148277
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.2129)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0148)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0642)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0293)
features.3.conv.6 tensor(0.0347)
features.4.conv.0 tensor(0.0273)
features.4.conv.3 tensor(0.0938)
features.4.conv.6 tensor(0.0796)
features.5.conv.0 tensor(0.0309)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.0959)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0538)
features.7.conv.0 tensor(0.0451)
features.7.conv.3 tensor(0.1166)
features.7.conv.6 tensor(0.0931)
features.8.conv.0 tensor(0.0559)
features.8.conv.3 tensor(0.1262)
features.8.conv.6 tensor(0.1102)
features.9.conv.0 tensor(0.0742)
features.9.conv.3 tensor(0.1505)
features.9.conv.6 tensor(0.1473)
features.10.conv.0 tensor(0.0372)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0723)
features.11.conv.0 tensor(0.1481)
features.11.conv.3 tensor(0.1144)
features.11.conv.6 tensor(0.4178)
features.12.conv.0 tensor(0.2957)
features.12.conv.3 tensor(0.1833)
features.12.conv.6 tensor(0.4595)
features.13.conv.0 tensor(0.0749)
features.13.conv.3 tensor(0.1645)
features.13.conv.6 tensor(0.1529)
features.14.conv.0 tensor(0.9528)
features.14.conv.3 tensor(0.1135)
features.14.conv.6 tensor(0.9330)
features.15.conv.0 tensor(0.9706)
features.15.conv.3 tensor(0.0916)
features.15.conv.6 tensor(0.9781)
features.16.conv.0 tensor(0.1171)
features.16.conv.3 tensor(0.1197)
features.16.conv.6 tensor(0.2196)
conv.0 tensor(0.2983)
tensor(914945.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.337883   Top1 90.020000   Top5 99.580000   BatchTime 0.113078
INFO - ==> Top1: 90.020    Top5: 99.580    Loss: 0.338
INFO - ==> Sparsity : 0.418
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
0.78859484
0.78856963
0.78853047
0.78849232
0.78847218
0.78844535
0.78842378
0.78840232
0.78839284
0.78837168
0.78834546
0.78831059
0.78829509
0.78827626
0.78825426
0.78822440
0.78819102
0.78818005
INFO - Training [66][   20/  196]   Loss 0.295948   Top1 89.609375   Top5 98.593750   BatchTime 0.440020   LR 0.000029
0.78815317
0.78813815
0.78812909
0.78811413
0.78809887
0.78808343
0.78806889
0.78806204
0.78804678
0.78803039
0.78803229
0.78803051
0.78801662
0.78800285
0.78798461
0.78798920
0.78799391
0.78800660
0.78800160
0.78799069
0.78799337
0.78799373
0.78797227
INFO - Training [66][   40/  196]   Loss 0.296432   Top1 89.335938   Top5 98.691406   BatchTime 0.398215   LR 0.000028
0.78799129
0.78800255
0.78799838
0.78800863
0.78799373
0.78800195
0.78801274
0.78800982
0.78799123
0.78796220
0.78797084
0.78795749
0.78793639
0.78792602
0.78791630
0.78790814
0.78790468
0.78789520
0.78789192
0.78788364
0.78788221
INFO - Training [66][   60/  196]   Loss 0.296150   Top1 89.459635   Top5 98.834635   BatchTime 0.387915   LR 0.000026
0.78786492
0.78787571
0.78788835
0.78787416
0.78786772
0.78785813
0.78785378
0.78784227
0.78781950
0.78780377
0.78780270
0.78780288
0.78781545
0.78783345
0.78784406
0.78784931
0.78784257
INFO - Training [66][   80/  196]   Loss 0.288072   Top1 89.721680   Top5 99.003906   BatchTime 0.383475   LR 0.000025
0.78782105
0.78782403
0.78780198
0.78777748
0.78775489
0.78772891
0.78771412
0.78769147
0.78765893
0.78765178
0.78765279
0.78764558
0.78764158
0.78762841
0.78765112
0.78764552
0.78764993
0.78762996
0.78762656
0.78762722
0.78762907
0.78761309
INFO - Training [66][  100/  196]   Loss 0.285841   Top1 89.878906   Top5 99.015625   BatchTime 0.378160   LR 0.000023
0.78759259
0.78759587
0.78758937
0.78758317
0.78758198
0.78759164
0.78759438
0.78757030
0.78755915
0.78755081
0.78755009
0.78755730
0.78758097
0.78759581
0.78758687
0.78760517
0.78762013
0.78762114
0.78763062
0.78764957
0.78764653
0.78763658
INFO - Training [66][  120/  196]   Loss 0.277491   Top1 90.185547   Top5 99.082031   BatchTime 0.375037   LR 0.000022
0.78764176
0.78765577
0.78765285
0.78764910
0.78764611
0.78765100
0.78765392
0.78766376
0.78765732
0.78765792
0.78767282
0.78767240
0.78767335
0.78766763
0.78767002
0.78765577
0.78766829
INFO - Training [66][  140/  196]   Loss 0.275294   Top1 90.309710   Top5 99.109933   BatchTime 0.373671   LR 0.000021
0.78767401
0.78767550
0.78766149
0.78766006
0.78765959
0.78766161
0.78766638
0.78765571
0.78764689
0.78763515
0.78762877
0.78761601
0.78759605
0.78760082
0.78759062
0.78757215
0.78757364
0.78756917
0.78755176
0.78755045
INFO - Training [66][  160/  196]   Loss 0.276605   Top1 90.266113   Top5 99.108887   BatchTime 0.376067   LR 0.000019
0.78754550
0.78753966
0.78752995
0.78754067
0.78754592
0.78752995
0.78752059
0.78750187
0.78750563
0.78749424
0.78746462
0.78746361
0.78745878
0.78744912
0.78743273
0.78742653
0.78742576
0.78742957
0.78742361
0.78741646
0.78741509
0.78740746
INFO - Training [66][  180/  196]   Loss 0.277456   Top1 90.249566   Top5 99.053819   BatchTime 0.375838   LR 0.000018
0.78741688
0.78741813
0.78741974
0.78740257
0.78741431
0.78741050
0.78740621
0.78740108
0.78740233
0.78739774
0.78738469
0.78737450
0.78736490
0.78737581
********************pre-trained*****************
INFO - ==> Top1: 90.314    Top5: 99.046    Loss: 0.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [66][   20/   40]   Loss 0.319129   Top1 90.097656   Top5 99.550781   BatchTime 0.145571
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.2148)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0145)
features.2.conv.3 tensor(0.0610)
features.2.conv.6 tensor(0.0642)
features.3.conv.0 tensor(0.0142)
features.3.conv.3 tensor(0.0285)
features.3.conv.6 tensor(0.0347)
features.4.conv.0 tensor(0.0282)
features.4.conv.3 tensor(0.0897)
features.4.conv.6 tensor(0.0793)
features.5.conv.0 tensor(0.0311)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.0968)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0540)
features.7.conv.0 tensor(0.0449)
features.7.conv.3 tensor(0.1160)
features.7.conv.6 tensor(0.0975)
features.8.conv.0 tensor(0.0557)
features.8.conv.3 tensor(0.1262)
features.8.conv.6 tensor(0.1084)
features.9.conv.0 tensor(0.0745)
features.9.conv.3 tensor(0.1508)
features.9.conv.6 tensor(0.1491)
features.10.conv.0 tensor(0.0373)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0725)
features.11.conv.0 tensor(0.1481)
features.11.conv.3 tensor(0.1138)
features.11.conv.6 tensor(0.4172)
features.12.conv.0 tensor(0.3030)
features.12.conv.3 tensor(0.1827)
features.12.conv.6 tensor(0.4622)
features.13.conv.0 tensor(0.0748)
features.13.conv.3 tensor(0.1634)
features.13.conv.6 tensor(0.1558)
features.14.conv.0 tensor(0.9528)
features.14.conv.3 tensor(0.1139)
features.14.conv.6 tensor(0.9351)
features.15.conv.0 tensor(0.9707)
features.15.conv.3 tensor(0.0916)
features.15.conv.6 tensor(0.9786)
features.16.conv.0 tensor(0.1175)
features.16.conv.3 tensor(0.1196)
features.16.conv.6 tensor(0.2437)
conv.0 tensor(0.3165)
tensor(931147.) 2188896.0
INFO - Validation [66][   40/   40]   Loss 0.317744   Top1 90.130000   Top5 99.640000   BatchTime 0.102603
INFO - ==> Top1: 90.130    Top5: 99.640    Loss: 0.318
INFO - ==> Sparsity : 0.425
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Scoreboard best 3 ==> Epoch [58][Top1: 91.280   Top5: 99.660]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
0.78738052
0.78738570
0.78738135
0.78736460
0.78735542
0.78735292
0.78733659
0.78732109
0.78732163
0.78731787
0.78734374
0.78734368
0.78734893
0.78736103
0.78736758
0.78735316
0.78735930
0.78737134
0.78736287
0.78735769
INFO - Training [67][   20/  196]   Loss 0.299862   Top1 89.628906   Top5 98.535156   BatchTime 0.410861   LR 0.000016
0.78735340
0.78734922
0.78735411
0.78737044
0.78737181
0.78737652
0.78738213
0.78739113
0.78739792
0.78741640
0.78742355
0.78744560
0.78743589
0.78746367
0.78746080
0.78746068
0.78746635
0.78746742
0.78747034
0.78746092
0.78743535
0.78742945
INFO - Training [67][   40/  196]   Loss 0.294204   Top1 89.892578   Top5 98.681641   BatchTime 0.387272   LR 0.000015
0.78743601
0.78742558
0.78742599
0.78740758
0.78740811
0.78741056
0.78739583
0.78738171
0.78736341
0.78736418
0.78737193
0.78738070
0.78736705
0.78737068
0.78736913
0.78737843
0.78737426
INFO - Training [67][   60/  196]   Loss 0.290756   Top1 89.973958   Top5 98.743490   BatchTime 0.378111   LR 0.000014
0.78737628
0.78737617
0.78737539
0.78738171
0.78739762
0.78739649
0.78740036
0.78742599
0.78742623
0.78741121
0.78741801
0.78741676
0.78741789
0.78743351
0.78744394
0.78743726
0.78744102
0.78743231
0.78743380
0.78742164
0.78741479
0.78741246
0.78741032
INFO - Training [67][   80/  196]   Loss 0.283535   Top1 90.205078   Top5 98.906250   BatchTime 0.371308   LR 0.000013
0.78739375
0.78739375
0.78737664
0.78735840
0.78734738
0.78734750
0.78732973
0.78732342
0.78732061
0.78730649
0.78730911
0.78731012
0.78729951
0.78728497
0.78727764
0.78724968
INFO - Training [67][  100/  196]   Loss 0.277419   Top1 90.371094   Top5 98.917969   BatchTime 0.370069   LR 0.000012
0.78723568
0.78722680
0.78719318
0.78718233
0.78717655
0.78718585
0.78718168
0.78715515
0.78713208
0.78712249
0.78711396
0.78709787
0.78709114
0.78707796
0.78706652
0.78706223
0.78705937
0.78704429
0.78702635
0.78703147
0.78703362
0.78701884
INFO - Training [67][  120/  196]   Loss 0.272983   Top1 90.511068   Top5 98.974609   BatchTime 0.369473   LR 0.000011
0.78699750
0.78700334
0.78700322
0.78700519
0.78700054
0.78698891
0.78699589
0.78698784
0.78699249
0.78699940
0.78700495
0.78699738
0.78700727
0.78700471
0.78697884
0.78696555
0.78695875
0.78695726
0.78696215
0.78695190
0.78694761
0.78695232
INFO - Training [67][  140/  196]   Loss 0.270540   Top1 90.658482   Top5 99.040179   BatchTime 0.369309   LR 0.000010
0.78694087
0.78693336
0.78693044
0.78692448
0.78692108
0.78691775
0.78691357
0.78690988
0.78690708
0.78691012
0.78691709
0.78691137
0.78691417
0.78691638
0.78691137
0.78691190
0.78692120
0.78692722
0.78692865
0.78692895
0.78691912
INFO - Training [67][  160/  196]   Loss 0.272483   Top1 90.627441   Top5 99.050293   BatchTime 0.370073   LR 0.000009
0.78691202
0.78690684
0.78688884
0.78687501
0.78687364
0.78685433
0.78684610
0.78683794
0.78683990
0.78682494
0.78683531
0.78682810
0.78681487
0.78680634
0.78680176
0.78678739
INFO - Training [67][  180/  196]   Loss 0.273959   Top1 90.577257   Top5 98.999566   BatchTime 0.371341   LR 0.000008
0.78676558
0.78675681
0.78674459
0.78674155
0.78672993
0.78673160
0.78671086
0.78671122
0.78669673
0.78668499
0.78668416
0.78669333
0.78669840
0.78670549
0.78671616
0.78670228
INFO - ==> Top1: 90.642    Top5: 99.002    Loss: 0.273
0.78671211
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [67][   20/   40]   Loss 0.296358   Top1 90.917969   Top5 99.609375   BatchTime 0.128831
INFO - Validation [67][   40/   40]   Loss 0.284815   Top1 91.380000   Top5 99.710000   BatchTime 0.091861
INFO - ==> Top1: 91.380    Top5: 99.710    Loss: 0.285
INFO - ==> Sparsity : 0.428
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [67][Top1: 91.380   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0142)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0645)
features.3.conv.0 tensor(0.0145)
features.3.conv.3 tensor(0.0293)
features.3.conv.6 tensor(0.0354)
features.4.conv.0 tensor(0.0277)
features.4.conv.3 tensor(0.0909)
features.4.conv.6 tensor(0.0825)
features.5.conv.0 tensor(0.0306)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.0968)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0543)
features.7.conv.0 tensor(0.0454)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.0984)
features.8.conv.0 tensor(0.0557)
features.8.conv.3 tensor(0.1267)
features.8.conv.6 tensor(0.1086)
features.9.conv.0 tensor(0.0743)
features.9.conv.3 tensor(0.1522)
features.9.conv.6 tensor(0.1480)
features.10.conv.0 tensor(0.0373)
features.10.conv.3 tensor(0.1027)
features.10.conv.6 tensor(0.0739)
features.11.conv.0 tensor(0.1483)
features.11.conv.3 tensor(0.1150)
features.11.conv.6 tensor(0.4165)
features.12.conv.0 tensor(0.3032)
features.12.conv.3 tensor(0.1815)
features.12.conv.6 tensor(0.4623)
features.13.conv.0 tensor(0.0748)
features.13.conv.3 tensor(0.1634)
features.13.conv.6 tensor(0.1542)
features.14.conv.0 tensor(0.9528)
features.14.conv.3 tensor(0.1137)
features.14.conv.6 tensor(0.9349)
features.15.conv.0 tensor(0.9707)
features.15.conv.3 tensor(0.0916)
features.15.conv.6 tensor(0.9786)
features.16.conv.0 tensor(0.1179)
features.16.conv.3 tensor(0.1201)
features.16.conv.6 tensor(0.2733)
conv.0 tensor(0.3106)
tensor(937852.) 2188896.0
0.78671521
0.78671241
0.78670907
0.78671354
0.78671998
0.78672409
0.78672403
0.78671402
0.78672600
0.78673822
0.78674078
0.78674197
0.78673792
0.78672928
0.78673077
0.78670710
0.78669757
0.78669137
0.78666228
0.78665012
INFO - Training [68][   20/  196]   Loss 0.287315   Top1 89.941406   Top5 98.300781   BatchTime 0.390002   LR 0.000007
0.78663892
0.78664666
0.78664619
0.78664678
0.78664368
0.78663594
0.78663176
0.78662282
0.78662837
0.78662694
0.78662080
0.78660166
0.78658724
0.78658497
0.78659099
0.78660160
0.78660560
0.78660649
0.78660989
0.78661591
0.78661823
0.78660715
0.78660583
INFO - Training [68][   40/  196]   Loss 0.279487   Top1 90.273438   Top5 98.564453   BatchTime 0.371215   LR 0.000006
0.78661549
0.78662276
0.78662604
0.78661627
0.78661102
0.78661907
0.78662544
0.78663641
0.78662694
0.78662473
0.78661633
0.78661573
0.78660381
0.78658861
0.78657842
0.78658152
INFO - Training [68][   60/  196]   Loss 0.279267   Top1 90.488281   Top5 98.632812   BatchTime 0.366976   LR 0.000006
0.78658283
0.78656667
0.78656864
0.78656965
0.78656083
0.78655660
0.78655416
0.78652787
0.78653222
0.78654045
0.78654087
0.78653830
0.78654808
0.78656453
0.78657925
0.78657073
0.78657538
0.78656906
0.78656262
0.78656244
0.78655535
0.78655469
INFO - Training [68][   80/  196]   Loss 0.278309   Top1 90.561523   Top5 98.754883   BatchTime 0.365798   LR 0.000005
0.78655970
0.78656662
0.78656244
0.78656554
0.78656816
0.78656703
0.78655058
0.78655660
0.78655088
0.78654808
0.78654087
0.78653735
0.78654402
0.78653878
0.78653938
0.78653288
0.78652155
INFO - Training [68][  100/  196]   Loss 0.272501   Top1 90.691406   Top5 98.804688   BatchTime 0.363920   LR 0.000004
0.78650689
0.78652102
0.78653181
0.78652024
0.78652519
0.78653222
0.78654146
0.78654683
0.78656220
0.78655869
0.78656948
0.78657556
0.78656411
0.78657669
0.78656757
0.78656918
0.78657198
0.78658193
0.78658015
0.78658992
0.78658926
0.78658402
0.78658146
INFO - Training [68][  120/  196]   Loss 0.270447   Top1 90.810547   Top5 98.870443   BatchTime 0.363208   LR 0.000004
0.78658170
0.78660047
0.78659403
0.78658694
0.78659630
0.78659272
0.78659403
0.78658485
0.78660429
0.78659922
0.78659171
0.78658730
0.78657711
0.78658307
0.78659505
0.78660530
0.78660440
0.78662258
0.78663933
0.78662884
0.78663778
INFO - Training [68][  140/  196]   Loss 0.269054   Top1 90.848214   Top5 98.942522   BatchTime 0.363856   LR 0.000003
0.78662747
0.78662747
0.78663903
0.78664792
0.78662747
0.78662914
0.78663254
0.78664315
0.78664136
0.78666180
0.78665817
0.78665924
0.78666574
0.78667819
0.78668147
0.78667504
0.78667897
0.78668505
0.78667706
0.78666842
0.78666705
INFO - Training [68][  160/  196]   Loss 0.272626   Top1 90.683594   Top5 98.930664   BatchTime 0.365747   LR 0.000003
0.78665739
0.78665566
0.78664780
0.78664768
0.78664356
0.78664511
0.78663784
0.78663737
0.78664106
0.78663331
0.78663456
0.78663141
0.78662705
0.78662884
0.78663570
0.78664696
0.78665107
0.78665733
0.78666574
INFO - Training [68][  180/  196]   Loss 0.272916   Top1 90.648872   Top5 98.895399   BatchTime 0.369810   LR 0.000002
0.78666383
0.78666365
0.78665918
0.78667158
0.78667295
0.78667247
0.78667116
0.78666663
0.78666598
0.78666049
0.78666091
INFO - ==> Top1: 90.668    Top5: 98.904    Loss: 0.272
0.78664774
0.78665340
0.78666270
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [68][   20/   40]   Loss 0.303033   Top1 90.976562   Top5 99.609375   BatchTime 0.127898
INFO - Validation [68][   40/   40]   Loss 0.289669   Top1 91.280000   Top5 99.740000   BatchTime 0.091883
INFO - ==> Top1: 91.280    Top5: 99.740    Loss: 0.290
INFO - ==> Sparsity : 0.429
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [67][Top1: 91.380   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0142)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0645)
features.3.conv.0 tensor(0.0145)
features.3.conv.3 tensor(0.0293)
features.3.conv.6 tensor(0.0349)
features.4.conv.0 tensor(0.0278)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.0809)
features.5.conv.0 tensor(0.0306)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.0973)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0541)
features.7.conv.0 tensor(0.0453)
features.7.conv.3 tensor(0.1160)
features.7.conv.6 tensor(0.0970)
features.8.conv.0 tensor(0.0557)
features.8.conv.3 tensor(0.1259)
features.8.conv.6 tensor(0.1074)
features.9.conv.0 tensor(0.0742)
features.9.conv.3 tensor(0.1516)
features.9.conv.6 tensor(0.1482)
features.10.conv.0 tensor(0.0374)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.0735)
features.11.conv.0 tensor(0.1486)
features.11.conv.3 tensor(0.1138)
features.11.conv.6 tensor(0.4166)
features.12.conv.0 tensor(0.3026)
features.12.conv.3 tensor(0.1829)
features.12.conv.6 tensor(0.4628)
features.13.conv.0 tensor(0.0749)
features.13.conv.3 tensor(0.1628)
features.13.conv.6 tensor(0.1556)
features.14.conv.0 tensor(0.9529)
features.14.conv.3 tensor(0.1134)
features.14.conv.6 tensor(0.9346)
features.15.conv.0 tensor(0.9707)
features.15.conv.3 tensor(0.0916)
features.15.conv.6 tensor(0.9792)
features.16.conv.0 tensor(0.1181)
features.16.conv.3 tensor(0.1198)
features.16.conv.6 tensor(0.2737)
conv.0 tensor(0.3115)
tensor(938445.) 2188896.0
0.78667146
0.78665787
0.78664887
0.78664994
0.78665096
0.78665024
0.78665799
0.78664505
0.78665984
0.78666192
0.78666025
0.78665823
0.78665900
0.78666323
0.78666556
0.78666115
0.78666443
0.78665406
INFO - Training [69][   20/  196]   Loss 0.288990   Top1 89.863281   Top5 98.750000   BatchTime 0.372102   LR 0.000002
0.78666735
0.78667349
0.78666252
0.78666860
0.78668076
0.78667080
0.78667146
0.78667450
0.78666699
0.78666949
0.78666741
0.78666645
0.78666896
0.78667074
0.78666854
0.78667283
0.78666365
0.78666031
0.78667474
0.78667676
0.78668082
0.78667361
0.78668511
INFO - Training [69][   40/  196]   Loss 0.293953   Top1 89.736328   Top5 98.808594   BatchTime 0.364016   LR 0.000001
0.78667521
0.78667587
0.78666735
0.78666049
0.78666663
0.78666574
0.78666383
0.78666759
0.78665870
0.78665566
0.78664184
0.78664768
0.78664166
0.78664839
0.78665125
0.78666097
INFO - Training [69][   60/  196]   Loss 0.296309   Top1 89.687500   Top5 98.873698   BatchTime 0.367878   LR 0.000001
0.78665221
0.78665578
0.78665215
0.78664851
0.78663737
0.78663737
0.78663588
0.78663009
0.78664243
0.78664017
0.78663868
0.78663975
0.78662950
0.78663874
0.78664482
0.78662664
0.78661793
0.78660828
0.78662544
0.78661454
0.78661174
0.78662276
INFO - Training [69][   80/  196]   Loss 0.294026   Top1 89.770508   Top5 98.930664   BatchTime 0.368344   LR 0.000001
0.78661472
0.78661931
0.78660619
0.78659815
0.78659868
0.78659493
0.78660160
0.78659225
0.78661019
0.78661317
0.78660476
0.78662544
0.78662539
0.78660923
0.78661412
0.78661424
0.78660518
0.78660643
0.78661382
0.78661209
0.78662390
0.78662747
INFO - Training [69][  100/  196]   Loss 0.284325   Top1 90.109375   Top5 98.960938   BatchTime 0.367373   LR 0.000000
0.78663528
0.78662658
0.78661656
0.78661340
0.78661776
0.78661674
0.78662205
0.78662378
0.78662235
0.78662813
0.78663415
0.78663534
0.78663975
0.78662890
0.78664190
0.78663880
INFO - Training [69][  120/  196]   Loss 0.280290   Top1 90.341797   Top5 99.016927   BatchTime 0.367530   LR 0.000000
0.78662765
0.78662360
0.78661597
0.78660846
0.78660035
0.78660160
0.78660339
0.78661132
0.78661305
0.78661573
0.78660291
0.78660089
0.78660160
0.78659695
0.78660631
0.78661048
0.78662109
0.78662097
0.78661805
0.78660798
0.78662241
0.78662068
INFO - Training [69][  140/  196]   Loss 0.278130   Top1 90.362723   Top5 99.040179   BatchTime 0.366918   LR 0.000000
0.78661782
0.78660965
0.78660053
0.78660023
0.78659809
0.78660142
0.78659970
0.78660065
0.78659397
0.78659225
0.78658646
0.78658307
0.78658825
0.78659129
0.78658730
0.78658497
0.78658187
0.78658956
0.78660023
0.78659612
0.78659767
INFO - Training [69][  160/  196]   Loss 0.281549   Top1 90.234375   Top5 99.008789   BatchTime 0.368074   LR 0.000000
0.78659779
0.78660238
0.78661382
0.78662175
0.78662008
0.78661811
0.78660589
0.78659880
0.78659135
0.78659928
0.78660619
0.78661048
0.78660530
0.78659809
0.78661638
0.78660500
0.78660029
0.78659081
0.78659844
0.78659719
0.78659993
0.78660828
INFO - Training [69][  180/  196]   Loss 0.282780   Top1 90.188802   Top5 98.971354   BatchTime 0.368206   LR 0.000000
0.78660774
0.78660339
0.78659207
0.78660774
0.78660405
0.78659856
0.78660119
0.78661054
0.78660637
0.78660548
0.78659314
INFO - ==> Top1: 90.192    Top5: 98.980    Loss: 0.282
0.78659320
0.78658843
0.78658372
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [69][   20/   40]   Loss 0.331882   Top1 89.824219   Top5 99.453125   BatchTime 0.124894
INFO - Validation [69][   40/   40]   Loss 0.332634   Top1 89.870000   Top5 99.600000   BatchTime 0.089092
INFO - ==> Top1: 89.870    Top5: 99.600    Loss: 0.333
INFO - ==> Sparsity : 0.429
INFO - Scoreboard best 1 ==> Epoch [60][Top1: 91.500   Top5: 99.760]
INFO - Scoreboard best 2 ==> Epoch [67][Top1: 91.380   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 91.280   Top5: 99.780]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/_checkpoint.pth.tar
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.2266)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0482)
features.2.conv.0 tensor(0.0142)
features.2.conv.3 tensor(0.0602)
features.2.conv.6 tensor(0.0642)
features.3.conv.0 tensor(0.0142)
features.3.conv.3 tensor(0.0285)
features.3.conv.6 tensor(0.0349)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.0806)
features.5.conv.0 tensor(0.0304)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.0973)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0541)
features.7.conv.0 tensor(0.0452)
features.7.conv.3 tensor(0.1166)
features.7.conv.6 tensor(0.0967)
features.8.conv.0 tensor(0.0557)
features.8.conv.3 tensor(0.1256)
features.8.conv.6 tensor(0.1075)
features.9.conv.0 tensor(0.0741)
features.9.conv.3 tensor(0.1516)
features.9.conv.6 tensor(0.1485)
features.10.conv.0 tensor(0.0373)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0735)
features.11.conv.0 tensor(0.1487)
features.11.conv.3 tensor(0.1136)
features.11.conv.6 tensor(0.4169)
features.12.conv.0 tensor(0.3028)
features.12.conv.3 tensor(0.1813)
features.12.conv.6 tensor(0.4627)
features.13.conv.0 tensor(0.0748)
features.13.conv.3 tensor(0.1640)
features.13.conv.6 tensor(0.1557)
features.14.conv.0 tensor(0.9528)
features.14.conv.3 tensor(0.1139)
features.14.conv.6 tensor(0.9349)
features.15.conv.0 tensor(0.9706)
features.15.conv.3 tensor(0.0911)
features.15.conv.6 tensor(0.9802)
features.16.conv.0 tensor(0.1178)
features.16.conv.3 tensor(0.1199)
features.16.conv.6 tensor(0.2739)
conv.0 tensor(0.3119)
tensor(938831.) 2188896.0
INFO - Validation [   20/   40]   Loss 0.331882   Top1 89.824219   Top5 99.453125   BatchTime 0.137348
INFO - Validation [   40/   40]   Loss 0.332634   Top1 89.870000   Top5 99.600000   BatchTime 0.097573
INFO - ==> Top1: 89.870    Top5: 99.600    Loss: 0.333
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
*************hard_pruning_mode*******************
INFO - Training [0][   20/  196]   Loss 0.553685   Top1 81.035156   Top5 97.480469   BatchTime 0.348189   LR 0.004999
INFO - Training [0][   40/  196]   Loss 0.545831   Top1 81.093750   Top5 97.695312   BatchTime 0.342538   LR 0.004995
INFO - Training [0][   60/  196]   Loss 0.534478   Top1 81.412760   Top5 97.871094   BatchTime 0.340876   LR 0.004989
INFO - Training [0][   80/  196]   Loss 0.526307   Top1 81.782227   Top5 97.963867   BatchTime 0.341312   LR 0.004980
INFO - Training [0][  100/  196]   Loss 0.515178   Top1 82.042969   Top5 98.089844   BatchTime 0.342757   LR 0.004968
INFO - Training [0][  120/  196]   Loss 0.509037   Top1 82.236328   Top5 98.177083   BatchTime 0.343368   LR 0.004954
INFO - Training [0][  140/  196]   Loss 0.510495   Top1 82.290737   Top5 98.189174   BatchTime 0.342533   LR 0.004938
INFO - Training [0][  160/  196]   Loss 0.514425   Top1 82.182617   Top5 98.132324   BatchTime 0.342142   LR 0.004919
INFO - Training [0][  180/  196]   Loss 0.514558   Top1 82.222222   Top5 98.109809   BatchTime 0.344930   LR 0.004897
INFO - ==> Top1: 82.212    Top5: 98.126    Loss: 0.515
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.456011   Top1 85.097656   Top5 99.179688   BatchTime 0.129922
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.2227)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0499)
features.2.conv.0 tensor(0.0156)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0645)
features.3.conv.0 tensor(0.0145)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0373)
features.4.conv.0 tensor(0.0337)
features.4.conv.3 tensor(0.0874)
features.4.conv.6 tensor(0.1003)
features.5.conv.0 tensor(0.0376)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.1462)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0560)
features.7.conv.0 tensor(0.0467)
features.7.conv.3 tensor(0.1262)
features.7.conv.6 tensor(0.1324)
features.8.conv.0 tensor(0.0668)
features.8.conv.3 tensor(0.1357)
features.8.conv.6 tensor(0.1522)
features.9.conv.0 tensor(0.0762)
features.9.conv.3 tensor(0.1554)
features.9.conv.6 tensor(0.1931)
features.10.conv.0 tensor(0.0452)
features.10.conv.3 tensor(0.1065)
features.10.conv.6 tensor(0.1001)
features.11.conv.0 tensor(0.1711)
features.11.conv.3 tensor(0.1341)
features.11.conv.6 tensor(0.5203)
features.12.conv.0 tensor(0.3675)
features.12.conv.3 tensor(0.1956)
features.12.conv.6 tensor(0.5488)
features.13.conv.0 tensor(0.0877)
features.13.conv.3 tensor(0.1734)
features.13.conv.6 tensor(0.2140)
features.14.conv.0 tensor(0.9624)
features.14.conv.3 tensor(0.1505)
features.14.conv.6 tensor(0.9516)
features.15.conv.0 tensor(0.9758)
features.15.conv.3 tensor(0.1002)
features.15.conv.6 tensor(0.9900)
features.16.conv.0 tensor(0.1361)
features.16.conv.3 tensor(0.1608)
features.16.conv.6 tensor(0.3601)
conv.0 tensor(0.4697)
tensor(1066745.) 2188896.0
INFO - Validation [0][   40/   40]   Loss 0.448178   Top1 85.080000   Top5 99.300000   BatchTime 0.092192
INFO - ==> Top1: 85.080    Top5: 99.300    Loss: 0.448
INFO - ==> Sparsity : 0.487
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 85.080   Top5: 99.300]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [1][   20/  196]   Loss 0.523037   Top1 81.855469   Top5 97.675781   BatchTime 0.403531   LR 0.004853
INFO - Training [1][   40/  196]   Loss 0.526947   Top1 82.041016   Top5 97.910156   BatchTime 0.367446   LR 0.004825
INFO - Training [1][   60/  196]   Loss 0.529521   Top1 81.842448   Top5 98.033854   BatchTime 0.353872   LR 0.004794
INFO - Training [1][   80/  196]   Loss 0.528598   Top1 81.914062   Top5 98.105469   BatchTime 0.353637   LR 0.004761
INFO - Training [1][  100/  196]   Loss 0.525088   Top1 82.050781   Top5 98.140625   BatchTime 0.350926   LR 0.004725
INFO - Training [1][  120/  196]   Loss 0.520186   Top1 82.141927   Top5 98.225911   BatchTime 0.349337   LR 0.004687
INFO - Training [1][  140/  196]   Loss 0.518188   Top1 82.193080   Top5 98.270089   BatchTime 0.347579   LR 0.004647
INFO - Training [1][  160/  196]   Loss 0.520242   Top1 82.155762   Top5 98.259277   BatchTime 0.346448   LR 0.004605
INFO - Training [1][  180/  196]   Loss 0.519092   Top1 82.187500   Top5 98.209635   BatchTime 0.346979   LR 0.004560
********************pre-trained*****************
INFO - ==> Top1: 82.162    Top5: 98.204    Loss: 0.519
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [1][   20/   40]   Loss 0.499055   Top1 83.867188   Top5 99.082031   BatchTime 0.125021
INFO - Validation [1][   40/   40]   Loss 0.506704   Top1 83.560000   Top5 99.150000   BatchTime 0.090157
INFO - ==> Top1: 83.560    Top5: 99.150    Loss: 0.507
INFO - ==> Sparsity : 0.509
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 85.080   Top5: 99.300]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 83.560   Top5: 99.150]
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.2188)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0421)
features.2.conv.0 tensor(0.0223)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0608)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0326)
features.4.conv.0 tensor(0.0355)
features.4.conv.3 tensor(0.0972)
features.4.conv.6 tensor(0.1134)
features.5.conv.0 tensor(0.0353)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.1836)
features.6.conv.0 tensor(0.0270)
features.6.conv.3 tensor(0.0411)
features.6.conv.6 tensor(0.0495)
features.7.conv.0 tensor(0.0574)
features.7.conv.3 tensor(0.1224)
features.7.conv.6 tensor(0.1669)
features.8.conv.0 tensor(0.0661)
features.8.conv.3 tensor(0.1348)
features.8.conv.6 tensor(0.1856)
features.9.conv.0 tensor(0.0767)
features.9.conv.3 tensor(0.1635)
features.9.conv.6 tensor(0.2253)
features.10.conv.0 tensor(0.0457)
features.10.conv.3 tensor(0.1013)
features.10.conv.6 tensor(0.1237)
features.11.conv.0 tensor(0.1661)
features.11.conv.3 tensor(0.1408)
features.11.conv.6 tensor(0.5517)
features.12.conv.0 tensor(0.4002)
features.12.conv.3 tensor(0.2031)
features.12.conv.6 tensor(0.5717)
features.13.conv.0 tensor(0.0948)
features.13.conv.3 tensor(0.1840)
features.13.conv.6 tensor(0.2486)
features.14.conv.0 tensor(0.9647)
features.14.conv.3 tensor(0.1613)
features.14.conv.6 tensor(0.9582)
features.15.conv.0 tensor(0.9755)
features.15.conv.3 tensor(0.1028)
features.15.conv.6 tensor(0.9907)
features.16.conv.0 tensor(0.1168)
features.16.conv.3 tensor(0.1703)
features.16.conv.6 tensor(0.4059)
conv.0 tensor(0.5256)
tensor(1114404.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [2][   20/  196]   Loss 0.530772   Top1 81.660156   Top5 97.558594   BatchTime 0.359103   LR 0.004477
INFO - Training [2][   40/  196]   Loss 0.536683   Top1 81.435547   Top5 97.705078   BatchTime 0.336316   LR 0.004426
INFO - Training [2][   60/  196]   Loss 0.528316   Top1 81.718750   Top5 97.792969   BatchTime 0.337956   LR 0.004374
INFO - Training [2][   80/  196]   Loss 0.518833   Top1 82.031250   Top5 97.915039   BatchTime 0.335263   LR 0.004320
INFO - Training [2][  100/  196]   Loss 0.510877   Top1 82.316406   Top5 97.996094   BatchTime 0.334193   LR 0.004264
INFO - Training [2][  120/  196]   Loss 0.507994   Top1 82.386068   Top5 98.082682   BatchTime 0.337540   LR 0.004206
INFO - Training [2][  140/  196]   Loss 0.507448   Top1 82.441406   Top5 98.127790   BatchTime 0.336905   LR 0.004146
INFO - Training [2][  160/  196]   Loss 0.509630   Top1 82.380371   Top5 98.103027   BatchTime 0.336783   LR 0.004085
INFO - Training [2][  180/  196]   Loss 0.508300   Top1 82.402344   Top5 98.083767   BatchTime 0.338024   LR 0.004022
********************pre-trained*****************
INFO - ==> Top1: 82.520    Top5: 98.128    Loss: 0.505
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [2][   20/   40]   Loss 0.503066   Top1 84.511719   Top5 99.316406   BatchTime 0.126987
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0412)
features.2.conv.0 tensor(0.0113)
features.2.conv.3 tensor(0.0563)
features.2.conv.6 tensor(0.0544)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0342)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.1296)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.2033)
features.6.conv.0 tensor(0.0272)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0539)
features.7.conv.0 tensor(0.0445)
features.7.conv.3 tensor(0.1299)
features.7.conv.6 tensor(0.1832)
features.8.conv.0 tensor(0.0791)
features.8.conv.3 tensor(0.1360)
features.8.conv.6 tensor(0.2078)
features.9.conv.0 tensor(0.0902)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.2532)
features.10.conv.0 tensor(0.0443)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.1421)
features.11.conv.0 tensor(0.1922)
features.11.conv.3 tensor(0.1424)
features.11.conv.6 tensor(0.5720)
features.12.conv.0 tensor(0.4189)
features.12.conv.3 tensor(0.2062)
features.12.conv.6 tensor(0.5879)
features.13.conv.0 tensor(0.1004)
features.13.conv.3 tensor(0.1746)
features.13.conv.6 tensor(0.2695)
features.14.conv.0 tensor(0.9625)
features.14.conv.3 tensor(0.1637)
features.14.conv.6 tensor(0.9583)
features.15.conv.0 tensor(0.9764)
features.15.conv.3 tensor(0.1133)
features.15.conv.6 tensor(0.9913)
features.16.conv.0 tensor(0.1355)
features.16.conv.3 tensor(0.1697)
features.16.conv.6 tensor(0.4305)
conv.0 tensor(0.5583)
tensor(1147736.) 2188896.0
INFO - Validation [2][   40/   40]   Loss 0.510246   Top1 84.140000   Top5 99.310000   BatchTime 0.092153
INFO - ==> Top1: 84.140    Top5: 99.310    Loss: 0.510
INFO - ==> Sparsity : 0.524
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 85.080   Top5: 99.300]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 84.140   Top5: 99.310]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 83.560   Top5: 99.150]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [3][   20/  196]   Loss 0.532209   Top1 81.308594   Top5 97.871094   BatchTime 0.406073   LR 0.003907
INFO - Training [3][   40/  196]   Loss 0.516122   Top1 82.148438   Top5 97.998047   BatchTime 0.340154   LR 0.003840
INFO - Training [3][   60/  196]   Loss 0.507735   Top1 82.480469   Top5 98.111979   BatchTime 0.329292   LR 0.003771
INFO - Training [3][   80/  196]   Loss 0.503746   Top1 82.744141   Top5 98.300781   BatchTime 0.334238   LR 0.003701
INFO - Training [3][  100/  196]   Loss 0.496669   Top1 82.976562   Top5 98.281250   BatchTime 0.335439   LR 0.003630
INFO - Training [3][  120/  196]   Loss 0.488545   Top1 83.304036   Top5 98.352865   BatchTime 0.336126   LR 0.003558
INFO - Training [3][  140/  196]   Loss 0.484601   Top1 83.470982   Top5 98.406808   BatchTime 0.335957   LR 0.003484
INFO - Training [3][  160/  196]   Loss 0.486732   Top1 83.344727   Top5 98.400879   BatchTime 0.333461   LR 0.003410
INFO - Training [3][  180/  196]   Loss 0.481988   Top1 83.424479   Top5 98.324653   BatchTime 0.333966   LR 0.003335
INFO - ==> Top1: 83.534    Top5: 98.350    Loss: 0.478
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [3][   20/   40]   Loss 0.421142   Top1 86.484375   Top5 99.375000   BatchTime 0.137279
INFO - Validation [3][   40/   40]   Loss 0.417999   Top1 86.410000   Top5 99.430000   BatchTime 0.097286
INFO - ==> Top1: 86.410    Top5: 99.430    Loss: 0.418
INFO - ==> Sparsity : 0.531
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 86.410   Top5: 99.430]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 85.080   Top5: 99.300]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 84.140   Top5: 99.310]
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.2324)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0150)
features.2.conv.3 tensor(0.0594)
features.2.conv.6 tensor(0.0532)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0286)
features.4.conv.3 tensor(0.0868)
features.4.conv.6 tensor(0.1473)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.2207)
features.6.conv.0 tensor(0.0239)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0535)
features.7.conv.0 tensor(0.0578)
features.7.conv.3 tensor(0.1282)
features.7.conv.6 tensor(0.1986)
features.8.conv.0 tensor(0.0800)
features.8.conv.3 tensor(0.1328)
features.8.conv.6 tensor(0.2213)
features.9.conv.0 tensor(0.0894)
features.9.conv.3 tensor(0.1620)
features.9.conv.6 tensor(0.2703)
features.10.conv.0 tensor(0.0430)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.1615)
features.11.conv.0 tensor(0.1841)
features.11.conv.3 tensor(0.1389)
features.11.conv.6 tensor(0.5819)
features.12.conv.0 tensor(0.4334)
features.12.conv.3 tensor(0.2141)
features.12.conv.6 tensor(0.5991)
features.13.conv.0 tensor(0.1046)
features.13.conv.3 tensor(0.1738)
features.13.conv.6 tensor(0.2881)
features.14.conv.0 tensor(0.9610)
features.14.conv.3 tensor(0.1654)
features.14.conv.6 tensor(0.9595)
features.15.conv.0 tensor(0.9774)
features.15.conv.3 tensor(0.1142)
features.15.conv.6 tensor(0.9917)
features.16.conv.0 tensor(0.1381)
features.16.conv.3 tensor(0.1737)
features.16.conv.6 tensor(0.4426)
conv.0 tensor(0.5715)
tensor(1163336.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [4][   20/  196]   Loss 0.467086   Top1 84.355469   Top5 97.734375   BatchTime 0.386073   LR 0.003200
INFO - Training [4][   40/  196]   Loss 0.462195   Top1 84.238281   Top5 98.154297   BatchTime 0.329378   LR 0.003122
INFO - Training [4][   60/  196]   Loss 0.457657   Top1 84.212240   Top5 98.229167   BatchTime 0.327842   LR 0.003044
INFO - Training [4][   80/  196]   Loss 0.451696   Top1 84.311523   Top5 98.378906   BatchTime 0.332347   LR 0.002965
INFO - Training [4][  100/  196]   Loss 0.447569   Top1 84.527344   Top5 98.375000   BatchTime 0.334299   LR 0.002886
INFO - Training [4][  120/  196]   Loss 0.444704   Top1 84.570312   Top5 98.476562   BatchTime 0.337335   LR 0.002806
INFO - Training [4][  140/  196]   Loss 0.444052   Top1 84.595424   Top5 98.512835   BatchTime 0.337159   LR 0.002726
INFO - Training [4][  160/  196]   Loss 0.444310   Top1 84.619141   Top5 98.505859   BatchTime 0.337630   LR 0.002646
INFO - Training [4][  180/  196]   Loss 0.442415   Top1 84.633247   Top5 98.467882   BatchTime 0.337905   LR 0.002566
INFO - ==> Top1: 84.662    Top5: 98.486    Loss: 0.441
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 0.370535   Top1 87.832031   Top5 99.570312   BatchTime 0.144138
INFO - Validation [4][   40/   40]   Loss 0.366862   Top1 87.740000   Top5 99.640000   BatchTime 0.099066
INFO - ==> Top1: 87.740    Top5: 99.640    Loss: 0.367
INFO - ==> Sparsity : 0.536
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 87.740   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 86.410   Top5: 99.430]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 85.080   Top5: 99.300]
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0334)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0548)
features.2.conv.6 tensor(0.0527)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0272)
features.4.conv.3 tensor(0.0856)
features.4.conv.6 tensor(0.1590)
features.5.conv.0 tensor(0.0410)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.2280)
features.6.conv.0 tensor(0.0249)
features.6.conv.3 tensor(0.0376)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0581)
features.7.conv.3 tensor(0.1357)
features.7.conv.6 tensor(0.2105)
features.8.conv.0 tensor(0.0790)
features.8.conv.3 tensor(0.1343)
features.8.conv.6 tensor(0.2351)
features.9.conv.0 tensor(0.0920)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.2815)
features.10.conv.0 tensor(0.0443)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.1762)
features.11.conv.0 tensor(0.1805)
features.11.conv.3 tensor(0.1453)
features.11.conv.6 tensor(0.5905)
features.12.conv.0 tensor(0.4404)
features.12.conv.3 tensor(0.2168)
features.12.conv.6 tensor(0.6021)
features.13.conv.0 tensor(0.1006)
features.13.conv.3 tensor(0.1777)
features.13.conv.6 tensor(0.2972)
features.14.conv.0 tensor(0.9526)
features.14.conv.3 tensor(0.1681)
features.14.conv.6 tensor(0.9578)
features.15.conv.0 tensor(0.9767)
features.15.conv.3 tensor(0.1140)
features.15.conv.6 tensor(0.9916)
features.16.conv.0 tensor(0.1343)
features.16.conv.3 tensor(0.1718)
features.16.conv.6 tensor(0.4526)
conv.0 tensor(0.5856)
tensor(1173046.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [5][   20/  196]   Loss 0.415988   Top1 85.527344   Top5 98.222656   BatchTime 0.356289   LR 0.002424
INFO - Training [5][   40/  196]   Loss 0.422089   Top1 85.615234   Top5 98.330078   BatchTime 0.325521   LR 0.002343
INFO - Training [5][   60/  196]   Loss 0.414117   Top1 85.657552   Top5 98.424479   BatchTime 0.329384   LR 0.002263
INFO - Training [5][   80/  196]   Loss 0.406903   Top1 85.991211   Top5 98.505859   BatchTime 0.332549   LR 0.002183
INFO - Training [5][  100/  196]   Loss 0.404480   Top1 85.980469   Top5 98.535156   BatchTime 0.331776   LR 0.002104
INFO - Training [5][  120/  196]   Loss 0.399163   Top1 86.162109   Top5 98.626302   BatchTime 0.335956   LR 0.002024
INFO - Training [5][  140/  196]   Loss 0.399728   Top1 86.171875   Top5 98.635603   BatchTime 0.337345   LR 0.001946
INFO - Training [5][  160/  196]   Loss 0.402704   Top1 86.066895   Top5 98.635254   BatchTime 0.336394   LR 0.001868
INFO - Training [5][  180/  196]   Loss 0.403620   Top1 85.985243   Top5 98.580729   BatchTime 0.336225   LR 0.001790
INFO - ==> Top1: 85.998    Top5: 98.578    Loss: 0.403
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.335831   Top1 89.179688   Top5 99.648438   BatchTime 0.143883
INFO - Validation [5][   40/   40]   Loss 0.327505   Top1 89.420000   Top5 99.700000   BatchTime 0.099513
INFO - ==> Top1: 89.420    Top5: 99.700    Loss: 0.328
INFO - ==> Sparsity : 0.537
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 89.420   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 87.740   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 86.410   Top5: 99.430]
features.0.conv.0 tensor(0.4896)
features.0.conv.3 tensor(0.2305)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0532)
features.2.conv.6 tensor(0.0550)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0352)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.0874)
features.4.conv.6 tensor(0.1649)
features.5.conv.0 tensor(0.0402)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.2337)
features.6.conv.0 tensor(0.0264)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0541)
features.7.conv.0 tensor(0.0553)
features.7.conv.3 tensor(0.1276)
features.7.conv.6 tensor(0.2160)
features.8.conv.0 tensor(0.0764)
features.8.conv.3 tensor(0.1351)
features.8.conv.6 tensor(0.2418)
features.9.conv.0 tensor(0.0901)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.2871)
features.10.conv.0 tensor(0.0445)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.1855)
features.11.conv.0 tensor(0.1913)
features.11.conv.3 tensor(0.1435)
features.11.conv.6 tensor(0.5897)
features.12.conv.0 tensor(0.4430)
features.12.conv.3 tensor(0.2145)
features.12.conv.6 tensor(0.6056)
features.13.conv.0 tensor(0.0994)
features.13.conv.3 tensor(0.1777)
features.13.conv.6 tensor(0.3007)
features.14.conv.0 tensor(0.9554)
features.14.conv.3 tensor(0.1654)
features.14.conv.6 tensor(0.9608)
features.15.conv.0 tensor(0.9771)
features.15.conv.3 tensor(0.1162)
features.15.conv.6 tensor(0.9902)
features.16.conv.0 tensor(0.1289)
features.16.conv.3 tensor(0.1692)
features.16.conv.6 tensor(0.4527)
conv.0 tensor(0.5891)
tensor(1176293.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [6][   20/  196]   Loss 0.396924   Top1 86.542969   Top5 98.105469   BatchTime 0.345513   LR 0.001655
INFO - Training [6][   40/  196]   Loss 0.388839   Top1 86.503906   Top5 98.398438   BatchTime 0.318207   LR 0.001580
INFO - Training [6][   60/  196]   Loss 0.382393   Top1 86.660156   Top5 98.509115   BatchTime 0.329074   LR 0.001506
INFO - Training [6][   80/  196]   Loss 0.376102   Top1 86.918945   Top5 98.598633   BatchTime 0.330108   LR 0.001432
INFO - Training [6][  100/  196]   Loss 0.374070   Top1 86.949219   Top5 98.664062   BatchTime 0.332690   LR 0.001360
INFO - Training [6][  120/  196]   Loss 0.369493   Top1 87.154948   Top5 98.714193   BatchTime 0.333239   LR 0.001289
INFO - Training [6][  140/  196]   Loss 0.370660   Top1 87.087054   Top5 98.752790   BatchTime 0.333859   LR 0.001220
INFO - Training [6][  160/  196]   Loss 0.371245   Top1 87.055664   Top5 98.742676   BatchTime 0.333652   LR 0.001151
INFO - Training [6][  180/  196]   Loss 0.369981   Top1 87.098524   Top5 98.695747   BatchTime 0.334208   LR 0.001084
INFO - ==> Top1: 87.120    Top5: 98.688    Loss: 0.370
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 0.299636   Top1 90.605469   Top5 99.609375   BatchTime 0.157376
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.2305)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0343)
features.2.conv.0 tensor(0.0165)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0480)
features.3.conv.0 tensor(0.0205)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0277)
features.4.conv.3 tensor(0.0845)
features.4.conv.6 tensor(0.1701)
features.5.conv.0 tensor(0.0387)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.2354)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0463)
features.6.conv.6 tensor(0.0539)
features.7.conv.0 tensor(0.0549)
features.7.conv.3 tensor(0.1282)
features.7.conv.6 tensor(0.2212)
features.8.conv.0 tensor(0.0735)
features.8.conv.3 tensor(0.1372)
features.8.conv.6 tensor(0.2433)
features.9.conv.0 tensor(0.0916)
features.9.conv.3 tensor(0.1644)
features.9.conv.6 tensor(0.2904)
features.10.conv.0 tensor(0.0440)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.1904)
features.11.conv.0 tensor(0.1948)
features.11.conv.3 tensor(0.1464)
features.11.conv.6 tensor(0.5907)
features.12.conv.0 tensor(0.4448)
features.12.conv.3 tensor(0.2168)
features.12.conv.6 tensor(0.6056)
features.13.conv.0 tensor(0.1013)
features.13.conv.3 tensor(0.1811)
features.13.conv.6 tensor(0.3049)
features.14.conv.0 tensor(0.9586)
features.14.conv.3 tensor(0.1676)
features.14.conv.6 tensor(0.9600)
features.15.conv.0 tensor(0.9763)
features.15.conv.3 tensor(0.1153)
features.15.conv.6 tensor(0.9912)
features.16.conv.0 tensor(0.1324)
features.16.conv.3 tensor(0.1708)
features.16.conv.6 tensor(0.4525)
conv.0 tensor(0.5908)
tensor(1179107.) 2188896.0
INFO - Validation [6][   40/   40]   Loss 0.289932   Top1 90.570000   Top5 99.720000   BatchTime 0.118560
INFO - ==> Top1: 90.570    Top5: 99.720    Loss: 0.290
INFO - ==> Sparsity : 0.539
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 89.420   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 87.740   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [7][   20/  196]   Loss 0.394794   Top1 86.132812   Top5 97.695312   BatchTime 0.356950   LR 0.000969
INFO - Training [7][   40/  196]   Loss 0.381416   Top1 86.650391   Top5 98.164062   BatchTime 0.322284   LR 0.000907
INFO - Training [7][   60/  196]   Loss 0.375449   Top1 86.783854   Top5 98.313802   BatchTime 0.331780   LR 0.000845
INFO - Training [7][   80/  196]   Loss 0.366861   Top1 87.045898   Top5 98.549805   BatchTime 0.332185   LR 0.000786
INFO - Training [7][  100/  196]   Loss 0.359906   Top1 87.316406   Top5 98.574219   BatchTime 0.335797   LR 0.000728
INFO - Training [7][  120/  196]   Loss 0.352536   Top1 87.584635   Top5 98.681641   BatchTime 0.337014   LR 0.000673
INFO - Training [7][  140/  196]   Loss 0.349400   Top1 87.712054   Top5 98.758371   BatchTime 0.338895   LR 0.000619
INFO - Training [7][  160/  196]   Loss 0.349862   Top1 87.666016   Top5 98.779297   BatchTime 0.338974   LR 0.000567
INFO - Training [7][  180/  196]   Loss 0.350244   Top1 87.647569   Top5 98.719618   BatchTime 0.338214   LR 0.000517
********************pre-trained*****************
INFO - ==> Top1: 87.732    Top5: 98.728    Loss: 0.348
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 0.299245   Top1 90.253906   Top5 99.589844   BatchTime 0.144705
INFO - Validation [7][   40/   40]   Loss 0.283881   Top1 90.530000   Top5 99.700000   BatchTime 0.100190
INFO - ==> Top1: 90.530    Top5: 99.700    Loss: 0.284
INFO - ==> Sparsity : 0.538
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 89.420   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4340)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0339)
features.2.conv.0 tensor(0.0150)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0506)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0409)
features.3.conv.6 tensor(0.0312)
features.4.conv.0 tensor(0.0265)
features.4.conv.3 tensor(0.0845)
features.4.conv.6 tensor(0.1717)
features.5.conv.0 tensor(0.0396)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.2363)
features.6.conv.0 tensor(0.0246)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0546)
features.7.conv.0 tensor(0.0531)
features.7.conv.3 tensor(0.1236)
features.7.conv.6 tensor(0.2238)
features.8.conv.0 tensor(0.0742)
features.8.conv.3 tensor(0.1380)
features.8.conv.6 tensor(0.2442)
features.9.conv.0 tensor(0.0914)
features.9.conv.3 tensor(0.1684)
features.9.conv.6 tensor(0.2913)
features.10.conv.0 tensor(0.0436)
features.10.conv.3 tensor(0.1007)
features.10.conv.6 tensor(0.1922)
features.11.conv.0 tensor(0.1924)
features.11.conv.3 tensor(0.1464)
features.11.conv.6 tensor(0.5907)
features.12.conv.0 tensor(0.4445)
features.12.conv.3 tensor(0.2139)
features.12.conv.6 tensor(0.6043)
features.13.conv.0 tensor(0.1025)
features.13.conv.3 tensor(0.1809)
features.13.conv.6 tensor(0.3053)
features.14.conv.0 tensor(0.9589)
features.14.conv.3 tensor(0.1660)
features.14.conv.6 tensor(0.9595)
features.15.conv.0 tensor(0.9760)
features.15.conv.3 tensor(0.1152)
features.15.conv.6 tensor(0.9906)
features.16.conv.0 tensor(0.1323)
features.16.conv.3 tensor(0.1719)
features.16.conv.6 tensor(0.4526)
conv.0 tensor(0.5889)
tensor(1178183.) 2188896.0
INFO - Training [8][   20/  196]   Loss 0.330756   Top1 87.792969   Top5 98.417969   BatchTime 0.414227   LR 0.000434
INFO - Training [8][   40/  196]   Loss 0.335301   Top1 87.666016   Top5 98.593750   BatchTime 0.350347   LR 0.000389
INFO - Training [8][   60/  196]   Loss 0.334777   Top1 88.014323   Top5 98.697917   BatchTime 0.343510   LR 0.000347
INFO - Training [8][   80/  196]   Loss 0.331427   Top1 88.286133   Top5 98.833008   BatchTime 0.343142   LR 0.000308
INFO - Training [8][  100/  196]   Loss 0.328101   Top1 88.394531   Top5 98.832031   BatchTime 0.342108   LR 0.000270
INFO - Training [8][  120/  196]   Loss 0.323147   Top1 88.583984   Top5 98.873698   BatchTime 0.342090   LR 0.000235
INFO - Training [8][  140/  196]   Loss 0.324069   Top1 88.621652   Top5 98.897879   BatchTime 0.341513   LR 0.000202
INFO - Training [8][  160/  196]   Loss 0.326969   Top1 88.510742   Top5 98.864746   BatchTime 0.340924   LR 0.000172
INFO - Training [8][  180/  196]   Loss 0.326818   Top1 88.506944   Top5 98.830295   BatchTime 0.340903   LR 0.000143
********************pre-trained*****************
INFO - ==> Top1: 88.640    Top5: 98.838    Loss: 0.323
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 0.301667   Top1 90.390625   Top5 99.609375   BatchTime 0.131012
features.0.conv.0 tensor(0.4479)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0326)
features.2.conv.0 tensor(0.0156)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0524)
features.3.conv.0 tensor(0.0165)
INFO - Validation [8][   40/   40]   Loss 0.287463   Top1 90.520000   Top5 99.710000   BatchTime 0.109104
INFO - ==> Top1: 90.520    Top5: 99.710    Loss: 0.287
INFO - ==> Sparsity : 0.538
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
features.3.conv.3 tensor(0.0401)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0269)
features.4.conv.3 tensor(0.0862)
features.4.conv.6 tensor(0.1724)
features.5.conv.0 tensor(0.0391)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.2362)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0545)
features.7.conv.0 tensor(0.0521)
features.7.conv.3 tensor(0.1227)
features.7.conv.6 tensor(0.2242)
features.8.conv.0 tensor(0.0732)
features.8.conv.3 tensor(0.1369)
features.8.conv.6 tensor(0.2442)
features.9.conv.0 tensor(0.0924)
features.9.conv.3 tensor(0.1655)
features.9.conv.6 tensor(0.2914)
features.10.conv.0 tensor(0.0446)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.1922)
features.11.conv.0 tensor(0.1913)
features.11.conv.3 tensor(0.1447)
features.11.conv.6 tensor(0.5897)
features.12.conv.0 tensor(0.4447)
features.12.conv.3 tensor(0.2141)
features.12.conv.6 tensor(0.6043)
features.13.conv.0 tensor(0.1036)
features.13.conv.3 tensor(0.1806)
features.13.conv.6 tensor(0.3056)
features.14.conv.0 tensor(0.9589)
features.14.conv.3 tensor(0.1666)
features.14.conv.6 tensor(0.9594)
features.15.conv.0 tensor(0.9758)
features.15.conv.3 tensor(0.1134)
features.15.conv.6 tensor(0.9906)
features.16.conv.0 tensor(0.1326)
features.16.conv.3 tensor(0.1713)
features.16.conv.6 tensor(0.4530)
conv.0 tensor(0.5880)
tensor(1177878.) 2188896.0
INFO - Training [9][   20/  196]   Loss 0.322914   Top1 88.632812   Top5 98.457031   BatchTime 0.440139   LR 0.000100
INFO - Training [9][   40/  196]   Loss 0.333758   Top1 88.261719   Top5 98.535156   BatchTime 0.369832   LR 0.000079
INFO - Training [9][   60/  196]   Loss 0.334300   Top1 88.131510   Top5 98.580729   BatchTime 0.337593   LR 0.000060
INFO - Training [9][   80/  196]   Loss 0.331584   Top1 88.339844   Top5 98.701172   BatchTime 0.339294   LR 0.000044
INFO - Training [9][  100/  196]   Loss 0.327360   Top1 88.570312   Top5 98.777344   BatchTime 0.339448   LR 0.000030
INFO - Training [9][  120/  196]   Loss 0.318705   Top1 88.922526   Top5 98.870443   BatchTime 0.338768   LR 0.000019
INFO - Training [9][  140/  196]   Loss 0.316545   Top1 88.962054   Top5 98.928571   BatchTime 0.337497   LR 0.000010
INFO - Training [9][  160/  196]   Loss 0.317503   Top1 88.896484   Top5 98.920898   BatchTime 0.336427   LR 0.000004
INFO - Training [9][  180/  196]   Loss 0.319366   Top1 88.789062   Top5 98.847656   BatchTime 0.336306   LR 0.000001
INFO - ==> Top1: 88.818    Top5: 98.848    Loss: 0.319
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.300845   Top1 90.351562   Top5 99.667969   BatchTime 0.134937
INFO - Validation [9][   40/   40]   Loss 0.288270   Top1 90.410000   Top5 99.750000   BatchTime 0.095189
INFO - ==> Top1: 90.410    Top5: 99.750    Loss: 0.288
INFO - ==> Sparsity : 0.538
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0130)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0321)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0532)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0394)
features.3.conv.6 tensor(0.0315)
features.4.conv.0 tensor(0.0259)
features.4.conv.3 tensor(0.0880)
features.4.conv.6 tensor(0.1719)
features.5.conv.0 tensor(0.0399)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2363)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0446)
features.6.conv.6 tensor(0.0549)
features.7.conv.0 tensor(0.0521)
features.7.conv.3 tensor(0.1238)
features.7.conv.6 tensor(0.2238)
features.8.conv.0 tensor(0.0730)
features.8.conv.3 tensor(0.1357)
features.8.conv.6 tensor(0.2443)
features.9.conv.0 tensor(0.0920)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.2914)
features.10.conv.0 tensor(0.0448)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.1921)
features.11.conv.0 tensor(0.1912)
features.11.conv.3 tensor(0.1445)
features.11.conv.6 tensor(0.5897)
features.12.conv.0 tensor(0.4447)
features.12.conv.3 tensor(0.2143)
features.12.conv.6 tensor(0.6043)
features.13.conv.0 tensor(0.1032)
features.13.conv.3 tensor(0.1800)
features.13.conv.6 tensor(0.3055)
features.14.conv.0 tensor(0.9586)
features.14.conv.3 tensor(0.1653)
features.14.conv.6 tensor(0.9596)
features.15.conv.0 tensor(0.9759)
features.15.conv.3 tensor(0.1153)
features.15.conv.6 tensor(0.9905)
features.16.conv.0 tensor(0.1321)
features.16.conv.3 tensor(0.1713)
features.16.conv.6 tensor(0.4527)
conv.0 tensor(0.5878)
tensor(1177562.) 2188896.0
INFO - Training [10][   20/  196]   Loss 0.362598   Top1 86.953125   Top5 98.222656   BatchTime 0.453577   LR 0.002500
INFO - Training [10][   40/  196]   Loss 0.378485   Top1 86.718750   Top5 98.359375   BatchTime 0.389361   LR 0.002499
INFO - Training [10][   60/  196]   Loss 0.377172   Top1 86.829427   Top5 98.515625   BatchTime 0.353408   LR 0.002499
INFO - Training [10][   80/  196]   Loss 0.379096   Top1 86.821289   Top5 98.623047   BatchTime 0.343126   LR 0.002497
INFO - Training [10][  100/  196]   Loss 0.379461   Top1 86.761719   Top5 98.589844   BatchTime 0.344466   LR 0.002496
INFO - Training [10][  120/  196]   Loss 0.375686   Top1 86.878255   Top5 98.694661   BatchTime 0.347429   LR 0.002494
INFO - Training [10][  140/  196]   Loss 0.376234   Top1 86.863839   Top5 98.738839   BatchTime 0.347217   LR 0.002492
INFO - Training [10][  160/  196]   Loss 0.381578   Top1 86.694336   Top5 98.715820   BatchTime 0.345927   LR 0.002490
INFO - Training [10][  180/  196]   Loss 0.384740   Top1 86.542969   Top5 98.632812   BatchTime 0.345012   LR 0.002487
INFO - ==> Top1: 86.514    Top5: 98.632    Loss: 0.385
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [10][   20/   40]   Loss 0.408409   Top1 87.421875   Top5 99.375000   BatchTime 0.134074
features.0.conv.0 tensor(0.4097)
features.0.conv.3 tensor(0.2285)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0127)
features.2.conv.3 tensor(0.0440)
features.2.conv.6 tensor(0.0486)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0355)
features.3.conv.6 tensor(0.0295)
features.4.conv.0 tensor(0.0299)
features.4.conv.3 tensor(0.0885)
features.4.conv.6 tensor(0.1816)
features.5.conv.0 tensor(0.0384)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.2437)
features.6.conv.0 tensor(0.0265)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0530)
features.7.conv.0 tensor(0.0528)
features.7.conv.3 tensor(0.1221)
features.7.conv.6 tensor(0.2340)
features.8.conv.0 tensor(0.0778)
features.8.conv.3 tensor(0.1418)
features.8.conv.6 tensor(0.2529)
features.9.conv.0 tensor(0.0942)
features.9.conv.3 tensor(0.1655)
features.9.conv.6 tensor(0.2998)
features.10.conv.0 tensor(0.0461)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.2017)
features.11.conv.0 tensor(0.1944)
features.11.conv.3 tensor(0.1443)
features.11.conv.6 tensor(0.5939)
features.12.conv.0 tensor(0.4489)
features.12.conv.3 tensor(0.2149)
features.12.conv.6 tensor(0.6126)
features.13.conv.0 tensor(0.0977)
features.13.conv.3 tensor(0.1780)
features.13.conv.6 tensor(0.3148)
features.14.conv.0 tensor(0.9626)
features.14.conv.3 tensor(0.1691)
features.14.conv.6 tensor(0.9604)
features.15.conv.0 tensor(0.9759)
features.15.conv.3 tensor(0.1153)
features.15.conv.6 tensor(0.9918)
features.16.conv.0 tensor(0.1418)
features.16.conv.3 tensor(0.1713)
features.16.conv.6 tensor(0.4623)
conv.0 tensor(0.5938)
tensor(1188420.) 2188896.0
INFO - Validation [10][   40/   40]   Loss 0.394246   Top1 87.180000   Top5 99.440000   BatchTime 0.094227
INFO - ==> Top1: 87.180    Top5: 99.440    Loss: 0.394
INFO - ==> Sparsity : 0.543
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [11][   20/  196]   Loss 0.400450   Top1 86.113281   Top5 98.222656   BatchTime 0.433460   LR 0.002481
INFO - Training [11][   40/  196]   Loss 0.418118   Top1 85.283203   Top5 98.427734   BatchTime 0.399959   LR 0.002478
INFO - Training [11][   60/  196]   Loss 0.419486   Top1 85.260417   Top5 98.489583   BatchTime 0.369274   LR 0.002474
INFO - Training [11][   80/  196]   Loss 0.412874   Top1 85.576172   Top5 98.613281   BatchTime 0.350055   LR 0.002470
INFO - Training [11][  100/  196]   Loss 0.404325   Top1 85.886719   Top5 98.593750   BatchTime 0.350326   LR 0.002465
INFO - Training [11][  120/  196]   Loss 0.397089   Top1 86.207682   Top5 98.671875   BatchTime 0.349628   LR 0.002460
INFO - Training [11][  140/  196]   Loss 0.396449   Top1 86.275112   Top5 98.702567   BatchTime 0.347770   LR 0.002455
INFO - Training [11][  160/  196]   Loss 0.399858   Top1 86.188965   Top5 98.688965   BatchTime 0.347455   LR 0.002450
INFO - Training [11][  180/  196]   Loss 0.398670   Top1 86.208767   Top5 98.639323   BatchTime 0.346162   LR 0.002444
********************pre-trained*****************
INFO - ==> Top1: 86.238    Top5: 98.646    Loss: 0.398
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 0.340187   Top1 89.082031   Top5 99.550781   BatchTime 0.135042
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.2305)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0502)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0363)
features.3.conv.6 tensor(0.0273)
features.4.conv.0 tensor(0.0326)
features.4.conv.3 tensor(0.0851)
features.4.conv.6 tensor(0.1888)
features.5.conv.0 tensor(0.0358)
features.5.conv.3 tensor(0.0799)
features.5.conv.6 tensor(0.2508)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0516)
features.7.conv.0 tensor(0.0554)
features.7.conv.3 tensor(0.1291)
features.7.conv.6 tensor(0.2445)
features.8.conv.0 tensor(0.0672)
features.8.conv.3 tensor(0.1322)
features.8.conv.6 tensor(0.2609)
features.9.conv.0 tensor(0.0801)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.3076)
features.10.conv.0 tensor(0.0472)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.2114)
features.11.conv.0 tensor(0.1817)
features.11.conv.3 tensor(0.1474)
features.11.conv.6 tensor(0.5996)
features.12.conv.0 tensor(0.4567)
features.12.conv.3 tensor(0.2135)
features.12.conv.6 tensor(0.6155)
features.13.conv.0 tensor(0.1107)
features.13.conv.3 tensor(0.1765)
features.13.conv.6 tensor(0.3221)
features.14.conv.0 tensor(0.9641)
features.14.conv.3 tensor(0.1681)
features.14.conv.6 tensor(0.9583)
features.15.conv.0 tensor(0.9769)
features.15.conv.3 tensor(0.1160)
features.15.conv.6 tensor(0.9922)
features.16.conv.0 tensor(0.1424)
features.16.conv.3 tensor(0.1736)
features.16.conv.6 tensor(0.4606)
conv.0 tensor(0.6053)
tensor(1194930.) 2188896.0
INFO - Validation [11][   40/   40]   Loss 0.333463   Top1 89.120000   Top5 99.540000   BatchTime 0.095004
INFO - ==> Top1: 89.120    Top5: 99.540    Loss: 0.333
INFO - ==> Sparsity : 0.546
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [12][   20/  196]   Loss 0.414129   Top1 85.429688   Top5 98.085938   BatchTime 0.417278   LR 0.002433
INFO - Training [12][   40/  196]   Loss 0.418007   Top1 85.283203   Top5 98.193359   BatchTime 0.384457   LR 0.002426
INFO - Training [12][   60/  196]   Loss 0.405701   Top1 85.722656   Top5 98.404948   BatchTime 0.372549   LR 0.002419
INFO - Training [12][   80/  196]   Loss 0.401847   Top1 85.976562   Top5 98.540039   BatchTime 0.350016   LR 0.002412
INFO - Training [12][  100/  196]   Loss 0.391790   Top1 86.328125   Top5 98.593750   BatchTime 0.330725   LR 0.002404
INFO - Training [12][  120/  196]   Loss 0.389831   Top1 86.445312   Top5 98.645833   BatchTime 0.324827   LR 0.002396
INFO - Training [12][  140/  196]   Loss 0.389688   Top1 86.453683   Top5 98.719308   BatchTime 0.327975   LR 0.002388
INFO - Training [12][  160/  196]   Loss 0.393771   Top1 86.271973   Top5 98.688965   BatchTime 0.329867   LR 0.002380
INFO - Training [12][  180/  196]   Loss 0.392195   Top1 86.338976   Top5 98.654514   BatchTime 0.331812   LR 0.002371
********************pre-trained*****************
INFO - ==> Top1: 86.312    Top5: 98.660    Loss: 0.392
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.368141   Top1 88.222656   Top5 99.414062   BatchTime 0.135362
INFO - Validation [12][   40/   40]   Loss 0.346644   Top1 88.650000   Top5 99.550000   BatchTime 0.093520
INFO - ==> Top1: 88.650    Top5: 99.550    Loss: 0.347
INFO - ==> Sparsity : 0.551
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4722)
features.0.conv.3 tensor(0.2422)
features.1.conv.0 tensor(0.0117)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0334)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0229)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0284)
features.4.conv.0 tensor(0.0291)
features.4.conv.3 tensor(0.0920)
features.4.conv.6 tensor(0.1947)
features.5.conv.0 tensor(0.0422)
features.5.conv.3 tensor(0.0822)
features.5.conv.6 tensor(0.2572)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0399)
features.6.conv.6 tensor(0.0504)
features.7.conv.0 tensor(0.0592)
features.7.conv.3 tensor(0.1322)
features.7.conv.6 tensor(0.2526)
features.8.conv.0 tensor(0.0765)
features.8.conv.3 tensor(0.1360)
features.8.conv.6 tensor(0.2682)
features.9.conv.0 tensor(0.0811)
features.9.conv.3 tensor(0.1649)
features.9.conv.6 tensor(0.3135)
features.10.conv.0 tensor(0.0395)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.2193)
features.11.conv.0 tensor(0.1870)
features.11.conv.3 tensor(0.1495)
features.11.conv.6 tensor(0.6029)
features.12.conv.0 tensor(0.4601)
features.12.conv.3 tensor(0.2106)
features.12.conv.6 tensor(0.6195)
features.13.conv.0 tensor(0.1158)
features.13.conv.3 tensor(0.1767)
features.13.conv.6 tensor(0.3272)
features.14.conv.0 tensor(0.9632)
features.14.conv.3 tensor(0.1667)
features.14.conv.6 tensor(0.9632)
features.15.conv.0 tensor(0.9731)
features.15.conv.3 tensor(0.1185)
features.15.conv.6 tensor(0.9912)
features.16.conv.0 tensor(0.1483)
features.16.conv.3 tensor(0.1762)
features.16.conv.6 tensor(0.4739)
conv.0 tensor(0.6117)
tensor(1205233.) 2188896.0
INFO - Training [13][   20/  196]   Loss 0.387426   Top1 86.601562   Top5 98.339844   BatchTime 0.416365   LR 0.002355
INFO - Training [13][   40/  196]   Loss 0.391755   Top1 86.445312   Top5 98.378906   BatchTime 0.378146   LR 0.002345
INFO - Training [13][   60/  196]   Loss 0.389549   Top1 86.588542   Top5 98.561198   BatchTime 0.372965   LR 0.002336
INFO - Training [13][   80/  196]   Loss 0.389948   Top1 86.425781   Top5 98.676758   BatchTime 0.365267   LR 0.002325
INFO - Training [13][  100/  196]   Loss 0.389015   Top1 86.503906   Top5 98.738281   BatchTime 0.358436   LR 0.002315
INFO - Training [13][  120/  196]   Loss 0.384263   Top1 86.686198   Top5 98.789062   BatchTime 0.345350   LR 0.002304
INFO - Training [13][  140/  196]   Loss 0.387028   Top1 86.682478   Top5 98.819754   BatchTime 0.340041   LR 0.002293
INFO - Training [13][  160/  196]   Loss 0.388083   Top1 86.596680   Top5 98.796387   BatchTime 0.339853   LR 0.002282
INFO - Training [13][  180/  196]   Loss 0.388919   Top1 86.540799   Top5 98.736979   BatchTime 0.340820   LR 0.002271
********************pre-trained*****************
INFO - ==> Top1: 86.490    Top5: 98.720    Loss: 0.391
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [13][   20/   40]   Loss 0.321876   Top1 89.550781   Top5 99.511719   BatchTime 0.137144
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0208)
features.2.conv.3 tensor(0.0424)
features.2.conv.6 tensor(0.0394)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0370)
features.3.conv.6 tensor(0.0284)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.0943)
features.4.conv.6 tensor(0.2031)
features.5.conv.0 tensor(0.0415)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.2627)
features.6.conv.0 tensor(0.0251)
features.6.conv.3 tensor(0.0347)
features.6.conv.6 tensor(0.0515)
features.7.conv.0 tensor(0.0651)
features.7.conv.3 tensor(0.1256)
features.7.conv.6 tensor(0.2615)
features.8.conv.0 tensor(0.0799)
features.8.conv.3 tensor(0.1354)
features.8.conv.6 tensor(0.2748)
features.9.conv.0 tensor(0.0833)
features.9.conv.3 tensor(0.1629)
features.9.conv.6 tensor(0.3182)
features.10.conv.0 tensor(0.0463)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.2259)
features.11.conv.0 tensor(0.2097)
features.11.conv.3 tensor(0.1435)
features.11.conv.6 tensor(0.6076)
features.12.conv.0 tensor(0.4632)
features.12.conv.3 tensor(0.2097)
features.12.conv.6 tensor(0.6253)
features.13.conv.0 tensor(0.1121)
features.13.conv.3 tensor(0.1750)
features.13.conv.6 tensor(0.3318)
features.14.conv.0 tensor(0.9636)
features.14.conv.3 tensor(0.1682)
features.14.conv.6 tensor(0.9607)
features.15.conv.0 tensor(0.9773)
features.15.conv.3 tensor(0.1196)
features.15.conv.6 tensor(0.9920)
features.16.conv.0 tensor(0.1499)
features.16.conv.3 tensor(0.1736)
features.16.conv.6 tensor(0.4774)
conv.0 tensor(0.6213)
tensor(1214318.) 2188896.0
INFO - Validation [13][   40/   40]   Loss 0.316162   Top1 89.580000   Top5 99.570000   BatchTime 0.095093
INFO - ==> Top1: 89.580    Top5: 99.570    Loss: 0.316
INFO - ==> Sparsity : 0.555
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [14][   20/  196]   Loss 0.394942   Top1 86.074219   Top5 98.359375   BatchTime 0.426755   LR 0.002250
INFO - Training [14][   40/  196]   Loss 0.393403   Top1 86.347656   Top5 98.398438   BatchTime 0.388795   LR 0.002238
INFO - Training [14][   60/  196]   Loss 0.395701   Top1 86.347656   Top5 98.528646   BatchTime 0.377609   LR 0.002225
INFO - Training [14][   80/  196]   Loss 0.391271   Top1 86.391602   Top5 98.662109   BatchTime 0.367232   LR 0.002213
INFO - Training [14][  100/  196]   Loss 0.386072   Top1 86.523438   Top5 98.687500   BatchTime 0.367989   LR 0.002200
INFO - Training [14][  120/  196]   Loss 0.378274   Top1 86.803385   Top5 98.779297   BatchTime 0.357246   LR 0.002186
INFO - Training [14][  140/  196]   Loss 0.376519   Top1 86.833147   Top5 98.830915   BatchTime 0.349819   LR 0.002173
INFO - Training [14][  160/  196]   Loss 0.380886   Top1 86.599121   Top5 98.820801   BatchTime 0.350027   LR 0.002159
INFO - Training [14][  180/  196]   Loss 0.380623   Top1 86.595052   Top5 98.778212   BatchTime 0.349423   LR 0.002145
INFO - ==> Top1: 86.662    Top5: 98.770    Loss: 0.380
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [14][   20/   40]   Loss 0.325123   Top1 89.355469   Top5 99.570312   BatchTime 0.146795
INFO - Validation [14][   40/   40]   Loss 0.309931   Top1 89.750000   Top5 99.630000   BatchTime 0.103103
INFO - ==> Top1: 89.750    Top5: 99.630    Loss: 0.310
INFO - ==> Sparsity : 0.557
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0165)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0422)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0355)
features.3.conv.6 tensor(0.0273)
features.4.conv.0 tensor(0.0247)
features.4.conv.3 tensor(0.0816)
features.4.conv.6 tensor(0.2088)
features.5.conv.0 tensor(0.0413)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.2666)
features.6.conv.0 tensor(0.0256)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0525)
features.7.conv.0 tensor(0.0640)
features.7.conv.3 tensor(0.1264)
features.7.conv.6 tensor(0.2699)
features.8.conv.0 tensor(0.0818)
features.8.conv.3 tensor(0.1351)
features.8.conv.6 tensor(0.2802)
features.9.conv.0 tensor(0.0859)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.3230)
features.10.conv.0 tensor(0.0454)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.2319)
features.11.conv.0 tensor(0.2120)
features.11.conv.3 tensor(0.1466)
features.11.conv.6 tensor(0.6087)
features.12.conv.0 tensor(0.4631)
features.12.conv.3 tensor(0.2124)
features.12.conv.6 tensor(0.6268)
features.13.conv.0 tensor(0.1191)
features.13.conv.3 tensor(0.1761)
features.13.conv.6 tensor(0.3379)
features.14.conv.0 tensor(0.9644)
features.14.conv.3 tensor(0.1681)
features.14.conv.6 tensor(0.9595)
features.15.conv.0 tensor(0.9784)
features.15.conv.3 tensor(0.1187)
features.15.conv.6 tensor(0.9920)
features.16.conv.0 tensor(0.1475)
features.16.conv.3 tensor(0.1694)
features.16.conv.6 tensor(0.4761)
conv.0 tensor(0.6291)
tensor(1218847.) 2188896.0
INFO - Training [15][   20/  196]   Loss 0.399073   Top1 85.566406   Top5 98.320312   BatchTime 0.424839   LR 0.002120
INFO - Training [15][   40/  196]   Loss 0.387026   Top1 86.357422   Top5 98.544922   BatchTime 0.381535   LR 0.002106
INFO - Training [15][   60/  196]   Loss 0.385024   Top1 86.386719   Top5 98.587240   BatchTime 0.373052   LR 0.002091
INFO - Training [15][   80/  196]   Loss 0.380904   Top1 86.596680   Top5 98.676758   BatchTime 0.363360   LR 0.002076
INFO - Training [15][  100/  196]   Loss 0.372814   Top1 86.851562   Top5 98.738281   BatchTime 0.360825   LR 0.002061
INFO - Training [15][  120/  196]   Loss 0.367916   Top1 87.119141   Top5 98.802083   BatchTime 0.359093   LR 0.002045
INFO - Training [15][  140/  196]   Loss 0.366995   Top1 87.151228   Top5 98.825335   BatchTime 0.350689   LR 0.002030
INFO - Training [15][  160/  196]   Loss 0.370839   Top1 87.011719   Top5 98.818359   BatchTime 0.340951   LR 0.002014
INFO - Training [15][  180/  196]   Loss 0.371403   Top1 87.005208   Top5 98.780382   BatchTime 0.330860   LR 0.001998
********************pre-trained*****************
INFO - ==> Top1: 87.052    Top5: 98.764    Loss: 0.371
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [15][   20/   40]   Loss 0.338073   Top1 89.277344   Top5 99.531250   BatchTime 0.134015
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0409)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0258)
features.4.conv.0 tensor(0.0301)
features.4.conv.3 tensor(0.0810)
features.4.conv.6 tensor(0.2129)
features.5.conv.0 tensor(0.0422)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.2689)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0370)
features.6.conv.6 tensor(0.0514)
features.7.conv.0 tensor(0.0576)
features.7.conv.3 tensor(0.1296)
features.7.conv.6 tensor(0.2737)
features.8.conv.0 tensor(0.0799)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.2839)
features.9.conv.0 tensor(0.0872)
features.9.conv.3 tensor(0.1632)
features.9.conv.6 tensor(0.3271)
features.10.conv.0 tensor(0.0433)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.2368)
features.11.conv.0 tensor(0.2244)
features.11.conv.3 tensor(0.1437)
features.11.conv.6 tensor(0.6129)
features.12.conv.0 tensor(0.4668)
features.12.conv.3 tensor(0.2133)
features.12.conv.6 tensor(0.6275)
features.13.conv.0 tensor(0.1136)
features.13.conv.3 tensor(0.1744)
features.13.conv.6 tensor(0.3406)
features.14.conv.0 tensor(0.9576)
features.14.conv.3 tensor(0.1688)
features.14.conv.6 tensor(0.9571)
features.15.conv.0 tensor(0.9788)
features.15.conv.3 tensor(0.1219)
features.15.conv.6 tensor(0.9916)
features.16.conv.0 tensor(0.1485)
features.16.conv.3 tensor(0.1747)
features.16.conv.6 tensor(0.4780)
conv.0 tensor(0.6348)
tensor(1221931.) 2188896.0
INFO - Validation [15][   40/   40]   Loss 0.325362   Top1 89.420000   Top5 99.670000   BatchTime 0.095850
INFO - ==> Top1: 89.420    Top5: 99.670    Loss: 0.325
INFO - ==> Sparsity : 0.558
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [16][   20/  196]   Loss 0.392315   Top1 86.640625   Top5 98.222656   BatchTime 0.427047   LR 0.001969
INFO - Training [16][   40/  196]   Loss 0.378204   Top1 86.943359   Top5 98.388672   BatchTime 0.383816   LR 0.001953
INFO - Training [16][   60/  196]   Loss 0.371003   Top1 87.063802   Top5 98.483073   BatchTime 0.366694   LR 0.001936
INFO - Training [16][   80/  196]   Loss 0.367623   Top1 87.241211   Top5 98.642578   BatchTime 0.359645   LR 0.001919
INFO - Training [16][  100/  196]   Loss 0.362760   Top1 87.468750   Top5 98.718750   BatchTime 0.354747   LR 0.001902
INFO - Training [16][  120/  196]   Loss 0.354536   Top1 87.760417   Top5 98.802083   BatchTime 0.350766   LR 0.001885
INFO - Training [16][  140/  196]   Loss 0.352566   Top1 87.776228   Top5 98.869978   BatchTime 0.351991   LR 0.001867
INFO - Training [16][  160/  196]   Loss 0.356722   Top1 87.631836   Top5 98.854980   BatchTime 0.351776   LR 0.001850
INFO - Training [16][  180/  196]   Loss 0.357164   Top1 87.641059   Top5 98.817274   BatchTime 0.345075   LR 0.001832
INFO - ==> Top1: 87.672    Top5: 98.810    Loss: 0.357
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [16][   20/   40]   Loss 0.317365   Top1 90.136719   Top5 99.589844   BatchTime 0.138016
features.0.conv.0 tensor(0.4549)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0343)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0469)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0386)
features.3.conv.6 tensor(0.0278)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.2174)
features.5.conv.0 tensor(0.0415)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.2734)
features.6.conv.0 tensor(0.0241)
features.6.conv.3 tensor(0.0422)
features.6.conv.6 tensor(0.0521)
features.7.conv.0 tensor(0.0553)
features.7.conv.3 tensor(0.1244)
features.7.conv.6 tensor(0.2791)
features.8.conv.0 tensor(0.0769)
features.8.conv.3 tensor(0.1311)
features.8.conv.6 tensor(0.2856)
features.9.conv.0 tensor(0.0874)
features.9.conv.3 tensor(0.1594)
features.9.conv.6 tensor(0.3301)
features.10.conv.0 tensor(0.0485)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.2409)
features.11.conv.0 tensor(0.2149)
features.11.conv.3 tensor(0.1426)
features.11.conv.6 tensor(0.6134)
features.12.conv.0 tensor(0.4699)
features.12.conv.3 tensor(0.2164)
features.12.conv.6 tensor(0.6275)
features.13.conv.0 tensor(0.1093)
features.13.conv.3 tensor(0.1713)
features.13.conv.6 tensor(0.3429)
features.14.conv.0 tensor(0.9558)
features.14.conv.3 tensor(0.1708)
features.14.conv.6 tensor(0.9584)
features.15.conv.0 tensor(0.9789)
features.15.conv.3 tensor(0.1206)
features.15.conv.6 tensor(0.9924)
features.16.conv.0 tensor(0.1441)
features.16.conv.3 tensor(0.1701)
features.16.conv.6 tensor(0.4813)
conv.0 tensor(0.6388)
tensor(1224051.) 2188896.0
INFO - Validation [16][   40/   40]   Loss 0.304218   Top1 90.340000   Top5 99.650000   BatchTime 0.097319
INFO - ==> Top1: 90.340    Top5: 99.650    Loss: 0.304
INFO - ==> Sparsity : 0.559
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 90.520   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [17][   20/  196]   Loss 0.362242   Top1 87.050781   Top5 98.222656   BatchTime 0.416332   LR 0.001800
INFO - Training [17][   40/  196]   Loss 0.360536   Top1 87.128906   Top5 98.496094   BatchTime 0.374623   LR 0.001782
INFO - Training [17][   60/  196]   Loss 0.361546   Top1 87.076823   Top5 98.593750   BatchTime 0.361019   LR 0.001764
INFO - Training [17][   80/  196]   Loss 0.358455   Top1 87.260742   Top5 98.691406   BatchTime 0.355006   LR 0.001746
INFO - Training [17][  100/  196]   Loss 0.353760   Top1 87.363281   Top5 98.769531   BatchTime 0.354262   LR 0.001727
INFO - Training [17][  120/  196]   Loss 0.352534   Top1 87.464193   Top5 98.795573   BatchTime 0.351557   LR 0.001708
INFO - Training [17][  140/  196]   Loss 0.352421   Top1 87.505580   Top5 98.850446   BatchTime 0.348553   LR 0.001690
INFO - Training [17][  160/  196]   Loss 0.352679   Top1 87.480469   Top5 98.852539   BatchTime 0.347889   LR 0.001671
INFO - Training [17][  180/  196]   Loss 0.349448   Top1 87.612847   Top5 98.843316   BatchTime 0.348183   LR 0.001652
INFO - ==> Top1: 87.620    Top5: 98.842    Loss: 0.349
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [17][   20/   40]   Loss 0.303853   Top1 90.507812   Top5 99.609375   BatchTime 0.169957
INFO - Validation [17][   40/   40]   Loss 0.296640   Top1 90.700000   Top5 99.620000   BatchTime 0.140525
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.2344)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0352)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0448)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0355)
features.3.conv.6 tensor(0.0286)
features.4.conv.0 tensor(0.0257)
features.4.conv.3 tensor(0.0833)
features.4.conv.6 tensor(0.2222)
features.5.conv.0 tensor(0.0420)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.2756)
features.6.conv.0 tensor(0.0257)
features.6.conv.3 tensor(0.0359)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0540)
features.7.conv.3 tensor(0.1244)
features.7.conv.6 tensor(0.2819)
features.8.conv.0 tensor(0.0750)
features.8.conv.3 tensor(0.1296)
features.8.conv.6 tensor(0.2889)
features.9.conv.0 tensor(0.0955)
features.9.conv.3 tensor(0.1609)
features.9.conv.6 tensor(0.3322)
features.10.conv.0 tensor(0.0480)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.2447)
features.11.conv.0 tensor(0.2160)
features.11.conv.3 tensor(0.1393)
features.11.conv.6 tensor(0.6156)
features.12.conv.0 tensor(0.4722)
features.12.conv.3 tensor(0.2126)
features.12.conv.6 tensor(0.6307)
features.13.conv.0 tensor(0.1139)
features.13.conv.3 tensor(0.1725)
features.13.conv.6 tensor(0.3444)
features.14.conv.0 tensor(0.9555)
features.14.conv.3 tensor(0.1716)
features.14.conv.6 tensor(0.9583)
features.15.conv.0 tensor(0.9782)
features.15.conv.3 tensor(0.1228)
features.15.conv.6 tensor(0.9924)
features.16.conv.0 tensor(0.1469)
features.16.conv.3 tensor(0.1712)
features.16.conv.6 tensor(0.4852)
conv.0 tensor(0.6419)
tensor(1228136.) 2188896.0
INFO - ==> Top1: 90.700    Top5: 99.620    Loss: 0.297
INFO - ==> Sparsity : 0.561
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 90.700   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [18][   20/  196]   Loss 0.355544   Top1 87.421875   Top5 98.496094   BatchTime 0.435166   LR 0.001618
INFO - Training [18][   40/  196]   Loss 0.361515   Top1 87.246094   Top5 98.583984   BatchTime 0.388903   LR 0.001599
INFO - Training [18][   60/  196]   Loss 0.356241   Top1 87.441406   Top5 98.619792   BatchTime 0.370816   LR 0.001579
INFO - Training [18][   80/  196]   Loss 0.355980   Top1 87.553711   Top5 98.735352   BatchTime 0.366707   LR 0.001560
INFO - Training [18][  100/  196]   Loss 0.345500   Top1 87.902344   Top5 98.796875   BatchTime 0.362883   LR 0.001540
INFO - Training [18][  120/  196]   Loss 0.338241   Top1 88.212891   Top5 98.850911   BatchTime 0.358891   LR 0.001521
INFO - Training [18][  140/  196]   Loss 0.336484   Top1 88.297991   Top5 98.909040   BatchTime 0.358489   LR 0.001501
INFO - Training [18][  160/  196]   Loss 0.337692   Top1 88.261719   Top5 98.896484   BatchTime 0.355066   LR 0.001482
INFO - Training [18][  180/  196]   Loss 0.335760   Top1 88.318142   Top5 98.904080   BatchTime 0.355093   LR 0.001462
********************pre-trained*****************
INFO - ==> Top1: 88.350    Top5: 98.912    Loss: 0.335
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 0.312658   Top1 90.234375   Top5 99.609375   BatchTime 0.141433
INFO - Validation [18][   40/   40]   Loss 0.304452   Top1 90.360000   Top5 99.710000   BatchTime 0.109562
INFO - ==> Top1: 90.360    Top5: 99.710    Loss: 0.304
INFO - ==> Sparsity : 0.561
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 90.700   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0124)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0509)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0200)
features.3.conv.3 tensor(0.0340)
features.3.conv.6 tensor(0.0278)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.0839)
features.4.conv.6 tensor(0.2259)
features.5.conv.0 tensor(0.0412)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.2778)
features.6.conv.0 tensor(0.0229)
features.6.conv.3 tensor(0.0394)
features.6.conv.6 tensor(0.0535)
features.7.conv.0 tensor(0.0586)
features.7.conv.3 tensor(0.1262)
features.7.conv.6 tensor(0.2836)
features.8.conv.0 tensor(0.0747)
features.8.conv.3 tensor(0.1273)
features.8.conv.6 tensor(0.2908)
features.9.conv.0 tensor(0.0963)
features.9.conv.3 tensor(0.1551)
features.9.conv.6 tensor(0.3348)
features.10.conv.0 tensor(0.0475)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.2483)
features.11.conv.0 tensor(0.2157)
features.11.conv.3 tensor(0.1424)
features.11.conv.6 tensor(0.6170)
features.12.conv.0 tensor(0.4744)
features.12.conv.3 tensor(0.2108)
features.12.conv.6 tensor(0.6304)
features.13.conv.0 tensor(0.1082)
features.13.conv.3 tensor(0.1721)
features.13.conv.6 tensor(0.3441)
features.14.conv.0 tensor(0.9575)
features.14.conv.3 tensor(0.1713)
features.14.conv.6 tensor(0.9587)
features.15.conv.0 tensor(0.9783)
features.15.conv.3 tensor(0.1213)
features.15.conv.6 tensor(0.9925)
features.16.conv.0 tensor(0.1410)
features.16.conv.3 tensor(0.1705)
features.16.conv.6 tensor(0.4856)
conv.0 tensor(0.6442)
tensor(1228918.) 2188896.0
INFO - Training [19][   20/  196]   Loss 0.348751   Top1 88.046875   Top5 98.437500   BatchTime 0.427738   LR 0.001427
INFO - Training [19][   40/  196]   Loss 0.348656   Top1 88.076172   Top5 98.564453   BatchTime 0.379131   LR 0.001407
INFO - Training [19][   60/  196]   Loss 0.346713   Top1 88.027344   Top5 98.613281   BatchTime 0.363486   LR 0.001387
INFO - Training [19][   80/  196]   Loss 0.343732   Top1 88.139648   Top5 98.720703   BatchTime 0.357484   LR 0.001367
INFO - Training [19][  100/  196]   Loss 0.337788   Top1 88.292969   Top5 98.781250   BatchTime 0.354810   LR 0.001347
INFO - Training [19][  120/  196]   Loss 0.330144   Top1 88.496094   Top5 98.880208   BatchTime 0.351569   LR 0.001327
INFO - Training [19][  140/  196]   Loss 0.328433   Top1 88.557478   Top5 98.948103   BatchTime 0.349644   LR 0.001307
INFO - Training [19][  160/  196]   Loss 0.332637   Top1 88.366699   Top5 98.945312   BatchTime 0.347748   LR 0.001287
INFO - Training [19][  180/  196]   Loss 0.332752   Top1 88.313802   Top5 98.921441   BatchTime 0.347149   LR 0.001266
INFO - ==> Top1: 88.312    Top5: 98.912    Loss: 0.333
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 0.319874   Top1 89.824219   Top5 99.687500   BatchTime 0.144729
INFO - Validation [19][   40/   40]   Loss 0.304810   Top1 90.300000   Top5 99.730000   BatchTime 0.104958
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0150)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0347)
features.3.conv.6 tensor(0.0278)
features.4.conv.0 tensor(0.0306)
features.4.conv.3 tensor(0.0851)
features.4.conv.6 tensor(0.2282)
features.5.conv.0 tensor(0.0409)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.2790)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0530)
features.7.conv.0 tensor(0.0614)
features.7.conv.3 tensor(0.1198)
features.7.conv.6 tensor(0.2861)
features.8.conv.0 tensor(0.0740)
features.8.conv.3 tensor(0.1282)
features.8.conv.6 tensor(0.2929)
features.9.conv.0 tensor(0.0971)
features.9.conv.3 tensor(0.1574)
features.9.conv.6 tensor(0.3359)
features.10.conv.0 tensor(0.0470)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.2514)
features.11.conv.0 tensor(0.2202)
features.11.conv.3 tensor(0.1445)
features.11.conv.6 tensor(0.6168)
features.12.conv.0 tensor(0.4765)
features.12.conv.3 tensor(0.2124)
features.12.conv.6 tensor(0.6299)
features.13.conv.0 tensor(0.1143)
features.13.conv.3 tensor(0.1750)
features.13.conv.6 tensor(0.3462)
features.14.conv.0 tensor(0.9588)
features.14.conv.3 tensor(0.1728)
features.14.conv.6 tensor(0.9595)
features.15.conv.0 tensor(0.9785)
features.15.conv.3 tensor(0.1212)
features.15.conv.6 tensor(0.9914)
features.16.conv.0 tensor(0.1360)
features.16.conv.3 tensor(0.1711)
features.16.conv.6 tensor(0.4897)
conv.0 tensor(0.6478)
tensor(1232293.) 2188896.0
INFO - ==> Top1: 90.300    Top5: 99.730    Loss: 0.305
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 90.700   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 90.530   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [20][   20/  196]   Loss 0.330168   Top1 88.105469   Top5 98.710938   BatchTime 0.394328   LR 0.001231
INFO - Training [20][   40/  196]   Loss 0.328988   Top1 88.310547   Top5 98.759766   BatchTime 0.367147   LR 0.001211
INFO - Training [20][   60/  196]   Loss 0.327878   Top1 88.424479   Top5 98.776042   BatchTime 0.354993   LR 0.001191
INFO - Training [20][   80/  196]   Loss 0.326791   Top1 88.442383   Top5 98.935547   BatchTime 0.351947   LR 0.001171
INFO - Training [20][  100/  196]   Loss 0.320623   Top1 88.656250   Top5 98.964844   BatchTime 0.349066   LR 0.001151
INFO - Training [20][  120/  196]   Loss 0.316476   Top1 88.782552   Top5 99.029948   BatchTime 0.349733   LR 0.001131
INFO - Training [20][  140/  196]   Loss 0.318744   Top1 88.699777   Top5 99.068080   BatchTime 0.349960   LR 0.001111
INFO - Training [20][  160/  196]   Loss 0.323950   Top1 88.535156   Top5 99.040527   BatchTime 0.347575   LR 0.001091
INFO - Training [20][  180/  196]   Loss 0.323611   Top1 88.537326   Top5 98.999566   BatchTime 0.346194   LR 0.001071
********************pre-trained*****************
INFO - ==> Top1: 88.578    Top5: 98.994    Loss: 0.322
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [20][   20/   40]   Loss 0.304687   Top1 90.429688   Top5 99.531250   BatchTime 0.143977
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0414)
features.3.conv.0 tensor(0.0156)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0293)
features.4.conv.0 tensor(0.0306)
features.4.conv.3 tensor(0.0828)
features.4.conv.6 tensor(0.2298)
features.5.conv.0 tensor(0.0425)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.2795)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0539)
features.7.conv.0 tensor(0.0612)
features.7.conv.3 tensor(0.1227)
features.7.conv.6 tensor(0.2874)
features.8.conv.0 tensor(0.0778)
features.8.conv.3 tensor(0.1273)
features.8.conv.6 tensor(0.2942)
features.9.conv.0 tensor(0.0978)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.3374)
features.10.conv.0 tensor(0.0426)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.2532)
features.11.conv.0 tensor(0.2138)
features.11.conv.3 tensor(0.1445)
features.11.conv.6 tensor(0.6174)
features.12.conv.0 tensor(0.4779)
features.12.conv.3 tensor(0.2141)
features.12.conv.6 tensor(0.6289)
features.13.conv.0 tensor(0.1107)
features.13.conv.3 tensor(0.1707)
features.13.conv.6 tensor(0.3466)
features.14.conv.0 tensor(0.9578)
features.14.conv.3 tensor(0.1725)
features.14.conv.6 tensor(0.9599)
features.15.conv.0 tensor(0.9784)
features.15.conv.3 tensor(0.1220)
features.15.conv.6 tensor(0.9921)
features.16.conv.0 tensor(0.1342)
features.16.conv.3 tensor(0.1737)
features.16.conv.6 tensor(0.4916)
conv.0 tensor(0.6471)
tensor(1232050.) 2188896.0
INFO - Validation [20][   40/   40]   Loss 0.290904   Top1 90.690000   Top5 99.680000   BatchTime 0.098784
INFO - ==> Top1: 90.690    Top5: 99.680    Loss: 0.291
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 90.700   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 90.690   Top5: 99.680]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 90.570   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [21][   20/  196]   Loss 0.326150   Top1 88.476562   Top5 98.574219   BatchTime 0.368221   LR 0.001036
INFO - Training [21][   40/  196]   Loss 0.335098   Top1 88.134766   Top5 98.447266   BatchTime 0.314055   LR 0.001016
INFO - Training [21][   60/  196]   Loss 0.327878   Top1 88.463542   Top5 98.593750   BatchTime 0.303720   LR 0.000996
INFO - Training [21][   80/  196]   Loss 0.328587   Top1 88.437500   Top5 98.715820   BatchTime 0.312255   LR 0.000976
INFO - Training [21][  100/  196]   Loss 0.319027   Top1 88.765625   Top5 98.800781   BatchTime 0.315975   LR 0.000957
INFO - Training [21][  120/  196]   Loss 0.311647   Top1 89.023438   Top5 98.873698   BatchTime 0.319619   LR 0.000937
INFO - Training [21][  140/  196]   Loss 0.307223   Top1 89.215960   Top5 98.934152   BatchTime 0.322460   LR 0.000918
INFO - Training [21][  160/  196]   Loss 0.311155   Top1 89.133301   Top5 98.911133   BatchTime 0.324868   LR 0.000899
INFO - Training [21][  180/  196]   Loss 0.311426   Top1 89.105903   Top5 98.891059   BatchTime 0.325650   LR 0.000879
INFO - ==> Top1: 89.156    Top5: 98.878    Loss: 0.310
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [21][   20/   40]   Loss 0.302973   Top1 90.839844   Top5 99.589844   BatchTime 0.135883
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0420)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0299)
features.4.conv.0 tensor(0.0314)
features.4.conv.3 tensor(0.0770)
features.4.conv.6 tensor(0.2313)
features.5.conv.0 tensor(0.0415)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.2804)
features.6.conv.0 tensor(0.0226)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0625)
features.7.conv.3 tensor(0.1270)
features.7.conv.6 tensor(0.2884)
features.8.conv.0 tensor(0.0773)
features.8.conv.3 tensor(0.1296)
features.8.conv.6 tensor(0.2952)
features.9.conv.0 tensor(0.0955)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.3378)
features.10.conv.0 tensor(0.0459)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.2552)
features.11.conv.0 tensor(0.2112)
features.11.conv.3 tensor(0.1449)
features.11.conv.6 tensor(0.6174)
features.12.conv.0 tensor(0.4789)
features.12.conv.3 tensor(0.2135)
features.12.conv.6 tensor(0.6290)
features.13.conv.0 tensor(0.1149)
features.13.conv.3 tensor(0.1715)
features.13.conv.6 tensor(0.3485)
features.14.conv.0 tensor(0.9585)
features.14.conv.3 tensor(0.1736)
features.14.conv.6 tensor(0.9596)
features.15.conv.0 tensor(0.9785)
features.15.conv.3 tensor(0.1222)
features.15.conv.6 tensor(0.9919)
features.16.conv.0 tensor(0.1343)
features.16.conv.3 tensor(0.1743)
features.16.conv.6 tensor(0.4878)
conv.0 tensor(0.6477)
tensor(1231740.) 2188896.0
INFO - Validation [21][   40/   40]   Loss 0.289777   Top1 90.790000   Top5 99.690000   BatchTime 0.096057
INFO - ==> Top1: 90.790    Top5: 99.690    Loss: 0.290
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 90.790   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 90.700   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 90.690   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [22][   20/  196]   Loss 0.317324   Top1 89.003906   Top5 98.554688   BatchTime 0.399020   LR 0.000846
INFO - Training [22][   40/  196]   Loss 0.318478   Top1 88.916016   Top5 98.710938   BatchTime 0.329911   LR 0.000827
INFO - Training [22][   60/  196]   Loss 0.315733   Top1 88.945312   Top5 98.828125   BatchTime 0.319163   LR 0.000808
INFO - Training [22][   80/  196]   Loss 0.314123   Top1 89.028320   Top5 98.964844   BatchTime 0.322152   LR 0.000789
INFO - Training [22][  100/  196]   Loss 0.307565   Top1 89.203125   Top5 99.019531   BatchTime 0.327387   LR 0.000770
INFO - Training [22][  120/  196]   Loss 0.302264   Top1 89.459635   Top5 99.078776   BatchTime 0.329346   LR 0.000752
INFO - Training [22][  140/  196]   Loss 0.300734   Top1 89.497768   Top5 99.126674   BatchTime 0.334502   LR 0.000734
INFO - Training [22][  160/  196]   Loss 0.302081   Top1 89.475098   Top5 99.155273   BatchTime 0.335892   LR 0.000715
INFO - Training [22][  180/  196]   Loss 0.304143   Top1 89.394531   Top5 99.092882   BatchTime 0.336048   LR 0.000697
INFO - ==> Top1: 89.436    Top5: 99.076    Loss: 0.302
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.316053   Top1 90.351562   Top5 99.648438   BatchTime 0.129195
INFO - Validation [22][   40/   40]   Loss 0.305085   Top1 90.510000   Top5 99.700000   BatchTime 0.094181
INFO - ==> Top1: 90.510    Top5: 99.700    Loss: 0.305
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 90.790   Top5: 99.690]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 90.700   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 90.690   Top5: 99.680]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0326)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0486)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0291)
features.4.conv.0 tensor(0.0308)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2323)
features.5.conv.0 tensor(0.0409)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.2814)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0534)
features.7.conv.0 tensor(0.0592)
features.7.conv.3 tensor(0.1262)
features.7.conv.6 tensor(0.2882)
features.8.conv.0 tensor(0.0779)
features.8.conv.3 tensor(0.1264)
features.8.conv.6 tensor(0.2957)
features.9.conv.0 tensor(0.0963)
features.9.conv.3 tensor(0.1617)
features.9.conv.6 tensor(0.3382)
features.10.conv.0 tensor(0.0464)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.2561)
features.11.conv.0 tensor(0.2140)
features.11.conv.3 tensor(0.1445)
features.11.conv.6 tensor(0.6172)
features.12.conv.0 tensor(0.4791)
features.12.conv.3 tensor(0.2124)
features.12.conv.6 tensor(0.6290)
features.13.conv.0 tensor(0.1131)
features.13.conv.3 tensor(0.1703)
features.13.conv.6 tensor(0.3490)
features.14.conv.0 tensor(0.9557)
features.14.conv.3 tensor(0.1730)
features.14.conv.6 tensor(0.9581)
features.15.conv.0 tensor(0.9788)
features.15.conv.3 tensor(0.1208)
features.15.conv.6 tensor(0.9912)
features.16.conv.0 tensor(0.1353)
features.16.conv.3 tensor(0.1742)
features.16.conv.6 tensor(0.4897)
conv.0 tensor(0.6480)
tensor(1232013.) 2188896.0
INFO - Training [23][   20/  196]   Loss 0.325187   Top1 88.828125   Top5 98.691406   BatchTime 0.447581   LR 0.000666
INFO - Training [23][   40/  196]   Loss 0.321544   Top1 88.613281   Top5 98.769531   BatchTime 0.369925   LR 0.000648
INFO - Training [23][   60/  196]   Loss 0.313215   Top1 88.919271   Top5 98.886719   BatchTime 0.335020   LR 0.000630
INFO - Training [23][   80/  196]   Loss 0.306612   Top1 89.184570   Top5 99.038086   BatchTime 0.334656   LR 0.000613
INFO - Training [23][  100/  196]   Loss 0.299396   Top1 89.457031   Top5 99.062500   BatchTime 0.335718   LR 0.000596
INFO - Training [23][  120/  196]   Loss 0.293948   Top1 89.615885   Top5 99.098307   BatchTime 0.337313   LR 0.000579
INFO - Training [23][  140/  196]   Loss 0.292786   Top1 89.718192   Top5 99.129464   BatchTime 0.338744   LR 0.000562
INFO - Training [23][  160/  196]   Loss 0.294229   Top1 89.658203   Top5 99.106445   BatchTime 0.339569   LR 0.000545
INFO - Training [23][  180/  196]   Loss 0.295826   Top1 89.578993   Top5 99.084201   BatchTime 0.338399   LR 0.000529
INFO - ==> Top1: 89.656    Top5: 99.090    Loss: 0.294
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.283952   Top1 91.093750   Top5 99.726562   BatchTime 0.129913
INFO - Validation [23][   40/   40]   Loss 0.273815   Top1 91.250000   Top5 99.750000   BatchTime 0.091274
INFO - ==> Top1: 91.250    Top5: 99.750    Loss: 0.274
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 91.250   Top5: 99.750]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 90.790   Top5: 99.690]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 90.700   Top5: 99.620]
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0330)
features.2.conv.0 tensor(0.0179)
features.2.conv.3 tensor(0.0478)
features.2.conv.6 tensor(0.0463)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0289)
features.4.conv.0 tensor(0.0319)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2331)
features.5.conv.0 tensor(0.0443)
features.5.conv.3 tensor(0.0712)
features.5.conv.6 tensor(0.2817)
features.6.conv.0 tensor(0.0195)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0530)
features.7.conv.0 tensor(0.0601)
features.7.conv.3 tensor(0.1264)
features.7.conv.6 tensor(0.2887)
features.8.conv.0 tensor(0.0797)
features.8.conv.3 tensor(0.1285)
features.8.conv.6 tensor(0.2959)
features.9.conv.0 tensor(0.0957)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.3387)
features.10.conv.0 tensor(0.0448)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.2565)
features.11.conv.0 tensor(0.2124)
features.11.conv.3 tensor(0.1437)
features.11.conv.6 tensor(0.6167)
features.12.conv.0 tensor(0.4790)
features.12.conv.3 tensor(0.2108)
features.12.conv.6 tensor(0.6290)
features.13.conv.0 tensor(0.1122)
features.13.conv.3 tensor(0.1701)
features.13.conv.6 tensor(0.3493)
features.14.conv.0 tensor(0.9553)
features.14.conv.3 tensor(0.1711)
features.14.conv.6 tensor(0.9577)
features.15.conv.0 tensor(0.9787)
features.15.conv.3 tensor(0.1219)
features.15.conv.6 tensor(0.9910)
features.16.conv.0 tensor(0.1360)
features.16.conv.3 tensor(0.1743)
features.16.conv.6 tensor(0.4893)
conv.0 tensor(0.6479)
tensor(1231677.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [24][   20/  196]   Loss 0.318093   Top1 88.964844   Top5 98.613281   BatchTime 0.428731   LR 0.000500
INFO - Training [24][   40/  196]   Loss 0.314463   Top1 89.111328   Top5 98.720703   BatchTime 0.361728   LR 0.000484
INFO - Training [24][   60/  196]   Loss 0.302892   Top1 89.518229   Top5 98.769531   BatchTime 0.339986   LR 0.000468
INFO - Training [24][   80/  196]   Loss 0.302943   Top1 89.526367   Top5 98.881836   BatchTime 0.340310   LR 0.000453
INFO - Training [24][  100/  196]   Loss 0.295683   Top1 89.777344   Top5 98.910156   BatchTime 0.341743   LR 0.000437
INFO - Training [24][  120/  196]   Loss 0.291982   Top1 89.879557   Top5 98.994141   BatchTime 0.339818   LR 0.000422
INFO - Training [24][  140/  196]   Loss 0.288290   Top1 90.002790   Top5 99.051339   BatchTime 0.342733   LR 0.000407
INFO - Training [24][  160/  196]   Loss 0.291501   Top1 89.877930   Top5 99.035645   BatchTime 0.344398   LR 0.000392
INFO - Training [24][  180/  196]   Loss 0.290505   Top1 89.889323   Top5 99.023438   BatchTime 0.343319   LR 0.000378
INFO - ==> Top1: 89.902    Top5: 99.008    Loss: 0.290
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [24][   20/   40]   Loss 0.282023   Top1 91.093750   Top5 99.648438   BatchTime 0.140104
INFO - Validation [24][   40/   40]   Loss 0.269213   Top1 91.570000   Top5 99.730000   BatchTime 0.098033
INFO - ==> Top1: 91.570    Top5: 99.730    Loss: 0.269
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 91.250   Top5: 99.750]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 90.790   Top5: 99.690]
features.0.conv.0 tensor(0.4132)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0648)
features.1.conv.6 tensor(0.0330)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0443)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0306)
features.4.conv.0 tensor(0.0319)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2329)
features.5.conv.0 tensor(0.0425)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2821)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0516)
features.7.conv.0 tensor(0.0595)
features.7.conv.3 tensor(0.1256)
features.7.conv.6 tensor(0.2894)
features.8.conv.0 tensor(0.0795)
features.8.conv.3 tensor(0.1299)
features.8.conv.6 tensor(0.2962)
features.9.conv.0 tensor(0.0952)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.3392)
features.10.conv.0 tensor(0.0439)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.2568)
features.11.conv.0 tensor(0.2088)
features.11.conv.3 tensor(0.1447)
features.11.conv.6 tensor(0.6160)
features.12.conv.0 tensor(0.4787)
features.12.conv.3 tensor(0.2139)
features.12.conv.6 tensor(0.6283)
features.13.conv.0 tensor(0.1099)
features.13.conv.3 tensor(0.1707)
features.13.conv.6 tensor(0.3487)
features.14.conv.0 tensor(0.9557)
features.14.conv.3 tensor(0.1709)
features.14.conv.6 tensor(0.9585)
features.15.conv.0 tensor(0.9787)
features.15.conv.3 tensor(0.1221)
features.15.conv.6 tensor(0.9915)
features.16.conv.0 tensor(0.1390)
features.16.conv.3 tensor(0.1727)
features.16.conv.6 tensor(0.4895)
conv.0 tensor(0.6484)
tensor(1232185.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [25][   20/  196]   Loss 0.300837   Top1 89.140625   Top5 98.613281   BatchTime 0.415439   LR 0.000353
INFO - Training [25][   40/  196]   Loss 0.298194   Top1 89.199219   Top5 98.681641   BatchTime 0.357067   LR 0.000339
INFO - Training [25][   60/  196]   Loss 0.290753   Top1 89.557292   Top5 98.795573   BatchTime 0.332662   LR 0.000325
INFO - Training [25][   80/  196]   Loss 0.295770   Top1 89.497070   Top5 98.950195   BatchTime 0.335672   LR 0.000312
INFO - Training [25][  100/  196]   Loss 0.287824   Top1 89.804688   Top5 98.984375   BatchTime 0.337194   LR 0.000299
INFO - Training [25][  120/  196]   Loss 0.281821   Top1 90.084635   Top5 99.069010   BatchTime 0.337703   LR 0.000286
INFO - Training [25][  140/  196]   Loss 0.280276   Top1 90.164621   Top5 99.140625   BatchTime 0.336590   LR 0.000273
INFO - Training [25][  160/  196]   Loss 0.282899   Top1 90.102539   Top5 99.099121   BatchTime 0.337469   LR 0.000261
INFO - Training [25][  180/  196]   Loss 0.283002   Top1 90.112847   Top5 99.066840   BatchTime 0.338087   LR 0.000248
INFO - ==> Top1: 90.116    Top5: 99.066    Loss: 0.282
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 0.280793   Top1 91.093750   Top5 99.609375   BatchTime 0.137537
INFO - Validation [25][   40/   40]   Loss 0.265257   Top1 91.550000   Top5 99.620000   BatchTime 0.096905
INFO - ==> Top1: 91.550    Top5: 99.620    Loss: 0.265
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 91.250   Top5: 99.750]
features.0.conv.0 tensor(0.4236)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0179)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0148)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0297)
features.4.conv.0 tensor(0.0309)
features.4.conv.3 tensor(0.0799)
features.4.conv.6 tensor(0.2334)
features.5.conv.0 tensor(0.0436)
features.5.conv.3 tensor(0.0694)
features.5.conv.6 tensor(0.2822)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0586)
features.7.conv.3 tensor(0.1247)
features.7.conv.6 tensor(0.2892)
features.8.conv.0 tensor(0.0798)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.2964)
features.9.conv.0 tensor(0.0944)
features.9.conv.3 tensor(0.1623)
features.9.conv.6 tensor(0.3392)
features.10.conv.0 tensor(0.0437)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.2570)
features.11.conv.0 tensor(0.2115)
features.11.conv.3 tensor(0.1439)
features.11.conv.6 tensor(0.6163)
features.12.conv.0 tensor(0.4789)
features.12.conv.3 tensor(0.2105)
features.12.conv.6 tensor(0.6277)
features.13.conv.0 tensor(0.1102)
features.13.conv.3 tensor(0.1682)
features.13.conv.6 tensor(0.3484)
features.14.conv.0 tensor(0.9549)
features.14.conv.3 tensor(0.1720)
features.14.conv.6 tensor(0.9576)
features.15.conv.0 tensor(0.9788)
features.15.conv.3 tensor(0.1221)
features.15.conv.6 tensor(0.9913)
features.16.conv.0 tensor(0.1385)
features.16.conv.3 tensor(0.1721)
features.16.conv.6 tensor(0.4888)
conv.0 tensor(0.6482)
tensor(1231613.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [26][   20/  196]   Loss 0.292384   Top1 89.882812   Top5 98.613281   BatchTime 0.406036   LR 0.000228
INFO - Training [26][   40/  196]   Loss 0.291630   Top1 89.765625   Top5 98.730469   BatchTime 0.371066   LR 0.000216
INFO - Training [26][   60/  196]   Loss 0.287724   Top1 89.759115   Top5 98.776042   BatchTime 0.343322   LR 0.000205
INFO - Training [26][   80/  196]   Loss 0.283118   Top1 89.921875   Top5 98.935547   BatchTime 0.339027   LR 0.000194
INFO - Training [26][  100/  196]   Loss 0.276784   Top1 90.160156   Top5 98.980469   BatchTime 0.339824   LR 0.000183
INFO - Training [26][  120/  196]   Loss 0.273328   Top1 90.305990   Top5 99.036458   BatchTime 0.341630   LR 0.000173
INFO - Training [26][  140/  196]   Loss 0.270600   Top1 90.440848   Top5 99.079241   BatchTime 0.342323   LR 0.000163
INFO - Training [26][  160/  196]   Loss 0.272211   Top1 90.368652   Top5 99.089355   BatchTime 0.341636   LR 0.000153
INFO - Training [26][  180/  196]   Loss 0.271827   Top1 90.434028   Top5 99.090712   BatchTime 0.342777   LR 0.000144
INFO - ==> Top1: 90.446    Top5: 99.086    Loss: 0.271
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.291800   Top1 91.074219   Top5 99.589844   BatchTime 0.137175
features.0.conv.0 tensor(0.4201)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0221)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0293)
features.3.conv.6 tensor(0.0319)
features.4.conv.0 tensor(0.0309)
features.4.conv.3 tensor(0.0799)
features.4.conv.6 tensor(0.2332)
features.5.conv.0 tensor(0.0439)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2822)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0535)
features.7.conv.0 tensor(0.0597)
features.7.conv.3 tensor(0.1256)
features.7.conv.6 tensor(0.2889)
features.8.conv.0 tensor(0.0801)
features.8.conv.3 tensor(0.1270)
features.8.conv.6 tensor(0.2963)
features.9.conv.0 tensor(0.0942)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.3392)
features.10.conv.0 tensor(0.0453)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.2571)
features.11.conv.0 tensor(0.2113)
features.11.conv.3 tensor(0.1441)
features.11.conv.6 tensor(0.6162)
features.12.conv.0 tensor(0.4785)
features.12.conv.3 tensor(0.2118)
features.12.conv.6 tensor(0.6275)
features.13.conv.0 tensor(0.1084)
features.13.conv.3 tensor(0.1711)
features.13.conv.6 tensor(0.3482)
features.14.conv.0 tensor(0.9545)
features.14.conv.3 tensor(0.1721)
features.14.conv.6 tensor(0.9575)
features.15.conv.0 tensor(0.9787)
features.15.conv.3 tensor(0.1218)
features.15.conv.6 tensor(0.9910)
features.16.conv.0 tensor(0.1375)
features.16.conv.3 tensor(0.1726)
features.16.conv.6 tensor(0.4877)
conv.0 tensor(0.6477)
tensor(1230707.) 2188896.0
INFO - Validation [26][   40/   40]   Loss 0.273662   Top1 91.450000   Top5 99.710000   BatchTime 0.095951
INFO - ==> Top1: 91.450    Top5: 99.710    Loss: 0.274
INFO - ==> Sparsity : 0.562
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 91.450   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [27][   20/  196]   Loss 0.292989   Top1 89.589844   Top5 98.476562   BatchTime 0.423825   LR 0.000128
INFO - Training [27][   40/  196]   Loss 0.286183   Top1 89.960938   Top5 98.662109   BatchTime 0.380287   LR 0.000119
INFO - Training [27][   60/  196]   Loss 0.281381   Top1 90.169271   Top5 98.802083   BatchTime 0.362270   LR 0.000111
INFO - Training [27][   80/  196]   Loss 0.278852   Top1 90.336914   Top5 98.916016   BatchTime 0.335528   LR 0.000102
INFO - Training [27][  100/  196]   Loss 0.271268   Top1 90.585938   Top5 98.992188   BatchTime 0.337324   LR 0.000095
INFO - Training [27][  120/  196]   Loss 0.267889   Top1 90.686849   Top5 99.078776   BatchTime 0.337601   LR 0.000087
INFO - Training [27][  140/  196]   Loss 0.267611   Top1 90.714286   Top5 99.123884   BatchTime 0.338453   LR 0.000080
INFO - Training [27][  160/  196]   Loss 0.270333   Top1 90.642090   Top5 99.074707   BatchTime 0.337865   LR 0.000073
INFO - Training [27][  180/  196]   Loss 0.270912   Top1 90.594618   Top5 99.055990   BatchTime 0.338939   LR 0.000066
INFO - ==> Top1: 90.654    Top5: 99.058    Loss: 0.270
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 0.301091   Top1 90.800781   Top5 99.589844   BatchTime 0.139952
INFO - Validation [27][   40/   40]   Loss 0.283924   Top1 91.110000   Top5 99.680000   BatchTime 0.098421
INFO - ==> Top1: 91.110    Top5: 99.680    Loss: 0.284
INFO - ==> Sparsity : 0.562
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 91.450   Top5: 99.710]
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0221)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0150)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0293)
features.4.conv.0 tensor(0.0308)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2332)
features.5.conv.0 tensor(0.0438)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2821)
features.6.conv.0 tensor(0.0202)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0540)
features.7.conv.0 tensor(0.0589)
features.7.conv.3 tensor(0.1250)
features.7.conv.6 tensor(0.2887)
features.8.conv.0 tensor(0.0798)
features.8.conv.3 tensor(0.1279)
features.8.conv.6 tensor(0.2964)
features.9.conv.0 tensor(0.0939)
features.9.conv.3 tensor(0.1597)
features.9.conv.6 tensor(0.3392)
features.10.conv.0 tensor(0.0448)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.2571)
features.11.conv.0 tensor(0.2115)
features.11.conv.3 tensor(0.1451)
features.11.conv.6 tensor(0.6162)
features.12.conv.0 tensor(0.4783)
features.12.conv.3 tensor(0.2118)
features.12.conv.6 tensor(0.6274)
features.13.conv.0 tensor(0.1085)
features.13.conv.3 tensor(0.1719)
features.13.conv.6 tensor(0.3483)
features.14.conv.0 tensor(0.9546)
features.14.conv.3 tensor(0.1715)
features.14.conv.6 tensor(0.9587)
features.15.conv.0 tensor(0.9788)
features.15.conv.3 tensor(0.1216)
features.15.conv.6 tensor(0.9914)
features.16.conv.0 tensor(0.1375)
features.16.conv.3 tensor(0.1726)
features.16.conv.6 tensor(0.4875)
conv.0 tensor(0.6478)
tensor(1230909.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [28][   20/  196]   Loss 0.283556   Top1 90.058594   Top5 98.691406   BatchTime 0.428244   LR 0.000055
INFO - Training [28][   40/  196]   Loss 0.293876   Top1 89.746094   Top5 98.857422   BatchTime 0.382423   LR 0.000050
INFO - Training [28][   60/  196]   Loss 0.286722   Top1 90.019531   Top5 98.938802   BatchTime 0.369219   LR 0.000044
INFO - Training [28][   80/  196]   Loss 0.277898   Top1 90.341797   Top5 99.067383   BatchTime 0.353728   LR 0.000039
INFO - Training [28][  100/  196]   Loss 0.273531   Top1 90.468750   Top5 99.093750   BatchTime 0.337146   LR 0.000034
INFO - Training [28][  120/  196]   Loss 0.268496   Top1 90.608724   Top5 99.134115   BatchTime 0.344292   LR 0.000030
INFO - Training [28][  140/  196]   Loss 0.266618   Top1 90.705915   Top5 99.174107   BatchTime 0.344231   LR 0.000026
INFO - Training [28][  160/  196]   Loss 0.269478   Top1 90.605469   Top5 99.167480   BatchTime 0.343495   LR 0.000022
INFO - Training [28][  180/  196]   Loss 0.270896   Top1 90.555556   Top5 99.138455   BatchTime 0.343464   LR 0.000018
INFO - ==> Top1: 90.578    Top5: 99.114    Loss: 0.270
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [28][   20/   40]   Loss 0.286300   Top1 90.996094   Top5 99.609375   BatchTime 0.145009
INFO - Validation [28][   40/   40]   Loss 0.268916   Top1 91.370000   Top5 99.710000   BatchTime 0.102348
INFO - ==> Top1: 91.370    Top5: 99.710    Loss: 0.269
INFO - ==> Sparsity : 0.562
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 91.450   Top5: 99.710]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4340)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0148)
features.3.conv.3 tensor(0.0332)
features.3.conv.6 tensor(0.0302)
features.4.conv.0 tensor(0.0308)
features.4.conv.3 tensor(0.0787)
features.4.conv.6 tensor(0.2332)
features.5.conv.0 tensor(0.0431)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.2821)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0534)
features.7.conv.0 tensor(0.0593)
features.7.conv.3 tensor(0.1244)
features.7.conv.6 tensor(0.2887)
features.8.conv.0 tensor(0.0798)
features.8.conv.3 tensor(0.1291)
features.8.conv.6 tensor(0.2964)
features.9.conv.0 tensor(0.0939)
features.9.conv.3 tensor(0.1591)
features.9.conv.6 tensor(0.3393)
features.10.conv.0 tensor(0.0451)
features.10.conv.3 tensor(0.0987)
features.10.conv.6 tensor(0.2571)
features.11.conv.0 tensor(0.2119)
features.11.conv.3 tensor(0.1437)
features.11.conv.6 tensor(0.6161)
features.12.conv.0 tensor(0.4784)
features.12.conv.3 tensor(0.2101)
features.12.conv.6 tensor(0.6275)
features.13.conv.0 tensor(0.1083)
features.13.conv.3 tensor(0.1717)
features.13.conv.6 tensor(0.3483)
features.14.conv.0 tensor(0.9546)
features.14.conv.3 tensor(0.1712)
features.14.conv.6 tensor(0.9582)
features.15.conv.0 tensor(0.9788)
features.15.conv.3 tensor(0.1216)
features.15.conv.6 tensor(0.9910)
features.16.conv.0 tensor(0.1377)
features.16.conv.3 tensor(0.1736)
features.16.conv.6 tensor(0.4876)
conv.0 tensor(0.6478)
tensor(1230842.) 2188896.0
INFO - Training [29][   20/  196]   Loss 0.268942   Top1 90.273438   Top5 98.671875   BatchTime 0.412908   LR 0.000013
INFO - Training [29][   40/  196]   Loss 0.285278   Top1 90.146484   Top5 98.750000   BatchTime 0.372738   LR 0.000010
INFO - Training [29][   60/  196]   Loss 0.282979   Top1 90.221354   Top5 98.880208   BatchTime 0.361256   LR 0.000008
INFO - Training [29][   80/  196]   Loss 0.279742   Top1 90.405273   Top5 99.013672   BatchTime 0.357017   LR 0.000005
INFO - Training [29][  100/  196]   Loss 0.271868   Top1 90.582031   Top5 99.062500   BatchTime 0.341251   LR 0.000004
INFO - Training [29][  120/  196]   Loss 0.268479   Top1 90.751953   Top5 99.111328   BatchTime 0.332655   LR 0.000002
INFO - Training [29][  140/  196]   Loss 0.268233   Top1 90.784040   Top5 99.165737   BatchTime 0.337497   LR 0.000001
INFO - Training [29][  160/  196]   Loss 0.271500   Top1 90.681152   Top5 99.130859   BatchTime 0.338137   LR 0.000001
INFO - Training [29][  180/  196]   Loss 0.272338   Top1 90.603299   Top5 99.092882   BatchTime 0.337773   LR 0.000000
INFO - ==> Top1: 90.644    Top5: 99.074    Loss: 0.272
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 0.289948   Top1 91.210938   Top5 99.628906   BatchTime 0.135893
INFO - Validation [29][   40/   40]   Loss 0.273547   Top1 91.560000   Top5 99.720000   BatchTime 0.095258
INFO - ==> Top1: 91.560    Top5: 99.720    Loss: 0.274
INFO - ==> Sparsity : 0.562
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2363)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0472)
features.3.conv.0 tensor(0.0142)
features.3.conv.3 tensor(0.0324)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.0793)
features.4.conv.6 tensor(0.2332)
features.5.conv.0 tensor(0.0435)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.2821)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0534)
features.7.conv.0 tensor(0.0595)
features.7.conv.3 tensor(0.1241)
features.7.conv.6 tensor(0.2887)
features.8.conv.0 tensor(0.0798)
features.8.conv.3 tensor(0.1279)
features.8.conv.6 tensor(0.2963)
features.9.conv.0 tensor(0.0938)
features.9.conv.3 tensor(0.1597)
features.9.conv.6 tensor(0.3392)
features.10.conv.0 tensor(0.0450)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.2571)
features.11.conv.0 tensor(0.2114)
features.11.conv.3 tensor(0.1443)
features.11.conv.6 tensor(0.6162)
features.12.conv.0 tensor(0.4785)
features.12.conv.3 tensor(0.2103)
features.12.conv.6 tensor(0.6276)
features.13.conv.0 tensor(0.1084)
features.13.conv.3 tensor(0.1709)
features.13.conv.6 tensor(0.3482)
features.14.conv.0 tensor(0.9549)
features.14.conv.3 tensor(0.1714)
features.14.conv.6 tensor(0.9582)
features.15.conv.0 tensor(0.9788)
features.15.conv.3 tensor(0.1213)
features.15.conv.6 tensor(0.9908)
features.16.conv.0 tensor(0.1376)
features.16.conv.3 tensor(0.1728)
features.16.conv.6 tensor(0.4876)
conv.0 tensor(0.6477)
tensor(1230789.) 2188896.0
INFO - Training [30][   20/  196]   Loss 0.300730   Top1 89.082031   Top5 98.476562   BatchTime 0.431912   LR 0.001250
INFO - Training [30][   40/  196]   Loss 0.303096   Top1 89.140625   Top5 98.662109   BatchTime 0.384164   LR 0.001250
INFO - Training [30][   60/  196]   Loss 0.300888   Top1 89.270833   Top5 98.756510   BatchTime 0.379915   LR 0.001250
INFO - Training [30][   80/  196]   Loss 0.302952   Top1 89.233398   Top5 98.891602   BatchTime 0.370280   LR 0.001250
INFO - Training [30][  100/  196]   Loss 0.303044   Top1 89.285156   Top5 98.910156   BatchTime 0.362947   LR 0.001250
INFO - Training [30][  120/  196]   Loss 0.299147   Top1 89.492188   Top5 98.984375   BatchTime 0.347522   LR 0.001249
INFO - Training [30][  140/  196]   Loss 0.295874   Top1 89.598214   Top5 99.068080   BatchTime 0.342788   LR 0.001249
INFO - Training [30][  160/  196]   Loss 0.301196   Top1 89.421387   Top5 99.025879   BatchTime 0.348152   LR 0.001249
INFO - Training [30][  180/  196]   Loss 0.300021   Top1 89.470486   Top5 99.001736   BatchTime 0.347896   LR 0.001248
INFO - ==> Top1: 89.462    Top5: 98.998    Loss: 0.300
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [30][   20/   40]   Loss 0.311933   Top1 90.449219   Top5 99.589844   BatchTime 0.137111
INFO - Validation [30][   40/   40]   Loss 0.298788   Top1 90.590000   Top5 99.640000   BatchTime 0.097418
INFO - ==> Top1: 90.590    Top5: 99.640    Loss: 0.299
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.2383)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0136)
features.2.conv.3 tensor(0.0424)
features.2.conv.6 tensor(0.0466)
features.3.conv.0 tensor(0.0156)
features.3.conv.3 tensor(0.0309)
features.3.conv.6 tensor(0.0254)
features.4.conv.0 tensor(0.0275)
features.4.conv.3 tensor(0.0804)
features.4.conv.6 tensor(0.2354)
features.5.conv.0 tensor(0.0420)
features.5.conv.3 tensor(0.0723)
features.5.conv.6 tensor(0.2837)
features.6.conv.0 tensor(0.0207)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0523)
features.7.conv.0 tensor(0.0576)
features.7.conv.3 tensor(0.1264)
features.7.conv.6 tensor(0.2904)
features.8.conv.0 tensor(0.0802)
features.8.conv.3 tensor(0.1262)
features.8.conv.6 tensor(0.2981)
features.9.conv.0 tensor(0.0920)
features.9.conv.3 tensor(0.1606)
features.9.conv.6 tensor(0.3401)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.2593)
features.11.conv.0 tensor(0.2086)
features.11.conv.3 tensor(0.1491)
features.11.conv.6 tensor(0.6185)
features.12.conv.0 tensor(0.4797)
features.12.conv.3 tensor(0.2105)
features.12.conv.6 tensor(0.6288)
features.13.conv.0 tensor(0.1134)
features.13.conv.3 tensor(0.1686)
features.13.conv.6 tensor(0.3501)
features.14.conv.0 tensor(0.9558)
features.14.conv.3 tensor(0.1727)
features.14.conv.6 tensor(0.9575)
features.15.conv.0 tensor(0.9789)
features.15.conv.3 tensor(0.1209)
features.15.conv.6 tensor(0.9911)
features.16.conv.0 tensor(0.1391)
features.16.conv.3 tensor(0.1718)
features.16.conv.6 tensor(0.4902)
conv.0 tensor(0.6494)
tensor(1233116.) 2188896.0
INFO - Training [31][   20/  196]   Loss 0.310403   Top1 89.101562   Top5 98.457031   BatchTime 0.422257   LR 0.001248
INFO - Training [31][   40/  196]   Loss 0.307181   Top1 89.072266   Top5 98.779297   BatchTime 0.379976   LR 0.001247
INFO - Training [31][   60/  196]   Loss 0.307749   Top1 88.984375   Top5 98.815104   BatchTime 0.371664   LR 0.001247
INFO - Training [31][   80/  196]   Loss 0.312078   Top1 88.793945   Top5 98.935547   BatchTime 0.364334   LR 0.001246
INFO - Training [31][  100/  196]   Loss 0.306623   Top1 89.070312   Top5 98.976562   BatchTime 0.361494   LR 0.001246
INFO - Training [31][  120/  196]   Loss 0.302374   Top1 89.231771   Top5 99.042969   BatchTime 0.354127   LR 0.001245
INFO - Training [31][  140/  196]   Loss 0.303210   Top1 89.268973   Top5 99.065290   BatchTime 0.339285   LR 0.001244
INFO - Training [31][  160/  196]   Loss 0.306545   Top1 89.165039   Top5 99.028320   BatchTime 0.330222   LR 0.001244
INFO - Training [31][  180/  196]   Loss 0.307750   Top1 89.114583   Top5 98.999566   BatchTime 0.333308   LR 0.001243
INFO - ==> Top1: 89.170    Top5: 98.984    Loss: 0.306
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [31][   20/   40]   Loss 0.294555   Top1 90.976562   Top5 99.687500   BatchTime 0.137075
INFO - Validation [31][   40/   40]   Loss 0.290809   Top1 90.740000   Top5 99.740000   BatchTime 0.095113
INFO - ==> Top1: 90.740    Top5: 99.740    Loss: 0.291
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4201)
features.0.conv.3 tensor(0.2422)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0579)
features.1.conv.6 tensor(0.0278)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0471)
features.2.conv.6 tensor(0.0454)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0285)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0273)
features.4.conv.3 tensor(0.0735)
features.4.conv.6 tensor(0.2375)
features.5.conv.0 tensor(0.0439)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.2858)
features.6.conv.0 tensor(0.0212)
features.6.conv.3 tensor(0.0365)
features.6.conv.6 tensor(0.0517)
features.7.conv.0 tensor(0.0561)
features.7.conv.3 tensor(0.1209)
features.7.conv.6 tensor(0.2945)
features.8.conv.0 tensor(0.0800)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.3000)
features.9.conv.0 tensor(0.0959)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.3424)
features.10.conv.0 tensor(0.0370)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.2612)
features.11.conv.0 tensor(0.1939)
features.11.conv.3 tensor(0.1453)
features.11.conv.6 tensor(0.6191)
features.12.conv.0 tensor(0.4756)
features.12.conv.3 tensor(0.2143)
features.12.conv.6 tensor(0.6299)
features.13.conv.0 tensor(0.1148)
features.13.conv.3 tensor(0.1665)
features.13.conv.6 tensor(0.3527)
features.14.conv.0 tensor(0.9579)
features.14.conv.3 tensor(0.1742)
features.14.conv.6 tensor(0.9579)
features.15.conv.0 tensor(0.9796)
features.15.conv.3 tensor(0.1222)
features.15.conv.6 tensor(0.9909)
features.16.conv.0 tensor(0.1338)
features.16.conv.3 tensor(0.1734)
features.16.conv.6 tensor(0.4893)
conv.0 tensor(0.6496)
tensor(1232238.) 2188896.0
INFO - Training [32][   20/  196]   Loss 0.309225   Top1 88.593750   Top5 98.574219   BatchTime 0.420445   LR 0.001242
INFO - Training [32][   40/  196]   Loss 0.306908   Top1 89.150391   Top5 98.691406   BatchTime 0.380037   LR 0.001241
INFO - Training [32][   60/  196]   Loss 0.307995   Top1 89.186198   Top5 98.710938   BatchTime 0.365429   LR 0.001240
INFO - Training [32][   80/  196]   Loss 0.306934   Top1 89.311523   Top5 98.828125   BatchTime 0.365504   LR 0.001239
INFO - Training [32][  100/  196]   Loss 0.304561   Top1 89.441406   Top5 98.867188   BatchTime 0.359449   LR 0.001238
INFO - Training [32][  120/  196]   Loss 0.296548   Top1 89.684245   Top5 98.961589   BatchTime 0.359350   LR 0.001237
INFO - Training [32][  140/  196]   Loss 0.296938   Top1 89.729353   Top5 99.029018   BatchTime 0.357291   LR 0.001236
INFO - Training [32][  160/  196]   Loss 0.298584   Top1 89.648438   Top5 99.023438   BatchTime 0.350715   LR 0.001235
INFO - Training [32][  180/  196]   Loss 0.300494   Top1 89.555122   Top5 98.956163   BatchTime 0.341989   LR 0.001234
INFO - ==> Top1: 89.552    Top5: 98.934    Loss: 0.300
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.324642   Top1 90.156250   Top5 99.628906   BatchTime 0.164176
INFO - Validation [32][   40/   40]   Loss 0.316818   Top1 90.160000   Top5 99.680000   BatchTime 0.109473
INFO - ==> Top1: 90.160    Top5: 99.680    Loss: 0.317
INFO - ==> Sparsity : 0.563
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2422)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0330)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0417)
features.2.conv.6 tensor(0.0454)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0243)
features.4.conv.0 tensor(0.0308)
features.4.conv.3 tensor(0.0741)
features.4.conv.6 tensor(0.2388)
features.5.conv.0 tensor(0.0433)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2856)
features.6.conv.0 tensor(0.0208)
features.6.conv.3 tensor(0.0353)
features.6.conv.6 tensor(0.0523)
features.7.conv.0 tensor(0.0540)
features.7.conv.3 tensor(0.1183)
features.7.conv.6 tensor(0.2970)
features.8.conv.0 tensor(0.0800)
features.8.conv.3 tensor(0.1299)
features.8.conv.6 tensor(0.3015)
features.9.conv.0 tensor(0.0913)
features.9.conv.3 tensor(0.1589)
features.9.conv.6 tensor(0.3434)
features.10.conv.0 tensor(0.0406)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.2632)
features.11.conv.0 tensor(0.1869)
features.11.conv.3 tensor(0.1449)
features.11.conv.6 tensor(0.6202)
features.12.conv.0 tensor(0.4758)
features.12.conv.3 tensor(0.2112)
features.12.conv.6 tensor(0.6296)
features.13.conv.0 tensor(0.1153)
features.13.conv.3 tensor(0.1605)
features.13.conv.6 tensor(0.3543)
features.14.conv.0 tensor(0.9648)
features.14.conv.3 tensor(0.1802)
features.14.conv.6 tensor(0.9321)
features.15.conv.0 tensor(0.9796)
features.15.conv.3 tensor(0.1237)
features.15.conv.6 tensor(0.9912)
features.16.conv.0 tensor(0.1370)
features.16.conv.3 tensor(0.1748)
features.16.conv.6 tensor(0.4930)
conv.0 tensor(0.6510)
tensor(1231543.) 2188896.0
INFO - Training [33][   20/  196]   Loss 0.324046   Top1 88.378906   Top5 98.613281   BatchTime 0.418644   LR 0.001232
INFO - Training [33][   40/  196]   Loss 0.324977   Top1 88.408203   Top5 98.691406   BatchTime 0.376082   LR 0.001230
INFO - Training [33][   60/  196]   Loss 0.317161   Top1 88.723958   Top5 98.756510   BatchTime 0.364733   LR 0.001229
INFO - Training [33][   80/  196]   Loss 0.311118   Top1 88.984375   Top5 98.857422   BatchTime 0.356902   LR 0.001228
INFO - Training [33][  100/  196]   Loss 0.302390   Top1 89.250000   Top5 98.941406   BatchTime 0.351416   LR 0.001226
INFO - Training [33][  120/  196]   Loss 0.301539   Top1 89.329427   Top5 98.987630   BatchTime 0.348485   LR 0.001225
INFO - Training [33][  140/  196]   Loss 0.300098   Top1 89.425223   Top5 99.029018   BatchTime 0.347855   LR 0.001224
INFO - Training [33][  160/  196]   Loss 0.301598   Top1 89.362793   Top5 99.023438   BatchTime 0.347409   LR 0.001222
INFO - Training [33][  180/  196]   Loss 0.303874   Top1 89.318576   Top5 98.975694   BatchTime 0.347206   LR 0.001221
INFO - ==> Top1: 89.286    Top5: 98.964    Loss: 0.304
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 0.377517   Top1 89.218750   Top5 99.531250   BatchTime 0.231588
INFO - Validation [33][   40/   40]   Loss 0.372696   Top1 88.830000   Top5 99.600000   BatchTime 0.186714
INFO - ==> Top1: 88.830    Top5: 99.600    Loss: 0.373
INFO - ==> Sparsity : 0.564
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4201)
features.0.conv.3 tensor(0.2422)
features.1.conv.0 tensor(0.0215)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0347)
features.2.conv.0 tensor(0.0182)
features.2.conv.3 tensor(0.0432)
features.2.conv.6 tensor(0.0434)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0295)
features.4.conv.0 tensor(0.0306)
features.4.conv.3 tensor(0.0660)
features.4.conv.6 tensor(0.2407)
features.5.conv.0 tensor(0.0412)
features.5.conv.3 tensor(0.0648)
features.5.conv.6 tensor(0.2873)
features.6.conv.0 tensor(0.0210)
features.6.conv.3 tensor(0.0324)
features.6.conv.6 tensor(0.0532)
features.7.conv.0 tensor(0.0508)
features.7.conv.3 tensor(0.1198)
features.7.conv.6 tensor(0.2965)
features.8.conv.0 tensor(0.0803)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.3033)
features.9.conv.0 tensor(0.0914)
features.9.conv.3 tensor(0.1577)
features.9.conv.6 tensor(0.3451)
features.10.conv.0 tensor(0.0407)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.2652)
features.11.conv.0 tensor(0.1806)
features.11.conv.3 tensor(0.1472)
features.11.conv.6 tensor(0.6211)
features.12.conv.0 tensor(0.4791)
features.12.conv.3 tensor(0.2106)
features.12.conv.6 tensor(0.6298)
features.13.conv.0 tensor(0.1121)
features.13.conv.3 tensor(0.1628)
features.13.conv.6 tensor(0.3563)
features.14.conv.0 tensor(0.9637)
features.14.conv.3 tensor(0.1853)
features.14.conv.6 tensor(0.9373)
features.15.conv.0 tensor(0.9796)
features.15.conv.3 tensor(0.1226)
features.15.conv.6 tensor(0.9912)
features.16.conv.0 tensor(0.1436)
features.16.conv.3 tensor(0.1715)
features.16.conv.6 tensor(0.4939)
conv.0 tensor(0.6536)
tensor(1234652.) 2188896.0
INFO - Training [34][   20/  196]   Loss 0.320275   Top1 88.945312   Top5 98.378906   BatchTime 0.451661   LR 0.001218
INFO - Training [34][   40/  196]   Loss 0.315969   Top1 88.925781   Top5 98.681641   BatchTime 0.393698   LR 0.001216
INFO - Training [34][   60/  196]   Loss 0.317134   Top1 89.055990   Top5 98.717448   BatchTime 0.373739   LR 0.001215
INFO - Training [34][   80/  196]   Loss 0.313496   Top1 89.238281   Top5 98.867188   BatchTime 0.366372   LR 0.001213
INFO - Training [34][  100/  196]   Loss 0.308710   Top1 89.335938   Top5 98.917969   BatchTime 0.365690   LR 0.001211
INFO - Training [34][  120/  196]   Loss 0.300968   Top1 89.557292   Top5 99.003906   BatchTime 0.359809   LR 0.001209
INFO - Training [34][  140/  196]   Loss 0.299467   Top1 89.598214   Top5 99.082031   BatchTime 0.354926   LR 0.001208
INFO - Training [34][  160/  196]   Loss 0.301364   Top1 89.521484   Top5 99.079590   BatchTime 0.353213   LR 0.001206
INFO - Training [34][  180/  196]   Loss 0.301484   Top1 89.498698   Top5 99.023438   BatchTime 0.351994   LR 0.001204
INFO - ==> Top1: 89.548    Top5: 99.028    Loss: 0.299
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [34][   20/   40]   Loss 0.332281   Top1 90.019531   Top5 99.570312   BatchTime 0.135475
INFO - Validation [34][   40/   40]   Loss 0.323838   Top1 89.860000   Top5 99.690000   BatchTime 0.093617
INFO - ==> Top1: 89.860    Top5: 99.690    Loss: 0.324
INFO - ==> Sparsity : 0.565
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2441)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0494)
features.2.conv.6 tensor(0.0411)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0247)
features.3.conv.6 tensor(0.0273)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.0723)
features.4.conv.6 tensor(0.2428)
features.5.conv.0 tensor(0.0396)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.2884)
features.6.conv.0 tensor(0.0194)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0517)
features.7.conv.0 tensor(0.0420)
features.7.conv.3 tensor(0.1212)
features.7.conv.6 tensor(0.2983)
features.8.conv.0 tensor(0.0761)
features.8.conv.3 tensor(0.1288)
features.8.conv.6 tensor(0.3045)
features.9.conv.0 tensor(0.0920)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.3463)
features.10.conv.0 tensor(0.0419)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.2675)
features.11.conv.0 tensor(0.1799)
features.11.conv.3 tensor(0.1433)
features.11.conv.6 tensor(0.6200)
features.12.conv.0 tensor(0.4812)
features.12.conv.3 tensor(0.2093)
features.12.conv.6 tensor(0.6315)
features.13.conv.0 tensor(0.1103)
features.13.conv.3 tensor(0.1591)
features.13.conv.6 tensor(0.3587)
features.14.conv.0 tensor(0.9640)
features.14.conv.3 tensor(0.1855)
features.14.conv.6 tensor(0.9365)
features.15.conv.0 tensor(0.9795)
features.15.conv.3 tensor(0.1244)
features.15.conv.6 tensor(0.9920)
features.16.conv.0 tensor(0.1481)
features.16.conv.3 tensor(0.1742)
features.16.conv.6 tensor(0.4941)
conv.0 tensor(0.6584)
tensor(1237426.) 2188896.0
INFO - Training [35][   20/  196]   Loss 0.323783   Top1 88.671875   Top5 98.457031   BatchTime 0.392769   LR 0.001201
INFO - Training [35][   40/  196]   Loss 0.323800   Top1 88.574219   Top5 98.652344   BatchTime 0.361623   LR 0.001199
INFO - Training [35][   60/  196]   Loss 0.314253   Top1 88.867188   Top5 98.743490   BatchTime 0.355438   LR 0.001197
INFO - Training [35][   80/  196]   Loss 0.311210   Top1 89.062500   Top5 98.881836   BatchTime 0.350505   LR 0.001195
INFO - Training [35][  100/  196]   Loss 0.305121   Top1 89.296875   Top5 98.945312   BatchTime 0.346966   LR 0.001192
INFO - Training [35][  120/  196]   Loss 0.297919   Top1 89.531250   Top5 98.994141   BatchTime 0.344642   LR 0.001190
INFO - Training [35][  140/  196]   Loss 0.299266   Top1 89.511719   Top5 99.062500   BatchTime 0.344013   LR 0.001188
INFO - Training [35][  160/  196]   Loss 0.301468   Top1 89.477539   Top5 99.060059   BatchTime 0.345225   LR 0.001186
INFO - Training [35][  180/  196]   Loss 0.300605   Top1 89.494358   Top5 99.036458   BatchTime 0.344582   LR 0.001184
INFO - ==> Top1: 89.422    Top5: 99.022    Loss: 0.302
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.372785   Top1 89.570312   Top5 99.511719   BatchTime 0.136414
INFO - Validation [35][   40/   40]   Loss 0.361641   Top1 89.400000   Top5 99.610000   BatchTime 0.096659
INFO - ==> Top1: 89.400    Top5: 99.610    Loss: 0.362
INFO - ==> Sparsity : 0.566
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4201)
features.0.conv.3 tensor(0.2461)
features.1.conv.0 tensor(0.0189)
features.1.conv.3 tensor(0.0567)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0142)
features.2.conv.3 tensor(0.0409)
features.2.conv.6 tensor(0.0414)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0216)
features.3.conv.6 tensor(0.0282)
features.4.conv.0 tensor(0.0267)
features.4.conv.3 tensor(0.0712)
features.4.conv.6 tensor(0.2438)
features.5.conv.0 tensor(0.0373)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.2886)
features.6.conv.0 tensor(0.0218)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0512)
features.7.conv.0 tensor(0.0529)
features.7.conv.3 tensor(0.1198)
features.7.conv.6 tensor(0.3000)
features.8.conv.0 tensor(0.0741)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.3062)
features.9.conv.0 tensor(0.0919)
features.9.conv.3 tensor(0.1591)
features.9.conv.6 tensor(0.3484)
features.10.conv.0 tensor(0.0436)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.2693)
features.11.conv.0 tensor(0.1821)
features.11.conv.3 tensor(0.1447)
features.11.conv.6 tensor(0.6220)
features.12.conv.0 tensor(0.4814)
features.12.conv.3 tensor(0.2105)
features.12.conv.6 tensor(0.6325)
features.13.conv.0 tensor(0.1064)
features.13.conv.3 tensor(0.1570)
features.13.conv.6 tensor(0.3594)
features.14.conv.0 tensor(0.9628)
features.14.conv.3 tensor(0.1859)
features.14.conv.6 tensor(0.9413)
features.15.conv.0 tensor(0.9794)
features.15.conv.3 tensor(0.1229)
features.15.conv.6 tensor(0.9915)
features.16.conv.0 tensor(0.1487)
features.16.conv.3 tensor(0.1734)
features.16.conv.6 tensor(0.4957)
conv.0 tensor(0.6587)
tensor(1239200.) 2188896.0
INFO - Training [36][   20/  196]   Loss 0.306138   Top1 89.433594   Top5 98.671875   BatchTime 0.353151   LR 0.001180
INFO - Training [36][   40/  196]   Loss 0.308310   Top1 89.316406   Top5 98.730469   BatchTime 0.315303   LR 0.001177
INFO - Training [36][   60/  196]   Loss 0.308000   Top1 89.355469   Top5 98.867188   BatchTime 0.296080   LR 0.001175
INFO - Training [36][   80/  196]   Loss 0.305209   Top1 89.389648   Top5 98.950195   BatchTime 0.307709   LR 0.001173
INFO - Training [36][  100/  196]   Loss 0.301750   Top1 89.554688   Top5 98.949219   BatchTime 0.313127   LR 0.001170
INFO - Training [36][  120/  196]   Loss 0.295102   Top1 89.798177   Top5 99.016927   BatchTime 0.317145   LR 0.001168
INFO - Training [36][  140/  196]   Loss 0.294277   Top1 89.818638   Top5 99.073661   BatchTime 0.322698   LR 0.001165
INFO - Training [36][  160/  196]   Loss 0.295866   Top1 89.733887   Top5 99.057617   BatchTime 0.326070   LR 0.001163
INFO - Training [36][  180/  196]   Loss 0.297836   Top1 89.665799   Top5 99.025608   BatchTime 0.327772   LR 0.001160
INFO - ==> Top1: 89.748    Top5: 99.018    Loss: 0.296
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [36][   20/   40]   Loss 0.326282   Top1 90.175781   Top5 99.570312   BatchTime 0.150843
INFO - Validation [36][   40/   40]   Loss 0.323947   Top1 90.330000   Top5 99.670000   BatchTime 0.103579
INFO - ==> Top1: 90.330    Top5: 99.670    Loss: 0.324
INFO - ==> Sparsity : 0.566
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.2461)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0295)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0434)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0208)
features.3.conv.6 tensor(0.0328)
features.4.conv.0 tensor(0.0272)
features.4.conv.3 tensor(0.0706)
features.4.conv.6 tensor(0.2453)
features.5.conv.0 tensor(0.0356)
features.5.conv.3 tensor(0.0718)
features.5.conv.6 tensor(0.2891)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0272)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0540)
features.7.conv.3 tensor(0.1195)
features.7.conv.6 tensor(0.3027)
features.8.conv.0 tensor(0.0818)
features.8.conv.3 tensor(0.1262)
features.8.conv.6 tensor(0.3071)
features.9.conv.0 tensor(0.0912)
features.9.conv.3 tensor(0.1565)
features.9.conv.6 tensor(0.3491)
features.10.conv.0 tensor(0.0441)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.2712)
features.11.conv.0 tensor(0.1829)
features.11.conv.3 tensor(0.1453)
features.11.conv.6 tensor(0.6214)
features.12.conv.0 tensor(0.4813)
features.12.conv.3 tensor(0.2101)
features.12.conv.6 tensor(0.6325)
features.13.conv.0 tensor(0.1084)
features.13.conv.3 tensor(0.1601)
features.13.conv.6 tensor(0.3600)
features.14.conv.0 tensor(0.9646)
features.14.conv.3 tensor(0.1882)
features.14.conv.6 tensor(0.9411)
features.15.conv.0 tensor(0.9791)
features.15.conv.3 tensor(0.1237)
features.15.conv.6 tensor(0.9909)
features.16.conv.0 tensor(0.1464)
features.16.conv.3 tensor(0.1728)
features.16.conv.6 tensor(0.4943)
conv.0 tensor(0.6591)
tensor(1239343.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [37][   20/  196]   Loss 0.317916   Top1 89.277344   Top5 98.457031   BatchTime 0.428783   LR 0.001155
INFO - Training [37][   40/  196]   Loss 0.314660   Top1 89.316406   Top5 98.632812   BatchTime 0.357874   LR 0.001153
INFO - Training [37][   60/  196]   Loss 0.306970   Top1 89.498698   Top5 98.750000   BatchTime 0.337328   LR 0.001150
INFO - Training [37][   80/  196]   Loss 0.303088   Top1 89.555664   Top5 98.891602   BatchTime 0.314075   LR 0.001147
INFO - Training [37][  100/  196]   Loss 0.298783   Top1 89.652344   Top5 98.972656   BatchTime 0.317568   LR 0.001144
INFO - Training [37][  120/  196]   Loss 0.292692   Top1 89.843750   Top5 99.026693   BatchTime 0.320362   LR 0.001142
INFO - Training [37][  140/  196]   Loss 0.292277   Top1 89.832589   Top5 99.073661   BatchTime 0.321907   LR 0.001139
INFO - Training [37][  160/  196]   Loss 0.294923   Top1 89.797363   Top5 99.072266   BatchTime 0.325471   LR 0.001136
INFO - Training [37][  180/  196]   Loss 0.297190   Top1 89.722222   Top5 99.027778   BatchTime 0.327223   LR 0.001133
INFO - ==> Top1: 89.720    Top5: 99.046    Loss: 0.296
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.324796   Top1 90.058594   Top5 99.609375   BatchTime 0.238224
INFO - Validation [37][   40/   40]   Loss 0.316744   Top1 90.130000   Top5 99.710000   BatchTime 0.147606
INFO - ==> Top1: 90.130    Top5: 99.710    Loss: 0.317
INFO - ==> Sparsity : 0.567
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.2461)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0286)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0424)
features.2.conv.6 tensor(0.0454)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0208)
features.3.conv.6 tensor(0.0293)
features.4.conv.0 tensor(0.0277)
features.4.conv.3 tensor(0.0671)
features.4.conv.6 tensor(0.2467)
features.5.conv.0 tensor(0.0400)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.2897)
features.6.conv.0 tensor(0.0221)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0520)
features.7.conv.0 tensor(0.0564)
features.7.conv.3 tensor(0.1215)
features.7.conv.6 tensor(0.3040)
features.8.conv.0 tensor(0.0818)
features.8.conv.3 tensor(0.1293)
features.8.conv.6 tensor(0.3080)
features.9.conv.0 tensor(0.0955)
features.9.conv.3 tensor(0.1594)
features.9.conv.6 tensor(0.3499)
features.10.conv.0 tensor(0.0449)
features.10.conv.3 tensor(0.1010)
features.10.conv.6 tensor(0.2731)
features.11.conv.0 tensor(0.1929)
features.11.conv.3 tensor(0.1420)
features.11.conv.6 tensor(0.6214)
features.12.conv.0 tensor(0.4817)
features.12.conv.3 tensor(0.2099)
features.12.conv.6 tensor(0.6326)
features.13.conv.0 tensor(0.1156)
features.13.conv.3 tensor(0.1617)
features.13.conv.6 tensor(0.3600)
features.14.conv.0 tensor(0.9643)
features.14.conv.3 tensor(0.1902)
features.14.conv.6 tensor(0.9387)
features.15.conv.0 tensor(0.9791)
features.15.conv.3 tensor(0.1237)
features.15.conv.6 tensor(0.9916)
features.16.conv.0 tensor(0.1419)
features.16.conv.3 tensor(0.1727)
features.16.conv.6 tensor(0.4953)
conv.0 tensor(0.6592)
tensor(1240053.) 2188896.0
INFO - Training [38][   20/  196]   Loss 0.307303   Top1 89.394531   Top5 98.789062   BatchTime 0.430826   LR 0.001128
INFO - Training [38][   40/  196]   Loss 0.305526   Top1 89.404297   Top5 99.023438   BatchTime 0.383563   LR 0.001125
INFO - Training [38][   60/  196]   Loss 0.299715   Top1 89.433594   Top5 99.029948   BatchTime 0.349471   LR 0.001122
INFO - Training [38][   80/  196]   Loss 0.299827   Top1 89.404297   Top5 99.077148   BatchTime 0.341195   LR 0.001119
INFO - Training [38][  100/  196]   Loss 0.294245   Top1 89.589844   Top5 99.085938   BatchTime 0.343758   LR 0.001116
INFO - Training [38][  120/  196]   Loss 0.292932   Top1 89.700521   Top5 99.111328   BatchTime 0.343754   LR 0.001112
INFO - Training [38][  140/  196]   Loss 0.291919   Top1 89.701451   Top5 99.143415   BatchTime 0.342846   LR 0.001109
INFO - Training [38][  160/  196]   Loss 0.294257   Top1 89.719238   Top5 99.106445   BatchTime 0.341374   LR 0.001106
INFO - Training [38][  180/  196]   Loss 0.296799   Top1 89.667969   Top5 99.069010   BatchTime 0.341576   LR 0.001103
INFO - ==> Top1: 89.696    Top5: 99.034    Loss: 0.296
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [38][   20/   40]   Loss 0.305963   Top1 90.039062   Top5 99.707031   BatchTime 0.140326
INFO - Validation [38][   40/   40]   Loss 0.294342   Top1 90.610000   Top5 99.780000   BatchTime 0.099388
INFO - ==> Top1: 90.610    Top5: 99.780    Loss: 0.294
INFO - ==> Sparsity : 0.567
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4618)
features.0.conv.3 tensor(0.2461)
features.1.conv.0 tensor(0.0169)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0282)
features.2.conv.0 tensor(0.0165)
features.2.conv.3 tensor(0.0455)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0291)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.0706)
features.4.conv.6 tensor(0.2480)
features.5.conv.0 tensor(0.0384)
features.5.conv.3 tensor(0.0706)
features.5.conv.6 tensor(0.2913)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0568)
features.7.conv.3 tensor(0.1189)
features.7.conv.6 tensor(0.3055)
features.8.conv.0 tensor(0.0818)
features.8.conv.3 tensor(0.1302)
features.8.conv.6 tensor(0.3092)
features.9.conv.0 tensor(0.0917)
features.9.conv.3 tensor(0.1577)
features.9.conv.6 tensor(0.3511)
features.10.conv.0 tensor(0.0439)
features.10.conv.3 tensor(0.0952)
features.10.conv.6 tensor(0.2746)
features.11.conv.0 tensor(0.1937)
features.11.conv.3 tensor(0.1449)
features.11.conv.6 tensor(0.6225)
features.12.conv.0 tensor(0.4817)
features.12.conv.3 tensor(0.2120)
features.12.conv.6 tensor(0.6336)
features.13.conv.0 tensor(0.1155)
features.13.conv.3 tensor(0.1613)
features.13.conv.6 tensor(0.3626)
features.14.conv.0 tensor(0.9649)
features.14.conv.3 tensor(0.1884)
features.14.conv.6 tensor(0.9395)
features.15.conv.0 tensor(0.9792)
features.15.conv.3 tensor(0.1236)
features.15.conv.6 tensor(0.9914)
features.16.conv.0 tensor(0.1421)
features.16.conv.3 tensor(0.1740)
features.16.conv.6 tensor(0.4966)
conv.0 tensor(0.6600)
tensor(1241445.) 2188896.0
INFO - Training [39][   20/  196]   Loss 0.305595   Top1 89.101562   Top5 98.847656   BatchTime 0.429424   LR 0.001097
INFO - Training [39][   40/  196]   Loss 0.310769   Top1 89.042969   Top5 98.906250   BatchTime 0.383822   LR 0.001094
INFO - Training [39][   60/  196]   Loss 0.308230   Top1 89.218750   Top5 98.932292   BatchTime 0.359597   LR 0.001090
INFO - Training [39][   80/  196]   Loss 0.309704   Top1 89.062500   Top5 98.989258   BatchTime 0.335808   LR 0.001087
INFO - Training [39][  100/  196]   Loss 0.300668   Top1 89.410156   Top5 98.984375   BatchTime 0.333327   LR 0.001084
INFO - Training [39][  120/  196]   Loss 0.296563   Top1 89.567057   Top5 99.049479   BatchTime 0.334634   LR 0.001080
INFO - Training [39][  140/  196]   Loss 0.296394   Top1 89.592634   Top5 99.076451   BatchTime 0.334992   LR 0.001077
INFO - Training [39][  160/  196]   Loss 0.296426   Top1 89.619141   Top5 99.074707   BatchTime 0.335603   LR 0.001073
INFO - Training [39][  180/  196]   Loss 0.294811   Top1 89.676649   Top5 99.032118   BatchTime 0.336215   LR 0.001070
INFO - ==> Top1: 89.690    Top5: 99.016    Loss: 0.294
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [39][   20/   40]   Loss 0.301817   Top1 90.644531   Top5 99.726562   BatchTime 0.134209
INFO - Validation [39][   40/   40]   Loss 0.289664   Top1 91.040000   Top5 99.790000   BatchTime 0.095332
INFO - ==> Top1: 91.040    Top5: 99.790    Loss: 0.290
INFO - ==> Sparsity : 0.567
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4236)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0221)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0339)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0417)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0191)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0267)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.0729)
features.4.conv.6 tensor(0.2493)
features.5.conv.0 tensor(0.0405)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.2923)
features.6.conv.0 tensor(0.0221)
features.6.conv.3 tensor(0.0295)
features.6.conv.6 tensor(0.0539)
features.7.conv.0 tensor(0.0566)
features.7.conv.3 tensor(0.1247)
features.7.conv.6 tensor(0.3066)
features.8.conv.0 tensor(0.0787)
features.8.conv.3 tensor(0.1325)
features.8.conv.6 tensor(0.3105)
features.9.conv.0 tensor(0.0927)
features.9.conv.3 tensor(0.1589)
features.9.conv.6 tensor(0.3519)
features.10.conv.0 tensor(0.0420)
features.10.conv.3 tensor(0.0972)
features.10.conv.6 tensor(0.2760)
features.11.conv.0 tensor(0.1924)
features.11.conv.3 tensor(0.1439)
features.11.conv.6 tensor(0.6237)
features.12.conv.0 tensor(0.4834)
features.12.conv.3 tensor(0.2105)
features.12.conv.6 tensor(0.6340)
features.13.conv.0 tensor(0.1163)
features.13.conv.3 tensor(0.1607)
features.13.conv.6 tensor(0.3638)
features.14.conv.0 tensor(0.9647)
features.14.conv.3 tensor(0.1891)
features.14.conv.6 tensor(0.9386)
features.15.conv.0 tensor(0.9791)
features.15.conv.3 tensor(0.1263)
features.15.conv.6 tensor(0.9906)
features.16.conv.0 tensor(0.1399)
features.16.conv.3 tensor(0.1755)
features.16.conv.6 tensor(0.4948)
conv.0 tensor(0.6594)
tensor(1240394.) 2188896.0
INFO - Training [40][   20/  196]   Loss 0.313153   Top1 88.847656   Top5 98.339844   BatchTime 0.441098   LR 0.001064
INFO - Training [40][   40/  196]   Loss 0.304492   Top1 89.160156   Top5 98.671875   BatchTime 0.388450   LR 0.001060
INFO - Training [40][   60/  196]   Loss 0.301678   Top1 89.348958   Top5 98.723958   BatchTime 0.373163   LR 0.001056
INFO - Training [40][   80/  196]   Loss 0.295688   Top1 89.580078   Top5 98.881836   BatchTime 0.346119   LR 0.001053
INFO - Training [40][  100/  196]   Loss 0.289082   Top1 89.789062   Top5 98.933594   BatchTime 0.335543   LR 0.001049
INFO - Training [40][  120/  196]   Loss 0.284763   Top1 89.931641   Top5 99.013672   BatchTime 0.340512   LR 0.001045
INFO - Training [40][  140/  196]   Loss 0.280937   Top1 90.027902   Top5 99.082031   BatchTime 0.339852   LR 0.001042
INFO - Training [40][  160/  196]   Loss 0.284771   Top1 89.924316   Top5 99.072266   BatchTime 0.339866   LR 0.001038
INFO - Training [40][  180/  196]   Loss 0.287441   Top1 89.865451   Top5 99.038628   BatchTime 0.340316   LR 0.001034
INFO - ==> Top1: 89.890    Top5: 99.036    Loss: 0.288
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [40][   20/   40]   Loss 0.297091   Top1 90.859375   Top5 99.648438   BatchTime 0.146406
INFO - Validation [40][   40/   40]   Loss 0.285325   Top1 91.160000   Top5 99.720000   BatchTime 0.100970
INFO - ==> Top1: 91.160    Top5: 99.720    Loss: 0.285
INFO - ==> Sparsity : 0.567
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0145)
features.2.conv.3 tensor(0.0417)
features.2.conv.6 tensor(0.0446)
features.3.conv.0 tensor(0.0171)
features.3.conv.3 tensor(0.0316)
features.3.conv.6 tensor(0.0252)
features.4.conv.0 tensor(0.0308)
features.4.conv.3 tensor(0.0752)
features.4.conv.6 tensor(0.2503)
features.5.conv.0 tensor(0.0423)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.2930)
features.6.conv.0 tensor(0.0207)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0590)
features.7.conv.3 tensor(0.1244)
features.7.conv.6 tensor(0.3080)
features.8.conv.0 tensor(0.0819)
features.8.conv.3 tensor(0.1314)
features.8.conv.6 tensor(0.3110)
features.9.conv.0 tensor(0.0962)
features.9.conv.3 tensor(0.1565)
features.9.conv.6 tensor(0.3527)
features.10.conv.0 tensor(0.0431)
features.10.conv.3 tensor(0.0929)
features.10.conv.6 tensor(0.2773)
features.11.conv.0 tensor(0.1859)
features.11.conv.3 tensor(0.1431)
features.11.conv.6 tensor(0.6242)
features.12.conv.0 tensor(0.4851)
features.12.conv.3 tensor(0.2095)
features.12.conv.6 tensor(0.6344)
features.13.conv.0 tensor(0.1164)
features.13.conv.3 tensor(0.1578)
features.13.conv.6 tensor(0.3648)
features.14.conv.0 tensor(0.9648)
features.14.conv.3 tensor(0.1900)
features.14.conv.6 tensor(0.9407)
features.15.conv.0 tensor(0.9793)
features.15.conv.3 tensor(0.1258)
features.15.conv.6 tensor(0.9899)
features.16.conv.0 tensor(0.1400)
features.16.conv.3 tensor(0.1744)
features.16.conv.6 tensor(0.4975)
conv.0 tensor(0.6601)
tensor(1242001.) 2188896.0
INFO - Training [41][   20/  196]   Loss 0.313998   Top1 88.750000   Top5 98.515625   BatchTime 0.432628   LR 0.001027
INFO - Training [41][   40/  196]   Loss 0.306522   Top1 88.994141   Top5 98.847656   BatchTime 0.383861   LR 0.001023
INFO - Training [41][   60/  196]   Loss 0.301766   Top1 89.244792   Top5 98.886719   BatchTime 0.367158   LR 0.001020
INFO - Training [41][   80/  196]   Loss 0.295174   Top1 89.565430   Top5 99.018555   BatchTime 0.342739   LR 0.001016
INFO - Training [41][  100/  196]   Loss 0.287279   Top1 89.929688   Top5 99.089844   BatchTime 0.334451   LR 0.001012
INFO - Training [41][  120/  196]   Loss 0.283079   Top1 90.126953   Top5 99.117839   BatchTime 0.334217   LR 0.001008
INFO - Training [41][  140/  196]   Loss 0.282271   Top1 90.167411   Top5 99.174107   BatchTime 0.338112   LR 0.001004
INFO - Training [41][  160/  196]   Loss 0.287574   Top1 89.970703   Top5 99.140625   BatchTime 0.337385   LR 0.001000
INFO - Training [41][  180/  196]   Loss 0.288060   Top1 89.969618   Top5 99.114583   BatchTime 0.337358   LR 0.000996
INFO - ==> Top1: 89.978    Top5: 99.106    Loss: 0.288
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [41][   20/   40]   Loss 0.309785   Top1 90.488281   Top5 99.609375   BatchTime 0.147977
INFO - Validation [41][   40/   40]   Loss 0.293482   Top1 90.920000   Top5 99.720000   BatchTime 0.101707
INFO - ==> Top1: 90.920    Top5: 99.720    Loss: 0.293
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0247)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0334)
features.2.conv.0 tensor(0.0185)
features.2.conv.3 tensor(0.0432)
features.2.conv.6 tensor(0.0457)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0278)
features.3.conv.6 tensor(0.0263)
features.4.conv.0 tensor(0.0269)
features.4.conv.3 tensor(0.0671)
features.4.conv.6 tensor(0.2513)
features.5.conv.0 tensor(0.0412)
features.5.conv.3 tensor(0.0590)
features.5.conv.6 tensor(0.2936)
features.6.conv.0 tensor(0.0207)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0577)
features.7.conv.3 tensor(0.1209)
features.7.conv.6 tensor(0.3087)
features.8.conv.0 tensor(0.0813)
features.8.conv.3 tensor(0.1262)
features.8.conv.6 tensor(0.3118)
features.9.conv.0 tensor(0.0949)
features.9.conv.3 tensor(0.1557)
features.9.conv.6 tensor(0.3531)
features.10.conv.0 tensor(0.0430)
features.10.conv.3 tensor(0.0975)
features.10.conv.6 tensor(0.2787)
features.11.conv.0 tensor(0.1900)
features.11.conv.3 tensor(0.1416)
features.11.conv.6 tensor(0.6246)
features.12.conv.0 tensor(0.4860)
features.12.conv.3 tensor(0.2101)
features.12.conv.6 tensor(0.6340)
features.13.conv.0 tensor(0.1116)
features.13.conv.3 tensor(0.1609)
features.13.conv.6 tensor(0.3661)
features.14.conv.0 tensor(0.9659)
features.14.conv.3 tensor(0.1899)
features.14.conv.6 tensor(0.9400)
features.15.conv.0 tensor(0.9796)
features.15.conv.3 tensor(0.1270)
features.15.conv.6 tensor(0.9898)
features.16.conv.0 tensor(0.1387)
features.16.conv.3 tensor(0.1751)
features.16.conv.6 tensor(0.4980)
conv.0 tensor(0.6606)
tensor(1242379.) 2188896.0
INFO - Training [42][   20/  196]   Loss 0.308764   Top1 88.867188   Top5 98.691406   BatchTime 0.442459   LR 0.000988
INFO - Training [42][   40/  196]   Loss 0.308008   Top1 89.062500   Top5 98.701172   BatchTime 0.397600   LR 0.000984
INFO - Training [42][   60/  196]   Loss 0.302466   Top1 89.381510   Top5 98.789062   BatchTime 0.373371   LR 0.000980
INFO - Training [42][   80/  196]   Loss 0.298030   Top1 89.536133   Top5 98.920898   BatchTime 0.348655   LR 0.000976
INFO - Training [42][  100/  196]   Loss 0.293632   Top1 89.765625   Top5 98.898438   BatchTime 0.335341   LR 0.000972
INFO - Training [42][  120/  196]   Loss 0.288129   Top1 89.928385   Top5 99.003906   BatchTime 0.334121   LR 0.000968
INFO - Training [42][  140/  196]   Loss 0.283260   Top1 90.083705   Top5 99.076451   BatchTime 0.333864   LR 0.000964
INFO - Training [42][  160/  196]   Loss 0.284530   Top1 90.031738   Top5 99.057617   BatchTime 0.338162   LR 0.000959
INFO - Training [42][  180/  196]   Loss 0.283765   Top1 90.056424   Top5 99.008247   BatchTime 0.337479   LR 0.000955
INFO - ==> Top1: 90.130    Top5: 99.002    Loss: 0.281
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [42][   20/   40]   Loss 0.286513   Top1 91.171875   Top5 99.628906   BatchTime 0.143783
INFO - Validation [42][   40/   40]   Loss 0.280739   Top1 91.400000   Top5 99.730000   BatchTime 0.099896
INFO - ==> Top1: 91.400    Top5: 99.730    Loss: 0.281
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  43
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4688)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0137)
features.1.conv.3 tensor(0.0602)
features.1.conv.6 tensor(0.0334)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0463)
features.2.conv.6 tensor(0.0460)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0270)
features.3.conv.6 tensor(0.0304)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.0758)
features.4.conv.6 tensor(0.2518)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.2939)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0330)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0543)
features.7.conv.3 tensor(0.1247)
features.7.conv.6 tensor(0.3094)
features.8.conv.0 tensor(0.0819)
features.8.conv.3 tensor(0.1276)
features.8.conv.6 tensor(0.3126)
features.9.conv.0 tensor(0.0946)
features.9.conv.3 tensor(0.1577)
features.9.conv.6 tensor(0.3540)
features.10.conv.0 tensor(0.0435)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.2795)
features.11.conv.0 tensor(0.1868)
features.11.conv.3 tensor(0.1435)
features.11.conv.6 tensor(0.6257)
features.12.conv.0 tensor(0.4891)
features.12.conv.3 tensor(0.2099)
features.12.conv.6 tensor(0.6338)
features.13.conv.0 tensor(0.1118)
features.13.conv.3 tensor(0.1572)
features.13.conv.6 tensor(0.3671)
features.14.conv.0 tensor(0.9669)
features.14.conv.3 tensor(0.1916)
features.14.conv.6 tensor(0.9396)
features.15.conv.0 tensor(0.9797)
features.15.conv.3 tensor(0.1288)
features.15.conv.6 tensor(0.9908)
features.16.conv.0 tensor(0.1414)
features.16.conv.3 tensor(0.1743)
features.16.conv.6 tensor(0.4982)
conv.0 tensor(0.6592)
tensor(1242811.) 2188896.0
INFO - Training [43][   20/  196]   Loss 0.292900   Top1 89.238281   Top5 98.554688   BatchTime 0.447265   LR 0.000947
INFO - Training [43][   40/  196]   Loss 0.292475   Top1 89.384766   Top5 98.779297   BatchTime 0.396344   LR 0.000943
INFO - Training [43][   60/  196]   Loss 0.290152   Top1 89.537760   Top5 98.873698   BatchTime 0.376964   LR 0.000939
INFO - Training [43][   80/  196]   Loss 0.285407   Top1 89.672852   Top5 99.038086   BatchTime 0.356201   LR 0.000934
INFO - Training [43][  100/  196]   Loss 0.279108   Top1 89.933594   Top5 99.093750   BatchTime 0.343472   LR 0.000930
INFO - Training [43][  120/  196]   Loss 0.273609   Top1 90.139974   Top5 99.147135   BatchTime 0.342570   LR 0.000926
INFO - Training [43][  140/  196]   Loss 0.274010   Top1 90.170201   Top5 99.168527   BatchTime 0.342342   LR 0.000921
INFO - Training [43][  160/  196]   Loss 0.276391   Top1 90.144043   Top5 99.160156   BatchTime 0.342349   LR 0.000917
INFO - Training [43][  180/  196]   Loss 0.276502   Top1 90.164931   Top5 99.108073   BatchTime 0.344010   LR 0.000912
INFO - ==> Top1: 90.170    Top5: 99.098    Loss: 0.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [43][   20/   40]   Loss 0.288585   Top1 91.367188   Top5 99.726562   BatchTime 0.153824
INFO - Validation [43][   40/   40]   Loss 0.277477   Top1 91.440000   Top5 99.740000   BatchTime 0.105649
INFO - ==> Top1: 91.440    Top5: 99.740    Loss: 0.277
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  44
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0208)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0448)
features.2.conv.6 tensor(0.0408)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0255)
features.3.conv.6 tensor(0.0280)
features.4.conv.0 tensor(0.0277)
features.4.conv.3 tensor(0.0677)
features.4.conv.6 tensor(0.2526)
features.5.conv.0 tensor(0.0391)
features.5.conv.3 tensor(0.0642)
features.5.conv.6 tensor(0.2949)
features.6.conv.0 tensor(0.0226)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0508)
features.7.conv.0 tensor(0.0546)
features.7.conv.3 tensor(0.1195)
features.7.conv.6 tensor(0.3094)
features.8.conv.0 tensor(0.0829)
features.8.conv.3 tensor(0.1291)
features.8.conv.6 tensor(0.3131)
features.9.conv.0 tensor(0.0911)
features.9.conv.3 tensor(0.1534)
features.9.conv.6 tensor(0.3546)
features.10.conv.0 tensor(0.0439)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2803)
features.11.conv.0 tensor(0.1875)
features.11.conv.3 tensor(0.1422)
features.11.conv.6 tensor(0.6258)
features.12.conv.0 tensor(0.4883)
features.12.conv.3 tensor(0.2087)
features.12.conv.6 tensor(0.6342)
features.13.conv.0 tensor(0.1122)
features.13.conv.3 tensor(0.1549)
features.13.conv.6 tensor(0.3659)
features.14.conv.0 tensor(0.9671)
features.14.conv.3 tensor(0.1894)
features.14.conv.6 tensor(0.9410)
features.15.conv.0 tensor(0.9798)
features.15.conv.3 tensor(0.1273)
features.15.conv.6 tensor(0.9904)
features.16.conv.0 tensor(0.1435)
features.16.conv.3 tensor(0.1745)
features.16.conv.6 tensor(0.4979)
conv.0 tensor(0.6608)
tensor(1243638.) 2188896.0
INFO - Training [44][   20/  196]   Loss 0.292202   Top1 89.199219   Top5 98.613281   BatchTime 0.443186   LR 0.000904
INFO - Training [44][   40/  196]   Loss 0.292041   Top1 89.599609   Top5 98.974609   BatchTime 0.397650   LR 0.000900
INFO - Training [44][   60/  196]   Loss 0.291762   Top1 89.759115   Top5 98.938802   BatchTime 0.377798   LR 0.000895
INFO - Training [44][   80/  196]   Loss 0.286632   Top1 89.838867   Top5 99.047852   BatchTime 0.350521   LR 0.000891
INFO - Training [44][  100/  196]   Loss 0.280060   Top1 90.082031   Top5 99.101562   BatchTime 0.338598   LR 0.000886
INFO - Training [44][  120/  196]   Loss 0.273602   Top1 90.325521   Top5 99.179688   BatchTime 0.339175   LR 0.000882
INFO - Training [44][  140/  196]   Loss 0.269788   Top1 90.452009   Top5 99.224330   BatchTime 0.339446   LR 0.000877
INFO - Training [44][  160/  196]   Loss 0.271321   Top1 90.415039   Top5 99.196777   BatchTime 0.338842   LR 0.000873
INFO - Training [44][  180/  196]   Loss 0.272962   Top1 90.310330   Top5 99.160156   BatchTime 0.338179   LR 0.000868
INFO - ==> Top1: 90.324    Top5: 99.148    Loss: 0.273
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [44][   20/   40]   Loss 0.297759   Top1 90.898438   Top5 99.765625   BatchTime 0.160176
INFO - Validation [44][   40/   40]   Loss 0.289424   Top1 90.940000   Top5 99.810000   BatchTime 0.108411
INFO - ==> Top1: 90.940    Top5: 99.810    Loss: 0.289
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  45
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0202)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0312)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0378)
features.2.conv.6 tensor(0.0402)
features.3.conv.0 tensor(0.0197)
features.3.conv.3 tensor(0.0255)
features.3.conv.6 tensor(0.0221)
features.4.conv.0 tensor(0.0295)
features.4.conv.3 tensor(0.0671)
features.4.conv.6 tensor(0.2524)
features.5.conv.0 tensor(0.0379)
features.5.conv.3 tensor(0.0631)
features.5.conv.6 tensor(0.2956)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0503)
features.7.conv.0 tensor(0.0557)
features.7.conv.3 tensor(0.1209)
features.7.conv.6 tensor(0.3104)
features.8.conv.0 tensor(0.0843)
features.8.conv.3 tensor(0.1319)
features.8.conv.6 tensor(0.3139)
features.9.conv.0 tensor(0.0888)
features.9.conv.3 tensor(0.1522)
features.9.conv.6 tensor(0.3553)
features.10.conv.0 tensor(0.0430)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.2810)
features.11.conv.0 tensor(0.1871)
features.11.conv.3 tensor(0.1422)
features.11.conv.6 tensor(0.6260)
features.12.conv.0 tensor(0.4893)
features.12.conv.3 tensor(0.2108)
features.12.conv.6 tensor(0.6337)
features.13.conv.0 tensor(0.1159)
features.13.conv.3 tensor(0.1559)
features.13.conv.6 tensor(0.3672)
features.14.conv.0 tensor(0.9672)
features.14.conv.3 tensor(0.1903)
features.14.conv.6 tensor(0.9420)
features.15.conv.0 tensor(0.9796)
features.15.conv.3 tensor(0.1280)
features.15.conv.6 tensor(0.9904)
features.16.conv.0 tensor(0.1479)
features.16.conv.3 tensor(0.1738)
features.16.conv.6 tensor(0.4996)
conv.0 tensor(0.6618)
tensor(1245862.) 2188896.0
INFO - Training [45][   20/  196]   Loss 0.281103   Top1 89.941406   Top5 98.789062   BatchTime 0.462208   LR 0.000860
INFO - Training [45][   40/  196]   Loss 0.286010   Top1 90.146484   Top5 98.818359   BatchTime 0.406013   LR 0.000855
INFO - Training [45][   60/  196]   Loss 0.286305   Top1 90.136719   Top5 98.815104   BatchTime 0.380953   LR 0.000850
INFO - Training [45][   80/  196]   Loss 0.283910   Top1 90.209961   Top5 98.989258   BatchTime 0.355927   LR 0.000846
INFO - Training [45][  100/  196]   Loss 0.277666   Top1 90.421875   Top5 99.070312   BatchTime 0.347151   LR 0.000841
INFO - Training [45][  120/  196]   Loss 0.271497   Top1 90.566406   Top5 99.121094   BatchTime 0.345604   LR 0.000836
INFO - Training [45][  140/  196]   Loss 0.270358   Top1 90.638951   Top5 99.151786   BatchTime 0.344389   LR 0.000832
INFO - Training [45][  160/  196]   Loss 0.272297   Top1 90.573730   Top5 99.138184   BatchTime 0.342757   LR 0.000827
INFO - Training [45][  180/  196]   Loss 0.271457   Top1 90.542535   Top5 99.129774   BatchTime 0.342871   LR 0.000822
INFO - ==> Top1: 90.528    Top5: 99.128    Loss: 0.272
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [45][   20/   40]   Loss 0.302261   Top1 90.859375   Top5 99.589844   BatchTime 0.155012
INFO - Validation [45][   40/   40]   Loss 0.293105   Top1 91.060000   Top5 99.690000   BatchTime 0.104428
INFO - ==> Top1: 91.060    Top5: 99.690    Loss: 0.293
INFO - ==> Sparsity : 0.568
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 91.550   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  46
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4062)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0145)
features.2.conv.3 tensor(0.0440)
features.2.conv.6 tensor(0.0388)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0278)
features.3.conv.6 tensor(0.0243)
features.4.conv.0 tensor(0.0280)
features.4.conv.3 tensor(0.0671)
features.4.conv.6 tensor(0.2529)
features.5.conv.0 tensor(0.0378)
features.5.conv.3 tensor(0.0625)
features.5.conv.6 tensor(0.2962)
features.6.conv.0 tensor(0.0243)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0514)
features.7.conv.0 tensor(0.0534)
features.7.conv.3 tensor(0.1198)
features.7.conv.6 tensor(0.3103)
features.8.conv.0 tensor(0.0843)
features.8.conv.3 tensor(0.1279)
features.8.conv.6 tensor(0.3146)
features.9.conv.0 tensor(0.0877)
features.9.conv.3 tensor(0.1545)
features.9.conv.6 tensor(0.3558)
features.10.conv.0 tensor(0.0448)
features.10.conv.3 tensor(0.0929)
features.10.conv.6 tensor(0.2817)
features.11.conv.0 tensor(0.1865)
features.11.conv.3 tensor(0.1426)
features.11.conv.6 tensor(0.6257)
features.12.conv.0 tensor(0.4899)
features.12.conv.3 tensor(0.2095)
features.12.conv.6 tensor(0.6336)
features.13.conv.0 tensor(0.1090)
features.13.conv.3 tensor(0.1534)
features.13.conv.6 tensor(0.3658)
features.14.conv.0 tensor(0.9672)
features.14.conv.3 tensor(0.1896)
features.14.conv.6 tensor(0.9404)
features.15.conv.0 tensor(0.9796)
features.15.conv.3 tensor(0.1280)
features.15.conv.6 tensor(0.9903)
features.16.conv.0 tensor(0.1470)
features.16.conv.3 tensor(0.1715)
features.16.conv.6 tensor(0.4976)
conv.0 tensor(0.6618)
tensor(1244219.) 2188896.0
INFO - Training [46][   20/  196]   Loss 0.271058   Top1 90.214844   Top5 98.808594   BatchTime 0.455302   LR 0.000814
INFO - Training [46][   40/  196]   Loss 0.278608   Top1 90.107422   Top5 98.876953   BatchTime 0.398253   LR 0.000809
INFO - Training [46][   60/  196]   Loss 0.278238   Top1 90.117188   Top5 98.997396   BatchTime 0.379877   LR 0.000804
INFO - Training [46][   80/  196]   Loss 0.274427   Top1 90.229492   Top5 99.106445   BatchTime 0.352583   LR 0.000799
INFO - Training [46][  100/  196]   Loss 0.272540   Top1 90.328125   Top5 99.128906   BatchTime 0.343998   LR 0.000794
INFO - Training [46][  120/  196]   Loss 0.267333   Top1 90.579427   Top5 99.176432   BatchTime 0.343783   LR 0.000789
INFO - Training [46][  140/  196]   Loss 0.265046   Top1 90.680804   Top5 99.227121   BatchTime 0.343758   LR 0.000785
INFO - Training [46][  160/  196]   Loss 0.268221   Top1 90.566406   Top5 99.216309   BatchTime 0.342758   LR 0.000780
INFO - Training [46][  180/  196]   Loss 0.270212   Top1 90.549045   Top5 99.168837   BatchTime 0.341865   LR 0.000775
INFO - ==> Top1: 90.550    Top5: 99.156    Loss: 0.271
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [46][   20/   40]   Loss 0.291489   Top1 91.640625   Top5 99.707031   BatchTime 0.146394
INFO - Validation [46][   40/   40]   Loss 0.273650   Top1 91.770000   Top5 99.800000   BatchTime 0.100739
INFO - ==> Top1: 91.770    Top5: 99.800    Loss: 0.274
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
features.0.conv.0 tensor(0.4062)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0228)
features.1.conv.3 tensor(0.0637)
features.1.conv.6 tensor(0.0295)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0432)
features.2.conv.6 tensor(0.0411)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0270)
features.3.conv.6 tensor(0.0269)
features.4.conv.0 tensor(0.0309)
features.4.conv.3 tensor(0.0700)
features.4.conv.6 tensor(0.2541)
features.5.conv.0 tensor(0.0379)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.2965)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0341)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0574)
features.7.conv.3 tensor(0.1181)
features.7.conv.6 tensor(0.3111)
features.8.conv.0 tensor(0.0838)
features.8.conv.3 tensor(0.1270)
features.8.conv.6 tensor(0.3147)
features.9.conv.0 tensor(0.0878)
features.9.conv.3 tensor(0.1571)
features.9.conv.6 tensor(0.3567)
features.10.conv.0 tensor(0.0461)
features.10.conv.3 tensor(0.0949)
features.10.conv.6 tensor(0.2825)
features.11.conv.0 tensor(0.1882)
features.11.conv.3 tensor(0.1431)
features.11.conv.6 tensor(0.6263)
features.12.conv.0 tensor(0.4914)
features.12.conv.3 tensor(0.2108)
features.12.conv.6 tensor(0.6332)
features.13.conv.0 tensor(0.1158)
features.13.conv.3 tensor(0.1537)
features.13.conv.6 tensor(0.3646)
features.14.conv.0 tensor(0.9674)
features.14.conv.3 tensor(0.1911)
features.14.conv.6 tensor(0.9426)
features.15.conv.0 tensor(0.9795)
features.15.conv.3 tensor(0.1267)
features.15.conv.6 tensor(0.9899)
features.16.conv.0 tensor(0.1443)
features.16.conv.3 tensor(0.1728)
features.16.conv.6 tensor(0.5000)
conv.0 tensor(0.6629)
tensor(1246075.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  47
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [47][   20/  196]   Loss 0.282997   Top1 90.156250   Top5 98.554688   BatchTime 0.426464   LR 0.000766
INFO - Training [47][   40/  196]   Loss 0.285497   Top1 89.873047   Top5 98.691406   BatchTime 0.379635   LR 0.000761
INFO - Training [47][   60/  196]   Loss 0.284141   Top1 89.856771   Top5 98.789062   BatchTime 0.343765   LR 0.000756
INFO - Training [47][   80/  196]   Loss 0.281802   Top1 90.009766   Top5 98.930664   BatchTime 0.330804   LR 0.000752
INFO - Training [47][  100/  196]   Loss 0.277189   Top1 90.199219   Top5 98.992188   BatchTime 0.333963   LR 0.000747
INFO - Training [47][  120/  196]   Loss 0.270589   Top1 90.514323   Top5 99.055990   BatchTime 0.335712   LR 0.000742
INFO - Training [47][  140/  196]   Loss 0.267569   Top1 90.597098   Top5 99.123884   BatchTime 0.335167   LR 0.000737
INFO - Training [47][  160/  196]   Loss 0.269811   Top1 90.522461   Top5 99.101562   BatchTime 0.335006   LR 0.000732
INFO - Training [47][  180/  196]   Loss 0.269737   Top1 90.514323   Top5 99.086372   BatchTime 0.336891   LR 0.000727
INFO - ==> Top1: 90.534    Top5: 99.096    Loss: 0.268
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [47][   20/   40]   Loss 0.289126   Top1 91.367188   Top5 99.707031   BatchTime 0.139885
INFO - Validation [47][   40/   40]   Loss 0.277715   Top1 91.420000   Top5 99.780000   BatchTime 0.100191
features.0.conv.0 tensor(0.4028)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0254)
features.1.conv.3 tensor(0.0532)
features.1.conv.6 tensor(0.0295)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0401)
features.2.conv.6 tensor(0.0411)
features.3.conv.0 tensor(0.0136)
features.3.conv.3 tensor(0.0278)
features.3.conv.6 tensor(0.0234)
features.4.conv.0 tensor(0.0275)
features.4.conv.3 tensor(0.0654)
features.4.conv.6 tensor(0.2547)
features.5.conv.0 tensor(0.0394)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2965)
features.6.conv.0 tensor(0.0239)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0558)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.3118)
features.8.conv.0 tensor(0.0865)
features.8.conv.3 tensor(0.1273)
features.8.conv.6 tensor(0.3157)
features.9.conv.0 tensor(0.0893)
features.9.conv.3 tensor(0.1554)
features.9.conv.6 tensor(0.3574)
features.10.conv.0 tensor(0.0461)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.2830)
features.11.conv.0 tensor(0.1859)
features.11.conv.3 tensor(0.1399)
features.11.conv.6 tensor(0.6261)
features.12.conv.0 tensor(0.4908)
features.12.conv.3 tensor(0.2093)
features.12.conv.6 tensor(0.6331)
features.13.conv.0 tensor(0.1120)
features.13.conv.3
INFO - ==> Top1: 91.420    Top5: 99.780    Loss: 0.278
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  48
features.13.conv.3 tensor(0.1539)
features.13.conv.6 tensor(0.3661)
features.14.conv.0 tensor(0.9674)
features.14.conv.3 tensor(0.1902)
features.14.conv.6 tensor(0.9418)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1274)
features.15.conv.6 tensor(0.9902)
features.16.conv.0 tensor(0.1451)
features.16.conv.3 tensor(0.1728)
features.16.conv.6 tensor(0.5000)
conv.0 tensor(0.6632)
tensor(1246121.) 2188896.0
INFO - Training [48][   20/  196]   Loss 0.267078   Top1 90.507812   Top5 98.710938   BatchTime 0.423681   LR 0.000718
INFO - Training [48][   40/  196]   Loss 0.277201   Top1 90.214844   Top5 98.769531   BatchTime 0.387407   LR 0.000713
INFO - Training [48][   60/  196]   Loss 0.273457   Top1 90.358073   Top5 98.919271   BatchTime 0.351185   LR 0.000708
INFO - Training [48][   80/  196]   Loss 0.275672   Top1 90.302734   Top5 98.994141   BatchTime 0.339617   LR 0.000703
INFO - Training [48][  100/  196]   Loss 0.269898   Top1 90.500000   Top5 99.046875   BatchTime 0.339752   LR 0.000698
INFO - Training [48][  120/  196]   Loss 0.264619   Top1 90.683594   Top5 99.111328   BatchTime 0.339025   LR 0.000693
INFO - Training [48][  140/  196]   Loss 0.262991   Top1 90.792411   Top5 99.151786   BatchTime 0.337966   LR 0.000688
INFO - Training [48][  160/  196]   Loss 0.264080   Top1 90.727539   Top5 99.162598   BatchTime 0.338196   LR 0.000683
INFO - Training [48][  180/  196]   Loss 0.264259   Top1 90.674913   Top5 99.127604   BatchTime 0.340452   LR 0.000678
INFO - ==> Top1: 90.730    Top5: 99.124    Loss: 0.263
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [48][   20/   40]   Loss 0.304248   Top1 91.308594   Top5 99.648438   BatchTime 0.151889
INFO - Validation [48][   40/   40]   Loss 0.294209   Top1 91.340000   Top5 99.760000   BatchTime 0.105496
INFO - ==> Top1: 91.340    Top5: 99.760    Loss: 0.294
INFO - ==> Sparsity : 0.570
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  49
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0532)
features.1.conv.6 tensor(0.0326)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0417)
features.2.conv.6 tensor(0.0440)
features.3.conv.0 tensor(0.0153)
features.3.conv.3 tensor(0.0278)
features.3.conv.6 tensor(0.0263)
features.4.conv.0 tensor(0.0291)
features.4.conv.3 tensor(0.0648)
features.4.conv.6 tensor(0.2549)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.2969)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0568)
features.7.conv.3 tensor(0.1181)
features.7.conv.6 tensor(0.3124)
features.8.conv.0 tensor(0.0833)
features.8.conv.3 tensor(0.1238)
features.8.conv.6 tensor(0.3157)
features.9.conv.0 tensor(0.0924)
features.9.conv.3 tensor(0.1536)
features.9.conv.6 tensor(0.3577)
features.10.conv.0 tensor(0.0445)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2833)
features.11.conv.0 tensor(0.1887)
features.11.conv.3 tensor(0.1377)
features.11.conv.6 tensor(0.6263)
features.12.conv.0 tensor(0.4913)
features.12.conv.3 tensor(0.2097)
features.12.conv.6 tensor(0.6348)
features.13.conv.0 tensor(0.1154)
features.13.conv.3 tensor(0.1526)
features.13.conv.6 tensor(0.3668)
features.14.conv.0 tensor(0.9678)
features.14.conv.3 tensor(0.1905)
features.14.conv.6 tensor(0.9416)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1269)
features.15.conv.6 tensor(0.9897)
features.16.conv.0 tensor(0.1462)
features.16.conv.3 tensor(0.1719)
features.16.conv.6 tensor(0.5007)
conv.0 tensor(0.6640)
tensor(1247335.) 2188896.0
INFO - Training [49][   20/  196]   Loss 0.279429   Top1 90.000000   Top5 98.437500   BatchTime 0.421545   LR 0.000669
INFO - Training [49][   40/  196]   Loss 0.275169   Top1 90.263672   Top5 98.671875   BatchTime 0.382265   LR 0.000664
INFO - Training [49][   60/  196]   Loss 0.276806   Top1 90.266927   Top5 98.828125   BatchTime 0.350863   LR 0.000659
INFO - Training [49][   80/  196]   Loss 0.275066   Top1 90.327148   Top5 98.916016   BatchTime 0.334420   LR 0.000654
INFO - Training [49][  100/  196]   Loss 0.267567   Top1 90.625000   Top5 98.976562   BatchTime 0.333635   LR 0.000649
INFO - Training [49][  120/  196]   Loss 0.261471   Top1 90.908203   Top5 99.055990   BatchTime 0.334118   LR 0.000644
INFO - Training [49][  140/  196]   Loss 0.259145   Top1 90.970982   Top5 99.135045   BatchTime 0.333741   LR 0.000639
INFO - Training [49][  160/  196]   Loss 0.260512   Top1 90.905762   Top5 99.099121   BatchTime 0.333351   LR 0.000634
INFO - Training [49][  180/  196]   Loss 0.260870   Top1 90.876736   Top5 99.110243   BatchTime 0.335913   LR 0.000629
INFO - ==> Top1: 90.870    Top5: 99.124    Loss: 0.261
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [49][   20/   40]   Loss 0.288499   Top1 91.367188   Top5 99.667969   BatchTime 0.139478
INFO - Validation [49][   40/   40]   Loss 0.281484   Top1 91.360000   Top5 99.770000   BatchTime 0.098106
INFO - ==> Top1: 91.360    Top5: 99.770    Loss: 0.281
INFO - ==> Sparsity : 0.570
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  50
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4062)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0241)
features.1.conv.3 tensor(0.0625)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0440)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0148)
features.3.conv.3 tensor(0.0239)
features.3.conv.6 tensor(0.0256)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.0602)
features.4.conv.6 tensor(0.2550)
features.5.conv.0 tensor(0.0376)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.2970)
features.6.conv.0 tensor(0.0199)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0557)
features.7.conv.3 tensor(0.1183)
features.7.conv.6 tensor(0.3128)
features.8.conv.0 tensor(0.0819)
features.8.conv.3 tensor(0.1238)
features.8.conv.6 tensor(0.3166)
features.9.conv.0 tensor(0.0929)
features.9.conv.3 tensor(0.1542)
features.9.conv.6 tensor(0.3579)
features.10.conv.0 tensor(0.0435)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.2839)
features.11.conv.0 tensor(0.1904)
features.11.conv.3 tensor(0.1395)
features.11.conv.6 tensor(0.6266)
features.12.conv.0 tensor(0.4913)
features.12.conv.3 tensor(0.2126)
features.12.conv.6 tensor(0.6346)
features.13.conv.0 tensor(0.1135)
features.13.conv.3 tensor(0.1526)
features.13.conv.6 tensor(0.3673)
features.14.conv.0 tensor(0.9677)
features.14.conv.3 tensor(0.1895)
features.14.conv.6 tensor(0.9402)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1266)
features.15.conv.6 tensor(0.9880)
features.16.conv.0 tensor(0.1451)
features.16.conv.3 tensor(0.1720)
features.16.conv.6 tensor(0.5008)
conv.0 tensor(0.6647)
tensor(1247006.) 2188896.0
INFO - Training [50][   20/  196]   Loss 0.296796   Top1 89.941406   Top5 98.535156   BatchTime 0.437616   LR 0.000620
INFO - Training [50][   40/  196]   Loss 0.287388   Top1 89.931641   Top5 98.779297   BatchTime 0.395796   LR 0.000615
INFO - Training [50][   60/  196]   Loss 0.283115   Top1 89.947917   Top5 98.912760   BatchTime 0.363654   LR 0.000610
INFO - Training [50][   80/  196]   Loss 0.278377   Top1 90.205078   Top5 99.047852   BatchTime 0.343658   LR 0.000605
INFO - Training [50][  100/  196]   Loss 0.272375   Top1 90.406250   Top5 99.089844   BatchTime 0.341941   LR 0.000600
INFO - Training [50][  120/  196]   Loss 0.264964   Top1 90.638021   Top5 99.160156   BatchTime 0.343382   LR 0.000595
INFO - Training [50][  140/  196]   Loss 0.261075   Top1 90.753348   Top5 99.213170   BatchTime 0.342780   LR 0.000590
INFO - Training [50][  160/  196]   Loss 0.260619   Top1 90.742188   Top5 99.204102   BatchTime 0.342022   LR 0.000585
INFO - Training [50][  180/  196]   Loss 0.259643   Top1 90.759549   Top5 99.171007   BatchTime 0.343405   LR 0.000580
INFO - ==> Top1: 90.806    Top5: 99.170    Loss: 0.260
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [50][   20/   40]   Loss 0.305357   Top1 90.742188   Top5 99.746094   BatchTime 0.143189
INFO - Validation [50][   40/   40]   Loss 0.292480   Top1 91.010000   Top5 99.800000   BatchTime 0.100372
INFO - ==> Top1: 91.010    Top5: 99.800    Loss: 0.292
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  51
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4201)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0291)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0417)
features.2.conv.6 tensor(0.0396)
features.3.conv.0 tensor(0.0159)
features.3.conv.3 tensor(0.0278)
features.3.conv.6 tensor(0.0232)
features.4.conv.0 tensor(0.0309)
features.4.conv.3 tensor(0.0706)
features.4.conv.6 tensor(0.2555)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.2972)
features.6.conv.0 tensor(0.0215)
features.6.conv.3 tensor(0.0336)
features.6.conv.6 tensor(0.0518)
features.7.conv.0 tensor(0.0548)
features.7.conv.3 tensor(0.1166)
features.7.conv.6 tensor(0.3134)
features.8.conv.0 tensor(0.0822)
features.8.conv.3 tensor(0.1279)
features.8.conv.6 tensor(0.3170)
features.9.conv.0 tensor(0.0915)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.3584)
features.10.conv.0 tensor(0.0428)
features.10.conv.3 tensor(0.0900)
features.10.conv.6 tensor(0.2842)
features.11.conv.0 tensor(0.1898)
features.11.conv.3 tensor(0.1366)
features.11.conv.6 tensor(0.6261)
features.12.conv.0 tensor(0.4909)
features.12.conv.3 tensor(0.2097)
features.12.conv.6 tensor(0.6342)
features.13.conv.0 tensor(0.1148)
features.13.conv.3 tensor(0.1516)
features.13.conv.6 tensor(0.3683)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1904)
features.14.conv.6 tensor(0.9376)
features.15.conv.0 tensor(0.9801)
features.15.conv.3 tensor(0.1265)
features.15.conv.6 tensor(0.9874)
features.16.conv.0 tensor(0.1423)
features.16.conv.3 tensor(0.1719)
features.16.conv.6 tensor(0.5011)
conv.0 tensor(0.6644)
tensor(1246038.) 2188896.0
INFO - Training [51][   20/  196]   Loss 0.281872   Top1 90.507812   Top5 98.417969   BatchTime 0.445468   LR 0.000571
INFO - Training [51][   40/  196]   Loss 0.272388   Top1 90.849609   Top5 98.701172   BatchTime 0.401349   LR 0.000566
INFO - Training [51][   60/  196]   Loss 0.272894   Top1 90.618490   Top5 98.841146   BatchTime 0.370422   LR 0.000561
INFO - Training [51][   80/  196]   Loss 0.267582   Top1 90.795898   Top5 98.945312   BatchTime 0.352888   LR 0.000556
INFO - Training [51][  100/  196]   Loss 0.261253   Top1 90.972656   Top5 99.000000   BatchTime 0.350117   LR 0.000551
INFO - Training [51][  120/  196]   Loss 0.257582   Top1 91.025391   Top5 99.062500   BatchTime 0.348647   LR 0.000546
INFO - Training [51][  140/  196]   Loss 0.256110   Top1 91.021205   Top5 99.112723   BatchTime 0.347705   LR 0.000541
INFO - Training [51][  160/  196]   Loss 0.259264   Top1 90.944824   Top5 99.116211   BatchTime 0.347400   LR 0.000536
INFO - Training [51][  180/  196]   Loss 0.261456   Top1 90.848524   Top5 99.099392   BatchTime 0.348243   LR 0.000531
INFO - ==> Top1: 90.970    Top5: 99.096    Loss: 0.259
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [51][   20/   40]   Loss 0.286849   Top1 91.328125   Top5 99.707031   BatchTime 0.147856
INFO - Validation [51][   40/   40]   Loss 0.275729   Top1 91.510000   Top5 99.800000   BatchTime 0.102618
INFO - ==> Top1: 91.510    Top5: 99.800    Loss: 0.276
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  52
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4340)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0486)
features.1.conv.6 tensor(0.0286)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0394)
features.2.conv.6 tensor(0.0399)
features.3.conv.0 tensor(0.0162)
features.3.conv.3 tensor(0.0301)
features.3.conv.6 tensor(0.0265)
features.4.conv.0 tensor(0.0291)
features.4.conv.3 tensor(0.0666)
features.4.conv.6 tensor(0.2559)
features.5.conv.0 tensor(0.0356)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.2974)
features.6.conv.0 tensor(0.0216)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0525)
features.7.conv.0 tensor(0.0553)
features.7.conv.3 tensor(0.1175)
features.7.conv.6 tensor(0.3141)
features.8.conv.0 tensor(0.0820)
features.8.conv.3 tensor(0.1273)
features.8.conv.6 tensor(0.3172)
features.9.conv.0 tensor(0.0923)
features.9.conv.3 tensor(0.1545)
features.9.conv.6 tensor(0.3587)
features.10.conv.0 tensor(0.0432)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2846)
features.11.conv.0 tensor(0.1882)
features.11.conv.3 tensor(0.1346)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4908)
features.12.conv.3 tensor(0.2095)
features.12.conv.6 tensor(0.6343)
features.13.conv.0 tensor(0.1098)
features.13.conv.3 tensor(0.1518)
features.13.conv.6 tensor(0.3691)
features.14.conv.0 tensor(0.9671)
features.14.conv.3 tensor(0.1897)
features.14.conv.6 tensor(0.9378)
features.15.conv.0 tensor(0.9802)
features.15.conv.3 tensor(0.1265)
features.15.conv.6 tensor(0.9877)
features.16.conv.0 tensor(0.1422)
features.16.conv.3 tensor(0.1700)
features.16.conv.6 tensor(0.5017)
conv.0 tensor(0.6647)
tensor(1246096.) 2188896.0
INFO - Training [52][   20/  196]   Loss 0.266188   Top1 90.781250   Top5 98.769531   BatchTime 0.445340   LR 0.000523
INFO - Training [52][   40/  196]   Loss 0.266009   Top1 90.742188   Top5 98.837891   BatchTime 0.390974   LR 0.000518
INFO - Training [52][   60/  196]   Loss 0.263867   Top1 90.820312   Top5 98.932292   BatchTime 0.357292   LR 0.000513
INFO - Training [52][   80/  196]   Loss 0.261624   Top1 90.834961   Top5 99.052734   BatchTime 0.348341   LR 0.000508
INFO - Training [52][  100/  196]   Loss 0.253954   Top1 91.132812   Top5 99.121094   BatchTime 0.347042   LR 0.000503
INFO - Training [52][  120/  196]   Loss 0.249614   Top1 91.272786   Top5 99.182943   BatchTime 0.348526   LR 0.000498
INFO - Training [52][  140/  196]   Loss 0.246876   Top1 91.358817   Top5 99.221540   BatchTime 0.350491   LR 0.000493
INFO - Training [52][  160/  196]   Loss 0.251087   Top1 91.188965   Top5 99.221191   BatchTime 0.352620   LR 0.000488
INFO - Training [52][  180/  196]   Loss 0.251946   Top1 91.193576   Top5 99.175347   BatchTime 0.351861   LR 0.000483
INFO - ==> Top1: 91.218    Top5: 99.158    Loss: 0.252
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [52][   20/   40]   Loss 0.289674   Top1 91.250000   Top5 99.648438   BatchTime 0.154706
INFO - Validation [52][   40/   40]   Loss 0.283158   Top1 91.400000   Top5 99.770000   BatchTime 0.105137
INFO - ==> Top1: 91.400    Top5: 99.770    Loss: 0.283
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 91.560   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  53
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0509)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0159)
features.2.conv.3 tensor(0.0355)
features.2.conv.6 tensor(0.0414)
features.3.conv.0 tensor(0.0185)
features.3.conv.3 tensor(0.0285)
features.3.conv.6 tensor(0.0237)
features.4.conv.0 tensor(0.0293)
features.4.conv.3 tensor(0.0654)
features.4.conv.6 tensor(0.2560)
features.5.conv.0 tensor(0.0365)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.2980)
features.6.conv.0 tensor(0.0200)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0526)
features.7.conv.3 tensor(0.1137)
features.7.conv.6 tensor(0.3138)
features.8.conv.0 tensor(0.0826)
features.8.conv.3 tensor(0.1259)
features.8.conv.6 tensor(0.3177)
features.9.conv.0 tensor(0.0927)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.3591)
features.10.conv.0 tensor(0.0420)
features.10.conv.3 tensor(0.0906)
features.10.conv.6 tensor(0.2850)
features.11.conv.0 tensor(0.1863)
features.11.conv.3 tensor(0.1370)
features.11.conv.6 tensor(0.6270)
features.12.conv.0 tensor(0.4901)
features.12.conv.3 tensor(0.2087)
features.12.conv.6 tensor(0.6345)
features.13.conv.0 tensor(0.1082)
features.13.conv.3 tensor(0.1520)
features.13.conv.6 tensor(0.3698)
features.14.conv.0 tensor(0.9674)
features.14.conv.3 tensor(0.1898)
features.14.conv.6 tensor(0.9373)
features.15.conv.0 tensor(0.9801)
features.15.conv.3 tensor(0.1274)
features.15.conv.6 tensor(0.9875)
features.16.conv.0 tensor(0.1442)
features.16.conv.3 tensor(0.1706)
features.16.conv.6 tensor(0.5021)
conv.0 tensor(0.6643)
tensor(1246160.) 2188896.0
INFO - Training [53][   20/  196]   Loss 0.269525   Top1 90.390625   Top5 98.515625   BatchTime 0.473423   LR 0.000474
INFO - Training [53][   40/  196]   Loss 0.271284   Top1 90.419922   Top5 98.857422   BatchTime 0.404297   LR 0.000470
INFO - Training [53][   60/  196]   Loss 0.264473   Top1 90.742188   Top5 98.958333   BatchTime 0.359035   LR 0.000465
INFO - Training [53][   80/  196]   Loss 0.256304   Top1 91.030273   Top5 99.082031   BatchTime 0.354055   LR 0.000460
INFO - Training [53][  100/  196]   Loss 0.252562   Top1 91.191406   Top5 99.117188   BatchTime 0.357692   LR 0.000455
INFO - Training [53][  120/  196]   Loss 0.246439   Top1 91.389974   Top5 99.176432   BatchTime 0.355914   LR 0.000450
INFO - Training [53][  140/  196]   Loss 0.245652   Top1 91.406250   Top5 99.218750   BatchTime 0.355633   LR 0.000445
INFO - Training [53][  160/  196]   Loss 0.249428   Top1 91.315918   Top5 99.194336   BatchTime 0.353729   LR 0.000441
INFO - Training [53][  180/  196]   Loss 0.249756   Top1 91.317274   Top5 99.168837   BatchTime 0.352245   LR 0.000436
INFO - ==> Top1: 91.242    Top5: 99.176    Loss: 0.250
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [53][   20/   40]   Loss 0.279578   Top1 91.875000   Top5 99.746094   BatchTime 0.150762
INFO - Validation [53][   40/   40]   Loss 0.270076   Top1 92.040000   Top5 99.800000   BatchTime 0.102397
INFO - ==> Top1: 92.040    Top5: 99.800    Loss: 0.270
INFO - ==> Sparsity : 0.570
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
features.0.conv.0 tensor(0.4479)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0486)
features.1.conv.6 tensor(0.0291)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0411)
features.3.conv.0 tensor(0.0168)
features.3.conv.3 tensor(0.0293)
features.3.conv.6 tensor(0.0276)
features.4.conv.0 tensor(0.0304)
features.4.conv.3 tensor(0.0677)
features.4.conv.6 tensor(0.2565)
features.5.conv.0 tensor(0.0405)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.2982)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0295)
features.6.conv.6 tensor(0.0521)
features.7.conv.0 tensor(0.0538)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.3143)
features.8.conv.0 tensor(0.0821)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.3173)
features.9.conv.0 tensor(0.0927)
features.9.conv.3 tensor(0.1565)
features.9.conv.6 tensor(0.3591)
features.10.conv.0 tensor(0.0412)
features.10.conv.3 tensor(0.0903)
features.10.conv.6 tensor(0.2854)
features.11.conv.0 tensor(0.1903)
features.11.conv.3 tensor(0.1360)
features.11.conv.6 tensor(0.6271)
features.12.conv.0 tensor(0.4901)
features.12.conv.3 tensor(0.2093)
features.12.conv.6 tensor(0.6341)
features.13.conv.0 tensor(0.1092)
features.13.conv.3 tensor(0.1532)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9672)
features.14.conv.3 tensor(0.1888)
features.14.conv.6 tensor(0.9394)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1275)
features.15.conv.6 tensor(0.9879)
features.16.conv.0 tensor(0.1435)
features.16.conv.3 tensor(0.1715)
features.16.conv.6 tensor(0.5029)
conv.0 tensor(0.6639)
tensor(1246813.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  54
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [54][   20/  196]   Loss 0.273036   Top1 90.195312   Top5 98.554688   BatchTime 0.356839   LR 0.000427
INFO - Training [54][   40/  196]   Loss 0.268544   Top1 90.449219   Top5 98.720703   BatchTime 0.346520   LR 0.000423
INFO - Training [54][   60/  196]   Loss 0.263525   Top1 90.839844   Top5 98.867188   BatchTime 0.346000   LR 0.000418
INFO - Training [54][   80/  196]   Loss 0.259068   Top1 90.976562   Top5 98.989258   BatchTime 0.345763   LR 0.000413
INFO - Training [54][  100/  196]   Loss 0.253148   Top1 91.164062   Top5 99.050781   BatchTime 0.348385   LR 0.000408
INFO - Training [54][  120/  196]   Loss 0.248651   Top1 91.347656   Top5 99.150391   BatchTime 0.347876   LR 0.000404
INFO - Training [54][  140/  196]   Loss 0.249326   Top1 91.344866   Top5 99.162946   BatchTime 0.347001   LR 0.000399
INFO - Training [54][  160/  196]   Loss 0.251339   Top1 91.242676   Top5 99.145508   BatchTime 0.345165   LR 0.000394
INFO - Training [54][  180/  196]   Loss 0.251326   Top1 91.284722   Top5 99.125434   BatchTime 0.343646   LR 0.000390
INFO - ==> Top1: 91.348    Top5: 99.128    Loss: 0.249
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [54][   20/   40]   Loss 0.304126   Top1 91.328125   Top5 99.589844   BatchTime 0.131713
INFO - Validation [54][   40/   40]   Loss 0.297979   Top1 91.070000   Top5 99.680000   BatchTime 0.102593
INFO - ==> Top1: 91.070    Top5: 99.680    Loss: 0.298
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  55
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0182)
features.1.conv.3 tensor(0.0521)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0156)
features.2.conv.3 tensor(0.0355)
features.2.conv.6 tensor(0.0420)
features.3.conv.0 tensor(0.0165)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0258)
features.4.conv.0 tensor(0.0322)
features.4.conv.3 tensor(0.0689)
features.4.conv.6 tensor(0.2568)
features.5.conv.0 tensor(0.0404)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.2985)
features.6.conv.0 tensor(0.0203)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0517)
features.7.conv.0 tensor(0.0533)
features.7.conv.3 tensor(0.1169)
features.7.conv.6 tensor(0.3145)
features.8.conv.0 tensor(0.0830)
features.8.conv.3 tensor(0.1227)
features.8.conv.6 tensor(0.3176)
features.9.conv.0 tensor(0.0911)
features.9.conv.3 tensor(0.1557)
features.9.conv.6 tensor(0.3593)
features.10.conv.0 tensor(0.0398)
features.10.conv.3 tensor(0.0903)
features.10.conv.6 tensor(0.2855)
features.11.conv.0 tensor(0.1885)
features.11.conv.3 tensor(0.1364)
features.11.conv.6 tensor(0.6269)
features.12.conv.0 tensor(0.4896)
features.12.conv.3 tensor(0.2099)
features.12.conv.6 tensor(0.6340)
features.13.conv.0 tensor(0.1050)
features.13.conv.3 tensor(0.1493)
features.13.conv.6 tensor(0.3700)
features.14.conv.0 tensor(0.9674)
features.14.conv.3 tensor(0.1898)
features.14.conv.6 tensor(0.9384)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1273)
features.15.conv.6 tensor(0.9876)
features.16.conv.0 tensor(0.1431)
features.16.conv.3 tensor(0.1706)
features.16.conv.6 tensor(0.5027)
conv.0 tensor(0.6647)
tensor(1246424.) 2188896.0
INFO - Training [55][   20/  196]   Loss 0.260893   Top1 90.527344   Top5 98.632812   BatchTime 0.352646   LR 0.000381
INFO - Training [55][   40/  196]   Loss 0.270853   Top1 90.429688   Top5 98.710938   BatchTime 0.337241   LR 0.000377
INFO - Training [55][   60/  196]   Loss 0.266645   Top1 90.683594   Top5 98.873698   BatchTime 0.336561   LR 0.000372
INFO - Training [55][   80/  196]   Loss 0.258840   Top1 91.035156   Top5 99.033203   BatchTime 0.340651   LR 0.000368
INFO - Training [55][  100/  196]   Loss 0.251543   Top1 91.304688   Top5 99.062500   BatchTime 0.343197   LR 0.000363
INFO - Training [55][  120/  196]   Loss 0.247218   Top1 91.455078   Top5 99.124349   BatchTime 0.343121   LR 0.000358
INFO - Training [55][  140/  196]   Loss 0.244709   Top1 91.492746   Top5 99.162946   BatchTime 0.343302   LR 0.000354
INFO - Training [55][  160/  196]   Loss 0.246889   Top1 91.391602   Top5 99.157715   BatchTime 0.343501   LR 0.000349
INFO - Training [55][  180/  196]   Loss 0.246758   Top1 91.391059   Top5 99.112413   BatchTime 0.343290   LR 0.000345
INFO - ==> Top1: 91.430    Top5: 99.114    Loss: 0.246
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [55][   20/   40]   Loss 0.331683   Top1 90.273438   Top5 99.648438   BatchTime 0.144936
INFO - Validation [55][   40/   40]   Loss 0.312540   Top1 90.580000   Top5 99.720000   BatchTime 0.103450
features.0.conv.0 tensor(0.3993)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0195)
features.1.conv.3 tensor(0.0486)
features.1.conv.6 tensor(0.0291)
features.2.conv.0 tensor(0.0194)
features.2.conv.3 tensor(0.0394)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0179)
features.3.conv.3 tensor(0.0278)
features.3.conv.6 tensor(0.0267)
features.4.conv.0 tensor(0.0303)
features.4.conv.3
INFO - ==> Top1: 90.580    Top5: 99.720    Loss: 0.313
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  56
features.4.conv.3 tensor(0.0683)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0384)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.2985)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0324)
features.6.conv.6 tensor(0.0513)
features.7.conv.0 tensor(0.0529)
features.7.conv.3 tensor(0.1172)
features.7.conv.6 tensor(0.3152)
features.8.conv.0 tensor(0.0834)
features.8.conv.3 tensor(0.1259)
features.8.conv.6 tensor(0.3176)
features.9.conv.0 tensor(0.0900)
features.9.conv.3 tensor(0.1554)
features.9.conv.6 tensor(0.3596)
features.10.conv.0 tensor(0.0401)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2856)
features.11.conv.0 tensor(0.1905)
features.11.conv.3 tensor(0.1360)
features.11.conv.6 tensor(0.6270)
features.12.conv.0 tensor(0.4894)
features.12.conv.3 tensor(0.2083)
features.12.conv.6 tensor(0.6342)
features.13.conv.0 tensor(0.1081)
features.13.conv.3 tensor(0.1503)
features.13.conv.6 tensor(0.3699)
features.14.conv.0 tensor(0.9673)
features.14.conv.3 tensor(0.1919)
features.14.conv.6 tensor(0.9374)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1294)
features.15.conv.6 tensor(0.9881)
features.16.conv.0 tensor(0.1424)
features.16.conv.3 tensor(0.1690)
features.16.conv.6 tensor(0.5027)
conv.0 tensor(0.6646)
tensor(1246534.) 2188896.0
INFO - Training [56][   20/  196]   Loss 0.267751   Top1 90.859375   Top5 98.828125   BatchTime 0.348685   LR 0.000337
INFO - Training [56][   40/  196]   Loss 0.265079   Top1 90.869141   Top5 98.876953   BatchTime 0.346874   LR 0.000333
INFO - Training [56][   60/  196]   Loss 0.259105   Top1 91.074219   Top5 98.964844   BatchTime 0.345304   LR 0.000328
INFO - Training [56][   80/  196]   Loss 0.257283   Top1 90.986328   Top5 99.062500   BatchTime 0.345889   LR 0.000324
INFO - Training [56][  100/  196]   Loss 0.247970   Top1 91.320312   Top5 99.136719   BatchTime 0.341798   LR 0.000319
INFO - Training [56][  120/  196]   Loss 0.243948   Top1 91.432292   Top5 99.153646   BatchTime 0.344858   LR 0.000315
INFO - Training [56][  140/  196]   Loss 0.242619   Top1 91.470424   Top5 99.213170   BatchTime 0.345293   LR 0.000311
INFO - Training [56][  160/  196]   Loss 0.245516   Top1 91.350098   Top5 99.194336   BatchTime 0.343967   LR 0.000306
INFO - Training [56][  180/  196]   Loss 0.244582   Top1 91.347656   Top5 99.166667   BatchTime 0.343889   LR 0.000302
INFO - ==> Top1: 91.366    Top5: 99.146    Loss: 0.244
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [56][   20/   40]   Loss 0.310181   Top1 90.820312   Top5 99.726562   BatchTime 0.157024
INFO - Validation [56][   40/   40]   Loss 0.291732   Top1 91.290000   Top5 99.790000   BatchTime 0.107513
features.0.conv.0 tensor(0.3924)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0486)
features.1.conv.6 tensor(0.0299)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0370)
features.2.conv.6 tensor(0.0440)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0270)
features.3.conv.6 tensor(0.0263)
features.4.conv.0 tensor(0.0314)
features.4.conv.3 tensor(0.0677)
features.4.conv.6 tensor(0.2573)
features.5.conv.0 tensor(0.0371)
features.5.conv.3 tensor(0.0654)
features.5.conv.6 tensor(0.2985)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0312)
features.6.conv.6 tensor(0.0526)
features.7.conv.0 tensor(0.0544)
features.7.conv.3 tensor(0.1152)
features.7.conv.6 tensor(0.3156)
features.8.conv.0 tensor(0.0850)
features.8.conv.3 tensor(0.1233)
features.8.conv.6 tensor(0.3177)
features.9.conv.0 tensor(0.0912)
features.9.conv.3 tensor(0.1542)
features.9.conv.6 tensor(0.3599)
features.10.conv.0 tensor(0.0398)
features.10.conv.3 tensor(0.0911)
features.10.conv.6 tensor(0.2858)
features.11.conv.0 tensor(0.1912)
features.11.conv.3 tensor(0.1356)
features.11.conv.6 tensor(0.6268)
features.12.conv.0 tensor(0.4886)
features.12.conv.3 tensor(0.2079)
features.12.conv.6 tensor(0.6339)
features.13.conv.0 tensor(0.1082)
features.13.conv.3 tensor(0.1483)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1924)
features.14.conv.6 tensor(0.9389)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1278)
features.15.conv.6 tensor(0.9876)
INFO - ==> Top1: 91.290    Top5: 99.790    Loss: 0.292
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  57
INFO - Training: 50000 samples (256 per mini-batch)
features.16.conv.0 tensor(0.1419)
features.16.conv.3 tensor(0.1697)
features.16.conv.6 tensor(0.5020)
conv.0 tensor(0.6645)
tensor(1246355.) 2188896.0
INFO - Training [57][   20/  196]   Loss 0.261273   Top1 90.976562   Top5 98.750000   BatchTime 0.365719   LR 0.000294
INFO - Training [57][   40/  196]   Loss 0.250839   Top1 91.142578   Top5 98.896484   BatchTime 0.353490   LR 0.000290
INFO - Training [57][   60/  196]   Loss 0.244908   Top1 91.445312   Top5 98.971354   BatchTime 0.348191   LR 0.000286
INFO - Training [57][   80/  196]   Loss 0.246261   Top1 91.425781   Top5 99.125977   BatchTime 0.351826   LR 0.000282
INFO - Training [57][  100/  196]   Loss 0.241812   Top1 91.656250   Top5 99.140625   BatchTime 0.349988   LR 0.000277
INFO - Training [57][  120/  196]   Loss 0.238670   Top1 91.822917   Top5 99.176432   BatchTime 0.347676   LR 0.000273
INFO - Training [57][  140/  196]   Loss 0.237751   Top1 91.833147   Top5 99.218750   BatchTime 0.345785   LR 0.000269
INFO - Training [57][  160/  196]   Loss 0.240359   Top1 91.711426   Top5 99.211426   BatchTime 0.345223   LR 0.000265
INFO - Training [57][  180/  196]   Loss 0.243799   Top1 91.603733   Top5 99.168837   BatchTime 0.344833   LR 0.000261
INFO - ==> Top1: 91.608    Top5: 99.164    Loss: 0.243
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [57][   20/   40]   Loss 0.293011   Top1 91.347656   Top5 99.726562   BatchTime 0.143929
INFO - Validation [57][   40/   40]   Loss 0.281505   Top1 91.450000   Top5 99.790000   BatchTime 0.100413
INFO - ==> Top1: 91.450    Top5: 99.790    Loss: 0.282
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  58
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3958)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0486)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0386)
features.2.conv.6 tensor(0.0451)
features.3.conv.0 tensor(0.0182)
features.3.conv.3 tensor(0.0255)
features.3.conv.6 tensor(0.0250)
features.4.conv.0 tensor(0.0311)
features.4.conv.3 tensor(0.0654)
features.4.conv.6 tensor(0.2575)
features.5.conv.0 tensor(0.0379)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.2985)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0523)
features.7.conv.0 tensor(0.0554)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.3158)
features.8.conv.0 tensor(0.0829)
features.8.conv.3 tensor(0.1250)
features.8.conv.6 tensor(0.3179)
features.9.conv.0 tensor(0.0903)
features.9.conv.3 tensor(0.1554)
features.9.conv.6 tensor(0.3599)
features.10.conv.0 tensor(0.0391)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.2859)
features.11.conv.0 tensor(0.1900)
features.11.conv.3 tensor(0.1360)
features.11.conv.6 tensor(0.6266)
features.12.conv.0 tensor(0.4892)
features.12.conv.3 tensor(0.2101)
features.12.conv.6 tensor(0.6338)
features.13.conv.0 tensor(0.1081)
features.13.conv.3 tensor(0.1512)
features.13.conv.6 tensor(0.3694)
features.14.conv.0 tensor(0.9676)
features.14.conv.3 tensor(0.1889)
features.14.conv.6 tensor(0.9370)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1279)
features.15.conv.6 tensor(0.9877)
features.16.conv.0 tensor(0.1413)
features.16.conv.3 tensor(0.1693)
features.16.conv.6 tensor(0.5017)
conv.0 tensor(0.6640)
tensor(1245659.) 2188896.0
INFO - Training [58][   20/  196]   Loss 0.258446   Top1 90.703125   Top5 98.691406   BatchTime 0.368115   LR 0.000254
INFO - Training [58][   40/  196]   Loss 0.273015   Top1 90.488281   Top5 98.818359   BatchTime 0.366853   LR 0.000250
INFO - Training [58][   60/  196]   Loss 0.262638   Top1 90.930990   Top5 98.873698   BatchTime 0.359455   LR 0.000246
INFO - Training [58][   80/  196]   Loss 0.257404   Top1 91.059570   Top5 99.047852   BatchTime 0.354718   LR 0.000242
INFO - Training [58][  100/  196]   Loss 0.249532   Top1 91.308594   Top5 99.097656   BatchTime 0.353313   LR 0.000238
INFO - Training [58][  120/  196]   Loss 0.244785   Top1 91.484375   Top5 99.163411   BatchTime 0.351011   LR 0.000234
INFO - Training [58][  140/  196]   Loss 0.242302   Top1 91.587612   Top5 99.227121   BatchTime 0.349088   LR 0.000230
INFO - Training [58][  160/  196]   Loss 0.242070   Top1 91.601562   Top5 99.206543   BatchTime 0.347944   LR 0.000226
INFO - Training [58][  180/  196]   Loss 0.243094   Top1 91.558160   Top5 99.190538   BatchTime 0.347313   LR 0.000222
INFO - ==> Top1: 91.564    Top5: 99.168    Loss: 0.242
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [58][   20/   40]   Loss 0.326967   Top1 90.429688   Top5 99.648438   BatchTime 0.172125
features.0.conv.0 tensor(0.4201)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0486)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0417)
features.2.conv.6 tensor(0.0414)
features.3.conv.0 tensor(0.0174)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0239)
features.4.conv.0 tensor(0.0327)
features.4.conv.3 tensor(0.0683)
features.4.conv.6 tensor(0.2573)
features.5.conv.0 tensor(0.0378)
features.5.conv.3 tensor(0.0700)
features.5.conv.6 tensor(0.2985)
features.6.conv.0 tensor(0.0223)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0553)
features.7.conv.3 tensor(0.1146)
features.7.conv.6 tensor(0.3157)
features.8.conv.0 tensor(0.0825)
features.8.conv.3 tensor(0.1253)
features.8.conv.6 tensor(0.3179)
features.9.conv.0 tensor(0.0906)
features.9.conv.3 tensor(0.1571)
features.9.conv.6 tensor(0.3600)
features.10.conv.0 tensor(0.0397)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.2859)
features.11.conv.0 tensor(0.1901)
features.11.conv.3 tensor(0.1352)
features.11.conv.6 tensor(0.6265)
features.12.conv.0 tensor(0.4895)
features.12.conv.3 tensor(0.2093)
features.12.conv.6 tensor(0.6342)
features.13.conv.0 tensor(0.1085)
features.13.conv.3 tensor(0.1497)
features.13.conv.6 tensor(0.3693)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1899)
features.14.conv.6 tensor(0.9383)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1282)
features.15.conv.6 tensor(0.9879)
features.16.conv.0 tensor(0.1406)
features.16.conv.3 tensor(0.1686)
features.16.conv.6 tensor(0.5027)
conv.0 tensor(0.6639)
tensor(1246069.) 2188896.0
INFO - Validation [58][   40/   40]   Loss 0.315623   Top1 90.610000   Top5 99.730000   BatchTime 0.111336
INFO - ==> Top1: 90.610    Top5: 99.730    Loss: 0.316
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 91.570   Top5: 99.730]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  59
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [59][   20/  196]   Loss 0.273748   Top1 90.488281   Top5 98.593750   BatchTime 0.392767   LR 0.000215
INFO - Training [59][   40/  196]   Loss 0.261264   Top1 90.947266   Top5 98.818359   BatchTime 0.365701   LR 0.000212
INFO - Training [59][   60/  196]   Loss 0.252846   Top1 91.132812   Top5 98.912760   BatchTime 0.353224   LR 0.000208
INFO - Training [59][   80/  196]   Loss 0.248053   Top1 91.279297   Top5 99.101562   BatchTime 0.351650   LR 0.000204
INFO - Training [59][  100/  196]   Loss 0.244671   Top1 91.449219   Top5 99.109375   BatchTime 0.348420   LR 0.000201
INFO - Training [59][  120/  196]   Loss 0.239717   Top1 91.650391   Top5 99.156901   BatchTime 0.347720   LR 0.000197
INFO - Training [59][  140/  196]   Loss 0.238872   Top1 91.699219   Top5 99.188058   BatchTime 0.347693   LR 0.000193
INFO - Training [59][  160/  196]   Loss 0.239593   Top1 91.647949   Top5 99.199219   BatchTime 0.346496   LR 0.000190
INFO - Training [59][  180/  196]   Loss 0.238638   Top1 91.697049   Top5 99.197049   BatchTime 0.348397   LR 0.000186
********************pre-trained*****************
INFO - ==> Top1: 91.710    Top5: 99.196    Loss: 0.238
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [59][   20/   40]   Loss 0.286536   Top1 91.816406   Top5 99.726562   BatchTime 0.153195
features.0.conv.0 tensor(0.4236)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0156)
features.1.conv.3 tensor(0.0521)
features.1.conv.6 tensor(0.0295)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0378)
features.2.conv.6 tensor(0.0440)
features.3.conv.0 tensor(0.0177)
features.3.conv.3 tensor(0.0239)
features.3.conv.6 tensor(0.0254)
features.4.conv.0 tensor(0.0314)
features.4.conv.3 tensor(0.0677)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0368)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.2985)
features.6.conv.0 tensor(0.0239)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0522)
features.7.conv.0 tensor(0.0543)
features.7.conv.3 tensor(0.1149)
features.7.conv.6 tensor(0.3158)
features.8.conv.0 tensor(0.0830)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.3179)
features.9.conv.0 tensor(0.0913)
features.9.conv.3 tensor(0.1580)
features.9.conv.6 tensor(0.3599)
features.10.conv.0 tensor(0.0399)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.2860)
features.11.conv.0 tensor(0.1901)
features.11.conv.3 tensor(0.1352)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4893)
features.12.conv.3 tensor(0.2087)
features.12.conv.6 tensor(0.6340)
features.13.conv.0 tensor(0.1072)
features.13.conv.3 tensor(0.1481)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1902)
features.14.conv.6 tensor(0.9382)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1279)
features.15.conv.6 tensor(0.9874)
features.16.conv.0 tensor(0.1384)
features.16.conv.3 tensor(0.1690)
features.16.conv.6 tensor(0.5019)
conv.0 tensor(0.6639)
tensor(1245296.) 2188896.0
INFO - Validation [59][   40/   40]   Loss 0.272076   Top1 91.920000   Top5 99.810000   BatchTime 0.104102
INFO - ==> Top1: 91.920    Top5: 99.810    Loss: 0.272
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  60
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [60][   20/  196]   Loss 0.262916   Top1 90.527344   Top5 98.769531   BatchTime 0.397784   LR 0.000180
INFO - Training [60][   40/  196]   Loss 0.262245   Top1 90.693359   Top5 98.798828   BatchTime 0.367412   LR 0.000176
INFO - Training [60][   60/  196]   Loss 0.247280   Top1 91.269531   Top5 98.951823   BatchTime 0.359707   LR 0.000173
INFO - Training [60][   80/  196]   Loss 0.245688   Top1 91.381836   Top5 99.077148   BatchTime 0.357515   LR 0.000169
INFO - Training [60][  100/  196]   Loss 0.239758   Top1 91.570312   Top5 99.140625   BatchTime 0.355418   LR 0.000166
INFO - Training [60][  120/  196]   Loss 0.235554   Top1 91.751302   Top5 99.215495   BatchTime 0.355304   LR 0.000162
INFO - Training [60][  140/  196]   Loss 0.234866   Top1 91.813616   Top5 99.268973   BatchTime 0.353678   LR 0.000159
INFO - Training [60][  160/  196]   Loss 0.238965   Top1 91.704102   Top5 99.252930   BatchTime 0.351621   LR 0.000156
INFO - Training [60][  180/  196]   Loss 0.238405   Top1 91.720920   Top5 99.238281   BatchTime 0.349750   LR 0.000152
********************pre-trained*****************
INFO - ==> Top1: 91.798    Top5: 99.230    Loss: 0.236
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4271)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0150)
features.1.conv.3 tensor(0.0521)
features.1.conv.6 tensor(0.0286)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0347)
features.2.conv.6 tensor(0.0428)
features.3.conv.0 tensor(0.0188)
features.3.conv.3 tensor(0.0247)
features.3.conv.6 tensor(0.0234)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.0683)
features.4.conv.6 tensor(0.2573)
features.5.conv.0 tensor(0.0376)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2985)
features.6.conv.0 tensor(0.0238)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0536)
features.7.conv.3 tensor(0.1137)
features.7.conv.6 tensor(0.3159)
features.8.conv.0 tensor(0.0818)
features.8.conv.3 tensor(0.1224)
features.8.conv.6 tensor(0.3180)
features.9.conv.0 tensor(0.0917)
features.9.conv.3 tensor(0.1557)
features.9.conv.6 tensor(0.3599)
features.10.conv.0 tensor(0.0396)
features.10.conv.3 tensor(0.0935)
features.10.conv.6 tensor(0.2860)
features.11.conv.0 tensor(0.1894)
features.11.conv.3 tensor(0.1345)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4900)
features.12.conv.3 tensor(0.2091)
features.12.conv.6 tensor(0.6339)
features.13.conv.0 tensor(0.1066)
features.13.conv.3 tensor(0.1495)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1905)
features.14.conv.6
INFO - Validation [60][   20/   40]   Loss 0.284669   Top1 91.718750   Top5 99.726562   BatchTime 0.147212
features.14.conv.6 tensor(0.9391)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1270)
features.15.conv.6 tensor(0.9869)
features.16.conv.0 tensor(0.1386)
features.16.conv.3 tensor(0.1704)
features.16.conv.6 tensor(0.5020)
conv.0 tensor(0.6640)
tensor(1245341.) 2188896.0
INFO - ==> Top1: 91.610    Top5: 99.790    Loss: 0.277
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  61
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [61][   20/  196]   Loss 0.249769   Top1 91.035156   Top5 98.789062   BatchTime 0.375657   LR 0.000147
INFO - Training [61][   40/  196]   Loss 0.246295   Top1 91.181641   Top5 98.916016   BatchTime 0.363156   LR 0.000143
INFO - Training [61][   60/  196]   Loss 0.244836   Top1 91.373698   Top5 98.990885   BatchTime 0.356521   LR 0.000140
INFO - Training [61][   80/  196]   Loss 0.244026   Top1 91.440430   Top5 99.101562   BatchTime 0.357210   LR 0.000137
INFO - Training [61][  100/  196]   Loss 0.237766   Top1 91.652344   Top5 99.128906   BatchTime 0.354842   LR 0.000134
INFO - Training [61][  120/  196]   Loss 0.233147   Top1 91.868490   Top5 99.176432   BatchTime 0.352673   LR 0.000131
INFO - Training [61][  140/  196]   Loss 0.231467   Top1 91.933594   Top5 99.218750   BatchTime 0.351789   LR 0.000128
INFO - Training [61][  160/  196]   Loss 0.233595   Top1 91.848145   Top5 99.191895   BatchTime 0.351293   LR 0.000125
INFO - Training [61][  180/  196]   Loss 0.233124   Top1 91.866319   Top5 99.171007   BatchTime 0.350614   LR 0.000122
********************pre-trained*****************
INFO - ==> Top1: 91.830    Top5: 99.174    Loss: 0.233
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [61][   20/   40]   Loss 0.288450   Top1 91.445312   Top5 99.648438   BatchTime 0.157444
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0143)
features.1.conv.3 tensor(0.0532)
features.1.conv.6 tensor(0.0295)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0194)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0245)
features.4.conv.0 tensor(0.0314)
features.4.conv.3 tensor(0.0660)
features.4.conv.6 tensor(0.2573)
features.5.conv.0 tensor(0.0360)
features.5.conv.3 tensor(0.0683)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0228)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0527)
features.7.conv.0 tensor(0.0540)
features.7.conv.3 tensor(0.1117)
features.7.conv.6 tensor(0.3160)
features.8.conv.0 tensor(0.0815)
features.8.conv.3 tensor(0.1218)
features.8.conv.6 tensor(0.3177)
features.9.conv.0 tensor(0.0916)
features.9.conv.3 tensor(0.1571)
features.9.conv.6 tensor(0.3599)
features.10.conv.0 tensor(0.0395)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2861)
features.11.conv.0 tensor(0.1885)
features.11.conv.3 tensor(0.1366)
features.11.conv.6 tensor(0.6265)
features.12.conv.0 tensor(0.4895)
features.12.conv.3 tensor(0.2081)
features.12.conv.6 tensor(0.6338)
features.13.conv.0 tensor(0.1067)
features.13.conv.3 tensor(0.1491)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9673)
features.14.conv.3 tensor(0.1899)
features.14.conv.6 tensor(0.9383)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1282)
features.15.conv.6 tensor(0.9869)
features.16.conv.0 tensor(0.1384)
features.16.conv.3 tensor(0.1704)
features.16.conv.6 tensor(0.5022)
conv.0 tensor(0.6636)
tensor(1245027.) 2188896.0
INFO - Validation [61][   40/   40]   Loss 0.278455   Top1 91.550000   Top5 99.780000   BatchTime 0.105713
INFO - ==> Top1: 91.550    Top5: 99.780    Loss: 0.278
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  62
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [62][   20/  196]   Loss 0.246571   Top1 91.445312   Top5 98.867188   BatchTime 0.429886   LR 0.000117
INFO - Training [62][   40/  196]   Loss 0.247754   Top1 91.445312   Top5 98.896484   BatchTime 0.394770   LR 0.000114
INFO - Training [62][   60/  196]   Loss 0.245178   Top1 91.562500   Top5 98.932292   BatchTime 0.373302   LR 0.000111
INFO - Training [62][   80/  196]   Loss 0.244611   Top1 91.567383   Top5 99.057617   BatchTime 0.366268   LR 0.000108
INFO - Training [62][  100/  196]   Loss 0.233048   Top1 91.964844   Top5 99.152344   BatchTime 0.360864   LR 0.000105
INFO - Training [62][  120/  196]   Loss 0.228757   Top1 92.106120   Top5 99.195964   BatchTime 0.358517   LR 0.000102
INFO - Training [62][  140/  196]   Loss 0.226208   Top1 92.170759   Top5 99.255022   BatchTime 0.356128   LR 0.000100
INFO - Training [62][  160/  196]   Loss 0.230348   Top1 91.994629   Top5 99.223633   BatchTime 0.355394   LR 0.000097
INFO - Training [62][  180/  196]   Loss 0.230590   Top1 92.003038   Top5 99.218750   BatchTime 0.354450   LR 0.000094
********************pre-trained*****************
INFO - ==> Top1: 92.002    Top5: 99.206    Loss: 0.230
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [62][   20/   40]   Loss 0.344696   Top1 89.980469   Top5 99.589844   BatchTime 0.154970
features.0.conv.0 tensor(0.4306)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0544)
features.1.conv.6 tensor(0.0304)
features.2.conv.0 tensor(0.0153)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0197)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0245)
features.4.conv.0 tensor(0.0309)
features.4.conv.3 tensor(0.0654)
features.4.conv.6 tensor(0.2573)
features.5.conv.0 tensor(0.0365)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0239)
features.6.conv.3 tensor(0.0318)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0538)
features.7.conv.3 tensor(0.1157)
features.7.conv.6 tensor(0.3161)
features.8.conv.0 tensor(0.0823)
features.8.conv.3 tensor(0.1209)
features.8.conv.6 tensor(0.3178)
features.9.conv.0 tensor(0.0919)
features.9.conv.3 tensor(0.1560)
features.9.conv.6 tensor(0.3597)
features.10.conv.0 tensor(0.0398)
features.10.conv.3 tensor(0.0926)
features.10.conv.6 tensor(0.2861)
features.11.conv.0 tensor(0.1887)
features.11.conv.3 tensor(0.1354)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4896)
features.12.conv.3 tensor(0.2085)
features.12.conv.6 tensor(0.6339)
features.13.conv.0 tensor(0.1070)
features.13.conv.3 tensor(0.1489)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9674)
features.14.conv.3 tensor(0.1911)
features.14.conv.6 tensor(0.9384)
features.15.conv.0 tensor(0.9798)
features.15.conv.3 tensor(0.1282)
features.15.conv.6 tensor(0.9865)
features.16.conv.0 tensor(0.1388)
features.16.conv.3 tensor(0.1692)
features.16.conv.6 tensor(0.5019)
conv.0 tensor(0.6635)
tensor(1244923.) 2188896.0
INFO - Validation [62][   40/   40]   Loss 0.333738   Top1 90.180000   Top5 99.680000   BatchTime 0.111152
INFO - ==> Top1: 90.180    Top5: 99.680    Loss: 0.334
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  63
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [63][   20/  196]   Loss 0.246199   Top1 91.132812   Top5 98.554688   BatchTime 0.413850   LR 0.000090
INFO - Training [63][   40/  196]   Loss 0.250698   Top1 91.054688   Top5 98.808594   BatchTime 0.380793   LR 0.000087
INFO - Training [63][   60/  196]   Loss 0.247089   Top1 91.158854   Top5 98.932292   BatchTime 0.371182   LR 0.000085
INFO - Training [63][   80/  196]   Loss 0.243140   Top1 91.362305   Top5 99.023438   BatchTime 0.367185   LR 0.000082
INFO - Training [63][  100/  196]   Loss 0.233096   Top1 91.714844   Top5 99.082031   BatchTime 0.362268   LR 0.000080
INFO - Training [63][  120/  196]   Loss 0.226181   Top1 91.995443   Top5 99.156901   BatchTime 0.359793   LR 0.000077
INFO - Training [63][  140/  196]   Loss 0.223973   Top1 92.073103   Top5 99.218750   BatchTime 0.357064   LR 0.000075
INFO - Training [63][  160/  196]   Loss 0.226972   Top1 92.006836   Top5 99.211426   BatchTime 0.357699   LR 0.000072
INFO - Training [63][  180/  196]   Loss 0.227151   Top1 92.018229   Top5 99.184028   BatchTime 0.356584   LR 0.000070
********************pre-trained*****************
INFO - ==> Top1: 92.068    Top5: 99.182    Loss: 0.226
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [63][   20/   40]   Loss 0.294981   Top1 91.367188   Top5 99.707031   BatchTime 0.199707
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0176)
features.1.conv.3 tensor(0.0544)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0162)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0428)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0255)
features.3.conv.6 tensor(0.0252)
features.4.conv.0 tensor(0.0311)
features.4.conv.3 tensor(0.0671)
features.4.conv.6 tensor(0.2573)
features.5.conv.0 tensor(0.0356)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0528)
features.7.conv.0 tensor(0.0533)
features.7.conv.3 tensor(0.1149)
features.7.conv.6 tensor(0.3160)
features.8.conv.0 tensor(0.0827)
features.8.conv.3 tensor(0.1218)
features.8.conv.6 tensor(0.3178)
features.9.conv.0 tensor(0.0913)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.3598)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2861)
features.11.conv.0 tensor(0.1876)
features.11.conv.3 tensor(0.1341)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4896)
features.12.conv.3 tensor(0.2081)
features.12.conv.6 tensor(0.6337)
features.13.conv.0 tensor(0.1071)
features.13.conv.3 tensor(0.1485)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9673)
features.14.conv.3 tensor(0.1905)
features.14.conv.6 tensor(0.9373)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1280)
features.15.conv.6 tensor(0.9870)
features.16.conv.0 tensor(0.1390)
features.16.conv.3 tensor(0.1688)
features.16.conv.6 tensor(0.5017)
conv.0 tensor(0.6633)
tensor(1244674.) 2188896.0
INFO - Validation [63][   40/   40]   Loss 0.286536   Top1 91.540000   Top5 99.760000   BatchTime 0.138653
INFO - ==> Top1: 91.540    Top5: 99.760    Loss: 0.287
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  64
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [64][   20/  196]   Loss 0.245485   Top1 91.210938   Top5 98.515625   BatchTime 0.462448   LR 0.000066
INFO - Training [64][   40/  196]   Loss 0.239046   Top1 91.494141   Top5 98.740234   BatchTime 0.400082   LR 0.000064
INFO - Training [64][   60/  196]   Loss 0.236312   Top1 91.582031   Top5 98.938802   BatchTime 0.377426   LR 0.000062
INFO - Training [64][   80/  196]   Loss 0.234293   Top1 91.694336   Top5 99.072266   BatchTime 0.371735   LR 0.000059
INFO - Training [64][  100/  196]   Loss 0.227945   Top1 91.859375   Top5 99.136719   BatchTime 0.368341   LR 0.000057
INFO - Training [64][  120/  196]   Loss 0.226073   Top1 91.985677   Top5 99.202474   BatchTime 0.365217   LR 0.000055
INFO - Training [64][  140/  196]   Loss 0.224547   Top1 92.061942   Top5 99.277344   BatchTime 0.363287   LR 0.000053
INFO - Training [64][  160/  196]   Loss 0.226921   Top1 91.992188   Top5 99.265137   BatchTime 0.361859   LR 0.000051
INFO - Training [64][  180/  196]   Loss 0.227428   Top1 91.979167   Top5 99.253472   BatchTime 0.360310   LR 0.000049
INFO - ==> Top1: 92.016    Top5: 99.234    Loss: 0.227
********************pre-trained*****************
validation quantized model on cpu
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [64][   20/   40]   Loss 0.309422   Top1 90.937500   Top5 99.648438   BatchTime 0.157311
features.0.conv.0 tensor(0.4410)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0308)
features.2.conv.0 tensor(0.0174)
features.2.conv.3 tensor(0.0370)
features.2.conv.6 tensor(0.0434)
features.3.conv.0 tensor(0.0203)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0250)
features.4.conv.0 tensor(0.0317)
features.4.conv.3 tensor(0.0666)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0358)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0234)
features.6.conv.3 tensor(0.0295)
features.6.conv.6 tensor(0.0524)
features.7.conv.0 tensor(0.0533)
features.7.conv.3 tensor(0.1140)
features.7.conv.6 tensor(0.3162)
features.8.conv.0 tensor(0.0826)
features.8.conv.3 tensor(0.1221)
features.8.conv.6 tensor(0.3179)
features.9.conv.0 tensor(0.0916)
features.9.conv.3 tensor(0.1574)
features.9.conv.6 tensor(0.3598)
features.10.conv.0 tensor(0.0397)
features.10.conv.3 tensor(0.0923)
features.10.conv.6 tensor(0.2861)
features.11.conv.0 tensor(0.1877)
features.11.conv.3 tensor(0.1335)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4898)
features.12.conv.3 tensor(0.2078)
features.12.conv.6 tensor(0.6336)
features.13.conv.0 tensor(0.1065)
features.13.conv.3 tensor(0.1499)
features.13.conv.6 tensor(0.3695)
features.14.conv.0 tensor(0.9674)
features.14.conv.3 tensor(0.1898)
features.14.conv.6 tensor(0.9375)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1285)
features.15.conv.6 tensor(0.9871)
features.16.conv.0 tensor(0.1388)
features.16.conv.3 tensor(0.1693)
features.16.conv.6 tensor(0.5015)
conv.0 tensor(0.6635)
tensor(1244695.) 2188896.0
INFO - Validation [64][   40/   40]   Loss 0.302381   Top1 91.020000   Top5 99.730000   BatchTime 0.117663
INFO - ==> Top1: 91.020    Top5: 99.730    Loss: 0.302
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  65
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [65][   20/  196]   Loss 0.250752   Top1 91.191406   Top5 98.535156   BatchTime 0.435496   LR 0.000046
INFO - Training [65][   40/  196]   Loss 0.249092   Top1 91.435547   Top5 98.730469   BatchTime 0.394032   LR 0.000044
INFO - Training [65][   60/  196]   Loss 0.240009   Top1 91.608073   Top5 98.880208   BatchTime 0.376745   LR 0.000042
INFO - Training [65][   80/  196]   Loss 0.240184   Top1 91.655273   Top5 99.028320   BatchTime 0.366721   LR 0.000040
INFO - Training [65][  100/  196]   Loss 0.232809   Top1 91.843750   Top5 99.105469   BatchTime 0.360184   LR 0.000039
INFO - Training [65][  120/  196]   Loss 0.227857   Top1 92.018229   Top5 99.166667   BatchTime 0.359289   LR 0.000037
INFO - Training [65][  140/  196]   Loss 0.227553   Top1 92.061942   Top5 99.210379   BatchTime 0.356473   LR 0.000035
INFO - Training [65][  160/  196]   Loss 0.229037   Top1 92.048340   Top5 99.208984   BatchTime 0.353770   LR 0.000033
INFO - Training [65][  180/  196]   Loss 0.229031   Top1 92.016059   Top5 99.177517   BatchTime 0.352244   LR 0.000032
********************pre-trained*****************
INFO - ==> Top1: 92.038    Top5: 99.174    Loss: 0.229
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [65][   20/   40]   Loss 0.288225   Top1 91.660156   Top5 99.667969   BatchTime 0.178680
features.0.conv.0 tensor(0.4340)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0177)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0254)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.0666)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0358)
features.5.conv.3 tensor(0.0677)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0231)
features.6.conv.3 tensor(0.0301)
features.6.conv.6 tensor(0.0526)
features.7.conv.0 tensor(0.0535)
features.7.conv.3 tensor(0.1134)
features.7.conv.6 tensor(0.3160)
features.8.conv.0 tensor(0.0827)
features.8.conv.3 tensor(0.1215)
features.8.conv.6 tensor(0.3179)
features.9.conv.0 tensor(0.0915)
features.9.conv.3 tensor(0.1554)
features.9.conv.6 tensor(0.3598)
features.10.conv.0 tensor(0.0397)
features.10.conv.3 tensor(0.0929)
features.10.conv.6 tensor(0.2862)
features.11.conv.0 tensor(0.1876)
features.11.conv.3 tensor(0.1350)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4896)
features.12.conv.3 tensor(0.2093)
features.12.conv.6 tensor(0.6336)
features.13.conv.0 tensor(0.1066)
features.13.conv.3 tensor(0.1497)
features.13.conv.6 tensor(0.3696)
features.14.conv.0 tensor(0.9676)
features.14.conv.3 tensor(0.1903)
features.14.conv.6 tensor(0.9382)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1280)
features.15.conv.6 tensor(0.9868)
features.16.conv.0 tensor(0.1388)
features.16.conv.3 tensor(0.1704)
features.16.conv.6 tensor(0.5014)
conv.0 tensor(0.6635)
tensor(1244787.) 2188896.0
INFO - Validation [65][   40/   40]   Loss 0.274153   Top1 91.770000   Top5 99.730000   BatchTime 0.151210
INFO - ==> Top1: 91.770    Top5: 99.730    Loss: 0.274
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  66
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [66][   20/  196]   Loss 0.250464   Top1 91.250000   Top5 98.769531   BatchTime 0.473535   LR 0.000029
INFO - Training [66][   40/  196]   Loss 0.249428   Top1 91.250000   Top5 98.847656   BatchTime 0.402733   LR 0.000028
INFO - Training [66][   60/  196]   Loss 0.245639   Top1 91.419271   Top5 98.971354   BatchTime 0.381171   LR 0.000026
INFO - Training [66][   80/  196]   Loss 0.238584   Top1 91.606445   Top5 99.121094   BatchTime 0.369426   LR 0.000025
INFO - Training [66][  100/  196]   Loss 0.232186   Top1 91.929688   Top5 99.179688   BatchTime 0.367130   LR 0.000023
INFO - Training [66][  120/  196]   Loss 0.226905   Top1 92.154948   Top5 99.225260   BatchTime 0.363612   LR 0.000022
INFO - Training [66][  140/  196]   Loss 0.226756   Top1 92.176339   Top5 99.268973   BatchTime 0.358874   LR 0.000021
INFO - Training [66][  160/  196]   Loss 0.228060   Top1 92.084961   Top5 99.252930   BatchTime 0.357410   LR 0.000019
INFO - Training [66][  180/  196]   Loss 0.228805   Top1 92.035590   Top5 99.251302   BatchTime 0.351886   LR 0.000018
********************pre-trained*****************
INFO - ==> Top1: 92.040    Top5: 99.252    Loss: 0.227
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [66][   20/   40]   Loss 0.293746   Top1 91.796875   Top5 99.628906   BatchTime 0.136200
features.0.conv.0 tensor(0.4514)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0437)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0255)
features.3.conv.6 tensor(0.0254)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.0671)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0360)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0301)
features.6.conv.6 tensor(0.0523)
features.7.conv.0 tensor(0.0532)
features.7.conv.3 tensor(0.1131)
features.7.conv.6 tensor(0.3160)
features.8.conv.0 tensor(0.0830)
features.8.conv.3 tensor(0.1207)
features.8.conv.6 tensor(0.3178)
features.9.conv.0 tensor(0.0916)
features.9.conv.3 tensor(0.1551)
features.9.conv.6 tensor(0.3598)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.2861)
features.11.conv.0 tensor(0.1877)
features.11.conv.3 tensor(0.1337)
features.11.conv.6 tensor(0.6265)
features.12.conv.0 tensor(0.4893)
features.12.conv.3 tensor(0.2070)
features.12.conv.6 tensor(0.6335)
features.13.conv.0 tensor(0.1066)
features.13.conv.3 tensor(0.1481)
features.13.conv.6 tensor(0.3696)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1896)
features.14.conv.6 tensor(0.9375)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1277)
features.15.conv.6 tensor(0.9868)
features.16.conv.0 tensor(0.1387)
features.16.conv.3 tensor(0.1689)
features.16.conv.6 tensor(0.5012)
conv.0 tensor(0.6635)
tensor(1244531.) 2188896.0
INFO - Validation [66][   40/   40]   Loss 0.284201   Top1 91.660000   Top5 99.720000   BatchTime 0.095125
INFO - ==> Top1: 91.660    Top5: 99.720    Loss: 0.284
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  67
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [67][   20/  196]   Loss 0.245847   Top1 91.113281   Top5 98.710938   BatchTime 0.433356   LR 0.000016
INFO - Training [67][   40/  196]   Loss 0.248298   Top1 91.162109   Top5 98.828125   BatchTime 0.381384   LR 0.000015
INFO - Training [67][   60/  196]   Loss 0.243553   Top1 91.549479   Top5 98.873698   BatchTime 0.365925   LR 0.000014
INFO - Training [67][   80/  196]   Loss 0.241344   Top1 91.547852   Top5 99.013672   BatchTime 0.361076   LR 0.000013
INFO - Training [67][  100/  196]   Loss 0.234500   Top1 91.804688   Top5 99.082031   BatchTime 0.355515   LR 0.000012
INFO - Training [67][  120/  196]   Loss 0.226953   Top1 92.044271   Top5 99.147135   BatchTime 0.353686   LR 0.000011
INFO - Training [67][  140/  196]   Loss 0.224698   Top1 92.176339   Top5 99.190848   BatchTime 0.352494   LR 0.000010
INFO - Training [67][  160/  196]   Loss 0.225073   Top1 92.106934   Top5 99.184570   BatchTime 0.351147   LR 0.000009
INFO - Training [67][  180/  196]   Loss 0.225456   Top1 92.107205   Top5 99.153646   BatchTime 0.344097   LR 0.000008
********************pre-trained*****************
INFO - ==> Top1: 92.116    Top5: 99.150    Loss: 0.225
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.4583)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0211)
features.3.conv.3 tensor(0.0262)
features.3.conv.6 tensor(0.0256)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.0671)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0356)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0524)
features.7.conv.0 tensor(0.0529)
features.7.conv.3 tensor(0.1137)
features.7.conv.6 tensor(0.3162)
features.8.conv.0 tensor(0.0828)
features.8.conv.3 tensor(0.1215)
features.8.conv.6 tensor(0.3178)
features.9.conv.0 tensor(0.0915)
features.9.conv.3 tensor(0.1545)
features.9.conv.6 tensor(0.3597)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0911)
features.10.conv.6 tensor(0.2861)
features.11.conv.0 tensor(0.1875)
features.11.conv.3 tensor(0.1345)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4895)
features.12.conv.3 tensor(0.2076)
features.12.conv.6 tensor(0.6336)
features.13.conv.0 tensor(0.1067)
features.13.conv.3 tensor(0.1480)
features.13.conv.6 tensor(0.3696)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1903)
features.14.conv.6 tensor(0.9376)
features.15.conv.0 tensor(0.9799)
features.15.conv.3 tensor(0.1279)
features.15.conv.6 tensor(0.9869)
features.16.conv.0 tensor(0.1388)
features.16.conv.3 tensor(0.1703)
features.16.conv.6 tensor(0.5012)
conv.0 tensor(0.6635)
tensor(1244597.) 2188896.0
INFO - Validation [67][   20/   40]   Loss 0.345864   Top1 89.882812   Top5 99.511719   BatchTime 0.145554
INFO - Validation [67][   40/   40]   Loss 0.336730   Top1 89.860000   Top5 99.620000   BatchTime 0.101574
INFO - ==> Top1: 89.860    Top5: 99.620    Loss: 0.337
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  68
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [68][   20/  196]   Loss 0.237456   Top1 91.171875   Top5 99.101562   BatchTime 0.460966   LR 0.000007
INFO - Training [68][   40/  196]   Loss 0.237626   Top1 91.357422   Top5 99.091797   BatchTime 0.398908   LR 0.000006
INFO - Training [68][   60/  196]   Loss 0.231117   Top1 91.705729   Top5 99.153646   BatchTime 0.375721   LR 0.000006
INFO - Training [68][   80/  196]   Loss 0.232315   Top1 91.689453   Top5 99.218750   BatchTime 0.368176   LR 0.000005
INFO - Training [68][  100/  196]   Loss 0.227303   Top1 91.878906   Top5 99.218750   BatchTime 0.364687   LR 0.000004
INFO - Training [68][  120/  196]   Loss 0.224722   Top1 92.027995   Top5 99.254557   BatchTime 0.360349   LR 0.000004
INFO - Training [68][  140/  196]   Loss 0.223292   Top1 92.151228   Top5 99.296875   BatchTime 0.358063   LR 0.000003
INFO - Training [68][  160/  196]   Loss 0.224551   Top1 92.128906   Top5 99.299316   BatchTime 0.355000   LR 0.000003
INFO - Training [68][  180/  196]   Loss 0.224498   Top1 92.148438   Top5 99.262153   BatchTime 0.348835   LR 0.000002
********************pre-trained*****************
INFO - ==> Top1: 92.172    Top5: 99.242    Loss: 0.224
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [68][   20/   40]   Loss 0.458188   Top1 86.777344   Top5 99.394531   BatchTime 0.150465
features.0.conv.0 tensor(0.4375)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0168)
features.2.conv.3 tensor(0.0363)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0214)
features.3.conv.3 tensor(0.0255)
features.3.conv.6 tensor(0.0256)
features.4.conv.0 tensor(0.0311)
features.4.conv.3 tensor(0.0683)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0355)
features.5.conv.3 tensor(0.0671)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0524)
features.7.conv.0 tensor(0.0531)
features.7.conv.3 tensor(0.1126)
features.7.conv.6 tensor(0.3160)
features.8.conv.0 tensor(0.0826)
features.8.conv.3 tensor(0.1209)
features.8.conv.6 tensor(0.3179)
features.9.conv.0 tensor(0.0916)
features.9.conv.3 tensor(0.1571)
features.9.conv.6 tensor(0.3597)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0914)
features.10.conv.6 tensor(0.2862)
features.11.conv.0 tensor(0.1875)
features.11.conv.3 tensor(0.1341)
features.11.conv.6 tensor(0.6264)
features.12.conv.0 tensor(0.4897)
features.12.conv.3 tensor(0.2074)
features.12.conv.6 tensor(0.6335)
features.13.conv.0 tensor(0.1068)
features.13.conv.3 tensor(0.1487)
features.13.conv.6 tensor(0.3696)
features.14.conv.0 tensor(0.9675)
features.14.conv.3 tensor(0.1905)
features.14.conv.6 tensor(0.9382)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1288)
features.15.conv.6 tensor(0.9866)
features.16.conv.0 tensor(0.1387)
features.16.conv.3 tensor(0.1698)
features.16.conv.6 tensor(0.5012)
conv.0 tensor(0.6635)
tensor(1244659.) 2188896.0
INFO - Validation [68][   40/   40]   Loss 0.446593   Top1 86.940000   Top5 99.490000   BatchTime 0.103403
INFO - ==> Top1: 86.940    Top5: 99.490    Loss: 0.447
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch  69
INFO - Training: 50000 samples (256 per mini-batch)
INFO - Training [69][   20/  196]   Loss 0.244078   Top1 91.191406   Top5 98.652344   BatchTime 0.427978   LR 0.000002
INFO - Training [69][   40/  196]   Loss 0.250525   Top1 91.064453   Top5 98.769531   BatchTime 0.390511   LR 0.000001
INFO - Training [69][   60/  196]   Loss 0.243051   Top1 91.354167   Top5 98.925781   BatchTime 0.376570   LR 0.000001
INFO - Training [69][   80/  196]   Loss 0.241242   Top1 91.459961   Top5 99.082031   BatchTime 0.370970   LR 0.000001
INFO - Training [69][  100/  196]   Loss 0.234740   Top1 91.730469   Top5 99.121094   BatchTime 0.365463   LR 0.000000
INFO - Training [69][  120/  196]   Loss 0.227237   Top1 92.057292   Top5 99.173177   BatchTime 0.360524   LR 0.000000
INFO - Training [69][  140/  196]   Loss 0.225908   Top1 92.117746   Top5 99.215960   BatchTime 0.357032   LR 0.000000
INFO - Training [69][  160/  196]   Loss 0.227539   Top1 92.036133   Top5 99.218750   BatchTime 0.354163   LR 0.000000
INFO - Training [69][  180/  196]   Loss 0.227729   Top1 92.033420   Top5 99.220920   BatchTime 0.345422   LR 0.000000
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 92.032    Top5: 99.236    Loss: 0.227
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [69][   20/   40]   Loss 0.308643   Top1 91.171875   Top5 99.589844   BatchTime 0.205046
features.0.conv.0 tensor(0.4653)
features.0.conv.3 tensor(0.2500)
features.1.conv.0 tensor(0.0163)
features.1.conv.3 tensor(0.0556)
features.1.conv.6 tensor(0.0317)
features.2.conv.0 tensor(0.0171)
features.2.conv.3 tensor(0.0370)
features.2.conv.6 tensor(0.0431)
features.3.conv.0 tensor(0.0208)
features.3.conv.3 tensor(0.0255)
features.3.conv.6 tensor(0.0256)
features.4.conv.0 tensor(0.0311)
features.4.conv.3 tensor(0.0666)
features.4.conv.6 tensor(0.2572)
features.5.conv.0 tensor(0.0355)
features.5.conv.3 tensor(0.0666)
features.5.conv.6 tensor(0.2987)
features.6.conv.0 tensor(0.0236)
features.6.conv.3 tensor(0.0307)
features.6.conv.6 tensor(0.0525)
features.7.conv.0 tensor(0.0530)
features.7.conv.3 tensor(0.1126)
features.7.conv.6 tensor(0.3161)
features.8.conv.0 tensor(0.0828)
features.8.conv.3 tensor(0.1212)
features.8.conv.6 tensor(0.3179)
features.9.conv.0 tensor(0.0914)
features.9.conv.3 tensor(0.1545)
features.9.conv.6 tensor(0.3597)
features.10.conv.0 tensor(0.0400)
features.10.conv.3 tensor(0.0909)
features.10.conv.6 tensor(0.2862)
features.11.conv.0 tensor(0.1875)
features.11.conv.3 tensor(0.1341)
features.11.conv.6 tensor(0.6266)
features.12.conv.0 tensor(0.4896)
features.12.conv.3 tensor(0.2072)
features.12.conv.6 tensor(0.6337)
features.13.conv.0 tensor(0.1069)
features.13.conv.3 tensor(0.1474)
features.13.conv.6 tensor(0.3697)
features.14.conv.0 tensor(0.9676)
features.14.conv.3 tensor(0.1917)
features.14.conv.6 tensor(0.9384)
features.15.conv.0 tensor(0.9800)
features.15.conv.3 tensor(0.1278)
features.15.conv.6 tensor(0.9867)
features.16.conv.0 tensor(0.1389)
features.16.conv.3 tensor(0.1699)
features.16.conv.6 tensor(0.5013)
conv.0 tensor(0.6636)
tensor(1244807.) 2188896.0
INFO - Validation [69][   40/   40]   Loss 0.298489   Top1 91.410000   Top5 99.700000   BatchTime 0.136545
INFO - ==> Top1: 91.410    Top5: 99.700    Loss: 0.298
INFO - ==> Sparsity : 0.569
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 92.040   Top5: 99.800]
INFO - Scoreboard best 2 ==> Epoch [59][Top1: 91.920   Top5: 99.810]
INFO - Scoreboard best 3 ==> Epoch [46][Top1: 91.770   Top5: 99.800]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-103844/88hard_pruning_checkpoint.pth.tar
INFO - >>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [   20/   40]   Loss 0.308643   Top1 91.171875   Top5 99.589844   BatchTime 0.141965
INFO - Validation [   40/   40]   Loss 0.298489   Top1 91.410000   Top5 99.700000   BatchTime 0.098250
INFO - ==> Top1: 91.410    Top5: 99.700    Loss: 0.298
INFO - ==> Sparsity : 0.000
INFO - Program completed sucessfully ... exiting ...