Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
*************soft_pruning_mode*******************
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.95438832
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.95019758
0.92534447
0.90970117
0.88706356
0.88660467
0.88887691
0.88908088
0.88781965
0.88592887
0.88469523
INFO - Training [0][   20/  196]   Loss 1.611376   Top1 53.398438   Top5 89.160156   BatchTime 0.438507   LR 0.004999
0.88544768
0.88646752
0.88661486
0.88842946
0.88729441
0.88362163
0.87832105
0.87442231
0.87104249
0.87634134
0.88499564
0.88825899
0.88713849
0.88641095
0.88750875
0.88795769
INFO - Training [0][   40/  196]   Loss 1.551107   Top1 52.568359   Top5 89.375000   BatchTime 0.415976   LR 0.004995
0.88928473
0.89171356
0.89413983
0.89658171
0.89809865
0.89964974
0.90102822
0.90117604
0.90007645
0.90232617
0.90115708
0.89194196
0.88046473
0.86893374
0.86093748
0.85690790
0.85964787
0.86303949
0.86705714
0.87071592
INFO - Training [0][   60/  196]   Loss 1.453531   Top1 54.895833   Top5 90.735677   BatchTime 0.408664   LR 0.004989
0.87292492
0.87467343
0.87611783
0.87665457
0.87696248
0.87767422
0.87916803
0.87947983
0.87996042
0.88057613
0.88139659
0.88232911
0.88310331
0.88336968
0.88404268
0.88499945
0.88602734
0.88663256
0.88680959
0.88698441
0.88715053
INFO - Training [0][   80/  196]   Loss 1.387360   Top1 56.997070   Top5 91.538086   BatchTime 0.400993   LR 0.004980
0.88708395
0.88712311
0.88739812
0.88467765
0.88270289
0.87809038
0.87124586
0.86696899
0.86132729
0.85811412
0.85576361
0.85305268
0.85345638
0.85167712
0.84943670
0.84679186
0.84585088
0.84804595
0.84825605
INFO - Training [0][  100/  196]   Loss 1.332664   Top1 58.406250   Top5 92.183594   BatchTime 0.382189   LR 0.004968
0.84774691
0.84829307
0.84981787
0.85082805
0.85175198
0.85226655
0.85301995
0.85343081
0.85271668
0.85210925
0.85152918
0.84759891
0.83542508
0.84303260
0.84767795
0.84849316
0.84937042
0.85026574
0.85087645
0.85194057
0.85246432
0.85245246
0.85182816
INFO - Training [0][  120/  196]   Loss 1.287025   Top1 59.853516   Top5 92.679036   BatchTime 0.376806   LR 0.004954
0.85148036
0.85181338
0.85436732
0.85584646
0.85701966
0.85760820
0.85832512
0.85881221
0.85939401
0.85994089
0.86048967
0.86072463
0.86076969
0.86098754
0.86135519
0.86173022
0.86204666
0.86335647
0.86480367
0.86564624
INFO - Training [0][  140/  196]   Loss 1.255807   Top1 60.756138   Top5 92.999442   BatchTime 0.379923   LR 0.004938
0.86574972
0.86574620
0.86531717
0.86487377
0.86438900
0.86412299
0.86316580
0.86271334
0.86212516
0.86178941
0.86129570
0.86069912
0.85959971
0.85883999
0.85829693
0.85830718
0.85781109
0.85716820
0.85628468
0.85520309
INFO - Training [0][  160/  196]   Loss 1.233939   Top1 61.462402   Top5 93.259277   BatchTime 0.384294   LR 0.004919
0.85419190
0.85317791
0.85166895
0.84903753
0.84725988
0.84683269
0.84501034
0.84453619
0.84364718
0.84289223
0.84151441
0.84209204
0.84211212
0.84062374
0.83687669
0.83268154
0.83354962
0.83512253
0.83435822
0.83042461
INFO - Training [0][  180/  196]   Loss 1.211923   Top1 62.131076   Top5 93.452691   BatchTime 0.385317   LR 0.004897
0.80839962
0.80655277
0.80187297
0.79744381
0.79730892
0.79827493
0.79701805
0.79604596
0.79669625
0.79622918
0.79635727
0.79696268
0.79667956
0.79634440
INFO - ==> Top1: 62.668    Top5: 93.596    Loss: 1.194
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79619747
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.711598   Top1 77.070312   Top5 98.066406   BatchTime 0.108116
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.4141)
features.1.conv.0 tensor(0.0436)
features.1.conv.3 tensor(0.0891)
features.1.conv.6 tensor(0.0660)
features.2.conv.0 tensor(0.0527)
features.2.conv.3 tensor(0.0640)
features.2.conv.6 tensor(0.0966)
features.3.conv.0 tensor(0.0373)
features.3.conv.3 tensor(0.0579)
features.3.conv.6 tensor(0.0716)
features.4.conv.0 tensor(0.0562)
features.4.conv.3 tensor(0.0995)
features.4.conv.6 tensor(0.1038)
features.5.conv.0 tensor(0.0609)
features.5.conv.3 tensor(0.0689)
features.5.conv.6 tensor(0.1032)
features.6.conv.0 tensor(0.0565)
features.6.conv.3 tensor(0.0382)
features.6.conv.6 tensor(0.0876)
features.7.conv.0 tensor(0.0973)
features.7.conv.3 tensor(0.0949)
features.7.conv.6 tensor(0.3217)
features.8.conv.0 tensor(0.1154)
features.8.conv.3 tensor(0.1042)
features.8.conv.6 tensor(0.2579)
features.9.conv.0 tensor(0.1049)
features.9.conv.3 tensor(0.1264)
features.9.conv.6 tensor(0.1353)
features.10.conv.0 tensor(0.0860)
features.10.conv.3 tensor(0.0920)
features.10.conv.6 tensor(0.1193)
features.11.conv.0 tensor(0.2059)
features.11.conv.3 tensor(0.0810)
features.11.conv.6 tensor(0.6986)
features.12.conv.0 tensor(0.2449)
features.12.conv.3 tensor(0.0866)
features.12.conv.6 tensor(0.5270)
features.13.conv.0 tensor(0.1305)
features.13.conv.3 tensor(0.1258)
features.13.conv.6 tensor(0.2028)
features.14.conv.0 tensor(0.8728)
features.14.conv.3 tensor(0.0825)
features.14.conv.6 tensor(0.9986)
features.15.conv.0 tensor(0.5867)
features.15.conv.3 tensor(0.0605)
features.15.conv.6 tensor(0.9620)
features.16.conv.0 tensor(0.0601)
features.16.conv.3 tensor(0.0742)
features.16.conv.6 tensor(0.1529)
conv.0 tensor(0.0759)
tensor(773538.) 2188896.0
INFO - Validation [0][   40/   40]   Loss 0.716208   Top1 76.540000   Top5 97.960000   BatchTime 0.079764
INFO - ==> Top1: 76.540    Top5: 97.960    Loss: 0.716
INFO - ==> Sparsity : 0.353
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.79694712
0.79739082
0.79916608
0.80153161
0.80235910
0.80097741
0.80054134
0.80041057
0.79892135
0.79905313
0.80150402
0.80110866
0.80062765
0.79969001
0.79903245
0.79797411
0.79497474
0.79079169
INFO - Training [1][   20/  196]   Loss 0.998995   Top1 67.929688   Top5 95.253906   BatchTime 0.433417   LR 0.004853
0.79134226
0.79426539
0.79640353
0.79039079
0.78819031
0.78732377
0.78692365
0.78682536
0.78659874
0.78462446
0.78021836
0.78776574
0.78632522
0.78475672
0.78562301
0.78800958
0.78959602
0.79217809
0.79438215
0.79558575
0.79708129
INFO - Training [1][   40/  196]   Loss 0.995723   Top1 68.925781   Top5 95.400391   BatchTime 0.409233   LR 0.004825
0.80103886
0.80093402
0.79526287
0.80205089
0.80485010
0.80471569
0.80428439
0.80383772
0.80348021
0.80247885
0.80171561
0.80160862
0.80185413
0.80196249
0.80203563
0.80223882
0.80204922
0.80203348
0.80207342
0.80198139
0.80218405
0.80192339
INFO - Training [1][   60/  196]   Loss 0.986206   Top1 69.082031   Top5 95.546875   BatchTime 0.391149   LR 0.004794
0.80161047
0.80182439
0.80186105
0.80195612
0.80204761
0.80243468
0.80322319
0.80369020
0.80380291
0.80394244
0.80356675
0.80369502
0.80388087
0.80381870
0.80360782
0.80349082
0.80349880
0.80342662
0.80321795
0.80288088
INFO - Training [1][   80/  196]   Loss 0.980940   Top1 69.208984   Top5 95.678711   BatchTime 0.372229   LR 0.004761
0.80267197
0.80260086
0.80263859
0.80244356
0.80234265
0.80219543
0.80219787
0.80204874
0.80204272
0.80198419
0.80190569
0.80160445
0.80123568
0.80105543
0.80082524
0.80079782
0.80034858
INFO - Training [1][  100/  196]   Loss 0.964892   Top1 69.742188   Top5 95.746094   BatchTime 0.366478   LR 0.004725
0.80017495
0.80011135
0.80001485
0.79982805
0.79960418
0.79968524
0.79984313
0.80003256
0.79948592
0.79862279
0.79842901
0.80061722
0.80060142
0.80081314
0.80097842
0.80098319
0.80096889
0.80078351
0.80097210
0.80090749
0.80050331
0.80039352
0.80040544
INFO - Training [1][  120/  196]   Loss 0.952230   Top1 70.159505   Top5 95.947266   BatchTime 0.363608   LR 0.004687
0.80048519
0.80040181
0.80029941
0.80011010
0.79961139
0.79886013
0.79884040
0.79907954
0.80062449
0.80025887
0.80048138
0.80112964
0.80089873
0.79882950
0.79657799
0.78821170
INFO - Training [1][  140/  196]   Loss 0.943636   Top1 70.496652   Top5 96.074219   BatchTime 0.364718   LR 0.004647
0.77840495
0.79992586
0.80305582
0.80317438
0.80333173
0.80308855
0.80279779
0.80297887
0.80298454
0.80272633
0.80233890
0.80164135
0.80123955
0.80145770
0.80086160
0.80080372
0.80061138
0.80066997
0.80059201
0.80051696
0.80039585
INFO - Training [1][  160/  196]   Loss 0.937684   Top1 70.646973   Top5 96.149902   BatchTime 0.366445   LR 0.004605
0.80017745
0.79978639
0.79930073
0.79870033
0.79883355
0.79831344
0.79810482
0.79781610
0.79781926
0.79757905
0.79696363
0.79632849
0.79551589
0.79482752
0.79357851
0.79506272
0.79440886
0.79266334
0.78868788
0.79002619
0.78334415
INFO - Training [1][  180/  196]   Loss 0.926140   Top1 71.069878   Top5 96.206597   BatchTime 0.368440   LR 0.004560
0.77320337
0.77057296
0.77030814
0.77003634
0.77064312
0.77641290
0.78504801
0.79123360
0.79462492
0.79460013
0.79413754
0.79394162
0.79372954
0.79363680
0.79368597
0.79320973
0.79167932
INFO - ==> Top1: 71.224    Top5: 96.198    Loss: 0.922
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.721811   Top1 76.875000   Top5 98.222656   BatchTime 0.117711
INFO - Validation [1][   40/   40]   Loss 0.724747   Top1 76.860000   Top5 98.220000   BatchTime 0.088105
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.1543)
features.1.conv.0 tensor(0.0430)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.0558)
features.2.conv.3 tensor(0.0617)
features.2.conv.6 tensor(0.0972)
features.3.conv.0 tensor(0.0341)
features.3.conv.3 tensor(0.0471)
features.3.conv.6 tensor(0.0727)
features.4.conv.0 tensor(0.0721)
features.4.conv.3 tensor(0.1019)
features.4.conv.6 tensor(0.0981)
features.5.conv.0 tensor(0.0604)
features.5.conv.3 tensor(0.0660)
features.5.conv.6 tensor(0.1068)
features.6.conv.0 tensor(0.2191)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0916)
features.7.conv.0 tensor(0.0950)
features.7.conv.3 tensor(0.0958)
features.7.conv.6 tensor(0.3083)
features.8.conv.0 tensor(0.1178)
features.8.conv.3 tensor(0.1195)
features.8.conv.6 tensor(0.2040)
features.9.conv.0 tensor(0.1060)
features.9.conv.3 tensor(0.1264)
features.9.conv.6 tensor(0.1351)
features.10.conv.0 tensor(0.0779)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.1035)
features.11.conv.0 tensor(0.4311)
features.11.conv.3 tensor(0.1385)
features.11.conv.6 tensor(0.1693)
features.12.conv.0 tensor(0.7655)
features.12.conv.3 tensor(0.1082)
features.12.conv.6 tensor(0.7630)
features.13.conv.0 tensor(0.1205)
features.13.conv.3 tensor(0.1416)
features.13.conv.6 tensor(0.1341)
features.14.conv.0 tensor(0.8789)
features.14.conv.3 tensor(0.0810)
features.14.conv.6 tensor(0.9027)
features.15.conv.0 tensor(0.6665)
features.15.conv.3 tensor(0.0597)
features.15.conv.6 tensor(0.9674)
features.16.conv.0 tensor(0.1318)
features.16.conv.3 tensor(0.0773)
features.16.conv.6 tensor(0.1964)
conv.0 tensor(0.1464)
tensor(843388.) 2188896.0
INFO - ==> Top1: 76.860    Top5: 98.220    Loss: 0.725
INFO - ==> Sparsity : 0.385
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.79231441
0.79202193
0.79031712
0.78765845
0.78599656
0.78442615
0.78288323
0.78209913
0.78024942
0.77849263
0.77754837
0.77536482
0.77617621
0.77496195
0.77344066
0.78007638
0.78412706
INFO - Training [2][   20/  196]   Loss 0.893217   Top1 71.699219   Top5 95.937500   BatchTime 0.410247   LR 0.004477
0.78865170
0.79382056
0.79531252
0.79664779
0.79980916
0.80109191
0.80479938
0.80647135
0.80778056
0.81017262
0.81322706
0.81679434
0.81838214
0.81962979
0.82060784
0.82074243
0.82093340
0.82044077
0.81967545
0.81878269
0.81792694
0.81717950
INFO - Training [2][   40/  196]   Loss 0.887082   Top1 72.519531   Top5 95.937500   BatchTime 0.382170   LR 0.004426
0.81616646
0.81507355
0.81394321
0.81249422
0.81137657
0.81069022
0.80996454
0.80908304
0.80823177
0.80777317
0.80743420
0.80713093
0.80660599
0.80606395
0.80549556
0.80496782
0.80464929
0.80414063
0.80354464
INFO - Training [2][   60/  196]   Loss 0.894165   Top1 72.226562   Top5 95.944010   BatchTime 0.359426   LR 0.004374
0.80348855
0.80322021
0.80290300
0.80255955
0.80225736
0.80199218
0.80188572
0.80152428
0.80120331
0.80085641
0.80076075
0.80053324
0.80026114
0.79988819
0.79973221
0.79959220
0.79954135
0.79940820
0.79929763
INFO - Training [2][   80/  196]   Loss 0.893639   Top1 72.080078   Top5 96.074219   BatchTime 0.348026   LR 0.004320
0.79932278
0.79926080
0.79933643
0.79921317
0.79894626
0.79888266
0.79868662
0.79805821
0.79768866
0.79718935
0.79648870
0.79597116
0.79547888
0.79512274
0.79475719
0.79453164
0.79412246
0.79398060
0.79341406
0.79291433
INFO - Training [2][  100/  196]   Loss 0.880479   Top1 72.468750   Top5 96.230469   BatchTime 0.338820   LR 0.004264
0.79214972
0.79122126
0.78992158
0.78852463
0.78758281
0.78634447
0.78528875
0.78506184
0.78582942
0.78637844
0.78753638
0.78895009
0.79134107
0.79116571
0.79093456
0.79099566
0.79058576
0.79017603
0.78976411
0.78916448
0.78873992
INFO - Training [2][  120/  196]   Loss 0.869335   Top1 72.945964   Top5 96.396484   BatchTime 0.348251   LR 0.004206
0.78816015
0.78727770
0.78627497
0.78535062
0.78366518
0.78239328
0.78087926
0.77896273
0.77808177
0.77741450
0.77625030
0.77484477
0.77515119
0.77523971
0.77469343
0.77404529
0.77357423
0.77265346
0.77247822
0.77247053
0.77241313
0.78682846
INFO - Training [2][  140/  196]   Loss 0.877715   Top1 72.564174   Top5 96.286272   BatchTime 0.349873   LR 0.004146
0.81218868
0.83156812
0.83890665
0.84897155
0.84532785
0.84304196
0.84106058
0.83993989
0.83934635
0.83827430
0.83762163
0.83905554
0.84183270
0.83985364
0.83791566
0.83658892
INFO - Training [2][  160/  196]   Loss 1.068692   Top1 66.157227   Top5 92.797852   BatchTime 0.352210   LR 0.004085
0.83563161
0.83450550
0.83429414
0.83355796
0.83326012
0.83155125
0.82801855
0.82395029
0.82045937
0.81810594
0.81633389
0.81494027
0.81241757
0.81038880
0.80812836
0.80950236
0.80999094
0.81028891
0.80972344
0.80912155
0.80850422
0.80810821
0.80773914
INFO - Training [2][  180/  196]   Loss 1.198591   Top1 60.659722   Top5 90.338542   BatchTime 0.353243   LR 0.004022
0.80748028
0.80757010
0.80752242
0.80744356
0.80717272
0.80687118
0.80664682
0.80654782
0.80658799
0.80645347
0.80662829
0.80702376
0.80700767
0.80709147
0.80721205
0.80757910
INFO - ==> Top1: 57.420    Top5: 89.018    Loss: 1.276
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80798978
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [2][   20/   40]   Loss 2.248263   Top1 17.656250   Top5 76.875000   BatchTime 0.107955
INFO - Validation [2][   40/   40]   Loss 2.246361   Top1 17.800000   Top5 77.040000   BatchTime 0.079455
features.0.conv.0 tensor(0.5139)
features.0.conv.3 tensor(0.1816)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0463)
features.2.conv.3 tensor(0.0741)
features.2.conv.6 tensor(0.0784)
features.3.conv.0 tensor(0.0498)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0588)
features.4.conv.0 tensor(0.5493)
features.4.conv.3 tensor(0.1094)
features.4.conv.6 tensor(0.0853)
features.5.conv.0 tensor(0.0485)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.5112)
features.6.conv.0 tensor(0.0363)
features.6.conv.3 tensor(0.0660)
features.6.conv.6 tensor(0.0653)
features.7.conv.0 tensor(0.0630)
features.7.conv.3 tensor(0.1314)
features.7.conv.6 tensor(0.1165)
features.8.conv.0 tensor(0.0620)
features.8.conv.3 tensor(0.1340)
features.8.conv.6 tensor(0.1479)
features.9.conv.0 tensor(0.0370)
features.9.conv.3 tensor(0.1525)
features.9.conv.6 tensor(0.1529)
features.10.conv.0 tensor(0.8876)
features.10.conv.3 tensor(0.1076)
features.10.conv.6 tensor(0.7901)
features.11.conv.0 tensor(0.1007)
features.11.conv.3 tensor(0.1740)
features.11.conv.6 tensor(0.2550)
features.12.conv.0 tensor(0.0887)
features.12.conv.3 tensor(0.1510)
features.12.conv.6 tensor(0.3085)
features.13.conv.0 tensor(0.1078)
features.13.conv.3 tensor(0.1555)
features.13.conv.6 tensor(0.1601)
features.14.conv.0 tensor(0.7479)
features.14.conv.3 tensor(0.1153)
features.14.conv.6 tensor(0.2700)
features.15.conv.0 tensor(0.6673)
features.15.conv.3 tensor(0.0664)
features.15.conv.6 tensor(0.9607)
features.16.conv.0 tensor(0.1027)
features.16.conv.3 tensor(0.0789)
features.16.conv.6 tensor(0.2100)
conv.0 tensor(0.5646)
tensor(862468.) 2188896.0
INFO - ==> Top1: 17.800    Top5: 77.040    Loss: 2.246
INFO - ==> Sparsity : 0.394
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 17.800   Top5: 77.040]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.80854249
0.80894333
0.80929041
0.80983847
0.81043559
0.81099916
0.81163990
0.81222433
0.81312859
0.81324160
0.81338894
0.81414467
0.81492007
0.81530035
0.81580353
0.81630784
0.81663203
0.81719220
INFO - Training [3][   20/  196]   Loss 2.129549   Top1 23.066406   Top5 75.468750   BatchTime 0.456794   LR 0.003907
0.81767732
0.81817710
0.81865764
0.81914788
0.81943637
0.82003307
0.82083827
0.82146317
0.82188457
0.82202482
0.82194698
0.82210934
0.82196665
0.82171595
0.82117671
0.82060361
0.82034165
0.82047480
0.82073158
0.82088828
0.82090241
INFO - Training [3][   40/  196]   Loss 2.093318   Top1 24.326172   Top5 77.626953   BatchTime 0.414672   LR 0.003840
0.82066959
0.82033390
0.82010210
0.81998783
0.81976980
0.81968254
0.81953484
0.81930298
0.81934381
0.81977415
0.82003361
0.81977326
0.81918931
0.81875759
0.81767011
0.81634724
0.81565225
INFO - Training [3][   60/  196]   Loss 2.060376   Top1 25.898438   Top5 78.372396   BatchTime 0.396901   LR 0.003771
0.81545895
0.81525105
0.81550407
0.81639856
0.81738400
0.81875777
0.81932092
0.81981981
0.82012898
0.82042736
0.82058382
0.81990385
0.81705165
0.81178373
0.80475390
0.80073243
0.80210561
0.81013733
0.81752354
0.82113200
0.82081956
0.81998223
0.81889427
INFO - Training [3][   80/  196]   Loss 2.036090   Top1 27.119141   Top5 79.238281   BatchTime 0.385270   LR 0.003701
0.81696111
0.81426960
0.81111312
0.80628419
0.80551618
0.80332315
0.79967928
0.79651147
0.79742646
0.79999101
0.80524558
0.81058145
0.81378919
0.81552500
0.81778181
0.82096058
0.81957853
0.81515825
0.80819559
INFO - Training [3][  100/  196]   Loss 2.015655   Top1 28.058594   Top5 79.988281   BatchTime 0.371728   LR 0.003630
0.80519634
0.80611765
0.81359458
0.81588334
0.81462640
0.81259483
0.80828446
0.80841106
0.81231540
0.81449634
0.81488800
0.81444979
0.81412894
0.81322050
0.81234258
0.81145263
0.81070280
0.80910367
0.80557787
0.80188775
0.79669809
INFO - Training [3][  120/  196]   Loss 1.996765   Top1 28.863932   Top5 80.625000   BatchTime 0.372807   LR 0.003558
0.79319137
0.79010540
0.78596449
0.78185439
0.77578747
0.77362877
0.76836610
0.76290131
0.76586050
0.77142769
0.77579838
0.77878398
0.78107768
0.78228933
0.78180343
0.78102499
0.77992606
0.77840143
0.77630973
0.77353346
0.77003330
0.76619768
INFO - Training [3][  140/  196]   Loss 1.983778   Top1 29.391741   Top5 81.049107   BatchTime 0.371157   LR 0.003484
0.76258188
0.75950474
0.75901902
0.75774390
0.75616890
0.75338018
0.75107181
0.74951351
0.74742979
0.74467242
0.74525297
0.74573499
0.74579549
0.74467307
0.74228323
0.74006760
0.73853165
INFO - Training [3][  160/  196]   Loss 1.973869   Top1 29.792480   Top5 81.257324   BatchTime 0.369106   LR 0.003410
0.73747462
0.73582304
0.73435569
0.73289949
0.73092020
0.72876483
0.72657776
0.72352034
0.72082317
0.71882057
0.71793735
0.71754366
0.71740580
0.71716821
0.71675259
0.71614099
0.71611518
0.71695590
0.71966076
0.72267926
0.72419155
0.72692925
INFO - Training [3][  180/  196]   Loss 1.960918   Top1 30.347222   Top5 81.569010   BatchTime 0.368531   LR 0.003335
0.73018390
0.73322201
0.73645693
0.73859143
0.74052173
0.74214131
0.74417835
0.74515301
0.74581152
0.74666619
0.74724758
0.74712855
0.74708086
0.74705386
0.74734360
0.74728978
********************pre-trained*****************
INFO - ==> Top1: 30.726    Top5: 81.746    Loss: 1.953
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 1.780799   Top1 36.210938   Top5 86.347656   BatchTime 0.115287
INFO - Validation [3][   40/   40]   Loss 1.769607   Top1 36.810000   Top5 86.290000   BatchTime 0.087522
INFO - ==> Top1: 36.810    Top5: 86.290    Loss: 1.770
INFO - ==> Sparsity : 0.404
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 36.810   Top5: 86.290]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.2480)
features.1.conv.0 tensor(0.0495)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.0787)
features.2.conv.6 tensor(0.0822)
features.3.conv.0 tensor(0.0515)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0601)
features.4.conv.0 tensor(0.4515)
features.4.conv.3 tensor(0.1615)
features.4.conv.6 tensor(0.0874)
features.5.conv.0 tensor(0.0485)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0552)
features.6.conv.0 tensor(0.0378)
features.6.conv.3 tensor(0.0694)
features.6.conv.6 tensor(0.0719)
features.7.conv.0 tensor(0.0671)
features.7.conv.3 tensor(0.1328)
features.7.conv.6 tensor(0.1318)
features.8.conv.0 tensor(0.0649)
features.8.conv.3 tensor(0.1345)
features.8.conv.6 tensor(0.8315)
features.9.conv.0 tensor(0.0415)
features.9.conv.3 tensor(0.1589)
features.9.conv.6 tensor(0.7515)
features.10.conv.0 tensor(0.8862)
features.10.conv.3 tensor(0.2150)
features.10.conv.6 tensor(0.8005)
features.11.conv.0 tensor(0.1128)
features.11.conv.3 tensor(0.1809)
features.11.conv.6 tensor(0.2525)
features.12.conv.0 tensor(0.0976)
features.12.conv.3 tensor(0.1576)
features.12.conv.6 tensor(0.3214)
features.13.conv.0 tensor(0.1174)
features.13.conv.3 tensor(0.1634)
features.13.conv.6 tensor(0.1708)
features.14.conv.0 tensor(0.7640)
features.14.conv.3 tensor(0.1166)
features.14.conv.6 tensor(0.8120)
features.15.conv.0 tensor(0.6841)
features.15.conv.3 tensor(0.0694)
features.15.conv.6 tensor(0.9637)
features.16.conv.0 tensor(0.1108)
features.16.conv.3 tensor(0.0801)
features.16.conv.6 tensor(0.1852)
conv.0 tensor(0.3339)
tensor(883539.) 2188896.0
0.74664795
0.74624681
0.74581474
0.74545282
0.74511772
0.74480355
0.74435270
0.74378616
0.74327540
0.74232537
0.74192446
0.74103904
0.74064726
0.73967087
0.73882067
0.73800755
0.73725682
0.73669624
0.73661023
0.73612219
INFO - Training [4][   20/  196]   Loss 1.840167   Top1 35.703125   Top5 84.277344   BatchTime 0.423494   LR 0.003200
0.73528123
0.73517865
0.73520559
0.73518807
0.73499072
0.73486459
0.73480976
0.73502070
0.73511559
0.73549855
0.73559368
0.73614657
0.73720264
0.73814893
0.73835343
0.73833507
INFO - Training [4][   40/  196]   Loss 1.835168   Top1 36.250000   Top5 84.667969   BatchTime 0.401879   LR 0.003122
0.73839921
0.73850936
0.73902833
0.74010140
0.74052519
0.74046969
0.74074495
0.74025732
0.73992902
0.73971838
0.73928720
0.73891592
0.73871398
0.73862427
0.73852718
0.73888618
0.73906046
0.73932952
0.73995268
0.74053150
0.74095052
0.74136060
0.74196833
INFO - Training [4][   60/  196]   Loss 1.822626   Top1 36.419271   Top5 85.058594   BatchTime 0.385121   LR 0.003044
0.74292678
0.74399644
0.74486983
0.74602610
0.74786025
0.74927211
0.75079131
0.75212389
0.75330037
0.75380933
0.75393242
0.75373143
0.75284111
0.75250173
0.75182420
0.75114256
0.75039744
0.74944973
0.74857688
0.74734849
0.74639690
INFO - Training [4][   80/  196]   Loss 1.822625   Top1 36.210938   Top5 85.190430   BatchTime 0.383101   LR 0.002965
0.74540287
0.74452263
0.74354255
0.74274194
0.74236184
0.74200565
0.74140793
0.74091554
0.74041021
0.73998195
0.73974425
0.73952889
0.73905301
0.73866349
0.73856616
0.73856789
0.73860115
0.73872429
0.73842198
INFO - Training [4][  100/  196]   Loss 1.816231   Top1 36.437500   Top5 85.292969   BatchTime 0.371790   LR 0.002886
0.73824000
0.73817307
0.73800975
0.73753023
0.73587310
0.73327941
0.72875047
0.72383136
0.72675288
0.72939354
0.73160386
0.73302674
0.73395830
0.73481327
0.73531938
0.73573601
0.73605818
0.73628622
0.73642337
0.73693520
INFO - Training [4][  120/  196]   Loss 1.808463   Top1 36.555990   Top5 85.507812   BatchTime 0.357254   LR 0.002806
0.73726326
0.73753810
0.73787671
0.73819315
0.73823255
0.73830569
0.73866260
0.73867363
0.73837650
0.73788971
0.73726934
0.73698294
0.73707783
0.73692566
0.73666745
0.73613954
0.73545319
0.73466545
0.73388809
0.73194450
0.72754818
0.71617436
INFO - Training [4][  140/  196]   Loss 1.804969   Top1 36.835938   Top5 85.541295   BatchTime 0.359419   LR 0.002726
0.71009874
0.71650922
0.73096633
0.73543435
0.73488772
0.73444986
0.73404604
0.73375732
0.73364860
0.73356462
0.73353744
0.73342949
0.73335379
0.73320895
0.73291248
0.73286784
INFO - Training [4][  160/  196]   Loss 1.805476   Top1 36.840820   Top5 85.507812   BatchTime 0.361961   LR 0.002646
0.73278576
0.73275518
0.73291576
0.73307610
0.73320895
0.73345083
0.73374557
0.73417884
0.73435932
0.73461562
0.73482043
0.73490578
0.73496133
0.73491383
0.73501498
0.73480934
0.73468596
0.73467773
0.73462546
0.73472792
0.73455262
INFO - Training [4][  180/  196]   Loss 1.796747   Top1 37.113715   Top5 85.687934   BatchTime 0.363928   LR 0.002566
0.73430580
0.73403943
0.73378283
0.73348099
0.73343545
0.73330420
0.73334706
0.73332399
0.73325843
0.73320806
0.73318720
0.73298454
0.73270357
0.73246115
0.73216057
0.73198152
0.73182362
0.73162854
********************pre-trained*****************
INFO - ==> Top1: 37.172    Top5: 85.740    Loss: 1.794
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 1.766984   Top1 38.828125   Top5 85.429688   BatchTime 0.125631
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.2402)
features.1.conv.0 tensor(0.0495)
features.1.conv.3 tensor(0.0938)
features.1.conv.6 tensor(0.0660)
features.2.conv.0 tensor(0.0483)
features.2.conv.3 tensor(0.0756)
features.2.conv.6 tensor(0.0819)
features.3.conv.0 tensor(0.0512)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0597)
features.4.conv.0 tensor(0.3843)
features.4.conv.3 tensor(0.1152)
features.4.conv.6 tensor(0.0884)
features.5.conv.0 tensor(0.0485)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0627)
features.6.conv.0 tensor(0.0360)
features.6.conv.3 tensor(0.0683)
features.6.conv.6 tensor(0.0723)
features.7.conv.0 tensor(0.0701)
features.7.conv.3 tensor(0.1340)
features.7.conv.6 tensor(0.2379)
features.8.conv.0 tensor(0.0680)
features.8.conv.3 tensor(0.1351)
features.8.conv.6 tensor(0.8950)
features.9.conv.0 tensor(0.0475)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.8285)
features.10.conv.0 tensor(0.8887)
features.10.conv.3 tensor(0.2564)
features.10.conv.6 tensor(0.7986)
features.11.conv.0 tensor(0.1179)
features.11.conv.3 tensor(0.1806)
features.11.conv.6 tensor(0.2526)
features.12.conv.0 tensor(0.0984)
features.12.conv.3 tensor(0.1580)
features.12.conv.6 tensor(0.3268)
features.13.conv.0 tensor(0.1237)
features.13.conv.3 tensor(0.1632)
features.13.conv.6 tensor(0.3364)
features.14.conv.0 tensor(0.7751)
features.14.conv.3 tensor(0.1123)
features.14.conv.6 tensor(0.8296)
features.15.conv.0 tensor(0.6891)
features.15.conv.3 tensor(0.0705)
features.15.conv.6 tensor(0.9654)
features.16.conv.0 tensor(0.1969)
features.16.conv.3 tensor(0.0807)
features.16.conv.6 tensor(0.1825)
conv.0 tensor(0.3472)
tensor(929085.) 2188896.0
INFO - Validation [4][   40/   40]   Loss 1.757661   Top1 39.130000   Top5 85.570000   BatchTime 0.089092
INFO - ==> Top1: 39.130    Top5: 85.570    Loss: 1.758
INFO - ==> Sparsity : 0.424
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 39.130   Top5: 85.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.73153877
0.73147941
0.73137641
0.73109889
0.73083961
0.73070490
0.73050749
0.73028338
0.73024154
0.73008442
0.73004961
0.72997719
0.72973967
0.72942919
0.72917962
0.72889668
0.72871530
0.72838300
INFO - Training [5][   20/  196]   Loss 1.755593   Top1 38.007812   Top5 85.820312   BatchTime 0.432077   LR 0.002424
0.72792476
0.72752881
0.72729391
0.72723782
0.72723317
0.72733021
0.72751820
0.72755718
0.72758132
0.72779125
0.72776198
0.72770256
0.72757858
0.72763908
0.72728628
0.72698361
0.72699714
0.72678679
0.72608358
0.72606438
0.72566718
0.72542721
INFO - Training [5][   40/  196]   Loss 1.762706   Top1 38.173828   Top5 85.927734   BatchTime 0.396863   LR 0.002343
0.72549003
0.72528511
0.72509605
0.72467488
0.72429162
0.72398835
0.72379160
0.72368538
0.72371626
0.72369248
0.72354555
0.72335964
0.72298837
0.72279739
0.72260243
0.72247112
0.72225422
INFO - Training [5][   60/  196]   Loss 1.752006   Top1 38.457031   Top5 86.328125   BatchTime 0.383591   LR 0.002263
0.72215486
0.72196084
0.72183734
0.72186428
0.72183985
0.72172946
0.72172189
0.72177792
0.72173268
0.72143221
0.72112834
0.72109348
0.72106963
0.72094554
0.72069561
0.72030097
0.71995986
0.71952236
0.71921307
0.71909910
0.71909893
0.71918052
INFO - Training [5][   80/  196]   Loss 1.743796   Top1 38.945312   Top5 86.611328   BatchTime 0.381159   LR 0.002183
0.71905571
0.71880734
0.71850139
0.71788251
0.71699691
0.71608078
0.71532613
0.71453959
0.71321315
0.71170592
0.70999247
0.70842272
0.70678276
0.70539010
0.70382088
0.70251083
0.70120949
0.70003659
0.69960058
0.69914728
0.69837767
INFO - Training [5][  100/  196]   Loss 1.740004   Top1 39.230469   Top5 86.707031   BatchTime 0.379999   LR 0.002104
0.69781435
0.69708908
0.69576049
0.69347721
0.69046438
0.68590659
0.68390423
0.68335658
0.68096840
0.67816544
0.67392379
0.66996956
0.66762787
0.66519791
0.66630447
0.66863889
0.66972011
0.67079532
0.67540693
0.67872810
INFO - Training [5][  120/  196]   Loss 1.735879   Top1 39.537760   Top5 86.761068   BatchTime 0.365933   LR 0.002024
0.68056613
0.68224710
0.68453085
0.68509889
0.68570054
0.68623596
0.68659985
0.68687147
0.68707812
0.68801433
0.68812871
0.68850124
0.68914694
0.68956548
0.69027215
0.69071156
INFO - Training [5][  140/  196]   Loss 1.732885   Top1 39.542411   Top5 86.894531   BatchTime 0.366763   LR 0.001946
0.69178510
0.69274384
0.69416392
0.69513458
0.69611090
0.69766825
0.69907242
0.70018756
0.70000625
0.69992560
0.70012867
0.70006067
0.69934630
0.69936419
0.69982797
0.69955635
0.69942629
0.69918275
0.69907314
0.69932407
0.69933081
0.69924307
INFO - Training [5][  160/  196]   Loss 1.734322   Top1 39.492188   Top5 86.801758   BatchTime 0.368053   LR 0.001868
0.69914395
0.69914681
0.69917041
0.69821042
0.69598413
0.69405401
0.69283056
0.69161588
0.69159126
0.69241536
0.69880593
0.69816881
0.69791257
0.69737113
0.69729298
0.69774806
0.69872355
0.69911510
0.69956905
0.69965953
0.69948745
INFO - Training [5][  180/  196]   Loss 1.730622   Top1 39.661458   Top5 86.807726   BatchTime 0.369134   LR 0.001790
0.69939560
0.69896775
0.69840354
0.69806898
0.69773680
0.69828945
0.69819373
0.69820887
0.69821978
0.69757253
0.69757831
0.69751114
0.69668132
0.69594771
0.69493777
0.69407886
0.69376665
INFO - ==> Top1: 39.744    Top5: 86.912    Loss: 1.728
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [5][   20/   40]   Loss 1.537372   Top1 46.523438   Top5 89.609375   BatchTime 0.114606
INFO - Validation [5][   40/   40]   Loss 1.528224   Top1 46.500000   Top5 89.600000   BatchTime 0.083075
INFO - ==> Top1: 46.500    Top5: 89.600    Loss: 1.528
INFO - ==> Sparsity : 0.516
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 46.500   Top5: 89.600]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3242)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0486)
features.2.conv.3 tensor(0.0795)
features.2.conv.6 tensor(0.0825)
features.3.conv.0 tensor(0.0521)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0597)
features.4.conv.0 tensor(0.4128)
features.4.conv.3 tensor(0.1105)
features.4.conv.6 tensor(0.0895)
features.5.conv.0 tensor(0.0485)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0656)
features.6.conv.0 tensor(0.0365)
features.6.conv.3 tensor(0.0706)
features.6.conv.6 tensor(0.0725)
features.7.conv.0 tensor(0.0723)
features.7.conv.3 tensor(0.1340)
features.7.conv.6 tensor(0.6425)
features.8.conv.0 tensor(0.0710)
features.8.conv.3 tensor(0.1357)
features.8.conv.6 tensor(0.9213)
features.9.conv.0 tensor(0.0466)
features.9.conv.3 tensor(0.1577)
features.9.conv.6 tensor(0.8694)
features.10.conv.0 tensor(0.8886)
features.10.conv.3 tensor(0.2601)
features.10.conv.6 tensor(0.8017)
features.11.conv.0 tensor(0.1209)
features.11.conv.3 tensor(0.1848)
features.11.conv.6 tensor(0.2516)
features.12.conv.0 tensor(0.1000)
features.12.conv.3 tensor(0.1613)
features.12.conv.6 tensor(0.3240)
features.13.conv.0 tensor(0.1279)
features.13.conv.3 tensor(0.1667)
features.13.conv.6 tensor(0.3779)
features.14.conv.0 tensor(0.7818)
features.14.conv.3 tensor(0.1093)
features.14.conv.6 tensor(0.9184)
features.15.conv.0 tensor(0.6972)
features.15.conv.3 tensor(0.0652)
features.15.conv.6 tensor(0.9717)
features.16.conv.0 tensor(0.2417)
features.16.conv.3 tensor(0.0781)
features.16.conv.6 tensor(0.1806)
conv.0 tensor(0.7395)
tensor(1129108.) 2188896.0
0.69309211
0.69197422
0.69113368
0.69141716
0.69123358
0.69178748
0.69289273
0.69343263
0.69337946
0.69295937
0.69277054
0.69193637
0.69125324
0.69078165
0.69024086
0.68958360
0.68962312
0.68899208
0.68872893
0.68860400
0.68846399
INFO - Training [6][   20/  196]   Loss 1.700062   Top1 41.015625   Top5 87.480469   BatchTime 0.418066   LR 0.001655
0.68819100
0.68833363
0.68821067
0.68804783
0.68832070
0.68800503
0.68814987
0.68856418
0.68915832
0.68966138
0.68977082
0.69012928
0.69015223
0.68978363
0.69003105
0.68982750
0.69006884
0.69060069
0.69136053
0.69190091
INFO - Training [6][   40/  196]   Loss 1.708169   Top1 40.634766   Top5 87.685547   BatchTime 0.402175   LR 0.001580
0.69232643
0.69212729
0.69267780
0.69264627
0.69247216
0.69252604
0.69258791
0.69281250
0.69276148
0.69274700
0.69266033
0.69227523
0.69225389
0.69246179
0.69217068
0.69213885
0.69200957
INFO - Training [6][   60/  196]   Loss 1.697733   Top1 40.891927   Top5 87.942708   BatchTime 0.392102   LR 0.001506
0.69152486
0.69127029
0.69134092
0.69102931
0.69089520
0.69047302
0.69019324
0.68953055
0.68901479
0.68870211
0.68820697
0.68722755
0.68640190
0.68580592
0.68573189
0.68580317
0.68603694
0.68635166
0.68661743
0.68663913
0.68656981
0.68644482
INFO - Training [6][   80/  196]   Loss 1.693274   Top1 41.152344   Top5 88.051758   BatchTime 0.384913   LR 0.001432
0.68675476
0.68682671
0.68701875
0.68723184
0.68685645
0.68660706
0.68588132
0.68522948
0.68390375
0.68241853
0.67997634
0.67698604
0.67569351
0.67548114
0.67662948
0.67613679
0.67600906
0.67758018
0.67915505
0.68073487
0.68209791
0.68194908
INFO - Training [6][  100/  196]   Loss 1.685183   Top1 41.410156   Top5 88.128906   BatchTime 0.380924   LR 0.001360
0.68076336
0.67944723
0.67787808
0.67581916
0.67605919
0.67627084
0.67702574
0.67702204
0.67629355
0.67585105
0.67517191
0.67295510
0.66937828
0.66788024
0.66890097
0.66970569
0.67032099
0.66891110
0.66829491
INFO - Training [6][  120/  196]   Loss 1.679174   Top1 41.595052   Top5 88.300781   BatchTime 0.371268   LR 0.001289
0.66825658
0.66877413
0.66996944
0.67133564
0.67142469
0.67166167
0.67269605
0.67135477
0.66964310
0.66809475
0.66699344
0.66579449
0.66432428
0.66453618
0.66434920
0.66454995
INFO - Training [6][  140/  196]   Loss 1.679170   Top1 41.640625   Top5 88.158482   BatchTime 0.369222   LR 0.001220
0.66556811
0.66584748
0.66600794
0.66605288
0.66626626
0.66641605
0.66675943
0.66714007
0.66757923
0.66796988
0.66805780
0.66839248
0.66846585
0.66946918
0.67080384
0.67135763
0.67210585
0.67257106
0.67296547
0.67283833
0.67332244
0.67332923
INFO - Training [6][  160/  196]   Loss 1.681945   Top1 41.506348   Top5 88.061523   BatchTime 0.368850   LR 0.001151
0.67233497
0.67199951
0.67209148
0.67248762
0.67321420
0.67422533
0.67527002
0.67597365
0.67614633
0.67639971
0.67687696
0.67696416
0.67720169
0.67726946
0.67705637
0.67686164
0.67653298
INFO - Training [6][  180/  196]   Loss 1.679995   Top1 41.584201   Top5 88.027344   BatchTime 0.366728   LR 0.001084
0.67607498
0.67524910
0.67402464
0.67265999
0.67133951
0.66967231
0.66854614
0.66770166
0.66689569
0.66624218
0.66532153
0.66466779
0.66475487
0.66535729
0.66578823
0.66632420
0.66662830
0.66652793
INFO - ==> Top1: 41.754    Top5: 88.094    Loss: 1.676
0.66679305
0.66699195
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [6][   20/   40]   Loss 1.396693   Top1 49.316406   Top5 92.910156   BatchTime 0.112366
INFO - Validation [6][   40/   40]   Loss 1.391666   Top1 49.530000   Top5 92.930000   BatchTime 0.082587
INFO - ==> Top1: 49.530    Top5: 92.930    Loss: 1.392
INFO - ==> Sparsity : 0.600
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 49.530   Top5: 92.930]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.3262)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.0772)
features.2.conv.6 tensor(0.0833)
features.3.conv.0 tensor(0.0521)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.0599)
features.4.conv.0 tensor(0.4144)
features.4.conv.3 tensor(0.1117)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0485)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0674)
features.6.conv.0 tensor(0.0374)
features.6.conv.3 tensor(0.0718)
features.6.conv.6 tensor(0.0729)
features.7.conv.0 tensor(0.0715)
features.7.conv.3 tensor(0.1331)
features.7.conv.6 tensor(0.6023)
features.8.conv.0 tensor(0.0733)
features.8.conv.3 tensor(0.1345)
features.8.conv.6 tensor(0.9257)
features.9.conv.0 tensor(0.0486)
features.9.conv.3 tensor(0.1591)
features.9.conv.6 tensor(0.8879)
features.10.conv.0 tensor(0.8855)
features.10.conv.3 tensor(0.2772)
features.10.conv.6 tensor(0.8028)
features.11.conv.0 tensor(0.1238)
features.11.conv.3 tensor(0.1842)
features.11.conv.6 tensor(0.2520)
features.12.conv.0 tensor(0.1004)
features.12.conv.3 tensor(0.1580)
features.12.conv.6 tensor(0.3239)
features.13.conv.0 tensor(0.1306)
features.13.conv.3 tensor(0.1645)
features.13.conv.6 tensor(0.4434)
features.14.conv.0 tensor(0.7859)
features.14.conv.3 tensor(0.1063)
features.14.conv.6 tensor(0.9172)
features.15.conv.0 tensor(0.6994)
features.15.conv.3 tensor(0.0671)
features.15.conv.6 tensor(0.9685)
features.16.conv.0 tensor(0.2776)
features.16.conv.3 tensor(0.0829)
features.16.conv.6 tensor(0.7117)
conv.0 tensor(0.7633)
tensor(1313934.) 2188896.0
0.66651326
0.66646731
0.66651970
0.66652888
0.66630608
0.66569567
0.66517043
0.66491282
0.66471839
0.66444737
0.66424000
0.66384274
0.66424358
0.66556889
0.66741669
0.66888052
0.66987848
0.67049766
0.67142659
0.67245829
0.67394745
INFO - Training [7][   20/  196]   Loss 1.637637   Top1 42.226562   Top5 87.910156   BatchTime 0.441129   LR 0.000969
0.67516834
0.67603254
0.67666870
0.67717499
0.67764348
0.67775393
0.67768723
0.67759538
0.67734808
0.67711163
0.67702109
0.67679721
0.67663860
0.67630929
0.67592281
0.67554802
INFO - Training [7][   40/  196]   Loss 1.648018   Top1 42.294922   Top5 88.076172   BatchTime 0.402087   LR 0.000907
0.67515224
0.67482758
0.67449367
0.67415673
0.67416662
0.67400688
0.67390209
0.67380464
0.67357528
0.67347944
0.67351562
0.67342931
0.67324531
0.67288464
0.67270523
0.67255008
0.67247367
0.67219198
0.67164171
0.67107081
0.67010748
0.66924274
INFO - Training [7][   60/  196]   Loss 1.646733   Top1 42.447917   Top5 88.300781   BatchTime 0.392909   LR 0.000845
0.66836035
0.66765326
0.66700512
0.66613090
0.66569006
0.66522825
0.66434908
0.66340357
0.66200674
0.66128236
0.66065878
0.66033024
0.66051197
0.65999609
0.65997565
0.66028273
0.65979534
0.65932620
0.65880048
0.65803438
0.65751743
INFO - Training [7][   80/  196]   Loss 1.643881   Top1 42.587891   Top5 88.505859   BatchTime 0.387267   LR 0.000786
0.65717560
0.65625340
0.65512109
0.65442258
0.65367633
0.65301049
0.65253830
0.65182710
0.65131617
0.65108639
0.65060633
0.64961547
0.64828885
0.64759535
0.64774638
0.64765126
0.64756799
INFO - Training [7][  100/  196]   Loss 1.641764   Top1 42.824219   Top5 88.460938   BatchTime 0.381917   LR 0.000728
0.64734519
0.64692241
0.64642543
0.64585209
0.64541990
0.64478838
0.64436918
0.64414048
0.64385629
0.64347357
0.64330214
0.64313340
0.64316440
0.64311284
0.64311051
0.64316738
0.64326507
0.64350373
INFO - Training [7][  120/  196]   Loss 1.639449   Top1 42.919922   Top5 88.645833   BatchTime 0.370486   LR 0.000673
0.64364719
0.64377457
0.64371723
0.64356780
0.64347917
0.64343095
0.64345062
0.64362216
0.64367837
0.64368075
0.64407021
0.64438397
0.64449859
0.64476037
0.64515138
0.64542866
0.64549536
0.64536852
0.64528614
0.64498454
0.64457726
0.64449477
0.64427495
0.64395356
0.64368373
0.64376515
0.64362311
INFO - Training [7][  140/  196]   Loss 1.641359   Top1 42.753906   Top5 88.730469   BatchTime 0.362220   LR 0.000619
0.64352101
0.64359051
0.64377874
0.64402449
0.64431900
0.64460486
0.64498687
0.64516157
0.64511579
0.64510572
0.64501899
0.64473706
0.64440954
0.64433944
0.64448065
0.64441043
0.64410627
INFO - Training [7][  160/  196]   Loss 1.645889   Top1 42.666016   Top5 88.566895   BatchTime 0.359094   LR 0.000567
0.64394933
0.64382327
0.64346647
0.64320588
0.64307457
0.64259464
0.64217883
0.64196026
0.64216572
0.64194745
0.64161378
0.64161044
0.64179462
0.64186198
0.64169413
0.64118218
0.64048111
0.64011049
0.64017117
0.64000690
0.64004070
0.64000922
INFO - Training [7][  180/  196]   Loss 1.643308   Top1 42.829861   Top5 88.613281   BatchTime 0.360243   LR 0.000517
0.64012486
0.64056182
0.64094198
0.64114583
0.64125770
0.64138913
0.64137965
0.64128792
0.64114583
0.64103854
0.64093524
0.64106977
0.64126444
0.64126164
0.64133555
********************pre-trained*****************
INFO - ==> Top1: 42.962    Top5: 88.636    Loss: 1.641
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [7][   20/   40]   Loss 1.385136   Top1 49.726562   Top5 92.851562   BatchTime 0.112420
INFO - Validation [7][   40/   40]   Loss 1.392155   Top1 49.320000   Top5 92.740000   BatchTime 0.082353
INFO - ==> Top1: 49.320    Top5: 92.740    Loss: 1.392
INFO - ==> Sparsity : 0.615
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 49.530   Top5: 92.930]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5556)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.0779)
features.2.conv.6 tensor(0.0833)
features.3.conv.0 tensor(0.0515)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0603)
features.4.conv.0 tensor(0.4251)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0483)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0687)
features.6.conv.0 tensor(0.0379)
features.6.conv.3 tensor(0.0735)
features.6.conv.6 tensor(0.0737)
features.7.conv.0 tensor(0.0725)
features.7.conv.3 tensor(0.1340)
features.7.conv.6 tensor(0.7217)
features.8.conv.0 tensor(0.0791)
features.8.conv.3 tensor(0.1343)
features.8.conv.6 tensor(0.9332)
features.9.conv.0 tensor(0.0494)
features.9.conv.3 tensor(0.1580)
features.9.conv.6 tensor(0.8691)
features.10.conv.0 tensor(0.8870)
features.10.conv.3 tensor(0.2720)
features.10.conv.6 tensor(0.8047)
features.11.conv.0 tensor(0.1246)
features.11.conv.3 tensor(0.1819)
features.11.conv.6 tensor(0.2514)
features.12.conv.0 tensor(0.1002)
features.12.conv.3 tensor(0.1576)
features.12.conv.6 tensor(0.3247)
features.13.conv.0 tensor(0.1380)
features.13.conv.3 tensor(0.1644)
features.13.conv.6 tensor(0.5097)
features.14.conv.0 tensor(0.7882)
features.14.conv.3 tensor(0.1063)
features.14.conv.6 tensor(0.9244)
features.15.conv.0 tensor(0.7015)
features.15.conv.3 tensor(0.0669)
features.15.conv.6 tensor(0.9705)
features.16.conv.0 tensor(0.5195)
features.16.conv.3 tensor(0.0832)
features.16.conv.6 tensor(0.6199)
conv.0 tensor(0.7933)
tensor(1346879.) 2188896.0
0.64134473
0.64133811
0.64120114
0.64108133
0.64096540
0.64085859
0.64080572
0.64080447
0.64087856
0.64089030
0.64088613
0.64067692
0.64049697
0.64035922
0.64008778
0.63973385
INFO - Training [8][   20/  196]   Loss 1.635266   Top1 43.476562   Top5 88.476562   BatchTime 0.398897   LR 0.000434
0.63950360
0.63930410
0.63915670
0.63877767
0.63842171
0.63805717
0.63764900
0.63654768
0.63579953
0.63484281
0.63406712
0.63277882
0.63166004
0.63075930
0.63070661
0.63069248
0.63099426
0.63102788
0.63097799
0.63121766
0.63150954
0.63148260
0.63125569
INFO - Training [8][   40/  196]   Loss 1.652332   Top1 42.958984   Top5 88.046875   BatchTime 0.377744   LR 0.000389
0.63103604
0.63085908
0.63085234
0.63054723
0.63035899
0.63029224
0.63039690
0.63073951
0.63090813
0.63093758
0.63116723
0.63114774
0.63129193
0.63154513
0.63147879
0.63155150
0.63164556
0.63225567
0.63280880
0.63332844
0.63349271
0.63376313
INFO - Training [8][   60/  196]   Loss 1.640685   Top1 43.450521   Top5 88.476562   BatchTime 0.376639   LR 0.000347
0.63424259
0.63469923
0.63505441
0.63529789
0.63560361
0.63571978
0.63601708
0.63614249
0.63646752
0.63663667
0.63675582
0.63674450
0.63658112
0.63652509
0.63641816
0.63612610
INFO - Training [8][   80/  196]   Loss 1.639243   Top1 43.505859   Top5 88.540039   BatchTime 0.371216   LR 0.000308
0.63587058
0.63573110
0.63563752
0.63544893
0.63519382
0.63501126
0.63481057
0.63464916
0.63448995
0.63432497
0.63422143
0.63422292
0.63431656
0.63433892
0.63435632
0.63432574
0.63429350
0.63427192
0.63418001
0.63411361
0.63407600
0.63397229
0.63390654
INFO - Training [8][  100/  196]   Loss 1.633412   Top1 43.613281   Top5 88.628906   BatchTime 0.366386   LR 0.000270
0.63389492
0.63371879
0.63361752
0.63344038
0.63325727
0.63315785
0.63307232
0.63306403
0.63312274
0.63306302
0.63295197
0.63289922
0.63291526
0.63287675
0.63291585
0.63301438
0.63310397
0.63319033
INFO - Training [8][  120/  196]   Loss 1.631051   Top1 43.629557   Top5 88.776042   BatchTime 0.363030   LR 0.000235
0.63325095
0.63327974
0.63329607
0.63328648
0.63321590
0.63308758
0.63304275
0.63303041
0.63291246
0.63283819
0.63282067
0.63272959
0.63269436
0.63265383
0.63260788
0.63260925
0.63259858
0.63248318
0.63230789
0.63222861
0.63210654
0.63204271
INFO - Training [8][  140/  196]   Loss 1.629273   Top1 43.702567   Top5 88.850446   BatchTime 0.362544   LR 0.000202
0.63193423
0.63183933
0.63167191
0.63147753
0.63135773
0.63116467
0.63107491
0.63092917
0.63074213
0.63049823
0.63035452
0.63027525
0.63023430
0.63022286
0.63029414
0.63034743
0.63043201
INFO - Training [8][  160/  196]   Loss 1.628588   Top1 43.688965   Top5 88.867188   BatchTime 0.362950   LR 0.000172
0.63044751
0.63048846
0.63058579
0.63065308
0.63067883
0.63073295
0.63080156
0.63091648
0.63102263
0.63111776
0.63113987
0.63116634
0.63115871
0.63116634
0.63112980
0.63106030
0.63095713
0.63094103
0.63089436
0.63090932
0.63083953
0.63078654
0.63071489
0.63071406
INFO - Training [8][  180/  196]   Loss 1.626816   Top1 43.728299   Top5 88.849826   BatchTime 0.358931   LR 0.000143
0.63070542
0.63065630
0.63060749
0.63053077
0.63046491
0.63043696
0.63037902
0.63030368
0.63023853
0.63017350
0.63009328
0.63010406
0.63009912
0.63009208
0.63014042
********************pre-trained*****************
INFO - ==> Top1: 43.832    Top5: 88.888    Loss: 1.624
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [8][   20/   40]   Loss 1.345832   Top1 51.914062   Top5 93.046875   BatchTime 0.116365
INFO - Validation [8][   40/   40]   Loss 1.353000   Top1 51.490000   Top5 93.110000   BatchTime 0.084939
INFO - ==> Top1: 51.490    Top5: 93.110    Loss: 1.353
INFO - ==> Sparsity : 0.638
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 51.490   Top5: 93.110]
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.3945)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0660)
features.2.conv.0 tensor(0.0495)
features.2.conv.3 tensor(0.0779)
features.2.conv.6 tensor(0.0833)
features.3.conv.0 tensor(0.0521)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.0597)
features.4.conv.0 tensor(0.4421)
features.4.conv.3 tensor(0.1111)
features.4.conv.6 tensor(0.0905)
features.5.conv.0 tensor(0.0483)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0695)
features.6.conv.0 tensor(0.0378)
features.6.conv.3 tensor(0.0712)
features.6.conv.6 tensor(0.0741)
features.7.conv.0 tensor(0.0730)
features.7.conv.3 tensor(0.1348)
features.7.conv.6 tensor(0.7282)
features.8.conv.0 tensor(0.0849)
features.8.conv.3 tensor(0.1345)
features.8.conv.6 tensor(0.9341)
features.9.conv.0 tensor(0.0498)
features.9.conv.3 tensor(0.1580)
features.9.conv.6 tensor(0.8782)
features.10.conv.0 tensor(0.8871)
features.10.conv.3 tensor(0.2726)
features.10.conv.6 tensor(0.8049)
features.11.conv.0 tensor(0.1252)
features.11.conv.3 tensor(0.1836)
features.11.conv.6 tensor(0.2515)
features.12.conv.0 tensor(0.1004)
features.12.conv.3 tensor(0.1574)
features.12.conv.6 tensor(0.3249)
features.13.conv.0 tensor(0.1488)
features.13.conv.3 tensor(0.1622)
features.13.conv.6 tensor(0.5792)
features.14.conv.0 tensor(0.7895)
features.14.conv.3 tensor(0.1041)
features.14.conv.6 tensor(0.9358)
features.15.conv.0 tensor(0.7020)
features.15.conv.3 tensor(0.0666)
features.15.conv.6 tensor(0.9725)
features.16.conv.0 tensor(0.5822)
features.16.conv.3 tensor(0.0809)
features.16.conv.6 tensor(0.7156)
conv.0 tensor(0.7930)
tensor(1395833.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.63015145
0.63016683
0.63016826
0.63018376
0.63020676
0.63018376
0.63020295
0.63021839
0.63025051
0.63024151
0.63023210
0.63025528
0.63019496
0.63015366
0.63012671
0.63010544
0.63011259
0.63003230
0.63001049
INFO - Training [9][   20/  196]   Loss 1.627125   Top1 43.125000   Top5 88.691406   BatchTime 0.440002   LR 0.000100
0.63003397
0.63004243
0.63002419
0.63002330
0.63003826
0.63001138
0.63002956
0.63002837
0.62992257
0.62989223
0.62993026
0.62989175
0.62986201
0.62983423
0.62972522
0.62964988
0.62963253
0.62954158
0.62949049
0.62946618
0.62941206
0.62944996
0.62948412
INFO - Training [9][   40/  196]   Loss 1.622228   Top1 43.486328   Top5 88.779297   BatchTime 0.398253   LR 0.000079
0.62949365
0.62946206
0.62938708
0.62940079
0.62937510
0.62935245
0.62935567
0.62936556
0.62934506
0.62933820
0.62936491
0.62931705
0.62931859
0.62928331
0.62925237
INFO - Training [9][   60/  196]   Loss 1.616052   Top1 43.919271   Top5 88.815104   BatchTime 0.394446   LR 0.000060
0.62926722
0.62926167
0.62923843
0.62931430
0.62937301
0.62936175
0.62938946
0.62939101
0.62943780
0.62944984
0.62945294
0.62944007
0.62944973
0.62937480
0.62939209
0.62941152
0.62941790
0.62941200
0.62939912
0.62939799
0.62938344
0.62937313
INFO - Training [9][   80/  196]   Loss 1.617051   Top1 43.740234   Top5 88.940430   BatchTime 0.385772   LR 0.000044
0.62933922
0.62932736
0.62930584
0.62929380
0.62928075
0.62925380
0.62925875
0.62923086
0.62920666
0.62922114
0.62919480
0.62920147
0.62916702
0.62915778
0.62911314
0.62913054
0.62913740
0.62911570
0.62909645
0.62907141
0.62903726
0.62904012
INFO - Training [9][  100/  196]   Loss 1.612857   Top1 43.894531   Top5 89.109375   BatchTime 0.381051   LR 0.000030
0.62903911
0.62902296
0.62904167
0.62903821
0.62901461
0.62900060
0.62897009
0.62901735
0.62900746
0.62901932
0.62904507
0.62901294
0.62901169
0.62900692
0.62901300
0.62899828
INFO - Training [9][  120/  196]   Loss 1.611648   Top1 43.776042   Top5 89.186198   BatchTime 0.381776   LR 0.000019
0.62900543
0.62900007
0.62902749
0.62901348
0.62900788
0.62903905
0.62904602
0.62906545
0.62905699
0.62907749
0.62909496
0.62910694
0.62908828
0.62905842
0.62901264
0.62903398
0.62901419
0.62900019
0.62902248
0.62905067
0.62902093
0.62900513
INFO - Training [9][  140/  196]   Loss 1.610988   Top1 43.842076   Top5 89.241071   BatchTime 0.378536   LR 0.000010
0.62899613
0.62902278
0.62901944
0.62901932
0.62899953
0.62899834
0.62900198
0.62896973
0.62897301
0.62893903
0.62894082
0.62896675
0.62894338
0.62896824
0.62896252
0.62894547
0.62894386
0.62894112
0.62890488
INFO - Training [9][  160/  196]   Loss 1.613489   Top1 43.859863   Top5 89.162598   BatchTime 0.371879   LR 0.000004
0.62888128
0.62888360
0.62888789
0.62888473
0.62886268
0.62886733
0.62883824
0.62882417
0.62882549
0.62881637
0.62881535
0.62880009
0.62879157
0.62878782
0.62878937
0.62877935
0.62879187
0.62879229
0.62880856
0.62878948
INFO - Training [9][  180/  196]   Loss 1.613521   Top1 43.823785   Top5 89.134115   BatchTime 0.362007   LR 0.000001
0.62879002
0.62879246
0.62880373
0.62881279
0.62881172
0.62880909
0.62880892
0.62879688
0.62876946
0.62874371
0.62874448
0.62873608
0.62874013
0.62877589
0.62876731
0.62876886
0.62877333
0.62874120
INFO - ==> Top1: 43.926    Top5: 89.148    Loss: 1.611
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [9][   20/   40]   Loss 1.406230   Top1 48.867188   Top5 92.148438   BatchTime 0.125170
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.0779)
features.2.conv.6 tensor(0.0833)
features.3.conv.0 tensor(0.0518)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0601)
features.4.conv.0 tensor(0.4417)
features.4.conv.3 tensor(0.1117)
features.4.conv.6 tensor(0.0903)
features.5.conv.0 tensor(0.0482)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0697)
features.6.conv.0 tensor(0.0374)
features.6.conv.3 tensor(0.0712)
features.6.conv.6 tensor(0.0742)
features.7.conv.0 tensor(0.0729)
features.7.conv.3 tensor(0.1343)
features.7.conv.6 tensor(0.7259)
features.8.conv.0 tensor(0.0857)
features.8.conv.3 tensor(0.1354)
features.8.conv.6 tensor(0.9338)
features.9.conv.0 tensor(0.0503)
features.9.conv.3 tensor(0.1560)
features.9.conv.6 tensor(0.8796)
features.10.conv.0 tensor(0.8876)
features.10.conv.3 tensor(0.2731)
features.10.conv.6 tensor(0.8044)
features.11.conv.0 tensor(0.1252)
features.11.conv.3 tensor(0.1838)
features.11.conv.6 tensor(0.2514)
features.12.conv.0 tensor(0.1008)
features.12.conv.3 tensor(0.1568)
features.12.conv.6 tensor(0.3246)
features.13.conv.0 tensor(0.1510)
features.13.conv.3 tensor(0.1620)
features.13.conv.6 tensor(0.5837)
features.14.conv.0 tensor(0.7896)
features.14.conv.3 tensor(0.1041)
features.14.conv.6 tensor(0.9371)
features.15.conv.0 tensor(0.7020)
features.15.conv.3 tensor(0.0675)
features.15.conv.6 tensor(0.9726)
features.16.conv.0 tensor(0.6029)
features.16.conv.3 tensor(0.0807)
features.16.conv.6 tensor(0.7130)
conv.0 tensor(0.7990)
tensor(1401380.) 2188896.0
INFO - Validation [9][   40/   40]   Loss 1.403055   Top1 49.080000   Top5 92.520000   BatchTime 0.088673
INFO - ==> Top1: 49.080    Top5: 92.520    Loss: 1.403
INFO - ==> Sparsity : 0.640
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 51.490   Top5: 93.110]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.62877071
0.63060522
0.63150352
0.63196224
0.63220000
0.63242239
0.63329417
0.63279366
0.63353479
0.63464731
0.63693500
0.63920838
0.64213747
0.64490843
0.64697468
0.64813739
0.64848101
0.64875817
0.65007412
0.65168762
INFO - Training [10][   20/  196]   Loss 1.652930   Top1 42.226562   Top5 87.753906   BatchTime 0.405811   LR 0.002500
0.65392637
0.65590149
0.65562725
0.65322798
0.65174419
0.65068364
0.64973170
0.64909214
0.64878738
0.64811450
0.64797795
0.64835852
0.64861763
0.64876795
0.64878947
0.64862311
0.64881921
0.64908254
0.64972955
0.65050590
0.65124434
0.65139174
INFO - Training [10][   40/  196]   Loss 1.660289   Top1 41.943359   Top5 88.105469   BatchTime 0.385936   LR 0.002499
0.65182853
0.65181357
0.65213060
0.65236628
0.65270311
0.65281838
0.65347368
0.65479779
0.65526587
0.65569139
0.65573382
0.65611959
0.65626925
0.65717977
0.65843260
0.65893018
0.65905738
INFO - Training [10][   60/  196]   Loss 1.659292   Top1 42.141927   Top5 87.942708   BatchTime 0.376432   LR 0.002499
0.65876728
0.65794754
0.65713334
0.65619695
0.65460265
0.65384650
0.65294844
0.65339786
0.65330845
0.65303051
0.65228540
0.65202355
0.65196759
0.65150779
0.65056467
0.64966720
0.64879799
0.64762127
INFO - Training [10][   80/  196]   Loss 1.663661   Top1 42.016602   Top5 88.120117   BatchTime 0.364844   LR 0.002497
0.64655471
0.64607275
0.64578718
0.64488798
0.64434832
0.64440495
0.64427906
0.64456326
0.64486170
0.64535499
0.64630991
0.64670485
0.64738190
0.64826602
0.64922971
0.65040761
0.65136760
0.65239555
0.65375006
0.65535134
0.65707803
0.65811652
0.65886742
INFO - Training [10][  100/  196]   Loss 1.660145   Top1 42.125000   Top5 88.292969   BatchTime 0.360842   LR 0.002496
0.66027027
0.66186482
0.66275740
0.66321594
0.66353697
0.66382939
0.66403401
0.66386181
0.66355455
0.66295761
0.66231245
0.66183811
0.66083896
0.65994966
0.65895802
0.65795416
0.65701461
0.65611356
INFO - Training [10][  120/  196]   Loss 1.655154   Top1 42.382812   Top5 88.466797   BatchTime 0.359225   LR 0.002494
0.65520865
0.65404749
0.65312058
0.65187567
0.65106237
0.65052247
0.64995831
0.64974838
0.64936471
0.64858669
0.64743340
0.64630574
0.64469188
0.64264566
0.64034104
0.63795620
0.63608599
0.63546473
0.63482672
0.63384253
0.63278842
INFO - Training [10][  140/  196]   Loss 1.650256   Top1 42.650670   Top5 88.577009   BatchTime 0.361909   LR 0.002492
0.63176143
0.63101667
0.62951618
0.62761605
0.62685698
0.62652087
0.62722617
0.62820452
0.62930119
0.63070506
0.63342863
0.63320905
0.63414013
0.63482535
0.63605809
0.63897240
0.64175737
0.64438301
0.64642537
0.64814818
0.64939553
0.65033495
0.65065241
INFO - Training [10][  160/  196]   Loss 1.651398   Top1 42.487793   Top5 88.535156   BatchTime 0.360266   LR 0.002490
0.65041494
0.65030223
0.65007484
0.64951473
0.64918572
0.64831865
0.64762306
0.64685249
0.64626729
0.64636737
0.64653128
0.64648348
0.64623094
0.64622551
0.64619219
INFO - Training [10][  180/  196]   Loss 1.649902   Top1 42.615017   Top5 88.485243   BatchTime 0.363093   LR 0.002487
0.64619309
0.64592642
0.64596796
0.64604092
0.64587760
0.64537561
0.64446706
0.64324570
0.64250511
0.64191550
0.64174628
0.64165032
0.64138132
0.64154494
0.64142525
0.64073867
0.64007962
0.63972425
0.64088386
INFO - ==> Top1: 42.766    Top5: 88.550    Loss: 1.647
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [10][   20/   40]   Loss 1.395709   Top1 50.273438   Top5 92.851562   BatchTime 0.116885
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.3945)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0495)
features.2.conv.3 tensor(0.0810)
features.2.conv.6 tensor(0.0830)
features.3.conv.0 tensor(0.0527)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.0597)
features.4.conv.0 tensor(0.4131)
features.4.conv.3 tensor(0.1123)
features.4.conv.6 tensor(0.0916)
features.5.conv.0 tensor(0.0482)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0724)
features.6.conv.0 tensor(0.0381)
features.6.conv.3 tensor(0.0706)
features.6.conv.6 tensor(0.0768)
features.7.conv.0 tensor(0.0711)
features.7.conv.3 tensor(0.1360)
features.7.conv.6 tensor(0.7159)
features.8.conv.0 tensor(0.4375)
features.8.conv.3 tensor(0.1357)
features.8.conv.6 tensor(0.9406)
features.9.conv.0 tensor(0.0472)
features.9.conv.3 tensor(0.1551)
features.9.conv.6 tensor(0.8962)
features.10.conv.0 tensor(0.8893)
features.10.conv.3 tensor(0.2789)
features.10.conv.6 tensor(0.8099)
features.11.conv.0 tensor(0.1204)
features.11.conv.3 tensor(0.1807)
features.11.conv.6 tensor(0.2503)
features.12.conv.0 tensor(0.0891)
features.12.conv.3 tensor(0.1615)
features.12.conv.6 tensor(0.3258)
features.13.conv.0 tensor(0.1206)
features.13.conv.3 tensor(0.1684)
features.13.conv.6 tensor(0.5446)
features.14.conv.0 tensor(0.7912)
features.14.conv.3 tensor(0.1060)
features.14.conv.6 tensor(0.9270)
features.15.conv.0 tensor(0.7046)
features.15.conv.3 tensor(0.0696)
features.15.conv.6 tensor(0.9709)
features.16.conv.0 tensor(0.0970)
features.16.conv.3 tensor(0.0814)
features.16.conv.6 tensor(0.6826)
conv.0 tensor(0.7543)
tensor(1297806.) 2188896.0
INFO - Validation [10][   40/   40]   Loss 1.403732   Top1 50.080000   Top5 92.910000   BatchTime 0.088160
INFO - ==> Top1: 50.080    Top5: 92.910    Loss: 1.404
INFO - ==> Sparsity : 0.593
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 51.490   Top5: 93.110]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
0.64142716
0.64205235
0.64229643
0.64239657
0.64261436
0.64267451
0.64314038
0.64361805
0.64402914
0.64438075
0.64462763
0.64443356
0.64444137
0.64476830
0.64495051
0.64499712
0.64507371
0.64510804
0.64431149
0.64328921
0.64246166
0.64766550
INFO - Training [11][   20/  196]   Loss 1.614631   Top1 43.554688   Top5 88.339844   BatchTime 0.371903   LR 0.002481
0.64911950
0.64834929
0.64781123
0.64696974
0.64654642
0.64621836
0.64536375
0.64439160
0.64359725
0.64420933
0.64381814
0.64219528
0.64098299
0.64008737
0.63933229
0.63868213
0.63865000
INFO - Training [11][   40/  196]   Loss 1.625259   Top1 43.281250   Top5 88.505859   BatchTime 0.360461   LR 0.002478
0.63878000
0.63922215
0.63965499
0.64049965
0.64169955
0.64250916
0.64293915
0.64357746
0.64366645
0.64397383
0.64470017
0.64517790
0.64544612
0.64580303
0.64608711
0.64573997
0.64614886
0.64577252
0.64512426
0.64472044
0.64416122
0.64326459
INFO - Training [11][   60/  196]   Loss 1.632524   Top1 43.196615   Top5 88.502604   BatchTime 0.365077   LR 0.002474
0.64240646
0.64116514
0.64023525
0.63955766
0.63924527
0.63934863
0.63948226
0.63954061
0.63946873
0.63944578
0.63934761
0.63921869
0.63889706
0.63892764
0.63878590
0.63905996
0.63930714
INFO - Training [11][   80/  196]   Loss 1.626960   Top1 43.564453   Top5 88.632812   BatchTime 0.362113   LR 0.002470
0.63926917
0.63909465
0.63891768
0.63880694
0.63878191
0.63889682
0.63936669
0.64054763
0.64141184
0.64250809
0.64347398
0.64453858
0.64555180
0.64670831
0.64757705
0.64827335
0.64927459
0.65018994
0.65134388
0.65319657
0.65461212
0.65460861
INFO - Training [11][  100/  196]   Loss 1.623205   Top1 43.613281   Top5 88.746094   BatchTime 0.362921   LR 0.002465
0.65441358
0.65413696
0.65383321
0.65299892
0.65197331
0.65165472
0.65119332
0.65058506
0.64995080
0.64969456
0.64946365
0.64933985
0.64975119
0.65054828
0.65120935
0.65149331
0.65189320
0.65127397
0.65045381
0.65031642
INFO - Training [11][  120/  196]   Loss 1.621094   Top1 43.860677   Top5 88.805339   BatchTime 0.369310   LR 0.002460
0.64994395
0.64941859
0.64910942
0.64890277
0.64924586
0.64975595
0.64962435
0.64942384
0.64918619
0.64891243
0.64863026
0.64815086
0.64760524
0.64700091
0.64663291
0.64611930
0.64587545
0.64604980
0.64644891
0.64699042
0.64713740
0.64753866
INFO - Training [11][  140/  196]   Loss 1.620185   Top1 43.925781   Top5 88.856027   BatchTime 0.368134   LR 0.002455
0.64791727
0.64817274
0.64856833
0.64903563
0.64922577
0.64953727
0.64970547
0.65019304
0.65076786
0.65081763
0.65054399
0.65014118
0.64989418
0.64975893
0.64960492
0.64959908
INFO - Training [11][  160/  196]   Loss 1.621244   Top1 43.835449   Top5 88.830566   BatchTime 0.367692   LR 0.002450
0.64928985
0.64926046
0.64976758
0.65083849
0.65144724
0.65152520
0.65203196
0.65256596
0.65326488
0.65399605
0.65418750
0.65345496
0.65262258
0.65171659
0.65093356
0.64970893
0.64826560
0.64652890
0.64430946
0.64258665
0.64036828
0.63878322
0.63749313
0.63604593
INFO - Training [11][  180/  196]   Loss 1.614565   Top1 44.049479   Top5 88.882378   BatchTime 0.364732   LR 0.002444
0.63438046
0.63284469
0.63167584
0.63019848
0.62898964
0.62825912
0.62731892
0.62662137
0.62687606
0.62697136
0.62718564
0.62649709
0.62584037
0.62568265
********************pre-trained*****************
INFO - ==> Top1: 44.158    Top5: 88.978    Loss: 1.612
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [11][   20/   40]   Loss 1.452486   Top1 50.117188   Top5 92.597656   BatchTime 0.112775
INFO - Validation [11][   40/   40]   Loss 1.455675   Top1 49.620000   Top5 92.750000   BatchTime 0.081909
INFO - ==> Top1: 49.620    Top5: 92.750    Loss: 1.456
INFO - ==> Sparsity : 0.620
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 51.490   Top5: 93.110]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3965)
features.1.conv.0 tensor(0.0521)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0503)
features.2.conv.3 tensor(0.0826)
features.2.conv.6 tensor(0.0833)
features.3.conv.0 tensor(0.0544)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0597)
features.4.conv.0 tensor(0.5049)
features.4.conv.3 tensor(0.1123)
features.4.conv.6 tensor(0.0924)
features.5.conv.0 tensor(0.0492)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0750)
features.6.conv.0 tensor(0.0381)
features.6.conv.3 tensor(0.0712)
features.6.conv.6 tensor(0.0775)
features.7.conv.0 tensor(0.0710)
features.7.conv.3 tensor(0.1348)
features.7.conv.6 tensor(0.7884)
features.8.conv.0 tensor(0.7936)
features.8.conv.3 tensor(0.1400)
features.8.conv.6 tensor(0.9486)
features.9.conv.0 tensor(0.0512)
features.9.conv.3 tensor(0.1568)
features.9.conv.6 tensor(0.9080)
features.10.conv.0 tensor(0.8839)
features.10.conv.3 tensor(0.2917)
features.10.conv.6 tensor(0.8078)
features.11.conv.0 tensor(0.1229)
features.11.conv.3 tensor(0.1815)
features.11.conv.6 tensor(0.2508)
features.12.conv.0 tensor(0.0904)
features.12.conv.3 tensor(0.1630)
features.12.conv.6 tensor(0.3249)
features.13.conv.0 tensor(0.1198)
features.13.conv.3 tensor(0.1694)
features.13.conv.6 tensor(0.1256)
features.14.conv.0 tensor(0.7934)
features.14.conv.3 tensor(0.1069)
features.14.conv.6 tensor(0.9144)
features.15.conv.0 tensor(0.7089)
features.15.conv.3 tensor(0.0731)
features.15.conv.6 tensor(0.9728)
features.16.conv.0 tensor(0.2762)
features.16.conv.3 tensor(0.0818)
features.16.conv.6 tensor(0.7676)
conv.0 tensor(0.8354)
tensor(1357171.) 2188896.0
0.62551498
0.62572724
0.62636733
0.62706983
0.62800354
0.62983704
0.63221282
0.63369972
0.63444996
0.63473839
0.63543975
0.63611084
0.63728863
0.63898432
0.64092690
0.64331466
0.64677364
0.65328521
INFO - Training [12][   20/  196]   Loss 1.607902   Top1 44.394531   Top5 89.003906   BatchTime 0.375197   LR 0.002433
0.65600663
0.65767145
0.65813941
0.65897691
0.65974742
0.66079789
0.66292828
0.66724074
0.67183942
0.67455798
0.67636514
0.67841583
0.67917210
0.67923009
0.67860657
0.67814302
0.67778718
0.67750180
0.67731720
INFO - Training [12][   40/  196]   Loss 1.603044   Top1 44.775391   Top5 88.994141   BatchTime 0.349347   LR 0.002426
0.67687219
0.67622560
0.67553598
0.67488337
0.67446184
0.67464435
0.67484963
0.67502296
0.67519617
0.67535728
0.67529064
0.67513782
0.67450041
0.67419481
0.67420501
0.67403668
0.67379165
0.67369574
0.67365986
0.67323035
0.67254835
0.67212880
INFO - Training [12][   60/  196]   Loss 1.600863   Top1 44.622396   Top5 89.095052   BatchTime 0.359478   LR 0.002419
0.67194313
0.67144674
0.67066628
0.67008448
0.66948360
0.66938740
0.66923982
0.66886425
0.66898572
0.66871655
0.66793633
0.66740358
0.66671139
0.66575646
0.66466725
0.66401196
0.66344541
0.66314161
0.66234159
0.66166013
0.66071218
INFO - Training [12][   80/  196]   Loss 1.589618   Top1 45.078125   Top5 89.423828   BatchTime 0.360382   LR 0.002412
0.65956742
0.65825844
0.65727049
0.65663034
0.65575546
0.65529799
0.65477020
0.65510213
0.65533572
0.65540075
0.65576321
0.65629965
0.65692896
0.65776789
0.65848482
0.65865922
0.65847743
0.65849501
0.65909773
0.65948904
0.65986538
INFO - Training [12][  100/  196]   Loss 1.591565   Top1 45.015625   Top5 89.429688   BatchTime 0.364810   LR 0.002404
0.66001230
0.66025484
0.66057116
0.66124684
0.66141295
0.66151512
0.66159755
0.66103047
0.66037136
0.66025174
0.66058880
0.66094017
0.66154057
0.66150266
0.66200310
0.66265172
0.66321844
INFO - Training [12][  120/  196]   Loss 1.588072   Top1 45.152995   Top5 89.628906   BatchTime 0.366455   LR 0.002396
0.66363662
0.66374671
0.66356975
0.66358298
0.66355419
0.66334873
0.66320366
0.66300982
0.66272968
0.66263783
0.66272366
0.66319692
0.66379321
0.66445047
0.66492897
0.66539395
0.66572118
0.66617167
0.66639394
0.66654813
0.66664886
INFO - Training [12][  140/  196]   Loss 1.589368   Top1 45.159040   Top5 89.698661   BatchTime 0.366038   LR 0.002388
0.66682714
0.66676849
0.66651893
0.66616988
0.66606015
0.66598433
0.66581666
0.66568768
0.66565979
0.66560894
0.66551149
0.66553497
0.66565859
0.66560745
0.66554922
0.66548747
0.66569823
0.66583490
0.66566890
0.66555339
0.66527331
0.66492027
INFO - Training [12][  160/  196]   Loss 1.589107   Top1 45.202637   Top5 89.702148   BatchTime 0.366873   LR 0.002380
0.66444904
0.66402656
0.66377175
0.66345572
0.66280389
0.66243529
0.66213238
0.66161656
0.66104567
0.66035444
0.65978432
0.65851510
0.65692139
0.65556264
0.65402657
0.65267205
0.65175515
INFO - Training [12][  180/  196]   Loss 1.583747   Top1 45.371094   Top5 89.811198   BatchTime 0.365684   LR 0.002371
0.65043288
0.64914757
0.64822835
0.64697528
0.64642918
0.64573783
0.64564979
0.64561623
0.64660263
0.64747906
0.64856172
0.64952224
0.65096390
0.65249044
0.65417629
0.65571970
INFO - ==> Top1: 45.448    Top5: 89.892    Loss: 1.581
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.65721494
0.65888178
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 1.595087   Top1 46.875000   Top5 90.742188   BatchTime 0.111863
INFO - Validation [12][   40/   40]   Loss 1.588041   Top1 46.910000   Top5 90.550000   BatchTime 0.083078
INFO - ==> Top1: 46.910    Top5: 90.550    Loss: 1.588
INFO - ==> Sparsity : 0.468
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 51.490   Top5: 93.110]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.4102)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.0826)
features.2.conv.6 tensor(0.0851)
features.3.conv.0 tensor(0.0538)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0601)
features.4.conv.0 tensor(0.4971)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.0934)
features.5.conv.0 tensor(0.0488)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0755)
features.6.conv.0 tensor(0.0376)
features.6.conv.3 tensor(0.0718)
features.6.conv.6 tensor(0.0772)
features.7.conv.0 tensor(0.0720)
features.7.conv.3 tensor(0.1351)
features.7.conv.6 tensor(0.7237)
features.8.conv.0 tensor(0.8415)
features.8.conv.3 tensor(0.1418)
features.8.conv.6 tensor(0.9498)
features.9.conv.0 tensor(0.0518)
features.9.conv.3 tensor(0.1586)
features.9.conv.6 tensor(0.9189)
features.10.conv.0 tensor(0.8896)
features.10.conv.3 tensor(0.2922)
features.10.conv.6 tensor(0.8043)
features.11.conv.0 tensor(0.1272)
features.11.conv.3 tensor(0.1836)
features.11.conv.6 tensor(0.2502)
features.12.conv.0 tensor(0.0924)
features.12.conv.3 tensor(0.1620)
features.12.conv.6 tensor(0.3263)
features.13.conv.0 tensor(0.1283)
features.13.conv.3 tensor(0.1684)
features.13.conv.6 tensor(0.3082)
features.14.conv.0 tensor(0.7973)
features.14.conv.3 tensor(0.1102)
features.14.conv.6 tensor(0.9248)
features.15.conv.0 tensor(0.7165)
features.15.conv.3 tensor(0.0766)
features.15.conv.6 tensor(0.9705)
features.16.conv.0 tensor(0.2920)
features.16.conv.3 tensor(0.0866)
features.16.conv.6 tensor(0.3622)
conv.0 tensor(0.2724)
tensor(1025182.) 2188896.0
0.66045064
0.66214633
0.66436267
0.66659802
0.66857487
0.66879040
0.66899109
0.66920704
0.66943192
0.66964638
0.66972154
0.66971749
0.66983074
0.66968477
0.66960442
0.66971028
0.66984421
0.66981971
0.66975659
0.66989440
INFO - Training [13][   20/  196]   Loss 1.559305   Top1 46.640625   Top5 90.078125   BatchTime 0.392437   LR 0.002355
0.67000997
0.66999280
0.67014122
0.67051786
0.67075813
0.67072129
0.67062265
0.67066711
0.67060447
0.67058474
0.67049396
0.67023045
0.67011863
0.67019856
0.67022365
0.67010742
0.66998076
0.66982251
0.66995507
0.67010206
0.67020410
0.67024297
INFO - Training [13][   40/  196]   Loss 1.560132   Top1 46.132812   Top5 90.097656   BatchTime 0.371009   LR 0.002345
0.67026013
0.67026711
0.67004609
0.66972286
0.66915560
0.66849935
0.66813493
0.66774738
0.66736197
0.66690254
0.66639829
0.66597128
0.66565853
0.66520971
0.66478437
0.66438317
0.66378427
INFO - Training [13][   60/  196]   Loss 1.560787   Top1 46.093750   Top5 90.234375   BatchTime 0.368732   LR 0.002336
0.66301656
0.66242141
0.66177660
0.66147858
0.66123551
0.66087115
0.66046804
0.66019887
0.65995204
0.65954179
0.65889090
0.65797842
0.65706539
0.65637660
0.65564930
0.65462220
0.65304941
0.65112835
0.64933079
0.64755946
0.64570946
INFO - Training [13][   80/  196]   Loss 1.560393   Top1 46.162109   Top5 90.219727   BatchTime 0.370837   LR 0.002325
0.64382136
0.64247805
0.64148664
0.64082605
0.64040267
0.63990653
0.64000374
0.64002353
0.64022791
0.64066529
0.64096987
0.64100134
0.64101255
0.64125687
0.64127427
0.64143294
0.64143473
0.64140755
0.64141047
0.64141059
0.64151704
0.64160168
INFO - Training [13][  100/  196]   Loss 1.556317   Top1 46.546875   Top5 90.285156   BatchTime 0.369971   LR 0.002315
0.64174306
0.64149565
0.64122885
0.64089978
0.64058113
0.64012617
0.64000160
0.64012492
0.64025313
0.64026004
0.64003956
0.64006108
0.63995904
0.63983339
0.63953632
0.63930756
0.63883060
INFO - Training [13][  120/  196]   Loss 1.555442   Top1 46.562500   Top5 90.234375   BatchTime 0.368751   LR 0.002304
0.63853329
0.63823485
0.63809979
0.63814390
0.63853616
0.63872820
0.63895428
0.63902271
0.63887936
0.63889962
0.63906652
0.63902903
0.63891107
0.63873482
0.63879734
0.63873261
0.63839722
0.63783127
0.63729107
0.63675666
0.63628477
INFO - Training [13][  140/  196]   Loss 1.554078   Top1 46.537388   Top5 90.340402   BatchTime 0.370308   LR 0.002293
0.63609314
0.63574636
0.63526398
0.63493884
0.63463098
0.63417250
0.63363773
0.63306898
0.63227528
0.63196588
0.63185567
0.63223356
0.63312846
0.63363302
0.63453060
0.63570201
0.63651669
0.63743198
0.63773483
0.63767684
0.63670975
0.63627332
INFO - Training [13][  160/  196]   Loss 1.556762   Top1 46.516113   Top5 90.236816   BatchTime 0.369456   LR 0.002282
0.63534474
0.63490444
0.63495666
0.63564152
0.63576299
0.63631129
0.63655037
0.63698792
0.63716882
0.63707870
0.63682568
0.63737196
0.63752466
0.63783377
0.63818413
0.63865906
0.63946241
INFO - Training [13][  180/  196]   Loss 1.553703   Top1 46.545139   Top5 90.234375   BatchTime 0.367271   LR 0.002271
0.63969904
0.64015114
0.64039338
0.64018124
0.64015502
0.64008600
0.64042902
0.64092863
0.64148861
0.64234883
0.64314407
0.64309001
0.64354187
0.64363658
0.64325827
0.64351648
0.64415729
INFO - ==> Top1: 46.646    Top5: 90.294    Loss: 1.551
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [13][   20/   40]   Loss 1.279761   Top1 54.453125   Top5 93.906250   BatchTime 0.133107
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0524)
features.2.conv.3 tensor(0.0833)
features.2.conv.6 tensor(0.0868)
features.3.conv.0 tensor(0.0524)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0601)
features.4.conv.0 tensor(0.4504)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.0942)
features.5.conv.0 tensor(0.0487)
features.5.conv.3 tensor(0.0735)
features.5.conv.6 tensor(0.0762)
features.6.conv.0 tensor(0.0381)
features.6.conv.3 tensor(0.0729)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0732)
features.7.conv.3 tensor(0.1351)
features.7.conv.6 tensor(0.7856)
features.8.conv.0 tensor(0.8492)
features.8.conv.3 tensor(0.1409)
features.8.conv.6 tensor(0.9480)
features.9.conv.0 tensor(0.0530)
features.9.conv.3 tensor(0.1603)
features.9.conv.6 tensor(0.9056)
features.10.conv.0 tensor(0.8896)
features.10.conv.3 tensor(0.2853)
features.10.conv.6 tensor(0.8117)
features.11.conv.0 tensor(0.1271)
features.11.conv.3 tensor(0.1879)
features.11.conv.6 tensor(0.2485)
features.12.conv.0 tensor(0.0939)
features.12.conv.3 tensor(0.1640)
features.12.conv.6 tensor(0.3242)
features.13.conv.0 tensor(0.1264)
features.13.conv.3 tensor(0.1711)
features.13.conv.6 tensor(0.3809)
features.14.conv.0 tensor(0.8039)
features.14.conv.3 tensor(0.1083)
features.14.conv.6 tensor(0.9309)
features.15.conv.0 tensor(0.7204)
features.15.conv.3 tensor(0.0775)
features.15.conv.6 tensor(0.9721)
features.16.conv.0 tensor(0.3218)
features.16.conv.3 tensor(0.0874)
features.16.conv.6 tensor(0.6861)
conv.0 tensor(0.2544)
tensor(1132527.) 2188896.0
INFO - Validation [13][   40/   40]   Loss 1.291737   Top1 53.680000   Top5 94.000000   BatchTime 0.095369
INFO - ==> Top1: 53.680    Top5: 94.000    Loss: 1.292
INFO - ==> Sparsity : 0.517
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 53.680   Top5: 94.000]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
0.64411479
0.64396310
0.64367765
0.64301544
0.64235073
0.64141780
0.64127022
0.64099735
0.64156806
0.64140755
0.64073956
0.64087909
0.64074445
0.64089352
0.64037359
0.64032894
0.64019215
INFO - Training [14][   20/  196]   Loss 1.560084   Top1 46.523438   Top5 89.335938   BatchTime 0.445279   LR 0.002250
0.64054513
0.64099956
0.64181858
0.64347368
0.64538395
0.64921194
0.65285367
0.65730995
0.66256505
0.66614121
0.66621327
0.66636616
0.66646409
0.66639334
0.66645443
0.66642594
0.66664171
0.66695511
0.66736841
0.66774404
INFO - Training [14][   40/  196]   Loss 1.551683   Top1 46.435547   Top5 89.843750   BatchTime 0.375580   LR 0.002238
0.66788900
0.66813993
0.66839302
0.66858917
0.66868705
0.66880345
0.66865182
0.66866028
0.66869867
0.66876668
0.66892189
0.66912502
0.66924608
0.66919225
0.66905606
0.66904825
0.66913158
0.66907728
0.66914147
0.66915500
0.66909939
0.66911334
0.66910321
INFO - Training [14][   60/  196]   Loss 1.540013   Top1 46.953125   Top5 90.039062   BatchTime 0.368545   LR 0.002225
0.66896230
0.66880482
0.66858709
0.66840726
0.66821659
0.66785508
0.66735178
0.66686749
0.66636032
0.66594583
0.66550726
0.66509968
0.66476262
0.66463113
0.66454494
0.66458130
0.66452962
INFO - Training [14][   80/  196]   Loss 1.534789   Top1 46.953125   Top5 90.278320   BatchTime 0.364855   LR 0.002213
0.66453689
0.66444206
0.66440088
0.66425955
0.66395926
0.66373086
0.66353160
0.66339916
0.66333836
0.66350669
0.66368634
0.66393918
0.66403836
0.66421133
0.66413689
0.66381085
0.66355354
0.66326499
0.66309214
0.66300339
0.66266340
0.66242719
INFO - Training [14][  100/  196]   Loss 1.524494   Top1 47.558594   Top5 90.339844   BatchTime 0.363605   LR 0.002200
0.66237313
0.66239530
0.66243464
0.66256320
0.66286498
0.66328430
0.66375315
0.66410309
0.66445756
0.66463912
0.66493887
0.66524094
0.66554898
0.66586506
0.66611719
0.66627103
0.66635323
0.66645008
0.66639620
0.66639698
0.66628700
0.66628921
INFO - Training [14][  120/  196]   Loss 1.519415   Top1 47.815755   Top5 90.504557   BatchTime 0.365269   LR 0.002186
0.66617382
0.66600406
0.66561854
0.66523892
0.66488183
0.66473603
0.66485405
0.66497576
0.66506433
0.66492254
0.66486156
0.66483182
0.66486853
0.66477430
0.66466630
0.66473567
INFO - Training [14][  140/  196]   Loss 1.515060   Top1 47.977121   Top5 90.616629   BatchTime 0.365193   LR 0.002173
0.66478175
0.66469890
0.66457546
0.66466928
0.66481370
0.66503704
0.66512734
0.66511494
0.66517162
0.66517407
0.66525042
0.66534477
0.66548628
0.66563851
0.66589701
0.66610581
0.66640127
0.66655767
0.66665888
0.66674185
0.66680849
0.66691816
INFO - Training [14][  160/  196]   Loss 1.518917   Top1 47.839355   Top5 90.544434   BatchTime 0.365587   LR 0.002159
0.66703182
0.66704386
0.66710520
0.66713345
0.66710734
0.66700953
0.66688067
0.66650701
0.66621977
0.66591430
0.66567355
0.66548878
0.66528791
0.66508573
0.66473740
0.66446173
0.66409087
0.66363907
0.66321379
0.66280514
INFO - Training [14][  180/  196]   Loss 1.518692   Top1 47.851562   Top5 90.527344   BatchTime 0.370018   LR 0.002145
0.66253537
0.66226184
0.66193479
0.66140920
0.66080242
0.66027689
0.65976471
0.65932477
0.65880996
0.65832198
0.65787059
0.65721542
0.65678638
0.65638328
0.65585214
INFO - ==> Top1: 47.888    Top5: 90.588    Loss: 1.518
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.65525466
0.65457672
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 1.266491   Top1 55.507812   Top5 94.414062   BatchTime 0.116169
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0495)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0527)
features.2.conv.3 tensor(0.0833)
features.2.conv.6 tensor(0.0880)
features.3.conv.0 tensor(0.0518)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.0595)
features.4.conv.0 tensor(0.4445)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.0954)
features.5.conv.0 tensor(0.0487)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0765)
features.6.conv.0 tensor(0.0400)
features.6.conv.3 tensor(0.0729)
features.6.conv.6 tensor(0.0771)
features.7.conv.0 tensor(0.0734)
features.7.conv.3 tensor(0.1351)
features.7.conv.6 tensor(0.7808)
features.8.conv.0 tensor(0.8653)
features.8.conv.3 tensor(0.1429)
features.8.conv.6 tensor(0.9522)
features.9.conv.0 tensor(0.0557)
features.9.conv.3 tensor(0.1583)
features.9.conv.6 tensor(0.9099)
features.10.conv.0 tensor(0.8897)
features.10.conv.3 tensor(0.2911)
features.10.conv.6 tensor(0.8057)
features.11.conv.0 tensor(0.1305)
features.11.conv.3 tensor(0.1894)
features.11.conv.6 tensor(0.2506)
features.12.conv.0 tensor(0.0970)
features.12.conv.3 tensor(0.1636)
features.12.conv.6 tensor(0.3259)
features.13.conv.0 tensor(0.1333)
INFO - Validation [14][   40/   40]   Loss 1.268568   Top1 55.040000   Top5 94.570000   BatchTime 0.083084
INFO - ==> Top1: 55.040    Top5: 94.570    Loss: 1.269
INFO - ==> Sparsity : 0.482
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 55.040   Top5: 94.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.13.conv.3 tensor(0.1698)
features.13.conv.6 tensor(0.3966)
features.14.conv.0 tensor(0.8061)
features.14.conv.3 tensor(0.1071)
features.14.conv.6 tensor(0.9169)
features.15.conv.0 tensor(0.7229)
features.15.conv.3 tensor(0.0810)
features.15.conv.6 tensor(0.9695)
features.16.conv.0 tensor(0.3452)
features.16.conv.3 tensor(0.0894)
features.16.conv.6 tensor(0.3994)
conv.0 tensor(0.2705)
tensor(1055609.) 2188896.0
0.65427434
0.65378118
0.65301883
0.65228796
0.65168136
0.65105242
0.65006214
0.64922041
0.64867616
0.64823145
0.64745945
0.64686632
0.64573085
0.64448649
0.64298505
0.64182222
0.64117736
0.64005196
0.63885736
INFO - Training [15][   20/  196]   Loss 1.504817   Top1 48.046875   Top5 89.785156   BatchTime 0.407226   LR 0.002120
0.63811666
0.63729703
0.63679081
0.63636255
0.63627702
0.63573730
0.63539553
0.63454205
0.63356853
0.63375902
0.63412440
0.63412893
0.63418496
0.63416058
0.63378793
0.63341171
0.63307983
0.63295734
0.63337362
0.63437802
INFO - Training [15][   40/  196]   Loss 1.506540   Top1 48.027344   Top5 89.970703   BatchTime 0.353137   LR 0.002106
0.63518614
0.63599217
0.63615370
0.63602597
0.63527602
0.63473976
0.63440961
0.63431525
0.63448888
0.63453162
0.63498807
0.63482511
0.63400602
0.63337564
0.63240433
0.63143986
0.63050598
0.62955815
0.62860703
0.62748045
0.62656301
0.62577236
0.62491107
INFO - Training [15][   60/  196]   Loss 1.498219   Top1 48.170573   Top5 90.240885   BatchTime 0.323729   LR 0.002091
0.62454712
0.62475634
0.62483984
0.62477672
0.62399864
0.62333864
0.62226158
0.62117523
0.61983836
0.61833483
0.61716044
0.61659873
0.61661661
0.61634552
0.61647719
0.61685807
0.61646599
0.61625344
INFO - Training [15][   80/  196]   Loss 1.499509   Top1 47.983398   Top5 90.458984   BatchTime 0.321989   LR 0.002076
0.61635900
0.61604601
0.61580366
0.61510617
0.61431336
0.61413050
0.61422348
0.61448532
0.61462158
0.61536664
0.61646277
0.61796361
0.61944121
0.62072074
0.62190235
0.62355459
0.62496310
INFO - Training [15][  100/  196]   Loss 1.494323   Top1 48.273438   Top5 90.523438   BatchTime 0.329337   LR 0.002061
0.62614363
0.62697929
0.62769139
0.62830621
0.62868541
0.62905943
0.62939954
0.62979770
0.63025486
0.63055432
0.63071811
0.63103056
0.63132465
0.63153356
0.63153660
0.63151121
0.63081968
0.62882715
0.62594700
0.61980110
0.61984599
0.61657155
INFO - Training [15][  120/  196]   Loss 1.493733   Top1 48.313802   Top5 90.690104   BatchTime 0.335573   LR 0.002045
0.62200713
0.62666577
0.62925738
0.63072205
0.63107991
0.63082296
0.63078219
0.63080466
0.63077217
0.63105822
0.63111669
0.63129163
0.63182765
0.63264745
0.63327610
0.63358819
0.63407618
0.63487953
0.63543725
0.63560736
0.63500351
INFO - Training [15][  140/  196]   Loss 1.492425   Top1 48.482143   Top5 90.672433   BatchTime 0.342915   LR 0.002030
0.63426030
0.63349533
0.63296366
0.63257599
0.63260335
0.63250387
0.63198876
0.63138562
0.63091558
0.63013935
0.62975651
0.62878215
0.62817001
0.62762505
0.62662584
0.62567014
0.62471467
INFO - Training [15][  160/  196]   Loss 1.492899   Top1 48.540039   Top5 90.644531   BatchTime 0.342874   LR 0.002014
0.62398338
0.62384319
0.62326580
0.62258691
0.62206864
0.62183058
0.62178546
0.62145591
0.62162876
0.62159282
0.62168467
0.62195861
0.62212247
0.62262237
0.62277132
0.62258774
0.62216759
0.62145859
0.62084371
0.62016666
0.62015152
0.62023884
INFO - Training [15][  180/  196]   Loss 1.489300   Top1 48.706597   Top5 90.672743   BatchTime 0.345175   LR 0.001998
0.62008798
0.61973661
0.61969376
0.61988902
0.62036741
0.62040889
0.62036490
0.62004101
0.61959469
0.61938781
0.61914390
0.61906570
0.61901259
0.61908275
0.61920679
0.61974663
0.62012064
INFO - ==> Top1: 48.754    Top5: 90.710    Loss: 1.488
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [15][   20/   40]   Loss 1.384201   Top1 51.425781   Top5 93.632812   BatchTime 0.114045
INFO - Validation [15][   40/   40]   Loss 1.403276   Top1 51.120000   Top5 93.530000   BatchTime 0.082316
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.4160)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0524)
features.2.conv.3 tensor(0.0818)
features.2.conv.6 tensor(0.0894)
features.3.conv.0 tensor(0.0532)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0595)
features.4.conv.0 tensor(0.4771)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.0972)
features.5.conv.0 tensor(0.0485)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0765)
features.6.conv.0 tensor(0.0394)
features.6.conv.3 tensor(0.0712)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0758)
features.7.conv.3 tensor(0.1345)
features.7.conv.6 tensor(0.7856)
features.8.conv.0 tensor(0.8583)
features.8.conv.3 tensor(0.1392)
features.8.conv.6 tensor(0.9578)
features.9.conv.0 tensor(0.0580)
features.9.conv.3 tensor(0.1615)
features.9.conv.6 tensor(0.8955)
features.10.conv.0 tensor(0.8894)
features.10.conv.3 tensor(0.2940)
features.10.conv.6 tensor(0.8147)
features.11.conv.0 tensor(0.1306)
features.11.conv.3 tensor(0.1900)
features.11.conv.6 tensor(0.2499)
features.12.conv.0 tensor(0.0970)
features.12.conv.3 tensor(0.1603)
features.12.conv.6 tensor(0.3246)
features.13.conv.0 tensor(0.0785)
features.13.conv.3 tensor(0.1773)
features.13.conv.6 tensor(0.4132)
features.14.conv.0 tensor(0.8085)
features.14.conv.3 tensor(0.1042)
features.14.conv.6 tensor(0.9189)
features.15.conv.0 tensor(0.7288)
features.15.conv.3 tensor(0.0823)
features.15.conv.6 tensor(0.9745)
features.16.conv.0 tensor(0.3772)
features.16.conv.3 tensor(0.0904)
features.16.conv.6 tensor(0.5322)
conv.0 tensor(0.7836)
tensor(1312679.) 2188896.0
INFO - ==> Top1: 51.120    Top5: 93.530    Loss: 1.403
INFO - ==> Sparsity : 0.600
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 55.040   Top5: 94.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
0.62085205
0.62138695
0.62165517
0.62164485
0.62165409
0.62235218
0.62258369
0.62277156
0.62299180
0.62303978
0.62253797
0.62206304
0.62187713
0.62182575
0.62190288
0.62191862
0.62217283
0.62255645
0.62292820
0.62316698
INFO - Training [16][   20/  196]   Loss 1.481882   Top1 49.511719   Top5 90.585938   BatchTime 0.424337   LR 0.001969
0.62338144
0.62363446
0.62389529
0.62406391
0.62420905
0.62422997
0.62416005
0.62397337
0.62378240
0.62338805
0.62287462
0.62260413
0.62239987
0.62232494
0.62227732
0.62256867
0.62260783
0.62261695
INFO - Training [16][   40/  196]   Loss 1.478254   Top1 49.443359   Top5 90.869141   BatchTime 0.381091   LR 0.001953
0.62270164
0.62246257
0.62272483
0.62281293
0.62258190
0.62277424
0.62273270
0.62284088
0.62312734
0.62355316
0.62341201
0.62349838
0.62345535
0.62358761
0.62352699
0.62342924
0.62340748
0.62362278
0.62364757
0.62367612
0.62402588
0.62420082
0.62432188
0.62455982
0.62503582
INFO - Training [16][   60/  196]   Loss 1.470105   Top1 49.576823   Top5 91.106771   BatchTime 0.362132   LR 0.001936
0.62558222
0.62611336
0.62655485
0.62675279
0.62691969
0.62691867
0.62673163
0.62668979
0.62650925
0.62606198
0.62549073
0.62470824
0.62382525
0.62271005
0.62161052
0.62083083
0.62072223
INFO - Training [16][   80/  196]   Loss 1.466391   Top1 49.697266   Top5 91.293945   BatchTime 0.358014   LR 0.001919
0.62073755
0.62057859
0.62071097
0.62064415
0.62067419
0.62073272
0.62042820
0.62036216
0.62021530
0.62009561
0.61980587
0.61924028
0.61878711
0.61839843
0.61809033
0.61764961
0.61749518
0.61764771
0.61724293
0.61701763
INFO - Training [16][  100/  196]   Loss 1.465374   Top1 49.960938   Top5 91.332031   BatchTime 0.364411   LR 0.001902
0.61706913
0.61701024
0.61672682
0.61679029
0.61669159
0.61642236
0.61615735
0.61604667
0.61635911
0.61623114
0.61618769
0.61629492
0.61641431
0.61636710
0.61615789
0.61625546
0.61626369
0.61587238
0.61566001
0.61546624
INFO - Training [16][  120/  196]   Loss 1.466392   Top1 49.918620   Top5 91.308594   BatchTime 0.366139   LR 0.001885
0.61539233
0.61522090
0.61488956
0.61459655
0.61444962
0.61442810
0.61415058
0.61398572
0.61368996
0.61342722
0.61303359
0.61259848
0.61202645
0.61150146
0.61098152
0.61084896
0.61096281
0.61112958
0.61130536
INFO - Training [16][  140/  196]   Loss 1.465516   Top1 49.910714   Top5 91.308594   BatchTime 0.364111   LR 0.001867
0.61187148
0.61236942
0.61296588
0.61363739
0.61416626
0.61502391
0.61551088
0.61592042
0.61628544
0.61666662
0.61710382
0.61752278
0.61823511
0.61865389
0.61897242
0.61949837
0.61947978
0.61933863
0.61922538
0.61917275
0.61908865
INFO - Training [16][  160/  196]   Loss 1.465587   Top1 49.924316   Top5 91.342773   BatchTime 0.365521   LR 0.001850
0.61886835
0.61886013
0.61884433
0.61875772
0.61887699
0.61887866
0.61902630
0.61939687
0.61979717
0.62035137
0.62056780
0.62066150
0.62076557
0.62095147
0.62128115
0.62161714
0.62206608
INFO - Training [16][  180/  196]   Loss 1.461454   Top1 50.078125   Top5 91.408420   BatchTime 0.364096   LR 0.001832
0.62252218
0.62308133
0.62365592
0.62404388
0.62433982
0.62469614
0.62477684
0.62495279
0.62511104
0.62546551
0.62556159
0.62551725
0.62552404
0.62552691
0.62545592
0.62577629
0.62610126
INFO - ==> Top1: 50.134    Top5: 91.426    Loss: 1.460
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.62630826
0.62639004
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 1.419301   Top1 51.601562   Top5 91.777344   BatchTime 0.112092
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3984)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0655)
features.2.conv.0 tensor(0.0530)
features.2.conv.3 tensor(0.0826)
features.2.conv.6 tensor(0.0903)
features.3.conv.0 tensor(0.0538)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.0592)
features.4.conv.0 tensor(0.4767)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.0988)
features.5.conv.0 tensor(0.0492)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0767)
features.6.conv.0 tensor(0.0389)
features.6.conv.3 tensor(0.0729)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0752)
features.7.conv.3 tensor(0.1331)
features.7.conv.6 tensor(0.7782)
features.8.conv.0 tensor(0.8584)
features.8.conv.3 tensor(0.1369)
features.8.conv.6 tensor(0.9644)
features.9.conv.0 tensor(0.0612)
features.9.conv.3 tensor(0.1617)
features.9.conv.6 tensor(0.9021)
features.10.conv.0 tensor(0.8903)
features.10.conv.3 tensor(0.2957)
features.10.conv.6 tensor(0.8131)
features.11.conv.0 tensor(0.1325)
features.11.conv.3 tensor(0.1923)
features.11.conv.6 tensor(0.2507)
features.12.conv.0 tensor(0.0994)
features.12.conv.3 tensor(0.1624)
features.12.conv.6 tensor(0.3255)
features.13.conv.0 tensor(0.0851)
features.13.conv.3 tensor(0.1794)
features.13.conv.6 tensor(0.4081)
features.14.conv.0 tensor(0.8126)
features.14.conv.3 tensor(0.1071)
features.14.conv.6 tensor(0.9343)
features.15.conv.0 tensor(0.7335)
features.15.conv.3 tensor(0.0834)
features.15.conv.6 tensor(0.9719)
features.16.conv.0 tensor(0.3995)
features.16.conv.3 tensor(0.0876)
features.16.conv.6 tensor(0.5404)
conv.0 tensor(0.5815)
tensor(1239615.) 2188896.0
INFO - Validation [16][   40/   40]   Loss 1.411652   Top1 52.040000   Top5 92.170000   BatchTime 0.082760
INFO - ==> Top1: 52.040    Top5: 92.170    Loss: 1.412
INFO - ==> Sparsity : 0.566
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 55.040   Top5: 94.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
0.62663329
0.62674105
0.62679726
0.62672120
0.62667263
0.62649316
0.62627047
0.62615287
0.62596864
0.62569219
0.62544036
0.62541592
0.62544125
0.62545675
0.62536615
0.62538075
0.62528026
0.62511981
INFO - Training [17][   20/  196]   Loss 1.481825   Top1 48.007812   Top5 91.523438   BatchTime 0.433843   LR 0.001800
0.62507510
0.62501681
0.62495804
0.62484896
0.62462795
0.62439191
0.62425333
0.62397921
0.62361538
0.62294626
0.62217307
0.62183511
0.62132233
0.62071532
0.62016028
0.61941630
0.61872947
0.61810917
0.61741000
0.61670154
0.61635834
INFO - Training [17][   40/  196]   Loss 1.464768   Top1 49.433594   Top5 91.591797   BatchTime 0.408594   LR 0.001782
0.61614811
0.61580354
0.61556154
0.61554432
0.61536789
0.61526859
0.61502481
0.61473811
0.61414629
0.61359924
0.61321598
0.61297107
0.61289805
0.61287040
0.61284393
0.61280537
0.61281246
INFO - Training [17][   60/  196]   Loss 1.457874   Top1 49.687500   Top5 91.660156   BatchTime 0.381869   LR 0.001764
0.61292678
0.61296779
0.61309451
0.61346066
0.61395150
0.61429441
0.61447954
0.61487657
0.61508960
0.61524063
0.61523831
0.61523211
0.61533827
0.61555785
0.61562753
0.61542791
0.61506104
0.61491418
0.61463785
0.61454731
0.61434513
0.61416870
0.61396587
0.61395663
0.61382878
0.61383611
0.61402106
INFO - Training [17][   80/  196]   Loss 1.452664   Top1 49.985352   Top5 91.586914   BatchTime 0.363073   LR 0.001746
0.61417049
0.61449587
0.61479455
0.61498260
0.61533523
0.61553037
0.61546600
0.61503285
0.61471516
0.61430103
0.61375439
0.61321014
0.61298001
0.61278385
INFO - Training [17][  100/  196]   Loss 1.446282   Top1 50.226562   Top5 91.609375   BatchTime 0.348754   LR 0.001727
0.61292863
0.61343139
0.61398751
0.61462724
0.61527848
0.61599022
0.61663949
0.61729878
0.61785614
0.61827815
0.61849588
0.61881381
0.61906189
0.61922735
0.61928004
0.61929280
0.61932945
0.61927044
0.61932117
0.61929232
0.61926979
0.61922932
0.61921644
INFO - Training [17][  120/  196]   Loss 1.449133   Top1 50.179036   Top5 91.507161   BatchTime 0.347619   LR 0.001708
0.61917138
0.61933881
0.61941224
0.61940509
0.61935771
0.61929882
0.61922741
0.61917490
0.61894453
0.61859792
0.61821502
0.61792952
0.61753303
0.61712432
0.61691409
0.61676180
0.61653638
0.61655271
0.61643028
0.61631292
0.61605334
0.61573231
INFO - Training [17][  140/  196]   Loss 1.446031   Top1 50.279018   Top5 91.590402   BatchTime 0.350671   LR 0.001690
0.61551124
0.61520398
0.61484474
0.61449218
0.61437452
0.61430532
0.61432970
0.61412877
0.61407548
0.61382151
0.61375171
0.61386919
0.61390090
0.61409825
0.61415023
0.61413574
0.61417657
INFO - Training [17][  160/  196]   Loss 1.448098   Top1 50.239258   Top5 91.586914   BatchTime 0.351725   LR 0.001671
0.61400414
0.61392999
0.61368108
0.61319810
0.61266196
0.61233830
0.61197257
0.61139017
0.61096078
0.61082697
0.61074305
0.61077988
0.61075258
0.61060005
0.61071002
0.61087978
0.61119235
0.61160201
0.61192250
0.61261797
INFO - Training [17][  180/  196]   Loss 1.443540   Top1 50.431858   Top5 91.629774   BatchTime 0.356989   LR 0.001652
0.61330312
0.61391866
0.61420465
0.61441493
0.61466509
0.61483246
0.61496639
0.61501682
0.61507934
0.61492842
0.61474568
0.61447841
0.61416858
0.61394274
0.61365366
0.61353505
INFO - ==> Top1: 50.590    Top5: 91.632    Loss: 1.441
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.61339194
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [17][   20/   40]   Loss 1.329883   Top1 52.695312   Top5 93.671875   BatchTime 0.126264
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.3965)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0903)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0538)
features.2.conv.3 tensor(0.0826)
features.2.conv.6 tensor(0.0920)
features.3.conv.0 tensor(0.0538)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.0595)
features.4.conv.0 tensor(0.4800)
features.4.conv.3 tensor(0.1140)
features.4.conv.6 tensor(0.0990)
features.5.conv.0 tensor(0.0487)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0770)
features.6.conv.0 tensor(0.0389)
features.6.conv.3 tensor(0.0718)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0776)
features.7.conv.3 tensor(0.1334)
features.7.conv.6 tensor(0.8090)
features.8.conv.0 tensor(0.8515)
features.8.conv.3 tensor(0.1386)
features.8.conv.6 tensor(0.9591)
features.9.conv.0 tensor(0.0623)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.9244)
features.10.conv.0 tensor(0.8906)
features.10.conv.3 tensor(0.2998)
features.10.conv.6 tensor(0.8155)
features.11.conv.0 tensor(0.1306)
features.11.conv.3 tensor(0.1919)
features.11.conv.6 tensor(0.2517)
features.12.conv.0 tensor(0.0991)
features.12.conv.3 tensor(0.1622)
features.12.conv.6 tensor(0.3252)
features.13.conv.0 tensor(0.0895)
features.13.conv.3 tensor(0.1775)
features.13.conv.6 tensor(0.4484)
features.14.conv.0 tensor(0.8141)
features.14.conv.3 tensor(0.1087)
features.14.conv.6 tensor(0.9289)
features.15.conv.0 tensor(0.7339)
features.15.conv.3 tensor(0.0825)
features.15.conv.6 tensor(0.9759)
features.16.conv.0 tensor(0.4157)
features.16.conv.3 tensor(0.0863)
features.16.conv.6 tensor(0.6612)
conv.0 tensor(0.7526)
tensor(1354441.) 2188896.0
INFO - Validation [17][   40/   40]   Loss 1.338304   Top1 52.910000   Top5 93.670000   BatchTime 0.087737
INFO - ==> Top1: 52.910    Top5: 93.670    Loss: 1.338
INFO - ==> Sparsity : 0.619
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 55.040   Top5: 94.570]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.61341327
0.61350650
0.61376286
0.61398768
0.61419284
0.61450046
0.61461586
0.61466295
0.61489445
0.61505967
0.61525941
0.61552387
0.61585855
0.61606294
0.61626375
0.61629969
0.61623126
0.61628717
INFO - Training [18][   20/  196]   Loss 1.430863   Top1 51.132812   Top5 91.250000   BatchTime 0.437456   LR 0.001618
0.61625469
0.61632270
0.61618054
0.61595064
0.61574537
0.61536282
0.61487603
0.61446548
0.61437440
0.61416805
0.61393511
0.61395884
0.61362553
0.61361879
0.61364818
0.61368179
0.61376572
0.61364293
0.61358082
0.61340725
0.61332196
0.61311477
INFO - Training [18][   40/  196]   Loss 1.442344   Top1 50.126953   Top5 91.259766   BatchTime 0.396957   LR 0.001599
0.61298084
0.61296767
0.61322862
0.61365366
0.61406070
0.61434150
0.61472660
0.61491477
0.61533010
0.61561549
0.61571360
0.61591184
0.61660558
0.61697054
0.61705256
0.61751276
0.61807114
0.61823207
0.61849016
0.61858070
0.61839038
0.61839157
INFO - Training [18][   60/  196]   Loss 1.439196   Top1 50.253906   Top5 91.341146   BatchTime 0.384781   LR 0.001579
0.61825132
0.61798972
0.61782420
0.61760801
0.61743832
0.61715013
0.61705059
0.61702967
0.61715055
0.61750400
0.61779368
0.61792129
0.61816138
0.61832821
0.61848205
0.61864519
INFO - Training [18][   80/  196]   Loss 1.428547   Top1 50.771484   Top5 91.625977   BatchTime 0.383212   LR 0.001560
0.61897725
0.61931401
0.61961305
0.61976606
0.62006235
0.62028682
0.62044042
0.62059307
0.62081516
0.62086934
0.62088305
0.62086612
0.62108916
0.62133700
0.62145221
0.62146389
0.62138420
0.62121248
0.62088758
0.62067920
0.62055218
0.62033427
0.62005663
0.61968410
0.61932218
INFO - Training [18][  100/  196]   Loss 1.421861   Top1 51.062500   Top5 91.765625   BatchTime 0.370931   LR 0.001540
0.61867970
0.61815149
0.61759961
0.61691576
0.61633748
0.61580151
0.61506695
0.61452419
0.61421943
0.61378807
0.61324930
0.61289334
0.61277711
0.61273521
0.61260968
0.61250591
0.61253256
0.61258429
INFO - Training [18][  120/  196]   Loss 1.415865   Top1 51.318359   Top5 91.888021   BatchTime 0.362792   LR 0.001521
0.61266667
0.61270106
0.61276025
0.61296326
0.61304599
0.61312962
0.61306387
0.61297446
0.61285585
0.61292946
0.61280912
0.61259335
0.61229903
0.61203700
0.61192381
0.61186427
0.61171204
INFO - Training [18][  140/  196]   Loss 1.415140   Top1 51.406250   Top5 91.930804   BatchTime 0.363465   LR 0.001501
0.61151910
0.61129743
0.61126381
0.61121613
0.61115438
0.61086702
0.61079222
0.61071640
0.61042601
0.61014229
0.60974914
0.60902661
0.60845262
0.60822552
0.60816461
0.60806060
0.60802007
0.60813367
0.60817617
0.60821760
0.60808671
INFO - Training [18][  160/  196]   Loss 1.417696   Top1 51.142578   Top5 91.923828   BatchTime 0.365780   LR 0.001482
0.60793108
0.60773498
0.60734230
0.60701114
0.60678798
0.60673279
0.60668778
0.60642308
0.60644323
0.60623413
0.60600996
0.60626131
0.60656613
0.60702503
0.60741478
0.60783088
0.60793388
0.60800868
0.60809857
0.60826403
0.60843331
0.60838181
INFO - Training [18][  180/  196]   Loss 1.416559   Top1 51.195747   Top5 91.937934   BatchTime 0.364894   LR 0.001462
0.60828024
0.60810763
0.60802883
0.60813880
0.60819399
0.60810494
0.60802817
0.60792285
0.60778105
0.60760009
0.60743099
0.60706127
0.60673332
0.60639012
0.60612923
********************pre-trained*****************
INFO - ==> Top1: 51.328    Top5: 91.952    Loss: 1.415
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 1.246145   Top1 55.156250   Top5 94.707031   BatchTime 0.111347
INFO - Validation [18][   40/   40]   Loss 1.260763   Top1 55.000000   Top5 94.540000   BatchTime 0.080634
INFO - ==> Top1: 55.000    Top5: 94.540    Loss: 1.261
INFO - ==> Sparsity : 0.621
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 55.040   Top5: 94.570]
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.4160)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.0818)
features.2.conv.6 tensor(0.0926)
features.3.conv.0 tensor(0.0532)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0603)
features.4.conv.0 tensor(0.5150)
features.4.conv.3 tensor(0.1123)
features.4.conv.6 tensor(0.0991)
features.5.conv.0 tensor(0.2842)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0771)
features.6.conv.0 tensor(0.0386)
features.6.conv.3 tensor(0.0735)
features.6.conv.6 tensor(0.0778)
features.7.conv.0 tensor(0.0801)
features.7.conv.3 tensor(0.1331)
features.7.conv.6 tensor(0.8447)
features.8.conv.0 tensor(0.8478)
features.8.conv.3 tensor(0.1383)
features.8.conv.6 tensor(0.9602)
features.9.conv.0 tensor(0.0661)
features.9.conv.3 tensor(0.1597)
features.9.conv.6 tensor(0.9117)
features.10.conv.0 tensor(0.8880)
features.10.conv.3 tensor(0.2928)
features.10.conv.6 tensor(0.8159)
features.11.conv.0 tensor(0.1310)
features.11.conv.3 tensor(0.1921)
features.11.conv.6 tensor(0.2510)
features.12.conv.0 tensor(0.1003)
features.12.conv.3 tensor(0.1626)
features.12.conv.6 tensor(0.3226)
features.13.conv.0 tensor(0.0975)
features.13.conv.3 tensor(0.1742)
features.13.conv.6 tensor(0.5267)
features.14.conv.0 tensor(0.8152)
features.14.conv.3 tensor(0.1053)
features.14.conv.6 tensor(0.9274)
features.15.conv.0 tensor(0.7362)
features.15.conv.3 tensor(0.0846)
features.15.conv.6 tensor(0.9812)
features.16.conv.0 tensor(0.4829)
features.16.conv.3 tensor(0.0881)
features.16.conv.6 tensor(0.6021)
conv.0 tensor(0.7551)
tensor(1358558.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
0.60532248
0.60436058
0.60340345
0.60275245
0.60239124
0.60486519
0.60750860
0.60987902
0.61197168
0.61314875
0.61403942
0.61463559
0.61496478
0.61517870
0.61541629
0.61547095
0.61557198
0.61558247
0.61581838
0.61592323
0.61597908
INFO - Training [19][   20/  196]   Loss 1.427571   Top1 50.566406   Top5 90.781250   BatchTime 0.438148   LR 0.001427
0.61598432
0.61611515
0.61624295
0.61644679
0.61652398
0.61658251
0.61661255
0.61661035
0.61653620
0.61644554
0.61632812
0.61619604
0.61606342
0.61596173
0.61593437
0.61583954
0.61561173
INFO - Training [19][   40/  196]   Loss 1.411915   Top1 51.718750   Top5 91.533203   BatchTime 0.393126   LR 0.001407
0.61539751
0.61501747
0.61469084
0.61434495
0.61402535
0.61379790
0.61359710
0.61346042
0.61326998
0.61309278
0.61273736
0.61224067
0.61189038
0.61168778
0.61145359
0.61114007
0.61079204
0.61065274
0.61046445
0.61002165
0.60953444
0.60906744
0.60852599
INFO - Training [19][   60/  196]   Loss 1.409873   Top1 51.790365   Top5 91.614583   BatchTime 0.379967   LR 0.001387
0.60791659
0.60722744
0.60663694
0.60628611
0.60624421
0.60638857
0.60657632
0.60664189
0.60687691
0.60710335
0.60705680
0.60703826
0.60720980
0.60714000
0.60731161
0.60747808
0.60772133
0.60795575
0.60813105
0.60855263
0.60901070
INFO - Training [19][   80/  196]   Loss 1.406297   Top1 51.855469   Top5 91.796875   BatchTime 0.378176   LR 0.001367
0.60949010
0.60995573
0.61049694
0.61096805
0.61131698
0.61171520
0.61209887
0.61241680
0.61263806
0.61269367
0.61270624
0.61290562
0.61313778
0.61336488
0.61349881
0.61367720
0.61366653
INFO - Training [19][  100/  196]   Loss 1.400478   Top1 52.097656   Top5 91.937500   BatchTime 0.375180   LR 0.001347
0.61371702
0.61397588
0.61419356
0.61414617
0.61410546
0.61407477
0.61403239
0.61384761
0.61369473
0.61357057
0.61338687
0.61293221
0.61256081
0.61217284
0.61180001
0.61161411
0.61128473
0.61103833
INFO - Training [19][  120/  196]   Loss 1.398131   Top1 52.138672   Top5 92.037760   BatchTime 0.365882   LR 0.001327
0.61093473
0.61073571
0.61046612
0.61023325
0.61008024
0.60981405
0.60949171
0.60912436
0.60886574
0.60878772
0.60876477
0.60866141
0.60869724
0.60863817
0.60854679
0.60844976
0.60857368
0.60866243
0.60869956
0.60888797
0.60909045
0.60939026
0.60964310
0.60974526
INFO - Training [19][  140/  196]   Loss 1.394585   Top1 52.360491   Top5 92.131696   BatchTime 0.362871   LR 0.001307
0.60979623
0.60978508
0.60988349
0.60985094
0.60993576
0.61000049
0.61008680
0.61011159
0.61012030
0.60996467
0.60965687
0.60935682
0.60905325
0.60894734
0.60883081
0.60890132
0.60896528
INFO - Training [19][  160/  196]   Loss 1.396143   Top1 52.233887   Top5 92.143555   BatchTime 0.361040   LR 0.001287
0.60896009
0.60891622
0.60887831
0.60865819
0.60858530
0.60843331
0.60820794
0.60791814
0.60776478
0.60757828
0.60752451
0.60762388
0.60766804
0.60775644
0.60796040
0.60821253
0.60843939
0.60862118
0.60876995
0.60909683
0.60934520
0.60964370
INFO - Training [19][  180/  196]   Loss 1.391635   Top1 52.404514   Top5 92.194010   BatchTime 0.360596   LR 0.001266
0.60995823
0.61014575
0.61020434
0.61028785
0.61034614
0.61037344
0.61035335
0.61036539
0.61036557
0.61045885
0.61058730
0.61052459
0.61051124
0.61052889
0.61044306
0.61034781
********************pre-trained*****************
INFO - ==> Top1: 52.506    Top5: 92.234    Loss: 1.390
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [19][   20/   40]   Loss 1.173363   Top1 59.121094   Top5 95.371094   BatchTime 0.113877
INFO - Validation [19][   40/   40]   Loss 1.176113   Top1 59.110000   Top5 95.250000   BatchTime 0.084007
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0547)
features.2.conv.3 tensor(0.0841)
features.2.conv.6 tensor(0.0926)
features.3.conv.0 tensor(0.0489)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.0592)
features.4.conv.0 tensor(0.4531)
features.4.conv.3 tensor(0.1134)
features.4.conv.6 tensor(0.0998)
features.5.conv.0 tensor(0.0483)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0775)
features.6.conv.0 tensor(0.0389)
features.6.conv.3 tensor(0.0723)
features.6.conv.6 tensor(0.0773)
features.7.conv.0 tensor(0.0781)
features.7.conv.3 tensor(0.1337)
features.7.conv.6 tensor(0.8350)
features.8.conv.0 tensor(0.8864)
features.8.conv.3 tensor(0.1400)
features.8.conv.6 tensor(0.9599)
features.9.conv.0 tensor(0.0640)
features.9.conv.3 tensor(0.1603)
features.9.conv.6 tensor(0.9189)
features.10.conv.0 tensor(0.8909)
features.10.conv.3 tensor(0.2946)
features.10.conv.6 tensor(0.8138)
features.11.conv.0 tensor(0.1317)
features.11.conv.3 tensor(0.1935)
features.11.conv.6 tensor(0.2513)
features.12.conv.0 tensor(0.1014)
features.12.conv.3 tensor(0.1634)
features.12.conv.6 tensor(0.3255)
features.13.conv.0 tensor(0.1064)
features.13.conv.3 tensor(0.1752)
features.13.conv.6 tensor(0.4457)
features.14.conv.0 tensor(0.8181)
features.14.conv.3 tensor(0.1049)
features.14.conv.6 tensor(0.9309)
features.15.conv.0 tensor(0.7399)
features.15.conv.3 tensor(0.0874)
features.15.conv.6 tensor(0.9748)
features.16.conv.0 tensor(0.4832)
features.16.conv.3 tensor(0.0878)
features.16.conv.6 tensor(0.6396)
conv.0 tensor(0.7610)
tensor(1365383.) 2188896.0
INFO - ==> Top1: 59.110    Top5: 95.250    Loss: 1.176
INFO - ==> Sparsity : 0.624
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 59.110   Top5: 95.250]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.61051774
0.61053425
0.61060995
0.61058176
0.61054826
0.61060703
0.61051989
0.61039758
0.61029303
0.61017078
0.61008543
0.61001849
0.61009425
0.60999322
0.60984868
0.60978270
0.60983133
0.60997081
0.61007637
0.61017334
0.61016566
INFO - Training [20][   20/  196]   Loss 1.410554   Top1 51.367188   Top5 91.074219   BatchTime 0.416722   LR 0.001231
0.61000061
0.60990006
0.60983181
0.60954624
0.60935265
0.60923249
0.60907501
0.60893983
0.60872328
0.60870224
0.60864025
0.60864079
0.60867482
0.60871869
0.60878599
0.60887319
0.60877120
INFO - Training [20][   40/  196]   Loss 1.389656   Top1 51.855469   Top5 92.011719   BatchTime 0.382173   LR 0.001211
0.60870856
0.60859847
0.60853207
0.60869032
0.60888177
0.60894257
0.60893530
0.60885978
0.60874265
0.60874963
0.60870606
0.60868478
0.60877991
0.60867220
0.60852611
0.60825676
0.60813558
0.60789406
0.60783660
0.60795552
0.60814607
0.60829669
INFO - Training [20][   60/  196]   Loss 1.379319   Top1 52.545573   Top5 92.122396   BatchTime 0.379446   LR 0.001191
0.60845512
0.60847259
0.60837227
0.60819334
0.60800046
0.60773605
0.60765254
0.60750502
0.60726607
0.60706985
0.60666734
0.60629672
0.60605890
0.60595930
0.60599923
0.60593045
0.60582799
0.60586137
0.60591990
0.60600823
0.60619891
0.60651970
INFO - Training [20][   80/  196]   Loss 1.380603   Top1 52.539062   Top5 92.138672   BatchTime 0.375789   LR 0.001171
0.60692149
0.60711467
0.60738146
0.60785562
0.60831308
0.60863972
0.60889173
0.60898948
0.60912544
0.60921472
0.60933822
0.60945165
0.60954499
0.60953498
0.60949886
0.60934919
0.60920525
INFO - Training [20][  100/  196]   Loss 1.376748   Top1 52.839844   Top5 92.207031   BatchTime 0.372586   LR 0.001151
0.60900706
0.60882074
0.60864121
0.60853601
0.60846668
0.60847855
0.60849965
0.60856402
0.60870904
0.60878563
0.60885596
0.60870439
0.60855746
0.60834533
0.60825717
0.60829943
0.60837126
0.60854888
0.60842037
0.60838908
0.60836244
0.60841525
0.60856539
0.60874683
0.60890377
INFO - Training [20][  120/  196]   Loss 1.374297   Top1 53.011068   Top5 92.255859   BatchTime 0.362528   LR 0.001131
0.60896093
0.60890031
0.60895628
0.60909069
0.60908467
0.60903877
0.60903370
0.60901994
0.60894167
0.60894251
0.60889804
0.60880542
0.60871363
0.60853201
0.60831463
0.60819131
0.60802037
0.60776645
INFO - Training [20][  140/  196]   Loss 1.374802   Top1 53.044085   Top5 92.371652   BatchTime 0.356986   LR 0.001111
0.60748911
0.60727704
0.60698146
0.60674077
0.60630012
0.60601854
0.60557252
0.60527825
0.60503942
0.60470492
0.60449612
0.60425514
0.60409558
0.60413361
0.60400516
0.60406327
0.60428709
INFO - Training [20][  160/  196]   Loss 1.379564   Top1 52.858887   Top5 92.297363   BatchTime 0.356446   LR 0.001091
0.60446423
0.60456032
0.60459250
0.60459822
0.60466236
0.60487378
0.60505790
0.60543650
0.60562724
0.60593122
0.60612488
0.60629582
0.60634142
0.60638881
0.60628712
0.60631955
0.60629767
0.60615194
0.60610217
0.60615087
0.60613710
0.60614383
0.60608679
INFO - Training [20][  180/  196]   Loss 1.379983   Top1 52.840712   Top5 92.263455   BatchTime 0.355700   LR 0.001071
0.60618645
0.60614866
0.60621703
0.60615009
0.60604578
0.60613364
0.60631329
0.60661411
0.60691488
0.60717851
0.60737062
0.60740268
0.60739195
0.60730976
********************pre-trained*****************
INFO - ==> Top1: 52.910    Top5: 92.296    Loss: 1.377
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 1.155930   Top1 58.203125   Top5 95.898438   BatchTime 0.118366
INFO - Validation [20][   40/   40]   Loss 1.152663   Top1 58.740000   Top5 95.780000   BatchTime 0.088870
INFO - ==> Top1: 58.740    Top5: 95.780    Loss: 1.153
INFO - ==> Sparsity : 0.627
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 59.110   Top5: 95.250]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.4121)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.0826)
features.2.conv.6 tensor(0.0935)
features.3.conv.0 tensor(0.0480)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0588)
features.4.conv.0 tensor(0.4832)
features.4.conv.3 tensor(0.1134)
features.4.conv.6 tensor(0.0996)
features.5.conv.0 tensor(0.0482)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0775)
features.6.conv.0 tensor(0.0391)
features.6.conv.3 tensor(0.0718)
features.6.conv.6 tensor(0.0773)
features.7.conv.0 tensor(0.0802)
features.7.conv.3 tensor(0.1328)
features.7.conv.6 tensor(0.8265)
features.8.conv.0 tensor(0.8959)
features.8.conv.3 tensor(0.1409)
features.8.conv.6 tensor(0.9600)
features.9.conv.0 tensor(0.0701)
features.9.conv.3 tensor(0.1600)
features.9.conv.6 tensor(0.9121)
features.10.conv.0 tensor(0.8881)
features.10.conv.3 tensor(0.2946)
features.10.conv.6 tensor(0.8107)
features.11.conv.0 tensor(0.1328)
features.11.conv.3 tensor(0.1941)
features.11.conv.6 tensor(0.2518)
features.12.conv.0 tensor(0.1022)
features.12.conv.3 tensor(0.1632)
features.12.conv.6 tensor(0.3231)
features.13.conv.0 tensor(0.1063)
features.13.conv.3 tensor(0.1740)
features.13.conv.6 tensor(0.4683)
features.14.conv.0 tensor(0.8181)
features.14.conv.3 tensor(0.1043)
features.14.conv.6 tensor(0.9358)
features.15.conv.0 tensor(0.7419)
features.15.conv.3 tensor(0.0870)
features.15.conv.6 tensor(0.9808)
features.16.conv.0 tensor(0.4888)
features.16.conv.3 tensor(0.0847)
features.16.conv.6 tensor(0.6385)
conv.0 tensor(0.7647)
tensor(1371507.) 2188896.0
0.60744530
0.60755044
0.60757434
0.60761648
0.60777551
0.60798025
0.60830927
0.60871071
0.60922980
0.60964507
0.61002058
0.61031479
0.61057168
0.61080408
0.61076522
0.61070430
0.61066586
0.61067975
0.61054873
0.61042166
0.61009228
0.60974884
INFO - Training [21][   20/  196]   Loss 1.366688   Top1 53.164062   Top5 91.933594   BatchTime 0.433513   LR 0.001036
0.60950035
0.60923946
0.60901296
0.60879451
0.60867530
0.60859835
0.60838610
0.60812593
0.60803324
0.60789609
0.60775256
0.60776263
0.60780257
0.60788077
0.60782248
0.60772377
0.60754186
INFO - Training [21][   40/  196]   Loss 1.377573   Top1 52.871094   Top5 92.148438   BatchTime 0.392736   LR 0.001016
0.60740036
0.60721862
0.60698164
0.60668921
0.60662013
0.60646713
0.60623789
0.60607183
0.60598564
0.60575563
0.60565674
0.60554695
0.60552078
0.60528994
0.60502338
0.60482299
0.60451281
0.60422516
0.60409206
0.60399002
0.60381275
0.60364544
0.60375273
INFO - Training [21][   60/  196]   Loss 1.371913   Top1 53.066406   Top5 92.493490   BatchTime 0.378098   LR 0.000996
0.60368460
0.60378939
0.60385120
0.60391068
0.60391587
0.60383868
0.60389632
0.60401255
0.60419327
0.60431087
0.60445553
0.60450470
0.60451978
0.60450947
0.60446995
0.60455555
0.60445213
INFO - Training [21][   80/  196]   Loss 1.375175   Top1 52.949219   Top5 92.485352   BatchTime 0.370881   LR 0.000976
0.60449094
0.60448766
0.60440266
0.60425788
0.60417759
0.60406446
0.60397291
0.60381222
0.60357612
0.60350811
0.60355598
0.60352641
0.60348189
0.60348874
0.60334557
0.60328817
0.60307550
0.60293627
0.60283428
0.60271412
0.60261369
0.60249335
0.60260904
INFO - Training [21][  100/  196]   Loss 1.370094   Top1 53.054688   Top5 92.570312   BatchTime 0.368435   LR 0.000957
0.60272366
0.60289180
0.60311121
0.60338092
0.60357958
0.60365838
0.60386133
0.60406876
0.60422057
0.60424542
0.60438764
0.60443324
0.60454100
0.60451376
0.60439092
0.60424578
0.60418278
INFO - Training [21][  120/  196]   Loss 1.369376   Top1 52.939453   Top5 92.574870   BatchTime 0.365347   LR 0.000937
0.60426277
0.60432678
0.60435730
0.60424018
0.60415781
0.60394615
0.60376930
0.60365486
0.60343575
0.60330099
0.60312414
0.60302263
0.60303551
0.60298145
0.60295832
0.60296857
0.60288721
0.60286647
0.60290349
0.60302860
INFO - Training [21][  140/  196]   Loss 1.367631   Top1 53.136161   Top5 92.650670   BatchTime 0.357157   LR 0.000918
0.60302907
0.60305595
0.60292423
0.60285395
0.60280818
0.60275257
0.60273075
0.60267943
0.60273987
0.60273677
0.60267317
0.60273439
0.60273618
0.60262692
0.60261536
0.60258424
0.60256130
0.60258979
0.60252988
0.60258222
0.60256535
0.60256332
0.60254663
INFO - Training [21][  160/  196]   Loss 1.364979   Top1 53.273926   Top5 92.658691   BatchTime 0.354687   LR 0.000899
0.60250932
0.60234952
0.60228962
0.60231489
0.60240203
0.60250932
0.60254896
0.60254484
0.60258061
0.60251760
0.60248321
0.60236079
0.60228902
0.60220391
0.60201949
0.60191494
0.60184991
INFO - Training [21][  180/  196]   Loss 1.360699   Top1 53.537326   Top5 92.632378   BatchTime 0.353968   LR 0.000879
0.60190380
0.60177672
0.60165524
0.60144299
0.60122448
0.60115045
0.60119885
0.60104746
0.60109299
0.60122466
0.60149395
0.60182709
0.60217965
0.60246330
0.60280460
0.60307980
INFO - ==> Top1: 53.670    Top5: 92.690    Loss: 1.357
0.60336858
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 1.091082   Top1 61.660156   Top5 95.898438   BatchTime 0.113435
INFO - Validation [21][   40/   40]   Loss 1.093117   Top1 61.560000   Top5 95.980000   BatchTime 0.083367
INFO - ==> Top1: 61.560    Top5: 95.980    Loss: 1.093
INFO - ==> Sparsity : 0.632
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 61.560   Top5: 95.980]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.3906)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.0810)
features.2.conv.6 tensor(0.0938)
features.3.conv.0 tensor(0.0475)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.0586)
features.4.conv.0 tensor(0.5129)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.1006)
features.5.conv.0 tensor(0.0487)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0780)
features.6.conv.0 tensor(0.0378)
features.6.conv.3 tensor(0.0729)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0781)
features.7.conv.3 tensor(0.1343)
features.7.conv.6 tensor(0.8384)
features.8.conv.0 tensor(0.8982)
features.8.conv.3 tensor(0.1409)
features.8.conv.6 tensor(0.9624)
features.9.conv.0 tensor(0.0712)
features.9.conv.3 tensor(0.1583)
features.9.conv.6 tensor(0.9099)
features.10.conv.0 tensor(0.8925)
features.10.conv.3 tensor(0.2972)
features.10.conv.6 tensor(0.8158)
features.11.conv.0 tensor(0.1330)
features.11.conv.3 tensor(0.1919)
features.11.conv.6 tensor(0.2527)
features.12.conv.0 tensor(0.1027)
features.12.conv.3 tensor(0.1622)
features.12.conv.6 tensor(0.3218)
features.13.conv.0 tensor(0.1097)
features.13.conv.3 tensor(0.1765)
features.13.conv.6 tensor(0.4760)
features.14.conv.0 tensor(0.8179)
features.14.conv.3 tensor(0.1034)
features.14.conv.6 tensor(0.9386)
features.15.conv.0 tensor(0.7415)
features.15.conv.3 tensor(0.0888)
features.15.conv.6 tensor(0.9785)
features.16.conv.0 tensor(0.4999)
features.16.conv.3 tensor(0.0863)
features.16.conv.6 tensor(0.6808)
conv.0 tensor(0.7556)
tensor(1384186.) 2188896.0
0.60381621
0.60419524
0.60453141
0.60475045
0.60489845
0.60508144
0.60521531
0.60529333
0.60523570
0.60511857
0.60493737
0.60476273
0.60459232
0.60433805
0.60407448
0.60381752
0.60358417
0.60330057
INFO - Training [22][   20/  196]   Loss 1.357938   Top1 53.046875   Top5 91.445312   BatchTime 0.432938   LR 0.000846
0.60310197
0.60303956
0.60297239
0.60286146
0.60283089
0.60277349
0.60262215
0.60254735
0.60251576
0.60260290
0.60253036
0.60243446
0.60245156
0.60254478
0.60251021
0.60250551
0.60244536
0.60256177
0.60269904
0.60270262
0.60264242
0.60256273
0.60244495
INFO - Training [22][   40/  196]   Loss 1.364095   Top1 52.724609   Top5 91.914062   BatchTime 0.387764   LR 0.000827
0.60235125
0.60236573
0.60218173
0.60203779
0.60206449
0.60201144
0.60205698
0.60202265
0.60196185
0.60205114
0.60206085
0.60217184
0.60216635
0.60212731
0.60212815
0.60206550
0.60187322
0.60174662
0.60173100
0.60173738
0.60176593
0.60179299
INFO - Training [22][   60/  196]   Loss 1.361443   Top1 53.144531   Top5 92.011719   BatchTime 0.378457   LR 0.000808
0.60190690
0.60200810
0.60215539
0.60221124
0.60229170
0.60236424
0.60241085
0.60244167
0.60254616
0.60266268
0.60276127
0.60275626
0.60284042
0.60295725
0.60302186
0.60315561
INFO - Training [22][   80/  196]   Loss 1.359297   Top1 53.173828   Top5 92.197266   BatchTime 0.375681   LR 0.000789
0.60328549
0.60325199
0.60326850
0.60331780
0.60328984
0.60340041
0.60331619
0.60329628
0.60329109
0.60327083
0.60318273
0.60314202
0.60307676
0.60308415
0.60312891
0.60318202
0.60323751
0.60339355
0.60344583
0.60337782
0.60345066
0.60363877
0.60380560
INFO - Training [22][  100/  196]   Loss 1.350067   Top1 53.601562   Top5 92.476562   BatchTime 0.370975   LR 0.000770
0.60385871
0.60387647
0.60388660
0.60400546
0.60411948
0.60427076
0.60443431
0.60457325
0.60469002
0.60483950
0.60491806
0.60490638
0.60490966
0.60485297
0.60481358
0.60470122
0.60460240
INFO - Training [22][  120/  196]   Loss 1.344243   Top1 53.850911   Top5 92.698568   BatchTime 0.368154   LR 0.000752
0.60460430
0.60464042
0.60455853
0.60452181
0.60443115
0.60438144
0.60444242
0.60450035
0.60460430
0.60459232
0.60451519
0.60453647
0.60451102
0.60429251
0.60416776
0.60417515
0.60399032
0.60379094
0.60359049
0.60345304
0.60341209
0.60334355
0.60339493
INFO - Training [22][  140/  196]   Loss 1.342237   Top1 54.104353   Top5 92.784598   BatchTime 0.364688   LR 0.000734
0.60344988
0.60352325
0.60359001
0.60349137
0.60339290
0.60323775
0.60307652
0.60288912
0.60258609
0.60219616
0.60174698
0.60126221
0.60100454
0.60084683
0.60083151
0.60066754
0.60084409
0.60109735
0.60136104
0.60134119
0.60127425
0.60118109
INFO - Training [22][  160/  196]   Loss 1.346963   Top1 53.969727   Top5 92.700195   BatchTime 0.367166   LR 0.000715
0.60102767
0.60098481
0.60082299
0.60067785
0.60048789
0.60053974
0.60056549
0.60058159
0.60062987
0.60062116
0.60066181
0.60061973
0.60066259
0.60074407
0.60087049
0.60100156
0.60116225
0.60133851
0.60147959
INFO - Training [22][  180/  196]   Loss 1.343133   Top1 54.136285   Top5 92.753906   BatchTime 0.360019   LR 0.000697
0.60158205
0.60160613
0.60151672
0.60145479
0.60149401
0.60154575
0.60158694
0.60170197
0.60173845
0.60184526
0.60198313
0.60211420
0.60206461
********************pre-trained*****************
INFO - ==> Top1: 54.144    Top5: 92.756    Loss: 1.342
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [22][   20/   40]   Loss 1.097062   Top1 61.699219   Top5 95.917969   BatchTime 0.114878
INFO - Validation [22][   40/   40]   Loss 1.093630   Top1 61.450000   Top5 95.990000   BatchTime 0.084164
INFO - ==> Top1: 61.450    Top5: 95.990    Loss: 1.094
INFO - ==> Sparsity : 0.636
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 61.560   Top5: 95.980]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3965)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0642)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.0795)
features.2.conv.6 tensor(0.0940)
features.3.conv.0 tensor(0.0466)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0588)
features.4.conv.0 tensor(0.5197)
features.4.conv.3 tensor(0.1123)
features.4.conv.6 tensor(0.1011)
features.5.conv.0 tensor(0.0615)
features.5.conv.3 tensor(0.0747)
features.5.conv.6 tensor(0.0780)
features.6.conv.0 tensor(0.0369)
features.6.conv.3 tensor(0.0747)
features.6.conv.6 tensor(0.0780)
features.7.conv.0 tensor(0.0780)
features.7.conv.3 tensor(0.1331)
features.7.conv.6 tensor(0.8242)
features.8.conv.0 tensor(0.9003)
features.8.conv.3 tensor(0.1406)
features.8.conv.6 tensor(0.9630)
features.9.conv.0 tensor(0.0716)
features.9.conv.3 tensor(0.1597)
features.9.conv.6 tensor(0.9131)
features.10.conv.0 tensor(0.8899)
features.10.conv.3 tensor(0.2986)
features.10.conv.6 tensor(0.8140)
features.11.conv.0 tensor(0.1328)
features.11.conv.3 tensor(0.1919)
features.11.conv.6 tensor(0.2527)
features.12.conv.0 tensor(0.1034)
features.12.conv.3 tensor(0.1615)
features.12.conv.6 tensor(0.3215)
features.13.conv.0 tensor(0.1213)
features.13.conv.3 tensor(0.1759)
features.13.conv.6 tensor(0.4934)
features.14.conv.0 tensor(0.8185)
features.14.conv.3 tensor(0.1034)
features.14.conv.6 tensor(0.9382)
features.15.conv.0 tensor(0.7423)
features.15.conv.3 tensor(0.0881)
features.15.conv.6 tensor(0.9801)
features.16.conv.0 tensor(0.5249)
features.16.conv.3 tensor(0.0866)
features.16.conv.6 tensor(0.6762)
conv.0 tensor(0.7607)
tensor(1391134.) 2188896.0
0.60206062
0.60210139
0.60211432
0.60216039
0.60218859
0.60224438
0.60228038
0.60224825
0.60224348
0.60228306
0.60235476
0.60249597
0.60262716
0.60271001
0.60287857
0.60296327
0.60300785
0.60306740
INFO - Training [23][   20/  196]   Loss 1.347605   Top1 53.437500   Top5 92.851562   BatchTime 0.399877   LR 0.000666
0.60302275
0.60311276
0.60309929
0.60308164
0.60312492
0.60315585
0.60325199
0.60331964
0.60339624
0.60348701
0.60357654
0.60363835
0.60358900
0.60362256
0.60372698
0.60367036
0.60361636
0.60363621
0.60375679
0.60378629
0.60375184
0.60378826
INFO - Training [23][   40/  196]   Loss 1.337742   Top1 54.130859   Top5 92.851562   BatchTime 0.383800   LR 0.000648
0.60386252
0.60378152
0.60366923
0.60355330
0.60339159
0.60338187
0.60333598
0.60327297
0.60309011
0.60301983
0.60313237
0.60329485
0.60335612
0.60336918
0.60337704
0.60336721
0.60328919
0.60316473
0.60301232
0.60293555
0.60287243
0.60283673
0.60267687
INFO - Training [23][   60/  196]   Loss 1.335091   Top1 54.212240   Top5 92.792969   BatchTime 0.370863   LR 0.000630
0.60245788
0.60228521
0.60205716
0.60185003
0.60159683
0.60143065
0.60116422
0.60096788
0.60081857
0.60054785
0.60039735
0.60020345
0.59994775
0.59970701
0.59951252
0.59951377
0.59947354
INFO - Training [23][   80/  196]   Loss 1.337137   Top1 54.194336   Top5 92.905273   BatchTime 0.366900   LR 0.000613
0.59935898
0.59934652
0.59937871
0.59940392
0.59934193
0.59925133
0.59918517
0.59898448
0.59877843
0.59869462
0.59866625
0.59882015
0.59909934
0.59934288
0.59946215
0.59957969
0.59953350
0.59951335
0.59954989
0.59955889
0.59964949
0.59973675
INFO - Training [23][  100/  196]   Loss 1.331236   Top1 54.500000   Top5 92.914062   BatchTime 0.365739   LR 0.000596
0.59979862
0.59990412
0.59998262
0.60017383
0.60023093
0.60030562
0.60041070
0.60038757
0.60032856
0.60024548
0.60022968
0.60019267
0.60008109
0.60006762
0.60005605
0.59987205
0.59980953
0.59969801
0.59973752
0.59990096
INFO - Training [23][  120/  196]   Loss 1.328591   Top1 54.557292   Top5 93.030599   BatchTime 0.370842   LR 0.000579
0.59993678
0.59997833
0.60019338
0.60043907
0.60056633
0.60058957
0.60061842
0.60068142
0.60077804
0.60083395
0.60083991
0.60090971
0.60097760
0.60115337
0.60118026
0.60125381
0.60141051
INFO - Training [23][  140/  196]   Loss 1.331341   Top1 54.547991   Top5 93.027344   BatchTime 0.368707   LR 0.000562
0.60143238
0.60138458
0.60137153
0.60143393
0.60135567
0.60145426
0.60147786
0.60149819
0.60145241
0.60152298
0.60156971
0.60158211
0.60158002
0.60155195
0.60160613
0.60166919
0.60161853
0.60150295
0.60142052
0.60136843
0.60128415
0.60122365
INFO - Training [23][  160/  196]   Loss 1.333828   Top1 54.460449   Top5 92.985840   BatchTime 0.368186   LR 0.000545
0.60122275
0.60111880
0.60090494
0.60081345
0.60074210
0.60061347
0.60048550
0.60033357
0.60018104
0.60015613
0.60012335
0.60005140
0.59997725
0.59988785
0.59974277
0.59967870
0.59964359
0.59965426
0.59969294
INFO - Training [23][  180/  196]   Loss 1.331635   Top1 54.600694   Top5 92.921007   BatchTime 0.363469   LR 0.000529
0.59968978
0.59972984
0.59986359
0.59979939
0.59972560
0.59951097
0.59940976
0.59935093
0.59934574
0.59934372
0.59930736
0.59940845
0.59951639
0.59951985
0.59949517
0.59942836
********************pre-trained*****************
INFO - ==> Top1: 54.666    Top5: 92.900    Loss: 1.330
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [23][   20/   40]   Loss 1.209776   Top1 58.300781   Top5 95.566406   BatchTime 0.111341
INFO - Validation [23][   40/   40]   Loss 1.226450   Top1 57.810000   Top5 95.470000   BatchTime 0.082492
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.3926)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0914)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0810)
features.2.conv.6 tensor(0.0949)
features.3.conv.0 tensor(0.0486)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0586)
features.4.conv.0 tensor(0.5057)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.1011)
features.5.conv.0 tensor(0.1400)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0781)
features.6.conv.0 tensor(0.0373)
features.6.conv.3 tensor(0.0741)
features.6.conv.6 tensor(0.0777)
features.7.conv.0 tensor(0.0780)
features.7.conv.3 tensor(0.1322)
features.7.conv.6 tensor(0.8318)
features.8.conv.0 tensor(0.9034)
features.8.conv.3 tensor(0.1403)
features.8.conv.6 tensor(0.9630)
features.9.conv.0 tensor(0.0720)
features.9.conv.3 tensor(0.1597)
features.9.conv.6 tensor(0.9079)
features.10.conv.0 tensor(0.8883)
features.10.conv.3 tensor(0.3006)
features.10.conv.6 tensor(0.8150)
features.11.conv.0 tensor(0.1340)
features.11.conv.3 tensor(0.1948)
features.11.conv.6 tensor(0.2520)
features.12.conv.0 tensor(0.1053)
features.12.conv.3 tensor(0.1628)
features.12.conv.6 tensor(0.3212)
features.13.conv.0 tensor(0.1202)
features.13.conv.3 tensor(0.1744)
features.13.conv.6 tensor(0.5211)
features.14.conv.0 tensor(0.8193)
features.14.conv.3 tensor(0.1021)
features.14.conv.6 tensor(0.9469)
features.15.conv.0 tensor(0.7434)
features.15.conv.3 tensor(0.0891)
features.15.conv.6 tensor(0.9772)
features.16.conv.0 tensor(0.5321)
features.16.conv.3 tensor(0.0846)
features.16.conv.6 tensor(0.7131)
conv.0 tensor(0.7694)
tensor(1411493.) 2188896.0
INFO - ==> Top1: 57.810    Top5: 95.470    Loss: 1.226
INFO - ==> Sparsity : 0.645
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 61.560   Top5: 95.980]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.59934658
0.59923971
0.59908473
0.59896559
0.59874928
0.59860760
0.59842384
0.59832686
0.59824991
0.59819591
0.59814215
0.59805965
0.59799534
0.59783423
0.59765571
0.59753031
0.59749609
0.59743488
0.59745944
INFO - Training [24][   20/  196]   Loss 1.348314   Top1 53.085938   Top5 92.871094   BatchTime 0.463149   LR 0.000500
0.59735131
0.59730172
0.59733903
0.59731227
0.59728968
0.59722149
0.59712869
0.59705478
0.59702629
0.59697592
0.59696114
0.59711987
0.59715068
0.59727812
0.59734994
0.59749818
0.59758914
0.59767139
0.59770942
0.59777069
0.59788603
0.59795469
0.59793633
INFO - Training [24][   40/  196]   Loss 1.329495   Top1 54.462891   Top5 93.173828   BatchTime 0.404874   LR 0.000484
0.59800893
0.59815103
0.59815454
0.59823507
0.59823853
0.59821266
0.59811932
0.59804446
0.59802973
0.59790665
0.59765816
0.59732491
0.59702003
0.59680873
0.59649998
0.59612972
INFO - Training [24][   60/  196]   Loss 1.322126   Top1 54.973958   Top5 92.962240   BatchTime 0.395938   LR 0.000468
0.59564388
0.59519625
0.59460342
0.59386057
0.59298319
0.59225476
0.59149957
0.59135616
0.59125668
0.59096199
0.59110808
0.59083325
0.59069216
0.59049761
0.59041017
0.58993828
0.58934647
0.58864689
0.58778572
0.58745539
0.58686686
0.58615178
0.58547378
INFO - Training [24][   80/  196]   Loss 1.326501   Top1 54.687500   Top5 93.007812   BatchTime 0.381794   LR 0.000453
0.58483279
0.58386934
0.58312571
0.58281201
0.58226097
0.58226299
0.58197922
0.58182448
0.58176571
0.58158225
0.58139575
0.58130199
0.58117521
0.58117342
0.58112538
0.58117169
0.58108449
0.58105856
0.58116853
0.58130300
0.58128816
0.58145416
INFO - Training [24][  100/  196]   Loss 1.324166   Top1 54.937500   Top5 92.976562   BatchTime 0.380068   LR 0.000437
0.58158821
0.58157808
0.58160108
0.58154547
0.58126140
0.58128226
0.58122754
0.58109313
0.58100563
0.58084989
0.58062798
0.58054179
0.58043444
0.58033180
0.58004087
0.57989085
INFO - Training [24][  120/  196]   Loss 1.320967   Top1 55.022786   Top5 93.128255   BatchTime 0.377368   LR 0.000422
0.57995796
0.57990384
0.57990509
0.58004731
0.58013123
0.57990122
0.57968366
0.57945681
0.57932979
0.57921296
0.57910031
0.57922220
0.57934856
0.57944041
0.57948416
0.57963771
0.57964784
0.57978171
0.57999367
0.58020747
0.58038306
0.58043861
0.58034986
INFO - Training [24][  140/  196]   Loss 1.320952   Top1 54.941406   Top5 93.211496   BatchTime 0.373818   LR 0.000407
0.58034223
0.58041757
0.58046621
0.58046645
0.58042073
0.58025676
0.58022785
0.58025891
0.58036327
0.58051640
0.58041191
0.58025032
0.58016616
0.58007586
0.57996690
0.57980549
INFO - Training [24][  160/  196]   Loss 1.322547   Top1 54.926758   Top5 93.117676   BatchTime 0.374347   LR 0.000392
0.57970959
0.57960469
0.57956952
0.57940590
0.57930309
0.57934833
0.57927448
0.57931215
0.57916796
0.57899612
0.57874793
0.57863224
0.57847291
0.57830554
0.57808900
0.57804352
0.57794106
0.57783318
0.57768214
0.57765043
0.57764870
0.57753235
0.57745421
0.57727307
INFO - Training [24][  180/  196]   Loss 1.320797   Top1 55.052083   Top5 93.077257   BatchTime 0.367622   LR 0.000378
0.57695884
0.57692045
0.57684988
0.57682985
0.57684302
0.57681310
0.57676440
0.57665217
0.57643318
0.57617223
0.57584298
0.57562470
0.57533419
0.57524312
INFO - ==> Top1: 55.172    Top5: 93.132    Loss: 1.317
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 1.074255   Top1 62.324219   Top5 95.937500   BatchTime 0.115475
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0651)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0810)
features.2.conv.6 tensor(0.0943)
features.3.conv.0 tensor(0.0489)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0584)
features.4.conv.0 tensor(0.4943)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.1009)
features.5.conv.0 tensor(0.7135)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0785)
features.6.conv.0 tensor(0.0368)
features.6.conv.3 tensor(0.0752)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0835)
features.7.conv.3 tensor(0.1334)
features.7.conv.6 tensor(0.8257)
features.8.conv.0 tensor(0.9036)
features.8.conv.3 tensor(0.1392)
features.8.conv.6 tensor(0.9629)
features.9.conv.0 tensor(0.0745)
features.9.conv.3 tensor(0.1583)
features.9.conv.6 tensor(0.9125)
features.10.conv.0 tensor(0.8882)
features.10.conv.3 tensor(0.3012)
features.10.conv.6 tensor(0.8180)
features.11.conv.0 tensor(0.1347)
features.11.conv.3 tensor(0.1929)
features.11.conv.6 tensor(0.2524)
features.12.conv.0 tensor(0.1055)
features.12.conv.3 tensor(0.1613)
features.12.conv.6 tensor(0.3205)
features.13.conv.0 tensor(0.1266)
features.13.conv.3 tensor(0.1750)
features.13.conv.6 tensor(0.5530)
features.14.conv.0 tensor(0.8194)
features.14.conv.3 tensor(0.1012)
features.14.conv.6 tensor(0.9438)
features.15.conv.0 tensor(0.7438)
features.15.conv.3 tensor(0.0906)
features.15.conv.6 tensor(0.9768)
features.16.conv.0 tensor(0.6007)
features.16.conv.3 tensor(0.0830)
features.16.conv.6 tensor(0.6956)
conv.0 tensor(0.7687)
tensor(1422943.) 2188896.0
INFO - Validation [24][   40/   40]   Loss 1.078236   Top1 62.560000   Top5 95.890000   BatchTime 0.092876
INFO - ==> Top1: 62.560    Top5: 95.890    Loss: 1.078
INFO - ==> Sparsity : 0.650
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 62.560   Top5: 95.890]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.57504487
0.57507956
0.57506508
0.57515150
0.57527858
0.57527351
0.57536924
0.57552308
0.57573849
0.57575089
0.57568353
0.57576990
0.57583791
0.57588726
0.57596397
0.57605606
0.57620823
0.57635307
INFO - Training [25][   20/  196]   Loss 1.317519   Top1 56.015625   Top5 92.578125   BatchTime 0.454892   LR 0.000353
0.57662708
0.57683557
0.57694447
0.57694012
0.57679623
0.57686222
0.57696164
0.57701427
0.57711858
0.57708436
0.57712895
0.57727623
0.57734829
0.57737488
0.57737654
0.57736468
0.57725972
0.57717544
0.57715750
0.57718807
0.57721198
INFO - Training [25][   40/  196]   Loss 1.308624   Top1 56.162109   Top5 92.968750   BatchTime 0.419914   LR 0.000339
0.57722145
0.57713240
0.57699710
0.57687604
0.57680637
0.57665557
0.57658052
0.57652342
0.57642782
0.57641751
0.57636982
0.57638443
0.57642239
0.57640982
0.57636666
0.57636386
0.57635713
0.57636780
0.57632661
0.57622087
0.57624829
0.57623261
INFO - Training [25][   60/  196]   Loss 1.302547   Top1 55.891927   Top5 93.098958   BatchTime 0.400705   LR 0.000325
0.57617337
0.57601362
0.57593167
0.57579827
0.57572567
0.57563120
0.57547951
0.57544231
0.57532686
0.57514536
0.57500774
0.57481307
0.57466733
0.57453704
0.57443404
0.57425994
0.57415301
0.57409739
0.57402545
0.57398593
0.57389688
INFO - Training [25][   80/  196]   Loss 1.300028   Top1 55.893555   Top5 93.203125   BatchTime 0.394871   LR 0.000312
0.57384259
0.57395869
0.57387018
0.57384086
0.57364804
0.57357347
0.57358414
0.57361996
0.57373369
0.57378352
0.57393217
0.57407135
0.57424766
0.57442349
0.57455403
0.57475024
0.57482088
INFO - Training [25][  100/  196]   Loss 1.298295   Top1 55.914062   Top5 93.300781   BatchTime 0.387663   LR 0.000299
0.57489574
0.57492977
0.57491213
0.57492179
0.57501024
0.57499111
0.57502681
0.57501549
0.57506490
0.57520217
0.57526439
0.57526815
0.57522702
0.57528359
0.57524240
0.57511568
0.57499886
0.57480669
0.57471859
0.57454950
0.57449251
INFO - Training [25][  120/  196]   Loss 1.294968   Top1 55.914714   Top5 93.470052   BatchTime 0.386608   LR 0.000286
0.57449126
0.57459784
0.57444590
0.57434124
0.57422101
0.57411325
0.57384926
0.57374501
0.57363063
0.57360262
0.57348078
0.57339162
0.57339454
0.57325739
0.57298785
0.57284802
0.57261240
0.57225120
0.57201242
0.57175726
0.57120579
0.57093912
INFO - Training [25][  140/  196]   Loss 1.298051   Top1 55.767299   Top5 93.496094   BatchTime 0.382751   LR 0.000273
0.57049650
0.57011062
0.57006598
0.57025796
0.57042813
0.57049012
0.57054853
0.57052374
0.57043886
0.57058191
0.57058251
0.57072270
0.57088685
0.57077575
0.57086068
0.57091355
INFO - Training [25][  160/  196]   Loss 1.300384   Top1 55.734863   Top5 93.442383   BatchTime 0.382281   LR 0.000261
0.57072031
0.57045573
0.57027996
0.57007080
0.57003337
0.57008111
0.56998646
0.56992859
0.56997669
0.56995749
0.56987095
0.56990600
0.57000023
0.56998533
0.56996590
0.56989807
0.56979728
0.56985718
0.56998962
0.57011849
0.57026172
0.57025844
INFO - Training [25][  180/  196]   Loss 1.296901   Top1 55.809462   Top5 93.433160   BatchTime 0.381149   LR 0.000248
0.57026458
0.57028341
0.57031620
0.57032508
0.57044715
0.57036418
0.57025999
0.57015020
0.57012469
0.57011569
0.57004416
0.57003617
0.57011110
0.57020706
0.57032269
0.57041764
********************pre-trained*****************
INFO - ==> Top1: 55.848    Top5: 93.482    Loss: 1.294
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 1.166911   Top1 58.789062   Top5 95.292969   BatchTime 0.117204
INFO - Validation [25][   40/   40]   Loss 1.179624   Top1 58.570000   Top5 95.210000   BatchTime 0.095990
INFO - ==> Top1: 58.570    Top5: 95.210    Loss: 1.180
INFO - ==> Sparsity : 0.656
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 62.560   Top5: 95.890]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0664)
features.2.conv.0 tensor(0.0512)
features.2.conv.3 tensor(0.0818)
features.2.conv.6 tensor(0.0946)
features.3.conv.0 tensor(0.0489)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.0588)
features.4.conv.0 tensor(0.4976)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.1007)
features.5.conv.0 tensor(0.7412)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0785)
features.6.conv.0 tensor(0.0378)
features.6.conv.3 tensor(0.0723)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0790)
features.7.conv.3 tensor(0.1317)
features.7.conv.6 tensor(0.8330)
features.8.conv.0 tensor(0.9036)
features.8.conv.3 tensor(0.1398)
features.8.conv.6 tensor(0.9630)
features.9.conv.0 tensor(0.0769)
features.9.conv.3 tensor(0.1583)
features.9.conv.6 tensor(0.9159)
features.10.conv.0 tensor(0.8893)
features.10.conv.3 tensor(0.2998)
features.10.conv.6 tensor(0.8185)
features.11.conv.0 tensor(0.1354)
features.11.conv.3 tensor(0.1927)
features.11.conv.6 tensor(0.2526)
features.12.conv.0 tensor(0.1060)
features.12.conv.3 tensor(0.1634)
features.12.conv.6 tensor(0.3202)
features.13.conv.0 tensor(0.1393)
features.13.conv.3 tensor(0.1755)
features.13.conv.6 tensor(0.5376)
features.14.conv.0 tensor(0.8198)
features.14.conv.3 tensor(0.1027)
features.14.conv.6 tensor(0.9461)
features.15.conv.0 tensor(0.7439)
features.15.conv.3 tensor(0.0910)
features.15.conv.6 tensor(0.9764)
features.16.conv.0 tensor(0.6315)
features.16.conv.3 tensor(0.0836)
features.16.conv.6 tensor(0.7155)
conv.0 tensor(0.7723)
tensor(1435421.) 2188896.0
0.57060516
0.57081944
0.57093304
0.57107139
0.57125854
0.57135439
0.57144910
0.57148629
0.57143939
0.57149971
0.57153469
0.57172441
0.57191002
0.57206309
0.57216400
0.57224125
0.57224137
0.57230103
0.57236749
0.57234496
0.57233691
0.57235384
0.57228291
INFO - Training [26][   20/  196]   Loss 1.302921   Top1 55.371094   Top5 92.851562   BatchTime 0.418818   LR 0.000228
0.57230723
0.57235974
0.57236522
0.57240689
0.57239062
0.57232356
0.57222432
0.57214665
0.57213777
0.57215649
0.57217079
0.57217747
0.57220000
0.57227302
0.57231891
0.57233715
0.57235497
INFO - Training [26][   40/  196]   Loss 1.318357   Top1 54.736328   Top5 92.998047   BatchTime 0.384927   LR 0.000216
0.57238299
0.57238650
0.57237881
0.57230610
0.57234150
0.57220876
0.57214040
0.57204050
0.57192492
0.57188010
0.57182842
0.57178354
0.57174015
0.57170689
0.57173359
0.57168835
0.57166690
0.57160622
0.57152832
0.57156909
0.57149899
0.57148212
0.57148767
INFO - Training [26][   60/  196]   Loss 1.308852   Top1 55.292969   Top5 93.040365   BatchTime 0.372568   LR 0.000205
0.57153755
0.57156801
0.57161939
0.57167655
0.57169664
0.57179111
0.57181048
0.57181442
0.57187498
0.57194835
0.57197684
0.57201374
0.57201421
0.57199180
0.57193959
0.57191575
0.57186478
0.57175851
INFO - Training [26][   80/  196]   Loss 1.304274   Top1 55.327148   Top5 93.281250   BatchTime 0.365814   LR 0.000194
0.57164598
0.57153285
0.57146817
0.57133967
0.57127434
0.57126194
0.57124907
0.57119524
0.57114071
0.57111102
0.57101524
0.57092613
0.57087326
0.57080698
0.57080019
0.57069170
0.57064587
INFO - Training [26][  100/  196]   Loss 1.297185   Top1 55.660156   Top5 93.382812   BatchTime 0.362633   LR 0.000183
0.57071739
0.57068449
0.57061112
0.57067049
0.57064956
0.57060522
0.57054174
0.57048297
0.57042134
0.57039481
0.57041144
0.57040083
0.57038480
0.57041132
0.57050759
0.57056290
0.57058221
0.57051939
0.57046217
0.57043815
0.57039809
INFO - Training [26][  120/  196]   Loss 1.295358   Top1 55.592448   Top5 93.414714   BatchTime 0.367286   LR 0.000173
0.57034504
0.57032907
0.57033461
0.57033145
0.57037038
0.57036299
0.57046688
0.57045007
0.57041752
0.57045031
0.57046533
0.57047540
0.57045472
0.57042742
0.57042634
0.57045037
0.57040018
0.57036740
0.57032520
0.57030064
0.57034224
INFO - Training [26][  140/  196]   Loss 1.292128   Top1 55.711496   Top5 93.476562   BatchTime 0.367233   LR 0.000163
0.57033402
0.57030076
0.57027727
0.57021403
0.57017595
0.57007086
0.57003492
0.56999481
0.56992847
0.56983870
0.56977886
0.56975913
0.56966478
0.56951624
0.56941986
0.56930763
0.56927007
0.56920278
0.56911391
0.56904298
0.56899679
0.56895262
INFO - Training [26][  160/  196]   Loss 1.293517   Top1 55.734863   Top5 93.439941   BatchTime 0.365850   LR 0.000153
0.56908458
0.56909543
0.56907135
0.56909764
0.56912571
0.56909299
0.56904984
0.56895089
0.56893182
0.56892532
0.56886059
0.56881946
0.56880039
0.56879431
0.56873858
0.56868470
0.56865144
0.56865120
0.56856841
0.56845218
0.56842327
INFO - Training [26][  180/  196]   Loss 1.294502   Top1 55.664062   Top5 93.378906   BatchTime 0.369602   LR 0.000144
0.56836259
0.56824279
0.56816059
0.56812459
0.56811041
0.56815475
0.56818992
0.56820560
0.56820726
0.56814623
0.56813776
INFO - ==> Top1: 55.688    Top5: 93.358    Loss: 1.294
0.56810921
0.56805891
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 1.026456   Top1 63.710938   Top5 96.386719   BatchTime 0.115201
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.3984)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0818)
features.2.conv.6 tensor(0.0943)
features.3.conv.0 tensor(0.0501)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0590)
features.4.conv.0 tensor(0.5101)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.1006)
features.5.conv.0 tensor(0.7562)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0785)
features.6.conv.0 tensor(0.0376)
features.6.conv.3 tensor(0.0729)
features.6.conv.6 tensor(0.0775)
features.7.conv.0 tensor(0.0793)
features.7.conv.3 tensor(0.1328)
features.7.conv.6 tensor(0.8311)
features.8.conv.0 tensor(0.9025)
features.8.conv.3 tensor(0.1406)
features.8.conv.6 tensor(0.9631)
features.9.conv.0 tensor(0.0790)
features.9.conv.3 tensor(0.1603)
features.9.conv.6 tensor(0.9120)
features.10.conv.0 tensor(0.8895)
features.10.conv.3 tensor(0.3001)
features.10.conv.6 tensor(0.8198)
features.11.conv.0 tensor(0.1356)
features.11.conv.3 tensor(0.1923)
features.11.conv.6 tensor(0.2524)
features.12.conv.0 tensor(0.1068)
features.12.conv.3 tensor(0.1634)
features.12.conv.6 tensor(0.3196)
features.13.conv.0 tensor(0.1668)
features.13.conv.3 tensor(0.1725)
features.13.conv.6 tensor(0.5755)
features.14.conv.0 tensor(0.8203)
features.14.conv.3 tensor(0.1010)
features.14.conv.6 tensor(0.9461)
features.15.conv.0 tensor(0.7442)
features.15.conv.3 tensor(0.0897)
features.15.conv.6 tensor(0.9759)
features.16.conv.0 tensor(0.6193)
features.16.conv.3 tensor(0.0841)
features.16.conv.6 tensor(0.7309)
conv.0 tensor(0.7888)
tensor(1450229.) 2188896.0
INFO - Validation [26][   40/   40]   Loss 1.028378   Top1 63.680000   Top5 96.560000   BatchTime 0.097059
INFO - ==> Top1: 63.680    Top5: 96.560    Loss: 1.028
INFO - ==> Sparsity : 0.663
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 63.680   Top5: 96.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
0.56805140
0.56805444
0.56805253
0.56804782
0.56809372
0.56815022
0.56818467
0.56812221
0.56810886
0.56818193
0.56819308
0.56812972
0.56805670
0.56807798
0.56796682
0.56791353
0.56789535
0.56784195
0.56780165
INFO - Training [27][   20/  196]   Loss 1.325082   Top1 54.335938   Top5 92.832031   BatchTime 0.432039   LR 0.000128
0.56773323
0.56765872
0.56764275
0.56773812
0.56773239
0.56777751
0.56769985
0.56768507
0.56773883
0.56780499
0.56781018
0.56775284
0.56770372
0.56770444
0.56774914
0.56769675
0.56761789
0.56760150
0.56757998
0.56756985
0.56757587
0.56764042
0.56769401
INFO - Training [27][   40/  196]   Loss 1.305159   Top1 55.078125   Top5 93.085938   BatchTime 0.389324   LR 0.000119
0.56773585
0.56772500
0.56777245
0.56788719
0.56785977
0.56788510
0.56785214
0.56786776
0.56789106
0.56788862
0.56792319
0.56798488
0.56795073
0.56793869
0.56793123
0.56791574
0.56793833
INFO - Training [27][   60/  196]   Loss 1.306542   Top1 55.162760   Top5 93.007812   BatchTime 0.379430   LR 0.000111
0.56787324
0.56782520
0.56782722
0.56782609
0.56783456
0.56783050
0.56778395
0.56774229
0.56768686
0.56764376
0.56764680
0.56766880
0.56766891
0.56767875
0.56768709
0.56770873
0.56770229
0.56773359
0.56769645
0.56770629
0.56767923
0.56771648
INFO - Training [27][   80/  196]   Loss 1.311556   Top1 54.941406   Top5 93.051758   BatchTime 0.375677   LR 0.000102
0.56764948
0.56765229
0.56770593
0.56764603
0.56764233
0.56760138
0.56759429
0.56756431
0.56755781
0.56759924
0.56757742
0.56756669
0.56761718
0.56760573
0.56756455
0.56757444
0.56762463
INFO - Training [27][  100/  196]   Loss 1.301250   Top1 55.382812   Top5 93.191406   BatchTime 0.371039   LR 0.000095
0.56763661
0.56765801
0.56762731
0.56758273
0.56758451
0.56763422
0.56765950
0.56764793
0.56761992
0.56762367
0.56758267
0.56758606
0.56756741
0.56754923
0.56756818
0.56755424
0.56756717
0.56753308
0.56750804
0.56748933
0.56744462
INFO - Training [27][  120/  196]   Loss 1.292449   Top1 55.670573   Top5 93.320312   BatchTime 0.372679   LR 0.000087
0.56745911
0.56740123
0.56738597
0.56736761
0.56733376
0.56736821
0.56741691
0.56741571
0.56746525
0.56750119
0.56752306
0.56749141
0.56749731
0.56753057
0.56755519
0.56756175
0.56758273
0.56762606
0.56768888
0.56771606
0.56771380
INFO - Training [27][  140/  196]   Loss 1.294397   Top1 55.770089   Top5 93.373326   BatchTime 0.373301   LR 0.000080
0.56776696
0.56776261
0.56779075
0.56780142
0.56782305
0.56788141
0.56793529
0.56793255
0.56799698
0.56796467
0.56797177
0.56795168
0.56794786
0.56792706
0.56796050
0.56792831
0.56798238
0.56799757
INFO - Training [27][  160/  196]   Loss 1.296622   Top1 55.744629   Top5 93.381348   BatchTime 0.368785   LR 0.000073
0.56802166
0.56794339
0.56791037
0.56789953
0.56787735
0.56781375
0.56775558
0.56773818
0.56771618
0.56775510
0.56765097
0.56763917
0.56764895
0.56763023
0.56761903
0.56760406
0.56763172
0.56761676
0.56761134
0.56763393
0.56765169
0.56761605
0.56761503
INFO - Training [27][  180/  196]   Loss 1.295346   Top1 55.815972   Top5 93.335503   BatchTime 0.366454   LR 0.000066
0.56762135
0.56762135
0.56760234
0.56760323
0.56758118
0.56759912
0.56760556
0.56759715
0.56754464
0.56753486
0.56750673
0.56752211
0.56750906
0.56756645
0.56752354
********************pre-trained*****************
INFO - ==> Top1: 55.920    Top5: 93.372    Loss: 1.293
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [27][   20/   40]   Loss 1.079197   Top1 62.441406   Top5 96.152344   BatchTime 0.171573
features.0.conv.0 tensor(0.5660)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0677)
features.2.conv.0 tensor(0.0518)
features.2.conv.3 tensor(0.0802)
features.2.conv.6 tensor(0.0949)
features.3.conv.0 tensor(0.0506)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.0588)
features.4.conv.0 tensor(0.5088)
features.4.conv.3 tensor(0.1123)
features.4.conv.6 tensor(0.1009)
features.5.conv.0 tensor(0.7591)
features.5.conv.3 tensor(0.0764)
features.5.conv.6 tensor(0.0785)
features.6.conv.0 tensor(0.0376)
features.6.conv.3 tensor(0.0741)
features.6.conv.6 tensor(0.0775)
features.7.conv.0 tensor(0.0793)
features.7.conv.3 tensor(0.1334)
features.7.conv.6 tensor(0.8304)
features.8.conv.0 tensor(0.9023)
features.8.conv.3 tensor(0.1398)
features.8.conv.6 tensor(0.9633)
features.9.conv.0 tensor(0.0795)
features.9.conv.3 tensor(0.1603)
features.9.conv.6 tensor(0.9127)
features.10.conv.0 tensor(0.8899)
features.10.conv.3 tensor(0.2992)
features.10.conv.6 tensor(0.8183)
features.11.conv.0 tensor(0.1353)
features.11.conv.3 tensor(0.1906)
features.11.conv.6 tensor(0.2524)
features.12.conv.0 tensor(0.1072)
features.12.conv.3 tensor(0.1636)
features.12.conv.6 tensor(0.3199)
features.13.conv.0 tensor(0.1862)
features.13.conv.3 tensor(0.1736)
features.13.conv.6 tensor(0.5814)
features.14.conv.0 tensor(0.8202)
features.14.conv.3 tensor(0.1002)
features.14.conv.6 tensor(0.9459)
features.15.conv.0 tensor(0.7441)
features.15.conv.3 tensor(0.0894)
features.15.conv.6 tensor(0.9766)
features.16.conv.0 tensor(0.6222)
features.16.conv.3 tensor(0.0838)
INFO - Validation [27][   40/   40]   Loss 1.082706   Top1 62.060000   Top5 96.080000   BatchTime 0.146145
INFO - ==> Top1: 62.060    Top5: 96.080    Loss: 1.083
INFO - ==> Sparsity : 0.662
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 63.680   Top5: 96.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
features.16.conv.6 tensor(0.7248)
conv.0 tensor(0.7878)
tensor(1450014.) 2188896.0
0.56748772
0.56746376
0.56742585
0.56743968
0.56746590
0.56749177
0.56750584
0.56743842
0.56744224
0.56742460
0.56743890
0.56742132
0.56739271
0.56739277
0.56737190
0.56733501
0.56735039
0.56736034
INFO - Training [28][   20/  196]   Loss 1.294855   Top1 55.820312   Top5 92.558594   BatchTime 0.430769   LR 0.000055
0.56734258
0.56731033
0.56735206
0.56735635
0.56730688
0.56727993
0.56728250
0.56728017
0.56729656
0.56726265
0.56727272
0.56729239
0.56729746
0.56727415
0.56728321
0.56729102
0.56729472
0.56727052
0.56726837
0.56725168
0.56726646
0.56726998
INFO - Training [28][   40/  196]   Loss 1.293948   Top1 55.917969   Top5 92.910156   BatchTime 0.399694   LR 0.000050
0.56727844
0.56731695
0.56736928
0.56735855
0.56738383
0.56738710
0.56741738
0.56737113
0.56735218
0.56733567
0.56736147
0.56729442
0.56731534
0.56728643
0.56727988
0.56723034
0.56724524
INFO - Training [28][   60/  196]   Loss 1.289558   Top1 56.093750   Top5 93.164062   BatchTime 0.383610   LR 0.000044
0.56720620
0.56724763
0.56725317
0.56733805
0.56730777
0.56725657
0.56722915
0.56724650
0.56720489
0.56716484
0.56710714
0.56711161
0.56713885
0.56715035
0.56710470
0.56715715
0.56705993
0.56711441
0.56711626
0.56711721
0.56706059
INFO - Training [28][   80/  196]   Loss 1.285016   Top1 56.210938   Top5 93.374023   BatchTime 0.381489   LR 0.000039
0.56708694
0.56700861
0.56703514
0.56699032
0.56693202
0.56688362
0.56687176
0.56683260
0.56680465
0.56688422
0.56697637
0.56698769
0.56696051
0.56690407
0.56687236
0.56681299
0.56679904
0.56673670
0.56672424
0.56668943
0.56670976
0.56672680
0.56671780
INFO - Training [28][  100/  196]   Loss 1.283568   Top1 56.234375   Top5 93.468750   BatchTime 0.377119   LR 0.000034
0.56669515
0.56670129
0.56659949
0.56659472
0.56660002
0.56668890
0.56667340
0.56663930
0.56659621
0.56661159
0.56659460
0.56658500
0.56651413
0.56656444
0.56646967
0.56649691
INFO - Training [28][  120/  196]   Loss 1.280983   Top1 56.318359   Top5 93.470052   BatchTime 0.374985   LR 0.000030
0.56647146
0.56645274
0.56639123
0.56638920
0.56640410
0.56639075
0.56641769
0.56634641
0.56625634
0.56622255
0.56617123
0.56619149
0.56616658
0.56614959
0.56610543
0.56608975
0.56605357
0.56606853
0.56597686
0.56621629
0.56622595
0.56617635
INFO - Training [28][  140/  196]   Loss 1.282186   Top1 56.297433   Top5 93.551897   BatchTime 0.374410   LR 0.000026
0.56614918
0.56614178
0.56607854
0.56606060
0.56604195
0.56606638
0.56603515
0.56607878
0.56604433
0.56607330
0.56610298
0.56616926
0.56611925
0.56612837
0.56606621
0.56608438
0.56605989
0.56606019
0.56603020
0.56599075
0.56588399
INFO - Training [28][  160/  196]   Loss 1.286820   Top1 56.179199   Top5 93.483887   BatchTime 0.374852   LR 0.000022
0.56579930
0.56578946
0.56599808
0.56604117
0.56609243
0.56608611
0.56610513
0.56610155
0.56615889
0.56611258
0.56610817
0.56607717
0.56608778
0.56609702
0.56611300
0.56610739
0.56610936
0.56610960
0.56609982
0.56608063
0.56608558
0.56605899
INFO - Training [28][  180/  196]   Loss 1.287853   Top1 56.098090   Top5 93.454861   BatchTime 0.373125   LR 0.000018
0.56604850
0.56601101
0.56602645
0.56601769
0.56604832
0.56598675
0.56603581
0.56602263
0.56598049
0.56596422
0.56599516
0.56599611
INFO - ==> Top1: 56.216    Top5: 93.474    Loss: 1.286
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.56602782
0.56600505
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [28][   20/   40]   Loss 1.039809   Top1 63.515625   Top5 96.269531   BatchTime 0.146940
INFO - Validation [28][   40/   40]   Loss 1.046170   Top1 63.190000   Top5 96.340000   BatchTime 0.126963
features.0.conv.0 tensor(0.5590)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0515)
features.2.conv.3 tensor(0.0810)
features.2.conv.6 tensor(0.0949)
features.3.conv.0 tensor(0.0498)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.0590)
features.4.conv.0 tensor(0.5104)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.1011)
features.5.conv.0 tensor(0.7607)
features.5.conv.3 tensor(0.0758)
features.5.conv.6 tensor(0.0785)
features.6.conv.0 tensor(0.0378)
features.6.conv.3 tensor(0.0729)
features.6.conv.6 tensor(0.0772)
features.7.conv.0 tensor(0.0795)
features.7.conv.3 tensor(0.1311)
features.7.conv.6 tensor(0.8310)
features.8.conv.0 tensor(0.9024)
features.8.conv.3 tensor(0.1389)
features.8.conv.6 tensor(0.9631)
features.9.conv.0 tensor(0.0803)
features.9.conv.3 tensor(0.1612)
features.9.conv.6 tensor(0.9143)
features.10.conv.0 tensor(0.8894)
features.10.conv.3 tensor(0.2977)
features.10.conv.6 tensor(0.8184)
features.11.conv.0 tensor(0.1353)
features.11.conv.3 tensor(0.1929)
features.11.conv.6 tensor(0.2521)
features.12.conv.0 tensor(0.1069)
features.12.conv.3 tensor(0.1630)
features.12.conv.6 tensor(0.3196)
features.13.conv.0 tensor(0.1919)
features.13.conv.3 tensor(0.1740)
features.13.conv.6 tensor(0.5910)
features.14.conv.0 tensor(0.8199)
features.14.conv.3 tensor(0.0998)
features.14.conv.6 tensor(0.9468)
features.15.conv.0 tensor(0.7440)
features.15.conv.3 tensor(0.0900)
features.15.conv.6 tensor(0.9770)
features.16.conv.0 tensor(0.6301)
features.16.conv.3 tensor(0.0844)
features.16.conv.6 tensor(0.7286)
conv.0 tensor(0.7937)
tensor(1456197.) 2188896.0
INFO - ==> Top1: 63.190    Top5: 96.340    Loss: 1.046
INFO - ==> Sparsity : 0.665
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 63.680   Top5: 96.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
0.56601506
0.56602246
0.56605762
0.56601334
0.56598014
0.56595027
0.56594199
0.56594920
0.56591713
0.56589329
0.56595981
0.56593096
0.56591135
0.56587017
0.56585795
0.56580490
0.56578946
0.56574285
0.56572258
0.56568009
0.56567812
INFO - Training [29][   20/  196]   Loss 1.293299   Top1 55.605469   Top5 93.378906   BatchTime 0.397295   LR 0.000013
0.56563187
0.56559348
0.56555480
0.56544417
0.56495726
0.56563997
0.56580710
0.56584364
0.56588274
0.56596619
0.56594390
0.56591755
0.56592697
0.56594813
0.56592226
0.56592059
0.56590867
INFO - Training [29][   40/  196]   Loss 1.276015   Top1 56.250000   Top5 93.398438   BatchTime 0.376639   LR 0.000010
0.56591427
0.56592852
0.56588155
0.56583285
0.56584817
0.56584215
0.56586158
0.56584191
0.56588048
0.56588173
0.56588334
0.56587780
0.56587052
0.56584901
0.56583291
0.56581473
0.56580651
0.56576943
0.56578100
0.56579143
0.56581253
0.56578916
0.56581247
INFO - Training [29][   60/  196]   Loss 1.282155   Top1 55.957031   Top5 93.268229   BatchTime 0.367548   LR 0.000008
0.56578332
0.56575495
0.56572014
0.56571579
0.56569618
0.56570297
0.56564891
0.56566954
0.56563741
0.56561124
0.56558275
0.56557900
0.56545144
0.56548798
0.56541169
0.56514567
0.56485176
INFO - Training [29][   80/  196]   Loss 1.283689   Top1 56.083984   Top5 93.364258   BatchTime 0.364307   LR 0.000005
0.56540573
0.56563723
0.56572264
0.56573486
0.56579173
0.56579256
0.56580871
0.56578672
0.56578189
0.56578338
0.56576687
0.56574601
0.56574100
0.56571209
0.56573635
0.56571770
0.56572068
0.56574684
0.56573796
0.56572878
0.56573725
0.56572700
INFO - Training [29][  100/  196]   Loss 1.276906   Top1 56.386719   Top5 93.453125   BatchTime 0.362247   LR 0.000004
0.56574523
0.56571978
0.56570792
0.56569141
0.56569445
0.56565666
0.56564409
0.56561494
0.56562942
0.56558537
0.56557488
0.56552911
0.56551558
0.56545204
0.56540257
0.56542379
0.56544763
INFO - Training [29][  120/  196]   Loss 1.274274   Top1 56.510417   Top5 93.522135   BatchTime 0.361441   LR 0.000002
0.56546438
0.56548411
0.56546712
0.56542802
0.56539077
0.56536669
0.56528312
0.56520683
0.56516510
0.56516415
0.56510782
0.56504136
0.56484228
0.56461531
0.56459886
0.56463522
0.56462359
0.56465137
0.56464094
0.56466687
0.56462324
0.56460035
0.56454337
INFO - Training [29][  140/  196]   Loss 1.278576   Top1 56.417411   Top5 93.454241   BatchTime 0.360539   LR 0.000001
0.56455344
0.56448042
0.56460035
0.56465286
0.56470293
0.56467426
0.56468606
0.56466180
0.56465816
0.56464380
0.56463081
0.56457829
0.56458384
0.56452328
0.56450719
0.56445235
0.56441855
0.56430674
0.56415802
0.56425750
0.56448072
0.56450123
INFO - Training [29][  160/  196]   Loss 1.281228   Top1 56.276855   Top5 93.461914   BatchTime 0.360375   LR 0.000001
0.56454545
0.56455064
0.56456792
0.56453007
0.56453705
0.56451356
0.56453562
0.56448156
0.56449026
0.56446457
0.56444722
0.56443739
0.56443459
0.56437910
0.56436193
0.56432199
INFO - Training [29][  180/  196]   Loss 1.280590   Top1 56.269531   Top5 93.498264   BatchTime 0.362464   LR 0.000000
0.56431627
0.56428385
0.56424201
0.56418014
0.56410432
0.56399876
0.56383640
0.56351840
0.56354266
0.56352031
0.56356472
0.56354439
0.56353718
0.56352371
0.56354547
INFO - ==> Top1: 56.370    Top5: 93.522    Loss: 1.278
0.56350261
0.56353039
0.56351596
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 1.068249   Top1 62.382812   Top5 96.308594   BatchTime 0.108885
INFO - Validation [29][   40/   40]   Loss 1.077279   Top1 62.420000   Top5 96.230000   BatchTime 0.082173
INFO - ==> Top1: 62.420    Top5: 96.230    Loss: 1.077
INFO - ==> Sparsity : 0.666
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 63.680   Top5: 96.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.4023)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0729)
features.2.conv.0 tensor(0.0512)
features.2.conv.3 tensor(0.0810)
features.2.conv.6 tensor(0.0949)
features.3.conv.0 tensor(0.0503)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0588)
features.4.conv.0 tensor(0.5106)
features.4.conv.3 tensor(0.1123)
features.4.conv.6 tensor(0.1011)
features.5.conv.0 tensor(0.7606)
features.5.conv.3 tensor(0.0752)
features.5.conv.6 tensor(0.0785)
features.6.conv.0 tensor(0.0378)
features.6.conv.3 tensor(0.0735)
features.6.conv.6 tensor(0.0774)
features.7.conv.0 tensor(0.0795)
features.7.conv.3 tensor(0.1334)
features.7.conv.6 tensor(0.8304)
features.8.conv.0 tensor(0.9023)
features.8.conv.3 tensor(0.1392)
features.8.conv.6 tensor(0.9633)
features.9.conv.0 tensor(0.0809)
features.9.conv.3 tensor(0.1609)
features.9.conv.6 tensor(0.9139)
features.10.conv.0 tensor(0.8894)
features.10.conv.3 tensor(0.2998)
features.10.conv.6 tensor(0.8187)
features.11.conv.0 tensor(0.1353)
features.11.conv.3 tensor(0.1921)
features.11.conv.6 tensor(0.2520)
features.12.conv.0 tensor(0.1069)
features.12.conv.3 tensor(0.1636)
features.12.conv.6 tensor(0.3198)
features.13.conv.0 tensor(0.1917)
features.13.conv.3 tensor(0.1734)
features.13.conv.6 tensor(0.5907)
features.14.conv.0 tensor(0.8196)
features.14.conv.3 tensor(0.0993)
features.14.conv.6 tensor(0.9467)
features.15.conv.0 tensor(0.7440)
features.15.conv.3 tensor(0.0900)
features.15.conv.6 tensor(0.9771)
features.16.conv.0 tensor(0.6320)
features.16.conv.3 tensor(0.0845)
features.16.conv.6 tensor(0.7290)
conv.0 tensor(0.7941)
tensor(1456736.) 2188896.0
0.56352371
0.56699944
0.56663311
0.56665641
0.56671625
0.56690919
0.56706899
0.56726414
0.56754953
0.56801176
0.56839526
0.56889445
0.56913233
0.56932932
0.56980622
0.57013255
0.57035965
0.57032919
0.57039738
0.57049763
0.57084137
0.57121307
INFO - Training [30][   20/  196]   Loss 1.332690   Top1 54.472656   Top5 91.992188   BatchTime 0.360711   LR 0.001250
0.57169896
0.57228452
0.57272542
0.57309008
0.57348877
0.57373637
0.57390070
0.57389069
0.57408065
0.57435417
0.57473928
0.57498211
0.57529807
0.57556218
0.57625574
INFO - Training [30][   40/  196]   Loss 1.338132   Top1 54.433594   Top5 92.353516   BatchTime 0.314778   LR 0.001250
0.57667649
0.57716662
0.57768852
0.57824051
0.57874465
0.57936525
0.57971460
0.58000624
0.58030754
0.58044994
0.58050472
0.58067244
0.58069885
0.58070529
0.58049661
0.58051807
0.58039433
0.58025366
0.57988256
0.57949537
0.57902843
0.57867342
0.57842499
INFO - Training [30][   60/  196]   Loss 1.319609   Top1 54.941406   Top5 92.656250   BatchTime 0.328117   LR 0.001250
0.57811815
0.57767731
0.57729280
0.57694870
0.57666725
0.57649958
0.57623750
0.57596254
0.57599616
0.57608563
0.57613403
0.57616752
0.57623506
0.57600391
0.57592899
0.57577479
0.57586795
0.57560182
0.57549411
0.57528430
0.57523400
0.57510918
INFO - Training [30][   80/  196]   Loss 1.317435   Top1 54.902344   Top5 92.919922   BatchTime 0.338717   LR 0.001250
0.57518667
0.57518411
0.57522160
0.57515270
0.57518226
0.57504600
0.57507199
0.57510495
0.57528234
0.57522810
0.57513595
0.57495296
0.57493168
0.57483023
0.57483977
0.57485396
INFO - Training [30][  100/  196]   Loss 1.308598   Top1 55.320312   Top5 93.062500   BatchTime 0.346254   LR 0.001250
0.57485896
0.57486778
0.57494539
0.57514012
0.57532322
0.57546639
0.57573360
0.57592708
0.57609922
0.57626450
0.57648748
0.57669306
0.57693148
0.57705420
0.57714677
0.57726389
0.57748598
0.57741380
0.57734543
0.57717943
0.57728440
0.57719952
INFO - Training [30][  120/  196]   Loss 1.300893   Top1 55.677083   Top5 93.271484   BatchTime 0.346992   LR 0.001249
0.57696623
0.57683283
0.57682568
0.57675272
0.57683092
0.57694441
0.57696545
0.57722431
0.57726061
0.57734460
0.57732195
0.57731724
0.57727510
0.57715565
0.57698095
0.57669431
0.57645559
0.57617342
0.57599550
0.57574189
0.57550848
0.57516515
INFO - Training [30][  140/  196]   Loss 1.299913   Top1 55.697545   Top5 93.320312   BatchTime 0.350579   LR 0.001249
0.57489055
0.57455456
0.57435364
0.57398057
0.57383686
0.57383567
0.57373571
0.57364440
0.57356191
0.57364613
0.57379711
0.57371312
0.57373977
0.57365501
0.57356602
0.57340211
0.57334822
INFO - Training [30][  160/  196]   Loss 1.303829   Top1 55.600586   Top5 93.295898   BatchTime 0.351413   LR 0.001249
0.57322502
0.57318509
0.57299393
0.57294995
0.57288641
0.57288438
0.57279915
0.57270801
0.57268363
0.57269645
0.57273465
0.57269329
0.57252806
0.57239944
0.57224911
0.57208949
0.57181716
0.57169038
0.57157087
0.57156509
0.57161671
0.57154667
INFO - Training [30][  180/  196]   Loss 1.301611   Top1 55.648872   Top5 93.315972   BatchTime 0.352631   LR 0.001248
0.57138276
0.57112920
0.57096666
0.57100081
0.57086056
0.57061952
0.57065827
0.57045430
0.57033664
0.57025266
0.57028168
0.57049489
0.57052052
0.57057577
0.57044864
********************pre-trained*****************
INFO - ==> Top1: 55.636    Top5: 93.350    Loss: 1.300
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 1.075287   Top1 63.203125   Top5 95.664062   BatchTime 0.113222
INFO - Validation [30][   40/   40]   Loss 1.086402   Top1 62.440000   Top5 95.650000   BatchTime 0.084108
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.3750)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0949)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0506)
features.2.conv.3 tensor(0.0802)
features.2.conv.6 tensor(0.0966)
features.3.conv.0 tensor(0.0492)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0586)
features.4.conv.0 tensor(0.5299)
features.4.conv.3 tensor(0.1128)
features.4.conv.6 tensor(0.1011)
features.5.conv.0 tensor(0.7907)
features.5.conv.3 tensor(0.0741)
features.5.conv.6 tensor(0.0789)
features.6.conv.0 tensor(0.0381)
features.6.conv.3 tensor(0.0752)
features.6.conv.6 tensor(0.0776)
features.7.conv.0 tensor(0.0817)
features.7.conv.3 tensor(0.1337)
features.7.conv.6 tensor(0.8518)
features.8.conv.0 tensor(0.8993)
features.8.conv.3 tensor(0.1380)
features.8.conv.6 tensor(0.9621)
features.9.conv.0 tensor(0.0712)
features.9.conv.3 tensor(0.1583)
features.9.conv.6 tensor(0.9178)
features.10.conv.0 tensor(0.8883)
features.10.conv.3 tensor(0.3030)
features.10.conv.6 tensor(0.8190)
features.11.conv.0 tensor(0.1265)
features.11.conv.3 tensor(0.1914)
features.11.conv.6 tensor(0.2475)
features.12.conv.0 tensor(0.1000)
features.12.conv.3 tensor(0.1605)
features.12.conv.6 tensor(0.3243)
features.13.conv.0 tensor(0.1070)
features.13.conv.3 tensor(0.1698)
features.13.conv.6 tensor(0.5563)
features.14.conv.0 tensor(0.8172)
features.14.conv.3 tensor(0.1024)
features.14.conv.6 tensor(0.9427)
features.15.conv.0 tensor(0.7431)
features.15.conv.3 tensor(0.0928)
features.15.conv.6 tensor(0.9812)
features.16.conv.0 tensor(0.5619)
features.16.conv.3 tensor(0.0851)
features.16.conv.6 tensor(0.7350)
conv.0 tensor(0.7709)
tensor(1429710.) 2188896.0
INFO - ==> Top1: 62.440    Top5: 95.650    Loss: 1.086
INFO - ==> Sparsity : 0.653
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 63.680   Top5: 96.560]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
0.57021296
0.57016242
0.57015270
0.57018447
0.57035875
0.57035422
0.57046020
0.57054347
0.57081056
0.57107425
0.57134831
0.57174265
0.57220197
0.57251483
0.57289195
0.57327586
0.57343709
0.57376605
0.57407981
INFO - Training [31][   20/  196]   Loss 1.327006   Top1 54.765625   Top5 92.343750   BatchTime 0.437432   LR 0.001248
0.57438713
0.57466966
0.57494956
0.57523787
0.57546312
0.57567841
0.57577902
0.57594848
0.57615179
0.57624245
0.57625300
0.57612997
0.57587183
0.57565933
0.57538623
0.57517934
0.57498968
0.57470977
0.57452911
0.57437569
INFO - Training [31][   40/  196]   Loss 1.316631   Top1 55.068359   Top5 92.626953   BatchTime 0.373707   LR 0.001247
0.57413739
0.57398832
0.57382876
0.57372463
0.57365239
0.57350624
0.57332963
0.57320035
0.57304394
0.57281351
0.57272869
0.57241029
0.57219923
0.57204121
0.57184631
0.57173461
0.57172734
0.57169968
0.57156879
INFO - Training [31][   60/  196]   Loss 1.310891   Top1 55.397135   Top5 92.753906   BatchTime 0.354410   LR 0.001247
0.57159549
0.57172352
0.57194746
0.57205236
0.57213819
0.57234925
0.57265145
0.57298011
0.57321876
0.57342923
0.57364231
0.57388091
0.57410097
0.57428092
0.57445949
0.57456267
0.57473117
0.57495081
0.57504284
0.57518196
0.57528418
0.57527405
0.57543844
INFO - Training [31][   80/  196]   Loss 1.303803   Top1 55.874023   Top5 92.885742   BatchTime 0.353122   LR 0.001246
0.57550138
0.57555348
0.57549208
0.57541567
0.57540762
0.57524824
0.57517231
0.57508898
0.57501107
0.57503355
0.57506275
0.57507640
0.57509977
0.57502764
0.57500321
0.57482195
0.57473236
INFO - Training [31][  100/  196]   Loss 1.294503   Top1 56.238281   Top5 93.050781   BatchTime 0.353591   LR 0.001246
0.57449901
0.57443005
0.57420605
0.57401419
0.57378387
0.57364517
0.57356310
0.57349879
0.57345748
0.57330441
0.57333690
0.57311952
0.57290387
0.57277453
0.57281125
0.57282335
0.57290918
0.57303256
0.57326365
0.57345164
0.57354367
0.57351607
0.57355624
INFO - Training [31][  120/  196]   Loss 1.293153   Top1 56.328125   Top5 93.141276   BatchTime 0.352720   LR 0.001245
0.57357991
0.57363981
0.57364565
0.57371050
0.57370955
0.57379794
0.57390332
0.57396024
0.57402700
0.57402217
0.57406986
0.57411665
0.57416391
0.57400775
0.57383543
0.57364273
0.57346392
INFO - Training [31][  140/  196]   Loss 1.293267   Top1 56.280692   Top5 93.150112   BatchTime 0.353174   LR 0.001244
0.57335103
0.57330024
0.57325810
0.57316220
0.57310539
0.57305652
0.57295501
0.57285088
0.57264656
0.57255411
0.57246387
0.57232356
0.57227319
0.57234955
0.57229072
0.57238823
0.57247007
0.57247341
0.57254612
0.57263476
0.57279450
0.57288319
0.57295918
INFO - Training [31][  160/  196]   Loss 1.293154   Top1 56.223145   Top5 93.222656   BatchTime 0.352734   LR 0.001244
0.57303047
0.57309276
0.57308501
0.57315332
0.57313716
0.57304412
0.57301140
0.57294780
0.57276714
0.57270390
0.57264239
0.57258314
0.57260281
0.57252747
0.57250029
0.57250100
0.57262415
0.57259661
0.57254827
0.57250768
0.57246298
INFO - Training [31][  180/  196]   Loss 1.288434   Top1 56.362847   Top5 93.298611   BatchTime 0.354484   LR 0.001243
0.57248515
0.57238579
0.57238156
0.57227749
0.57220471
0.57212269
0.57209307
0.57204527
0.57203418
0.57195598
0.57194835
INFO - ==> Top1: 56.480    Top5: 93.352    Loss: 1.284
0.57189715
0.57187843
0.57180732
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [31][   20/   40]   Loss 1.007875   Top1 64.707031   Top5 96.757812   BatchTime 0.113347
INFO - Validation [31][   40/   40]   Loss 1.010022   Top1 64.710000   Top5 96.920000   BatchTime 0.084145
INFO - ==> Top1: 64.710    Top5: 96.920    Loss: 1.010
INFO - ==> Sparsity : 0.643
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 76.860   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 76.540   Top5: 97.960]
INFO - Scoreboard best 3 ==> Epoch [31][Top1: 64.710   Top5: 96.920]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-093040/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.3965)
features.1.conv.0 tensor(0.0443)
features.1.conv.3 tensor(0.0926)
features.1.conv.6 tensor(0.0668)
features.2.conv.0 tensor(0.0486)
features.2.conv.3 tensor(0.0802)
features.2.conv.6 tensor(0.0966)
features.3.conv.0 tensor(0.0498)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.0579)
features.4.conv.0 tensor(0.5024)
features.4.conv.3 tensor(0.1163)
features.4.conv.6 tensor(0.1001)
features.5.conv.0 tensor(0.8416)
features.5.conv.3 tensor(0.0729)
features.5.conv.6 tensor(0.0793)
features.6.conv.0 tensor(0.0386)
features.6.conv.3 tensor(0.0735)
features.6.conv.6 tensor(0.0793)
features.7.conv.0 tensor(0.0853)
features.7.conv.3 tensor(0.1328)
features.7.conv.6 tensor(0.8433)
features.8.conv.0 tensor(0.9084)
features.8.conv.3 tensor(0.1386)
features.8.conv.6 tensor(0.9641)
features.9.conv.0 tensor(0.0724)
features.9.conv.3 tensor(0.1591)
features.9.conv.6 tensor(0.9104)
features.10.conv.0 tensor(0.8872)
features.10.conv.3 tensor(0.3053)
features.10.conv.6 tensor(0.8222)
features.11.conv.0 tensor(0.1278)
features.11.conv.3 tensor(0.1873)
features.11.conv.6 tensor(0.2472)
features.12.conv.0 tensor(0.1017)
features.12.conv.3 tensor(0.1595)
features.12.conv.6 tensor(0.3231)
features.13.conv.0 tensor(0.1107)
features.13.conv.3 tensor(0.1725)
features.13.conv.6 tensor(0.5683)
features.14.conv.0 tensor(0.8201)
features.14.conv.3 tensor(0.1010)
features.14.conv.6 tensor(0.9412)
features.15.conv.0 tensor(0.7431)
features.15.conv.3 tensor(0.0927)
features.15.conv.6 tensor(0.9802)
features.16.conv.0 tensor(0.5531)
features.16.conv.3 tensor(0.0878)
features.16.conv.6 tensor(0.6734)
conv.0 tensor(0.7629)
tensor(1407850.) 2188896.0
0.57176399
0.57162803
0.57157761
0.57161230
0.57155949
0.57147527
0.57151520
0.57166249
0.57183725
0.57195181
0.57208085
0.57217574
0.57231724
0.57238716
0.57253450
0.57276738
0.57297081
0.57322186
0.57351071
0.57366306
0.57375634
0.57380676
0.57394397
INFO - Training [32][   20/  196]   Loss 1.268999   Top1 56.601562   Top5 93.320312   BatchTime 0.421198   LR 0.001242
0.57392836
0.57387900
0.57378185
0.57369107
0.57357955
0.57354736
0.57345098
0.57340115
0.57339203
0.57340086
0.57330024
0.57328945
0.57324207
0.57330185
0.57318383
0.57324147
0.57318646
0.57321244
0.57321191
0.57320076
INFO - Training [32][   40/  196]   Loss 1.291605   Top1 55.585938   Top5 93.388672   BatchTime 0.367428   LR 0.001241
0.57318246
0.57320863
0.57324678
0.57322133
0.57317251
0.57296574
0.57281035
0.57263291
0.57243168
0.57222694
0.57205671
0.57193625
0.57187474
0.57184130
0.57188785
0.57179195
0.57173842
0.57164675
0.57150269
INFO - Training [32][   60/  196]   Loss 1.281183   Top1 56.269531   Top5 93.411458   BatchTime 0.343625   LR 0.001240
0.57146537
0.57155567
0.57159406
0.57168156
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    main()
  File "main_slsq.py", line 77, in main
    trainer.train_qat_slsq(train_loader, val_loader, test_loader,qat_model, teacher_model,criterion,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 187, in train_one_epoch_slsq
    optimizer.step()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/adamw.py", line 162, in step
    adamw(params_with_grad,
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/adamw.py", line 219, in adamw
    func(params,
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/adamw.py", line 316, in _single_tensor_adamw
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt