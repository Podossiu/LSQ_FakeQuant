
Files already downloaded and verified
Files already downloaded and verified
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.0005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.0005
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.00000000
0.95131010
0.88827842
0.89571798
0.95947737
0.96069562
0.95708615
0.95144790
0.95215470
0.95123470
0.95117146
INFO - Training [0][   20/  196]   Loss 1.748031   Top1 39.277344   Top5 83.906250   BatchTime 0.497655   LR 0.000500
0.95076817
0.95009333
0.94946808
0.94791389
0.94704986
0.94543254
0.94341904
0.94213015
0.93971699
0.93789506
0.93600667
0.93441963
0.93309551
0.93153197
0.92953002
0.92781585
0.92612839
0.92466241
0.92332894
INFO - Training [0][   40/  196]   Loss 1.679672   Top1 41.171875   Top5 85.468750   BatchTime 0.458632   LR 0.000500
0.92200297
0.92052633
0.91986150
0.91911715
0.91814679
0.91732258
0.91736680
0.91784996
0.92079270
0.92066765
0.92005724
0.91939110
0.91896278
0.91839564
0.91799176
0.91748732
0.91684014
0.91517043
0.91559696
0.91484398
0.91426581
0.91371018
0.91300708
0.91235024
INFO - Training [0][   60/  196]   Loss 1.594833   Top1 44.140625   Top5 87.233073   BatchTime 0.447867   LR 0.000499
0.91191494
0.91154927
0.91108894
0.91088104
0.90869206
0.90961802
0.90877908
0.90819794
0.90717083
0.90639198
0.90529472
0.90389037
0.90303755
0.90239662
0.90097743
0.89996707
0.89874917
0.89859647
0.89760768
INFO - Training [0][   80/  196]   Loss 1.532244   Top1 46.435547   Top5 88.266602   BatchTime 0.439321   LR 0.000498
0.89667666
0.89593083
0.89590400
0.89492530
0.89394552
0.89323610
0.89306158
0.89294952
0.89290226
0.89263040
0.89235449
0.89198929
0.89167291
0.89128399
0.89104956
0.89107341
0.89083338
0.89044446
INFO - Training [0][  100/  196]   Loss 1.477511   Top1 48.445312   Top5 89.269531   BatchTime 0.420043   LR 0.000497
0.88887852
0.88791835
0.88711184
0.88690937
0.88637912
0.88414246
0.88203430
0.88119322
0.88334531
0.88196260
0.88132548
0.88111490
0.88080192
0.88083768
0.88042355
0.88050538
0.88061863
0.88059521
0.88088375
0.88125163
0.88185376
0.88118255
INFO - Training [0][  120/  196]   Loss 1.430346   Top1 50.260417   Top5 89.983724   BatchTime 0.410721   LR 0.000495
0.87951928
0.87782663
0.87591392
0.87417066
0.87234992
0.87069803
0.86898124
0.86859429
0.86735421
0.86644667
0.86609268
0.86528885
0.86470842
0.86364800
0.86328596
0.86211306
0.86150205
0.86081541
0.85988754
INFO - Training [0][  140/  196]   Loss 1.401348   Top1 51.289062   Top5 90.438058   BatchTime 0.411027   LR 0.000494
0.85935926
0.85824281
0.85600334
0.85368401
0.84894842
0.84826255
0.84686279
0.84589726
0.84464443
0.84378052
0.84238422
0.84240109
0.84271771
0.84224975
0.84191304
0.84198868
0.84169567
0.84153378
INFO - Training [0][  160/  196]   Loss 1.379442   Top1 52.036133   Top5 90.771484   BatchTime 0.415515   LR 0.000492
0.84131408
0.84115589
0.84058100
0.84059918
0.83978921
0.83957112
0.83908397
0.83824939
0.83736163
0.83683634
0.83647346
0.83627766
0.83558434
0.83487844
0.83458620
0.83490574
0.83620077
0.83590370
0.83609670
0.83630168
0.83784497
0.84306782
0.84496242
0.84483868
INFO - Training [0][  180/  196]   Loss 1.357036   Top1 52.801649   Top5 91.082899   BatchTime 0.415026   LR 0.000490
0.84478319
0.84458083
0.84431273
0.84389430
0.84331095
0.84304535
0.84250212
0.84239972
0.84162891
0.84100646
0.84026408
0.83976227
********************pre-trained*****************
INFO - ==> Top1: 53.478    Top5: 91.308    Loss: 1.338
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [0][   20/   40]   Loss 0.906012   Top1 68.945312   Top5 96.855469   BatchTime 0.120737
INFO - Validation [0][   40/   40]   Loss 0.904762   Top1 68.840000   Top5 97.040000   BatchTime 0.087876
features.0.conv.0 tensor(0.3472)
features.0.conv.3 tensor(0.1035)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0760)
features.2.conv.0 tensor(0.0799)
features.2.conv.3 tensor(0.3310)
features.2.conv.6 tensor(0.1959)
features.3.conv.0 tensor(0.0741)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1018)
features.4.conv.0 tensor(0.0964)
features.4.conv.3 tensor(0.3171)
features.4.conv.6 tensor(0.1680)
features.5.conv.0 tensor(0.3693)
features.5.conv.3 tensor(0.4340)
features.5.conv.6 tensor(0.1090)
features.6.conv.0 tensor(0.0575)
features.6.conv.3 tensor(0.0637)
features.6.conv.6 tensor(0.0854)
features.7.conv.0 tensor(0.1820)
features.7.conv.3 tensor(0.4517)
features.7.conv.6 tensor(0.1934)
features.8.conv.0 tensor(0.4773)
features.8.conv.3 tensor(0.5396)
features.8.conv.6 tensor(0.1263)
features.9.conv.0 tensor(0.4205)
features.9.conv.3 tensor(0.5495)
features.9.conv.6 tensor(0.1352)
features.10.conv.0 tensor(0.0907)
features.10.conv.3 tensor(0.1123)
features.10.conv.6 tensor(0.1079)
features.11.conv.0 tensor(0.4175)
features.11.conv.3 tensor(0.6470)
features.11.conv.6 tensor(0.1765)
features.12.conv.0 tensor(0.4861)
features.12.conv.3 tensor(0.6802)
features.12.conv.6 tensor(0.7253)
features.13.conv.0 tensor(0.3120)
features.13.conv.3 tensor(0.4973)
features.13.conv.6 tensor(0.0882)
features.14.conv.0 tensor(0.6725)
features.14.conv.3 tensor(0.8822)
features.14.conv.6 tensor(0.7262)
features.15.conv.0 tensor(0.6451)
features.15.conv.3 tensor(0.8888)
features.15.conv.6 tensor(0.9116)
features.16.conv.0 tensor(0.4919)
features.16.conv.3 tensor(0.8222)
features.16.conv.6 tensor(0.0516)
conv.0 tensor(0.0465)
tensor(781204.) 2188896.0
INFO - ==> Top1: 68.840    Top5: 97.040    Loss: 0.905
INFO - ==> Sparsity : 0.357
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 68.840   Top5: 97.040]
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.83882946
0.83785379
0.83654392
0.83558142
0.83472598
0.83371216
0.83351463
0.83314562
0.83265883
0.83163720
0.82994705
0.82809478
0.82635540
0.82534939
0.82554108
0.82575661
0.82526833
0.82436359
0.82360691
0.82313067
INFO - Training [1][   20/  196]   Loss 1.131047   Top1 61.289062   Top5 94.218750   BatchTime 0.432736   LR 0.000485
0.82305920
0.82305771
0.82306057
0.82267964
0.82256943
0.82303357
0.82261044
0.82253599
0.82259518
0.82186675
0.82147837
0.82177454
0.82169574
0.82143390
0.82072204
0.82091850
0.82064366
0.82069737
0.81958157
0.81901556
0.81918943
0.81882942
0.81909931
INFO - Training [1][   40/  196]   Loss 1.117619   Top1 61.992188   Top5 94.238281   BatchTime 0.393802   LR 0.000482
0.82039946
0.82341754
0.82260191
0.82241631
0.82246411
0.82199001
0.82141006
0.82118291
0.82152677
0.82125711
0.82064998
0.82015508
0.82007873
0.81988984
0.81974971
0.81944519
0.81872815
0.81757569
0.81801927
INFO - Training [1][   60/  196]   Loss 1.108429   Top1 62.037760   Top5 94.218750   BatchTime 0.368161   LR 0.000479
0.82199085
0.83925986
0.83996648
0.83980626
0.83986551
0.83894742
0.83829725
0.83928335
0.83906895
0.83874261
0.83860070
0.83956766
0.83940488
0.83948171
0.83939373
0.83917344
0.83883554
0.83836597
0.83760095
0.83694637
0.83674735
0.83695084
INFO - Training [1][   80/  196]   Loss 1.102478   Top1 62.080078   Top5 94.350586   BatchTime 0.365726   LR 0.000476
0.83669919
0.83636349
0.83596438
0.83511460
0.83443290
0.83432895
0.83380115
0.83358973
0.83304363
0.83266157
0.83232421
0.83193493
0.83162218
0.83081663
0.83006066
0.82956207
INFO - Training [1][  100/  196]   Loss 1.103151   Top1 61.925781   Top5 94.062500   BatchTime 0.366056   LR 0.000473
0.82904702
0.82835948
0.82794827
0.82721901
0.82682377
0.82666010
0.82635671
0.82620686
0.82608795
0.82580185
0.82534337
0.82506245
0.82511127
0.82479447
0.82512665
0.82463288
0.82427734
0.82578856
0.82783985
0.84177268
0.84260702
INFO - Training [1][  120/  196]   Loss 1.084860   Top1 62.539062   Top5 94.355469   BatchTime 0.369671   LR 0.000469
0.84181112
0.84117776
0.84038043
0.83977276
0.83889860
0.83837670
0.83809274
0.83752471
0.83742982
0.83682138
0.83677715
0.83705193
0.83686972
0.83669001
0.83658409
0.83660942
0.83645743
0.83673096
0.83709055
0.83772606
0.84044510
INFO - Training [1][  140/  196]   Loss 1.077470   Top1 62.896205   Top5 94.483817   BatchTime 0.370219   LR 0.000465
0.84591722
0.84641886
0.84671748
0.84699297
0.84668028
0.84637320
0.84654528
0.84642583
0.84618860
0.84618640
0.84619766
0.84615672
0.84594244
0.84588879
0.84587604
0.84556627
0.84533232
0.84477210
0.84423250
0.84425825
0.84427160
0.84435529
INFO - Training [1][  160/  196]   Loss 1.070694   Top1 63.117676   Top5 94.519043   BatchTime 0.370915   LR 0.000460
0.84436584
0.84410071
0.84444290
0.84392381
0.84403002
0.84361178
0.84372759
0.84359974
0.84329659
0.84305215
0.84306300
0.84304720
0.84345639
0.84365219
0.84392875
0.84433013
INFO - Training [1][  180/  196]   Loss 1.058265   Top1 63.487413   Top5 94.557292   BatchTime 0.370279   LR 0.000456
0.84466326
0.84475285
0.84467828
0.84456450
0.84351188
0.84461272
0.84477985
0.84498084
0.84496522
0.84492546
0.84491014
0.84446579
0.84472984
0.84485471
0.84457868
0.84461063
********************pre-trained*****************
INFO - ==> Top1: 63.710    Top5: 94.618    Loss: 1.053
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.836607   Top1 71.640625   Top5 97.617188   BatchTime 0.124102
features.0.conv.0 tensor(0.3924)
features.0.conv.3 tensor(0.0977)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0734)
features.2.conv.0 tensor(0.0619)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.2384)
features.3.conv.0 tensor(0.0706)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.0996)
features.4.conv.0 tensor(0.0734)
features.4.conv.3 tensor(0.3096)
features.4.conv.6 tensor(0.1569)
features.5.conv.0 tensor(0.3486)
features.5.conv.3 tensor(0.4363)
features.5.conv.6 tensor(0.1074)
features.6.conv.0 tensor(0.0549)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0819)
features.7.conv.0 tensor(0.1285)
features.7.conv.3 tensor(0.4525)
features.7.conv.6 tensor(0.1892)
features.8.conv.0 tensor(0.3009)
features.8.conv.3 tensor(0.5356)
features.8.conv.6 tensor(0.1191)
features.9.conv.0 tensor(0.4252)
features.9.conv.3 tensor(0.5535)
features.9.conv.6 tensor(0.1285)
features.10.conv.0 tensor(0.0724)
features.10.conv.3 tensor(0.1105)
features.10.conv.6 tensor(0.1077)
features.11.conv.0 tensor(0.4380)
features.11.conv.3 tensor(0.6510)
features.11.conv.6 tensor(0.1816)
features.12.conv.0 tensor(0.4520)
features.12.conv.3 tensor(0.6669)
features.12.conv.6 tensor(0.7977)
features.13.conv.0 tensor(0.3079)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.0902)
features.14.conv.0 tensor(0.6743)
features.14.conv.3 tensor(0.8839)
features.14.conv.6 tensor(0.7951)
features.15.conv.0 tensor(0.3766)
features.15.conv.3 tensor(0.8950)
features.15.conv.6 tensor(0.9544)
features.16.conv.0 tensor(0.4122)
features.16.conv.3 tensor(0.8051)
features.16.conv.6 tensor(0.0683)
conv.0 tensor(0.0557)
tensor(750596.) 2188896.0
INFO - Validation [1][   40/   40]   Loss 0.849961   Top1 70.580000   Top5 97.810000   BatchTime 0.092352
INFO - ==> Top1: 70.580    Top5: 97.810    Loss: 0.850
INFO - ==> Sparsity : 0.343
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 70.580   Top5: 97.810]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 68.840   Top5: 97.040]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.84622335
0.84644377
0.84660578
0.84654576
0.84640729
0.84619200
0.84625810
0.84619087
0.84580564
0.84593713
0.84583288
0.84541476
0.84536827
0.84504420
0.84531552
0.84543371
0.84467953
0.84497285
0.84491241
0.84476984
0.84407282
0.84426802
0.84421235
0.84413046
0.84428930
INFO - Training [2][   20/  196]   Loss 1.018253   Top1 64.765625   Top5 94.609375   BatchTime 0.401732   LR 0.000448
0.84418452
0.84386498
0.84406340
0.84371269
0.84363502
0.84382045
0.84340197
0.84331471
0.84290469
0.84241408
0.84220195
0.84182346
0.84159046
0.84122962
0.84089404
0.84056431
0.84010923
INFO - Training [2][   40/  196]   Loss 0.996852   Top1 66.025391   Top5 94.824219   BatchTime 0.371495   LR 0.000443
0.83986843
0.83955467
0.83921772
0.83933020
0.83917052
0.83873540
0.83875591
0.83882540
0.83858335
0.83790845
0.83760083
0.83733356
0.83679599
0.83628517
0.83573884
0.83528125
0.83466494
0.83420920
0.83385903
0.83365005
0.83305043
INFO - Training [2][   60/  196]   Loss 0.972672   Top1 66.731771   Top5 95.169271   BatchTime 0.370608   LR 0.000437
0.83301693
0.83260208
0.83236426
0.83208239
0.83220929
0.83174664
0.83119315
0.83115059
0.83121264
0.83077264
0.83106560
0.83137387
0.83143669
0.83075786
0.83025223
0.83000386
0.82964313
0.82925653
0.82858139
0.82837886
0.82804823
INFO - Training [2][   80/  196]   Loss 0.956142   Top1 67.148438   Top5 95.468750   BatchTime 0.375129   LR 0.000432
0.82753974
0.82697845
0.82697529
0.82703739
0.82695997
0.82660943
0.82613117
0.82566482
0.82528365
0.82467538
0.82401556
0.82308656
0.82117426
0.82058042
0.82009047
0.81950349
0.81882018
INFO - Training [2][  100/  196]   Loss 0.944650   Top1 67.589844   Top5 95.585938   BatchTime 0.372132   LR 0.000426
0.81845057
0.81763405
0.81711185
0.81645310
0.81594121
0.81523401
0.81468254
0.81391495
0.81336921
0.81296021
0.81259316
0.81239802
0.81212151
0.81188178
0.81106931
0.81096572
0.81059939
0.81045264
0.81090659
0.81091607
0.81120718
0.81161004
INFO - Training [2][  120/  196]   Loss 0.939003   Top1 67.819010   Top5 95.729167   BatchTime 0.370582   LR 0.000421
0.81170946
0.81207341
0.81240588
0.81288868
0.81510174
0.82214123
0.82252312
0.82253063
0.82245880
0.82280296
0.82283688
0.82238072
0.82204413
0.82212126
0.82168496
0.82134283
0.82166529
0.82183677
0.82142866
0.82144898
0.82532561
0.82712859
INFO - Training [2][  140/  196]   Loss 0.938102   Top1 67.910156   Top5 95.792411   BatchTime 0.370396   LR 0.000415
0.82717931
0.82674545
0.82699865
0.82730621
0.82718325
0.82694703
0.82713932
0.82705206
0.82697266
0.82683325
0.82699394
0.82684517
0.82693112
0.82735080
0.82747585
0.82733262
INFO - Training [2][  160/  196]   Loss 0.938531   Top1 67.810059   Top5 95.761719   BatchTime 0.369383   LR 0.000409
0.82799155
0.82774401
0.82720429
0.82715154
0.82705390
0.82717377
0.82687646
0.82607830
0.82485539
0.82340997
0.82294542
0.82294405
0.82294357
0.82268304
0.82219934
0.82197064
0.82191044
0.82147026
0.82103515
0.82048118
0.82038862
0.82016426
INFO - Training [2][  180/  196]   Loss 0.932761   Top1 68.016493   Top5 95.705295   BatchTime 0.369511   LR 0.000402
0.82029611
0.82082731
0.82061088
0.82073963
0.82058179
0.82025111
0.82027930
0.82020092
0.81984371
0.81987584
0.81995028
0.82026023
0.82054430
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 68.146    Top5: 95.758    Loss: 0.929
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [2][   20/   40]   Loss 0.743746   Top1 74.707031   Top5 98.164062   BatchTime 0.126015
features.0.conv.0 tensor(0.4444)
features.0.conv.3 tensor(0.1133)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0825)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1759)
features.3.conv.0 tensor(0.0755)
features.3.conv.3 tensor(0.0887)
features.3.conv.6 tensor(0.1026)
features.4.conv.0 tensor(0.0622)
features.4.conv.3 tensor(0.3154)
features.4.conv.6 tensor(0.1626)
features.5.conv.0 tensor(0.3273)
features.5.conv.3 tensor(0.4369)
features.5.conv.6 tensor(0.1045)
features.6.conv.0 tensor(0.0550)
features.6.conv.3 tensor(0.0608)
features.6.conv.6 tensor(0.0865)
features.7.conv.0 tensor(0.1107)
features.7.conv.3 tensor(0.4488)
features.7.conv.6 tensor(0.1981)
features.8.conv.0 tensor(0.2517)
features.8.conv.3 tensor(0.5365)
features.8.conv.6 tensor(0.1342)
features.9.conv.0 tensor(0.4237)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.1324)
features.10.conv.0 tensor(0.0820)
features.10.conv.3 tensor(0.1102)
features.10.conv.6 tensor(0.1061)
features.11.conv.0 tensor(0.4380)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.3248)
features.12.conv.0 tensor(0.4092)
features.12.conv.3 tensor(0.6661)
features.12.conv.6 tensor(0.8050)
features.13.conv.0 tensor(0.3149)
features.13.conv.3 tensor(0.4882)
features.13.conv.6 tensor(0.0933)
features.14.conv.0 tensor(0.6619)
features.14.conv.3 tensor(0.8826)
features.14.conv.6 tensor(0.8033)
features.15.conv.0 tensor(0.6684)
features.15.conv.3 tensor(0.8978)
features.15.conv.6 tensor(0.9617)
features.16.conv.0 tensor(0.4304)
features.16.conv.3 tensor(0.8039)
features.16.conv.6 tensor(0.0732)
conv.0 tensor(0.0616)
tensor(808075.) 2188896.0
INFO - Validation [2][   40/   40]   Loss 0.738469   Top1 74.640000   Top5 98.370000   BatchTime 0.095720
INFO - ==> Top1: 74.640    Top5: 98.370    Loss: 0.738
INFO - ==> Sparsity : 0.369
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 74.640   Top5: 98.370]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 70.580   Top5: 97.810]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 68.840   Top5: 97.040]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.82098466
0.82085663
0.82088077
0.82080066
0.82137030
0.82172555
0.82212877
0.82284033
0.82327777
0.82339859
0.82339889
0.82349741
0.82347828
0.82347876
0.82322598
0.82301652
0.82321864
0.82359749
0.82369781
0.82429653
0.82529807
0.82536042
0.82544452
INFO - Training [3][   20/  196]   Loss 0.901661   Top1 68.847656   Top5 95.468750   BatchTime 0.462366   LR 0.000391
0.82579309
0.82585651
0.82600397
0.82571083
0.82575923
0.82537955
0.82567990
0.82586545
0.82559019
0.82603061
0.82600987
0.82453400
0.82538372
0.82543147
0.82516706
0.82479686
0.82488960
0.82479358
0.82473373
0.82464331
0.82424217
INFO - Training [3][   40/  196]   Loss 0.891675   Top1 69.667969   Top5 95.634766   BatchTime 0.420978   LR 0.000384
0.82400346
0.82353997
0.82326686
0.82285547
0.82245314
0.82238716
0.82213795
0.82210290
0.82167053
0.82105166
0.82022399
0.81994551
0.82032114
0.82025033
0.81990469
0.81986785
INFO - Training [3][   60/  196]   Loss 0.880395   Top1 69.941406   Top5 95.872396   BatchTime 0.406044   LR 0.000377
0.81971675
0.81942791
0.81932074
0.81948239
0.81931025
0.81960040
0.81937420
0.81941032
0.81968731
0.82055449
0.82326925
0.82329816
0.82276803
0.82283390
0.82314181
0.82310051
0.82274991
0.82269663
0.82194072
0.82170814
0.82138908
0.82111764
INFO - Training [3][   80/  196]   Loss 0.873330   Top1 70.185547   Top5 96.064453   BatchTime 0.396542   LR 0.000370
0.82046264
0.82014328
0.81991065
0.81949407
0.81911588
0.81841087
0.81801319
0.81732166
0.81636941
0.81561053
0.81505638
0.81431657
0.81392914
0.81282669
0.81225359
0.81233931
0.81190008
0.81158304
0.81135476
0.81098109
0.81083804
0.81056511
INFO - Training [3][  100/  196]   Loss 0.860913   Top1 70.562500   Top5 96.226562   BatchTime 0.390412   LR 0.000363
0.81055778
0.81047130
0.81014949
0.80988109
0.80965728
0.80919933
0.80925727
0.80864894
0.80868256
0.80828220
0.80776215
0.80790401
0.80724144
0.80682158
0.80750686
0.80844122
INFO - Training [3][  120/  196]   Loss 0.857946   Top1 70.651042   Top5 96.321615   BatchTime 0.386391   LR 0.000356
0.80875826
0.80853212
0.80844730
0.80866098
0.80863863
0.80854201
0.80838084
0.80830753
0.80838537
0.80859059
0.80891663
0.80923647
0.80950093
0.81031173
0.81086338
0.81813914
0.82113028
0.82173151
0.82251072
0.82270789
0.82276028
0.82312256
0.82311577
0.82295269
INFO - Training [3][  140/  196]   Loss 0.853353   Top1 70.742188   Top5 96.369978   BatchTime 0.380763   LR 0.000348
0.82278973
0.82291234
0.82298416
0.82281393
0.82263392
0.82279110
0.82269984
0.82269067
0.82291538
0.82264131
0.82244515
0.82208347
0.82187611
0.82189560
0.82197404
0.82164645
0.82159859
INFO - Training [3][  160/  196]   Loss 0.852031   Top1 70.732422   Top5 96.372070   BatchTime 0.375642   LR 0.000341
0.82138067
0.82123351
0.82085788
0.82079828
0.82079148
0.82053953
0.82052344
0.82061040
0.82071233
0.82071912
0.82095951
0.82069856
0.82075632
0.82085949
0.82087773
0.82073450
0.82016921
0.82006806
0.81995589
0.81957710
0.81953067
0.81953722
0.81949902
INFO - Training [3][  180/  196]   Loss 0.846999   Top1 70.857205   Top5 96.371528   BatchTime 0.372490   LR 0.000333
0.81941372
0.81957406
0.81931943
0.81921792
0.81912297
0.81871212
0.81853068
0.81831700
0.81812388
0.81788963
0.81738728
0.81670469
********************pre-trained*****************
INFO - ==> Top1: 70.884    Top5: 96.374    Loss: 0.846
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [3][   20/   40]   Loss 0.627357   Top1 79.257812   Top5 98.339844   BatchTime 0.126360
features.0.conv.0 tensor(0.4792)
features.0.conv.3 tensor(0.1191)
features.1.conv.0 tensor(0.0605)
features.1.conv.3 tensor(0.0764)
features.1.conv.6 tensor(0.0790)
features.2.conv.0 tensor(0.0660)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.1745)
features.3.conv.0 tensor(0.0723)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1063)
features.4.conv.0 tensor(0.0420)
features.4.conv.3 tensor(0.3171)
features.4.conv.6 tensor(0.1571)
features.5.conv.0 tensor(0.3247)
features.5.conv.3 tensor(0.4387)
features.5.conv.6 tensor(0.0988)
features.6.conv.0 tensor(0.0537)
features.6.conv.3 tensor(0.0608)
features.6.conv.6 tensor(0.0852)
features.7.conv.0 tensor(0.1071)
features.7.conv.3 tensor(0.4442)
features.7.conv.6 tensor(0.2017)
features.8.conv.0 tensor(0.2482)
features.8.conv.3 tensor(0.5391)
features.8.conv.6 tensor(0.1369)
features.9.conv.0 tensor(0.2870)
features.9.conv.3 tensor(0.5420)
features.9.conv.6 tensor(0.1507)
features.10.conv.0 tensor(0.0789)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.1046)
features.11.conv.0 tensor(0.4472)
features.11.conv.3 tensor(0.6499)
features.11.conv.6 tensor(0.1904)
features.12.conv.0 tensor(0.4167)
features.12.conv.3 tensor(0.6661)
features.12.conv.6 tensor(0.8123)
features.13.conv.0 tensor(0.3140)
features.13.conv.3 tensor(0.4936)
features.13.conv.6 tensor(0.0936)
features.14.conv.0 tensor(0.6659)
features.14.conv.3 tensor(0.8818)
features.14.conv.6 tensor(0.8018)
features.15.conv.0 tensor(0.6753)
features.15.conv.3 tensor(0.9000)
features.15.conv.6 tensor(0.9632)
features.16.conv.0 tensor(0.5776)
features.16.conv.3 tensor(0.8052)
features.16.conv.6 tensor(0.0767)
conv.0 tensor(0.0679)
tensor(826668.) 2188896.0
INFO - Validation [3][   40/   40]   Loss 0.628435   Top1 78.940000   Top5 98.400000   BatchTime 0.090827
INFO - ==> Top1: 78.940    Top5: 98.400    Loss: 0.628
INFO - ==> Sparsity : 0.378
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 78.940   Top5: 98.400]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 74.640   Top5: 98.370]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 70.580   Top5: 97.810]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.81664228
0.81636024
0.81527573
0.81465989
0.81441760
0.81434917
0.81520087
0.81480497
0.81464082
0.81451857
0.81427509
0.81394017
0.81355482
0.81335908
0.81323409
0.81308150
0.81266099
0.81239206
0.81216866
0.81187254
0.81205904
0.81193405
0.81172329
0.81179559
INFO - Training [4][   20/  196]   Loss 0.816540   Top1 71.210938   Top5 96.367188   BatchTime 0.436953   LR 0.000320
0.81198663
0.81206006
0.81211680
0.81166595
0.81140190
0.81115806
0.81116170
0.81116444
0.81126606
0.81145495
0.81144065
0.81136006
0.81145841
0.81186289
0.81192493
0.81186658
0.81206667
INFO - Training [4][   40/  196]   Loss 0.804411   Top1 72.011719   Top5 96.630859   BatchTime 0.402405   LR 0.000312
0.81192529
0.81346148
0.82282704
0.82782310
0.82744247
0.82733381
0.82759273
0.82789433
0.82805073
0.82820076
0.82832456
0.82823265
0.82826036
0.82844740
0.82845253
0.82820547
0.82803881
0.82801461
0.82783002
0.82758719
0.82744813
INFO - Training [4][   60/  196]   Loss 0.805562   Top1 72.076823   Top5 96.803385   BatchTime 0.391550   LR 0.000304
0.82730728
0.82738149
0.82710499
0.82725650
0.82704121
0.82691640
0.82720995
0.82729274
0.82721376
0.82721978
0.82724845
0.82733691
0.82745677
0.82745153
0.82699627
0.82678795
0.82670110
0.82641065
0.82558358
0.82489586
0.82589823
0.82630885
INFO - Training [4][   80/  196]   Loss 0.804220   Top1 72.031250   Top5 96.855469   BatchTime 0.384360   LR 0.000296
0.82681781
0.82675809
0.82641333
0.82689339
0.82648289
0.82619649
0.82637197
0.82612020
0.82607305
0.82598042
0.82559639
0.82535785
0.82517970
0.82467496
0.82459331
0.82461768
0.82448727
0.82451540
0.82430899
0.82376403
INFO - Training [4][  100/  196]   Loss 0.791124   Top1 72.652344   Top5 96.902344   BatchTime 0.384493   LR 0.000289
0.82326484
0.82251179
0.82197601
0.82128054
0.82071483
0.82030237
0.82003504
0.81975532
0.81960613
0.81967807
0.81988990
0.81933814
0.81981391
0.81934023
0.81917870
0.81929117
0.81934470
0.81983870
0.81961113
0.81968546
INFO - Training [4][  120/  196]   Loss 0.782192   Top1 73.040365   Top5 96.940104   BatchTime 0.388275   LR 0.000281
0.81988770
0.81977177
0.81984764
0.81999034
0.81960374
0.81866544
0.81813174
0.81797487
0.81799531
0.81823295
0.81970495
0.81928051
0.81933618
0.81908691
0.81906319
0.81899977
0.81867743
0.81828856
INFO - Training [4][  140/  196]   Loss 0.780273   Top1 73.205915   Top5 96.953125   BatchTime 0.396363   LR 0.000273
0.81843960
0.81775641
0.81751239
0.81721401
0.81680286
0.81707621
0.81672400
0.81648523
0.81665665
0.81683499
0.81698149
0.81678808
0.81650692
0.81645459
0.81671029
0.81717795
0.81822556
0.82666487
0.82678664
INFO - Training [4][  160/  196]   Loss 0.780411   Top1 73.183594   Top5 96.921387   BatchTime 0.399447   LR 0.000265
0.82679206
0.82647717
0.82641131
0.82626629
0.82603627
0.82605791
0.82597309
0.82598877
0.82584363
0.82596815
0.82619768
0.82606274
0.82597500
0.82605690
0.82581830
0.82585084
0.82598913
0.82597148
0.82603031
0.82609618
0.82563448
INFO - Training [4][  180/  196]   Loss 0.775821   Top1 73.235677   Top5 96.905382   BatchTime 0.398609   LR 0.000257
0.82532930
0.82528543
0.82538521
0.82552665
0.82560688
0.82545048
0.82564723
0.82581228
0.82571137
0.82539016
0.82539642
0.82532817
0.82506943
0.82501185
********************pre-trained*****************
validation quantized model on cpu
INFO - ==> Top1: 73.348    Top5: 96.890    Loss: 0.773
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [4][   20/   40]   Loss 0.544815   Top1 82.128906   Top5 98.808594   BatchTime 0.129748
features.0.conv.0 tensor(0.4965)
features.0.conv.3 tensor(0.1055)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0720)
features.2.conv.0 tensor(0.0654)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.1626)
features.3.conv.0 tensor(0.0654)
features.3.conv.3 tensor(0.0926)
features.3.conv.6 tensor(0.1044)
features.4.conv.0 tensor(0.0425)
features.4.conv.3 tensor(0.3177)
features.4.conv.6 tensor(0.1636)
features.5.conv.0 tensor(0.3228)
features.5.conv.3 tensor(0.4363)
features.5.conv.6 tensor(0.0985)
features.6.conv.0 tensor(0.0555)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0867)
features.7.conv.0 tensor(0.1034)
features.7.conv.3 tensor(0.4433)
features.7.conv.6 tensor(0.2064)
features.8.conv.0 tensor(0.2601)
features.8.conv.3 tensor(0.5394)
features.8.conv.6 tensor(0.1372)
features.9.conv.0 tensor(0.2914)
features.9.conv.3 tensor(0.5431)
features.9.conv.6 tensor(0.1602)
features.10.conv.0 tensor(0.0677)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.1065)
features.11.conv.0 tensor(0.4679)
features.11.conv.3 tensor(0.6547)
features.11.conv.6 tensor(0.2023)
features.12.conv.0 tensor(0.4091)
features.12.conv.3 tensor(0.6657)
features.12.conv.6 tensor(0.7940)
features.13.conv.0 tensor(0.3148)
features.13.conv.3 tensor(0.4931)
features.13.conv.6 tensor(0.0947)
features.14.conv.0 tensor(0.6742)
features.14.conv.3 tensor(0.8813)
features.14.conv.6 tensor(0.8152)
features.15.conv.0 tensor(0.7045)
features.15.conv.3 tensor(0.9005)
features.15.conv.6 tensor(0.9626)
features.16.conv.0 tensor(0.3545)
features.16.conv.3 tensor(0.8057)
features.16.conv.6 tensor(0.0779)
conv.0 tensor(0.0686)
tensor(801709.) 2188896.0
INFO - Validation [4][   40/   40]   Loss 0.538274   Top1 81.960000   Top5 99.020000   BatchTime 0.095433
INFO - ==> Top1: 81.960    Top5: 99.020    Loss: 0.538
INFO - ==> Sparsity : 0.366
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 81.960   Top5: 99.020]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 78.940   Top5: 98.400]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 74.640   Top5: 98.370]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.82486051
0.82492459
0.82476503
0.82443571
0.82382548
0.82375425
0.82365292
0.82341856
0.82336742
0.82309645
0.82273024
0.82282132
0.82320708
0.82320875
0.82418078
0.82420230
0.82417709
0.82390147
0.82394445
0.82372189
0.82352537
0.82313997
0.82276201
INFO - Training [5][   20/  196]   Loss 0.755978   Top1 73.769531   Top5 96.562500   BatchTime 0.536730   LR 0.000242
0.82282597
0.82268703
0.82244343
0.82199407
0.82206100
0.82267863
0.82291275
0.82284641
0.82272691
0.82257581
0.82224548
0.82214868
0.82194746
0.82186711
0.82199073
0.82177991
0.82178330
0.82205737
0.82190233
0.82190311
0.82210946
INFO - Training [5][   40/  196]   Loss 0.766214   Top1 73.564453   Top5 96.572266   BatchTime 0.505214   LR 0.000234
0.82205296
0.82177055
0.82199532
0.82209259
0.82225251
0.82196504
0.82264113
0.82297200
0.82299346
0.82310957
0.82317048
0.82286668
0.82272834
0.82243502
0.82262093
0.82266414
0.82241827
0.82239658
INFO - Training [5][   60/  196]   Loss 0.751283   Top1 74.049479   Top5 96.660156   BatchTime 0.486197   LR 0.000226
0.82215697
0.82181978
0.82165873
0.82158607
0.82147998
0.82151169
0.82143688
0.82124096
0.82107836
0.82111508
0.82110941
0.82101327
0.82076013
0.82028985
0.81998861
0.81975347
0.81965643
0.81952882
0.81947148
0.81974554
0.81943679
0.81936085
INFO - Training [5][   80/  196]   Loss 0.738411   Top1 74.458008   Top5 96.850586   BatchTime 0.477229   LR 0.000218
0.81978828
0.81994945
0.81954771
0.81922066
0.81910378
0.81895727
0.81876880
0.81852847
0.81859833
0.81871372
0.81893432
0.81871396
0.81871045
0.81877774
0.81852406
0.81802100
0.81823671
0.81819588
INFO - Training [5][  100/  196]   Loss 0.730052   Top1 74.722656   Top5 96.976562   BatchTime 0.474152   LR 0.000210
0.81827760
0.81810784
0.81788653
0.81710202
0.81608820
0.81533277
0.81494725
0.81460637
0.81449676
0.81433415
0.81422871
0.81429225
0.81491286
0.81500548
0.81518686
0.81578809
0.81519866
0.81498510
0.81454855
0.81450510
0.81535512
0.81511754
INFO - Training [5][  120/  196]   Loss 0.723373   Top1 74.895833   Top5 97.089844   BatchTime 0.471314   LR 0.000202
0.81488663
0.81429392
0.81383592
0.81349528
0.81345195
0.81333011
0.81313503
0.81321073
0.81315744
0.81300014
0.81254423
0.81204182
0.81163293
0.81108540
0.81092656
0.81073391
INFO - Training [5][  140/  196]   Loss 0.722545   Top1 74.972098   Top5 97.128906   BatchTime 0.457634   LR 0.000195
0.81049591
0.81018907
0.81029266
0.81045109
0.81018776
0.81016928
0.80983508
0.80967271
0.80967140
0.80975407
0.80938303
0.80913115
0.80896544
0.80891889
0.80871010
0.80890131
0.80908835
0.80896705
0.80880129
0.80873477
0.80834174
INFO - Training [5][  160/  196]   Loss 0.722591   Top1 74.970703   Top5 97.126465   BatchTime 0.437749   LR 0.000187
0.80822992
0.80816799
0.80792975
0.80787677
0.80761456
0.80743980
0.80740625
0.80737638
0.80735731
0.80758774
0.80754787
0.80740625
0.80705911
0.80706024
0.80675274
0.80648559
0.80623412
0.80587983
0.80574888
0.80548143
0.80561191
INFO - Training [5][  180/  196]   Loss 0.721179   Top1 75.047743   Top5 97.081163   BatchTime 0.420131   LR 0.000179
0.80551785
0.80556113
0.80540186
0.80527115
0.80529165
0.80506891
0.80507797
0.80499095
0.80456328
0.80440599
0.80404431
0.80420315
0.80378288
0.80360323
********************pre-trained*****************
INFO - ==> Top1: 75.192    Top5: 97.104    Loss: 0.719
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5000)
features.0.conv.3 tensor(0.1113)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0605)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.1577)
features.3.conv.0 tensor(0.0576)
features.3.conv.3 tensor(0.0864)
features.3.conv.6 tensor(0.1081)
features.4.conv.0 tensor(0.0474)
features.4.conv.3 tensor(0.3183)
features.4.conv.6 tensor(0.1646)
features.5.conv.0 tensor(0.3242)
features.5.conv.3 tensor(0.4404)
features.5.conv.6 tensor(0.1019)
features.6.conv.0 tensor(0.0547)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0817)
features.7.conv.0 tensor(0.1078)
features.7.conv.3 tensor(0.4442)
features.7.conv.6 tensor(0.2030)
features.8.conv.0 tensor(0.3073)
features.8.conv.3 tensor(0.5388)
features.8.conv.6 tensor(0.1404)
features.9.conv.0 tensor(0.3077)
features.9.conv.3 tensor(0.5448)
features.9.conv.6 tensor(0.1560)
features.10.conv.0 tensor(0.0713)
features.10.conv.3 tensor(0.1082)
features.10.conv.6 tensor(0.1065)
features.11.conv.0 tensor(0.4641)
features.11.conv.3 tensor(0.6545)
features.11.conv.6 tensor(0.2218)
features.12.conv.0 tensor(0.4835)
features.12.conv.3 tensor(0.6663)
features.12.conv.6 tensor(0.7964)
features.13.conv.0 tensor(0.3174)
features.13.conv.3 tensor(0.4925)
features.13.conv.6 tensor(0.0955)
features.14.conv.0 tensor(0.6773)
features.14.conv.3 tensor(0.8778)
features.14.conv.6 tensor(0.8101)
features.15.conv.0 tensor(0.7114)
features.15.conv.3 tensor(0.9003)
features.15.conv.6 tensor(0.9638)
features.16.conv.0 tensor(0.4153)
features.16.conv.3 tensor(0.8065)
features.16.conv.6 tensor(0.0821)
conv.0 tensor(0.0712)
tensor(821289.) 2188896.0
INFO - Validation [5][   20/   40]   Loss 0.513020   Top1 82.753906   Top5 98.808594   BatchTime 0.131619
INFO - Validation [5][   40/   40]   Loss 0.502582   Top1 82.940000   Top5 99.110000   BatchTime 0.093363
INFO - ==> Top1: 82.940    Top5: 99.110    Loss: 0.503
INFO - ==> Sparsity : 0.375
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 82.940   Top5: 99.110]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 81.960   Top5: 99.020]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 78.940   Top5: 98.400]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.80352980
0.80327767
0.80327749
0.80338848
0.80362815
0.80365229
0.80372858
0.80362231
0.80334586
0.80335069
0.80361617
0.80382687
0.80491787
0.80501187
0.80494487
0.80479187
0.80703115
0.80723226
0.80765659
0.80788141
0.80783439
0.80801976
0.80779874
0.80782026
INFO - Training [6][   20/  196]   Loss 0.716131   Top1 75.156250   Top5 96.972656   BatchTime 0.466193   LR 0.000166
0.80782300
0.80772644
0.80779088
0.80753660
0.80736387
0.80747294
0.80731541
0.80719692
0.80709213
0.80747777
0.80746949
0.80756909
0.80761552
0.80744666
0.80765837
0.80752599
0.80745244
0.80740637
INFO - Training [6][   40/  196]   Loss 0.712944   Top1 75.166016   Top5 96.953125   BatchTime 0.456130   LR 0.000158
0.80766159
0.80721587
0.80720019
0.80761260
0.80689085
0.80668563
0.80673242
0.80677778
0.80678988
0.80665773
0.80639654
0.80641228
0.80626273
0.80659878
0.80654961
0.80619961
0.80619222
0.80602342
INFO - Training [6][   60/  196]   Loss 0.697201   Top1 75.696615   Top5 97.194010   BatchTime 0.447349   LR 0.000151
0.80604517
0.80607170
0.80616796
0.80610824
0.80612904
0.80631691
0.80609733
0.80592710
0.80565172
0.80552167
0.80533934
0.80533153
0.80534661
0.80540240
0.80546087
0.80562770
0.80556965
0.80533749
0.80546099
0.80543172
0.80534565
0.80536717
0.80561203
0.80568141
INFO - Training [6][   80/  196]   Loss 0.687685   Top1 76.137695   Top5 97.363281   BatchTime 0.443353   LR 0.000143
0.80578989
0.80568063
0.80598116
0.80594277
0.80602622
0.80598748
0.80591416
0.80568635
0.80582386
0.80567425
0.80567509
0.80565083
0.80530190
0.80501693
0.80496079
0.80482739
0.80470234
0.80457073
INFO - Training [6][  100/  196]   Loss 0.680756   Top1 76.386719   Top5 97.468750   BatchTime 0.441162   LR 0.000136
0.80423689
0.80414695
0.80418891
0.80416226
0.80418342
0.80436593
0.80444920
0.80444872
0.80413204
0.80421919
0.80407423
0.80395985
0.80400509
0.80410826
0.80402958
0.80416656
0.80413359
0.80380720
0.80362248
0.80339801
0.80351013
0.80331093
0.80318832
INFO - Training [6][  120/  196]   Loss 0.678543   Top1 76.490885   Top5 97.519531   BatchTime 0.442791   LR 0.000129
0.80294794
0.80290365
0.80290860
0.80299222
0.80281174
0.80299360
0.80297458
0.80257398
0.80219132
0.80213976
0.80192888
0.80204940
0.80217350
0.80196601
0.80156696
0.80139196
0.80131441
0.80126137
0.80131012
INFO - Training [6][  140/  196]   Loss 0.677810   Top1 76.512277   Top5 97.578125   BatchTime 0.437120   LR 0.000122
0.80137318
0.80132985
0.80133140
0.80130494
0.80092430
0.80046713
0.80027986
0.80040967
0.80014092
0.80005586
0.79992712
0.79998726
0.79988617
0.80073768
0.80058438
0.80047339
0.80023766
0.80013478
INFO - Training [6][  160/  196]   Loss 0.679017   Top1 76.442871   Top5 97.568359   BatchTime 0.437779   LR 0.000115
0.80000192
0.80015182
0.80009848
0.79974329
0.79970407
0.79981321
0.79971099
0.79964823
0.79956847
0.79931808
0.79952437
0.79972988
0.79937983
0.79942536
0.79912984
0.79882735
0.79863453
0.79871970
0.79872841
INFO - Training [6][  180/  196]   Loss 0.675653   Top1 76.523438   Top5 97.530382   BatchTime 0.437795   LR 0.000108
0.79851258
0.79865146
0.79848677
0.79831326
0.79813325
0.79793203
0.79739994
0.79720092
0.79701847
0.79670674
0.79653728
0.79644722
0.79565185
0.79626477
0.79634732
********************pre-trained*****************
INFO - ==> Top1: 76.542    Top5: 97.538    Loss: 0.676
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [6][   20/   40]   Loss 0.472143   Top1 84.218750   Top5 99.140625   BatchTime 0.140661
features.0.conv.0 tensor(0.5035)
features.0.conv.3 tensor(0.1113)
features.1.conv.0 tensor(0.0443)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0807)
features.2.conv.0 tensor(0.0732)
features.2.conv.3 tensor(0.3418)
features.2.conv.6 tensor(0.1519)
features.3.conv.0 tensor(0.0579)
features.3.conv.3 tensor(0.0903)
features.3.conv.6 tensor(0.1092)
features.4.conv.0 tensor(0.0418)
features.4.conv.3 tensor(0.3177)
features.4.conv.6 tensor(0.1642)
features.5.conv.0 tensor(0.3228)
features.5.conv.3 tensor(0.4369)
features.5.conv.6 tensor(0.0999)
features.6.conv.0 tensor(0.0539)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0836)
features.7.conv.0 tensor(0.1085)
features.7.conv.3 tensor(0.4413)
features.7.conv.6 tensor(0.2053)
features.8.conv.0 tensor(0.2965)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.1351)
features.9.conv.0 tensor(0.3227)
features.9.conv.3 tensor(0.5486)
features.9.conv.6 tensor(0.1605)
features.10.conv.0 tensor(0.0725)
features.10.conv.3 tensor(0.1068)
features.10.conv.6 tensor(0.1051)
features.11.conv.0 tensor(0.4847)
features.11.conv.3 tensor(0.6547)
features.11.conv.6 tensor(0.1998)
features.12.conv.0 tensor(0.5076)
features.12.conv.3 tensor(0.6680)
features.12.conv.6 tensor(0.8000)
features.13.conv.0 tensor(0.3084)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.0941)
features.14.conv.0 tensor(0.6834)
features.14.conv.3 tensor(0.8773)
features.14.conv.6 tensor(0.8148)
features.15.conv.0 tensor(0.7224)
features.15.conv.3 tensor(0.9005)
features.15.conv.6 tensor(0.9641)
features.16.conv.0 tensor(0.4927)
features.16.conv.3 tensor(0.8062)
features.16.conv.6 tensor(0.0817)
conv.0 tensor(0.0722)
tensor(837816.) 2188896.0
INFO - Validation [6][   40/   40]   Loss 0.464300   Top1 84.380000   Top5 99.340000   BatchTime 0.096521
INFO - ==> Top1: 84.380    Top5: 99.340    Loss: 0.464
INFO - ==> Sparsity : 0.383
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 84.380   Top5: 99.340]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 82.940   Top5: 99.110]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 81.960   Top5: 99.020]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.79641569
0.79640245
0.79634291
0.79612988
0.79555410
0.79550129
0.79502231
0.79516703
0.79502171
0.79477131
0.79456758
0.79448283
0.79445112
0.79446590
0.79440737
0.79425019
0.79410273
0.79435158
0.79438615
0.79452449
0.79442751
0.79415363
INFO - Training [7][   20/  196]   Loss 0.676156   Top1 76.484375   Top5 96.894531   BatchTime 0.526456   LR 0.000097
0.79372174
0.79343462
0.79320514
0.79310304
0.79296029
0.79284960
0.79274094
0.79266804
0.79277164
0.79294395
0.79325819
0.79321474
0.79302591
0.79294765
0.79272854
0.79255813
0.79245162
0.79246342
0.79248226
INFO - Training [7][   40/  196]   Loss 0.663657   Top1 76.748047   Top5 97.480469   BatchTime 0.479403   LR 0.000091
0.79252833
0.79254860
0.79232460
0.79246593
0.79246831
0.79241794
0.79241502
0.79227406
0.79232621
0.79259378
0.79272372
0.79305738
0.79319936
0.79364389
0.79394680
0.79434878
0.79455823
0.79536426
0.79545921
0.79575956
INFO - Training [7][   60/  196]   Loss 0.655015   Top1 77.115885   Top5 97.513021   BatchTime 0.451590   LR 0.000085
0.79570103
0.79586846
0.79544240
0.79533118
0.79510432
0.79485089
0.79492623
0.79523450
0.79527903
0.79525185
0.79522300
0.79509825
0.79517376
0.79529446
0.79509652
0.79507256
0.79496497
0.79487258
0.79460204
0.79478198
0.79469466
0.79473233
INFO - Training [7][   80/  196]   Loss 0.650612   Top1 77.265625   Top5 97.617188   BatchTime 0.449453   LR 0.000079
0.79470736
0.79468876
0.79462838
0.79438347
0.79403389
0.79378957
0.79341352
0.79338366
0.79337358
0.79346019
0.79342449
0.79349625
0.79371762
0.79353142
0.79332936
0.79320329
0.79337281
0.79347104
0.79339981
INFO - Training [7][  100/  196]   Loss 0.643531   Top1 77.472656   Top5 97.695312   BatchTime 0.445373   LR 0.000073
0.79363918
0.79357076
0.79346740
0.79326588
0.79281861
0.79250997
0.79242200
0.79257923
0.79250008
0.79262739
0.79268825
0.79267341
0.79288495
0.79245913
0.79225141
0.79232997
0.79261833
0.79263723
0.79252404
INFO - Training [7][  120/  196]   Loss 0.636001   Top1 77.662760   Top5 97.805990   BatchTime 0.440622   LR 0.000067
0.79245806
0.79240829
0.79235774
0.79239780
0.79244065
0.79234248
0.79218292
0.79221469
0.79210103
0.79199576
0.79209369
0.79210776
0.79186171
0.79184222
0.79179436
0.79177541
0.79188514
0.79175240
0.79155678
0.79152393
0.79147238
INFO - Training [7][  140/  196]   Loss 0.635557   Top1 77.762277   Top5 97.820871   BatchTime 0.446070   LR 0.000062
0.79178369
0.79176080
0.79268366
0.79272056
0.79289681
0.79297143
0.79299790
0.79416674
0.79400623
0.79391658
0.79378629
0.79381847
0.79388100
0.79415488
0.79433399
0.79445040
0.79455411
0.79446858
0.79463851
0.79482853
0.79489839
0.79560751
INFO - Training [7][  160/  196]   Loss 0.635653   Top1 77.763672   Top5 97.807617   BatchTime 0.449326   LR 0.000057
0.79654551
0.79775441
0.79859096
0.79865509
0.79919749
0.79916310
0.79901385
0.79923666
0.79915595
0.79907936
0.79927713
0.79912460
0.79922396
INFO - Training [7][  180/  196]   Loss 0.636011   Top1 77.805990   Top5 97.799479   BatchTime 0.450552   LR 0.000052
0.79919773
0.79936594
0.79923773
0.79907376
0.79899281
0.79897153
0.79884601
0.79884207
0.79860634
0.79866499
0.79879147
0.79898417
0.79901499
0.79910707
0.79918081
0.79896468
0.79900438
INFO - ==> Top1: 77.898    Top5: 97.806    Loss: 0.634
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79886490
0.79857814
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.442541   Top1 84.882812   Top5 99.199219   BatchTime 0.139074
INFO - Validation [7][   40/   40]   Loss 0.435236   Top1 85.060000   Top5 99.360000   BatchTime 0.096267
INFO - ==> Top1: 85.060    Top5: 99.360    Loss: 0.435
INFO - ==> Sparsity : 0.386
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 84.380   Top5: 99.340]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 82.940   Top5: 99.110]
features.0.conv.0 tensor(0.5069)
features.0.conv.3 tensor(0.1055)
features.1.conv.0 tensor(0.0410)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0807)
features.2.conv.0 tensor(0.0723)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1516)
features.3.conv.0 tensor(0.0593)
features.3.conv.3 tensor(0.0887)
features.3.conv.6 tensor(0.1048)
features.4.conv.0 tensor(0.0465)
features.4.conv.3 tensor(0.3194)
features.4.conv.6 tensor(0.1624)
features.5.conv.0 tensor(0.3245)
features.5.conv.3 tensor(0.4363)
features.5.conv.6 tensor(0.0996)
features.6.conv.0 tensor(0.0549)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0845)
features.7.conv.0 tensor(0.1138)
features.7.conv.3 tensor(0.4413)
features.7.conv.6 tensor(0.2115)
features.8.conv.0 tensor(0.3002)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.1369)
features.9.conv.0 tensor(0.3121)
features.9.conv.3 tensor(0.5477)
features.9.conv.6 tensor(0.1739)
features.10.conv.0 tensor(0.0709)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.1055)
features.11.conv.0 tensor(0.4900)
features.11.conv.3 tensor(0.6528)
features.11.conv.6 tensor(0.1997)
features.12.conv.0 tensor(0.4885)
features.12.conv.3 tensor(0.6674)
features.12.conv.6 tensor(0.7998)
features.13.conv.0 tensor(0.3086)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.0937)
features.14.conv.0 tensor(0.6933)
features.14.conv.3 tensor(0.8770)
features.14.conv.6 tensor(0.8175)
features.15.conv.0 tensor(0.7448)
features.15.conv.3 tensor(0.9008)
features.15.conv.6 tensor(0.9647)
features.16.conv.0 tensor(0.5017)
features.16.conv.3 tensor(0.8062)
features.16.conv.6 tensor(0.0821)
conv.0 tensor(0.0731)
tensor(844822.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.79865694
0.79852766
0.79841858
0.79834223
0.79817581
0.79810369
0.79794425
0.79799521
0.79812640
0.79822934
0.79844052
0.79811537
0.79798311
0.79780310
0.79770046
0.79761541
INFO - Training [8][   20/  196]   Loss 0.607749   Top1 79.160156   Top5 97.226562   BatchTime 0.529231   LR 0.000043
0.79765701
0.79782921
0.79800987
0.79792172
0.79787594
0.79805231
0.79800701
0.79754102
0.79740512
0.79739892
0.79750365
0.79751194
0.79758346
0.79786295
0.79790813
0.79754615
0.79648012
0.79619133
0.79630548
0.79628044
0.79624838
0.79625034
0.79588407
0.79575980
INFO - Training [8][   40/  196]   Loss 0.635961   Top1 78.232422   Top5 97.304688   BatchTime 0.468564   LR 0.000039
0.79533184
0.79519528
0.79532468
0.79540277
0.79529941
0.79509205
0.79511464
0.79492462
0.79456270
0.79425776
0.79376203
0.79373056
0.79385090
0.79390401
0.79384869
0.79392952
0.79384089
0.79381043
0.79387945
0.79375279
INFO - Training [8][   60/  196]   Loss 0.632216   Top1 78.229167   Top5 97.395833   BatchTime 0.447259   LR 0.000035
0.79397649
0.79380327
0.79379386
0.79342818
0.79320502
0.79322010
0.79309994
0.79285181
0.79279381
0.79253209
0.79230624
0.79227930
0.79218727
0.79217154
0.79231220
0.79264605
0.79261231
0.79233235
0.79231685
INFO - Training [8][   80/  196]   Loss 0.631965   Top1 78.388672   Top5 97.485352   BatchTime 0.444082   LR 0.000031
0.79231972
0.79224116
0.79228234
0.79230219
0.79244524
0.79272777
0.79298753
0.79304314
0.79305631
0.79323155
0.79288322
0.79271632
0.79251826
0.79240853
0.79227346
0.79229861
0.79240948
0.79259014
INFO - Training [8][  100/  196]   Loss 0.625692   Top1 78.523438   Top5 97.613281   BatchTime 0.442061   LR 0.000027
0.79259276
0.79270470
0.79254073
0.79258120
0.79266030
0.79269308
0.79272103
0.79296768
0.79273421
0.79265732
0.79257679
0.79341930
0.79352200
0.79333448
0.79309422
0.79306751
0.79321605
0.79351002
0.79367650
0.79368442
0.79337537
INFO - Training [8][  120/  196]   Loss 0.617003   Top1 78.873698   Top5 97.744141   BatchTime 0.447272   LR 0.000023
0.79344136
0.79338604
0.79322815
0.79331577
0.79336798
0.79320145
0.79328805
0.79318964
0.79293400
0.79283279
0.79273415
0.79276985
0.79281884
0.79274887
0.79285890
0.79296738
0.79282212
0.79293543
0.79297638
0.79278719
0.79279608
INFO - Training [8][  140/  196]   Loss 0.613422   Top1 79.003906   Top5 97.815290   BatchTime 0.450820   LR 0.000020
0.79296237
0.79287016
0.79263163
0.79250401
0.79262698
0.79277074
0.79266393
0.79278213
0.79285604
0.79288507
0.79260594
0.79253340
0.79253906
0.79249519
0.79254460
0.79250145
0.79267681
INFO - Training [8][  160/  196]   Loss 0.613559   Top1 79.028320   Top5 97.858887   BatchTime 0.454553   LR 0.000017
0.79250449
0.79240209
0.79228157
0.79238749
0.79246253
0.79279840
0.79276061
0.79254490
0.79235005
0.79247171
0.79266536
0.79263413
0.79233366
0.79224312
0.79230779
0.79208541
0.79208922
0.79214972
0.79220372
0.79216027
0.79208273
0.79213995
INFO - Training [8][  180/  196]   Loss 0.611524   Top1 79.071181   Top5 97.864583   BatchTime 0.455583   LR 0.000014
0.79215044
0.79221112
0.79215038
0.79205763
0.79201007
0.79216778
0.79229128
0.79205132
0.79194760
0.79222953
0.79228276
0.79237801
0.79225844
0.79220837
0.79213470
0.79203629
0.79216546
INFO - ==> Top1: 79.054    Top5: 97.886    Loss: 0.610
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79238093
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [8][   20/   40]   Loss 0.426701   Top1 85.742188   Top5 99.277344   BatchTime 0.130726
INFO - Validation [8][   40/   40]   Loss 0.420141   Top1 85.750000   Top5 99.410000   BatchTime 0.095648
INFO - ==> Top1: 85.750    Top5: 99.410    Loss: 0.420
INFO - ==> Sparsity : 0.387
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 84.380   Top5: 99.340]
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.1016)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0749)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1502)
features.3.conv.0 tensor(0.0616)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1055)
features.4.conv.0 tensor(0.0456)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1623)
features.5.conv.0 tensor(0.3241)
features.5.conv.3 tensor(0.4363)
features.5.conv.6 tensor(0.0998)
features.6.conv.0 tensor(0.0553)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.1144)
features.7.conv.3 tensor(0.4404)
features.7.conv.6 tensor(0.2079)
features.8.conv.0 tensor(0.3011)
features.8.conv.3 tensor(0.5394)
features.8.conv.6 tensor(0.1381)
features.9.conv.0 tensor(0.3222)
features.9.conv.3 tensor(0.5457)
features.9.conv.6 tensor(0.1715)
features.10.conv.0 tensor(0.0716)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.1053)
features.11.conv.0 tensor(0.4998)
features.11.conv.3 tensor(0.6526)
features.11.conv.6 tensor(0.1948)
features.12.conv.0 tensor(0.4914)
features.12.conv.3 tensor(0.6672)
features.12.conv.6 tensor(0.7975)
features.13.conv.0 tensor(0.3110)
features.13.conv.3 tensor(0.4905)
features.13.conv.6 tensor(0.0938)
features.14.conv.0 tensor(0.6961)
features.14.conv.3 tensor(0.8764)
features.14.conv.6 tensor(0.8172)
features.15.conv.0 tensor(0.7422)
features.15.conv.3 tensor(0.9007)
features.15.conv.6 tensor(0.9640)
features.16.conv.0 tensor(0.5115)
features.16.conv.3 tensor(0.8059)
features.16.conv.6 tensor(0.0815)
conv.0 tensor(0.0733)
tensor(846660.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.79243940
0.79226226
0.79222256
0.79197973
0.79201686
0.79171330
0.79125226
0.79100996
0.79092437
0.79119009
0.79146451
0.79120880
0.79094762
0.79086941
0.79074985
0.79067308
0.79054797
0.79053861
INFO - Training [9][   20/  196]   Loss 0.602736   Top1 79.355469   Top5 97.714844   BatchTime 0.506419   LR 0.000010
0.79041058
0.79077166
0.79122275
0.79146349
0.79153401
0.79145992
0.79155707
0.79138428
0.79104918
0.79077965
0.79090005
0.79077214
0.79086763
0.79098284
0.79105985
0.79110539
0.79083759
0.79075515
0.79093027
0.79136908
0.79113948
INFO - Training [9][   40/  196]   Loss 0.620587   Top1 78.886719   Top5 97.539062   BatchTime 0.446128   LR 0.000008
0.79128510
0.79123038
0.79131621
0.79135489
0.79126763
0.79125935
0.79134995
0.79110801
0.79097253
0.79107755
0.79118609
0.79116571
0.79128683
0.79132813
0.79126292
0.79094213
0.79079282
0.79079485
0.79076868
0.79107976
0.79088300
INFO - Training [9][   60/  196]   Loss 0.609709   Top1 79.160156   Top5 97.714844   BatchTime 0.424344   LR 0.000006
0.79078346
0.79100645
0.79085594
0.79068470
0.79088223
0.79089433
0.79077899
0.79079813
0.79073006
0.79092520
0.79073763
0.79073948
0.79034233
0.79003048
0.78977573
0.78961110
0.78958589
0.78965849
INFO - Training [9][   80/  196]   Loss 0.609447   Top1 79.174805   Top5 97.900391   BatchTime 0.429817   LR 0.000004
0.78949118
0.78951311
0.78951925
0.78935355
0.78974247
0.78968978
0.78949827
0.78937703
0.78941101
0.78932768
0.78924829
0.78917342
0.78903049
0.78930378
0.78912657
0.78911906
0.78907299
0.78902966
0.78925139
0.78926212
0.78942490
INFO - Training [9][  100/  196]   Loss 0.600467   Top1 79.394531   Top5 97.957031   BatchTime 0.438620   LR 0.000003
0.78929543
0.78882855
0.78881264
0.78883094
0.78883284
0.78847378
0.78819776
0.78808695
0.78790206
0.78805012
0.78793079
0.78769606
0.78771794
0.78774691
0.78815484
0.78832537
0.78818697
INFO - Training [9][  120/  196]   Loss 0.592343   Top1 79.677734   Top5 98.033854   BatchTime 0.443990   LR 0.000002
0.78800094
0.78787899
0.78799742
0.78813434
0.78803146
0.78814924
0.78852391
0.78848648
0.78832603
0.78809702
0.78791857
0.78762370
0.78764081
0.78766543
0.78748310
0.78747213
0.78759623
0.78755236
0.78739035
0.78737438
0.78747296
0.78742164
INFO - Training [9][  140/  196]   Loss 0.591964   Top1 79.690290   Top5 98.108259   BatchTime 0.447515   LR 0.000001
0.78743279
0.78739345
0.78744572
0.78739130
0.78765005
0.78796440
0.78766477
0.78728664
0.78717506
0.78706765
0.78697830
0.78696835
0.78697431
0.78701478
0.78714710
0.78716660
0.78698021
0.78673965
0.78686118
0.78657579
0.78634846
INFO - Training [9][  160/  196]   Loss 0.597592   Top1 79.479980   Top5 98.076172   BatchTime 0.449103   LR 0.000000
0.78636920
0.78608441
0.78595871
0.78565603
0.78558409
0.78574955
0.78572690
0.78564256
0.78548634
0.78517669
0.78492403
0.78453344
0.78429562
0.78410172
0.78384870
0.78381675
0.78355181
INFO - Training [9][  180/  196]   Loss 0.594269   Top1 79.533420   Top5 98.083767   BatchTime 0.452459   LR 0.000000
0.78318381
0.78295624
0.78258544
0.78236103
0.78219569
0.78195775
0.78147465
0.78104007
0.78048551
0.78018773
0.77991402
0.77949291
0.77887297
0.77843696
0.77788758
0.77706909
0.77692056
0.77683020
INFO - ==> Top1: 79.638    Top5: 98.092    Loss: 0.591
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77672666
0.77669215
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.426788   Top1 85.703125   Top5 99.335938   BatchTime 0.133854
INFO - Validation [9][   40/   40]   Loss 0.418328   Top1 85.710000   Top5 99.440000   BatchTime 0.097225
INFO - ==> Top1: 85.710    Top5: 99.440    Loss: 0.418
INFO - ==> Sparsity : 0.387
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5104)
features.0.conv.3 tensor(0.1016)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0749)
features.2.conv.3 tensor(0.3441)
features.2.conv.6 tensor(0.1499)
features.3.conv.0 tensor(0.0616)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1059)
features.4.conv.0 tensor(0.0457)
features.4.conv.3 tensor(0.3119)
features.4.conv.6 tensor(0.1619)
features.5.conv.0 tensor(0.3244)
features.5.conv.3 tensor(0.4369)
features.5.conv.6 tensor(0.0994)
features.6.conv.0 tensor(0.0562)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0843)
features.7.conv.0 tensor(0.1141)
features.7.conv.3 tensor(0.4398)
features.7.conv.6 tensor(0.2082)
features.8.conv.0 tensor(0.3022)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.1381)
features.9.conv.0 tensor(0.3239)
features.9.conv.3 tensor(0.5457)
features.9.conv.6 tensor(0.1720)
features.10.conv.0 tensor(0.0712)
features.10.conv.3 tensor(0.1045)
features.10.conv.6 tensor(0.1054)
features.11.conv.0 tensor(0.5016)
features.11.conv.3 tensor(0.6524)
features.11.conv.6 tensor(0.1934)
features.12.conv.0 tensor(0.4919)
features.12.conv.3 tensor(0.6672)
features.12.conv.6 tensor(0.7972)
features.13.conv.0 tensor(0.3125)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.0944)
features.14.conv.0 tensor(0.6976)
features.14.conv.3 tensor(0.8763)
features.14.conv.6 tensor(0.8173)
features.15.conv.0 tensor(0.7437)
features.15.conv.3 tensor(0.9007)
features.15.conv.6 tensor(0.9639)
features.16.conv.0 tensor(0.5149)
features.16.conv.3 tensor(0.8061)
features.16.conv.6 tensor(0.0818)
conv.0 tensor(0.0736)
tensor(848138.) 2188896.0
0.77668613
0.79199737
0.79059649
0.79198700
0.79513395
0.79649764
0.79869938
0.80103439
0.80232036
0.80238789
0.80232143
0.80189490
0.80309880
0.80293405
0.80264378
0.80310094
0.80309170
0.80293661
0.80232251
INFO - Training [10][   20/  196]   Loss 0.658661   Top1 76.855469   Top5 97.089844   BatchTime 0.531122   LR 0.000250
0.80204827
0.80221587
0.80136651
0.80161768
0.80291992
0.81029934
0.81293416
0.81267315
0.81251490
0.81306177
0.81392151
0.81641644
0.82202190
0.82216650
0.82216507
0.82237911
0.82286966
INFO - Training [10][   40/  196]   Loss 0.667045   Top1 76.269531   Top5 97.294922   BatchTime 0.448907   LR 0.000250
0.82384962
0.82411665
0.82580882
0.82630849
0.82643795
0.82675219
0.82682174
0.82699192
0.82749337
0.82770777
0.82801908
0.82898378
0.83047473
0.83558565
0.83879209
0.83901089
0.83936375
0.83960980
0.83990425
0.83978957
0.83976352
0.84009707
0.84322816
0.84503591
INFO - Training [10][   60/  196]   Loss 0.670867   Top1 76.347656   Top5 97.363281   BatchTime 0.408828   LR 0.000250
0.84545326
0.84520733
0.84559542
0.84583777
0.84578085
0.84628516
0.84664553
0.84695548
0.84724927
0.84746492
0.84777170
0.84816724
0.84795135
0.84770203
0.84790665
0.84817332
0.84827286
0.84806287
0.84836376
0.84856755
INFO - Training [10][   80/  196]   Loss 0.673537   Top1 76.347656   Top5 97.456055   BatchTime 0.407029   LR 0.000250
0.84891152
0.84859037
0.84873950
0.84824067
0.84830666
0.84834176
0.84808022
0.84784889
0.84805793
0.84779584
0.84765625
0.84795874
0.84781623
0.84752280
0.84777629
0.84781927
0.84772927
INFO - Training [10][  100/  196]   Loss 0.672836   Top1 76.488281   Top5 97.394531   BatchTime 0.419352   LR 0.000250
0.84777576
0.84758580
0.84802258
0.84803557
0.84908307
0.84936357
0.84925532
0.84919751
0.84920466
0.84918326
0.84887111
0.84853965
0.84840000
0.84835762
0.84850091
0.84875739
0.84871531
0.84878057
0.84896189
0.84878123
0.84865063
0.84854585
INFO - Training [10][  120/  196]   Loss 0.673757   Top1 76.572266   Top5 97.386068   BatchTime 0.425233   LR 0.000249
0.84851581
0.84846193
0.84828264
0.84829086
0.84839159
0.84820569
0.84786010
0.84780794
0.84759188
0.84728193
0.84698254
0.84620881
0.84606129
0.84573835
0.84563166
0.84552693
0.84534055
0.84520680
0.84538102
0.84540772
0.84527999
INFO - Training [10][  140/  196]   Loss 0.670631   Top1 76.702009   Top5 97.486049   BatchTime 0.431290   LR 0.000249
0.84559530
0.84515762
0.84496188
0.84495497
0.84471536
0.84480941
0.84480190
0.84615147
0.84641016
0.84592688
0.84564286
0.84487724
0.84502184
0.84511924
0.84488928
0.84463483
0.84420049
0.84402710
INFO - Training [10][  160/  196]   Loss 0.673717   Top1 76.596680   Top5 97.475586   BatchTime 0.434913   LR 0.000249
0.84293181
0.84216303
0.84233177
0.84180832
0.84100831
0.84016418
0.83980763
0.83946508
0.83843344
0.83742559
0.83700120
0.83681262
0.83623648
0.83641845
0.83631390
0.83626723
0.83637774
0.83658564
0.83585817
0.83574486
0.83526206
0.83526325
INFO - Training [10][  180/  196]   Loss 0.674636   Top1 76.534288   Top5 97.419705   BatchTime 0.437219   LR 0.000249
0.83518654
0.83534598
0.83547413
0.83539373
0.83505017
0.83521926
0.83512384
0.83538538
0.83545446
0.83539146
0.83486605
0.83462971
0.83515888
INFO - ==> Top1: 76.434    Top5: 97.406    Loss: 0.677
0.83488297
0.83548725
0.83574665
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.511570   Top1 82.812500   Top5 99.101562   BatchTime 0.130100
INFO - Validation [10][   40/   40]   Loss 0.507250   Top1 82.710000   Top5 99.180000   BatchTime 0.093307
INFO - ==> Top1: 82.710    Top5: 99.180    Loss: 0.507
INFO - ==> Sparsity : 0.362
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0699)
features.2.conv.0 tensor(0.0550)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.1878)
features.3.conv.0 tensor(0.0613)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.1081)
features.4.conv.0 tensor(0.0397)
features.4.conv.3 tensor(0.3218)
features.4.conv.6 tensor(0.1672)
features.5.conv.0 tensor(0.3244)
features.5.conv.3 tensor(0.4311)
features.5.conv.6 tensor(0.1011)
features.6.conv.0 tensor(0.0563)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0855)
features.7.conv.0 tensor(0.0967)
features.7.conv.3 tensor(0.4407)
features.7.conv.6 tensor(0.1980)
features.8.conv.0 tensor(0.2758)
features.8.conv.3 tensor(0.5399)
features.8.conv.6 tensor(0.1375)
features.9.conv.0 tensor(0.2626)
features.9.conv.3 tensor(0.5475)
features.9.conv.6 tensor(0.1569)
features.10.conv.0 tensor(0.0680)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0647)
features.11.conv.0 tensor(0.3164)
features.11.conv.3 tensor(0.6510)
features.11.conv.6 tensor(0.2764)
features.12.conv.0 tensor(0.4177)
features.12.conv.3 tensor(0.6532)
features.12.conv.6 tensor(0.5264)
features.13.conv.0 tensor(0.3072)
features.13.conv.3 tensor(0.4898)
features.13.conv.6 tensor(0.1038)
features.14.conv.0 tensor(0.6872)
features.14.conv.3 tensor(0.8759)
features.14.conv.6 tensor(0.8279)
features.15.conv.0 tensor(0.7167)
features.15.conv.3 tensor(0.9012)
features.15.conv.6 tensor(0.9658)
features.16.conv.0 tensor(0.3529)
features.16.conv.3 tensor(0.8065)
features.16.conv.6 tensor(0.0902)
conv.0 tensor(0.0722)
tensor(792519.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
0.83570492
0.83545965
0.83527744
0.83548003
0.83517623
0.83617413
0.83572704
0.83480889
0.83371133
0.83231103
0.83155036
0.83134902
0.83081490
0.83016497
0.82971114
0.82967937
0.82971054
0.82922268
0.82935935
INFO - Training [11][   20/  196]   Loss 0.704709   Top1 75.761719   Top5 97.148438   BatchTime 0.538425   LR 0.000248
0.82937610
0.82954907
0.82987416
0.82994735
0.82956463
0.82962191
0.82967311
0.82961518
0.82966542
0.82948250
0.82973701
0.82930529
0.82909650
0.82929593
0.82907653
0.82922691
0.82913983
INFO - Training [11][   40/  196]   Loss 0.702055   Top1 75.712891   Top5 97.285156   BatchTime 0.497570   LR 0.000248
0.82906961
0.82879382
0.82842225
0.82859570
0.82826680
0.82797879
0.82761842
0.82759106
0.82744354
0.82710510
0.82631642
0.82592964
0.82569093
0.82551438
0.82517987
0.82562071
0.82504791
0.82470465
0.82450831
0.82436216
0.82416654
0.82419944
INFO - Training [11][   60/  196]   Loss 0.695293   Top1 75.826823   Top5 97.447917   BatchTime 0.446508   LR 0.000247
0.82362241
0.82326204
0.82274389
0.82216620
0.82144672
0.82068503
0.82024533
0.81994694
0.81969607
0.81954324
0.81930250
0.81920356
0.81950343
0.81943345
0.81920016
0.81884533
0.81882817
0.81899083
INFO - Training [11][   80/  196]   Loss 0.690260   Top1 76.025391   Top5 97.534180   BatchTime 0.421897   LR 0.000247
0.81889141
0.81883198
0.81890160
0.81900811
0.81871289
0.81866509
0.81860644
0.81847239
0.81967402
0.82856244
0.82842851
0.82948494
0.82915878
0.82867277
0.82838941
0.82813221
0.82751030
0.82750344
0.82641280
0.82540768
0.82430828
0.82346410
INFO - Training [11][  100/  196]   Loss 0.678770   Top1 76.457031   Top5 97.578125   BatchTime 0.432651   LR 0.000247
0.82267660
0.82198989
0.82075983
0.82100701
0.82146698
0.82139260
0.82136774
0.82153982
0.82175958
0.82203472
0.82225573
0.82238805
0.82274562
0.82372910
0.82472026
0.82646179
0.82944727
0.83531648
0.83517063
0.83530897
0.83491236
INFO - Training [11][  120/  196]   Loss 0.671112   Top1 76.829427   Top5 97.666016   BatchTime 0.438992   LR 0.000246
0.83455926
0.83332938
0.83393544
0.83491492
0.83489895
0.83531380
0.83865774
0.83983779
0.83971316
0.83939701
0.83917499
0.83908689
0.83891052
0.83870631
0.83872050
0.83901453
0.83886921
0.83846736
INFO - Training [11][  140/  196]   Loss 0.668568   Top1 76.964286   Top5 97.695312   BatchTime 0.440911   LR 0.000246
0.83828759
0.83853120
0.83863205
0.83841753
0.83828288
0.83744723
0.83708578
0.83694512
0.83745342
0.83794630
0.83834714
0.83780533
0.83848721
0.83839113
0.83856577
0.83879405
0.83825094
0.83827043
0.83843714
0.83846849
0.83849138
INFO - Training [11][  160/  196]   Loss 0.673393   Top1 76.755371   Top5 97.661133   BatchTime 0.443211   LR 0.000245
0.83865649
0.83824646
0.83831954
0.83850515
0.83836710
0.83841544
0.83844334
0.83832771
0.83840328
0.83854544
0.83864766
0.83865434
0.83856285
0.83842862
0.83840090
0.83847493
0.83838129
0.83856976
0.83864343
0.83859110
0.83893126
0.83869934
INFO - Training [11][  180/  196]   Loss 0.672445   Top1 76.814236   Top5 97.649740   BatchTime 0.446038   LR 0.000244
0.83851445
0.83833396
0.83830392
0.83794987
0.83805811
0.83810335
0.83732969
0.83722860
0.83707178
0.83709800
0.83682209
0.83702940
0.83700919
INFO - ==> Top1: 76.830    Top5: 97.650    Loss: 0.671
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83712846
0.83703434
0.83705652
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [11][   20/   40]   Loss 0.520022   Top1 82.285156   Top5 98.945312   BatchTime 0.133912
INFO - Validation [11][   40/   40]   Loss 0.523798   Top1 81.790000   Top5 99.160000   BatchTime 0.096801
features.0.conv.0 tensor(0.5174)
features.0.conv.3 tensor(0.1504)
features.1.conv.0 tensor(0.0514)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0777)
features.2.conv.0 tensor(0.0460)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.1852)
features.3.conv.0 tensor(0.0527)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.1085)
features.4.conv.0 tensor(0.0343)
features.4.conv.3 tensor(0.3194)
features.4.conv.6 tensor(0.1641)
features.5.conv.0 tensor(0.3218)
features.5.conv.3 tensor(0.4334)
features.5.conv.6 tensor(0.1021)
features.6.conv.0 tensor(0.0511)
features.6.conv.3 tensor(0.0660)
features.6.conv.6 tensor(0.0845)
features.7.conv.0 tensor(0.0935)
features.7.conv.3 tensor(0.4433)
features.7.conv.6 tensor(0.2055)
features.8.conv.0 tensor(0.2502)
features.8.conv.3 tensor(0.5385)
features.8.conv.6 tensor(0.1378)
features.9.conv.0 tensor(0.2637)
features.9.conv.3 tensor(0.5431)
features.9.conv.6 tensor(0.1542)
features.10.conv.0 tensor(0.0688)
features.10.conv.3 tensor(0.0992)
features.10.conv.6 tensor(0.0659)
features.11.conv.0 tensor(0.3780)
features.11.conv.3 tensor(0.6495)
features.11.conv.6 tensor(0.1905)
features.12.conv.0 tensor(0.3327)
features.12.conv.3 tensor(0.6528)
features.12.conv.6 tensor(0.5262)
features.13.conv.0 tensor(0.3092)
features.13.conv.3 tensor(0.4919)
features.13.conv.6 tensor(0.1042)
features.14.conv.0 tensor(0.6907)
features.14.conv.3 tensor(0.8721)
features.14.conv.6 tensor(0.8275)
features.15.conv.0 tensor(0.7216)
features.15.conv.3 tensor(0.9020)
features.15.conv.6 tensor(0.9649)
features.16.conv.0 tensor(0.3547)
features.16.conv.3 tensor(0.8049)
features.16.conv.6 tensor(0.1014)
conv.0 tensor(0.0736)
tensor(791279.) 2188896.0
INFO - ==> Top1: 81.790    Top5: 99.160    Loss: 0.524
INFO - ==> Sparsity : 0.361
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
0.83729869
0.83699602
0.83692843
0.83687639
0.83687454
0.83696723
0.83799183
0.83816683
0.83821064
0.83788544
0.83777004
0.83733791
0.83740741
0.83725530
0.83715415
0.83708870
0.83709806
0.83766794
INFO - Training [12][   20/  196]   Loss 0.683704   Top1 76.875000   Top5 97.109375   BatchTime 0.553059   LR 0.000243
0.83765537
0.83712238
0.83723831
0.83717936
0.83714443
0.83740008
0.83745772
0.83741581
0.83742338
0.83708131
0.83706313
0.83700913
0.83684349
0.83625305
0.83545130
0.83559072
0.83599180
0.83607161
0.83573538
0.83578843
0.83587098
INFO - Training [12][   40/  196]   Loss 0.679913   Top1 76.650391   Top5 97.324219   BatchTime 0.506826   LR 0.000243
0.83567953
0.83561814
0.83552778
0.83537728
0.83549058
0.83574539
0.83572298
0.83537960
0.83503228
0.83488184
0.83464688
0.83445603
0.83445638
0.83388102
0.83379990
0.83373082
0.83328605
0.83294046
0.83283663
0.83235830
0.83204687
INFO - Training [12][   60/  196]   Loss 0.673471   Top1 76.940104   Top5 97.460938   BatchTime 0.467966   LR 0.000242
0.83202660
0.83181822
0.83132732
0.83124816
0.83084232
0.83073944
0.83027130
0.83001709
0.82924348
0.82858080
0.82831520
0.82796705
0.82767743
0.82724106
0.82690185
0.82682961
INFO - Training [12][   80/  196]   Loss 0.674571   Top1 76.904297   Top5 97.573242   BatchTime 0.443124   LR 0.000241
0.82673436
0.82643944
0.82631880
0.82614243
0.82594007
0.82634789
0.82681370
0.82749325
0.82929015
0.82912153
0.82954979
0.83074039
0.83068377
0.83046812
0.83012009
0.83004487
0.82965827
0.82908100
0.82880515
0.82841837
INFO - Training [12][  100/  196]   Loss 0.666543   Top1 77.109375   Top5 97.644531   BatchTime 0.436873   LR 0.000240
0.82818496
0.82766908
0.82729995
0.82653117
0.82661003
0.82621682
0.82609576
0.82570356
0.82542700
0.82546860
0.82511437
0.82447892
0.82418221
0.82394445
0.82398003
0.82364428
0.82370377
0.82332546
0.82293510
0.82284480
0.82323343
0.82313579
INFO - Training [12][  120/  196]   Loss 0.657585   Top1 77.412109   Top5 97.708333   BatchTime 0.441222   LR 0.000240
0.82250732
0.82226169
0.82220036
0.82206702
0.82188785
0.82187271
0.82158715
0.82168525
0.82129776
0.82129443
0.82089561
0.82087374
0.82107377
0.82086700
0.82083994
0.82060027
0.82019091
0.81967109
0.81940562
0.81920868
0.81915784
INFO - Training [12][  140/  196]   Loss 0.655454   Top1 77.525112   Top5 97.737165   BatchTime 0.446099   LR 0.000239
0.81902885
0.81858510
0.81822592
0.81790441
0.81748658
0.81737310
0.81739956
0.81729138
0.81748545
0.81706154
0.81720138
0.81696892
0.81694680
0.81675327
0.81674743
0.81692821
0.81734186
0.81711304
0.81720680
0.81667763
0.81691796
INFO - Training [12][  160/  196]   Loss 0.658729   Top1 77.355957   Top5 97.729492   BatchTime 0.450589   LR 0.000238
0.81704712
0.81704295
0.81698322
0.81717587
0.81740940
0.81745809
0.81729889
0.81833595
0.82068849
0.82778245
0.83022618
0.83696383
0.83741134
0.83736408
0.83719480
0.83736038
0.83670169
INFO - Training [12][  180/  196]   Loss 0.657423   Top1 77.421875   Top5 97.708333   BatchTime 0.452578   LR 0.000237
0.83724177
0.83803242
0.83858007
0.83843178
0.83883476
0.83857906
0.83830988
0.83837020
0.83851200
0.83827221
0.83831316
0.83831036
0.83823842
0.83796310
0.83837163
0.83818954
0.83807015
INFO - ==> Top1: 77.504    Top5: 97.726    Loss: 0.655
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.83792430
0.83792102
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.472163   Top1 84.277344   Top5 99.277344   BatchTime 0.138401
INFO - Validation [12][   40/   40]   Loss 0.465796   Top1 84.200000   Top5 99.390000   BatchTime 0.100066
INFO - ==> Top1: 84.200    Top5: 99.390    Loss: 0.466
INFO - ==> Sparsity : 0.368
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5312)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.3426)
features.2.conv.6 tensor(0.1985)
features.3.conv.0 tensor(0.0587)
features.3.conv.3 tensor(0.0872)
features.3.conv.6 tensor(0.1076)
features.4.conv.0 tensor(0.0386)
features.4.conv.3 tensor(0.3142)
features.4.conv.6 tensor(0.1696)
features.5.conv.0 tensor(0.3203)
features.5.conv.3 tensor(0.4340)
features.5.conv.6 tensor(0.1032)
features.6.conv.0 tensor(0.0535)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0837)
features.7.conv.0 tensor(0.0922)
features.7.conv.3 tensor(0.4410)
features.7.conv.6 tensor(0.2181)
features.8.conv.0 tensor(0.2439)
features.8.conv.3 tensor(0.5434)
features.8.conv.6 tensor(0.1536)
features.9.conv.0 tensor(0.2624)
features.9.conv.3 tensor(0.5489)
features.9.conv.6 tensor(0.1587)
features.10.conv.0 tensor(0.0651)
features.10.conv.3 tensor(0.1033)
features.10.conv.6 tensor(0.0685)
features.11.conv.0 tensor(0.3818)
features.11.conv.3 tensor(0.6507)
features.11.conv.6 tensor(0.2046)
features.12.conv.0 tensor(0.3597)
features.12.conv.3 tensor(0.6532)
features.12.conv.6 tensor(0.5251)
features.13.conv.0 tensor(0.3112)
features.13.conv.3 tensor(0.4931)
features.13.conv.6 tensor(0.1055)
features.14.conv.0 tensor(0.6978)
features.14.conv.3 tensor(0.8682)
features.14.conv.6 tensor(0.8363)
features.15.conv.0 tensor(0.7405)
features.15.conv.3 tensor(0.9021)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.3700)
features.16.conv.3 tensor(0.8052)
features.16.conv.6 tensor(0.1047)
conv.0 tensor(0.0766)
tensor(804937.) 2188896.0
0.83773547
0.83774465
0.83763355
0.83749324
0.83753735
0.83778489
0.83804470
0.83764690
0.83666188
0.83677369
0.83708298
0.83645517
0.83632171
0.83618802
0.83614600
0.83583289
0.83555090
0.83585459
0.83520716
0.83611363
INFO - Training [13][   20/  196]   Loss 0.689297   Top1 75.585938   Top5 97.285156   BatchTime 0.528522   LR 0.000235
0.83586907
0.83604103
0.83573979
0.83541667
0.83492762
0.83454269
0.83475542
0.83497632
0.83481002
0.83667654
0.83677673
0.83685595
0.83706844
0.83663332
0.83668858
0.83664095
0.83651698
0.83653361
INFO - Training [13][   40/  196]   Loss 0.674926   Top1 76.435547   Top5 97.304688   BatchTime 0.489427   LR 0.000235
0.83640486
0.83598936
0.83597869
0.83592814
0.83608454
0.83590806
0.83579785
0.83544874
0.83543301
0.83530271
0.83519667
0.83518624
0.83524269
0.83522671
0.83533293
0.83551633
0.83542126
0.83516419
0.83485818
0.83482271
INFO - Training [13][   60/  196]   Loss 0.657891   Top1 77.050781   Top5 97.519531   BatchTime 0.454214   LR 0.000234
0.83481163
0.83480382
0.83498478
0.83449972
0.83459145
0.83468574
0.83468044
0.83486807
0.83487523
0.83462489
0.83454359
0.83442497
0.83432859
0.83412045
0.83396435
0.83405614
0.83393008
0.83388329
0.83363754
INFO - Training [13][   80/  196]   Loss 0.647436   Top1 77.573242   Top5 97.583008   BatchTime 0.449395   LR 0.000233
0.83317459
0.83299118
0.83261985
0.83249563
0.83219463
0.83193564
0.83209789
0.83183861
0.83141541
0.83137041
0.83069241
0.83049822
0.82997370
0.82981908
0.82991517
0.83007312
0.83010256
0.83021039
0.83027023
0.83016700
INFO - Training [13][  100/  196]   Loss 0.643899   Top1 77.656250   Top5 97.675781   BatchTime 0.439144   LR 0.000232
0.83001542
0.82994819
0.82976127
0.82970011
0.82983923
0.82950872
0.82991755
0.82960242
0.82944816
0.82940668
0.82918668
0.82894337
0.82868677
0.82854229
0.82871825
0.82812166
0.82843906
0.82840550
0.82857549
0.82805240
0.82784182
0.82785338
0.82783204
INFO - Training [13][  120/  196]   Loss 0.637840   Top1 77.822266   Top5 97.796224   BatchTime 0.437188   LR 0.000230
0.82857853
0.83005786
0.82986784
0.82937223
0.82957852
0.82932055
0.82969666
0.82983315
0.82990730
0.83067942
0.83059138
0.83039296
0.83053195
0.83052355
0.83037221
0.83062667
0.83079582
0.83079940
INFO - Training [13][  140/  196]   Loss 0.633193   Top1 78.030134   Top5 97.879464   BatchTime 0.438638   LR 0.000229
0.83185083
0.83650291
0.83945107
0.83900434
0.83876038
0.83884579
0.83872157
0.83819348
0.83786982
0.83791274
0.83793348
0.83782190
0.83842355
0.83823210
0.83833361
0.83805972
0.83776027
0.83779931
INFO - Training [13][  160/  196]   Loss 0.633527   Top1 77.963867   Top5 97.875977   BatchTime 0.439988   LR 0.000228
0.83755696
0.83727443
0.83692646
0.83667284
0.83596075
0.83524805
0.83484298
0.83443981
0.83411103
0.83369279
0.83320314
0.83291793
0.83254200
0.83232623
0.83181763
0.83182299
0.83180141
0.83186918
0.83181787
0.83143562
0.83161217
0.83183014
0.83176368
INFO - Training [13][  180/  196]   Loss 0.634812   Top1 77.879774   Top5 97.847222   BatchTime 0.440070   LR 0.000227
0.83175457
0.83168167
0.83179486
0.83207107
0.83256567
0.83285010
0.83267075
0.83258349
0.83329225
0.83278996
0.83289874
0.83304882
0.83307010
0.83298987
0.83277196
0.83283108
0.83274484
********************pre-trained*****************
INFO - ==> Top1: 77.818    Top5: 97.842    Loss: 0.635
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [13][   20/   40]   Loss 0.453233   Top1 85.253906   Top5 99.238281   BatchTime 0.130746
INFO - Validation [13][   40/   40]   Loss 0.451273   Top1 84.790000   Top5 99.350000   BatchTime 0.093266
features.0.conv.0 tensor(0.5347)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0752)
features.1.conv.6 tensor(0.0764)
features.2.conv.0 tensor(0.0446)
features.2.conv.3 tensor(0.3511)
features.2.conv.6 tensor(0.1872)
features.3.conv.0 tensor(0.0593)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1070)
features.4.conv.0 tensor(0.0345)
features.4.conv.3 tensor(0.3183)
features.4.conv.6 tensor(0.1740)
features.5.conv.0 tensor(0.3188)
features.5.conv.3 tensor(0.4358)
features.5.conv.6 tensor(0.1024)
features.6.conv.0 tensor(0.0534)
features.6.conv.3 tensor(0.0619)
features.6.conv.6 tensor(0.0850)
features.7.conv.0 tensor(0.0895)
features.7.conv.3 tensor(0.4433)
features.7.conv.6 tensor(0.2180)
features.8.conv.0 tensor(0.2500)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.1540)
features.9.conv.0 tensor(0.2637)
features.9.conv.3 tensor(0.5422)
features.9.conv.6 tensor(0.1563)
features.10.conv.0 tensor(0.0705)
features.10.conv.3 tensor(0.1001)
features.10.conv.6 tensor(0.0706)
features.11.conv.0 tensor(0.2842)
features.11.conv.3 tensor(0.6507)
features.11.conv.6 tensor(0.1899)
features.12.conv.0 tensor(0.3930)
features.12.conv.3 tensor(0.6543)
features.12.conv.6 tensor(0.5266)
features.13.conv.0 tensor(0.3119)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.1079)
features.14.conv.0 tensor(0.7048)
features.14.conv.3 tensor(0.8650)
features.14.conv.6 tensor(0.8319)
features.15.conv.0 tensor(0.7202)
features.15.conv.3 tensor(0.9021)
features.15.conv.6 tensor(0.9667)
features.16.conv.0 tensor(0.3949)
features.16.conv.3 tensor(0.8051)
features.16.conv.6 tensor(0.1115)
conv.0 tensor(0.0776)
tensor(804635.) 2188896.0
INFO - ==> Top1: 84.790    Top5: 99.350    Loss: 0.451
INFO - ==> Sparsity : 0.368
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
0.83240014
0.83220536
0.83252883
0.83246583
0.83198106
0.83205503
0.83193755
0.83171618
0.83149576
0.83112627
0.83090895
0.83052599
0.83027416
0.83010536
0.82974547
0.82973570
0.82952726
0.82994419
INFO - Training [14][   20/  196]   Loss 0.639997   Top1 77.773438   Top5 97.265625   BatchTime 0.560821   LR 0.000225
0.83214009
0.83231407
0.83210742
0.83186108
0.83129364
0.83094138
0.83016497
0.82996070
0.82970399
0.82952887
0.82943434
0.82919109
0.82888013
0.82881045
0.82871884
0.82863402
0.82886660
0.82891238
0.82934093
0.82932138
0.82914543
0.82917762
0.82904512
INFO - Training [14][   40/  196]   Loss 0.636892   Top1 77.666016   Top5 97.441406   BatchTime 0.497013   LR 0.000224
0.82903939
0.82886851
0.82857192
0.82842737
0.82842535
0.82815546
0.82800055
0.82760739
0.82757473
0.82752472
0.82733089
0.82713008
0.82679951
0.82659262
0.82648325
0.82619828
0.82573986
0.82557565
INFO - Training [14][   60/  196]   Loss 0.637443   Top1 77.714844   Top5 97.630208   BatchTime 0.474141   LR 0.000223
0.82552856
0.82520652
0.82506126
0.82482421
0.82435024
0.82425171
0.82429427
0.82445586
0.82433969
0.82433879
0.82364273
0.82366502
0.82415330
0.82455951
0.82502997
0.82548821
0.82560223
0.82632470
INFO - Training [14][   80/  196]   Loss 0.630088   Top1 78.002930   Top5 97.719727   BatchTime 0.466422   LR 0.000221
0.82718325
0.82867432
0.83157611
0.83481985
0.83519417
0.83520901
0.83516806
0.83522362
0.83507019
0.83506948
0.83487356
0.83473122
0.83457929
0.83434534
0.83419389
0.83404291
0.83392823
0.83369792
0.83379018
0.83377504
INFO - Training [14][  100/  196]   Loss 0.626176   Top1 78.300781   Top5 97.703125   BatchTime 0.455173   LR 0.000220
0.83385724
0.83373725
0.83381820
0.83380544
0.83385134
0.83402306
0.83362734
0.83347315
0.83359182
0.83377105
0.83376259
0.83354235
0.83368582
0.83360398
0.83365476
0.83341658
0.83320415
0.83303487
0.83244807
0.83158571
0.83141506
INFO - Training [14][  120/  196]   Loss 0.618959   Top1 78.580729   Top5 97.884115   BatchTime 0.442531   LR 0.000219
0.83133113
0.83142155
0.83149427
0.83125687
0.83127189
0.83110338
0.83151293
0.83147895
0.83241874
0.83222681
0.83218068
0.83195454
0.83160627
0.83150184
0.83141506
0.83125895
0.83136040
0.83122611
0.83112627
0.83137655
0.83140463
0.83142507
INFO - Training [14][  140/  196]   Loss 0.615827   Top1 78.694196   Top5 97.943638   BatchTime 0.445429   LR 0.000217
0.83169419
0.83174753
0.83173764
0.83167636
0.83150667
0.83131015
0.83135921
0.83111238
0.83090937
0.83049834
0.83046103
0.83040375
0.83031297
0.83026785
0.83019435
0.82997894
0.83004415
INFO - Training [14][  160/  196]   Loss 0.618675   Top1 78.579102   Top5 97.902832   BatchTime 0.449070   LR 0.000216
0.82968473
0.82976770
0.82961005
0.82938361
0.82934505
0.82911789
0.82890230
0.82822186
0.82794589
0.82798219
0.82750696
0.82707822
0.82630092
0.82599688
0.82679617
0.82700294
0.82613486
0.82574099
0.82533610
0.82539570
0.82526934
0.82505095
INFO - Training [14][  180/  196]   Loss 0.618953   Top1 78.552517   Top5 97.873264   BatchTime 0.448183   LR 0.000215
0.82480013
0.82446676
0.82437521
0.82450795
0.82493436
0.82496172
0.82501197
0.82611734
0.82588929
0.82563072
0.82505816
0.82491755
0.82514155
0.82504690
INFO - ==> Top1: 78.614    Top5: 97.862    Loss: 0.618
0.82490349
0.82479525
0.82454872
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 0.456257   Top1 84.453125   Top5 99.277344   BatchTime 0.163823
INFO - Validation [14][   40/   40]   Loss 0.448264   Top1 84.750000   Top5 99.470000   BatchTime 0.109878
INFO - ==> Top1: 84.750    Top5: 99.470    Loss: 0.448
INFO - ==> Sparsity : 0.369
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5451)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0838)
features.2.conv.0 tensor(0.0472)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.1748)
features.3.conv.0 tensor(0.0579)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1087)
features.4.conv.0 tensor(0.0369)
features.4.conv.3 tensor(0.3206)
features.4.conv.6 tensor(0.1727)
features.5.conv.0 tensor(0.3179)
features.5.conv.3 tensor(0.4346)
features.5.conv.6 tensor(0.1009)
features.6.conv.0 tensor(0.0452)
features.6.conv.3 tensor(0.0596)
features.6.conv.6 tensor(0.0837)
features.7.conv.0 tensor(0.0930)
features.7.conv.3 tensor(0.4421)
features.7.conv.6 tensor(0.2219)
features.8.conv.0 tensor(0.2239)
features.8.conv.3 tensor(0.5411)
features.8.conv.6 tensor(0.1439)
features.9.conv.0 tensor(0.2618)
features.9.conv.3 tensor(0.5448)
features.9.conv.6 tensor(0.1532)
features.10.conv.0 tensor(0.0675)
features.10.conv.3 tensor(0.1047)
features.10.conv.6 tensor(0.0719)
features.11.conv.0 tensor(0.3410)
features.11.conv.3 tensor(0.6510)
features.11.conv.6 tensor(0.1915)
features.12.conv.0 tensor(0.4988)
features.12.conv.3 tensor(0.6503)
features.12.conv.6 tensor(0.5431)
features.13.conv.0 tensor(0.3127)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.1063)
features.14.conv.0 tensor(0.7072)
features.14.conv.3 tensor(0.8624)
features.14.conv.6 tensor(0.8095)
features.15.conv.0 tensor(0.7317)
features.15.conv.3 tensor(0.9031)
features.15.conv.6 tensor(0.9661)
features.16.conv.0 tensor(0.3825)
features.16.conv.3 tensor(0.8051)
features.16.conv.6 tensor(0.1049)
conv.0 tensor(0.0783)
tensor(808501.) 2188896.0
0.82468295
0.82427496
0.82393742
0.82395148
0.82392842
0.82384777
0.82367820
0.82316864
0.82285076
0.82225925
0.82204068
0.82234001
0.82252860
0.82254648
0.82264221
0.82269412
INFO - Training [15][   20/  196]   Loss 0.602028   Top1 78.652344   Top5 97.558594   BatchTime 0.533057   LR 0.000212
0.82262552
0.82269114
0.82230496
0.82184458
0.82181698
0.82181615
0.82192159
0.82182109
0.82206410
0.82222944
0.82248318
0.82286239
0.82389730
0.82425433
0.82451844
0.82440889
0.82429528
0.82429785
0.82453424
0.82450455
INFO - Training [15][   40/  196]   Loss 0.612886   Top1 78.408203   Top5 97.753906   BatchTime 0.470893   LR 0.000211
0.82453829
0.82422358
0.82442456
0.82464713
0.82440728
0.82397342
0.82399356
0.82380486
0.82390076
0.82428396
0.82432038
0.82453710
0.82429910
0.82421660
0.82381099
0.82358295
0.82313746
0.82303518
0.82283449
0.82248372
0.82245928
0.82233530
0.82223582
INFO - Training [15][   60/  196]   Loss 0.605574   Top1 78.782552   Top5 97.858073   BatchTime 0.458315   LR 0.000209
0.82201087
0.82170475
0.82126325
0.82062292
0.82002234
0.81981057
0.81963354
0.81905478
0.81873447
0.81833804
0.81771225
0.81702614
0.81638068
0.81619346
0.81579280
0.81523788
0.81462514
0.81415659
INFO - Training [15][   80/  196]   Loss 0.610584   Top1 78.676758   Top5 97.871094   BatchTime 0.455469   LR 0.000208
0.81383121
0.81385028
0.81354606
0.81321919
0.81272781
0.81278700
0.81284267
0.81314749
0.81330484
0.81529301
0.81572366
0.81562322
0.81583554
0.81679684
0.81967717
0.82749623
0.82769710
0.82775587
0.82755470
0.82764524
0.82760483
INFO - Training [15][  100/  196]   Loss 0.604559   Top1 78.875000   Top5 97.937500   BatchTime 0.456156   LR 0.000206
0.82769269
0.82780647
0.82786494
0.82772124
0.82783866
0.82790935
0.82761949
0.82771832
0.82730740
0.82723010
0.82722145
0.82709289
0.82688344
0.82696944
0.82690293
0.82711601
0.82706738
0.82692254
0.82655007
0.82662266
0.82650387
INFO - Training [15][  120/  196]   Loss 0.601807   Top1 78.916016   Top5 98.059896   BatchTime 0.446455   LR 0.000205
0.82639748
0.82624805
0.82637495
0.82650667
0.82630432
0.82643175
0.82662898
0.82663721
0.82662332
0.82624501
0.82648659
0.82716244
0.82728714
0.82737178
0.82777685
0.82791883
0.82808000
0.82783711
0.82770920
INFO - Training [15][  140/  196]   Loss 0.599871   Top1 79.015067   Top5 98.077567   BatchTime 0.443354   LR 0.000203
0.82778782
0.82793760
0.82767081
0.82751679
0.82718492
0.82704419
0.82688749
0.82674855
0.82673949
0.82680559
0.82671839
0.82631767
0.82586259
0.82567811
0.82548892
0.82533592
0.82510006
0.82508856
INFO - Training [15][  160/  196]   Loss 0.602404   Top1 78.955078   Top5 98.061523   BatchTime 0.444907   LR 0.000201
0.82516181
0.82508254
0.82478118
0.82473540
0.82479054
0.82490361
0.82472181
0.82462078
0.82457578
0.82402658
0.82382709
0.82411575
0.82490277
0.82514709
0.82508659
0.82440007
0.82431400
0.82423395
0.82402474
0.82388604
0.82423526
INFO - Training [15][  180/  196]   Loss 0.602526   Top1 78.951823   Top5 97.992622   BatchTime 0.446935   LR 0.000200
0.82505351
0.82470310
0.82446009
0.82452047
0.82440174
0.82420701
0.82417744
0.82399088
0.82391793
0.82375932
0.82393670
0.82417381
0.82394171
0.82366258
0.82366127
0.82377356
0.82367247
0.82353705
INFO - ==> Top1: 79.004    Top5: 97.988    Loss: 0.601
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.82329893
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.461274   Top1 84.335938   Top5 99.355469   BatchTime 0.135256
INFO - Validation [15][   40/   40]   Loss 0.457815   Top1 84.120000   Top5 99.420000   BatchTime 0.095691
INFO - ==> Top1: 84.120    Top5: 99.420    Loss: 0.458
INFO - ==> Sparsity : 0.373
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.060   Top5: 99.360]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5486)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0773)
features.2.conv.0 tensor(0.0417)
features.2.conv.3 tensor(0.3341)
features.2.conv.6 tensor(0.1782)
features.3.conv.0 tensor(0.0532)
features.3.conv.3 tensor(0.0741)
features.3.conv.6 tensor(0.1070)
features.4.conv.0 tensor(0.0322)
features.4.conv.3 tensor(0.3142)
features.4.conv.6 tensor(0.1732)
features.5.conv.0 tensor(0.2861)
features.5.conv.3 tensor(0.4381)
features.5.conv.6 tensor(0.1024)
features.6.conv.0 tensor(0.0522)
features.6.conv.3 tensor(0.0631)
features.6.conv.6 tensor(0.0848)
features.7.conv.0 tensor(0.0911)
features.7.conv.3 tensor(0.4424)
features.7.conv.6 tensor(0.2130)
features.8.conv.0 tensor(0.2238)
features.8.conv.3 tensor(0.5440)
features.8.conv.6 tensor(0.1508)
features.9.conv.0 tensor(0.2623)
features.9.conv.3 tensor(0.5451)
features.9.conv.6 tensor(0.1599)
features.10.conv.0 tensor(0.0656)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.0719)
features.11.conv.0 tensor(0.3705)
features.11.conv.3 tensor(0.6501)
features.11.conv.6 tensor(0.1987)
features.12.conv.0 tensor(0.4627)
features.12.conv.3 tensor(0.6541)
features.12.conv.6 tensor(0.5192)
features.13.conv.0 tensor(0.3001)
features.13.conv.3 tensor(0.4867)
features.13.conv.6 tensor(0.1073)
features.14.conv.0 tensor(0.7152)
features.14.conv.3 tensor(0.8623)
features.14.conv.6 tensor(0.8394)
features.15.conv.0 tensor(0.7376)
features.15.conv.3 tensor(0.9038)
features.15.conv.6 tensor(0.9673)
features.16.conv.0 tensor(0.3751)
features.16.conv.3 tensor(0.8050)
features.16.conv.6 tensor(0.1144)
conv.0 tensor(0.0799)
tensor(815787.) 2188896.0
0.82314473
0.82348382
0.82341331
0.82317007
0.82294410
0.82291019
0.82229072
0.82196778
0.82155901
0.82124555
0.82104164
0.82105148
0.82097745
0.82084101
0.82098073
INFO - Training [16][   20/  196]   Loss 0.622821   Top1 78.203125   Top5 97.382812   BatchTime 0.533238   LR 0.000197
0.82058585
0.82035762
0.82062978
0.82054013
0.82034087
0.82041508
0.82007432
0.82011771
0.82039315
0.82075310
0.82091790
0.82108420
0.82138902
0.82150227
0.82303286
0.82339424
0.82446414
0.82438421
0.82452267
0.82454455
0.82418507
INFO - Training [16][   40/  196]   Loss 0.630314   Top1 77.890625   Top5 97.568359   BatchTime 0.474750   LR 0.000195
0.82413679
0.82431763
0.82403028
0.82404530
0.82450396
0.82523268
0.82618266
0.82803673
0.82926637
0.83110183
0.83126873
0.83129209
0.83083230
0.83075958
0.83077997
0.83080798
0.83105189
0.83262444
0.84144646
0.84330213
0.84327751
0.84304082
INFO - Training [16][   60/  196]   Loss 0.619030   Top1 78.294271   Top5 97.682292   BatchTime 0.467259   LR 0.000194
0.84305626
0.84325123
0.84323847
0.84348869
0.84356332
0.84322381
0.84319252
0.84311581
0.84288740
0.84264416
0.84266371
0.84232920
0.84215939
0.84195822
0.84152180
0.84131861
0.84125084
INFO - Training [16][   80/  196]   Loss 0.615338   Top1 78.530273   Top5 97.802734   BatchTime 0.464244   LR 0.000192
0.84117341
0.84086281
0.84081942
0.84065288
0.84047025
0.84045947
0.84008193
0.83991486
0.83957046
0.83928555
0.83925313
0.83890772
0.83894885
0.83864748
0.83855224
0.83818561
0.83795297
0.83763605
0.83739942
0.83688414
0.83661145
0.83648199
INFO - Training [16][  100/  196]   Loss 0.607534   Top1 78.812500   Top5 97.863281   BatchTime 0.464828   LR 0.000190
0.83640045
0.83654952
0.83678728
0.83644348
0.83640629
0.83617049
0.83577251
0.83536732
0.83514440
0.83494961
0.83460861
0.83431119
0.83453768
0.83448750
0.83406329
0.83396029
0.83391392
0.83397347
0.83353382
INFO - Training [16][  120/  196]   Loss 0.600297   Top1 79.026693   Top5 97.949219   BatchTime 0.457907   LR 0.000188
0.83303469
0.83259523
0.83248782
0.83259416
0.83260900
0.83260536
0.83261585
0.83273524
0.83269364
0.83271080
0.83301723
0.83385837
0.83777195
0.84210885
0.84262425
0.84271228
0.84272730
0.84260297
0.84244776
0.84230357
0.84195507
INFO - Training [16][  140/  196]   Loss 0.594596   Top1 79.257812   Top5 98.027344   BatchTime 0.447581   LR 0.000187
0.84222817
0.84231144
0.84224105
0.84207153
0.84249467
0.84228951
0.84189695
0.84178138
0.84168077
0.84149337
0.84141177
0.84130925
0.84134585
0.84124410
0.84131163
0.84140939
0.84125316
0.84099156
0.84062326
0.84058380
INFO - Training [16][  160/  196]   Loss 0.593770   Top1 79.304199   Top5 97.988281   BatchTime 0.452100   LR 0.000185
0.84022903
0.84019357
0.84016496
0.84006977
0.83996570
0.83965135
0.83955413
0.83952415
0.83954990
0.83962464
0.83992296
0.84010714
0.83982062
0.83971643
0.83953542
0.83975255
0.84236550
0.84317291
0.84294969
INFO - Training [16][  180/  196]   Loss 0.591375   Top1 79.364149   Top5 97.979601   BatchTime 0.450747   LR 0.000183
0.84258854
0.84265697
0.84228402
0.84267312
0.84249544
0.84240806
0.84217060
0.84189731
0.84175861
0.84183407
0.84218973
0.84180087
0.84173852
0.84138125
0.84138596
0.84158313
0.84118706
INFO - ==> Top1: 79.542    Top5: 98.018    Loss: 0.587
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.84186703
0.84171897
0.84171253
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.421510   Top1 86.191406   Top5 99.375000   BatchTime 0.131885
INFO - Validation [16][   40/   40]   Loss 0.406953   Top1 86.300000   Top5 99.500000   BatchTime 0.092973
INFO - ==> Top1: 86.300    Top5: 99.500    Loss: 0.407
INFO - ==> Sparsity : 0.370
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 86.300   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 85.710   Top5: 99.440]
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.1133)
features.1.conv.0 tensor(0.0404)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0725)
features.2.conv.0 tensor(0.0535)
features.2.conv.3 tensor(0.3326)
features.2.conv.6 tensor(0.1800)
features.3.conv.0 tensor(0.0501)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1122)
features.4.conv.0 tensor(0.0334)
features.4.conv.3 tensor(0.3171)
features.4.conv.6 tensor(0.1711)
features.5.conv.0 tensor(0.2832)
features.5.conv.3 tensor(0.4334)
features.5.conv.6 tensor(0.0999)
features.6.conv.0 tensor(0.0505)
features.6.conv.3 tensor(0.0584)
features.6.conv.6 tensor(0.0920)
features.7.conv.0 tensor(0.0949)
features.7.conv.3 tensor(0.4398)
features.7.conv.6 tensor(0.2096)
features.8.conv.0 tensor(0.2350)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.1481)
features.9.conv.0 tensor(0.2627)
features.9.conv.3 tensor(0.5457)
features.9.conv.6 tensor(0.1593)
features.10.conv.0 tensor(0.0626)
features.10.conv.3 tensor(0.1021)
features.10.conv.6 tensor(0.0707)
features.11.conv.0 tensor(0.3138)
features.11.conv.3 tensor(0.6491)
features.11.conv.6 tensor(0.2082)
features.12.conv.0 tensor(0.3298)
features.12.conv.3 tensor(0.6526)
features.12.conv.6 tensor(0.5320)
features.13.conv.0 tensor(0.3024)
features.13.conv.3 tensor(0.4877)
features.13.conv.6 tensor(0.1073)
features.14.conv.0 tensor(0.7178)
features.14.conv.3 tensor(0.8617)
features.14.conv.6 tensor(0.8523)
features.15.conv.0 tensor(0.7390)
features.15.conv.3 tensor(0.9042)
features.15.conv.6 tensor(0.9673)
features.16.conv.0 tensor(0.3730)
features.16.conv.3 tensor(0.8052)
features.16.conv.6 tensor(0.1167)
conv.0 tensor(0.0810)
tensor(810239.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
0.84183311
0.84183490
0.84186476
0.84171873
0.84194696
0.84217334
0.84202921
0.84175241
0.84170282
0.84168738
0.84164238
0.84130526
0.84105808
0.84077281
0.84062725
0.84035212
0.84010512
INFO - Training [17][   20/  196]   Loss 0.602414   Top1 79.082031   Top5 97.812500   BatchTime 0.488401   LR 0.000180
0.84017217
0.84016979
0.84022713
0.84004307
0.83961219
0.83942795
0.83959490
0.83951849
0.83908778
0.83890861
0.83907366
0.83907908
0.83921802
0.83934885
0.83934522
0.83945072
0.83912903
0.83893168
0.83854443
0.83853686
0.83857745
0.83839279
INFO - Training [17][   40/  196]   Loss 0.584010   Top1 79.814453   Top5 97.890625   BatchTime 0.470191   LR 0.000178
0.83835894
0.83854717
0.83858180
0.83848321
0.83826023
0.83823037
0.83811200
0.83789414
0.83740419
0.83696139
0.83717930
0.83730263
0.83707875
0.83686119
0.83682495
0.83674300
0.83666039
0.83641082
INFO - Training [17][   60/  196]   Loss 0.582849   Top1 79.817708   Top5 97.851562   BatchTime 0.473249   LR 0.000176
0.83608133
0.83608711
0.83700502
0.83709866
0.83664280
0.83669025
0.83651823
0.83609837
0.83565950
0.83536279
0.83508170
0.83494842
0.83455646
0.83467150
0.83579540
0.83627409
0.83592683
0.83571917
0.83600187
0.83569390
INFO - Training [17][   80/  196]   Loss 0.579705   Top1 79.887695   Top5 98.017578   BatchTime 0.474100   LR 0.000175
0.83604449
0.83576441
0.83521026
0.83516091
0.83512330
0.83462346
0.83448488
0.83480453
0.83481276
0.83475411
0.83441901
0.83419955
0.83374500
0.83342147
0.83319181
0.83292454
0.83314002
0.83273798
INFO - Training [17][  100/  196]   Loss 0.572380   Top1 80.125000   Top5 98.136719   BatchTime 0.466996   LR 0.000173
0.83227766
0.83182079
0.83169091
0.83170021
0.83176970
0.83143574
0.83106667
0.83078116
0.83044994
0.83034503
0.83033741
0.83039087
0.83010656
0.82993567
0.82951403
0.82934064
0.82902348
0.82879853
0.82847011
0.82839268
0.82836318
0.82806331
0.82777900
0.82733274
INFO - Training [17][  120/  196]   Loss 0.566618   Top1 80.328776   Top5 98.216146   BatchTime 0.460780   LR 0.000171
0.82720208
0.82694751
0.82649040
0.82610863
0.82601064
0.82511002
0.82449669
0.82370222
0.82240540
0.82165390
0.82100487
0.82077175
0.82084137
0.81998539
0.81987441
0.82000262
0.81997097
INFO - Training [17][  140/  196]   Loss 0.565495   Top1 80.334821   Top5 98.272879   BatchTime 0.458738   LR 0.000169
0.82021368
0.82002401
0.81970435
0.81980604
0.81967598
0.81957203
0.81955105
0.81939578
0.81908816
0.81916273
0.81924134
0.81920010
0.81911665
0.81866205
0.81838489
0.81827760
0.81792158
0.81766373
0.81747735
0.81741774
0.81732398
0.81692111
INFO - Training [17][  160/  196]   Loss 0.566085   Top1 80.253906   Top5 98.234863   BatchTime 0.461037   LR 0.000167
0.81646001
0.81637669
0.81687504
0.81696862
0.81672907
0.81656647
0.81664592
0.81663811
0.81679201
0.81763607
0.82036358
0.82459086
0.82462651
0.82427424
0.82369435
0.82334948
0.82325679
0.82275838
0.82253128
0.82249933
0.82248020
INFO - Training [17][  180/  196]   Loss 0.565645   Top1 80.238715   Top5 98.213976   BatchTime 0.460468   LR 0.000165
0.82272840
0.82264531
0.82293212
0.82329947
0.82289350
0.82260412
0.82264179
0.82249218
0.82249469
0.82202280
0.82136101
0.82081407
0.82018620
0.81914657
INFO - ==> Top1: 80.318    Top5: 98.226    Loss: 0.564
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.81845307
0.81777257
0.81729782
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [17][   20/   40]   Loss 0.415222   Top1 85.468750   Top5 99.492188   BatchTime 0.138707
INFO - Validation [17][   40/   40]   Loss 0.397608   Top1 86.130000   Top5 99.560000   BatchTime 0.105340
INFO - ==> Top1: 86.130    Top5: 99.560    Loss: 0.398
INFO - ==> Sparsity : 0.380
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 86.300   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 86.130   Top5: 99.560]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 85.750   Top5: 99.410]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5625)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0593)
features.2.conv.3 tensor(0.3318)
features.2.conv.6 tensor(0.1701)
features.3.conv.0 tensor(0.0475)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1115)
features.4.conv.0 tensor(0.0350)
features.4.conv.3 tensor(0.3206)
features.4.conv.6 tensor(0.1720)
features.5.conv.0 tensor(0.2855)
features.5.conv.3 tensor(0.4363)
features.5.conv.6 tensor(0.0985)
features.6.conv.0 tensor(0.0487)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0868)
features.7.conv.0 tensor(0.0957)
features.7.conv.3 tensor(0.4624)
features.7.conv.6 tensor(0.3858)
features.8.conv.0 tensor(0.2273)
features.8.conv.3 tensor(0.5431)
features.8.conv.6 tensor(0.1445)
features.9.conv.0 tensor(0.2707)
features.9.conv.3 tensor(0.5434)
features.9.conv.6 tensor(0.1557)
features.10.conv.0 tensor(0.0662)
features.10.conv.3 tensor(0.0975)
features.10.conv.6 tensor(0.0697)
features.11.conv.0 tensor(0.3725)
features.11.conv.3 tensor(0.6503)
features.11.conv.6 tensor(0.2095)
features.12.conv.0 tensor(0.4185)
features.12.conv.3 tensor(0.6537)
features.12.conv.6 tensor(0.5229)
features.13.conv.0 tensor(0.3035)
features.13.conv.3 tensor(0.4871)
features.13.conv.6 tensor(0.1069)
features.14.conv.0 tensor(0.7240)
features.14.conv.3 tensor(0.8590)
features.14.conv.6 tensor(0.8894)
features.15.conv.0 tensor(0.7443)
features.15.conv.3 tensor(0.9042)
features.15.conv.6 tensor(0.9670)
features.16.conv.0 tensor(0.3864)
features.16.conv.3 tensor(0.8049)
features.16.conv.6 tensor(0.1152)
conv.0 tensor(0.0818)
tensor(831570.) 2188896.0
0.81684375
0.81631857
0.81588274
0.81518131
0.81485105
0.81412119
0.81355852
0.81261337
0.81179219
0.81201935
0.81163651
0.81105006
0.81076330
0.81096762
0.81099206
0.81078976
0.81067121
0.81062824
INFO - Training [18][   20/  196]   Loss 0.575032   Top1 80.488281   Top5 97.832031   BatchTime 0.523595   LR 0.000162
0.81062204
0.81026143
0.80995268
0.80979967
0.80960649
0.80927330
0.80908281
0.80909729
0.80944598
0.81000572
0.80965245
0.80984020
0.80983198
0.80979919
0.80949455
0.80924177
0.80923325
INFO - Training [18][   40/  196]   Loss 0.563340   Top1 80.888672   Top5 97.910156   BatchTime 0.486010   LR 0.000160
0.80927467
0.80909753
0.80876404
0.80872005
0.80876970
0.80887359
0.80911720
0.80917162
0.80892259
0.80876708
0.80853176
0.80855715
0.80827498
0.80821514
0.80828887
0.80801529
0.80815399
0.80813992
0.80787516
0.80779171
0.80788743
0.80773175
INFO - Training [18][   60/  196]   Loss 0.558639   Top1 80.904948   Top5 97.955729   BatchTime 0.478052   LR 0.000158
0.80761933
0.80759341
0.80779570
0.80765998
0.80766320
0.80781037
0.80779648
0.80772930
0.80794531
0.80755079
0.80741125
0.80724615
0.80674815
0.80671120
0.80642807
0.80741453
0.80723286
0.80722314
0.80710137
0.80717212
0.80709803
0.80711222
0.80700749
INFO - Training [18][   80/  196]   Loss 0.557119   Top1 80.893555   Top5 98.144531   BatchTime 0.468686   LR 0.000156
0.80670238
0.80681986
0.80682027
0.80614763
0.80545998
0.80490422
0.80454975
0.80478710
0.80492514
0.80481672
0.80445313
0.80400240
0.80405325
0.80457431
0.80477881
0.80476409
0.80507761
0.80513149
0.80512094
INFO - Training [18][  100/  196]   Loss 0.548389   Top1 81.097656   Top5 98.218750   BatchTime 0.460678   LR 0.000154
0.80525118
0.80537772
0.80526125
0.80518281
0.80531460
0.80512303
0.80635679
0.80633670
0.80642265
0.80631840
0.80634254
0.80585164
0.80498379
0.80458790
0.80420154
0.80374795
0.80337590
INFO - Training [18][  120/  196]   Loss 0.546827   Top1 81.113281   Top5 98.271484   BatchTime 0.459572   LR 0.000152
0.80285966
0.80233818
0.80150670
0.80067796
0.80020267
0.79918599
0.79844809
0.79774129
0.79730076
0.79723686
0.79674190
0.79603833
0.79581070
0.79506809
0.79413986
0.79399830
0.79404879
0.79430294
0.79454255
0.79514569
0.79620415
INFO - Training [18][  140/  196]   Loss 0.546573   Top1 81.146763   Top5 98.331473   BatchTime 0.461506   LR 0.000150
0.79794860
0.79977280
0.80016339
0.79998606
0.79971933
0.79986942
0.80009907
0.79999506
0.80038697
0.80052799
0.80050904
0.80057472
0.80034506
0.80024725
0.80043900
0.80023462
0.79972517
0.79990566
INFO - Training [18][  160/  196]   Loss 0.546813   Top1 81.152344   Top5 98.303223   BatchTime 0.459321   LR 0.000148
0.80004257
0.79972816
0.79937094
0.79887486
0.79864424
0.79858178
0.79864788
0.79855406
0.79856455
0.79824424
0.79822159
0.79789966
0.79788703
0.79815662
0.79764444
0.79772109
0.79785126
0.79769617
0.79757404
0.79738092
0.79758745
0.79731381
0.79727691
INFO - Training [18][  180/  196]   Loss 0.545320   Top1 81.100260   Top5 98.294271   BatchTime 0.457634   LR 0.000146
0.79734105
0.79733479
0.79752719
0.79752290
0.79789817
0.79860687
0.79845810
0.79807281
0.79778630
0.79751271
0.79723513
0.79722250
0.79695445
0.79656833
0.79725939
0.79787356
0.79767787
0.79743248
INFO - ==> Top1: 81.112    Top5: 98.292    Loss: 0.545
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [18][   20/   40]   Loss 0.406645   Top1 86.210938   Top5 99.394531   BatchTime 0.147298
INFO - Validation [18][   40/   40]   Loss 0.392571   Top1 86.640000   Top5 99.550000   BatchTime 0.101063
INFO - ==> Top1: 86.640    Top5: 99.550    Loss: 0.393
INFO - ==> Sparsity : 0.389
INFO - Scoreboard best 1 ==> Epoch [18][Top1: 86.640   Top5: 99.550]
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 86.300   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 86.130   Top5: 99.560]
features.0.conv.0 tensor(0.5694)
features.0.conv.3 tensor(0.1191)
features.1.conv.0 tensor(0.0384)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0773)
features.2.conv.0 tensor(0.0666)
features.2.conv.3 tensor(0.3395)
features.2.conv.6 tensor(0.1742)
features.3.conv.0 tensor(0.0532)
features.3.conv.3 tensor(0.0849)
features.3.conv.6 tensor(0.1148)
features.4.conv.0 tensor(0.0386)
features.4.conv.3 tensor(0.3148)
features.4.conv.6 tensor(0.1729)
features.5.conv.0 tensor(0.2866)
features.5.conv.3 tensor(0.4294)
features.5.conv.6 tensor(0.0959)
features.6.conv.0 tensor(0.0498)
features.6.conv.3 tensor(0.0515)
features.6.conv.6 tensor(0.0859)
features.7.conv.0 tensor(0.0938)
features.7.conv.3 tensor(0.4444)
features.7.conv.6 tensor(0.4438)
features.8.conv.0 tensor(0.2270)
features.8.conv.3 tensor(0.5437)
features.8.conv.6 tensor(0.1458)
features.9.conv.0 tensor(0.2727)
features.9.conv.3 tensor(0.5425)
features.9.conv.6 tensor(0.1591)
features.10.conv.0 tensor(0.0628)
features.10.conv.3 tensor(0.1004)
features.10.conv.6 tensor(0.0711)
features.11.conv.0 tensor(0.3811)
features.11.conv.3 tensor(0.6497)
features.11.conv.6 tensor(0.2151)
features.12.conv.0 tensor(0.4366)
features.12.conv.3 tensor(0.6539)
features.12.conv.6 tensor(0.5210)
features.13.conv.0 tensor(0.2353)
features.13.conv.3 tensor(0.4873)
features.13.conv.6 tensor(0.1058)
features.14.conv.0 tensor(0.7299)
features.14.conv.3 tensor(0.8589)
features.14.conv.6 tensor(0.8947)
features.15.conv.0 tensor(0.7459)
features.15.conv.3 tensor(0.9044)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.4949)
features.16.conv.3 tensor(0.8053)
features.16.conv.6 tensor(0.1216)
conv.0 tensor(0.0835)
tensor(852284.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
0.79717511
0.79678357
0.79652345
0.79634029
0.79643428
0.79654449
0.79654032
0.79654497
0.79674178
0.79648274
0.79629654
0.79614806
0.79574829
0.79548657
0.79522228
0.79513782
0.79502279
INFO - Training [19][   20/  196]   Loss 0.560304   Top1 80.429688   Top5 97.792969   BatchTime 0.472340   LR 0.000143
0.79518735
0.79503107
0.79563880
0.79526991
0.79485095
0.79426175
0.79389852
0.79348576
0.79344952
0.79332882
0.79317623
0.79341811
0.79333550
0.79359943
0.79375547
0.79392004
0.79445726
0.79516786
0.79578412
0.79690963
INFO - Training [19][   40/  196]   Loss 0.556115   Top1 80.664062   Top5 98.085938   BatchTime 0.441908   LR 0.000141
0.80049998
0.80862969
0.81005329
0.80977529
0.80982810
0.80976802
0.80976534
0.80973983
0.80960107
0.80935824
0.80934691
0.80930370
0.80913633
0.80924565
0.80942589
0.80940968
0.80923069
0.80905265
0.80923754
0.80939877
0.80898505
INFO - Training [19][   60/  196]   Loss 0.548282   Top1 80.950521   Top5 98.190104   BatchTime 0.417323   LR 0.000139
0.80888849
0.80852562
0.80840671
0.80818021
0.80798960
0.80804771
0.80823088
0.80804724
0.80807173
0.80821735
0.80867946
0.80878204
0.80842412
0.80822366
0.80809182
0.80813986
0.80798352
0.80749261
INFO - Training [19][   80/  196]   Loss 0.547399   Top1 80.927734   Top5 98.305664   BatchTime 0.397397   LR 0.000137
0.80733025
0.80690730
0.80639511
0.80632645
0.80639255
0.80629128
0.80612677
0.80594265
0.80673558
0.80716765
0.80750805
0.80768102
0.80769908
0.80767339
0.80739951
0.80737233
0.80746424
0.80771691
0.80772543
0.80759615
0.80748761
0.80763382
INFO - Training [19][  100/  196]   Loss 0.542998   Top1 81.113281   Top5 98.343750   BatchTime 0.390936   LR 0.000135
0.80746698
0.80727863
0.80701810
0.80691093
0.80687612
0.80684054
0.80680263
0.80678231
0.80661440
0.80631518
0.80617505
0.80622548
0.80592936
0.80601734
0.80632281
0.80574352
0.80577421
0.80571556
0.80572832
0.80545926
0.80517352
0.80506718
INFO - Training [19][  120/  196]   Loss 0.537889   Top1 81.380208   Top5 98.424479   BatchTime 0.387853   LR 0.000133
0.80479890
0.80453020
0.80434543
0.80460042
0.80436397
0.80433995
0.80428660
0.80428994
0.80382442
0.80363232
0.80371678
0.80383682
0.80404764
0.80417871
0.80413008
0.80448610
INFO - Training [19][  140/  196]   Loss 0.536030   Top1 81.414621   Top5 98.479353   BatchTime 0.385930   LR 0.000131
0.80542147
0.80735141
0.80890310
0.80917871
0.80875182
0.80885446
0.80870271
0.80889493
0.80890906
0.80890274
0.80884552
0.80911875
0.80918854
0.80912006
0.80884320
0.80885434
0.80867887
0.80840278
0.80827320
0.80819571
0.80814767
INFO - Training [19][  160/  196]   Loss 0.539066   Top1 81.237793   Top5 98.420410   BatchTime 0.384216   LR 0.000129
0.80798841
0.80800158
0.80801451
0.80787468
0.80782765
0.80770606
0.80765367
0.80735880
0.80731386
0.80693209
0.80664545
0.80667043
0.80646610
0.80641711
0.80630273
0.80662042
0.80648232
0.80607420
0.80597293
0.80622065
0.80616045
0.80577904
0.80522782
INFO - Training [19][  180/  196]   Loss 0.537933   Top1 81.234809   Top5 98.383247   BatchTime 0.380874   LR 0.000127
0.80547011
0.80598992
0.80626208
0.80646467
0.80629992
0.80624783
0.80625850
0.80588496
0.80579984
0.80549788
0.80519974
0.80517125
0.80524558
0.80507052
0.80486280
0.80455643
INFO - ==> Top1: 81.262    Top5: 98.340    Loss: 0.537
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 0.386001   Top1 87.031250   Top5 99.355469   BatchTime 0.130393
INFO - Validation [19][   40/   40]   Loss 0.378546   Top1 87.220000   Top5 99.500000   BatchTime 0.091924
INFO - ==> Top1: 87.220    Top5: 99.500    Loss: 0.379
INFO - ==> Sparsity : 0.382
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 87.220   Top5: 99.500]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 86.640   Top5: 99.550]
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 86.300   Top5: 99.500]
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.1250)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0720)
features.2.conv.0 tensor(0.0530)
features.2.conv.3 tensor(0.3395)
features.2.conv.6 tensor(0.1858)
features.3.conv.0 tensor(0.0576)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1148)
features.4.conv.0 tensor(0.0404)
features.4.conv.3 tensor(0.3148)
features.4.conv.6 tensor(0.1689)
features.5.conv.0 tensor(0.2865)
features.5.conv.3 tensor(0.4317)
features.5.conv.6 tensor(0.0998)
features.6.conv.0 tensor(0.0547)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0850)
features.7.conv.0 tensor(0.0938)
features.7.conv.3 tensor(0.4392)
features.7.conv.6 tensor(0.4475)
features.8.conv.0 tensor(0.2347)
features.8.conv.3 tensor(0.5428)
features.8.conv.6 tensor(0.1472)
features.9.conv.0 tensor(0.2663)
features.9.conv.3 tensor(0.5414)
features.9.conv.6 tensor(0.1591)
features.10.conv.0 tensor(0.0609)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0702)
features.11.conv.0 tensor(0.3962)
features.11.conv.3 tensor(0.6507)
features.11.conv.6 tensor(0.2142)
features.12.conv.0 tensor(0.4359)
features.12.conv.3 tensor(0.6547)
features.12.conv.6 tensor(0.5161)
features.13.conv.0 tensor(0.2381)
features.13.conv.3 tensor(0.4929)
features.13.conv.6 tensor(0.1055)
features.14.conv.0 tensor(0.7391)
features.14.conv.3 tensor(0.8565)
features.14.conv.6 tensor(0.8701)
features.15.conv.0 tensor(0.7508)
features.15.conv.3 tensor(0.9041)
features.15.conv.6 tensor(0.9671)
features.16.conv.0 tensor(0.3928)
features.16.conv.3 tensor(0.8046)
features.16.conv.6 tensor(0.1243)
conv.0 tensor(0.0834)
tensor(836381.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.80473870
0.80461520
0.80433971
0.80411375
0.80413526
0.80422187
0.80362839
0.80341232
0.80384302
0.80908251
0.81063932
0.81005698
0.81046307
0.81026173
0.80992502
0.80926490
0.80885941
0.80869478
INFO - Training [20][   20/  196]   Loss 0.529941   Top1 81.250000   Top5 98.046875   BatchTime 0.459440   LR 0.000123
0.80857164
0.80879807
0.80877489
0.80857384
0.80828792
0.80822015
0.80786031
0.80740541
0.80699158
0.80668616
0.80657166
0.80631071
0.80613762
0.80541617
0.80505711
0.80462813
0.80424845
INFO - Training [20][   40/  196]   Loss 0.536847   Top1 81.123047   Top5 98.154297   BatchTime 0.408239   LR 0.000121
0.80409276
0.80384624
0.80374080
0.80330342
0.80395252
0.80333680
0.80305922
0.80224097
0.80189854
0.80184609
0.80172414
0.80151856
0.80133426
0.80156863
0.80200452
0.80313158
0.80293024
0.80289233
0.80245477
0.80227113
0.80223966
INFO - Training [20][   60/  196]   Loss 0.535094   Top1 81.269531   Top5 98.144531   BatchTime 0.400778   LR 0.000119
0.80213785
0.80161566
0.80130947
0.80109394
0.80092299
0.80075419
0.80044019
0.80032808
0.80023140
0.80035287
0.80039036
0.80067593
0.80033422
0.80059993
0.80063570
0.80053085
0.80034405
0.79989815
0.79992199
0.80004936
0.80009764
INFO - Training [20][   80/  196]   Loss 0.535279   Top1 81.298828   Top5 98.208008   BatchTime 0.393801   LR 0.000117
0.79990363
0.79990041
0.80025345
0.80008990
0.80223519
0.80270678
0.80264676
0.80278945
0.80270863
0.80304164
0.80330741
0.80359703
0.80361736
0.80392355
0.80422759
0.80390215
0.80353749
0.80330831
0.80294490
0.80239189
0.80150479
0.80221993
INFO - Training [20][  100/  196]   Loss 0.526422   Top1 81.476562   Top5 98.269531   BatchTime 0.387313   LR 0.000115
0.80248690
0.80264485
0.80263305
0.80245167
0.80205381
0.80183911
0.80187386
0.80181193
0.80169398
0.80159342
0.80169332
0.80157483
0.80128437
0.80109400
0.80099458
0.80082440
INFO - Training [20][  120/  196]   Loss 0.519574   Top1 81.699219   Top5 98.313802   BatchTime 0.384755   LR 0.000113
0.80036169
0.80039704
0.80018097
0.79985964
0.79962564
0.79936641
0.79923046
0.79920411
0.79923779
0.79900891
0.79900205
0.79876417
0.79858476
0.79831338
0.79789752
0.79767913
0.79760438
0.79739958
0.79733324
0.79738182
0.79727888
0.79690510
INFO - Training [20][  140/  196]   Loss 0.520879   Top1 81.637835   Top5 98.381696   BatchTime 0.385097   LR 0.000111
0.79684436
0.79672402
0.79658878
0.79644442
0.79636997
0.79642206
0.79667562
0.79634118
0.79598159
0.79599452
0.79603171
0.79571682
0.79546601
0.79483360
0.79472959
0.79480761
0.79458773
0.79446822
0.79381227
0.79380184
0.79355156
INFO - Training [20][  160/  196]   Loss 0.522446   Top1 81.621094   Top5 98.383789   BatchTime 0.382679   LR 0.000109
0.79341269
0.79294193
0.79275852
0.79286355
0.79267079
0.79268140
0.79251146
0.79251558
0.79205066
0.79197949
0.79204214
0.79223794
0.79210228
0.79195148
0.79231286
0.79234862
0.79228932
INFO - Training [20][  180/  196]   Loss 0.522937   Top1 81.647135   Top5 98.350694   BatchTime 0.378731   LR 0.000107
0.79245925
0.79258078
0.79221189
0.79215974
0.79204887
0.79210889
0.79197937
0.79192114
0.79173672
0.79178816
0.79183805
0.79173928
0.79194337
0.79188067
0.79144210
0.79130363
0.79134905
INFO - ==> Top1: 81.658    Top5: 98.340    Loss: 0.523
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79204732
0.79386163
0.79558754
0.79750180
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [20][   20/   40]   Loss 0.382878   Top1 87.402344   Top5 99.453125   BatchTime 0.131923
INFO - Validation [20][   40/   40]   Loss 0.377274   Top1 87.390000   Top5 99.600000   BatchTime 0.093817
INFO - ==> Top1: 87.390    Top5: 99.600    Loss: 0.377
INFO - ==> Sparsity : 0.391
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 87.390   Top5: 99.600]
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 87.220   Top5: 99.500]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 86.640   Top5: 99.550]
features.0.conv.0 tensor(0.5729)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0712)
features.2.conv.0 tensor(0.0570)
features.2.conv.3 tensor(0.3387)
features.2.conv.6 tensor(0.1843)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1128)
features.4.conv.0 tensor(0.0342)
features.4.conv.3 tensor(0.3137)
features.4.conv.6 tensor(0.1714)
features.5.conv.0 tensor(0.2868)
features.5.conv.3 tensor(0.4352)
features.5.conv.6 tensor(0.1021)
features.6.conv.0 tensor(0.0537)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0856)
features.7.conv.0 tensor(0.0951)
features.7.conv.3 tensor(0.4410)
features.7.conv.6 tensor(0.4540)
features.8.conv.0 tensor(0.3110)
features.8.conv.3 tensor(0.5457)
features.8.conv.6 tensor(0.1471)
features.9.conv.0 tensor(0.2738)
features.9.conv.3 tensor(0.5408)
features.9.conv.6 tensor(0.1575)
features.10.conv.0 tensor(0.0637)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0700)
features.11.conv.0 tensor(0.3240)
features.11.conv.3 tensor(0.6537)
features.11.conv.6 tensor(0.1998)
features.12.conv.0 tensor(0.4293)
features.12.conv.3 tensor(0.6547)
features.12.conv.6 tensor(0.5188)
features.13.conv.0 tensor(0.2451)
features.13.conv.3 tensor(0.4932)
features.13.conv.6 tensor(0.1051)
features.14.conv.0 tensor(0.7363)
features.14.conv.3 tensor(0.8558)
features.14.conv.6 tensor(0.8793)
features.15.conv.0 tensor(0.7594)
features.15.conv.3 tensor(0.9042)
features.15.conv.6 tensor(0.9670)
features.16.conv.0 tensor(0.4989)
features.16.conv.3 tensor(0.8051)
features.16.conv.6 tensor(0.1342)
conv.0 tensor(0.0842)
tensor(855971.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
0.79921263
0.80359811
0.80357271
0.80337638
0.80315411
0.80314672
0.80315983
0.80330366
0.80309892
0.80302161
0.80305618
0.80288035
0.80276060
0.80262947
0.80262333
0.80241585
0.80225807
INFO - Training [21][   20/  196]   Loss 0.525300   Top1 81.406250   Top5 97.988281   BatchTime 0.405993   LR 0.000104
0.80240148
0.80206472
0.80215341
0.80206692
0.80199534
0.80180854
0.80182648
0.80182844
0.80174369
0.80155259
0.80173689
0.80335820
0.80335432
0.80363709
0.80347878
0.80335474
0.80333614
0.80308801
0.80301905
0.80307835
0.80247355
0.80249298
INFO - Training [21][   40/  196]   Loss 0.525506   Top1 81.464844   Top5 98.154297   BatchTime 0.386377   LR 0.000102
0.80193734
0.80140787
0.80117631
0.80132711
0.80094284
0.80077583
0.80071348
0.80070174
0.80082268
0.80058289
0.80019718
0.80005562
0.79992342
0.79978752
0.79976648
0.79966748
0.79936910
0.79917663
0.80046421
0.80070025
INFO - Training [21][   60/  196]   Loss 0.519247   Top1 81.731771   Top5 98.222656   BatchTime 0.388469   LR 0.000100
0.80084419
0.80123675
0.80084050
0.80067170
0.80077946
0.80080616
0.80062807
0.80061734
0.80053759
0.80047309
0.80043018
0.80057311
0.80087245
0.80059582
0.80042464
0.80023861
INFO - Training [21][   80/  196]   Loss 0.516472   Top1 81.840820   Top5 98.349609   BatchTime 0.384802   LR 0.000098
0.79998881
0.79973435
0.79970473
0.79970640
0.79980493
0.79995871
0.79985541
0.79998130
0.80038053
0.80081421
0.80091566
0.80086803
0.80102617
0.80068624
0.80049193
0.80055928
0.80037725
0.80030811
0.80024302
0.79987216
0.79990536
0.79981333
INFO - Training [21][  100/  196]   Loss 0.515187   Top1 81.851562   Top5 98.339844   BatchTime 0.379911   LR 0.000096
0.80006844
0.80020577
0.79994452
0.79993343
0.80049628
0.80105186
0.80119795
0.80111122
0.80105305
0.80112195
0.80124116
0.80137575
0.80148673
0.80114019
0.80105549
0.80125421
0.80092067
0.80055285
0.80089223
0.80087143
0.80077732
0.80075729
0.80089724
INFO - Training [21][  120/  196]   Loss 0.510440   Top1 81.956380   Top5 98.414714   BatchTime 0.376593   LR 0.000094
0.80096513
0.80067861
0.80043858
0.80042064
0.80053407
0.80030876
0.80038911
0.80006123
0.79956782
0.79937923
0.79953831
0.79932272
0.79926401
0.79910320
0.79895073
0.79890299
INFO - Training [21][  140/  196]   Loss 0.506523   Top1 82.061942   Top5 98.493304   BatchTime 0.373939   LR 0.000092
0.79878074
0.79885799
0.79878944
0.79882026
0.79896200
0.79882878
0.79863542
0.79863429
0.79848546
0.79832619
0.79818600
0.79810661
0.79811531
0.79758924
0.79655093
0.79627883
0.79626447
0.79608446
0.79583210
0.79544175
0.79527289
0.79494196
INFO - Training [21][  160/  196]   Loss 0.510274   Top1 82.058105   Top5 98.452148   BatchTime 0.374188   LR 0.000090
0.79443777
0.79400063
0.79351574
0.79354274
0.79354692
0.79346871
0.79323161
0.79290468
0.79262763
0.79244572
0.79225945
0.79244030
0.79256070
0.79240060
0.79239041
0.79219288
0.79226965
0.79219973
0.79184216
0.79123116
0.79105228
0.79094291
INFO - Training [21][  180/  196]   Loss 0.508906   Top1 82.105035   Top5 98.465712   BatchTime 0.373245   LR 0.000088
0.79110312
0.79124624
0.79169643
0.79157352
0.79142511
0.79101616
0.79053795
0.79039437
0.79013544
0.79001778
0.79024088
0.79038131
0.79039091
0.79041976
0.79025918
0.79005110
INFO - ==> Top1: 82.234    Top5: 98.466    Loss: 0.506
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 0.373317   Top1 87.636719   Top5 99.570312   BatchTime 0.132695
INFO - Validation [21][   40/   40]   Loss 0.364989   Top1 87.640000   Top5 99.610000   BatchTime 0.093778
INFO - ==> Top1: 87.640    Top5: 99.610    Loss: 0.365
INFO - ==> Sparsity : 0.394
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 87.640   Top5: 99.610]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 87.390   Top5: 99.600]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 87.220   Top5: 99.500]
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0734)
features.2.conv.0 tensor(0.0613)
features.2.conv.3 tensor(0.3326)
features.2.conv.6 tensor(0.1843)
features.3.conv.0 tensor(0.0521)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1094)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.3084)
features.4.conv.6 tensor(0.1722)
features.5.conv.0 tensor(0.2853)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.1077)
features.6.conv.0 tensor(0.0552)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0856)
features.7.conv.0 tensor(0.0920)
features.7.conv.3 tensor(0.4401)
features.7.conv.6 tensor(0.4453)
features.8.conv.0 tensor(0.2575)
features.8.conv.3 tensor(0.5440)
features.8.conv.6 tensor(0.1472)
features.9.conv.0 tensor(0.2741)
features.9.conv.3 tensor(0.5425)
features.9.conv.6 tensor(0.1595)
features.10.conv.0 tensor(0.0638)
features.10.conv.3 tensor(0.0955)
features.10.conv.6 tensor(0.0697)
features.11.conv.0 tensor(0.3860)
features.11.conv.3 tensor(0.6522)
features.11.conv.6 tensor(0.2016)
features.12.conv.0 tensor(0.3960)
features.12.conv.3 tensor(0.6545)
features.12.conv.6 tensor(0.5274)
features.13.conv.0 tensor(0.2342)
features.13.conv.3 tensor(0.4927)
features.13.conv.6 tensor(0.1056)
features.14.conv.0 tensor(0.7441)
features.14.conv.3 tensor(0.8549)
features.14.conv.6 tensor(0.8888)
features.15.conv.0 tensor(0.7619)
features.15.conv.3 tensor(0.9038)
features.15.conv.6 tensor(0.9663)
features.16.conv.0 tensor(0.5157)
features.16.conv.3 tensor(0.8046)
features.16.conv.6 tensor(0.1357)
conv.0 tensor(0.0848)
tensor(862192.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.79008442
0.78942376
0.78994268
0.79083252
0.79049003
0.79031140
0.79007107
0.79008597
0.78986967
0.78962922
0.78955102
0.78946000
0.78936237
0.78910482
0.78880811
INFO - Training [22][   20/  196]   Loss 0.507532   Top1 82.402344   Top5 98.203125   BatchTime 0.467415   LR 0.000085
0.78878766
0.78871018
0.78835374
0.78834212
0.78844911
0.78834110
0.78812838
0.78797942
0.78796637
0.78810138
0.78813910
0.78787190
0.78765410
0.78747404
0.78746480
0.78742909
0.78731978
0.78777635
0.78826880
0.78856903
0.78853792
INFO - Training [22][   40/  196]   Loss 0.510535   Top1 82.373047   Top5 98.183594   BatchTime 0.429967   LR 0.000083
0.78866816
0.78845215
0.78832370
0.78847438
0.78851223
0.78868914
0.78934914
0.79815745
0.80171746
0.80130351
0.80120826
0.80110055
0.80032277
0.80068207
0.80123425
0.80135155
0.80162913
0.80165935
0.80144399
0.80122143
0.80118614
INFO - Training [22][   60/  196]   Loss 0.503702   Top1 82.662760   Top5 98.222656   BatchTime 0.413497   LR 0.000081
0.80124950
0.80079573
0.80055338
0.80065715
0.80044615
0.80028206
0.79986191
0.79973286
0.79964370
0.79946381
0.79930836
0.79962665
0.79979402
0.79951590
0.79951149
0.79902130
0.79860699
0.79871732
0.79875338
0.79877138
0.79858166
INFO - Training [22][   80/  196]   Loss 0.505953   Top1 82.656250   Top5 98.295898   BatchTime 0.406552   LR 0.000079
0.79864144
0.79898286
0.79896569
0.79906625
0.79878348
0.79870480
0.79898429
0.79852146
0.79862589
0.79895836
0.79902864
0.79924196
0.79900068
0.79910505
0.79908496
0.79938930
0.79901350
0.79884171
0.79887855
0.79902333
0.79912299
INFO - Training [22][  100/  196]   Loss 0.500199   Top1 82.773438   Top5 98.332031   BatchTime 0.399187   LR 0.000077
0.79886031
0.79837281
0.79856229
0.79855388
0.79828221
0.79805213
0.79818630
0.79833102
0.79802471
0.79810280
0.79832077
0.79827142
0.79841304
0.79831743
0.79821438
0.79790092
0.79760438
INFO - Training [22][  120/  196]   Loss 0.496241   Top1 82.955729   Top5 98.398438   BatchTime 0.393651   LR 0.000075
0.79761642
0.79768360
0.79791832
0.79816669
0.79869998
0.79979253
0.79990947
0.79990953
0.79962289
0.79975915
0.79991585
0.80011606
0.79977810
0.79943329
0.79937035
0.79916978
0.79887038
0.79888594
0.79919273
0.79928571
0.79969126
0.79950291
INFO - Training [22][  140/  196]   Loss 0.496927   Top1 82.935268   Top5 98.448661   BatchTime 0.389991   LR 0.000073
0.79933983
0.79936141
0.79921728
0.79902828
0.79897577
0.79911220
0.79934245
0.79936838
0.79907048
0.79905587
0.79927218
0.79931688
0.79915708
0.79906321
0.79850233
0.79804891
0.79786468
0.79817849
0.79866636
0.79879028
0.79869419
INFO - Training [22][  160/  196]   Loss 0.500870   Top1 82.775879   Top5 98.420410   BatchTime 0.388131   LR 0.000072
0.79884619
0.79901963
0.79917651
0.79920501
0.79904807
0.79896724
0.79867762
0.79847771
0.79846865
0.79810715
0.79797095
0.79709613
0.79716069
0.79729813
0.79704982
0.79692149
INFO - Training [22][  180/  196]   Loss 0.499335   Top1 82.819010   Top5 98.426649   BatchTime 0.385820   LR 0.000070
0.79719913
0.79670686
0.79657698
0.79621696
0.79620117
0.79589194
0.79548395
0.79515064
0.79496533
0.79473001
0.79475397
0.79451847
0.79406697
0.79355794
0.79351401
0.79321313
0.79305893
0.79266429
INFO - ==> Top1: 82.828    Top5: 98.422    Loss: 0.499
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79240471
0.79228580
0.79166901
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [22][   20/   40]   Loss 0.363028   Top1 88.164062   Top5 99.589844   BatchTime 0.200838
INFO - Validation [22][   40/   40]   Loss 0.352581   Top1 88.160000   Top5 99.620000   BatchTime 0.145999
INFO - ==> Top1: 88.160    Top5: 99.620    Loss: 0.353
INFO - ==> Sparsity : 0.395
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 88.160   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 87.640   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 87.390   Top5: 99.600]
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1328)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0729)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0648)
features.2.conv.3 tensor(0.3326)
features.2.conv.6 tensor(0.1811)
features.3.conv.0 tensor(0.0530)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1107)
features.4.conv.0 tensor(0.0312)
features.4.conv.3 tensor(0.3102)
features.4.conv.6 tensor(0.1712)
features.5.conv.0 tensor(0.2822)
features.5.conv.3 tensor(0.4294)
features.5.conv.6 tensor(0.1047)
features.6.conv.0 tensor(0.0547)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0842)
features.7.conv.0 tensor(0.0932)
features.7.conv.3 tensor(0.4439)
features.7.conv.6 tensor(0.4507)
features.8.conv.0 tensor(0.2509)
features.8.conv.3 tensor(0.5425)
features.8.conv.6 tensor(0.1476)
features.9.conv.0 tensor(0.2745)
features.9.conv.3 tensor(0.5408)
features.9.conv.6 tensor(0.1529)
features.10.conv.0 tensor(0.0647)
features.10.conv.3 tensor(0.0975)
features.10.conv.6 tensor(0.0703)
features.11.conv.0 tensor(0.4052)
features.11.conv.3 tensor(0.6510)
features.11.conv.6 tensor(0.2039)
features.12.conv.0 tensor(0.4463)
features.12.conv.3 tensor(0.6553)
features.12.conv.6 tensor(0.5340)
features.13.conv.0 tensor(0.2335)
features.13.conv.3 tensor(0.4929)
features.13.conv.6 tensor(0.1059)
features.14.conv.0 tensor(0.7479)
features.14.conv.3 tensor(0.8539)
features.14.conv.6 tensor(0.8951)
features.15.conv.0 tensor(0.7727)
features.15.conv.3 tensor(0.9038)
features.15.conv.6 tensor(0.9668)
features.16.conv.0 tensor(0.4433)
features.16.conv.3 tensor(0.8050)
features.16.conv.6 tensor(0.1559)
conv.0 tensor(0.0848)
tensor(864734.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
0.79138857
0.79121017
0.79081172
0.79069132
0.79061383
0.79036367
0.79011917
0.78994393
0.78937244
0.78850913
0.78814554
0.78805363
0.78754061
0.78709859
0.78657705
INFO - Training [23][   20/  196]   Loss 0.532486   Top1 81.503906   Top5 97.675781   BatchTime 0.473481   LR 0.000067
0.78595531
0.78568101
0.78567237
0.78561449
0.78573978
0.78570920
0.78559148
0.78506267
0.78496653
0.78482771
0.78536779
0.78561568
0.78553575
0.78571498
0.78594053
0.78623921
0.78654420
0.78657943
0.78639275
0.78623337
0.78627986
INFO - Training [23][   40/  196]   Loss 0.521780   Top1 81.777344   Top5 98.007812   BatchTime 0.434210   LR 0.000065
0.78629005
0.78616959
0.78634143
0.78622657
0.78583097
0.78588063
0.78607213
0.78591847
0.78609610
0.78581256
0.78517514
0.78558046
0.78565258
0.78550136
0.78558081
0.78557932
0.78576422
0.78578365
0.78538299
0.78538978
0.78542125
INFO - Training [23][   60/  196]   Loss 0.514250   Top1 81.992188   Top5 98.151042   BatchTime 0.415422   LR 0.000063
0.78522635
0.78510296
0.78511554
0.78523344
0.78508931
0.78521597
0.78531420
0.78498876
0.78466427
0.78487068
0.78466213
0.78461611
0.78444612
0.78441375
0.78439027
0.78436369
0.78476381
0.78502488
0.78477514
0.78478122
0.78434497
0.78412741
INFO - Training [23][   80/  196]   Loss 0.508103   Top1 82.353516   Top5 98.261719   BatchTime 0.401178   LR 0.000061
0.78411734
0.78453404
0.78484666
0.78476638
0.78459561
0.78464240
0.78436470
0.78410786
0.78394866
0.78369254
0.78367209
0.78351784
0.78329718
0.78326643
0.78313142
0.78334659
0.78331220
INFO - Training [23][  100/  196]   Loss 0.497613   Top1 82.679688   Top5 98.386719   BatchTime 0.394261   LR 0.000060
0.78319317
0.78300112
0.78295749
0.78266615
0.78265858
0.78266400
0.78261048
0.78247780
0.78253096
0.78266191
0.78292441
0.78297752
0.78313696
0.78329909
0.78323644
0.78298622
0.78297418
0.78288031
0.78283495
0.78272527
0.78274184
INFO - Training [23][  120/  196]   Loss 0.490067   Top1 82.893880   Top5 98.483073   BatchTime 0.388300   LR 0.000058
0.78302509
0.78291923
0.78316492
0.78309530
0.78312224
0.78298223
0.78291041
0.78286129
0.78290617
0.78305489
0.78327101
0.78332329
0.78309703
0.78296596
0.78308272
0.78293544
0.78313214
0.78326708
0.78308398
0.78290033
0.78288752
0.78304666
0.78317404
INFO - Training [23][  140/  196]   Loss 0.488217   Top1 82.974330   Top5 98.512835   BatchTime 0.384526   LR 0.000056
0.78308469
0.78315997
0.78299409
0.78290230
0.78308183
0.78316092
0.78295845
0.78302008
0.78309917
0.78317982
0.78325015
0.78301668
0.78281981
0.78256726
0.78272891
0.78261310
INFO - Training [23][  160/  196]   Loss 0.488955   Top1 82.927246   Top5 98.491211   BatchTime 0.380602   LR 0.000055
0.78261316
0.78249377
0.78257561
0.78263277
0.78291291
0.78306556
0.78302270
0.78301370
0.78289890
0.78265405
0.78243285
0.78221250
0.78232038
0.78255510
0.78240001
0.78256845
0.78268319
0.78275591
0.78258598
INFO - Training [23][  180/  196]   Loss 0.487969   Top1 82.940538   Top5 98.465712   BatchTime 0.372713   LR 0.000053
0.78231394
0.78223634
0.78213131
0.78222227
0.78203326
0.78211087
0.78213120
0.78217125
0.78213829
0.78233063
0.78215939
0.78209168
0.78147471
0.78115177
0.78084105
0.78085476
0.78072089
0.78084534
0.78084677
0.78073543
0.78078169
INFO - ==> Top1: 83.048    Top5: 98.466    Loss: 0.486
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [23][   20/   40]   Loss 0.354299   Top1 88.398438   Top5 99.570312   BatchTime 0.161438
INFO - Validation [23][   40/   40]   Loss 0.342660   Top1 88.460000   Top5 99.650000   BatchTime 0.153293
INFO - ==> Top1: 88.460    Top5: 99.650    Loss: 0.343
INFO - ==> Sparsity : 0.400
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 88.460   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.160   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 87.640   Top5: 99.610]
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0521)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0707)
features.2.conv.0 tensor(0.0703)
features.2.conv.3 tensor(0.3349)
features.2.conv.6 tensor(0.1814)
features.3.conv.0 tensor(0.0541)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1098)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1725)
features.5.conv.0 tensor(0.2863)
features.5.conv.3 tensor(0.4300)
features.5.conv.6 tensor(0.1029)
features.6.conv.0 tensor(0.0531)
features.6.conv.3 tensor(0.0596)
features.6.conv.6 tensor(0.0850)
features.7.conv.0 tensor(0.0901)
features.7.conv.3 tensor(0.4439)
features.7.conv.6 tensor(0.4506)
features.8.conv.0 tensor(0.2497)
features.8.conv.3 tensor(0.5394)
features.8.conv.6 tensor(0.1465)
features.9.conv.0 tensor(0.2798)
features.9.conv.3 tensor(0.5420)
features.9.conv.6 tensor(0.1579)
features.10.conv.0 tensor(0.0631)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0712)
features.11.conv.0 tensor(0.4221)
features.11.conv.3 tensor(0.6514)
features.11.conv.6 tensor(0.2075)
features.12.conv.0 tensor(0.4538)
features.12.conv.3 tensor(0.6532)
features.12.conv.6 tensor(0.5319)
features.13.conv.0 tensor(0.2361)
features.13.conv.3 tensor(0.4909)
features.13.conv.6 tensor(0.1044)
features.14.conv.0 tensor(0.7496)
features.14.conv.3 tensor(0.8546)
features.14.conv.6 tensor(0.8965)
features.15.conv.0 tensor(0.7756)
features.15.conv.3 tensor(0.9039)
features.15.conv.6 tensor(0.9660)
features.16.conv.0 tensor(0.5309)
features.16.conv.3 tensor(0.8046)
features.16.conv.6 tensor(0.1338)
conv.0 tensor(0.0885)
tensor(875266.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.78171808
0.78163475
0.78186363
0.78355944
0.78334427
0.78354669
0.78352958
0.78372395
0.78356147
0.78337389
0.78329790
0.78316414
0.78336006
0.78363788
0.78378844
0.78383005
0.78369480
0.78362787
0.78357047
0.78354943
INFO - Training [24][   20/  196]   Loss 0.492960   Top1 83.574219   Top5 98.242188   BatchTime 0.503491   LR 0.000050
0.78326273
0.78314036
0.78311968
0.78322679
0.78315419
0.78313714
0.78302532
0.78314108
0.78310907
0.78305918
0.78291905
0.78281015
0.78271639
0.78278100
0.78275424
0.78272402
0.78288674
0.78288919
0.78310370
0.78325945
INFO - Training [24][   40/  196]   Loss 0.486163   Top1 83.535156   Top5 98.349609   BatchTime 0.457578   LR 0.000048
0.78334230
0.78336024
0.78322017
0.78290433
0.78285354
0.78278565
0.78259796
0.78253049
0.78253800
0.78277785
0.78244138
0.78225875
0.78229684
0.78235108
0.78228557
0.78220224
0.78209108
0.78215641
INFO - Training [24][   60/  196]   Loss 0.484551   Top1 83.157552   Top5 98.385417   BatchTime 0.452805   LR 0.000047
0.78231138
0.78222644
0.78217983
0.78200769
0.78202331
0.78203571
0.78205937
0.78181565
0.78203827
0.78199273
0.78175223
0.78172201
0.78178686
0.78161216
0.78124154
0.78108436
0.78115672
0.78116125
0.78114361
0.78137594
0.78145474
0.78159887
INFO - Training [24][   80/  196]   Loss 0.483520   Top1 83.222656   Top5 98.393555   BatchTime 0.451206   LR 0.000045
0.78154141
0.78161842
0.78189373
0.78177083
0.78130710
0.78130364
0.78117257
0.78086793
0.78086513
0.78101075
0.78104717
0.78097618
0.78120971
0.78135538
0.78126240
0.78135878
0.78142917
0.78107059
INFO - Training [24][  100/  196]   Loss 0.477344   Top1 83.375000   Top5 98.457031   BatchTime 0.450689   LR 0.000044
0.78073525
0.78063941
0.78059173
0.78048652
0.78030294
0.78060842
0.78084004
0.78075475
0.78080726
0.78105736
0.78107071
0.78103876
0.78111821
0.78095752
0.78077936
0.78078866
0.78070116
0.78060299
0.78063679
0.78055370
0.78068197
0.78067869
INFO - Training [24][  120/  196]   Loss 0.473445   Top1 83.583984   Top5 98.489583   BatchTime 0.453105   LR 0.000042
0.78082138
0.78080541
0.78104109
0.78072232
0.78094822
0.78097087
0.78108305
0.78125089
0.78109884
0.78117317
0.78120297
0.78129917
0.78130239
0.78137499
0.78138053
0.78086972
0.78014600
0.78033322
INFO - Training [24][  140/  196]   Loss 0.470510   Top1 83.683036   Top5 98.593750   BatchTime 0.450969   LR 0.000041
0.78131539
0.78133672
0.78131253
0.78113234
0.78108585
0.78103876
0.78092742
0.78076941
0.78088152
0.78065443
0.78055620
0.78054398
0.78045005
0.78045708
0.78041166
0.78052813
0.78039122
0.78035599
0.78053319
INFO - Training [24][  160/  196]   Loss 0.471119   Top1 83.630371   Top5 98.581543   BatchTime 0.444154   LR 0.000039
0.78054863
0.78049839
0.78033531
0.78044432
0.78053719
0.78041929
0.78029263
0.78002381
0.77995497
0.78016067
0.78046447
0.78071243
0.78088719
0.78048331
0.78042370
0.78038853
0.78027713
0.78028440
0.78018224
INFO - Training [24][  180/  196]   Loss 0.469802   Top1 83.632812   Top5 98.569878   BatchTime 0.444359   LR 0.000038
0.78017271
0.77997643
0.77989382
0.77964211
0.77960777
0.77978539
0.77960503
0.77962327
0.77937901
0.77939987
0.77901411
0.77889609
0.77906215
0.77915812
0.77900749
0.77906877
0.77915817
INFO - ==> Top1: 83.724    Top5: 98.572    Loss: 0.468
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77918857
0.77907181
0.77911633
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [24][   20/   40]   Loss 0.352317   Top1 88.222656   Top5 99.609375   BatchTime 0.142579
INFO - Validation [24][   40/   40]   Loss 0.338483   Top1 88.600000   Top5 99.660000   BatchTime 0.110272
INFO - ==> Top1: 88.600    Top5: 99.660    Loss: 0.338
INFO - ==> Sparsity : 0.403
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 88.600   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 88.460   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.160   Top5: 99.620]
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.1289)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0738)
features.2.conv.0 tensor(0.0625)
features.2.conv.3 tensor(0.3318)
features.2.conv.6 tensor(0.1797)
features.3.conv.0 tensor(0.0564)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1085)
features.4.conv.0 tensor(0.0295)
features.4.conv.3 tensor(0.3090)
features.4.conv.6 tensor(0.1743)
features.5.conv.0 tensor(0.2863)
features.5.conv.3 tensor(0.4311)
features.5.conv.6 tensor(0.1029)
features.6.conv.0 tensor(0.0518)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0827)
features.7.conv.0 tensor(0.0925)
features.7.conv.3 tensor(0.4410)
features.7.conv.6 tensor(0.4497)
features.8.conv.0 tensor(0.2533)
features.8.conv.3 tensor(0.5402)
features.8.conv.6 tensor(0.1469)
features.9.conv.0 tensor(0.2797)
features.9.conv.3 tensor(0.5420)
features.9.conv.6 tensor(0.1523)
features.10.conv.0 tensor(0.0623)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.0715)
features.11.conv.0 tensor(0.4109)
features.11.conv.3 tensor(0.6528)
features.11.conv.6 tensor(0.2029)
features.12.conv.0 tensor(0.4598)
features.12.conv.3 tensor(0.6545)
features.12.conv.6 tensor(0.5304)
features.13.conv.0 tensor(0.2371)
features.13.conv.3 tensor(0.4904)
features.13.conv.6 tensor(0.1039)
features.14.conv.0 tensor(0.7580)
features.14.conv.3 tensor(0.8538)
features.14.conv.6 tensor(0.8983)
features.15.conv.0 tensor(0.7785)
features.15.conv.3 tensor(0.9037)
features.15.conv.6 tensor(0.9658)
features.16.conv.0 tensor(0.5598)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.1445)
conv.0 tensor(0.0847)
tensor(882742.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.77893037
0.77867341
0.77893412
0.77893054
0.77890366
0.77878225
0.77869391
0.77886689
0.77879876
0.77900845
0.77901864
0.77901226
0.77877551
0.77877831
0.77879268
0.77843285
0.77850217
0.77851063
INFO - Training [25][   20/  196]   Loss 0.468304   Top1 83.632812   Top5 98.457031   BatchTime 0.534697   LR 0.000035
0.77869409
0.77898610
0.77871960
0.77865052
0.77855343
0.77840710
0.77855861
0.77836877
0.77819133
0.77831900
0.77837408
0.77846187
0.77860641
0.77830893
0.77823693
0.77812427
0.77786499
0.77757221
INFO - Training [25][   40/  196]   Loss 0.477396   Top1 83.300781   Top5 98.417969   BatchTime 0.484318   LR 0.000034
0.77742040
0.77735209
0.77743006
0.77743745
0.77752686
0.77786779
0.77822304
0.77808696
0.77806067
0.77787960
0.77777392
0.77760911
0.77749264
0.77745223
0.77744216
0.77741355
0.77723086
0.77705348
0.77715683
0.77722490
INFO - Training [25][   60/  196]   Loss 0.478485   Top1 83.333333   Top5 98.509115   BatchTime 0.455048   LR 0.000033
0.77736080
0.77730614
0.77738011
0.77738243
0.77741784
0.77729589
0.77740824
0.77755123
0.77762455
0.77754313
0.77906328
0.78024131
0.78035778
0.78037602
0.78046632
0.78041995
0.78038406
0.78058243
0.78095686
0.78061444
0.78036749
0.78035748
0.78058624
INFO - Training [25][   80/  196]   Loss 0.477536   Top1 83.393555   Top5 98.569336   BatchTime 0.452104   LR 0.000031
0.78059530
0.78050572
0.78042018
0.78045934
0.78027791
0.78020972
0.78028971
0.78024793
0.78037781
0.78018695
0.77998674
0.78015149
0.78020597
0.78047347
0.78016710
0.77995604
0.78020287
INFO - Training [25][  100/  196]   Loss 0.467936   Top1 83.757812   Top5 98.574219   BatchTime 0.454944   LR 0.000030
0.78021777
0.78036165
0.78052509
0.78033298
0.78029871
0.78039515
0.78029782
0.78019589
0.78057915
0.78089422
0.78084248
0.78069741
0.78054756
0.78053296
0.78022975
0.78026205
0.78009361
0.78024167
0.78009951
0.78000504
0.78010964
0.78013641
0.78005540
INFO - Training [25][  120/  196]   Loss 0.463536   Top1 83.919271   Top5 98.639323   BatchTime 0.452281   LR 0.000029
0.77999800
0.77983183
0.78003675
0.78016806
0.78010011
0.77993894
0.78020132
0.77993137
0.77974433
0.77984291
0.77985358
0.78013283
0.77992618
0.77968609
0.77989674
0.77971458
0.77947789
0.77922684
0.77924907
0.77925998
0.77935535
INFO - Training [25][  140/  196]   Loss 0.461139   Top1 84.017857   Top5 98.696987   BatchTime 0.442164   LR 0.000027
0.77943009
0.77958679
0.77941227
0.77944571
0.77910960
0.77876663
0.77873129
0.77875119
0.77885872
0.77901477
0.77938497
0.77973670
0.77962738
0.77913219
0.77907056
0.77924758
0.77923727
0.77919793
0.77892774
INFO - Training [25][  160/  196]   Loss 0.464767   Top1 83.950195   Top5 98.666992   BatchTime 0.438428   LR 0.000026
0.77868301
0.77857691
0.77860832
0.77850056
0.77841365
0.77830029
0.77827883
0.77851975
0.77837235
0.77878368
0.77845597
0.77832097
0.77817386
0.77789092
0.77770394
0.77733660
0.77704263
0.77688342
0.77670151
0.77656454
0.77669066
INFO - Training [25][  180/  196]   Loss 0.461632   Top1 84.016927   Top5 98.639323   BatchTime 0.443170   LR 0.000025
0.77659285
0.77655536
0.77671522
0.77664143
0.77682531
0.77660358
0.77664363
0.77683216
0.77723032
0.77728474
0.77731246
0.77715945
0.77699620
INFO - ==> Top1: 84.050    Top5: 98.612    Loss: 0.461
0.77697438
0.77691334
0.77693093
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [25][   20/   40]   Loss 0.343121   Top1 88.535156   Top5 99.589844   BatchTime 0.140119
INFO - Validation [25][   40/   40]   Loss 0.334714   Top1 88.620000   Top5 99.720000   BatchTime 0.098199
INFO - ==> Top1: 88.620    Top5: 99.720    Loss: 0.335
INFO - ==> Sparsity : 0.405
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.620   Top5: 99.720]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 88.600   Top5: 99.660]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 88.460   Top5: 99.650]
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.1250)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0699)
features.2.conv.0 tensor(0.0639)
features.2.conv.3 tensor(0.3310)
features.2.conv.6 tensor(0.1823)
features.3.conv.0 tensor(0.0556)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1098)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1737)
features.5.conv.0 tensor(0.2892)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.1024)
features.6.conv.0 tensor(0.0509)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0854)
features.7.conv.0 tensor(0.0931)
features.7.conv.3 tensor(0.4413)
features.7.conv.6 tensor(0.4521)
features.8.conv.0 tensor(0.2566)
features.8.conv.3 tensor(0.5394)
features.8.conv.6 tensor(0.1471)
features.9.conv.0 tensor(0.2811)
features.9.conv.3 tensor(0.5425)
features.9.conv.6 tensor(0.1520)
features.10.conv.0 tensor(0.0625)
features.10.conv.3 tensor(0.0964)
features.10.conv.6 tensor(0.0706)
features.11.conv.0 tensor(0.4183)
features.11.conv.3 tensor(0.6526)
features.11.conv.6 tensor(0.2027)
features.12.conv.0 tensor(0.4566)
features.12.conv.3 tensor(0.6551)
features.12.conv.6 tensor(0.5335)
features.13.conv.0 tensor(0.2393)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.1045)
features.14.conv.0 tensor(0.7636)
features.14.conv.3 tensor(0.8543)
features.14.conv.6 tensor(0.8973)
features.15.conv.0 tensor(0.7845)
features.15.conv.3 tensor(0.9035)
features.15.conv.6 tensor(0.9658)
features.16.conv.0 tensor(0.5650)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.1482)
conv.0 tensor(0.0846)
tensor(887016.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
0.77691865
0.77659184
0.77652764
0.77662504
0.77670723
0.77683640
0.77659601
0.77629662
0.77599722
0.77612180
0.77623796
0.77641386
0.77635425
0.77632099
0.77637708
0.77636653
0.77602172
0.77569723
0.77546263
INFO - Training [26][   20/  196]   Loss 0.468961   Top1 83.339844   Top5 98.339844   BatchTime 0.539995   LR 0.000023
0.77550107
0.77573317
0.77582294
0.77582413
0.77580887
0.77596575
0.77584517
0.77579522
0.77573991
0.77592796
0.77598327
0.77595675
0.77593112
0.77583098
0.77567416
0.77566606
0.77569157
0.77554709
INFO - Training [26][   40/  196]   Loss 0.471755   Top1 83.359375   Top5 98.408203   BatchTime 0.481453   LR 0.000022
0.77565008
0.77577490
0.77579701
0.77596378
0.77631837
0.77678317
0.77653176
0.77618682
0.77590251
0.77573061
0.77574158
0.77558845
0.77537233
0.77545828
0.77568942
0.77579677
0.77574360
0.77566773
0.77577937
0.77576703
0.77613103
0.77628726
INFO - Training [26][   60/  196]   Loss 0.467168   Top1 83.645833   Top5 98.502604   BatchTime 0.446986   LR 0.000021
0.77602100
0.77602154
0.77586925
0.77559358
0.77537769
0.77537483
0.77550846
0.77521831
0.77563423
0.77575457
0.77566874
0.77574074
0.77555734
0.77532226
0.77531898
0.77534330
0.77530658
INFO - Training [26][   80/  196]   Loss 0.468070   Top1 83.735352   Top5 98.569336   BatchTime 0.450698   LR 0.000019
0.77534193
0.77562785
0.77581525
0.77571082
0.77573401
0.77547884
0.77534765
0.77532870
0.77550083
0.77533633
0.77524143
0.77527875
0.77515262
0.77537233
0.77513671
0.77504337
0.77521700
0.77512175
0.77493626
0.77481204
0.77480501
0.77492291
0.77531618
INFO - Training [26][  100/  196]   Loss 0.459055   Top1 84.042969   Top5 98.617188   BatchTime 0.449479   LR 0.000018
0.77535057
0.77527100
0.77562726
0.77535832
0.77514172
0.77496731
0.77483827
0.77457786
0.77448517
0.77444738
0.77451336
0.77451646
0.77449924
0.77490491
0.77491301
0.77485830
0.77479750
0.77468413
0.77447367
0.77457970
0.77436203
INFO - Training [26][  120/  196]   Loss 0.454630   Top1 84.205729   Top5 98.710938   BatchTime 0.451214   LR 0.000017
0.77427971
0.77433681
0.77450329
0.77470905
0.77447468
0.77439207
0.77425683
0.77413911
0.77402967
0.77413368
0.77428782
0.77429944
0.77440161
0.77455980
0.77453631
0.77448541
0.77432007
0.77419651
0.77409208
0.77423155
INFO - Training [26][  140/  196]   Loss 0.456824   Top1 84.160156   Top5 98.763951   BatchTime 0.445603   LR 0.000016
0.77419567
0.77409923
0.77437949
0.77457607
0.77446246
0.77411270
0.77407813
0.77415061
0.77424556
0.77443707
0.77436113
0.77430397
0.77462393
0.77488738
0.77464813
0.77445108
0.77437150
0.77429694
INFO - Training [26][  160/  196]   Loss 0.460245   Top1 84.025879   Top5 98.764648   BatchTime 0.446485   LR 0.000015
0.77434880
0.77438980
0.77411586
0.77404708
0.77429026
0.77445590
0.77460253
0.77443165
0.77439547
0.77434325
0.77443165
0.77453882
0.77443022
0.77417439
0.77418017
0.77428699
0.77423388
0.77438742
0.77433497
0.77424037
0.77433217
INFO - Training [26][  180/  196]   Loss 0.457899   Top1 84.088542   Top5 98.736979   BatchTime 0.447714   LR 0.000014
0.77409226
0.77412468
0.77426362
0.77424771
0.77412921
0.77408725
0.77420276
0.77434790
0.77427238
0.77426368
0.77421635
0.77416390
0.77416593
0.77415675
INFO - ==> Top1: 84.090    Top5: 98.740    Loss: 0.456
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.77406579
0.77407086
0.77394813
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [26][   20/   40]   Loss 0.343721   Top1 88.613281   Top5 99.570312   BatchTime 0.145473
INFO - Validation [26][   40/   40]   Loss 0.331980   Top1 88.910000   Top5 99.640000   BatchTime 0.103131
INFO - ==> Top1: 88.910    Top5: 99.640    Loss: 0.332
INFO - ==> Sparsity : 0.408
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.620   Top5: 99.720]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 88.600   Top5: 99.660]
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1250)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0663)
features.2.conv.3 tensor(0.3310)
features.2.conv.6 tensor(0.1826)
features.3.conv.0 tensor(0.0567)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1098)
features.4.conv.0 tensor(0.0299)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1732)
features.5.conv.0 tensor(0.2879)
features.5.conv.3 tensor(0.4334)
features.5.conv.6 tensor(0.1032)
features.6.conv.0 tensor(0.0493)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.0924)
features.7.conv.3 tensor(0.4413)
features.7.conv.6 tensor(0.4526)
features.8.conv.0 tensor(0.2587)
features.8.conv.3 tensor(0.5388)
features.8.conv.6 tensor(0.1473)
features.9.conv.0 tensor(0.2922)
features.9.conv.3 tensor(0.5431)
features.9.conv.6 tensor(0.1496)
features.10.conv.0 tensor(0.0622)
features.10.conv.3 tensor(0.0958)
features.10.conv.6 tensor(0.0704)
features.11.conv.0 tensor(0.4263)
features.11.conv.3 tensor(0.6526)
features.11.conv.6 tensor(0.2049)
features.12.conv.0 tensor(0.4573)
features.12.conv.3 tensor(0.6551)
features.12.conv.6 tensor(0.5324)
features.13.conv.0 tensor(0.2390)
features.13.conv.3 tensor(0.4907)
features.13.conv.6 tensor(0.1042)
features.14.conv.0 tensor(0.7673)
features.14.conv.3 tensor(0.8542)
features.14.conv.6 tensor(0.8977)
features.15.conv.0 tensor(0.7882)
features.15.conv.3 tensor(0.9032)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.5844)
features.16.conv.3 tensor(0.8049)
features.16.conv.6 tensor(0.1524)
conv.0 tensor(0.0850)
tensor(893371.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  27
INFO - Training: 50000 samples (256 per mini-batch)
0.77390426
0.77405661
0.77409530
0.77417815
0.77374500
0.77368778
0.77362561
0.77355951
0.77339751
0.77335054
0.77334142
0.77324975
0.77333242
0.77312791
0.77351421
0.77390099
0.77371007
0.77369398
0.77348274
INFO - Training [27][   20/  196]   Loss 0.445022   Top1 84.082031   Top5 98.183594   BatchTime 0.557944   LR 0.000013
0.77343422
0.77333719
0.77324480
0.77311140
0.77300340
0.77265185
0.77263933
0.77279043
0.77253151
0.77238417
0.77267545
0.77308434
0.77340198
0.77352929
0.77302569
0.77272606
0.77270931
0.77243108
0.77212143
INFO - Training [27][   40/  196]   Loss 0.463359   Top1 83.671875   Top5 98.281250   BatchTime 0.487422   LR 0.000012
0.77201796
0.77194697
0.77192074
0.77175814
0.77181637
0.77200699
0.77227020
0.77213490
0.77182680
0.77205008
0.77200496
0.77193290
0.77169114
0.77170920
0.77146775
0.77117944
0.77101308
0.77106732
0.77115399
0.77119905
INFO - Training [27][   60/  196]   Loss 0.465349   Top1 83.580729   Top5 98.326823   BatchTime 0.455633   LR 0.000011
0.77129942
0.77152103
0.77183288
0.77173138
0.77232867
0.77198815
0.77148539
0.77133733
0.77131557
0.77131242
0.77119291
0.77099097
0.77086079
0.77103508
0.77108401
0.77119404
0.77124304
0.77098709
0.77072424
0.77063584
0.77070385
0.77089351
INFO - Training [27][   80/  196]   Loss 0.465728   Top1 83.613281   Top5 98.491211   BatchTime 0.460761   LR 0.000010
0.77095646
0.77098477
0.77094275
0.77098995
0.77121627
0.77116418
0.77103251
0.77099043
0.77102983
0.77109772
0.77112734
0.77098358
0.77111733
0.77119851
0.77118105
0.77093697
0.77100307
INFO - Training [27][  100/  196]   Loss 0.459229   Top1 84.007812   Top5 98.562500   BatchTime 0.463092   LR 0.000009
0.77090657
0.77070892
0.77053785
0.77041352
0.77041775
0.77045596
0.77051973
0.77042884
0.77024609
0.77004486
0.76997614
0.76994920
0.76987541
0.76978189
0.76993614
0.77002394
0.77013069
0.77009416
0.76983094
INFO - Training [27][  120/  196]   Loss 0.453759   Top1 84.270833   Top5 98.658854   BatchTime 0.451478   LR 0.000009
0.76971108
0.76976287
0.76970822
0.76971042
0.76966304
0.76959658
0.76973927
0.76981419
0.77026826
0.77055281
0.77028334
0.77017421
0.77003378
0.76964891
0.76936555
0.76920843
0.76911730
0.76900053
0.76891112
0.76873827
0.76861048
INFO - Training [27][  140/  196]   Loss 0.451840   Top1 84.299665   Top5 98.699777   BatchTime 0.442565   LR 0.000008
0.76853067
0.76859093
0.76875705
0.76883531
0.76900017
0.76882398
0.76881993
0.76894492
0.76874757
0.76853251
0.76816356
0.76811028
0.76828063
0.76831925
0.76825458
0.76810336
0.76791763
0.76790601
0.76806462
0.76820993
0.76824808
0.76836789
0.76860195
INFO - Training [27][  160/  196]   Loss 0.453906   Top1 84.243164   Top5 98.671875   BatchTime 0.442638   LR 0.000007
0.76855707
0.76848900
0.76866579
0.76864272
0.76826555
0.76805890
0.76806921
0.76798683
0.76794583
0.76783568
0.76786929
0.76801300
0.76776218
0.76770359
0.76764429
0.76759106
0.76761830
INFO - Training [27][  180/  196]   Loss 0.452299   Top1 84.318576   Top5 98.632812   BatchTime 0.445767   LR 0.000007
0.76768774
0.76768231
0.76762414
0.76775724
0.76761448
0.76765633
0.76755267
0.76766050
0.76776242
0.76784045
0.76782572
0.76763058
0.76768029
0.76771361
0.76772624
0.76755869
0.76730353
INFO - ==> Top1: 84.278    Top5: 98.642    Loss: 0.452
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.76730573
0.76749760
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 0.336802   Top1 88.750000   Top5 99.648438   BatchTime 0.147626
INFO - Validation [27][   40/   40]   Loss 0.327737   Top1 88.880000   Top5 99.700000   BatchTime 0.103657
INFO - ==> Top1: 88.880    Top5: 99.700    Loss: 0.328
INFO - ==> Sparsity : 0.409
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.620   Top5: 99.720]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1230)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0725)
features.2.conv.0 tensor(0.0666)
features.2.conv.3 tensor(0.3295)
features.2.conv.6 tensor(0.1837)
features.3.conv.0 tensor(0.0547)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1100)
features.4.conv.0 tensor(0.0282)
features.4.conv.3 tensor(0.3067)
features.4.conv.6 tensor(0.1725)
features.5.conv.0 tensor(0.2863)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.1024)
features.6.conv.0 tensor(0.0487)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0839)
features.7.conv.0 tensor(0.0918)
features.7.conv.3 tensor(0.4407)
features.7.conv.6 tensor(0.4532)
features.8.conv.0 tensor(0.2620)
features.8.conv.3 tensor(0.5385)
features.8.conv.6 tensor(0.1487)
features.9.conv.0 tensor(0.2870)
features.9.conv.3 tensor(0.5425)
features.9.conv.6 tensor(0.1522)
features.10.conv.0 tensor(0.0620)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0699)
features.11.conv.0 tensor(0.4275)
features.11.conv.3 tensor(0.6532)
features.11.conv.6 tensor(0.2052)
features.12.conv.0 tensor(0.4565)
features.12.conv.3 tensor(0.6547)
features.12.conv.6 tensor(0.5326)
features.13.conv.0 tensor(0.2393)
features.13.conv.3 tensor(0.4905)
features.13.conv.6 tensor(0.1088)
features.14.conv.0 tensor(0.7709)
features.14.conv.3 tensor(0.8541)
features.14.conv.6 tensor(0.8981)
features.15.conv.0 tensor(0.7899)
features.15.conv.3 tensor(0.9031)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.5855)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.1548)
conv.0 tensor(0.0849)
tensor(895564.) 2188896.0
INFO - Training: 50000 samples (256 per mini-batch)
0.76775628
0.76752824
0.76769501
0.76763844
0.76766813
0.76767987
0.76773155
0.76777494
0.76782888
0.76775312
0.76773566
0.76792145
0.76779836
0.76775348
0.76776373
0.76756728
0.76749760
0.76750094
INFO - Training [28][   20/  196]   Loss 0.451444   Top1 83.867188   Top5 98.496094   BatchTime 0.552458   LR 0.000006
0.76759720
0.76753557
0.76767898
0.76758766
0.76774490
0.76752335
0.76752526
0.76751065
0.76718795
0.76715446
0.76711613
0.76708156
0.76708841
0.76712817
0.76704872
0.76702398
0.76709980
0.76689535
0.76686466
0.76683056
0.76666510
0.76641703
INFO - Training [28][   40/  196]   Loss 0.464065   Top1 83.671875   Top5 98.398438   BatchTime 0.510475   LR 0.000005
0.76629454
0.76620787
0.76619315
0.76599628
0.76589733
0.76588309
0.76581061
0.76581204
0.76575017
0.76578444
0.76575887
0.76588154
0.76574236
0.76568842
0.76568079
0.76568371
0.76563925
0.76557529
0.76562428
INFO - Training [28][   60/  196]   Loss 0.458851   Top1 83.958333   Top5 98.548177   BatchTime 0.477563   LR 0.000004
0.76572198
0.76575959
0.76609910
0.76619685
0.76618028
0.76612622
0.76600957
0.76582342
0.76574880
0.76569992
0.76564556
0.76562512
0.76564050
0.76564622
0.76568562
0.76570219
0.76567882
0.76563329
0.76561368
0.76558638
INFO - Training [28][   80/  196]   Loss 0.460379   Top1 84.013672   Top5 98.579102   BatchTime 0.457113   LR 0.000004
0.76546538
0.76538414
0.76536608
0.76531702
0.76586670
0.76580507
0.76573044
0.76564163
0.76560032
0.76557010
0.76553690
0.76546228
0.76542193
0.76533318
0.76527959
0.76529181
0.76542145
0.76556563
INFO - Training [28][  100/  196]   Loss 0.456630   Top1 84.179688   Top5 98.597656   BatchTime 0.457811   LR 0.000003
0.76561975
0.76551062
0.76532871
0.76520973
0.76501834
0.76476324
0.76476771
0.76498020
0.76509064
0.76514280
0.76520121
0.76532745
0.76529658
0.76520622
0.76498246
0.76484221
0.76500326
0.76504743
0.76486218
0.76460785
0.76425856
0.76450002
0.76465851
INFO - Training [28][  120/  196]   Loss 0.452595   Top1 84.231771   Top5 98.649089   BatchTime 0.453131   LR 0.000003
0.76449198
0.76431680
0.76430869
0.76440442
0.76433003
0.76452953
0.76453084
0.76439613
0.76445097
0.76441592
0.76428974
0.76438618
0.76437998
0.76418442
0.76418388
0.76431030
0.76424336
0.76424873
0.76423901
0.76424974
0.76422364
INFO - Training [28][  140/  196]   Loss 0.446637   Top1 84.494978   Top5 98.680246   BatchTime 0.443592   LR 0.000003
0.76418114
0.76418585
0.76415348
0.76414979
0.76421303
0.76427281
0.76420975
0.76410067
0.76404238
0.76405168
0.76400971
0.76404279
0.76397216
0.76410633
0.76403576
0.76397943
0.76401967
0.76398474
INFO - Training [28][  160/  196]   Loss 0.447189   Top1 84.470215   Top5 98.703613   BatchTime 0.442994   LR 0.000002
0.76395947
0.76392770
0.76389110
0.76385343
0.76381695
0.76388758
0.76393557
0.76393002
0.76405197
0.76396215
0.76388341
0.76385957
0.76383668
0.76381338
0.76380426
0.76379144
0.76376718
INFO - Training [28][  180/  196]   Loss 0.445370   Top1 84.546441   Top5 98.665365   BatchTime 0.444437   LR 0.000002
0.76370275
0.76376468
0.76384342
0.76375860
0.76382440
0.76383781
0.76381958
0.76378781
0.76369357
0.76363909
0.76365674
0.76367354
0.76366800
0.76366615
0.76360106
0.76352638
0.76347172
0.76362032
INFO - ==> Top1: 84.562    Top5: 98.654    Loss: 0.444
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.76379079
0.76377755
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.341434   Top1 88.496094   Top5 99.628906   BatchTime 0.204059
INFO - Validation [28][   40/   40]   Loss 0.330291   Top1 88.720000   Top5 99.700000   BatchTime 0.143722
INFO - ==> Top1: 88.720    Top5: 99.700    Loss: 0.330
INFO - ==> Sparsity : 0.410
INFO - Scoreboard best 1 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.720   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1250)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0725)
features.2.conv.0 tensor(0.0677)
features.2.conv.3 tensor(0.3310)
features.2.conv.6 tensor(0.1846)
features.3.conv.0 tensor(0.0541)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1105)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1724)
features.5.conv.0 tensor(0.2865)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.1017)
features.6.conv.0 tensor(0.0487)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0843)
features.7.conv.0 tensor(0.0914)
features.7.conv.3 tensor(0.4401)
features.7.conv.6 tensor(0.4530)
features.8.conv.0 tensor(0.2629)
features.8.conv.3 tensor(0.5382)
features.8.conv.6 tensor(0.1490)
features.9.conv.0 tensor(0.2906)
features.9.conv.3 tensor(0.5428)
features.9.conv.6 tensor(0.1545)
features.10.conv.0 tensor(0.0620)
features.10.conv.3 tensor(0.0978)
features.10.conv.6 tensor(0.0702)
features.11.conv.0 tensor(0.4294)
features.11.conv.3 tensor(0.6532)
features.11.conv.6 tensor(0.2051)
features.12.conv.0 tensor(0.4570)
features.12.conv.3 tensor(0.6545)
features.12.conv.6 tensor(0.5327)
features.13.conv.0 tensor(0.2403)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.1067)
features.14.conv.0 tensor(0.7715)
features.14.conv.3 tensor(0.8539)
features.14.conv.6 tensor(0.8982)
features.15.conv.0 tensor(0.7905)
features.15.conv.3 tensor(0.9030)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.5890)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.1556)
conv.0 tensor(0.0851)
tensor(896822.) 2188896.0
0.76373225
0.76371574
0.76369351
0.76367933
0.76363266
0.76360703
0.76358229
0.76354009
0.76350176
0.76346046
0.76339865
0.76331288
0.76304281
0.76309913
0.76337117
0.76345861
0.76350158
0.76352340
0.76354337
0.76356697
INFO - Training [29][   20/  196]   Loss 0.480619   Top1 83.222656   Top5 98.164062   BatchTime 0.548177   LR 0.000001
0.76350695
0.76337582
0.76328278
0.76312190
0.76305145
0.76300716
0.76308161
0.76305020
0.76300681
0.76303726
0.76297033
0.76294720
0.76298410
0.76300663
0.76299846
0.76303381
0.76302302
0.76301479
0.76293200
0.76287884
INFO - Training [29][   40/  196]   Loss 0.466457   Top1 83.779297   Top5 98.417969   BatchTime 0.517569   LR 0.000001
0.76279002
0.76278472
0.76276153
0.76276469
0.76273376
0.76272035
0.76272672
0.76268679
0.76270378
0.76267773
0.76266438
0.76265174
0.76266134
0.76263684
0.76261473
0.76259220
0.76261407
0.76260173
0.76262385
INFO - Training [29][   60/  196]   Loss 0.460497   Top1 83.919271   Top5 98.483073   BatchTime 0.484651   LR 0.000001
0.76259106
0.76259232
0.76258421
0.76258373
0.76257974
0.76258636
0.76254654
0.76254618
0.76256430
0.76255864
0.76255566
0.76254004
0.76255244
0.76254898
0.76254678
0.76254106
0.76253444
0.76253361
0.76252645
0.76252782
0.76251560
INFO - Training [29][   80/  196]   Loss 0.458270   Top1 84.072266   Top5 98.574219   BatchTime 0.462338   LR 0.000001
0.76251376
0.76251781
0.76251996
0.76250988
0.76250875
0.76250535
0.76250088
0.76249993
0.76250392
0.76250410
0.76249462
0.76249582
0.76249176
0.76248848
0.76248890
0.76249176
0.76248723
INFO - Training [29][  100/  196]   Loss 0.451813   Top1 84.242188   Top5 98.636719   BatchTime 0.462378   LR 0.000000
0.76248717
0.76248336
0.76249039
0.76248127
0.76248986
0.76249129
0.76248807
0.76248336
0.76248789
0.76248938
0.76248878
0.76248270
0.76248795
0.76248568
0.76248705
0.76249611
0.76249170
0.76249099
0.76248628
0.76247877
0.76247925
0.76247454
INFO - Training [29][  120/  196]   Loss 0.445934   Top1 84.414062   Top5 98.671875   BatchTime 0.460548   LR 0.000000
0.76247495
0.76247656
0.76247519
0.76247251
0.76246756
0.76246679
0.76246613
0.76246470
0.76246101
0.76246780
0.76246470
0.76246613
0.76246274
0.76246136
0.76246160
0.76246113
0.76246589
0.76246649
0.76247215
0.76247305
INFO - Training [29][  140/  196]   Loss 0.446228   Top1 84.369420   Top5 98.713728   BatchTime 0.451558   LR 0.000000
0.76246846
0.76247114
0.76246816
0.76246655
0.76246369
0.76247072
0.76246363
0.76245564
0.76245546
0.76245916
0.76245928
0.76245683
0.76245999
0.76245546
0.76245397
0.76245689
0.76245242
0.76246160
0.76246005
INFO - Training [29][  160/  196]   Loss 0.448544   Top1 84.313965   Top5 98.688965   BatchTime 0.448057   LR 0.000000
0.76246679
0.76246029
0.76246107
0.76246351
0.76245999
0.76246303
0.76246482
0.76246083
0.76245946
0.76245743
0.76246089
0.76245701
0.76245475
0.76245564
0.76246279
0.76246190
0.76245886
0.76246303
0.76246667
0.76247483
INFO - Training [29][  180/  196]   Loss 0.449287   Top1 84.214410   Top5 98.654514   BatchTime 0.453469   LR 0.000000
0.76247573
0.76247174
0.76247603
0.76247263
0.76247501
0.76247418
0.76247603
0.76247823
0.76247680
0.76247662
0.76247561
0.76247203
0.76247466
0.76247501
0.76247531
0.76246721
0.76246428
INFO - ==> Top1: 84.286    Top5: 98.662    Loss: 0.448
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.76247096
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [29][   20/   40]   Loss 0.339314   Top1 88.574219   Top5 99.628906   BatchTime 0.171510
INFO - Validation [29][   40/   40]   Loss 0.329231   Top1 88.960000   Top5 99.710000   BatchTime 0.110529
INFO - ==> Top1: 88.960    Top5: 99.710    Loss: 0.329
INFO - ==> Sparsity : 0.410
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1270)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0725)
features.2.conv.0 tensor(0.0671)
features.2.conv.3 tensor(0.3302)
features.2.conv.6 tensor(0.1846)
features.3.conv.0 tensor(0.0538)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1100)
features.4.conv.0 tensor(0.0285)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1725)
features.5.conv.0 tensor(0.2861)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.1021)
features.6.conv.0 tensor(0.0495)
features.6.conv.3 tensor(0.0532)
features.6.conv.6 tensor(0.0841)
features.7.conv.0 tensor(0.0913)
features.7.conv.3 tensor(0.4401)
features.7.conv.6 tensor(0.4531)
features.8.conv.0 tensor(0.2629)
features.8.conv.3 tensor(0.5382)
features.8.conv.6 tensor(0.1490)
features.9.conv.0 tensor(0.2912)
features.9.conv.3 tensor(0.5428)
features.9.conv.6 tensor(0.1544)
features.10.conv.0 tensor(0.0621)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0704)
features.11.conv.0 tensor(0.4293)
features.11.conv.3 tensor(0.6532)
features.11.conv.6 tensor(0.2052)
features.12.conv.0 tensor(0.4570)
features.12.conv.3 tensor(0.6545)
features.12.conv.6 tensor(0.5326)
features.13.conv.0 tensor(0.2405)
features.13.conv.3 tensor(0.4904)
features.13.conv.6 tensor(0.1068)
features.14.conv.0 tensor(0.7718)
features.14.conv.3 tensor(0.8539)
features.14.conv.6 tensor(0.8982)
features.15.conv.0 tensor(0.7905)
features.15.conv.3 tensor(0.9030)
features.15.conv.6 tensor(0.9656)
features.16.conv.0 tensor(0.5891)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.1566)
conv.0 tensor(0.0851)
tensor(897241.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
0.76247621
0.77102542
0.77150154
0.77072793
0.77006024
0.77010953
0.77018768
0.77018428
0.77001256
0.76961470
0.76930261
0.76934177
0.76952946
0.77071548
0.77217132
0.77248394
0.77249724
0.77374566
0.77401602
INFO - Training [30][   20/  196]   Loss 0.467313   Top1 83.671875   Top5 98.417969   BatchTime 0.540379   LR 0.000125
0.77404648
0.77421916
0.77429014
0.77440417
0.77448529
0.77695984
0.77931970
0.77941245
0.77965915
0.78015006
0.78058618
0.78100598
0.78281981
0.78430760
0.78691113
0.78991836
0.79224581
INFO - Training [30][   40/  196]   Loss 0.485834   Top1 82.958984   Top5 98.320312   BatchTime 0.496699   LR 0.000125
0.79504597
0.79640985
0.79646361
0.79644495
0.79672408
0.79768145
0.79933107
0.79934841
0.79898781
0.79906702
0.79882097
0.79887182
0.79873174
0.79858816
0.79850358
0.79878104
0.79845899
0.79843658
0.79827023
0.79820734
0.79818344
INFO - Training [30][   60/  196]   Loss 0.490218   Top1 82.975260   Top5 98.378906   BatchTime 0.461033   LR 0.000125
0.79803991
0.79796112
0.79798251
0.79802203
0.79791248
0.79774833
0.79764646
0.79762292
0.79775387
0.79776722
0.79761034
0.79745358
0.79732847
0.79751503
0.79743886
0.79739451
0.79750973
0.79770976
0.79742146
0.79759347
INFO - Training [30][   80/  196]   Loss 0.491908   Top1 82.822266   Top5 98.466797   BatchTime 0.447260   LR 0.000125
0.79778129
0.79738992
0.79749370
0.79743862
0.79750061
0.79744238
0.79745722
0.79743385
0.79745591
0.79746068
0.79747874
0.79753721
0.79726470
0.79733872
0.79756075
0.79752314
0.79754466
0.79747057
0.79744446
0.79749805
0.79758543
0.79764241
INFO - Training [30][  100/  196]   Loss 0.491672   Top1 82.769531   Top5 98.410156   BatchTime 0.448614   LR 0.000125
0.79771489
0.79774147
0.79746479
0.79731375
0.79730225
0.79730177
0.79698980
0.79694939
0.79711062
0.79713714
0.79732132
0.79752249
0.79750621
0.79739547
0.79749149
0.79741442
0.79741418
0.79769140
0.79747093
INFO - Training [30][  120/  196]   Loss 0.489099   Top1 82.952474   Top5 98.476562   BatchTime 0.445094   LR 0.000125
0.79733819
0.79719305
0.79751253
0.79757166
0.79757905
0.79746461
0.79697734
0.79726255
0.79726386
0.79697812
0.79663110
0.79631960
0.79612046
0.79567224
0.79572868
0.79574007
0.79519612
0.79493433
0.79468352
0.79463100
INFO - Training [30][  140/  196]   Loss 0.487623   Top1 83.027344   Top5 98.510045   BatchTime 0.437910   LR 0.000125
0.79450107
0.79440755
0.79422474
0.79406601
0.79418528
0.79417098
0.79399717
0.79387683
0.79366899
0.79337376
0.79333055
0.79340130
0.79325050
0.79324466
0.79314566
0.79300880
0.79337746
0.79330343
0.79312104
0.79317617
0.79287958
0.79255319
INFO - Training [30][  160/  196]   Loss 0.491514   Top1 82.917480   Top5 98.486328   BatchTime 0.439214   LR 0.000125
0.79236054
0.79242235
0.79228354
0.79219919
0.79190993
0.79203445
0.79213232
0.79247683
0.79261523
0.79233396
0.79247129
0.79312569
0.79299974
0.79320562
0.79366672
0.79368085
0.79351413
0.79314268
INFO - Training [30][  180/  196]   Loss 0.492842   Top1 82.860243   Top5 98.439670   BatchTime 0.442567   LR 0.000125
0.79254264
0.79198617
0.79197949
0.79206300
0.79185498
0.79166096
0.79151422
0.79143488
0.79129690
0.79122269
0.79105234
0.79075170
0.79064095
0.79050416
0.79028004
0.79044920
0.79037267
INFO - ==> Top1: 82.836    Top5: 98.460    Loss: 0.492
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79007137
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [30][   20/   40]   Loss 0.384591   Top1 87.285156   Top5 99.492188   BatchTime 0.139269
INFO - Validation [30][   40/   40]   Loss 0.371226   Top1 87.590000   Top5 99.590000   BatchTime 0.098435
INFO - ==> Top1: 87.590    Top5: 99.590    Loss: 0.371
INFO - ==> Sparsity : 0.395
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0764)
features.2.conv.0 tensor(0.0472)
features.2.conv.3 tensor(0.3356)
features.2.conv.6 tensor(0.1814)
features.3.conv.0 tensor(0.0579)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1100)
features.4.conv.0 tensor(0.0291)
features.4.conv.3 tensor(0.3084)
features.4.conv.6 tensor(0.1792)
features.5.conv.0 tensor(0.2860)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.1359)
features.6.conv.0 tensor(0.0477)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0876)
features.7.conv.0 tensor(0.0922)
features.7.conv.3 tensor(0.4421)
features.7.conv.6 tensor(0.4767)
features.8.conv.0 tensor(0.2548)
features.8.conv.3 tensor(0.5391)
features.8.conv.6 tensor(0.1517)
features.9.conv.0 tensor(0.2884)
features.9.conv.3 tensor(0.5437)
features.9.conv.6 tensor(0.1484)
features.10.conv.0 tensor(0.0589)
features.10.conv.3 tensor(0.0995)
features.10.conv.6 tensor(0.0712)
features.11.conv.0 tensor(0.4044)
features.11.conv.3 tensor(0.6501)
features.11.conv.6 tensor(0.2067)
features.12.conv.0 tensor(0.4695)
features.12.conv.3 tensor(0.6541)
features.12.conv.6 tensor(0.5392)
features.13.conv.0 tensor(0.2314)
features.13.conv.3 tensor(0.4919)
features.13.conv.6 tensor(0.1024)
features.14.conv.0 tensor(0.7729)
features.14.conv.3 tensor(0.8519)
features.14.conv.6 tensor(0.8981)
features.15.conv.0 tensor(0.7908)
features.15.conv.3 tensor(0.9034)
features.15.conv.6 tensor(0.9667)
features.16.conv.0 tensor(0.4301)
features.16.conv.3 tensor(0.8039)
features.16.conv.6 tensor(0.1330)
conv.0 tensor(0.0837)
tensor(864668.) 2188896.0
0.78984803
0.78952843
0.78933167
0.78921562
0.78878957
0.78854704
0.78835338
0.78831828
0.78853953
0.79088497
0.79065126
0.79062557
0.79025471
0.78988892
0.79066688
0.79036480
0.78999764
0.78969127
0.78940618
INFO - Training [31][   20/  196]   Loss 0.513048   Top1 81.542969   Top5 98.105469   BatchTime 0.556460   LR 0.000125
0.78943872
0.78919828
0.78901023
0.78897500
0.78972656
0.79272306
0.79700750
0.79993969
0.79986942
0.79945332
0.79931414
0.79893917
0.79861635
0.79814720
0.79786366
0.79762280
0.79732430
0.79677361
INFO - Training [31][   40/  196]   Loss 0.517418   Top1 81.757812   Top5 98.134766   BatchTime 0.507426   LR 0.000125
0.79641849
0.79609662
0.79599774
0.79604119
0.79601383
0.79591078
0.79561156
0.79537034
0.79519325
0.79518932
0.79511374
0.79517251
0.79498887
0.79478228
0.79470766
0.79456151
0.79439884
0.79453647
0.79462087
0.79443705
0.79430473
0.79421413
0.79394871
INFO - Training [31][   60/  196]   Loss 0.503202   Top1 82.356771   Top5 98.203125   BatchTime 0.479062   LR 0.000125
0.79361516
0.79331827
0.79300892
0.79357684
0.79296452
0.79317909
0.79328442
0.79307640
0.79294872
0.79285419
0.79279917
0.79250199
0.79235786
0.79246724
0.79274052
0.79262334
0.79252040
INFO - Training [31][   80/  196]   Loss 0.497266   Top1 82.578125   Top5 98.354492   BatchTime 0.453622   LR 0.000125
0.79205441
0.79187733
0.79179054
0.79157025
0.79124868
0.79078990
0.79031307
0.79099613
0.79095012
0.79057276
0.79043782
0.78994328
0.79043949
0.79038811
0.79024005
0.79009801
0.79019469
0.79046822
0.79010886
0.79004574
0.78963763
0.78963780
INFO - Training [31][  100/  196]   Loss 0.488285   Top1 82.972656   Top5 98.429688   BatchTime 0.455083   LR 0.000125
0.78960127
0.78957671
0.79005080
0.79144746
0.79125851
0.79119384
0.79105580
0.79079932
0.79063529
0.79059231
0.79060525
0.79025793
0.79019743
0.79020852
0.78999567
0.79001045
0.79011160
0.78982466
0.78999150
0.78983837
INFO - Training [31][  120/  196]   Loss 0.485817   Top1 83.121745   Top5 98.509115   BatchTime 0.459631   LR 0.000125
0.78971249
0.78988266
0.78984642
0.78983814
0.78984338
0.78967434
0.78935677
0.78927761
0.78934437
0.78942406
0.78957075
0.78965849
0.78898352
0.78909004
0.78907758
0.78888869
0.78871930
0.78867948
0.78869313
INFO - Training [31][  140/  196]   Loss 0.484519   Top1 83.191964   Top5 98.557478   BatchTime 0.456377   LR 0.000124
0.78855449
0.78835005
0.78812253
0.78788489
0.78770906
0.78761017
0.78751588
0.78775430
0.78788751
0.78845966
0.78917021
0.78975934
0.78996712
0.79018080
0.79053038
0.79058307
0.79111111
0.79345483
0.79400420
0.79416573
0.79412627
INFO - Training [31][  160/  196]   Loss 0.487305   Top1 83.090820   Top5 98.532715   BatchTime 0.446898   LR 0.000124
0.79459417
0.79475254
0.79449797
0.79449022
0.79636592
0.79636729
0.79646176
0.79623795
0.79622114
0.79607111
0.79627901
0.79607952
0.79609436
0.79637301
0.79619426
0.79615086
0.79609472
0.79598767
0.79587430
0.79578674
0.79593521
0.79602051
INFO - Training [31][  180/  196]   Loss 0.489445   Top1 82.894965   Top5 98.500434   BatchTime 0.448265   LR 0.000124
0.79617578
0.79613012
0.79642808
0.79663754
0.79654640
0.79659939
0.79660046
0.79652148
0.79634398
0.79610813
0.79622227
0.79619431
INFO - ==> Top1: 82.922    Top5: 98.502    Loss: 0.490
0.79621661
0.79616064
0.79637134
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [31][   20/   40]   Loss 0.369518   Top1 87.480469   Top5 99.570312   BatchTime 0.141395
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0648)
features.1.conv.6 tensor(0.0760)
features.2.conv.0 tensor(0.0492)
features.2.conv.3 tensor(0.3387)
features.2.conv.6 tensor(0.1823)
features.3.conv.0 tensor(0.0553)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1120)
features.4.conv.0 tensor(0.0283)
features.4.conv.3 tensor(0.3084)
features.4.conv.6 tensor(0.1833)
features.5.conv.0 tensor(0.2894)
features.5.conv.3 tensor(0.4346)
features.5.conv.6 tensor(0.1367)
features.6.conv.0 tensor(0.0508)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0848)
features.7.conv.0 tensor(0.0938)
features.7.conv.3 tensor(0.4416)
features.7.conv.6 tensor(0.2241)
features.8.conv.0 tensor(0.2536)
features.8.conv.3 tensor(0.5379)
features.8.conv.6 tensor(0.1508)
features.9.conv.0 tensor(0.2756)
features.9.conv.3 tensor(0.5422)
features.9.conv.6 tensor(0.1508)
features.10.conv.0 tensor(0.0629)
features.10.conv.3 tensor(0.0984)
features.10.conv.6 tensor(0.0715)
features.11.conv.0 tensor(0.4021)
features.11.conv.3 tensor(0.6491)
features.11.conv.6 tensor(0.1988)
features.12.conv.0 tensor(0.4485)
features.12.conv.3 tensor(0.6553)
features.12.conv.6 tensor(0.4463)
features.13.conv.0 tensor(0.2330)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.1037)
features.14.conv.0 tensor(0.7735)
features.14.conv.3 tensor(0.8500)
features.14.conv.6 tensor(0.9029)
features.15.conv.0 tensor(0.7904)
features.15.conv.3 tensor(0.9051)
features.15.conv.6 tensor(0.9682)
features.16.conv.0 tensor(0.5762)
features.16.conv.3 tensor(0.8046)
features.16.conv.6 tensor(0.1335)
conv.0 tensor(0.0838)
tensor(875300.) 2188896.0
INFO - Validation [31][   40/   40]   Loss 0.363428   Top1 87.670000   Top5 99.610000   BatchTime 0.097731
INFO - ==> Top1: 87.670    Top5: 99.610    Loss: 0.363
INFO - ==> Sparsity : 0.400
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
0.79659069
0.79665285
0.79680109
0.80186653
0.80607492
0.80661672
0.80648619
0.80656821
0.80639637
0.80646056
0.80645287
0.80641115
0.80611628
0.80591851
0.80579042
0.80581021
0.80570573
0.80578268
INFO - Training [32][   20/  196]   Loss 0.493791   Top1 82.988281   Top5 97.910156   BatchTime 0.545232   LR 0.000124
0.80550259
0.80521584
0.80547547
0.80533421
0.80531639
0.80551738
0.80555695
0.80563360
0.80552042
0.80553108
0.80535752
0.80510890
0.80501646
0.80484158
0.80486435
0.80482048
0.80493551
0.80481964
0.80494946
0.80506182
0.80490679
0.80452377
INFO - Training [32][   40/  196]   Loss 0.492052   Top1 82.773438   Top5 98.076172   BatchTime 0.503608   LR 0.000124
0.80436921
0.80443889
0.80442011
0.80429387
0.80442667
0.80443966
0.80451989
0.80438662
0.80403161
0.80412906
0.80423975
0.80401266
0.80384576
0.80380034
0.80348003
0.80348557
0.80329514
0.80333114
INFO - Training [32][   60/  196]   Loss 0.490112   Top1 82.858073   Top5 98.196615   BatchTime 0.479609   LR 0.000124
0.80332673
0.80333495
0.80325037
0.80320889
0.80325246
0.80272472
0.80266190
0.80285281
0.80300242
0.80294299
0.80302638
0.80280888
0.80239546
0.80215156
0.80216086
0.80229169
0.80273968
0.80255967
0.80319721
0.80404544
INFO - Training [32][   80/  196]   Loss 0.487737   Top1 82.929688   Top5 98.286133   BatchTime 0.461584   LR 0.000124
0.80371398
0.80376744
0.80362606
0.80327398
0.80295146
0.80280411
0.80267733
0.80284041
0.80276066
0.80282038
0.80293167
0.80274284
0.80266255
0.80284017
0.80323428
0.80291218
0.80296391
0.80334067
0.80356455
0.80448747
0.80451775
0.80440813
INFO - Training [32][  100/  196]   Loss 0.486233   Top1 83.003906   Top5 98.371094   BatchTime 0.459062   LR 0.000124
0.80425531
0.80429679
0.80433792
0.80416733
0.80401504
0.80405909
0.80389953
0.80391949
0.80391598
0.80426323
0.80407757
0.80377793
0.80351776
0.80331051
0.80338937
0.80341870
0.80325496
INFO - Training [32][  120/  196]   Loss 0.484077   Top1 83.157552   Top5 98.427734   BatchTime 0.459201   LR 0.000124
0.80319470
0.80350167
0.80373865
0.80404550
0.80419058
0.80418932
0.80470693
0.80507827
0.80481642
0.80536067
0.80553037
0.80597013
0.80628079
0.80586869
0.80589169
0.80607468
0.80620629
0.80611449
0.80585581
0.80551243
0.80529642
0.80515450
0.80498832
0.80480009
INFO - Training [32][  140/  196]   Loss 0.483042   Top1 83.250558   Top5 98.498884   BatchTime 0.454789   LR 0.000124
0.80465919
0.80464339
0.80448776
0.80427843
0.80427563
0.80443150
0.80394322
0.80400187
0.80370051
0.80361909
0.80367500
0.80357808
0.80341542
0.80288160
0.80305612
0.80311114
0.80290753
0.80286342
0.80281299
INFO - Training [32][  160/  196]   Loss 0.486194   Top1 83.161621   Top5 98.469238   BatchTime 0.450420   LR 0.000123
0.80259717
0.80266768
0.80258304
0.80224329
0.80187380
0.80174869
0.80154514
0.80140179
0.80110824
0.80070025
0.80058044
0.80057192
0.80043602
0.80010504
0.79982996
0.79944432
0.79927129
INFO - Training [32][  180/  196]   Loss 0.486112   Top1 83.105469   Top5 98.463542   BatchTime 0.453194   LR 0.000123
0.79923183
0.79915148
0.79865116
0.79840362
0.79844397
0.79830700
0.79775745
0.79736197
0.79769963
0.79759651
0.79763198
0.79736573
0.79706204
0.79707485
0.79734427
0.79753524
0.79807550
INFO - ==> Top1: 83.146    Top5: 98.456    Loss: 0.486
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79919416
0.80541837
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [32][   20/   40]   Loss 0.401635   Top1 86.601562   Top5 99.414062   BatchTime 0.138385
INFO - Validation [32][   40/   40]   Loss 0.383702   Top1 86.980000   Top5 99.580000   BatchTime 0.096434
INFO - ==> Top1: 86.980    Top5: 99.580    Loss: 0.384
INFO - ==> Sparsity : 0.405
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0590)
features.1.conv.6 tensor(0.0729)
features.2.conv.0 tensor(0.0512)
features.2.conv.3 tensor(0.3356)
features.2.conv.6 tensor(0.1881)
features.3.conv.0 tensor(0.0576)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1126)
features.4.conv.0 tensor(0.0243)
features.4.conv.3 tensor(0.3154)
features.4.conv.6 tensor(0.1782)
features.5.conv.0 tensor(0.2866)
features.5.conv.3 tensor(0.4317)
features.5.conv.6 tensor(0.1349)
features.6.conv.0 tensor(0.0492)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.0928)
features.7.conv.3 tensor(0.4392)
features.7.conv.6 tensor(0.2295)
features.8.conv.0 tensor(0.2489)
features.8.conv.3 tensor(0.5336)
features.8.conv.6 tensor(0.1523)
features.9.conv.0 tensor(0.2763)
features.9.conv.3 tensor(0.5414)
features.9.conv.6 tensor(0.1595)
features.10.conv.0 tensor(0.0603)
features.10.conv.3 tensor(0.0990)
features.10.conv.6 tensor(0.0717)
features.11.conv.0 tensor(0.3554)
features.11.conv.3 tensor(0.6481)
features.11.conv.6 tensor(0.2038)
features.12.conv.0 tensor(0.4548)
features.12.conv.3 tensor(0.6545)
features.12.conv.6 tensor(0.5109)
features.13.conv.0 tensor(0.2330)
features.13.conv.3 tensor(0.4859)
features.13.conv.6 tensor(0.1057)
features.14.conv.0 tensor(0.7983)
features.14.conv.3 tensor(0.8508)
features.14.conv.6 tensor(0.9343)
features.15.conv.0 tensor(0.7944)
features.15.conv.3 tensor(0.9051)
features.15.conv.6 tensor(0.9682)
features.16.conv.0 tensor(0.5725)
features.16.conv.3 tensor(0.8049)
features.16.conv.6 tensor(0.1369)
conv.0 tensor(0.0827)
tensor(886426.) 2188896.0
0.81161231
0.81181335
0.81138200
0.81161779
0.81175315
0.81147403
0.81110269
0.81067973
0.81022865
0.80968958
0.80943388
0.80882746
0.80820996
0.80790240
0.80736321
0.80733067
0.80687791
0.80677223
0.80663002
INFO - Training [33][   20/  196]   Loss 0.493260   Top1 83.027344   Top5 98.027344   BatchTime 0.546767   LR 0.000123
0.80639327
0.80631024
0.80648154
0.80630559
0.80609781
0.80576879
0.80575687
0.80594254
0.80574411
0.80579907
0.80554265
0.80565107
0.80568516
0.80567968
0.80552948
0.80568463
0.80561060
0.80524272
0.80495262
0.80516773
0.80500823
INFO - Training [33][   40/  196]   Loss 0.499060   Top1 82.753906   Top5 98.173828   BatchTime 0.511206   LR 0.000123
0.80460733
0.80446070
0.80417478
0.80411184
0.80422837
0.80445415
0.80448568
0.80455667
0.80467480
0.80460149
0.80451590
0.80444157
0.80440766
0.80420929
0.80403787
0.80437714
0.80450785
0.80434370
0.80439264
0.80425048
0.80402845
INFO - Training [33][   60/  196]   Loss 0.490356   Top1 83.092448   Top5 98.333333   BatchTime 0.502941   LR 0.000123
0.80432981
0.80462915
0.80432308
0.80445307
0.80467963
0.80470061
0.80489016
0.80446905
0.80443585
0.80454296
0.80421877
0.80390549
0.80416960
0.80386299
0.80362576
0.80342990
0.80345243
0.80361986
INFO - Training [33][   80/  196]   Loss 0.487300   Top1 83.291016   Top5 98.432617   BatchTime 0.486313   LR 0.000123
0.80378765
0.80368578
0.80358088
0.80377734
0.80357587
0.80363119
0.80320203
0.80318832
0.80323797
0.80314910
0.80300993
0.80296707
0.80299795
0.80273432
0.80260354
0.80254591
0.80277401
0.80252153
0.80278867
0.80291802
0.80277479
INFO - Training [33][  100/  196]   Loss 0.483681   Top1 83.398438   Top5 98.484375   BatchTime 0.464416   LR 0.000123
0.80246133
0.80241621
0.80236411
0.80224210
0.80173773
0.80137551
0.80136836
0.80119234
0.80038440
0.80000687
0.79949915
0.79927462
0.79934514
0.80018920
0.79998916
0.79962307
0.79902917
0.79858714
INFO - Training [33][  120/  196]   Loss 0.481043   Top1 83.476562   Top5 98.531901   BatchTime 0.463117   LR 0.000123
0.79832989
0.79804921
0.79766232
0.79712063
0.79665339
0.79636997
0.79614371
0.79542184
0.79495251
0.79469800
0.79457664
0.79419398
0.79405069
0.79365122
0.79335815
0.79271895
0.79218608
0.79201406
INFO - Training [33][  140/  196]   Loss 0.480160   Top1 83.487723   Top5 98.565848   BatchTime 0.456022   LR 0.000122
0.79137999
0.79106629
0.79085755
0.79070222
0.79058212
0.79034173
0.79016876
0.78974038
0.78976965
0.78975290
0.78978270
0.78999227
0.78990144
0.78995377
0.79028839
0.79071170
0.79131836
0.79135346
0.79124874
0.79096025
0.79103225
0.79124302
0.79124731
0.79105717
INFO - Training [33][  160/  196]   Loss 0.485134   Top1 83.288574   Top5 98.537598   BatchTime 0.454061   LR 0.000122
0.79105628
0.79119468
0.79153728
0.79104894
0.79075050
0.79079723
0.79049015
0.79028791
0.79005790
0.78997749
0.78987533
0.78986084
0.78986239
0.78957129
0.78936678
0.78925878
0.78949606
INFO - Training [33][  180/  196]   Loss 0.485702   Top1 83.229167   Top5 98.489583   BatchTime 0.454123   LR 0.000122
0.78929865
0.78905690
0.78896075
0.78910893
0.78915238
0.78907210
0.78888112
0.78893101
0.78894818
0.78873557
0.78871679
0.78890747
0.78882903
0.78853339
0.78851646
0.78880018
0.78917336
0.78911763
INFO - ==> Top1: 83.240    Top5: 98.476    Loss: 0.485
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.78879344
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [33][   20/   40]   Loss 0.371604   Top1 87.792969   Top5 99.531250   BatchTime 0.139453
INFO - Validation [33][   40/   40]   Loss 0.360960   Top1 87.830000   Top5 99.650000   BatchTime 0.099072
INFO - ==> Top1: 87.830    Top5: 99.650    Loss: 0.361
INFO - ==> Sparsity : 0.408
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5764)
features.0.conv.3 tensor(0.1406)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0677)
features.2.conv.0 tensor(0.0434)
features.2.conv.3 tensor(0.3356)
features.2.conv.6 tensor(0.1866)
features.3.conv.0 tensor(0.0550)
features.3.conv.3 tensor(0.0718)
features.3.conv.6 tensor(0.1128)
features.4.conv.0 tensor(0.0286)
features.4.conv.3 tensor(0.3090)
features.4.conv.6 tensor(0.1807)
features.5.conv.0 tensor(0.2843)
features.5.conv.3 tensor(0.4311)
features.5.conv.6 tensor(0.1382)
features.6.conv.0 tensor(0.0470)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0845)
features.7.conv.0 tensor(0.0914)
features.7.conv.3 tensor(0.4363)
features.7.conv.6 tensor(0.2311)
features.8.conv.0 tensor(0.2416)
features.8.conv.3 tensor(0.5362)
features.8.conv.6 tensor(0.1516)
features.9.conv.0 tensor(0.2866)
features.9.conv.3 tensor(0.5399)
features.9.conv.6 tensor(0.1482)
features.10.conv.0 tensor(0.0590)
features.10.conv.3 tensor(0.0961)
features.10.conv.6 tensor(0.0728)
features.11.conv.0 tensor(0.4220)
features.11.conv.3 tensor(0.6483)
features.11.conv.6 tensor(0.2050)
features.12.conv.0 tensor(0.4463)
features.12.conv.3 tensor(0.6555)
features.12.conv.6 tensor(0.5221)
features.13.conv.0 tensor(0.2337)
features.13.conv.3 tensor(0.4905)
features.13.conv.6 tensor(0.1050)
features.14.conv.0 tensor(0.7871)
features.14.conv.3 tensor(0.8502)
features.14.conv.6 tensor(0.9439)
features.15.conv.0 tensor(0.7903)
features.15.conv.3 tensor(0.9053)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.5959)
features.16.conv.3 tensor(0.8046)
features.16.conv.6 tensor(0.1354)
conv.0 tensor(0.0836)
tensor(892638.) 2188896.0
0.78865176
0.78823930
0.78810310
0.78784186
0.78760010
0.78759581
0.78767002
0.78771323
0.78778857
0.78792411
0.78750545
0.78759855
0.78774834
0.78733671
0.78765398
0.78783470
0.78771281
0.78759283
0.78778994
INFO - Training [34][   20/  196]   Loss 0.516959   Top1 81.386719   Top5 97.851562   BatchTime 0.565283   LR 0.000122
0.78795296
0.78833103
0.79146737
0.79144603
0.79151529
0.79142463
0.79137409
0.79130602
0.79147351
0.79127872
0.79122740
0.79146361
0.79137188
0.79127491
0.79132003
0.79110646
0.79088575
0.79077023
0.79073387
0.79095173
0.79095888
INFO - Training [34][   40/  196]   Loss 0.496458   Top1 82.656250   Top5 98.007812   BatchTime 0.526082   LR 0.000122
0.79103869
0.79127687
0.79118603
0.79117632
0.79097456
0.79116195
0.79115814
0.79125458
0.79090071
0.79081422
0.79085422
0.79076356
0.79096842
0.79112810
0.79118317
0.79121691
0.79122174
INFO - Training [34][   60/  196]   Loss 0.488486   Top1 82.792969   Top5 98.177083   BatchTime 0.506842   LR 0.000121
0.79138267
0.79126787
0.79094964
0.79128700
0.79119247
0.79097229
0.79111964
0.79158056
0.79202026
0.79280972
0.79289967
0.79310137
0.79335886
0.79313713
0.79322380
0.79307377
0.79292947
0.79305989
0.79329580
0.79335529
0.79289228
0.79293531
INFO - Training [34][   80/  196]   Loss 0.491994   Top1 82.807617   Top5 98.300781   BatchTime 0.492415   LR 0.000121
0.79299319
0.79309881
0.79286343
0.79264718
0.79266793
0.79277664
0.79281956
0.79286397
0.79283768
0.79284948
0.79280150
0.79270214
0.79251629
0.79246563
0.79231209
0.79217088
0.79221129
0.79206824
0.79190880
INFO - Training [34][  100/  196]   Loss 0.484479   Top1 83.152344   Top5 98.367188   BatchTime 0.476185   LR 0.000121
0.79202336
0.79179293
0.79146844
0.79126233
0.79114336
0.79100186
0.79096156
0.79108346
0.79091769
0.79084748
0.79064000
0.79048234
0.79023606
0.79002589
0.79006809
0.78999430
0.79003829
0.79005772
0.78995478
0.78977990
0.78958559
0.78967410
INFO - Training [34][  120/  196]   Loss 0.482027   Top1 83.261719   Top5 98.460286   BatchTime 0.473361   LR 0.000121
0.78978896
0.78985268
0.78997558
0.78999358
0.79004198
0.79005808
0.78981930
0.78971988
0.78943610
0.78935200
0.78914547
0.78902221
0.78880155
0.78874362
0.78876674
0.78890169
0.78862411
0.78858936
0.78834420
0.78838158
INFO - Training [34][  140/  196]   Loss 0.481783   Top1 83.264509   Top5 98.479353   BatchTime 0.465225   LR 0.000121
0.78843474
0.78826219
0.78820330
0.78799856
0.78814268
0.78848541
0.78852308
0.78845239
0.78807539
0.78805238
0.78814173
0.78803021
0.78810579
0.78824091
0.78848141
0.78850585
0.78847641
0.78835738
INFO - Training [34][  160/  196]   Loss 0.481750   Top1 83.249512   Top5 98.481445   BatchTime 0.461684   LR 0.000121
0.78835309
0.78827161
0.78840017
0.78840590
0.78856647
0.78859478
0.78877628
0.78978729
0.79351193
0.80546862
0.80737150
0.80728394
0.80752593
0.80806595
0.80923247
0.81228560
0.81446582
0.81822228
0.81834829
0.81827414
0.81839663
INFO - Training [34][  180/  196]   Loss 0.481482   Top1 83.279080   Top5 98.448351   BatchTime 0.463896   LR 0.000120
0.81837511
0.81815022
0.81819415
0.81825715
0.81851441
0.81826860
0.81841785
0.81819135
0.81799382
0.81818330
0.81830686
0.81838000
0.81824315
0.81833524
0.81833929
0.81822240
0.81797945
INFO - ==> Top1: 83.364    Top5: 98.442    Loss: 0.479
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.364693   Top1 87.734375   Top5 99.492188   BatchTime 0.141309
INFO - Validation [34][   40/   40]   Loss 0.354414   Top1 87.980000   Top5 99.630000   BatchTime 0.103618
INFO - ==> Top1: 87.980    Top5: 99.630    Loss: 0.354
INFO - ==> Sparsity : 0.399
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5799)
features.0.conv.3 tensor(0.1426)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0773)
features.2.conv.0 tensor(0.0463)
features.2.conv.3 tensor(0.3356)
features.2.conv.6 tensor(0.1855)
features.3.conv.0 tensor(0.0602)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1100)
features.4.conv.0 tensor(0.0275)
features.4.conv.3 tensor(0.3061)
features.4.conv.6 tensor(0.1781)
features.5.conv.0 tensor(0.2756)
features.5.conv.3 tensor(0.4288)
features.5.conv.6 tensor(0.1382)
features.6.conv.0 tensor(0.0470)
features.6.conv.3 tensor(0.0608)
features.6.conv.6 tensor(0.0837)
features.7.conv.0 tensor(0.0927)
features.7.conv.3 tensor(0.4366)
features.7.conv.6 tensor(0.2263)
features.8.conv.0 tensor(0.2553)
features.8.conv.3 tensor(0.5359)
features.8.conv.6 tensor(0.1535)
features.9.conv.0 tensor(0.2702)
features.9.conv.3 tensor(0.5402)
features.9.conv.6 tensor(0.1475)
features.10.conv.0 tensor(0.0581)
features.10.conv.3 tensor(0.0975)
features.10.conv.6 tensor(0.0723)
features.11.conv.0 tensor(0.3470)
features.11.conv.3 tensor(0.6497)
features.11.conv.6 tensor(0.1972)
features.12.conv.0 tensor(0.4555)
features.12.conv.3 tensor(0.6551)
features.12.conv.6 tensor(0.5152)
features.13.conv.0 tensor(0.2342)
features.13.conv.3 tensor(0.4894)
features.13.conv.6 tensor(0.1037)
features.14.conv.0 tensor(0.7933)
features.14.conv.3 tensor(0.8510)
features.14.conv.6 tensor(0.9472)
features.15.conv.0 tensor(0.7908)
features.15.conv.3 tensor(0.9054)
features.15.conv.6 tensor(0.9685)
features.16.conv.0 tensor(0.4915)
features.16.conv.3 tensor(0.8049)
features.16.conv.6 tensor(0.1363)
conv.0 tensor(0.0838)
tensor(873898.) 2188896.0
0.81842446
0.82019144
0.82028133
0.82036030
0.82018822
0.81975532
0.82015634
0.82001233
0.82003230
0.81992376
0.81962293
0.81968236
0.81982106
0.81996238
0.82009363
0.81996191
0.82015407
0.82006198
0.82015604
INFO - Training [35][   20/  196]   Loss 0.484915   Top1 83.007812   Top5 98.144531   BatchTime 0.564072   LR 0.000120
0.82003963
0.81991464
0.81969124
0.81951267
0.81934375
0.81918377
0.81904203
0.81871372
0.81881481
0.81876498
0.81908691
0.81896603
0.81910664
0.81963092
0.81944317
0.81916642
0.81896114
0.81909508
INFO - Training [35][   40/  196]   Loss 0.483187   Top1 82.734375   Top5 98.310547   BatchTime 0.511069   LR 0.000120
0.81914032
0.81899309
0.81898367
0.81910145
0.81910068
0.81939733
0.81957060
0.81933457
0.81938773
0.81917459
0.81925243
0.81896538
0.81910312
0.81906050
0.81929123
0.81893736
0.81899625
0.81900632
0.81892914
0.81898087
0.81912279
0.81906193
INFO - Training [35][   60/  196]   Loss 0.480882   Top1 82.734375   Top5 98.385417   BatchTime 0.492592   LR 0.000120
0.81907284
0.81908005
0.81874233
0.81882477
0.81870627
0.81865555
0.81873626
0.81877887
0.81857300
0.81856650
0.81842506
0.81801951
0.81805456
0.81806028
0.81783187
0.81736809
0.81694603
0.81665951
INFO - Training [35][   80/  196]   Loss 0.477779   Top1 82.929688   Top5 98.442383   BatchTime 0.479655   LR 0.000119
0.81645471
0.81618488
0.81597733
0.81582618
0.81593704
0.81494045
0.81565183
0.81560200
0.81549066
0.81434798
0.81469244
0.81469637
0.81441498
0.81424826
0.81421995
0.81434119
0.81430978
0.81397325
0.81342477
0.81306815
0.81249577
0.81210095
0.81087250
INFO - Training [35][  100/  196]   Loss 0.473955   Top1 83.070312   Top5 98.488281   BatchTime 0.470229   LR 0.000119
0.81127697
0.81129241
0.81107241
0.81051701
0.81019348
0.80997795
0.80934501
0.80914360
0.80864018
0.80813366
0.80806583
0.80738181
0.80687898
0.80575264
0.80482817
0.80464274
0.80452305
INFO - Training [35][  120/  196]   Loss 0.469079   Top1 83.297526   Top5 98.574219   BatchTime 0.468621   LR 0.000119
0.80421972
0.80366284
0.80327320
0.80300766
0.80273038
0.80203205
0.80154496
0.80125350
0.80077958
0.80053532
0.80017841
0.79980254
0.79942799
0.79909801
0.79879761
0.79856479
0.79805750
0.79773349
0.79749352
INFO - Training [35][  140/  196]   Loss 0.469843   Top1 83.351004   Top5 98.643973   BatchTime 0.460702   LR 0.000119
0.79718322
0.79709840
0.79705352
0.79675841
0.79665738
0.79626620
0.79619062
0.79601389
0.79553509
0.79507393
0.79460132
0.79416835
0.79387695
0.79337561
0.79298073
0.79293835
0.79294175
0.79254204
0.79243100
0.79224700
0.79185236
0.79160959
0.79197705
0.79268283
0.79249913
INFO - Training [35][  160/  196]   Loss 0.472513   Top1 83.305664   Top5 98.603516   BatchTime 0.454801   LR 0.000119
0.79211348
0.79243207
0.79263908
0.79252231
0.79253876
0.79227144
0.79200655
0.79212242
0.79176760
0.79141659
0.79107147
0.79101366
0.79115939
0.79113895
0.79110533
0.79100114
0.79117501
INFO - Training [35][  180/  196]   Loss 0.472500   Top1 83.361545   Top5 98.567708   BatchTime 0.455335   LR 0.000118
0.79155499
0.79160184
0.79162633
0.79127818
0.79110014
0.79101497
0.79115260
0.79125273
0.79107171
0.79107684
0.79099679
0.79095775
0.79086852
0.79101229
0.79157531
0.79195720
0.79215848
INFO - ==> Top1: 83.386    Top5: 98.566    Loss: 0.471
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79238182
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [35][   20/   40]   Loss 0.376123   Top1 87.363281   Top5 99.511719   BatchTime 0.142667
INFO - Validation [35][   40/   40]   Loss 0.369699   Top1 87.390000   Top5 99.610000   BatchTime 0.111336
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.1367)
features.1.conv.0 tensor(0.0560)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.0460)
features.2.conv.3 tensor(0.3349)
features.2.conv.6 tensor(0.1866)
features.3.conv.0 tensor(0.0628)
features.3.conv.3 tensor(0.0748)
features.3.conv.6 tensor(0.1113)
features.4.conv.0 tensor(0.0249)
features.4.conv.3 tensor(0.3073)
features.4.conv.6 tensor(0.1776)
features.5.conv.0 tensor(0.2692)
features.5.conv.3 tensor(0.4317)
features.5.conv.6 tensor(0.1359)
features.6.conv.0 tensor(0.0452)
features.6.conv.3 tensor(0.0631)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.0920)
features.7.conv.3 tensor(0.4387)
features.7.conv.6 tensor(0.2131)
features.8.conv.0 tensor(0.2343)
features.8.conv.3 tensor(0.5370)
features.8.conv.6 tensor(0.1545)
features.9.conv.0 tensor(0.2703)
features.9.conv.3 tensor(0.5411)
features.9.conv.6 tensor(0.1552)
features.10.conv.0 tensor(0.0587)
features.10.conv.3 tensor(0.0966)
features.10.conv.6 tensor(0.0727)
features.11.conv.0 tensor(0.3957)
features.11.conv.3 tensor(0.6472)
features.11.conv.6 tensor(0.2248)
features.12.conv.0 tensor(0.4572)
features.12.conv.3 tensor(0.6543)
features.12.conv.6 tensor(0.5264)
features.13.conv.0 tensor(0.2329)
features.13.conv.3 tensor(0.4875)
features.13.conv.6 tensor(0.1040)
features.14.conv.0 tensor(0.7948)
features.14.conv.3 tensor(0.8499)
features.14.conv.6 tensor(0.9480)
features.15.conv.0 tensor(0.7912)
features.15.conv.3 tensor(0.9059)
features.15.conv.6 tensor(0.9680)
features.16.conv.0 tensor(0.5968)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.1423)
conv.0 tensor(0.0842)
tensor(896594.) 2188896.0
INFO - ==> Top1: 87.390    Top5: 99.610    Loss: 0.370
INFO - ==> Sparsity : 0.410
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
0.79269576
0.79244345
0.79268187
0.79285645
0.79262775
0.79276693
0.79289526
0.79297423
0.79307514
0.79267550
0.79252309
0.79248351
0.79270440
0.79255944
0.79267848
0.79269403
0.79264313
0.79254210
INFO - Training [36][   20/  196]   Loss 0.481515   Top1 83.320312   Top5 97.832031   BatchTime 0.563772   LR 0.000118
0.79221076
0.79205692
0.79192430
0.79168206
0.79164106
0.79139704
0.79100293
0.79082096
0.79080546
0.79104090
0.79103059
0.79061472
0.79057038
0.79073572
0.79023892
0.79033363
0.79363352
0.80660695
0.80744737
0.80738407
0.80714679
0.80694377
INFO - Training [36][   40/  196]   Loss 0.490982   Top1 82.792969   Top5 98.017578   BatchTime 0.510840   LR 0.000118
0.80705082
0.80685961
0.80625641
0.80599177
0.80584300
0.80569381
0.80595565
0.80593759
0.80592412
0.80626220
0.80678278
0.80683464
0.80953395
0.81233436
0.81325310
0.81341946
0.81352043
INFO - Training [36][   60/  196]   Loss 0.491984   Top1 82.877604   Top5 98.183594   BatchTime 0.489968   LR 0.000117
0.81362784
0.81325114
0.81308621
0.81273252
0.81260121
0.81217027
0.81197888
0.81222415
0.81209332
0.81210303
0.81153715
0.81110698
0.81087959
0.81029963
0.81009442
0.81026256
0.81050581
0.81056613
0.81066537
0.81067199
0.81201744
INFO - Training [36][   80/  196]   Loss 0.487781   Top1 82.993164   Top5 98.310547   BatchTime 0.466119   LR 0.000117
0.81215209
0.81207633
0.81195480
0.81214100
0.81208551
0.81233180
0.81234485
0.81205559
0.81233609
0.81254071
0.81207490
0.81237257
0.81228548
0.81230277
0.81191111
0.81208241
0.81236327
0.81217825
0.81214410
0.81251097
0.81290853
0.81304985
INFO - Training [36][  100/  196]   Loss 0.478493   Top1 83.281250   Top5 98.378906   BatchTime 0.462516   LR 0.000117
0.81349307
0.81362778
0.81351906
0.81378651
0.81448925
0.81462997
0.81503320
0.81634855
0.81646824
0.81609690
0.81618208
0.81628472
0.81624818
0.81623316
0.81635499
0.81620139
0.81628698
0.81617355
0.81624293
0.81601036
0.81579304
INFO - Training [36][  120/  196]   Loss 0.475257   Top1 83.378906   Top5 98.473307   BatchTime 0.465271   LR 0.000117
0.81583500
0.81605887
0.81598842
0.81574434
0.81567824
0.81551468
0.81581295
0.81582588
0.81565982
0.81563032
0.81552798
0.81530082
0.81520778
0.81539816
0.81526095
0.81522065
0.81507671
0.81490761
0.81509757
INFO - Training [36][  140/  196]   Loss 0.473069   Top1 83.473772   Top5 98.518415   BatchTime 0.459112   LR 0.000117
0.81513834
0.81521273
0.81495982
0.81483251
0.81511444
0.81484753
0.81464839
0.81449103
0.81454128
0.81438470
0.81438011
0.81457669
0.81477314
0.81505048
0.81514513
0.81475121
0.81456524
0.81429750
0.81406122
INFO - Training [36][  160/  196]   Loss 0.475485   Top1 83.439941   Top5 98.535156   BatchTime 0.454985   LR 0.000116
0.81409621
0.81393409
0.81367326
0.81330955
0.81306547
0.81296033
0.81224966
0.81201386
0.81206369
0.81175947
0.81147957
0.81130624
0.81119543
0.81096047
0.81048083
0.81021076
0.80980909
0.80945522
0.80896872
0.80886060
0.80848116
INFO - Training [36][  180/  196]   Loss 0.473828   Top1 83.465712   Top5 98.535156   BatchTime 0.457220   LR 0.000116
0.80833203
0.80788660
0.80714989
0.80695701
0.80664653
0.80634397
0.80620217
0.80555838
0.80521059
0.80529600
0.80522835
0.80499291
0.80458772
INFO - ==> Top1: 83.490    Top5: 98.552    Loss: 0.473
0.80449104
0.80435449
0.80401713
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [36][   20/   40]   Loss 0.356271   Top1 87.382812   Top5 99.472656   BatchTime 0.140098
INFO - Validation [36][   40/   40]   Loss 0.346923   Top1 88.000000   Top5 99.570000   BatchTime 0.098805
features.0.conv.0 tensor(0.5833)
features.0.conv.3 tensor(0.1309)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0648)
features.1.conv.6 tensor(0.0716)
features.2.conv.0 tensor(0.0457)
features.2.conv.3 tensor(0.3349)
features.2.conv.6 tensor(0.1861)
features.3.conv.0 tensor(0.0590)
features.3.conv.3 tensor(0.0725)
features.3.conv.6 tensor(0.1120)
features.4.conv.0 tensor(0.0246)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1787)
features.5.conv.0 tensor(0.2694)
features.5.conv.3 tensor(0.4253)
features.5.conv.6 tensor(0.1388)
features.6.conv.0 tensor(0.0487)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0846)
features.7.conv.0 tensor(0.0927)
features.7.conv.3 tensor(0.4381)
features.7.conv.6 tensor(0.2210)
features.8.conv.0 tensor(0.2033)
features.8.conv.3 tensor(0.5379)
features.8.conv.6 tensor(0.1546)
features.9.conv.0 tensor(0.2739)
features.9.conv.3 tensor(0.5411)
features.9.conv.6 tensor(0.1522)
features.10.conv.0 tensor(0.0545)
features.10.conv.3 tensor(0.0981)
features.10.conv.6 tensor(0.0739)
features.11.conv.0 tensor(0.4018)
features.11.conv.3 tensor(0.6483)
features.11.conv.6 tensor(0.2109)
features.12.conv.0 tensor(0.4475)
features.12.conv.3 tensor(0.6541)
features.12.conv.6 tensor(0.5109)
features.13.conv.0 tensor(0.2348)
features.13.conv.3 tensor(0.4882)
features.13.conv.6 tensor(0.1044)
features.14.conv.0 tensor(0.7982)
features.14.conv.3 tensor(0.8490)
features.14.conv.6 tensor(0.9485)
features.15.conv.0 tensor(0.7934)
features.15.conv.3 tensor(0.9066)
features.15.conv.6 tensor(0.9676)
features.16.conv.0 tensor(0.4896)
features.16.conv.3 tensor(0.8044)
features.16.conv.6 tensor(0.1389)
conv.0 tensor(0.0839)
tensor(877570.) 2188896.0
INFO - ==> Top1: 88.000    Top5: 99.570    Loss: 0.347
INFO - ==> Sparsity : 0.401
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
0.80348229
0.80325341
0.80328685
0.80286366
0.80279452
0.80236650
0.80245751
0.80219322
0.80196822
0.80163532
0.80157799
0.80145574
0.80149269
0.80127639
0.80089390
0.80076474
0.80073452
0.80026245
0.79971963
INFO - Training [37][   20/  196]   Loss 0.473293   Top1 83.613281   Top5 98.066406   BatchTime 0.555232   LR 0.000116
0.79917401
0.79896849
0.79852802
0.79800153
0.79770708
0.79755491
0.79738784
0.79715115
0.79703790
0.79685587
0.79678255
0.79674822
0.79656971
0.79641777
0.79623532
0.79621172
0.79591709
0.79601908
0.79554778
0.79493982
0.79519075
INFO - Training [37][   40/  196]   Loss 0.478317   Top1 83.486328   Top5 98.115234   BatchTime 0.505847   LR 0.000115
0.79540771
0.79517335
0.79506791
0.79515028
0.79509044
0.79479146
0.79447377
0.79439837
0.79459053
0.79457492
0.79469335
0.79477376
0.79519790
0.79685932
0.79687828
0.79681331
0.79682404
0.79663432
INFO - Training [37][   60/  196]   Loss 0.472483   Top1 83.522135   Top5 98.144531   BatchTime 0.492273   LR 0.000115
0.79655898
0.79639560
0.79635775
0.79636794
0.79620326
0.79628259
0.79562712
0.79539770
0.79554057
0.79510695
0.79489666
0.79472679
0.79479927
0.79468113
0.79459864
0.79451948
0.79428571
0.79439038
0.79438275
0.79416180
0.79412001
0.79495907
0.79482663
INFO - Training [37][   80/  196]   Loss 0.475327   Top1 83.574219   Top5 98.344727   BatchTime 0.473304   LR 0.000115
0.79461336
0.79454803
0.79446822
0.79421479
0.79405499
0.79369819
0.79374057
0.79400992
0.79432923
0.79385257
0.79353845
0.79348093
0.79379743
0.79409999
0.79371291
0.79362881
0.79341304
0.79360378
INFO - Training [37][  100/  196]   Loss 0.466017   Top1 83.707031   Top5 98.500000   BatchTime 0.469926   LR 0.000114
0.79391259
0.79367459
0.79334301
0.79277170
0.79285824
0.79284531
0.79307985
0.79302275
0.79300147
0.79288954
0.79308307
0.79310507
0.79277217
0.79166043
0.79292834
0.79266000
0.79250336
0.79248935
0.79236108
0.79220706
0.79243439
INFO - Training [37][  120/  196]   Loss 0.461509   Top1 83.867188   Top5 98.538411   BatchTime 0.470218   LR 0.000114
0.79247826
0.79220527
0.79220092
0.79194957
0.79185158
0.79450852
0.80721545
0.80929530
0.80923802
0.80901998
0.80882210
0.80890286
0.80863148
0.80859417
0.80878127
0.80876911
0.80864906
INFO - Training [37][  140/  196]   Loss 0.463347   Top1 83.875558   Top5 98.574219   BatchTime 0.467494   LR 0.000114
0.80879170
0.80841726
0.80834156
0.80815113
0.80805349
0.80779940
0.80731177
0.80713964
0.80705655
0.80696869
0.80680007
0.80644846
0.80648094
0.80671144
0.80708271
0.80728263
0.80665869
0.80657208
0.80658960
0.80632895
0.80617499
0.80575144
0.80498654
0.80479199
INFO - Training [37][  160/  196]   Loss 0.464860   Top1 83.806152   Top5 98.601074   BatchTime 0.462542   LR 0.000114
0.80467224
0.80481392
0.80470026
0.80455452
0.80561018
0.80581838
0.80561614
0.80564028
0.80548334
0.80541587
0.80548334
0.80528158
0.80513179
0.80478585
0.80482274
0.80461222
0.80435371
INFO - Training [37][  180/  196]   Loss 0.463235   Top1 83.873698   Top5 98.565538   BatchTime 0.462371   LR 0.000113
0.80446059
0.80427587
0.80440462
0.80412644
0.80367732
0.80356234
0.80315280
0.80334032
0.80435175
0.80402058
0.80352259
0.80313200
0.80304009
0.80292195
0.80273134
0.80247843
0.80266470
INFO - ==> Top1: 83.890    Top5: 98.576    Loss: 0.462
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.80236351
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [37][   20/   40]   Loss 0.360743   Top1 87.890625   Top5 99.492188   BatchTime 0.140104
INFO - Validation [37][   40/   40]   Loss 0.345513   Top1 88.460000   Top5 99.590000   BatchTime 0.097786
INFO - ==> Top1: 88.460    Top5: 99.590    Loss: 0.346
INFO - ==> Sparsity : 0.402
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  38
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.5972)
features.0.conv.3 tensor(0.1406)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0567)
features.1.conv.6 tensor(0.0755)
features.2.conv.0 tensor(0.0437)
features.2.conv.3 tensor(0.3387)
features.2.conv.6 tensor(0.1829)
features.3.conv.0 tensor(0.0582)
features.3.conv.3 tensor(0.0694)
features.3.conv.6 tensor(0.1113)
features.4.conv.0 tensor(0.0290)
features.4.conv.3 tensor(0.3079)
features.4.conv.6 tensor(0.1776)
features.5.conv.0 tensor(0.2679)
features.5.conv.3 tensor(0.4271)
features.5.conv.6 tensor(0.1369)
features.6.conv.0 tensor(0.0501)
features.6.conv.3 tensor(0.0556)
features.6.conv.6 tensor(0.0833)
features.7.conv.0 tensor(0.0920)
features.7.conv.3 tensor(0.4378)
features.7.conv.6 tensor(0.2286)
features.8.conv.0 tensor(0.2247)
features.8.conv.3 tensor(0.5385)
features.8.conv.6 tensor(0.1564)
features.9.conv.0 tensor(0.4166)
features.9.conv.3 tensor(0.5437)
features.9.conv.6 tensor(0.1604)
features.10.conv.0 tensor(0.0556)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.0734)
features.11.conv.0 tensor(0.4548)
features.11.conv.3 tensor(0.6485)
features.11.conv.6 tensor(0.2191)
features.12.conv.0 tensor(0.4528)
features.12.conv.3 tensor(0.6549)
features.12.conv.6 tensor(0.5284)
features.13.conv.0 tensor(0.2339)
features.13.conv.3 tensor(0.4886)
features.13.conv.6 tensor(0.1033)
features.14.conv.0 tensor(0.7960)
features.14.conv.3 tensor(0.8495)
features.14.conv.6 tensor(0.9483)
features.15.conv.0 tensor(0.7936)
features.15.conv.3 tensor(0.9061)
features.15.conv.6 tensor(0.9677)
features.16.conv.0 tensor(0.4366)
features.16.conv.3 tensor(0.8050)
features.16.conv.6 tensor(0.1426)
conv.0 tensor(0.0837)
tensor(879074.) 2188896.0
0.80225945
0.80190885
0.80178541
0.80176264
0.80184180
0.80183929
0.80193532
0.80246699
0.80254310
0.80239755
0.80236256
0.80276543
0.80278754
0.80263072
0.80266672
0.80271429
0.80252451
0.80240518
0.80253643
INFO - Training [38][   20/  196]   Loss 0.460842   Top1 83.808594   Top5 98.085938   BatchTime 0.553234   LR 0.000113
0.80273855
0.80289370
0.80335480
0.80350852
0.80338150
0.80348456
0.80366582
0.80301082
0.80319077
0.80315971
0.80323994
0.80323267
0.80314785
0.80307096
0.80290604
0.80268532
0.80262536
0.80281305
INFO - Training [38][   40/  196]   Loss 0.471112   Top1 83.505859   Top5 98.242188   BatchTime 0.506062   LR 0.000112
0.80288273
0.80310786
0.80325925
0.80335093
0.80330241
0.80362374
0.80359536
0.80373472
0.80428576
0.80406874
0.80483204
0.80711240
0.80749947
0.80796850
0.80794650
0.80757040
0.80730152
0.80714405
0.80705261
0.80705804
0.80700696
INFO - Training [38][   60/  196]   Loss 0.463637   Top1 83.886719   Top5 98.333333   BatchTime 0.493235   LR 0.000112
0.80695605
0.80669403
0.80642545
0.80607051
0.80609858
0.80586904
0.80539829
0.80521607
0.80486804
0.80449980
0.80442059
0.80446863
0.80438751
0.80383068
0.80343741
0.80325544
0.80220085
0.80211312
0.80195308
INFO - Training [38][   80/  196]   Loss 0.466792   Top1 83.706055   Top5 98.452148   BatchTime 0.475894   LR 0.000112
0.80174685
0.80126166
0.80110306
0.80142701
0.80103421
0.80061221
0.80070406
0.80057752
0.79986566
0.79953462
0.79965550
0.79915142
0.79861069
0.79794645
0.79734188
0.79724878
0.79683387
0.79626703
0.79541934
0.79507774
0.79475468
INFO - Training [38][  100/  196]   Loss 0.458443   Top1 83.945312   Top5 98.531250   BatchTime 0.476872   LR 0.000112
0.79514480
0.79511327
0.79488140
0.79512179
0.79495507
0.79475349
0.79476523
0.79470837
0.79450822
0.79478902
0.79422665
0.79387867
0.79410094
0.79399943
0.79396540
0.79368025
0.79377991
0.79389066
0.79413873
0.79384857
0.79364949
INFO - Training [38][  120/  196]   Loss 0.457672   Top1 84.000651   Top5 98.613281   BatchTime 0.477644   LR 0.000111
0.79347873
0.79315513
0.79306078
0.79328108
0.79365748
0.79356706
0.79354018
0.79375839
0.79377043
0.79366642
0.79371887
0.79361564
0.79366452
0.79377848
0.79367965
0.79394174
0.79399258
0.79376614
0.79375315
INFO - Training [38][  140/  196]   Loss 0.458285   Top1 84.029018   Top5 98.674665   BatchTime 0.470883   LR 0.000111
0.79368019
0.79363370
0.79360062
0.79341865
0.79349428
0.79362518
0.79366374
0.79342496
0.79347855
0.79355896
0.79337001
0.79323554
0.79329324
0.79340351
0.79341847
0.79334891
0.79333532
0.79352403
0.79356426
0.79323047
0.79303741
0.79307437
INFO - Training [38][  160/  196]   Loss 0.460660   Top1 83.945312   Top5 98.649902   BatchTime 0.467633   LR 0.000111
0.79329056
0.79311329
0.79326934
0.79358166
0.79349899
0.79358077
0.79338491
0.79313761
0.79276031
0.79260868
0.79211318
0.79231799
0.79222363
0.79190046
0.79151469
0.79083127
0.79069543
INFO - Training [38][  180/  196]   Loss 0.461568   Top1 83.854167   Top5 98.624132   BatchTime 0.467221   LR 0.000110
0.79068828
0.79041570
0.79039121
0.79014981
0.78952557
0.78942394
0.78950369
0.78942162
0.78985941
0.79051155
0.79040480
0.79028696
0.79037005
0.79034930
0.79040521
0.79042870
0.79005963
INFO - ==> Top1: 83.864    Top5: 98.606    Loss: 0.461
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.78983504
0.78961533
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [38][   20/   40]   Loss 0.349026   Top1 88.300781   Top5 99.511719   BatchTime 0.143268
INFO - Validation [38][   40/   40]   Loss 0.338434   Top1 88.660000   Top5 99.630000   BatchTime 0.099011
INFO - ==> Top1: 88.660    Top5: 99.630    Loss: 0.338
INFO - ==> Sparsity : 0.412
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.880   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  39
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.6007)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0477)
features.2.conv.3 tensor(0.3372)
features.2.conv.6 tensor(0.1811)
features.3.conv.0 tensor(0.0538)
features.3.conv.3 tensor(0.0694)
features.3.conv.6 tensor(0.1161)
features.4.conv.0 tensor(0.0299)
features.4.conv.3 tensor(0.3119)
features.4.conv.6 tensor(0.1797)
features.5.conv.0 tensor(0.2664)
features.5.conv.3 tensor(0.4294)
features.5.conv.6 tensor(0.1348)
features.6.conv.0 tensor(0.0456)
features.6.conv.3 tensor(0.0550)
features.6.conv.6 tensor(0.0836)
features.7.conv.0 tensor(0.0898)
features.7.conv.3 tensor(0.4395)
features.7.conv.6 tensor(0.2315)
features.8.conv.0 tensor(0.2399)
features.8.conv.3 tensor(0.5391)
features.8.conv.6 tensor(0.1567)
features.9.conv.0 tensor(0.2277)
features.9.conv.3 tensor(0.5379)
features.9.conv.6 tensor(0.1527)
features.10.conv.0 tensor(0.0573)
features.10.conv.3 tensor(0.0938)
features.10.conv.6 tensor(0.0740)
features.11.conv.0 tensor(0.4021)
features.11.conv.3 tensor(0.6478)
features.11.conv.6 tensor(0.2191)
features.12.conv.0 tensor(0.4534)
features.12.conv.3 tensor(0.6562)
features.12.conv.6 tensor(0.5320)
features.13.conv.0 tensor(0.2328)
features.13.conv.3 tensor(0.4873)
features.13.conv.6 tensor(0.1063)
features.14.conv.0 tensor(0.8009)
features.14.conv.3 tensor(0.8495)
features.14.conv.6 tensor(0.9472)
features.15.conv.0 tensor(0.8011)
features.15.conv.3 tensor(0.9050)
features.15.conv.6 tensor(0.9663)
features.16.conv.0 tensor(0.6194)
features.16.conv.3 tensor(0.8047)
features.16.conv.6 tensor(0.1421)
conv.0 tensor(0.0839)
tensor(901742.) 2188896.0
0.78962022
0.78963953
0.78934503
0.78945959
0.78941256
0.78935200
0.78934616
0.78898859
0.78896052
0.78918171
0.78903461
0.78905594
0.78905916
0.78921181
0.78928846
0.78927428
0.78933090
0.78935421
0.78945613
INFO - Training [39][   20/  196]   Loss 0.467967   Top1 83.437500   Top5 98.457031   BatchTime 0.562794   LR 0.000110
0.78908145
0.78905118
0.78918344
0.78916705
0.78940946
0.78948486
0.78943115
0.78964090
0.78975666
0.79005784
0.79097545
0.79330850
0.80529845
0.80745119
0.80734015
0.80736321
0.80768353
0.80777830
0.80762148
0.80769891
0.80753344
INFO - Training [39][   40/  196]   Loss 0.456830   Top1 83.935547   Top5 98.535156   BatchTime 0.514199   LR 0.000109
0.80747443
0.80719364
0.80706406
0.80688131
0.80670255
0.80659664
0.80650634
0.80631423
0.80657995
0.80705887
0.80721766
0.80710018
0.80697781
0.80708760
0.80687463
0.80708593
0.80716008
0.80686378
0.80665773
INFO - Training [39][   60/  196]   Loss 0.456242   Top1 84.003906   Top5 98.593750   BatchTime 0.487614   LR 0.000109
0.80688757
0.80695641
0.80705380
0.80738342
0.80731237
0.80697083
0.80694211
0.80661738
0.80640697
0.80655617
0.80695111
0.80710751
0.80672681
0.80662453
0.80685347
0.80661809
0.80654156
0.80664307
0.80693775
0.80690110
0.80702245
INFO - Training [39][   80/  196]   Loss 0.455033   Top1 84.194336   Top5 98.671875   BatchTime 0.482589   LR 0.000109
0.80716920
0.80707538
0.80715328
0.80727273
0.80729979
0.80717134
0.80720657
0.80703449
0.80710822
0.80705601
0.80689728
0.80679578
0.80681849
0.80677295
0.80699313
0.80693316
0.80735058
0.80751204
0.80793053
0.80856913
0.80857694
INFO - Training [39][  100/  196]   Loss 0.456020   Top1 84.152344   Top5 98.714844   BatchTime 0.482142   LR 0.000108
0.80922091
0.80941612
0.80922401
0.80961877
0.81034750
0.81153655
0.81224859
0.81223118
0.81189811
0.81164497
0.81196934
0.81165099
0.81153822
0.81158662
0.81176025
0.81168473
0.81176215
INFO - Training [39][  120/  196]   Loss 0.453058   Top1 84.287109   Top5 98.759766   BatchTime 0.480922   LR 0.000108
0.81165057
0.81128079
0.81101161
0.81090128
0.81104624
0.81094939
0.81075859
0.81124645
0.81157690
0.81113011
0.81086123
0.81060827
0.80995977
0.80957222
0.80960339
0.80974591
0.80990583
0.80992752
0.81021380
0.81028503
INFO - Training [39][  140/  196]   Loss 0.451569   Top1 84.296875   Top5 98.783482   BatchTime 0.469207   LR 0.000108
0.81040692
0.81037754
0.81059766
0.81017452
0.80942303
0.80950660
0.80985016
0.80991095
0.80950737
0.80963069
0.80948818
0.80943257
0.80944973
0.80954653
0.80908340
0.80878288
0.80930287
0.80936992
0.80927050
0.80905700
0.80889040
INFO - Training [39][  160/  196]   Loss 0.453439   Top1 84.213867   Top5 98.764648   BatchTime 0.470121   LR 0.000107
0.80860239
0.80838460
0.80839819
0.80833477
0.80830663
0.80815578
0.80803299
0.80809730
0.80851519
0.80870062
0.80828530
0.80748433
0.80726862
0.80751067
0.80772144
0.80768543
0.80792451
0.80753291
0.80769080
0.80785257
0.80772519
INFO - Training [39][  180/  196]   Loss 0.454710   Top1 84.151476   Top5 98.719618   BatchTime 0.470613   LR 0.000107
0.80741578
0.80692536
0.80679595
0.80662024
0.80665928
0.80617291
0.80565590
0.80510020
0.80462468
0.80442762
0.80381691
0.80342776
0.80322284
0.80274963
0.80209470
0.80177784
********************pre-trained*****************
INFO - ==> Top1: 84.128    Top5: 98.692    Loss: 0.455
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [39][   20/   40]   Loss 0.339412   Top1 88.515625   Top5 99.531250   BatchTime 0.161014
INFO - Validation [39][   40/   40]   Loss 0.323512   Top1 89.040000   Top5 99.660000   BatchTime 0.109708
INFO - ==> Top1: 89.040    Top5: 99.660    Loss: 0.324
INFO - ==> Sparsity : 0.401
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 89.040   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
features.0.conv.0 tensor(0.6076)
features.0.conv.3 tensor(0.1465)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0751)
features.2.conv.0 tensor(0.0509)
features.2.conv.3 tensor(0.3326)
features.2.conv.6 tensor(0.1823)
features.3.conv.0 tensor(0.0613)
features.3.conv.3 tensor(0.0718)
features.3.conv.6 tensor(0.1174)
features.4.conv.0 tensor(0.0316)
features.4.conv.3 tensor(0.3113)
features.4.conv.6 tensor(0.1795)
features.5.conv.0 tensor(0.2677)
features.5.conv.3 tensor(0.4311)
features.5.conv.6 tensor(0.1340)
features.6.conv.0 tensor(0.0479)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0826)
features.7.conv.0 tensor(0.0953)
features.7.conv.3 tensor(0.4389)
features.7.conv.6 tensor(0.2239)
features.8.conv.0 tensor(0.2098)
features.8.conv.3 tensor(0.5365)
features.8.conv.6 tensor(0.1574)
features.9.conv.0 tensor(0.2435)
features.9.conv.3 tensor(0.5422)
features.9.conv.6 tensor(0.1557)
features.10.conv.0 tensor(0.0566)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.0713)
features.11.conv.0 tensor(0.4015)
features.11.conv.3 tensor(0.6495)
features.11.conv.6 tensor(0.2122)
features.12.conv.0 tensor(0.4503)
features.12.conv.3 tensor(0.6539)
features.12.conv.6 tensor(0.5267)
features.13.conv.0 tensor(0.2326)
features.13.conv.3 tensor(0.4867)
features.13.conv.6 tensor(0.1042)
features.14.conv.0 tensor(0.7990)
features.14.conv.3 tensor(0.8498)
features.14.conv.6 tensor(0.9497)
features.15.conv.0 tensor(0.7999)
features.15.conv.3 tensor(0.9032)
features.15.conv.6 tensor(0.9663)
features.16.conv.0 tensor(0.4670)
features.16.conv.3 tensor(0.8043)
features.16.conv.6 tensor(0.1449)
conv.0 tensor(0.0840)
tensor(877719.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  40
INFO - Training: 50000 samples (256 per mini-batch)
0.80132884
0.80077070
0.80022526
0.80013764
0.80023158
0.80001181
0.79944813
0.79873121
0.79824418
0.79791754
0.79741073
0.79652637
0.79569846
0.79538685
0.79489768
0.79461825
0.79429775
0.79436159
0.79418349
INFO - Training [40][   20/  196]   Loss 0.458789   Top1 83.867188   Top5 97.988281   BatchTime 0.550369   LR 0.000106
0.79422122
0.79400355
0.79340309
0.79263532
0.79247332
0.79166049
0.79096884
0.79048491
0.79034930
0.78989905
0.78938437
0.78893626
0.78872597
0.78829491
0.78774875
0.78735399
0.78694189
0.78689921
0.78758198
INFO - Training [40][   40/  196]   Loss 0.463615   Top1 83.740234   Top5 98.164062   BatchTime 0.479468   LR 0.000106
0.78745162
0.78757095
0.78741103
0.78745282
0.78772837
0.78741664
0.78756291
0.78775620
0.78787667
0.78804862
0.78814483
0.78885627
0.79030776
0.79292941
0.79763478
0.80441302
0.80487061
0.80478406
0.80475390
0.80483264
0.80462879
0.80421585
INFO - Training [40][   60/  196]   Loss 0.463487   Top1 83.613281   Top5 98.333333   BatchTime 0.478845   LR 0.000106
0.80424893
0.80380112
0.80370778
0.80354697
0.80374384
0.80405116
0.80412239
0.80431247
0.80438161
0.80421287
0.80577284
0.81210274
0.81697899
0.81695139
0.81681699
0.81675678
0.81680566
0.81681162
0.81633264
0.81627983
0.81626904
INFO - Training [40][   80/  196]   Loss 0.460090   Top1 83.769531   Top5 98.500977   BatchTime 0.474559   LR 0.000105
0.81638426
0.81649828
0.81670928
0.81682926
0.81682235
0.81667030
0.81694067
0.81649262
0.81662482
0.81613517
0.81603700
0.81600606
0.81588584
0.81565702
0.81586516
0.81579125
0.81569529
0.81588846
0.81607592
INFO - Training [40][  100/  196]   Loss 0.458621   Top1 83.683594   Top5 98.531250   BatchTime 0.467443   LR 0.000105
0.81629300
0.81612200
0.81592590
0.81591517
0.81584269
0.81580025
0.81602514
0.81555736
0.81539494
0.81583607
0.81596875
0.81584537
0.81591439
0.81583720
0.81564039
0.81550682
0.81538039
0.81500733
0.81466895
INFO - Training [40][  120/  196]   Loss 0.454390   Top1 83.805339   Top5 98.623047   BatchTime 0.459256   LR 0.000105
0.81453246
0.81462020
0.81458050
0.81451070
0.81447273
0.81461012
0.81488568
0.81494832
0.81505430
0.81494647
0.81512219
0.81504047
0.81515270
0.81521368
0.81550419
0.81570774
0.81619328
0.81596887
0.81576550
0.81568605
0.81550980
0.81540334
INFO - Training [40][  140/  196]   Loss 0.452697   Top1 83.942522   Top5 98.683036   BatchTime 0.460036   LR 0.000104
0.81524795
0.81511980
0.81532949
0.81536698
0.81518775
0.81493163
0.81428319
0.81376320
0.81345636
0.81309944
0.81283432
0.81258297
0.81261122
0.81238747
0.81190944
0.81175166
0.81191015
0.81177872
INFO - Training [40][  160/  196]   Loss 0.455833   Top1 83.889160   Top5 98.657227   BatchTime 0.458395   LR 0.000104
0.81178880
0.81172508
0.81226426
0.81244999
0.81231076
0.81226015
0.81247956
0.81253219
0.81251591
0.81236315
0.81241208
0.81214815
0.81186408
0.81198460
0.81192732
0.81139731
0.81129509
0.81121260
0.81098431
INFO - Training [40][  180/  196]   Loss 0.456768   Top1 83.875868   Top5 98.621962   BatchTime 0.464770   LR 0.000103
0.81044060
0.81020600
0.80986720
0.80962592
0.80914062
0.80876917
0.81245124
0.81415403
0.81407672
0.81383359
0.81377822
0.81382966
0.81340218
0.81289601
0.81246006
0.81209540
0.81175739
INFO - ==> Top1: 84.032    Top5: 98.624    Loss: 0.454
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.81131154
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [40][   20/   40]   Loss 0.347011   Top1 88.574219   Top5 99.667969   BatchTime 0.141968
INFO - Validation [40][   40/   40]   Loss 0.342047   Top1 88.440000   Top5 99.700000   BatchTime 0.098703
INFO - ==> Top1: 88.440    Top5: 99.700    Loss: 0.342
INFO - ==> Sparsity : 0.401
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 89.040   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  41
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.6076)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0443)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0437)
features.2.conv.3 tensor(0.3310)
features.2.conv.6 tensor(0.1794)
features.3.conv.0 tensor(0.0576)
features.3.conv.3 tensor(0.0694)
features.3.conv.6 tensor(0.1137)
features.4.conv.0 tensor(0.0353)
features.4.conv.3 tensor(0.3108)
features.4.conv.6 tensor(0.1800)
features.5.conv.0 tensor(0.3022)
features.5.conv.3 tensor(0.4300)
features.5.conv.6 tensor(0.1351)
features.6.conv.0 tensor(0.0433)
features.6.conv.3 tensor(0.0561)
features.6.conv.6 tensor(0.0836)
features.7.conv.0 tensor(0.0911)
features.7.conv.3 tensor(0.4375)
features.7.conv.6 tensor(0.2265)
features.8.conv.0 tensor(0.2203)
features.8.conv.3 tensor(0.5382)
features.8.conv.6 tensor(0.1571)
features.9.conv.0 tensor(0.2358)
features.9.conv.3 tensor(0.5405)
features.9.conv.6 tensor(0.1551)
features.10.conv.0 tensor(0.0516)
features.10.conv.3 tensor(0.0940)
features.10.conv.6 tensor(0.0719)
features.11.conv.0 tensor(0.4089)
features.11.conv.3 tensor(0.6472)
features.11.conv.6 tensor(0.2150)
features.12.conv.0 tensor(0.3354)
features.12.conv.3 tensor(0.6534)
features.12.conv.6 tensor(0.5298)
features.13.conv.0 tensor(0.2329)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.1048)
features.14.conv.0 tensor(0.8037)
features.14.conv.3 tensor(0.8507)
features.14.conv.6 tensor(0.9496)
features.15.conv.0 tensor(0.8060)
features.15.conv.3 tensor(0.9043)
features.15.conv.6 tensor(0.9669)
features.16.conv.0 tensor(0.4943)
features.16.conv.3 tensor(0.8049)
features.16.conv.6 tensor(0.1453)
conv.0 tensor(0.0837)
tensor(878185.) 2188896.0
0.81082594
0.81089073
0.81039965
0.80966485
0.80753869
0.80720991
0.80749166
0.80712855
0.80690747
0.80662435
0.80678898
0.80713880
0.80730647
0.80717313
0.80702984
0.80694777
0.80684340
0.80679214
0.80776966
0.80783617
0.80770290
INFO - Training [41][   20/  196]   Loss 0.466253   Top1 83.593750   Top5 98.398438   BatchTime 0.546749   LR 0.000103
0.80792958
0.80771679
0.80776674
0.80794227
0.80780333
0.80777752
0.80804598
0.80776894
0.80811083
0.80809718
0.80792785
0.80759698
0.80740863
0.80726081
0.80708313
0.80706626
0.80664265
0.80638874
0.80622047
0.80606067
INFO - Training [41][   40/  196]   Loss 0.474334   Top1 83.369141   Top5 98.271484   BatchTime 0.471785   LR 0.000102
0.80560201
0.80542350
0.80537117
0.80559450
0.80552965
0.80583829
0.80563033
0.80552989
0.80550474
0.80533260
0.80517238
0.80535442
0.81295192
0.81830466
0.81840974
0.81848812
0.81829602
0.81815165
0.81808931
INFO - Training [41][   60/  196]   Loss 0.464503   Top1 83.756510   Top5 98.411458   BatchTime 0.452470   LR 0.000102
0.81725198
0.81679630
0.81644106
0.81592917
0.81576020
0.81583416
0.81612271
0.81624418
0.81595546
0.81572276
0.81515694
0.81493819
0.81462586
0.81459856
0.81420982
0.81415737
0.81383020
INFO - Training [41][   80/  196]   Loss 0.462577   Top1 83.793945   Top5 98.510742   BatchTime 0.454762   LR 0.000102
0.81360579
0.81384313
0.81373215
0.81367129
0.81355715
0.81359076
0.81371564
0.81330585
0.81307536
0.81309313
0.81291348
0.81277782
0.81261814
0.81246030
0.81423092
0.81498182
0.81491554
0.81498337
0.81530291
0.81510574
0.81499779
0.81475365
0.81449890
0.81435674
INFO - Training [41][  100/  196]   Loss 0.456537   Top1 84.074219   Top5 98.578125   BatchTime 0.446952   LR 0.000101
0.81403601
0.81391293
0.81394815
0.81397986
0.81370264
0.81376302
0.81375754
0.81414831
0.81408888
0.81365448
0.81327081
0.81307977
0.81313759
0.81315178
0.81306303
0.81300700
0.81304693
0.81295598
INFO - Training [41][  120/  196]   Loss 0.450324   Top1 84.283854   Top5 98.623047   BatchTime 0.447325   LR 0.000101
0.81306911
0.81287396
0.81289154
0.81287348
0.81252891
0.81231856
0.81240171
0.81219590
0.81236714
0.81231987
0.81218779
0.81193846
0.81184059
0.81188941
0.81194735
0.81199193
0.81199157
0.81171036
0.81134629
0.81114972
0.81101793
INFO - Training [41][  140/  196]   Loss 0.446431   Top1 84.400112   Top5 98.708147   BatchTime 0.450715   LR 0.000100
0.81083483
0.81067091
0.81045729
0.81022602
0.81012887
0.80958378
0.80938399
0.80950809
0.80969977
0.80939633
0.80907774
0.80896032
0.80897725
0.80899966
0.80908585
0.80912554
0.80872190
0.80847585
0.80834085
0.80805808
INFO - Training [41][  160/  196]   Loss 0.448491   Top1 84.284668   Top5 98.725586   BatchTime 0.457143   LR 0.000100
0.80774266
0.80736697
0.80703288
0.80685103
0.80656356
0.80629057
0.80621260
0.80604857
0.80580300
0.80566096
0.80558318
0.80530006
0.80509478
0.80480117
0.80436182
0.80402637
0.80394894
INFO - Training [41][  180/  196]   Loss 0.449087   Top1 84.318576   Top5 98.684896   BatchTime 0.458464   LR 0.000100
0.80379397
0.80344343
0.80316645
0.80271661
0.80237448
0.80210900
0.80168062
0.80165684
0.80163217
0.80134094
0.80093426
0.80046886
0.80000204
0.79942912
0.79857039
0.79795140
0.79757249
INFO - ==> Top1: 84.442    Top5: 98.688    Loss: 0.446
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.79705656
0.79667658
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [41][   20/   40]   Loss 0.340805   Top1 88.613281   Top5 99.648438   BatchTime 0.139974
INFO - Validation [41][   40/   40]   Loss 0.334357   Top1 88.840000   Top5 99.660000   BatchTime 0.097180
INFO - ==> Top1: 88.840    Top5: 99.660    Loss: 0.334
INFO - ==> Sparsity : 0.403
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 89.040   Top5: 99.660]
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 88.960   Top5: 99.710]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 88.910   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-072416/_checkpoint.pth.tar
INFO - >>>>>> Epoch  42
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.6146)
features.0.conv.3 tensor(0.1387)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.0356)
features.2.conv.3 tensor(0.3295)
features.2.conv.6 tensor(0.1791)
features.3.conv.0 tensor(0.0605)
features.3.conv.3 tensor(0.0671)
features.3.conv.6 tensor(0.1150)
features.4.conv.0 tensor(0.0303)
features.4.conv.3 tensor(0.3044)
features.4.conv.6 tensor(0.1816)
features.5.conv.0 tensor(0.2716)
features.5.conv.3 tensor(0.4288)
features.5.conv.6 tensor(0.1393)
features.6.conv.0 tensor(0.0443)
features.6.conv.3 tensor(0.0584)
features.6.conv.6 tensor(0.0824)
features.7.conv.0 tensor(0.0933)
features.7.conv.3 tensor(0.4355)
features.7.conv.6 tensor(0.2280)
features.8.conv.0 tensor(0.2378)
features.8.conv.3 tensor(0.5385)
features.8.conv.6 tensor(0.1572)
features.9.conv.0 tensor(0.2386)
features.9.conv.3 tensor(0.5385)
features.9.conv.6 tensor(0.1523)
features.10.conv.0 tensor(0.0529)
features.10.conv.3 tensor(0.0943)
features.10.conv.6 tensor(0.0722)
features.11.conv.0 tensor(0.4055)
features.11.conv.3 tensor(0.6503)
features.11.conv.6 tensor(0.2151)
features.12.conv.0 tensor(0.4400)
features.12.conv.3 tensor(0.6535)
features.12.conv.6 tensor(0.5284)
features.13.conv.0 tensor(0.2464)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.1052)
features.14.conv.0 tensor(0.8061)
features.14.conv.3 tensor(0.8508)
features.14.conv.6 tensor(0.9514)
features.15.conv.0 tensor(0.8019)
features.15.conv.3 tensor(0.9039)
features.15.conv.6 tensor(0.9669)
features.16.conv.0 tensor(0.4790)
features.16.conv.3 tensor(0.8043)
features.16.conv.6 tensor(0.1452)
conv.0 tensor(0.0832)
tensor(882267.) 2188896.0
0.79637325
0.79610300
0.79552764
0.79533440
0.79488194
0.79398227
0.79331815
0.79289639
0.79349005
0.79364008
0.79364735
0.79365486
0.79344743
0.79276901
0.79248297
0.79211181
0.79205519
0.79174030
0.79189998
INFO - Training [42][   20/  196]   Loss 0.484091   Top1 82.890625   Top5 98.066406   BatchTime 0.553609   LR 0.000099
0.79158348
0.79168946
0.79138327
0.79127538
0.79110128
0.79130793
0.79104769
0.79080182
0.79071862
0.79062408
0.79074055
0.79119915
0.79095256
0.79104567
0.79122365
0.79092282
0.79063892
0.79064459
0.79078227
INFO - Training [42][   40/  196]   Loss 0.459990   Top1 83.837891   Top5 98.339844   BatchTime 0.492482   LR 0.000098
0.79225248
0.79231793
0.79193258
0.79185575
0.79188275
0.79201466
0.79203218
0.79172170
0.79184151
0.79205525
0.79205573
0.79186273
0.79200989
0.79220504
0.79269862
0.79218817
0.79221582
0.79237974
0.79220581
0.79212284
0.79205662
0.79233789
0.79268420
INFO - Training [42][   60/  196]   Loss 0.459271   Top1 83.932292   Top5 98.417969   BatchTime 0.476175   LR 0.000098
0.79240131
0.79251736
0.79205817
0.79200447
0.79222006
0.79239219
0.79248512
0.79239756
0.79251570
0.79243052
0.79209316
0.79208720
0.79223621
0.79236233
0.79211009
0.79219615
0.79244423
0.79257739
INFO - Training [42][   80/  196]   Loss 0.454658   Top1 84.052734   Top5 98.515625   BatchTime 0.468224   LR 0.000098
0.79255664
0.79233605
0.79208618
0.79206485
0.79185957
0.79183096
0.79187799
0.79191023
0.79215395
0.79207432
0.79196852
0.79181826
0.79172474
0.79173303
0.79198986
0.79202640
0.79210705
0.79249460
0.79230702
INFO - Training [42][  100/  196]   Loss 0.450415   Top1 84.269531   Top5 98.566406   BatchTime 0.458757   LR 0.000097
0.79191589
0.79218262
0.79207438
0.79211682
0.79189897
0.79167479
0.79130983
0.79190004
0.79178631
0.79183626
0.79203755
0.79186678
0.79202706
0.79161358
0.79090297
0.79074961
0.79101682
0.79099095
0.79094452
0.79085678
0.79035342
INFO - Training [42][  120/  196]   Loss 0.446543   Top1 84.375000   Top5 98.639323   BatchTime 0.461834   LR 0.000097
0.79034537
0.78994799
0.78961354
0.78927702
0.78929400
0.78905064
0.78901750
0.78891575
0.78881592
0.78886646
0.78886706
0.78890204
0.79017216
0.79034603
0.79025191
0.78996712
0.78954464
0.78942353
0.78935909
0.78936678
INFO - Training [42][  140/  196]   Loss 0.444746   Top1 84.472656   Top5 98.674665   BatchTime 0.467787   LR 0.000096
0.78925323
0.78905845
0.78904110
0.78903770
0.78910840
0.78882152
0.78888690
0.78871691
0.78862798
0.78855467
0.78862721
0.78851640
0.78836679
0.78826791
0.78816849
0.78806734
0.78815001
0.78815293
0.78814328
0.78787428
Exception ignored in: <Finalize object, dead>
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/util.py", line 201, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.8/multiprocessing/util.py", line 441, in close_fds
    os.close(fd)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    optimizer, lr_scheduler, args.epochs, monitors, args, init_qparams = False, hard_pruning = True)
  File "main_slsq.py", line 77, in main
    logger.info(('Optimizer: %s' % optimizer).replace('\n', '\n' + ' ' * 11))
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 154, in train_one_epoch_slsq
    outputs = qat_model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 140, in forward
    x = self.features(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 93, in forward
    return self.skip_add.add(x, self.conv(x))
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1211, in _call_impl
    hook_result = hook(self, input, result)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 117, in _observer_forward_hook
    return self.activation_post_process(output)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/quan/observer.py", line 153, in forward
    if self.observer_enabled[0] == 1:
KeyboardInterrupt