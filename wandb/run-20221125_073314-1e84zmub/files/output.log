Files already downloaded and verified
Files already downloaded and verified
INFO - Dataset `cifar10` size:
          Training Set = 50000 (196)
        Validation Set = 10000 (40)
              Test Set = 10000 (40)
INFO - Created `MobileNetv2` model
          Use pre-trained model = True
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
INFO - Optimizer: AdamW (
           Parameter Group 0
               amsgrad: False
               betas: (0.9, 0.999)
               capturable: False
               eps: 1e-08
               foreach: None
               lr: 0.0005
               maximize: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineWarmRestartsLr`
    Update per batch: True
             Group 0: 0.0005
********************pre-trained*****************
*************soft_pruning_mode*******************
INFO - >>>>>> Epoch   0
INFO - Training: 50000 samples (256 per mini-batch)
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
0.00000000
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
0.00000000
0.95131010
0.86234623
0.86681271
0.87990636
0.87549424
0.86315340
0.85806173
0.85282224
0.84852564
0.84270608
INFO - Training [0][   20/  196]   Loss 1.777520   Top1 39.257812   Top5 83.769531   BatchTime 0.608522   LR 0.000500
0.83807838
0.83432120
0.83062243
0.82680613
0.82271260
0.81864512
0.81506038
0.81138092
0.80744147
0.80365711
0.80007291
0.79683864
0.79367173
0.79067868
0.78743678
0.78506982
0.78292173
INFO - Training [0][   40/  196]   Loss 1.718416   Top1 40.927734   Top5 85.312500   BatchTime 0.592712   LR 0.000500
0.78053802
0.77852327
0.77620327
0.77367353
0.77209103
0.77061307
0.76888120
0.76721787
0.76569510
0.76452434
0.76281267
0.76085156
0.75909615
0.75759411
0.75588655
0.75452721
0.75304395
0.75136906
0.74982601
0.74839234
0.74687439
INFO - Training [0][   60/  196]   Loss 1.633960   Top1 43.834635   Top5 86.972656   BatchTime 0.582066   LR 0.000499
0.74501878
0.74304962
0.74167007
0.73998886
0.73862851
0.73731023
0.73729730
0.73569804
0.73439026
0.73283732
0.73156655
0.73061138
0.72969985
0.72857112
0.72757494
0.72646177
0.72537166
0.72441024
0.72355449
0.72261202
0.72154385
0.72063667
0.71954691
INFO - Training [0][   80/  196]   Loss 1.566632   Top1 46.416016   Top5 88.222656   BatchTime 0.575580   LR 0.000498
0.71826541
0.71671283
0.71593159
0.71521443
0.71403056
0.71238166
0.71050364
0.70769882
0.70531487
0.70245469
0.69964916
0.69642943
0.69298035
0.68985581
0.68653858
0.68144923
0.67844218
INFO - Training [0][  100/  196]   Loss 1.509553   Top1 48.597656   Top5 89.171875   BatchTime 0.549887   LR 0.000497
0.67490870
0.67198461
0.66895628
0.66604102
0.66323096
0.66054875
0.65765023
0.65552384
0.65334296
0.65146977
0.64959580
0.64846063
0.64742124
0.64623070
0.64515495
0.64357394
0.64186025
0.63993073
0.63760221
0.63537925
0.63313037
0.63085175
INFO - Training [0][  120/  196]   Loss 1.462802   Top1 50.361328   Top5 89.876302   BatchTime 0.552638   LR 0.000495
0.62856013
0.62576890
0.62368238
0.62201548
0.62059236
0.61902714
0.61745822
0.61617732
0.61502099
0.61389351
0.61247426
0.61136460
0.61024737
0.60893011
0.60791969
0.60693431
0.60623729
INFO - Training [0][  140/  196]   Loss 1.431639   Top1 51.473214   Top5 90.376674   BatchTime 0.554294   LR 0.000494
0.60564512
0.60499549
0.60414809
0.60289276
0.60146749
0.60059410
0.60038149
0.59992301
0.59925038
0.59838617
0.59750640
0.59709316
0.59638590
0.59559274
0.59484822
0.59371060
0.59240162
0.59202337
0.59199142
0.59114808
0.59021884
INFO - Training [0][  160/  196]   Loss 1.419953   Top1 51.831055   Top5 90.400391   BatchTime 0.543915   LR 0.000492
0.58924061
0.58805305
0.58645141
0.58495069
0.58335352
0.58178037
0.58015442
0.57898980
0.57785773
0.57707894
0.57628441
0.57580382
0.57509351
0.57441401
0.57400548
0.57355905
0.57339138
0.57299924
0.57253760
0.57198292
0.57159287
0.57101607
INFO - Training [0][  180/  196]   Loss 1.398123   Top1 52.571615   Top5 90.718316   BatchTime 0.535498   LR 0.000490
0.57059103
0.57053041
0.57023096
0.56985992
0.56982130
0.57010388
0.57036638
0.57065338
0.57082713
0.57089055
0.57067180
0.57030541
0.57016152
0.57000941
INFO - ==> Top1: 52.720    Top5: 90.504    Loss: 1.391
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.56993741
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [0][   20/   40]   Loss 1.008255   Top1 67.285156   Top5 96.582031   BatchTime 0.243005
INFO - Validation [0][   40/   40]   Loss 1.012251   Top1 67.070000   Top5 96.580000   BatchTime 0.151830
INFO - ==> Top1: 67.070    Top5: 96.580    Loss: 1.012
INFO - ==> Sparsity : 0.495
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 67.070   Top5: 96.580]
features.0.conv.0 tensor(0.2639)
features.0.conv.3 tensor(0.3223)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0838)
features.2.conv.0 tensor(0.2503)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.5851)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0910)
features.3.conv.6 tensor(0.1137)
features.4.conv.0 tensor(0.1543)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.5457)
features.5.conv.0 tensor(0.4408)
features.5.conv.3 tensor(0.4334)
features.5.conv.6 tensor(0.3706)
features.6.conv.0 tensor(0.0625)
features.6.conv.3 tensor(0.0527)
features.6.conv.6 tensor(0.0917)
features.7.conv.0 tensor(0.5097)
features.7.conv.3 tensor(0.4578)
features.7.conv.6 tensor(0.5900)
features.8.conv.0 tensor(0.5917)
features.8.conv.3 tensor(0.5275)
features.8.conv.6 tensor(0.7054)
features.9.conv.0 tensor(0.5739)
features.9.conv.3 tensor(0.5573)
features.9.conv.6 tensor(0.7384)
features.10.conv.0 tensor(0.0921)
features.10.conv.3 tensor(0.1166)
features.10.conv.6 tensor(0.1077)
features.11.conv.0 tensor(0.7477)
features.11.conv.3 tensor(0.6514)
features.11.conv.6 tensor(0.8429)
features.12.conv.0 tensor(0.7155)
features.12.conv.3 tensor(0.6784)
features.12.conv.6 tensor(0.8794)
features.13.conv.0 tensor(0.3687)
features.13.conv.3 tensor(0.4975)
features.13.conv.6 tensor(0.0899)
features.14.conv.0 tensor(0.8371)
features.14.conv.3 tensor(0.8395)
features.14.conv.6 tensor(0.9581)
features.15.conv.0 tensor(0.8256)
features.15.conv.3 tensor(0.8890)
features.15.conv.6 tensor(0.9504)
features.16.conv.0 tensor(0.7159)
features.16.conv.3 tensor(0.8139)
features.16.conv.6 tensor(0.1165)
conv.0 tensor(0.0780)
tensor(1082639.) 2188896.0
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::TupleConstruct type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)
  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   1
INFO - Training: 50000 samples (256 per mini-batch)
0.56965607
0.56952262
0.56937140
0.56919557
0.57015175
0.56981963
0.56952512
0.56903541
0.56872094
0.56854182
0.56846440
0.56806982
0.56785786
0.56779617
0.56760031
0.56745547
0.56729937
INFO - Training [1][   20/  196]   Loss 1.203538   Top1 59.199219   Top5 93.671875   BatchTime 0.520699   LR 0.000485
0.56716073
0.56698596
0.56703663
0.56701314
0.56712037
0.56735545
0.56713128
0.56715804
0.56712657
0.56703579
0.56712252
0.56695646
0.56703913
0.56696093
0.56701070
0.56701523
0.56688374
0.56683123
0.56699842
0.56698316
0.56697839
0.56692719
0.56688142
0.56685805
INFO - Training [1][   40/  196]   Loss 1.199622   Top1 59.531250   Top5 92.939453   BatchTime 0.462190   LR 0.000482
0.56683105
0.56672442
0.56642586
0.56628197
0.56611753
0.56603086
0.56608576
0.56614351
0.56605476
0.56590205
0.56570983
0.56546825
0.56529975
0.56535989
0.56541836
0.56558150
0.56617355
0.58942574
0.59218842
INFO - Training [1][   60/  196]   Loss 1.190922   Top1 59.934896   Top5 93.261719   BatchTime 0.450648   LR 0.000479
0.59277266
0.59274417
0.59280932
0.59276921
0.59271801
0.59265763
0.59266442
0.59263152
0.59259695
0.59249693
0.59260887
0.59248930
0.59239310
0.59237182
0.59214336
0.59212106
0.59197247
0.59183097
INFO - Training [1][   80/  196]   Loss 1.175638   Top1 60.307617   Top5 93.681641   BatchTime 0.449383   LR 0.000476
0.59173763
0.59178758
0.59147549
0.59141272
0.59128815
0.59114349
0.59094793
0.59077173
0.59046072
0.59018809
0.59006143
0.58997971
0.58980536
0.58953643
0.58936554
0.58933133
0.58915132
0.58875138
0.58801216
0.58784419
0.58750015
0.58728409
INFO - Training [1][  100/  196]   Loss 1.155652   Top1 61.027344   Top5 93.953125   BatchTime 0.451505   LR 0.000473
0.58716321
0.58693331
0.58674920
0.58669049
0.58661562
0.58667755
0.58643073
0.58630711
0.58615446
0.58607215
0.58609188
0.58606035
0.58594006
0.58581263
0.58578950
0.58556902
0.58529335
0.58501756
INFO - Training [1][  120/  196]   Loss 1.138195   Top1 61.722005   Top5 94.182943   BatchTime 0.450832   LR 0.000469
0.58482879
0.58459246
0.58426881
0.58385670
0.58350188
0.58296418
0.58266878
0.58230025
0.58204538
0.58172750
0.58134967
0.58075863
0.58021265
0.57948977
0.57910222
0.57887787
0.57863986
0.57829940
0.57794553
0.57764405
0.57752091
0.57768971
0.57768172
INFO - Training [1][  140/  196]   Loss 1.126012   Top1 62.187500   Top5 94.383371   BatchTime 0.447287   LR 0.000465
0.57789385
0.57838714
0.57896703
0.57897335
0.57893282
0.57901889
0.57917428
0.57910013
0.57914048
0.57924271
0.57918572
0.57935143
0.57949746
0.57965636
0.57970083
0.57966959
0.57960308
0.57960814
0.57965213
0.57975537
0.58182430
INFO - Training [1][  160/  196]   Loss 1.115595   Top1 62.485352   Top5 94.499512   BatchTime 0.441580   LR 0.000460
0.58228725
0.58244669
0.58228117
0.58227974
0.58236498
0.58236861
0.58243394
0.58235168
0.58220243
0.58205289
0.58192974
0.58183086
0.58173692
0.58146816
0.58141375
0.58129966
0.58115715
0.58106792
0.58111948
INFO - Training [1][  180/  196]   Loss 1.105122   Top1 62.773438   Top5 94.574653   BatchTime 0.437488   LR 0.000456
0.58109719
0.58099782
0.58099061
0.58101165
0.58093750
0.58083999
0.58063692
0.58047092
0.58040762
0.58028984
0.58012569
0.58013827
0.58006907
0.58016735
INFO - ==> Top1: 63.078    Top5: 94.650    Loss: 1.097
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.58006644
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [1][   20/   40]   Loss 0.751461   Top1 74.824219   Top5 97.734375   BatchTime 0.110929
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.3809)
features.1.conv.0 tensor(0.0599)
features.1.conv.3 tensor(0.0683)
features.1.conv.6 tensor(0.0786)
features.2.conv.0 tensor(0.1852)
features.2.conv.3 tensor(0.3434)
features.2.conv.6 tensor(0.6328)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0864)
features.3.conv.6 tensor(0.1083)
features.4.conv.0 tensor(0.1160)
features.4.conv.3 tensor(0.3223)
features.4.conv.6 tensor(0.5762)
features.5.conv.0 tensor(0.4266)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.5684)
features.6.conv.0 tensor(0.0638)
features.6.conv.3 tensor(0.0671)
features.6.conv.6 tensor(0.0903)
features.7.conv.0 tensor(0.4837)
features.7.conv.3 tensor(0.4644)
features.7.conv.6 tensor(0.6983)
features.8.conv.0 tensor(0.5734)
features.8.conv.3 tensor(0.5286)
features.8.conv.6 tensor(0.7643)
features.9.conv.0 tensor(0.5809)
features.9.conv.3 tensor(0.5596)
features.9.conv.6 tensor(0.7953)
features.10.conv.0 tensor(0.0778)
features.10.conv.3 tensor(0.1085)
features.10.conv.6 tensor(0.1083)
features.11.conv.0 tensor(0.7261)
features.11.conv.3 tensor(0.6507)
features.11.conv.6 tensor(0.8768)
features.12.conv.0 tensor(0.6972)
features.12.conv.3 tensor(0.6788)
features.12.conv.6 tensor(0.8902)
features.13.conv.0 tensor(0.2857)
features.13.conv.3 tensor(0.4878)
features.13.conv.6 tensor(0.0905)
features.14.conv.0 tensor(0.8411)
features.14.conv.3 tensor(0.8396)
features.14.conv.6 tensor(0.9629)
INFO - Validation [1][   40/   40]   Loss 0.753864   Top1 74.310000   Top5 97.880000   BatchTime 0.082512
INFO - ==> Top1: 74.310    Top5: 97.880    Loss: 0.754
INFO - ==> Sparsity : 0.460
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 74.310   Top5: 97.880]
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 67.070   Top5: 96.580]
features.15.conv.0 tensor(0.8410)
features.15.conv.3 tensor(0.8910)
features.15.conv.6 tensor(0.9657)
features.16.conv.0 tensor(0.2477)
features.16.conv.3 tensor(0.8120)
features.16.conv.6 tensor(0.0624)
conv.0 tensor(0.0898)
tensor(1005962.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   2
INFO - Training: 50000 samples (256 per mini-batch)
0.57970059
0.57968932
0.57951206
0.57935792
0.57921845
0.57904190
0.57887381
0.57882154
0.57881820
0.57884312
0.57883137
0.57874519
0.57873714
0.57865095
0.57865793
0.57830483
0.57810497
0.57798797
INFO - Training [2][   20/  196]   Loss 1.025391   Top1 65.332031   Top5 94.921875   BatchTime 0.479020   LR 0.000448
0.57789320
0.57782185
0.57764000
0.57748491
0.57714111
0.57694864
0.57635552
0.57583004
0.57538372
0.57500446
0.57454312
0.57431310
0.57382143
0.57312477
0.57245266
0.57177752
0.57103449
0.57024401
0.56975824
INFO - Training [2][   40/  196]   Loss 1.016561   Top1 65.839844   Top5 95.029297   BatchTime 0.453093   LR 0.000443
0.56900221
0.56845355
0.56784219
0.56700391
0.56620747
0.56545788
0.56472552
0.56406415
0.56330806
0.56263685
0.56193471
0.56148744
0.56104106
0.56053221
0.55992728
0.55952412
0.55898911
0.55836034
0.55784845
0.55699033
0.55645907
0.55592328
0.55553579
INFO - Training [2][   60/  196]   Loss 0.999769   Top1 66.386719   Top5 95.377604   BatchTime 0.450091   LR 0.000437
0.55502367
0.55452287
0.55410749
0.55365562
0.55330044
0.55301440
0.55281013
0.55244893
0.55194449
0.55153108
0.55126369
0.55108154
0.55078208
0.55051106
0.55028981
0.54999638
0.54959023
0.54900807
0.54870659
0.54824942
0.54801667
INFO - Training [2][   80/  196]   Loss 0.980444   Top1 66.953125   Top5 95.551758   BatchTime 0.456499   LR 0.000432
0.54784864
0.54759872
0.54730403
0.54677165
0.54655349
0.54614139
0.54543394
0.54517871
0.54458642
0.54454666
0.54465002
0.54468286
0.54480284
0.54512978
0.54566652
0.54605216
0.54748690
INFO - Training [2][  100/  196]   Loss 0.966569   Top1 67.574219   Top5 95.703125   BatchTime 0.457583   LR 0.000426
0.54763722
0.54771650
0.54763412
0.54749900
0.54733360
0.54713565
0.54716122
0.54727799
0.54712456
0.54683602
0.54653835
0.54624248
0.54588723
0.54574215
0.54558498
0.54547691
0.54537076
0.54526621
0.54520530
0.54514134
0.54482877
0.54477257
0.54466063
INFO - Training [2][  120/  196]   Loss 0.961016   Top1 67.802734   Top5 95.784505   BatchTime 0.455348   LR 0.000421
0.54466194
0.54468638
0.54467934
0.54483652
0.54508996
0.54509789
0.54506177
0.54477614
0.54470533
0.54460984
0.54458147
0.54452181
0.54468793
0.54478723
0.54505551
0.54504675
0.54494578
INFO - Training [2][  140/  196]   Loss 0.958371   Top1 67.859933   Top5 95.884487   BatchTime 0.456142   LR 0.000415
0.54507977
0.54445988
0.54419041
0.54450583
0.54486269
0.54501659
0.54581231
0.54607952
0.54594469
0.54572731
0.54564291
0.54552376
0.54532778
0.54514462
0.54508072
0.54495859
0.54483056
0.54460609
0.54435748
INFO - Training [2][  160/  196]   Loss 0.957358   Top1 67.900391   Top5 95.847168   BatchTime 0.451706   LR 0.000409
0.54422534
0.54406053
0.54403758
0.54400301
0.54392010
0.54379743
0.54372084
0.54350203
0.54348195
0.54348642
0.54344517
0.54346442
0.54345328
0.54338926
0.54327679
0.54311901
0.54300499
0.54290891
0.54282123
0.54278666
0.54279459
INFO - Training [2][  180/  196]   Loss 0.952073   Top1 68.103299   Top5 95.831163   BatchTime 0.442023   LR 0.000402
0.54276544
0.54262900
0.54265827
0.54263330
0.54237491
0.54218668
0.54211211
0.54218382
0.54204351
0.54194117
0.54206407
0.54204255
0.54195374
0.54193139
0.54199898
0.54211181
0.54181808
0.54134142
********************pre-trained*****************
INFO - ==> Top1: 68.240    Top5: 95.884    Loss: 0.948
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [2][   20/   40]   Loss 0.696995   Top1 77.304688   Top5 98.164062   BatchTime 0.112500
INFO - Validation [2][   40/   40]   Loss 0.692760   Top1 76.670000   Top5 98.220000   BatchTime 0.084708
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.4219)
features.1.conv.0 tensor(0.0456)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.1146)
features.2.conv.0 tensor(0.1450)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.6473)
features.3.conv.0 tensor(0.0752)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.1144)
features.4.conv.0 tensor(0.0951)
features.4.conv.3 tensor(0.3264)
features.4.conv.6 tensor(0.6095)
features.5.conv.0 tensor(0.4255)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.6100)
features.6.conv.0 tensor(0.0703)
features.6.conv.3 tensor(0.0613)
features.6.conv.6 tensor(0.0918)
features.7.conv.0 tensor(0.4961)
features.7.conv.3 tensor(0.4598)
features.7.conv.6 tensor(0.7215)
features.8.conv.0 tensor(0.6174)
features.8.conv.3 tensor(0.5266)
features.8.conv.6 tensor(0.7879)
features.9.conv.0 tensor(0.6062)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.8086)
features.10.conv.0 tensor(0.0778)
features.10.conv.3 tensor(0.1111)
features.10.conv.6 tensor(0.1058)
features.11.conv.0 tensor(0.7524)
features.11.conv.3 tensor(0.6526)
features.11.conv.6 tensor(0.8934)
features.12.conv.0 tensor(0.7246)
features.12.conv.3 tensor(0.6811)
features.12.conv.6 tensor(0.9040)
features.13.conv.0 tensor(0.3600)
features.13.conv.3 tensor(0.4921)
features.13.conv.6 tensor(0.0924)
features.14.conv.0 tensor(0.8721)
features.14.conv.3 tensor(0.8392)
features.14.conv.6 tensor(0.9698)
features.15.conv.0 tensor(0.8435)
features.15.conv.3 tensor(0.8911)
features.15.conv.6 tensor(0.9701)
features.16.conv.0 tensor(0.6819)
features.16.conv.3 tensor(0.8116)
features.16.conv.6 tensor(0.1361)
conv.0 tensor(0.0925)
tensor(1116079.) 2188896.0
INFO - ==> Top1: 76.670    Top5: 98.220    Loss: 0.693
INFO - ==> Sparsity : 0.510
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 76.670   Top5: 98.220]
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 74.310   Top5: 97.880]
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 67.070   Top5: 96.580]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   3
INFO - Training: 50000 samples (256 per mini-batch)
0.54108161
0.54086781
0.54052317
0.54025745
0.54023564
0.54008138
0.53992999
0.53962475
0.53935987
0.53901941
0.53870678
0.53845745
0.53810209
0.53770536
0.53728360
0.53688008
0.53636235
0.53596002
0.53554189
0.53541082
0.53507972
INFO - Training [3][   20/  196]   Loss 0.910322   Top1 69.433594   Top5 95.917969   BatchTime 0.483345   LR 0.000391
0.53487056
0.53466302
0.53448588
0.53438526
0.53441417
0.53425306
0.53405982
0.53379303
0.53352481
0.53332609
0.53348237
0.53345007
0.53334349
0.53360909
0.53392118
0.53418934
0.53452921
0.53582722
INFO - Training [3][   40/  196]   Loss 0.909476   Top1 69.775391   Top5 95.917969   BatchTime 0.459048   LR 0.000384
0.53574246
0.53560662
0.53549910
0.53533989
0.53498483
0.53489476
0.53491145
0.53476626
0.53461039
0.53443718
0.53431493
0.53412390
0.53392464
0.53386462
0.53379822
0.53357321
0.53327894
INFO - Training [3][   60/  196]   Loss 0.902364   Top1 69.856771   Top5 96.009115   BatchTime 0.460567   LR 0.000377
0.53321767
0.53322273
0.53319305
0.53294897
0.53279865
0.53274268
0.53266990
0.53255117
0.53245288
0.53243089
0.53224206
0.53215057
0.53216988
0.53221709
0.53198785
0.53181940
0.53174931
0.53143454
0.53126377
0.53082007
0.53052139
0.53020263
0.53001153
INFO - Training [3][   80/  196]   Loss 0.893593   Top1 70.053711   Top5 96.083984   BatchTime 0.459176   LR 0.000370
0.52969247
0.52922040
0.52841794
0.52832627
0.52819794
0.52800339
0.52781820
0.52768481
0.52775609
0.52776998
0.52763748
0.52754068
0.52747661
0.52729011
0.52712160
0.52705681
0.52694583
0.52668428
INFO - Training [3][  100/  196]   Loss 0.881031   Top1 70.539062   Top5 96.238281   BatchTime 0.454746   LR 0.000363
0.52651572
0.52627754
0.52694356
0.52682823
0.52651405
0.52629244
0.52600348
0.52563339
0.52534842
0.52514303
0.52557760
0.52820432
0.52799565
0.52779448
0.52781951
0.52740073
0.52711338
0.52677590
0.52665913
0.52644819
0.52618915
0.52590412
0.52584684
INFO - Training [3][  120/  196]   Loss 0.875809   Top1 70.680339   Top5 96.402995   BatchTime 0.451518   LR 0.000356
0.52584648
0.52564001
0.52556026
0.52542740
0.52535987
0.52525532
0.52504092
0.52477032
0.52461636
0.52468640
0.52448702
0.52430743
0.52423579
0.52424967
0.52415174
0.52391350
0.52374721
0.52341932
INFO - Training [3][  140/  196]   Loss 0.872812   Top1 70.733817   Top5 96.462054   BatchTime 0.450086   LR 0.000348
0.52313769
0.52318090
0.52274960
0.52250862
0.52244580
0.52228177
0.52212018
0.52200633
0.52186495
0.52148890
0.52135020
0.52139145
0.52109975
0.52090603
0.52068084
0.52172458
0.52182400
0.52156031
INFO - Training [3][  160/  196]   Loss 0.871037   Top1 70.734863   Top5 96.481934   BatchTime 0.446751   LR 0.000341
0.52113307
0.52061683
0.52043867
0.52054936
0.52074611
0.52106220
0.52119380
0.52118129
0.52098650
0.52072483
0.52047169
0.52022749
0.52016020
0.52014267
0.51992977
0.51990128
0.51981992
0.51962286
0.51936722
0.51915866
0.51923466
0.51918906
0.51913983
0.51907611
INFO - Training [3][  180/  196]   Loss 0.866225   Top1 70.846354   Top5 96.464844   BatchTime 0.444277   LR 0.000333
0.51902896
0.51904261
0.51901585
0.51902163
0.51892287
0.51897812
0.51906562
0.51921320
0.51938587
0.51948214
0.51903099
0.51855707
0.51792860
0.51723278
INFO - ==> Top1: 70.936    Top5: 96.486    Loss: 0.864
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.51714033
0.51702899
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [3][   20/   40]   Loss 0.585800   Top1 79.941406   Top5 98.828125   BatchTime 0.130499
INFO - Validation [3][   40/   40]   Loss 0.590111   Top1 79.620000   Top5 98.840000   BatchTime 0.091780
INFO - ==> Top1: 79.620    Top5: 98.840    Loss: 0.590
INFO - ==> Sparsity : 0.567
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 79.620   Top5: 98.840]
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 76.670   Top5: 98.220]
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 74.310   Top5: 97.880]
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.4434)
features.1.conv.0 tensor(0.0599)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0894)
features.2.conv.0 tensor(0.1264)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.6667)
features.3.conv.0 tensor(0.0793)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1131)
features.4.conv.0 tensor(0.1227)
features.4.conv.3 tensor(0.3235)
features.4.conv.6 tensor(0.6195)
features.5.conv.0 tensor(0.4294)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.6401)
features.6.conv.0 tensor(0.0669)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0920)
features.7.conv.0 tensor(0.5468)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.7382)
features.8.conv.0 tensor(0.6371)
features.8.conv.3 tensor(0.5292)
features.8.conv.6 tensor(0.8004)
features.9.conv.0 tensor(0.6283)
features.9.conv.3 tensor(0.5587)
features.9.conv.6 tensor(0.8219)
features.10.conv.0 tensor(0.0766)
features.10.conv.3 tensor(0.1108)
features.10.conv.6 tensor(0.1064)
features.11.conv.0 tensor(0.7644)
features.11.conv.3 tensor(0.6508)
features.11.conv.6 tensor(0.8998)
features.12.conv.0 tensor(0.7379)
features.12.conv.3 tensor(0.6807)
features.12.conv.6 tensor(0.9081)
features.13.conv.0 tensor(0.2296)
features.13.conv.3 tensor(0.4884)
features.13.conv.6 tensor(0.1252)
features.14.conv.0 tensor(0.8804)
features.14.conv.3 tensor(0.8392)
features.14.conv.6 tensor(0.9724)
features.15.conv.0 tensor(0.8603)
features.15.conv.3 tensor(0.8909)
features.15.conv.6 tensor(0.9721)
features.16.conv.0 tensor(0.7042)
features.16.conv.3 tensor(0.8116)
features.16.conv.6 tensor(0.5094)
conv.0 tensor(0.0948)
tensor(1241118.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   4
INFO - Training: 50000 samples (256 per mini-batch)
0.51658398
0.51633936
0.51609600
0.51586843
0.51568973
0.51566428
0.51554525
0.51539785
0.51535892
0.51566482
0.51653928
0.51634079
0.51612538
0.51594895
0.51578796
0.51572847
0.51570749
INFO - Training [4][   20/  196]   Loss 0.840134   Top1 71.523438   Top5 96.191406   BatchTime 0.481773   LR 0.000320
0.51569772
0.51555234
0.51525271
0.51513875
0.51510292
0.51501900
0.51502657
0.51505888
0.51500499
0.51484728
0.51458091
0.51434004
0.51400584
0.51377046
0.51339859
0.51318145
0.51296127
0.51252609
0.51222956
0.51178253
0.51162839
0.51149881
0.51138318
INFO - Training [4][   40/  196]   Loss 0.830217   Top1 72.431641   Top5 96.572266   BatchTime 0.458160   LR 0.000312
0.51131648
0.51120847
0.51120079
0.51117295
0.51109833
0.51090199
0.51093292
0.51091051
0.51073694
0.51059550
0.51044977
0.51037526
0.51040673
0.51030439
0.51017380
0.50973773
0.50962573
0.50960428
INFO - Training [4][   60/  196]   Loss 0.831660   Top1 71.998698   Top5 96.699219   BatchTime 0.456618   LR 0.000304
0.50936443
0.50927323
0.50903094
0.50888401
0.50862044
0.50858825
0.50856829
0.50859708
0.50860250
0.50833565
0.50839901
0.50840276
0.50842530
0.50835705
0.50832027
0.50810790
0.50766474
0.50720805
0.50683033
0.50662512
0.50652045
INFO - Training [4][   80/  196]   Loss 0.831032   Top1 71.972656   Top5 96.772461   BatchTime 0.458809   LR 0.000296
0.50651306
0.50659525
0.50651360
0.50655431
0.50664473
0.50685030
0.50687581
0.50702685
0.50710976
0.50723696
0.50720572
0.50717050
0.50710654
0.50710624
0.50716978
0.50737762
0.50873375
0.50889593
INFO - Training [4][  100/  196]   Loss 0.823105   Top1 72.367188   Top5 96.820312   BatchTime 0.458406   LR 0.000289
0.50877720
0.50858563
0.50847948
0.50842625
0.50841266
0.50840986
0.50859070
0.50866157
0.50874275
0.50871176
0.50849545
0.50840145
0.50830793
0.50829887
0.50828880
0.50821304
0.50830549
0.50825322
0.50819957
0.50802612
0.50776303
0.50759643
INFO - Training [4][  120/  196]   Loss 0.816355   Top1 72.652995   Top5 96.904297   BatchTime 0.456812   LR 0.000281
0.50738817
0.50727689
0.50723386
0.50724387
0.50717145
0.50698495
0.50678343
0.50665689
0.50637448
0.50587755
0.50548869
0.50527769
0.50501740
0.50475377
0.50446433
0.50426459
0.50418103
0.50377810
0.50356239
INFO - Training [4][  140/  196]   Loss 0.816241   Top1 72.826451   Top5 96.936384   BatchTime 0.451241   LR 0.000273
0.50328308
0.50309426
0.50293326
0.50286049
0.50288445
0.50263262
0.50236970
0.50203693
0.50172424
0.50127882
0.50094706
0.50056481
0.49991399
0.49900997
0.49823788
0.49760449
0.49680412
0.49595228
0.49523678
INFO - Training [4][  160/  196]   Loss 0.814966   Top1 72.827148   Top5 96.926270   BatchTime 0.446286   LR 0.000265
0.49456406
0.49410596
0.49352634
0.49331021
0.49300665
0.49285379
0.49266452
0.49214336
0.49137521
0.49102065
0.49043533
0.48972753
0.48920637
0.48873851
0.48819676
0.48786545
0.48747966
0.48743522
0.48737672
0.48725352
0.48718479
0.48719820
0.48710644
INFO - Training [4][  180/  196]   Loss 0.808484   Top1 73.005642   Top5 96.911892   BatchTime 0.446366   LR 0.000257
0.48689473
0.48685175
0.48694542
0.48707104
0.48710755
0.48733297
0.48708478
0.48713049
0.48712355
0.48715615
0.48738155
0.48735154
0.48742232
0.48735225
INFO - ==> Top1: 73.162    Top5: 96.896    Loss: 0.804
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.48755670
0.48793113
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [4][   20/   40]   Loss 0.541478   Top1 81.562500   Top5 99.062500   BatchTime 0.148228
INFO - Validation [4][   40/   40]   Loss 0.530496   Top1 81.960000   Top5 99.240000   BatchTime 0.101912
INFO - ==> Top1: 81.960    Top5: 99.240    Loss: 0.530
INFO - ==> Sparsity : 0.615
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 81.960   Top5: 99.240]
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 79.620   Top5: 98.840]
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 76.670   Top5: 98.220]
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.4590)
features.1.conv.0 tensor(0.0592)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.1525)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.6733)
features.3.conv.0 tensor(0.0804)
features.3.conv.3 tensor(0.0841)
features.3.conv.6 tensor(0.1137)
features.4.conv.0 tensor(0.1152)
features.4.conv.3 tensor(0.3229)
features.4.conv.6 tensor(0.6299)
features.5.conv.0 tensor(0.4487)
features.5.conv.3 tensor(0.4334)
features.5.conv.6 tensor(0.6520)
features.6.conv.0 tensor(0.0677)
features.6.conv.3 tensor(0.0584)
features.6.conv.6 tensor(0.0898)
features.7.conv.0 tensor(0.5610)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.7398)
features.8.conv.0 tensor(0.6391)
features.8.conv.3 tensor(0.5289)
features.8.conv.6 tensor(0.8116)
features.9.conv.0 tensor(0.6316)
features.9.conv.3 tensor(0.5547)
features.9.conv.6 tensor(0.8299)
features.10.conv.0 tensor(0.0812)
features.10.conv.3 tensor(0.1076)
features.10.conv.6 tensor(0.1188)
features.11.conv.0 tensor(0.7717)
features.11.conv.3 tensor(0.6476)
features.11.conv.6 tensor(0.9099)
features.12.conv.0 tensor(0.7515)
features.12.conv.3 tensor(0.6813)
features.12.conv.6 tensor(0.9096)
features.13.conv.0 tensor(0.3366)
features.13.conv.3 tensor(0.4886)
features.13.conv.6 tensor(0.3742)
features.14.conv.0 tensor(0.8932)
features.14.conv.3 tensor(0.8391)
features.14.conv.6 tensor(0.9734)
features.15.conv.0 tensor(0.8684)
features.15.conv.3 tensor(0.8909)
features.15.conv.6 tensor(0.9713)
features.16.conv.0 tensor(0.7071)
features.16.conv.3 tensor(0.8112)
features.16.conv.6 tensor(0.7192)
conv.0 tensor(0.1041)
tensor(1345513.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   5
INFO - Training: 50000 samples (256 per mini-batch)
0.48833299
0.48883381
0.48899886
0.48910174
0.48922652
0.48917413
0.48925006
0.48917478
0.48905596
0.48894152
0.48878506
0.48864511
0.48854008
0.48841789
0.48831406
0.48814926
0.48796195
INFO - Training [5][   20/  196]   Loss 0.771886   Top1 73.867188   Top5 96.660156   BatchTime 0.503922   LR 0.000242
0.48744330
0.48739174
0.48723367
0.48691061
0.48674038
0.48654085
0.48646927
0.48634613
0.48623607
0.48617196
0.48607928
0.48593229
0.48594502
0.48584804
0.48571333
0.48563308
0.48552474
0.48538405
INFO - Training [5][   40/  196]   Loss 0.792007   Top1 73.359375   Top5 96.630859   BatchTime 0.469939   LR 0.000234
0.48528373
0.48519725
0.48516577
0.48510101
0.48506603
0.48498255
0.48490170
0.48484135
0.48480880
0.48494709
0.48641789
0.48628852
0.48617974
0.48607171
0.48597297
0.48588696
0.48582056
0.48573494
0.48569387
0.48569185
0.48568267
0.48565999
0.48562470
INFO - Training [5][   60/  196]   Loss 0.777883   Top1 73.684896   Top5 96.770833   BatchTime 0.457923   LR 0.000226
0.48564637
0.48567805
0.48584354
0.48617724
0.48634854
0.48624870
0.48620203
0.48608667
0.48586723
0.48567772
0.48554829
0.48545098
0.48528945
0.48524207
0.48484915
0.48448372
0.48417786
0.48401782
INFO - Training [5][   80/  196]   Loss 0.765901   Top1 74.091797   Top5 96.923828   BatchTime 0.452043   LR 0.000218
0.48382923
0.48370400
0.48363507
0.48347721
0.48324230
0.48307642
0.48278955
0.48250231
0.48230138
0.48217565
0.48206329
0.48173356
0.48150915
0.48139355
0.48135263
0.48119444
0.48099396
0.48090687
0.48062357
0.48053297
0.48052433
0.48051655
0.48045519
INFO - Training [5][  100/  196]   Loss 0.757847   Top1 74.429688   Top5 97.011719   BatchTime 0.449829   LR 0.000210
0.48033631
0.48030326
0.48030013
0.48037446
0.48041707
0.48042661
0.48037025
0.48015860
0.48003769
0.47994366
0.47988954
0.47983596
0.47976246
0.47975579
0.47990227
0.48014137
0.48011196
0.48019055
INFO - Training [5][  120/  196]   Loss 0.748831   Top1 74.804688   Top5 97.099609   BatchTime 0.448277   LR 0.000202
0.48029175
0.48014665
0.48002344
0.47992828
0.47991416
0.47988543
0.47970143
0.47972417
0.47952417
0.47933534
0.47918516
0.47893602
0.47877407
0.47872332
0.47856960
0.47841671
0.47841492
0.47841790
0.47827461
0.47819498
INFO - Training [5][  140/  196]   Loss 0.747895   Top1 74.874442   Top5 97.162388   BatchTime 0.440218   LR 0.000195
0.47826976
0.47810632
0.47811997
0.47812104
0.47816485
0.47825736
0.47819385
0.47805652
0.47801691
0.47803453
0.47797394
0.47797731
0.47799134
0.47809938
0.47827843
0.47843578
0.47852999
0.47855052
0.47847804
0.47842801
0.47834525
0.47827828
INFO - Training [5][  160/  196]   Loss 0.748723   Top1 74.875488   Top5 97.145996   BatchTime 0.443949   LR 0.000187
0.47823283
0.47815481
0.47815627
0.47815648
0.47818545
0.47814998
0.47811586
0.47806707
0.47810668
0.47800288
0.47789124
0.47784221
0.47778252
0.47779965
0.47780076
0.47771364
0.47767526
0.47744325
INFO - Training [5][  180/  196]   Loss 0.744809   Top1 74.969618   Top5 97.126736   BatchTime 0.445610   LR 0.000179
0.47732285
0.47725195
0.47722334
0.47717297
0.47714233
0.47710145
0.47706285
0.47702631
0.47700042
0.47699839
0.47704911
0.47711009
0.47691318
0.47668973
INFO - ==> Top1: 75.100    Top5: 97.152    Loss: 0.742
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.47653446
0.47647280
0.47634575
0.47626922
0.47619846
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [5][   20/   40]   Loss 0.490464   Top1 83.750000   Top5 99.160156   BatchTime 0.115553
INFO - Validation [5][   40/   40]   Loss 0.483148   Top1 83.900000   Top5 99.280000   BatchTime 0.093074
INFO - ==> Top1: 83.900    Top5: 99.280    Loss: 0.483
INFO - ==> Sparsity : 0.644
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 83.900   Top5: 99.280]
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 81.960   Top5: 99.240]
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 79.620   Top5: 98.840]
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.4727)
features.1.conv.0 tensor(0.0651)
features.1.conv.3 tensor(0.0660)
features.1.conv.6 tensor(0.0933)
features.2.conv.0 tensor(0.1600)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.6814)
features.3.conv.0 tensor(0.0694)
features.3.conv.3 tensor(0.0841)
features.3.conv.6 tensor(0.1122)
features.4.conv.0 tensor(0.1118)
features.4.conv.3 tensor(0.3212)
features.4.conv.6 tensor(0.6403)
features.5.conv.0 tensor(0.4548)
features.5.conv.3 tensor(0.4311)
features.5.conv.6 tensor(0.6623)
features.6.conv.0 tensor(0.0667)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0908)
features.7.conv.0 tensor(0.5772)
features.7.conv.3 tensor(0.4549)
features.7.conv.6 tensor(0.7528)
features.8.conv.0 tensor(0.6538)
features.8.conv.3 tensor(0.5289)
features.8.conv.6 tensor(0.8169)
features.9.conv.0 tensor(0.6478)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.8322)
features.10.conv.0 tensor(0.0816)
features.10.conv.3 tensor(0.1102)
features.10.conv.6 tensor(0.1312)
features.11.conv.0 tensor(0.7825)
features.11.conv.3 tensor(0.6474)
features.11.conv.6 tensor(0.9106)
features.12.conv.0 tensor(0.7547)
features.12.conv.3 tensor(0.6811)
features.12.conv.6 tensor(0.9124)
features.13.conv.0 tensor(0.3622)
features.13.conv.3 tensor(0.4911)
features.13.conv.6 tensor(0.4376)
features.14.conv.0 tensor(0.8973)
features.14.conv.3 tensor(0.8390)
features.14.conv.6 tensor(0.9756)
features.15.conv.0 tensor(0.8790)
features.15.conv.3 tensor(0.8900)
features.15.conv.6 tensor(0.9715)
features.16.conv.0 tensor(0.7176)
features.16.conv.3 tensor(0.8113)
features.16.conv.6 tensor(0.8153)
conv.0 tensor(0.1540)
tensor(1410303.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   6
INFO - Training: 50000 samples (256 per mini-batch)
0.47618440
0.47622579
0.47639409
0.47646692
0.47642118
0.47621962
0.47614715
0.47606626
0.47601128
0.47591141
0.47585303
0.47578683
0.47574919
0.47575316
0.47571772
0.47564867
0.47557488
INFO - Training [6][   20/  196]   Loss 0.729834   Top1 75.605469   Top5 96.933594   BatchTime 0.531934   LR 0.000166
0.47543404
0.47529906
0.47520217
0.47518119
0.47508076
0.47507504
0.47504997
0.47514781
0.47534269
0.47541982
0.47536251
0.47529069
0.47523066
0.47510841
0.47500330
0.47501773
0.47504660
0.47505829
0.47501251
0.47501111
0.47503802
0.47475961
INFO - Training [6][   40/  196]   Loss 0.732770   Top1 75.410156   Top5 97.109375   BatchTime 0.494687   LR 0.000158
0.47462964
0.47458190
0.47447205
0.47437084
0.47424456
0.47423935
0.47421759
0.47420490
0.47413304
0.47407404
0.47415906
0.47409064
0.47398141
0.47369745
0.47358182
0.47339743
0.47324327
INFO - Training [6][   60/  196]   Loss 0.723876   Top1 75.722656   Top5 97.259115   BatchTime 0.486970   LR 0.000151
0.47311682
0.47301674
0.47289377
0.47272307
0.47251293
0.47231507
0.47219321
0.47205633
0.47164083
0.47159365
0.47175598
0.47165686
0.47163728
0.47162506
0.47153103
0.47149265
0.47134763
0.47123325
0.47111300
0.47096723
0.47083169
0.47072437
INFO - Training [6][   80/  196]   Loss 0.713264   Top1 76.176758   Top5 97.343750   BatchTime 0.478052   LR 0.000143
0.47065106
0.47066149
0.47060001
0.47060248
0.47058448
0.47049445
0.47034445
0.47024044
0.47011751
0.47004142
0.46993530
0.46980348
0.46976957
0.46975172
0.46966550
0.46962005
0.46956709
INFO - Training [6][  100/  196]   Loss 0.705929   Top1 76.453125   Top5 97.425781   BatchTime 0.474385   LR 0.000136
0.46950409
0.46941853
0.46927705
0.46917486
0.46907678
0.46896401
0.46889046
0.46880233
0.46871921
0.46868446
0.46865377
0.46859086
0.46855342
0.46851948
0.46848565
0.46840027
0.46834716
0.46830678
0.46822193
0.46808749
INFO - Training [6][  120/  196]   Loss 0.701591   Top1 76.621094   Top5 97.506510   BatchTime 0.461990   LR 0.000129
0.46800727
0.46792459
0.46782511
0.46774450
0.46765676
0.46760529
0.46759477
0.46754715
0.46752423
0.46748188
0.46743602
0.46740997
0.46740007
0.46746352
0.46752638
0.46761361
0.46768811
0.46773317
0.46774864
0.46772352
0.46774438
0.46782124
0.46780071
INFO - Training [6][  140/  196]   Loss 0.698844   Top1 76.724330   Top5 97.608817   BatchTime 0.458505   LR 0.000122
0.46804920
0.46852681
0.46890697
0.46897614
0.46894291
0.46889237
0.46882436
0.46873978
0.46862713
0.46854922
0.46847329
0.46838111
0.46828771
0.46819535
0.46813068
0.46809143
0.46804851
0.46803665
INFO - Training [6][  160/  196]   Loss 0.699012   Top1 76.738281   Top5 97.602539   BatchTime 0.456685   LR 0.000115
0.46800521
0.46796018
0.46788195
0.46782911
0.46777084
0.46773607
0.46769646
0.46769634
0.46767083
0.46762487
0.46767217
0.46758157
0.46779004
0.46766809
0.46735790
0.46731260
0.46718857
0.46708298
0.46695346
0.46682969
INFO - Training [6][  180/  196]   Loss 0.696886   Top1 76.770833   Top5 97.597656   BatchTime 0.450558   LR 0.000108
0.46670526
0.46661621
0.46655571
0.46648571
0.46640098
0.46622902
0.46608526
0.46593177
0.46575215
0.46559250
0.46531260
0.46508050
0.46500963
0.46494129
0.46489272
0.46484566
0.46484610
0.46476454
0.46467626
INFO - ==> Top1: 76.784    Top5: 97.596    Loss: 0.697
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.46458659
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [6][   20/   40]   Loss 0.464134   Top1 84.511719   Top5 99.140625   BatchTime 0.225224
INFO - Validation [6][   40/   40]   Loss 0.459105   Top1 84.640000   Top5 99.390000   BatchTime 0.152214
INFO - ==> Top1: 84.640    Top5: 99.390    Loss: 0.459
INFO - ==> Sparsity : 0.660
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 84.640   Top5: 99.390]
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 83.900   Top5: 99.280]
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 81.960   Top5: 99.240]
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.4707)
features.1.conv.0 tensor(0.0638)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1479)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.6861)
features.3.conv.0 tensor(0.0813)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1159)
features.4.conv.0 tensor(0.2056)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.6502)
features.5.conv.0 tensor(0.4870)
features.5.conv.3 tensor(0.4248)
features.5.conv.6 tensor(0.6683)
features.6.conv.0 tensor(0.0661)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0925)
features.7.conv.0 tensor(0.5960)
features.7.conv.3 tensor(0.4592)
features.7.conv.6 tensor(0.7609)
features.8.conv.0 tensor(0.6882)
features.8.conv.3 tensor(0.5292)
features.8.conv.6 tensor(0.8234)
features.9.conv.0 tensor(0.6657)
features.9.conv.3 tensor(0.5570)
features.9.conv.6 tensor(0.8345)
features.10.conv.0 tensor(0.0824)
features.10.conv.3 tensor(0.1045)
features.10.conv.6 tensor(0.1427)
features.11.conv.0 tensor(0.7962)
features.11.conv.3 tensor(0.6491)
features.11.conv.6 tensor(0.9129)
features.12.conv.0 tensor(0.7827)
features.12.conv.3 tensor(0.6827)
features.12.conv.6 tensor(0.9157)
features.13.conv.0 tensor(0.4022)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.4899)
features.14.conv.0 tensor(0.9032)
features.14.conv.3 tensor(0.8392)
features.14.conv.6 tensor(0.9754)
features.15.conv.0 tensor(0.8870)
features.15.conv.3 tensor(0.8903)
features.15.conv.6 tensor(0.9723)
features.16.conv.0 tensor(0.7171)
features.16.conv.3 tensor(0.8106)
features.16.conv.6 tensor(0.8669)
conv.0 tensor(0.1614)
tensor(1444500.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   7
INFO - Training: 50000 samples (256 per mini-batch)
0.46452123
0.46448690
0.46442667
0.46440056
0.46434453
0.46431184
0.46423593
0.46418598
0.46417931
0.46412826
0.46409410
0.46410269
0.46412653
0.46415251
0.46412843
0.46410403
0.46408552
0.46406636
0.46407032
INFO - Training [7][   20/  196]   Loss 0.692646   Top1 76.660156   Top5 97.226562   BatchTime 0.507042   LR 0.000097
0.46400663
0.46405423
0.46405381
0.46402466
0.46401915
0.46397173
0.46390748
0.46383920
0.46376610
0.46373534
0.46367219
0.46364293
0.46362919
0.46360803
0.46359769
0.46355990
0.46352857
0.46351862
INFO - Training [7][   40/  196]   Loss 0.690333   Top1 77.041016   Top5 97.509766   BatchTime 0.472198   LR 0.000091
0.46344927
0.46338993
0.46330273
0.46322206
0.46317691
0.46314549
0.46312737
0.46309638
0.46303728
0.46295077
0.46292710
0.46288109
0.46282566
0.46276450
0.46273163
0.46273249
0.46267924
0.46265420
INFO - Training [7][   60/  196]   Loss 0.678508   Top1 77.317708   Top5 97.610677   BatchTime 0.463714   LR 0.000085
0.46261334
0.46258524
0.46254453
0.46245554
0.46235374
0.46221355
0.46210659
0.46203113
0.46196350
0.46188951
0.46185759
0.46180648
0.46179456
0.46176085
0.46174958
0.46176109
0.46173120
0.46170086
0.46169636
0.46164563
0.46160385
0.46155456
INFO - Training [7][   80/  196]   Loss 0.675154   Top1 77.348633   Top5 97.749023   BatchTime 0.462011   LR 0.000079
0.46144581
0.46132588
0.46124768
0.46118656
0.46112728
0.46107644
0.46107745
0.46107495
0.46106529
0.46105754
0.46105355
0.46109137
0.46110517
0.46103361
0.46092814
0.46087959
0.46083152
0.46077481
0.46068972
INFO - Training [7][  100/  196]   Loss 0.665846   Top1 77.718750   Top5 97.789062   BatchTime 0.452865   LR 0.000073
0.46059874
0.46052551
0.46042785
0.46033195
0.46025476
0.46016261
0.46011099
0.46008283
0.46005103
0.46004340
0.46004385
0.46003288
0.46000317
0.45997012
0.45997047
0.45997456
0.45997322
0.45992067
0.45986065
0.45980743
0.45976958
0.45977879
0.45976061
INFO - Training [7][  120/  196]   Loss 0.660253   Top1 78.027344   Top5 97.884115   BatchTime 0.450602   LR 0.000067
0.45972091
0.45968997
0.45967221
0.45966491
0.45967686
0.45969424
0.45967647
0.45962709
0.45961726
0.45960447
0.45955679
0.45952585
0.45946109
0.45942619
0.45938268
0.45933381
0.45929575
INFO - Training [7][  140/  196]   Loss 0.659400   Top1 78.085938   Top5 97.932478   BatchTime 0.452537   LR 0.000062
0.45923597
0.45915583
0.45912576
0.45910275
0.45907179
0.45903426
0.45898846
0.45895302
0.45890734
0.45889834
0.45886981
0.45885292
0.45881423
0.45879486
0.45880684
0.45880440
0.45881113
0.45880377
0.45880705
0.45882583
0.45883152
INFO - Training [7][  160/  196]   Loss 0.661623   Top1 78.098145   Top5 97.922363   BatchTime 0.444337   LR 0.000057
0.45885661
0.45887318
0.45888627
0.45889822
0.45890886
0.45887336
0.45885271
0.45885709
0.45887065
0.45886609
0.45887062
0.45886916
0.45882922
0.45877960
0.45873812
0.45867813
0.45862833
0.45859316
INFO - Training [7][  180/  196]   Loss 0.662183   Top1 77.983941   Top5 97.894965   BatchTime 0.443903   LR 0.000052
0.45856655
0.45852941
0.45850781
0.45848942
0.45844945
0.45843780
0.45838797
0.45839092
0.45837826
0.45835310
0.45832565
0.45830441
0.45827043
0.45823532
0.45822132
0.45819008
0.45818022
0.45816135
INFO - ==> Top1: 78.072    Top5: 97.908    Loss: 0.660
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.45815635
0.45813432
0.45811349
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [7][   20/   40]   Loss 0.441231   Top1 85.312500   Top5 99.296875   BatchTime 0.125172
INFO - Validation [7][   40/   40]   Loss 0.432110   Top1 85.320000   Top5 99.460000   BatchTime 0.093203
INFO - ==> Top1: 85.320    Top5: 99.460    Loss: 0.432
INFO - ==> Sparsity : 0.671
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 85.320   Top5: 99.460]
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 84.640   Top5: 99.390]
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 83.900   Top5: 99.280]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0684)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.1554)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.6913)
features.3.conv.0 tensor(0.0781)
features.3.conv.3 tensor(0.0856)
features.3.conv.6 tensor(0.1265)
features.4.conv.0 tensor(0.2109)
features.4.conv.3 tensor(0.3218)
features.4.conv.6 tensor(0.6532)
features.5.conv.0 tensor(0.4972)
features.5.conv.3 tensor(0.4300)
features.5.conv.6 tensor(0.6691)
features.6.conv.0 tensor(0.0653)
features.6.conv.3 tensor(0.0584)
features.6.conv.6 tensor(0.0887)
features.7.conv.0 tensor(0.6250)
features.7.conv.3 tensor(0.4575)
features.7.conv.6 tensor(0.7646)
features.8.conv.0 tensor(0.6889)
features.8.conv.3 tensor(0.5298)
features.8.conv.6 tensor(0.8254)
features.9.conv.0 tensor(0.6701)
features.9.conv.3 tensor(0.5558)
features.9.conv.6 tensor(0.8374)
features.10.conv.0 tensor(0.0811)
features.10.conv.3 tensor(0.1079)
features.10.conv.6 tensor(0.1490)
features.11.conv.0 tensor(0.8052)
features.11.conv.3 tensor(0.6495)
features.11.conv.6 tensor(0.9149)
features.12.conv.0 tensor(0.7878)
features.12.conv.3 tensor(0.6813)
features.12.conv.6 tensor(0.9164)
features.13.conv.0 tensor(0.4206)
features.13.conv.3 tensor(0.4898)
features.13.conv.6 tensor(0.5541)
features.14.conv.0 tensor(0.9058)
features.14.conv.3 tensor(0.8389)
features.14.conv.6 tensor(0.9762)
features.15.conv.0 tensor(0.8919)
features.15.conv.3 tensor(0.8903)
features.15.conv.6 tensor(0.9728)
features.16.conv.0 tensor(0.7284)
features.16.conv.3 tensor(0.8110)
features.16.conv.6 tensor(0.8927)
conv.0 tensor(0.1737)
tensor(1469811.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   8
INFO - Training: 50000 samples (256 per mini-batch)
0.45810834
0.45806971
0.45803288
0.45799351
0.45799065
0.45797372
0.45797223
0.45793775
0.45787704
0.45783892
0.45778731
0.45772833
0.45770347
0.45766348
0.45764059
0.45760357
0.45754412
INFO - Training [8][   20/  196]   Loss 0.626540   Top1 79.316406   Top5 97.265625   BatchTime 0.518106   LR 0.000043
0.45746151
0.45738828
0.45732296
0.45725736
0.45721436
0.45717704
0.45713437
0.45708862
0.45703378
0.45695725
0.45691985
0.45688331
0.45682123
0.45675132
0.45671168
0.45667592
0.45665979
0.45662048
0.45659703
0.45655802
0.45653525
0.45648992
INFO - Training [8][   40/  196]   Loss 0.658461   Top1 78.251953   Top5 97.480469   BatchTime 0.490579   LR 0.000039
0.45647061
0.45646584
0.45645586
0.45643499
0.45643568
0.45642123
0.45640787
0.45641068
0.45640048
0.45640209
0.45636889
0.45636457
0.45634678
0.45633629
0.45633876
0.45632291
0.45631894
0.45633769
INFO - Training [8][   60/  196]   Loss 0.658543   Top1 78.190104   Top5 97.467448   BatchTime 0.480911   LR 0.000035
0.45634449
0.45635515
0.45636252
0.45635140
0.45634297
0.45633557
0.45632482
0.45630565
0.45628795
0.45627320
0.45623645
0.45617923
0.45612368
0.45605478
0.45600498
0.45595309
0.45588639
0.45582885
0.45578858
0.45573473
0.45570019
0.45564023
INFO - Training [8][   80/  196]   Loss 0.657178   Top1 78.168945   Top5 97.597656   BatchTime 0.468799   LR 0.000031
0.45559290
0.45555010
0.45549774
0.45544869
0.45540407
0.45536456
0.45532981
0.45530435
0.45528316
0.45525852
0.45522740
0.45519808
0.45515585
0.45512342
0.45508465
0.45506701
0.45503777
0.45503223
0.45501912
0.45499742
INFO - Training [8][  100/  196]   Loss 0.649398   Top1 78.359375   Top5 97.734375   BatchTime 0.457054   LR 0.000027
0.45497537
0.45495212
0.45493528
0.45491880
0.45489952
0.45485839
0.45483649
0.45479921
0.45477900
0.45473257
0.45470408
0.45469835
0.45465475
0.45464528
0.45463213
0.45463830
0.45464039
0.45463428
INFO - Training [8][  120/  196]   Loss 0.640869   Top1 78.720703   Top5 97.832031   BatchTime 0.452999   LR 0.000023
0.45462960
0.45463091
0.45463976
0.45463315
0.45462090
0.45459798
0.45458773
0.45453200
0.45449740
0.45446953
0.45446193
0.45445114
0.45444176
0.45442936
0.45442003
0.45442143
0.45440263
0.45438418
0.45436826
0.45435035
0.45433307
INFO - Training [8][  140/  196]   Loss 0.637987   Top1 78.769531   Top5 97.865513   BatchTime 0.444522   LR 0.000020
0.45430648
0.45429656
0.45428458
0.45425779
0.45423937
0.45420393
0.45417479
0.45416176
0.45414454
0.45412642
0.45411059
0.45409578
0.45407274
0.45405003
0.45401421
0.45398793
0.45395273
0.45392451
0.45390308
INFO - Training [8][  160/  196]   Loss 0.639691   Top1 78.693848   Top5 97.871094   BatchTime 0.441188   LR 0.000017
0.45388806
0.45387110
0.45385578
0.45382792
0.45379320
0.45376307
0.45374745
0.45371613
0.45368302
0.45365766
0.45364389
0.45361692
0.45360801
0.45358145
0.45357126
0.45356727
0.45355207
0.45354995
INFO - Training [8][  180/  196]   Loss 0.636506   Top1 78.793403   Top5 97.890625   BatchTime 0.441403   LR 0.000014
0.45352352
0.45352006
0.45350969
0.45351207
0.45348358
0.45346722
0.45345238
0.45343474
0.45341784
0.45339420
0.45337555
0.45337003
0.45336616
0.45335081
0.45332715
0.45330593
0.45328981
INFO - ==> Top1: 78.848    Top5: 97.896    Loss: 0.634
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.45327008
0.45325634
0.45323211
0.45320976
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [8][   20/   40]   Loss 0.430374   Top1 85.371094   Top5 99.335938   BatchTime 0.116252
INFO - Validation [8][   40/   40]   Loss 0.424145   Top1 85.640000   Top5 99.460000   BatchTime 0.083138
INFO - ==> Top1: 85.640    Top5: 99.460    Loss: 0.424
INFO - ==> Sparsity : 0.679
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 85.640   Top5: 99.460]
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 85.320   Top5: 99.460]
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 84.640   Top5: 99.390]
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0690)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0825)
features.2.conv.0 tensor(0.1577)
features.2.conv.3 tensor(0.3526)
features.2.conv.6 tensor(0.6910)
features.3.conv.0 tensor(0.0787)
features.3.conv.3 tensor(0.0833)
features.3.conv.6 tensor(0.1319)
features.4.conv.0 tensor(0.2319)
features.4.conv.3 tensor(0.3189)
features.4.conv.6 tensor(0.6533)
features.5.conv.0 tensor(0.5034)
features.5.conv.3 tensor(0.4294)
features.5.conv.6 tensor(0.6709)
features.6.conv.0 tensor(0.0648)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0890)
features.7.conv.0 tensor(0.6310)
features.7.conv.3 tensor(0.4560)
features.7.conv.6 tensor(0.7668)
features.8.conv.0 tensor(0.6924)
features.8.conv.3 tensor(0.5281)
features.8.conv.6 tensor(0.8260)
features.9.conv.0 tensor(0.6746)
features.9.conv.3 tensor(0.5564)
features.9.conv.6 tensor(0.8376)
features.10.conv.0 tensor(0.0831)
features.10.conv.3 tensor(0.1100)
features.10.conv.6 tensor(0.1538)
features.11.conv.0 tensor(0.8119)
features.11.conv.3 tensor(0.6503)
features.11.conv.6 tensor(0.9151)
features.12.conv.0 tensor(0.7912)
features.12.conv.3 tensor(0.6823)
features.12.conv.6 tensor(0.9158)
features.13.conv.0 tensor(0.4706)
features.13.conv.3 tensor(0.4898)
features.13.conv.6 tensor(0.5755)
features.14.conv.0 tensor(0.9061)
features.14.conv.3 tensor(0.8390)
features.14.conv.6 tensor(0.9763)
features.15.conv.0 tensor(0.8935)
features.15.conv.3 tensor(0.8900)
features.15.conv.6 tensor(0.9730)
features.16.conv.0 tensor(0.7375)
features.16.conv.3 tensor(0.8106)
features.16.conv.6 tensor(0.9021)
conv.0 tensor(0.1891)
tensor(1486870.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch   9
INFO - Training: 50000 samples (256 per mini-batch)
0.45319232
0.45316857
0.45316198
0.45315087
0.45313948
0.45314059
0.45312226
0.45310408
0.45308354
0.45307541
0.45306021
0.45305097
0.45305073
0.45304510
0.45302504
0.45301780
0.45302615
INFO - Training [9][   20/  196]   Loss 0.622073   Top1 79.082031   Top5 97.480469   BatchTime 0.517095   LR 0.000010
0.45302352
0.45300826
0.45298690
0.45297310
0.45295328
0.45294574
0.45292562
0.45290434
0.45289478
0.45288494
0.45287278
0.45286098
0.45283335
0.45280775
0.45279673
0.45278227
0.45277482
0.45276871
0.45276585
0.45273614
0.45272219
0.45268869
INFO - Training [9][   40/  196]   Loss 0.638395   Top1 78.535156   Top5 97.470703   BatchTime 0.486172   LR 0.000008
0.45267993
0.45266610
0.45265165
0.45265803
0.45265198
0.45263308
0.45263723
0.45262432
0.45261788
0.45260969
0.45261130
0.45260710
0.45259538
0.45259303
0.45259374
0.45259252
0.45258757
0.45259023
INFO - Training [9][   60/  196]   Loss 0.632786   Top1 78.756510   Top5 97.636719   BatchTime 0.474540   LR 0.000006
0.45257515
0.45257357
0.45256558
0.45256639
0.45256278
0.45254406
0.45254052
0.45253745
0.45251513
0.45250079
0.45248768
0.45248282
0.45247895
0.45246994
0.45246086
0.45245796
0.45245424
0.45244211
0.45244154
INFO - Training [9][   80/  196]   Loss 0.635814   Top1 78.618164   Top5 97.871094   BatchTime 0.457719   LR 0.000004
0.45244446
0.45243606
0.45242524
0.45242211
0.45241714
0.45240179
0.45239049
0.45239481
0.45238960
0.45238221
0.45237473
0.45236769
0.45236543
0.45235756
0.45233765
0.45232791
0.45232207
0.45232093
0.45231131
0.45230547
0.45230159
0.45228967
0.45228729
INFO - Training [9][  100/  196]   Loss 0.627994   Top1 78.925781   Top5 97.929688   BatchTime 0.456612   LR 0.000003
0.45228678
0.45229623
0.45229796
0.45228699
0.45227718
0.45227540
0.45226607
0.45225942
0.45226336
0.45225695
0.45226109
0.45225549
0.45224777
0.45224923
0.45224673
0.45225310
0.45224977
0.45224798
0.45224610
INFO - Training [9][  120/  196]   Loss 0.620755   Top1 79.225260   Top5 97.968750   BatchTime 0.451600   LR 0.000002
0.45224494
0.45224401
0.45224866
0.45225200
0.45224378
0.45224750
0.45225397
0.45225403
0.45225403
0.45224762
0.45225072
0.45224902
0.45225042
0.45224723
0.45224768
0.45225152
0.45224789
0.45224774
INFO - Training [9][  140/  196]   Loss 0.618093   Top1 79.335938   Top5 98.038504   BatchTime 0.447314   LR 0.000001
0.45223913
0.45223978
0.45223469
0.45222613
0.45221975
0.45222545
0.45222384
0.45222643
0.45222127
0.45222563
0.45223936
0.45223933
0.45224196
0.45225087
0.45224723
0.45225790
0.45225385
0.45226187
0.45226699
0.45226592
0.45226321
0.45225906
0.45226312
INFO - Training [9][  160/  196]   Loss 0.624251   Top1 79.099121   Top5 97.980957   BatchTime 0.447536   LR 0.000000
0.45226693
0.45226929
0.45225552
0.45225155
0.45224863
0.45223424
0.45224267
0.45223707
0.45223153
0.45222285
0.45222417
0.45223063
0.45221898
0.45221832
0.45222327
0.45221761
0.45221442
0.45221701
INFO - Training [9][  180/  196]   Loss 0.620670   Top1 79.244792   Top5 97.966580   BatchTime 0.447384   LR 0.000000
0.45222175
0.45222113
0.45221809
0.45221588
0.45222023
0.45221412
0.45222211
0.45222256
0.45221925
0.45221800
0.45221692
0.45221844
0.45221695
0.45220989
0.45221251
0.45220962
0.45221525
INFO - ==> Top1: 79.368    Top5: 97.984    Loss: 0.618
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.45222050
0.45221508
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [9][   20/   40]   Loss 0.426904   Top1 85.781250   Top5 99.335938   BatchTime 0.123509
INFO - Validation [9][   40/   40]   Loss 0.417058   Top1 86.010000   Top5 99.480000   BatchTime 0.092842
INFO - ==> Top1: 86.010    Top5: 99.480    Loss: 0.417
INFO - ==> Sparsity : 0.680
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.640   Top5: 99.460]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.320   Top5: 99.460]
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.4766)
features.1.conv.0 tensor(0.0690)
features.1.conv.3 tensor(0.0787)
features.1.conv.6 tensor(0.0812)
features.2.conv.0 tensor(0.1594)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.6916)
features.3.conv.0 tensor(0.0790)
features.3.conv.3 tensor(0.0841)
features.3.conv.6 tensor(0.1328)
features.4.conv.0 tensor(0.2323)
features.4.conv.3 tensor(0.3194)
features.4.conv.6 tensor(0.6532)
features.5.conv.0 tensor(0.5037)
features.5.conv.3 tensor(0.4294)
features.5.conv.6 tensor(0.6711)
features.6.conv.0 tensor(0.0649)
features.6.conv.3 tensor(0.0567)
features.6.conv.6 tensor(0.0891)
features.7.conv.0 tensor(0.6323)
features.7.conv.3 tensor(0.4557)
features.7.conv.6 tensor(0.7673)
features.8.conv.0 tensor(0.6932)
features.8.conv.3 tensor(0.5275)
features.8.conv.6 tensor(0.8261)
features.9.conv.0 tensor(0.6753)
features.9.conv.3 tensor(0.5567)
features.9.conv.6 tensor(0.8377)
features.10.conv.0 tensor(0.0837)
features.10.conv.3 tensor(0.1088)
features.10.conv.6 tensor(0.1544)
features.11.conv.0 tensor(0.8120)
features.11.conv.3 tensor(0.6497)
features.11.conv.6 tensor(0.9150)
features.12.conv.0 tensor(0.7916)
features.12.conv.3 tensor(0.6823)
features.12.conv.6 tensor(0.9158)
features.13.conv.0 tensor(0.4766)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.5802)
features.14.conv.0 tensor(0.9062)
features.14.conv.3 tensor(0.8389)
features.14.conv.6 tensor(0.9764)
features.15.conv.0 tensor(0.8936)
features.15.conv.3 tensor(0.8902)
features.15.conv.6 tensor(0.9730)
features.16.conv.0 tensor(0.7393)
features.16.conv.3 tensor(0.8106)
features.16.conv.6 tensor(0.9033)
conv.0 tensor(0.1904)
tensor(1489029.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  10
INFO - Training: 50000 samples (256 per mini-batch)
0.45222378
0.45230719
0.45232955
0.45214283
0.45190647
0.45171174
0.45154089
0.45133331
0.45131066
0.45128611
0.45134220
0.45150018
0.45158419
0.45167923
0.45170918
0.45167217
0.45159835
0.45164406
INFO - Training [10][   20/  196]   Loss 0.695661   Top1 76.972656   Top5 96.875000   BatchTime 0.537547   LR 0.000250
0.45169768
0.45181051
0.45207989
0.45231894
0.45252219
0.45283225
0.45318398
0.45345902
0.45371190
0.45386398
0.45406711
0.45425346
0.45454326
0.45496032
0.45527938
0.45571855
0.45582160
INFO - Training [10][   40/  196]   Loss 0.702491   Top1 76.660156   Top5 97.050781   BatchTime 0.489383   LR 0.000250
0.45601621
0.45615134
0.45621136
0.45632872
0.45642713
0.45652109
0.45666325
0.45679265
0.45722041
0.45755845
0.45746776
0.45746613
0.45752609
0.45759219
0.45772591
0.45778462
0.45775947
0.45772105
0.45769879
0.45774376
0.45761681
0.45753041
0.45739058
INFO - Training [10][   60/  196]   Loss 0.696104   Top1 76.855469   Top5 97.233073   BatchTime 0.476981   LR 0.000250
0.45733410
0.45735863
0.45724708
0.45720619
0.45724609
0.45719159
0.45703188
0.45690951
0.45684120
0.45676270
0.45681065
0.45695809
0.45704040
0.45714822
0.45718899
0.45723128
0.45738822
0.45728245
0.45724291
0.45715693
INFO - Training [10][   80/  196]   Loss 0.699545   Top1 76.821289   Top5 97.343750   BatchTime 0.455526   LR 0.000250
0.45707971
0.45712128
0.45722064
0.45720303
0.45737046
0.45737132
0.45732683
0.45723993
0.45725751
0.45741749
0.45739463
0.45752126
0.45747599
0.45756051
0.45758837
0.45764852
0.45772627
0.45781243
0.45775625
INFO - Training [10][  100/  196]   Loss 0.696179   Top1 76.882812   Top5 97.398438   BatchTime 0.429472   LR 0.000250
0.45775631
0.45787263
0.45790258
0.45790082
0.45789573
0.45780849
0.45769146
0.45766437
0.45760587
0.45766047
0.45759228
0.45768568
0.45769185
0.45756936
0.45744851
0.45739844
0.45737249
0.45743516
0.45752624
0.45763299
0.45776260
INFO - Training [10][  120/  196]   Loss 0.690795   Top1 77.073568   Top5 97.519531   BatchTime 0.420474   LR 0.000249
0.45782927
0.45775038
0.45788613
0.45805046
0.45842224
0.45871472
0.45886111
0.45889384
0.45874214
0.45863175
0.45852262
0.45853627
0.45854929
0.45857981
0.45863160
0.45884287
0.45927098
0.45917228
0.45914274
INFO - Training [10][  140/  196]   Loss 0.691184   Top1 77.059152   Top5 97.594866   BatchTime 0.423280   LR 0.000249
0.45922440
0.45926818
0.45928741
0.45920658
0.45921886
0.45927957
0.45933801
0.45935220
0.45929909
0.45928147
0.45920709
0.45907032
0.45904142
0.45907283
0.45912838
0.45891201
0.45881993
0.45889643
0.45906913
0.45908478
0.45907620
INFO - Training [10][  160/  196]   Loss 0.698217   Top1 76.748047   Top5 97.553711   BatchTime 0.429548   LR 0.000249
0.45886639
0.45889580
0.45895857
0.45896012
0.45898136
0.45897055
0.45893383
0.45905423
0.45895463
0.45897597
0.45914373
0.45928088
0.45921391
0.45930749
0.45923963
0.45899349
0.45891976
0.45875588
0.45849717
0.45830178
0.45834860
INFO - Training [10][  180/  196]   Loss 0.697462   Top1 76.701389   Top5 97.558594   BatchTime 0.434567   LR 0.000249
0.45847684
0.45857200
0.45848033
0.45843598
0.45822284
0.45809928
0.45793450
0.45775923
0.45773509
0.45773154
0.45787933
0.45795089
0.45809138
INFO - ==> Top1: 76.748    Top5: 97.558    Loss: 0.697
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.45828336
0.45828119
0.45810792
0.45806241
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [10][   20/   40]   Loss 0.514067   Top1 83.027344   Top5 98.867188   BatchTime 0.127661
INFO - Validation [10][   40/   40]   Loss 0.498435   Top1 83.360000   Top5 99.060000   BatchTime 0.093294
INFO - ==> Top1: 83.360    Top5: 99.060    Loss: 0.498
INFO - ==> Sparsity : 0.678
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.640   Top5: 99.460]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.320   Top5: 99.460]
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.4922)
features.1.conv.0 tensor(0.0527)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1516)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.6947)
features.3.conv.0 tensor(0.0856)
features.3.conv.3 tensor(0.0895)
features.3.conv.6 tensor(0.1079)
features.4.conv.0 tensor(0.2017)
features.4.conv.3 tensor(0.3258)
features.4.conv.6 tensor(0.6561)
features.5.conv.0 tensor(0.4564)
features.5.conv.3 tensor(0.4277)
features.5.conv.6 tensor(0.6732)
features.6.conv.0 tensor(0.0627)
features.6.conv.3 tensor(0.0579)
features.6.conv.6 tensor(0.0924)
features.7.conv.0 tensor(0.5901)
features.7.conv.3 tensor(0.4572)
features.7.conv.6 tensor(0.7662)
features.8.conv.0 tensor(0.6717)
features.8.conv.3 tensor(0.5324)
features.8.conv.6 tensor(0.8266)
features.9.conv.0 tensor(0.6543)
features.9.conv.3 tensor(0.5573)
features.9.conv.6 tensor(0.8393)
features.10.conv.0 tensor(0.0815)
features.10.conv.3 tensor(0.1059)
features.10.conv.6 tensor(0.1556)
features.11.conv.0 tensor(0.7900)
features.11.conv.3 tensor(0.6491)
features.11.conv.6 tensor(0.9201)
features.12.conv.0 tensor(0.7648)
features.12.conv.3 tensor(0.6813)
features.12.conv.6 tensor(0.9184)
features.13.conv.0 tensor(0.4018)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.5753)
features.14.conv.0 tensor(0.9077)
features.14.conv.3 tensor(0.8394)
features.14.conv.6 tensor(0.9763)
features.15.conv.0 tensor(0.8987)
features.15.conv.3 tensor(0.8904)
features.15.conv.6 tensor(0.9743)
features.16.conv.0 tensor(0.7222)
features.16.conv.3 tensor(0.8102)
features.16.conv.6 tensor(0.9065)
conv.0 tensor(0.2001)
tensor(1483099.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  11
INFO - Training: 50000 samples (256 per mini-batch)
0.45805851
0.45803982
0.45802370
0.45805484
0.45803216
0.45808262
0.45825493
0.45827535
0.45817927
0.45808533
0.45804688
0.45803311
0.45790556
0.45784056
0.45774105
0.45779812
0.45781380
0.45768183
0.45762852
INFO - Training [11][   20/  196]   Loss 0.711005   Top1 76.191406   Top5 97.050781   BatchTime 0.509154   LR 0.000248
0.45762086
0.45747694
0.45739707
0.45746949
0.45739561
0.45740059
0.45737118
0.45736602
0.45735967
0.45738199
0.45740828
0.45745638
0.45743069
0.45742226
0.45731491
0.45732674
0.45731667
INFO - Training [11][   40/  196]   Loss 0.706822   Top1 76.376953   Top5 97.275391   BatchTime 0.490130   LR 0.000248
0.45730984
0.45737872
0.45733643
0.45741323
0.45757067
0.45752609
0.45763397
0.45761546
0.45767802
0.45760638
0.45759955
0.45763320
0.45757648
0.45759770
0.45752048
0.45748550
0.45760295
0.45766640
0.45774114
0.45774364
0.45785624
0.45776609
0.45783564
INFO - Training [11][   60/  196]   Loss 0.702946   Top1 76.328125   Top5 97.428385   BatchTime 0.474242   LR 0.000247
0.45787451
0.45792949
0.45795488
0.45792964
0.45788965
0.45785680
0.45796978
0.45793369
0.45789859
0.45780692
0.45776412
0.45765680
0.45762065
0.45765132
0.45765200
0.45768049
0.45772961
INFO - Training [11][   80/  196]   Loss 0.702216   Top1 76.333008   Top5 97.563477   BatchTime 0.468386   LR 0.000247
0.45783660
0.45800418
0.45787296
0.45767325
0.45743096
0.45731291
0.45716691
0.45704925
0.45697701
0.45692503
0.45686784
0.45673662
0.45681661
0.45676753
0.45673233
0.45659661
0.45659730
0.45657346
0.45656422
0.45654488
INFO - Training [11][  100/  196]   Loss 0.695102   Top1 76.558594   Top5 97.679688   BatchTime 0.452548   LR 0.000247
0.45660901
0.45667857
0.45684224
0.45686996
0.45677105
0.45677727
0.45674387
0.45684052
0.45692593
0.45699680
0.45704529
0.45705014
0.45702711
0.45708323
0.45722342
0.45725009
0.45733583
0.45721269
0.45728254
0.45718095
0.45722654
0.45728332
INFO - Training [11][  120/  196]   Loss 0.687081   Top1 76.796875   Top5 97.786458   BatchTime 0.438244   LR 0.000246
0.45735431
0.45748919
0.45741677
0.45751503
0.45748198
0.45747077
0.45745942
0.45744476
0.45742837
0.45741627
0.45742527
0.45750839
0.45757297
0.45758399
0.45754921
0.45746621
0.45747590
0.45744401
INFO - Training [11][  140/  196]   Loss 0.689063   Top1 76.858259   Top5 97.779018   BatchTime 0.438667   LR 0.000246
0.45728004
0.45694077
0.45691648
0.45699593
0.45716769
0.45722792
0.45718595
0.45709684
0.45708886
0.45705721
0.45694941
0.45693862
0.45687729
0.45683616
0.45680308
0.45675743
0.45670012
0.45655155
0.45649853
0.45648223
0.45653284
0.45649287
0.45645133
INFO - Training [11][  160/  196]   Loss 0.690037   Top1 76.821289   Top5 97.756348   BatchTime 0.440232   LR 0.000245
0.45630351
0.45623881
0.45615700
0.45608726
0.45611033
0.45607910
0.45612773
0.45614907
0.45619684
0.45626196
0.45624864
0.45618808
0.45618019
0.45615768
0.45614243
0.45615679
0.45618388
INFO - Training [11][  180/  196]   Loss 0.690330   Top1 76.814236   Top5 97.695312   BatchTime 0.442073   LR 0.000244
0.45623326
0.45627734
0.45639652
0.45660952
0.45717829
0.45846283
0.45967978
0.46018955
0.46033916
0.46039453
0.46034184
0.46015987
0.45998058
0.45983741
0.45955005
0.45921910
0.45897356
0.45871717
INFO - ==> Top1: 76.948    Top5: 97.686    Loss: 0.688
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.45857432
0.45826638
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [11][   20/   40]   Loss 0.498427   Top1 83.300781   Top5 99.082031   BatchTime 0.120834
INFO - Validation [11][   40/   40]   Loss 0.488659   Top1 83.300000   Top5 99.290000   BatchTime 0.090270
INFO - ==> Top1: 83.300    Top5: 99.290    Loss: 0.489
INFO - ==> Sparsity : 0.679
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.640   Top5: 99.460]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.320   Top5: 99.460]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  12
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.5098)
features.1.conv.0 tensor(0.0592)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1348)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.6939)
features.3.conv.0 tensor(0.0819)
features.3.conv.3 tensor(0.0918)
features.3.conv.6 tensor(0.1178)
features.4.conv.0 tensor(0.1637)
features.4.conv.3 tensor(0.3270)
features.4.conv.6 tensor(0.6522)
features.5.conv.0 tensor(0.4497)
features.5.conv.3 tensor(0.4306)
features.5.conv.6 tensor(0.6812)
features.6.conv.0 tensor(0.0654)
features.6.conv.3 tensor(0.0486)
features.6.conv.6 tensor(0.0868)
features.7.conv.0 tensor(0.5915)
features.7.conv.3 tensor(0.4566)
features.7.conv.6 tensor(0.7692)
features.8.conv.0 tensor(0.6794)
features.8.conv.3 tensor(0.5284)
features.8.conv.6 tensor(0.8300)
features.9.conv.0 tensor(0.6536)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.8398)
features.10.conv.0 tensor(0.0849)
features.10.conv.3 tensor(0.1088)
features.10.conv.6 tensor(0.1526)
features.11.conv.0 tensor(0.7918)
features.11.conv.3 tensor(0.6485)
features.11.conv.6 tensor(0.9191)
features.12.conv.0 tensor(0.7721)
features.12.conv.3 tensor(0.6813)
features.12.conv.6 tensor(0.9187)
features.13.conv.0 tensor(0.4434)
features.13.conv.3 tensor(0.4896)
features.13.conv.6 tensor(0.5921)
features.14.conv.0 tensor(0.9144)
features.14.conv.3 tensor(0.8398)
features.14.conv.6 tensor(0.9760)
features.15.conv.0 tensor(0.9014)
features.15.conv.3 tensor(0.8899)
features.15.conv.6 tensor(0.9739)
features.16.conv.0 tensor(0.7275)
features.16.conv.3 tensor(0.8104)
features.16.conv.6 tensor(0.9082)
conv.0 tensor(0.1916)
tensor(1486642.) 2188896.0
0.45807496
0.45788601
0.45774490
0.45761636
0.45746467
0.45726955
0.45718014
0.45718086
0.45703897
0.45691907
0.45684734
0.45683813
0.45676219
0.45664552
0.45653749
0.45643330
0.45630166
INFO - Training [12][   20/  196]   Loss 0.700092   Top1 76.992188   Top5 97.285156   BatchTime 0.527776   LR 0.000243
0.45618215
0.45606667
0.45597464
0.45589674
0.45578283
0.45568633
0.45560837
0.45548639
0.45538178
0.45536211
0.45536906
0.45536926
0.45540124
0.45543468
0.45541298
0.45537221
0.45529664
0.45527655
0.45521891
0.45513526
0.45501244
0.45490563
INFO - Training [12][   40/  196]   Loss 0.699333   Top1 76.660156   Top5 97.197266   BatchTime 0.489681   LR 0.000243
0.45489866
0.45491174
0.45486334
0.45490435
0.45494911
0.45499471
0.45503321
0.45508018
0.45510224
0.45510858
0.45510998
0.45519346
0.45530763
0.45546117
0.45556459
0.45580980
0.45625681
INFO - Training [12][   60/  196]   Loss 0.684996   Top1 77.174479   Top5 97.337240   BatchTime 0.479876   LR 0.000242
0.45653853
0.45655161
0.45644397
0.45638394
0.45623988
0.45614535
0.45607355
0.45600140
0.45600340
0.45597231
0.45583063
0.45575175
0.45581728
0.45590201
0.45582476
0.45576498
0.45561770
0.45530301
0.45516223
0.45514405
0.45517388
0.45508352
0.45512846
INFO - Training [12][   80/  196]   Loss 0.688627   Top1 77.055664   Top5 97.431641   BatchTime 0.470772   LR 0.000241
0.45503786
0.45498422
0.45491594
0.45487842
0.45487741
0.45478889
0.45476156
0.45469981
0.45466417
0.45467007
0.45463926
0.45462847
0.45464155
0.45465186
0.45462096
0.45463774
0.45456302
0.45445213
0.45437732
INFO - Training [12][  100/  196]   Loss 0.681293   Top1 77.261719   Top5 97.500000   BatchTime 0.457119   LR 0.000240
0.45431492
0.45424902
0.45414922
0.45415670
0.45416534
0.45424443
0.45426413
0.45441842
0.45448661
0.45459417
0.45466852
0.45470506
0.45472074
0.45466989
0.45466241
0.45471236
INFO - Training [12][  120/  196]   Loss 0.675720   Top1 77.399089   Top5 97.584635   BatchTime 0.438971   LR 0.000240
0.45474082
0.45473942
0.45478991
0.45479161
0.45474797
0.45478842
0.45479223
0.45484957
0.45483628
0.45477739
0.45482376
0.45484936
0.45484376
0.45492008
0.45494217
0.45498237
0.45505202
0.45507935
0.45497635
0.45497370
0.45498255
0.45488688
0.45485121
INFO - Training [12][  140/  196]   Loss 0.673734   Top1 77.458147   Top5 97.709263   BatchTime 0.417020   LR 0.000239
0.45485824
0.45477399
0.45468062
0.45454621
0.45446581
0.45438856
0.45443100
0.45445210
0.45446685
0.45438102
0.45439065
0.45434645
0.45418701
0.45409462
0.45399973
0.45398173
0.45401871
0.45409706
0.45421487
0.45469508
0.45536518
INFO - Training [12][  160/  196]   Loss 0.679909   Top1 77.270508   Top5 97.666016   BatchTime 0.412219   LR 0.000238
0.45568040
0.45576751
0.45590025
0.45575950
0.45560297
0.45548800
0.45525888
0.45507938
0.45502275
0.45490924
0.45473564
0.45461342
0.45453990
0.45459041
0.45464370
0.45464110
0.45459819
0.45455796
0.45440421
0.45423999
INFO - Training [12][  180/  196]   Loss 0.678609   Top1 77.243924   Top5 97.630208   BatchTime 0.411891   LR 0.000237
0.45420814
0.45416546
0.45408571
0.45399755
0.45391649
0.45386559
0.45385849
0.45377886
0.45370066
0.45366955
0.45352903
0.45345569
0.45334119
0.45326945
0.45323402
0.45317498
INFO - ==> Top1: 77.374    Top5: 97.668    Loss: 0.676
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.45307136
0.45308629
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [12][   20/   40]   Loss 0.477670   Top1 83.632812   Top5 99.296875   BatchTime 0.126691
INFO - Validation [12][   40/   40]   Loss 0.468073   Top1 84.160000   Top5 99.440000   BatchTime 0.088337
INFO - ==> Top1: 84.160    Top5: 99.440    Loss: 0.468
INFO - ==> Sparsity : 0.685
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.640   Top5: 99.460]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.320   Top5: 99.460]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  13
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.5234)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0671)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.1444)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.7020)
features.3.conv.0 tensor(0.0822)
features.3.conv.3 tensor(0.0903)
features.3.conv.6 tensor(0.1302)
features.4.conv.0 tensor(0.1854)
features.4.conv.3 tensor(0.3252)
features.4.conv.6 tensor(0.6593)
features.5.conv.0 tensor(0.4556)
features.5.conv.3 tensor(0.4288)
features.5.conv.6 tensor(0.6823)
features.6.conv.0 tensor(0.0620)
features.6.conv.3 tensor(0.0573)
features.6.conv.6 tensor(0.0881)
features.7.conv.0 tensor(0.5992)
features.7.conv.3 tensor(0.4595)
features.7.conv.6 tensor(0.7775)
features.8.conv.0 tensor(0.6763)
features.8.conv.3 tensor(0.5307)
features.8.conv.6 tensor(0.8276)
features.9.conv.0 tensor(0.6497)
features.9.conv.3 tensor(0.5558)
features.9.conv.6 tensor(0.8422)
features.10.conv.0 tensor(0.0730)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.1648)
features.11.conv.0 tensor(0.7938)
features.11.conv.3 tensor(0.6520)
features.11.conv.6 tensor(0.9218)
features.12.conv.0 tensor(0.7740)
features.12.conv.3 tensor(0.6813)
features.12.conv.6 tensor(0.9225)
features.13.conv.0 tensor(0.4565)
features.13.conv.3 tensor(0.4875)
features.13.conv.6 tensor(0.6023)
features.14.conv.0 tensor(0.9146)
features.14.conv.3 tensor(0.8399)
features.14.conv.6 tensor(0.9770)
features.15.conv.0 tensor(0.9104)
features.15.conv.3 tensor(0.8911)
features.15.conv.6 tensor(0.9761)
features.16.conv.0 tensor(0.7312)
features.16.conv.3 tensor(0.8110)
features.16.conv.6 tensor(0.9100)
conv.0 tensor(0.2065)
tensor(1498750.) 2188896.0
0.45308790
0.45306367
0.45303401
0.45299044
0.45294976
0.45290482
0.45284545
0.45275480
0.45267349
0.45264813
0.45266667
0.45271277
0.45277265
0.45277873
0.45281640
0.45289943
0.45298132
INFO - Training [13][   20/  196]   Loss 0.674353   Top1 77.402344   Top5 97.441406   BatchTime 0.411272   LR 0.000235
0.45309120
0.45303541
0.45301330
0.45314810
0.45312950
0.45310047
0.45290688
0.45273307
0.45260975
0.45253301
0.45249093
0.45243952
0.45239738
0.45230755
0.45221919
0.45215440
0.45214918
INFO - Training [13][   40/  196]   Loss 0.680979   Top1 77.246094   Top5 97.519531   BatchTime 0.382750   LR 0.000235
0.45212817
0.45208052
0.45213884
0.45229504
0.45232144
0.45230353
0.45240051
0.45229790
0.45222098
0.45208374
0.45202199
0.45201936
0.45195076
0.45191669
0.45178136
0.45164484
0.45140946
0.45133427
0.45134747
0.45142588
0.45152116
0.45156807
0.45160106
INFO - Training [13][   60/  196]   Loss 0.675662   Top1 77.337240   Top5 97.649740   BatchTime 0.377283   LR 0.000234
0.45161474
0.45168716
0.45174745
0.45184928
0.45187542
0.45192134
0.45193380
0.45198244
0.45207912
0.45205745
0.45204946
0.45203745
0.45187899
0.45180944
0.45169315
0.45170048
0.45170230
0.45172855
0.45181444
0.45193446
0.45202741
0.45206514
INFO - Training [13][   80/  196]   Loss 0.673163   Top1 77.338867   Top5 97.729492   BatchTime 0.371006   LR 0.000233
0.45209303
0.45219421
0.45219040
0.45222121
0.45237848
0.45251408
0.45266560
0.45280680
0.45283264
0.45289689
0.45289424
0.45284784
0.45283026
0.45277679
0.45275980
0.45273358
0.45273367
INFO - Training [13][  100/  196]   Loss 0.663658   Top1 77.667969   Top5 97.843750   BatchTime 0.367996   LR 0.000232
0.45280221
0.45279750
0.45267761
0.45259652
0.45267799
0.45278725
0.45277923
0.45275936
0.45275986
0.45276502
0.45265070
0.45263568
0.45262572
0.45264733
0.45268261
0.45270747
0.45265251
0.45256403
0.45249301
0.45250979
0.45253217
0.45252505
INFO - Training [13][  120/  196]   Loss 0.661437   Top1 77.802734   Top5 97.851562   BatchTime 0.366180   LR 0.000230
0.45260057
0.45303169
0.45374060
0.45391786
0.45389780
0.45383638
0.45380238
0.45375046
0.45361778
0.45344588
0.45332691
0.45315561
0.45301661
0.45288181
0.45261917
0.45243910
0.45241147
0.45241931
0.45241377
0.45241749
INFO - Training [13][  140/  196]   Loss 0.659841   Top1 77.876674   Top5 97.926897   BatchTime 0.359276   LR 0.000229
0.45240781
0.45237312
0.45245507
0.45265612
0.45280197
0.45289281
0.45263469
0.45258483
0.45248878
0.45240811
0.45242089
0.45235434
0.45240429
0.45243591
0.45242879
0.45235145
0.45238653
0.45239973
0.45247760
0.45257714
0.45243385
INFO - Training [13][  160/  196]   Loss 0.660394   Top1 77.792969   Top5 97.907715   BatchTime 0.347899   LR 0.000228
0.45231149
0.45231020
0.45229667
0.45212427
0.45203200
0.45200261
0.45200023
0.45208606
0.45206723
0.45203534
0.45209271
0.45211193
0.45215333
0.45227942
0.45227450
0.45224527
0.45226905
0.45229822
INFO - Training [13][  180/  196]   Loss 0.660916   Top1 77.808160   Top5 97.862413   BatchTime 0.346680   LR 0.000227
0.45227751
0.45217246
0.45218626
0.45219144
0.45225418
0.45232484
0.45231399
0.45225412
0.45221886
0.45214745
0.45218787
0.45220339
0.45210397
0.45216039
0.45225576
0.45211062
INFO - ==> Top1: 77.864    Top5: 97.854    Loss: 0.660
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.45208639
0.45208639
0.45214823
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [13][   20/   40]   Loss 0.474274   Top1 83.671875   Top5 99.238281   BatchTime 0.128493
INFO - Validation [13][   40/   40]   Loss 0.462402   Top1 84.080000   Top5 99.380000   BatchTime 0.097201
INFO - ==> Top1: 84.080    Top5: 99.380    Loss: 0.462
INFO - ==> Sparsity : 0.691
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 85.640   Top5: 99.460]
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 85.320   Top5: 99.460]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  14
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.5273)
features.1.conv.0 tensor(0.0566)
features.1.conv.3 tensor(0.0718)
features.1.conv.6 tensor(0.0859)
features.2.conv.0 tensor(0.1508)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.7066)
features.3.conv.0 tensor(0.0880)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1168)
features.4.conv.0 tensor(0.2301)
features.4.conv.3 tensor(0.3252)
features.4.conv.6 tensor(0.6577)
features.5.conv.0 tensor(0.4536)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.6839)
features.6.conv.0 tensor(0.0573)
features.6.conv.3 tensor(0.0544)
features.6.conv.6 tensor(0.0890)
features.7.conv.0 tensor(0.5849)
features.7.conv.3 tensor(0.4557)
features.7.conv.6 tensor(0.7811)
features.8.conv.0 tensor(0.6723)
features.8.conv.3 tensor(0.5286)
features.8.conv.6 tensor(0.8317)
features.9.conv.0 tensor(0.6645)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.8464)
features.10.conv.0 tensor(0.0684)
features.10.conv.3 tensor(0.1085)
features.10.conv.6 tensor(0.1765)
features.11.conv.0 tensor(0.7978)
features.11.conv.3 tensor(0.6514)
features.11.conv.6 tensor(0.9226)
features.12.conv.0 tensor(0.7720)
features.12.conv.3 tensor(0.6836)
features.12.conv.6 tensor(0.9230)
features.13.conv.0 tensor(0.4676)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.6094)
features.14.conv.0 tensor(0.9164)
features.14.conv.3 tensor(0.8396)
features.14.conv.6 tensor(0.9772)
features.15.conv.0 tensor(0.9146)
features.15.conv.3 tensor(0.8905)
features.15.conv.6 tensor(0.9766)
features.16.conv.0 tensor(0.7321)
features.16.conv.3 tensor(0.8111)
features.16.conv.6 tensor(0.9124)
conv.0 tensor(0.2326)
tensor(1513577.) 2188896.0
0.45214939
0.45218956
0.45225748
0.45239455
0.45235634
0.45243302
0.45253658
0.45256022
0.45280227
0.45357412
0.45356461
0.45352924
0.45348844
0.45343605
0.45345750
0.45327359
0.45312673
0.45301971
INFO - Training [14][   20/  196]   Loss 0.653455   Top1 77.890625   Top5 97.363281   BatchTime 0.475065   LR 0.000225
0.45295238
0.45286781
0.45290381
0.45302561
0.45291093
0.45285642
0.45286041
0.45290130
0.45282122
0.45282593
0.45272249
0.45273849
0.45276144
0.45273849
0.45271522
0.45263550
0.45270675
0.45277289
0.45279950
0.45287293
0.45282200
0.45266971
INFO - Training [14][   40/  196]   Loss 0.675076   Top1 77.402344   Top5 97.460938   BatchTime 0.424454   LR 0.000224
0.45273495
0.45263317
0.45251295
0.45238030
0.45231825
0.45238367
0.45244527
0.45240924
0.45240587
0.45246148
0.45247763
0.45243707
0.45265242
0.45273432
0.45272854
0.45280698
INFO - Training [14][   60/  196]   Loss 0.673959   Top1 77.434896   Top5 97.447917   BatchTime 0.407705   LR 0.000223
0.45278707
0.45282286
0.45281783
0.45278084
0.45254046
0.45227617
0.45219478
0.45216778
0.45208731
0.45201740
0.45200387
0.45204428
0.45198750
0.45193380
0.45195714
0.45202523
0.45200333
0.45209476
0.45210862
0.45187652
0.45177943
0.45176074
INFO - Training [14][   80/  196]   Loss 0.660153   Top1 77.919922   Top5 97.680664   BatchTime 0.394801   LR 0.000221
0.45170364
0.45169899
0.45168692
0.45168352
0.45168722
0.45174667
0.45180243
0.45187148
0.45201749
0.45222333
0.45209596
0.45211065
0.45182970
0.45169613
0.45162004
0.45158234
0.45149943
INFO - Training [14][  100/  196]   Loss 0.651021   Top1 78.289062   Top5 97.796875   BatchTime 0.384521   LR 0.000220
0.45141318
0.45138523
0.45130038
0.45125687
0.45123088
0.45122495
0.45118809
0.45133340
0.45145702
0.45142713
0.45124224
0.45115021
0.45112038
0.45108011
0.45111904
0.45111778
0.45109740
0.45115548
0.45149338
0.45126942
0.45118129
0.45106682
0.45093390
INFO - Training [14][  120/  196]   Loss 0.647847   Top1 78.466797   Top5 97.867839   BatchTime 0.381682   LR 0.000219
0.45084834
0.45078620
0.45073727
0.45076418
0.45078692
0.45077172
0.45079300
0.45076355
0.45085111
0.45093048
0.45089179
0.45074451
0.45075160
0.45077196
0.45065680
0.45063010
0.45054895
0.45050877
0.45059726
0.45056185
0.45045915
0.45046929
INFO - Training [14][  140/  196]   Loss 0.645263   Top1 78.582589   Top5 97.912946   BatchTime 0.379182   LR 0.000217
0.45041138
0.45041943
0.45036307
0.45035735
0.45045134
0.45034334
0.45042920
0.45045212
0.45036474
0.45022449
0.45014983
0.45001021
0.44990668
0.44991061
0.44987473
0.44989729
0.44968858
0.44966772
INFO - Training [14][  160/  196]   Loss 0.643568   Top1 78.676758   Top5 97.900391   BatchTime 0.371925   LR 0.000216
0.44966871
0.44956985
0.44954872
0.44954571
0.44951349
0.44953239
0.44956365
0.44963589
0.44976863
0.44977164
0.44980258
0.44988534
0.44986650
0.44992235
0.44991854
0.44984519
0.44980881
0.44978350
0.44979477
0.44983977
0.45019352
0.45124173
INFO - Training [14][  180/  196]   Loss 0.641306   Top1 78.702257   Top5 97.873264   BatchTime 0.361308   LR 0.000215
0.45133096
0.45110917
0.45116130
0.45111728
0.45093468
0.45074621
0.45041627
0.45021260
0.45019341
0.45018798
0.45002118
0.44991615
0.44982955
0.44972837
INFO - ==> Top1: 78.832    Top5: 97.924    Loss: 0.637
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.44973171
0.44971871
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [14][   20/   40]   Loss 0.445532   Top1 85.214844   Top5 99.160156   BatchTime 0.121152
INFO - Validation [14][   40/   40]   Loss 0.427189   Top1 85.720000   Top5 99.310000   BatchTime 0.088049
INFO - ==> Top1: 85.720    Top5: 99.310    Loss: 0.427
INFO - ==> Sparsity : 0.697
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 85.720   Top5: 99.310]
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 85.640   Top5: 99.460]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  15
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2812)
features.0.conv.3 tensor(0.5410)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0613)
features.1.conv.6 tensor(0.0903)
features.2.conv.0 tensor(0.1568)
features.2.conv.3 tensor(0.3557)
features.2.conv.6 tensor(0.7098)
features.3.conv.0 tensor(0.0909)
features.3.conv.3 tensor(0.0810)
features.3.conv.6 tensor(0.1228)
features.4.conv.0 tensor(0.2140)
features.4.conv.3 tensor(0.3223)
features.4.conv.6 tensor(0.6680)
features.5.conv.0 tensor(0.4661)
features.5.conv.3 tensor(0.4300)
features.5.conv.6 tensor(0.6873)
features.6.conv.0 tensor(0.0570)
features.6.conv.3 tensor(0.0538)
features.6.conv.6 tensor(0.0892)
features.7.conv.0 tensor(0.6031)
features.7.conv.3 tensor(0.4540)
features.7.conv.6 tensor(0.7808)
features.8.conv.0 tensor(0.6795)
features.8.conv.3 tensor(0.5304)
features.8.conv.6 tensor(0.8303)
features.9.conv.0 tensor(0.6590)
features.9.conv.3 tensor(0.5593)
features.9.conv.6 tensor(0.8471)
features.10.conv.0 tensor(0.0753)
features.10.conv.3 tensor(0.1088)
features.10.conv.6 tensor(0.1489)
features.11.conv.0 tensor(0.8037)
features.11.conv.3 tensor(0.6501)
features.11.conv.6 tensor(0.9281)
features.12.conv.0 tensor(0.7814)
features.12.conv.3 tensor(0.6802)
features.12.conv.6 tensor(0.9248)
features.13.conv.0 tensor(0.4802)
features.13.conv.3 tensor(0.4915)
features.13.conv.6 tensor(0.6165)
features.14.conv.0 tensor(0.9201)
features.14.conv.3 tensor(0.8399)
features.14.conv.6 tensor(0.9765)
features.15.conv.0 tensor(0.9148)
features.15.conv.3 tensor(0.8881)
features.15.conv.6 tensor(0.9763)
features.16.conv.0 tensor(0.7363)
features.16.conv.3 tensor(0.8104)
features.16.conv.6 tensor(0.9131)
conv.0 tensor(0.2507)
tensor(1524614.) 2188896.0
0.44964424
0.44957712
0.44954613
0.44954380
0.44953927
0.44955572
0.44957682
0.44953391
0.44952044
0.44945699
0.44944325
0.44942361
0.44938260
0.44927937
0.44918981
0.44918257
INFO - Training [15][   20/  196]   Loss 0.652222   Top1 77.617188   Top5 97.675781   BatchTime 0.457540   LR 0.000212
0.44922584
0.44917354
0.44914368
0.44916508
0.44922376
0.44912648
0.44901738
0.44901615
0.44895881
0.44889838
0.44883764
0.44881085
0.44877720
0.44884422
0.44886619
0.44888517
0.44882706
0.44881469
0.44874600
0.44873738
0.44870269
INFO - Training [15][   40/  196]   Loss 0.652743   Top1 77.705078   Top5 97.705078   BatchTime 0.413105   LR 0.000211
0.44861126
0.44853619
0.44848624
0.44843680
0.44844615
0.44845316
0.44837806
0.44842175
0.44842228
0.44839483
0.44822904
0.44826633
0.44826037
0.44822329
0.44826329
0.44823578
0.44824231
0.44818568
0.44812539
0.44803971
0.44796053
0.44785818
INFO - Training [15][   60/  196]   Loss 0.642392   Top1 78.372396   Top5 97.819010   BatchTime 0.394391   LR 0.000209
0.44781056
0.44774741
0.44765687
0.44755280
0.44755301
0.44755769
0.44751427
0.44752610
0.44755957
0.44751003
0.44751576
0.44749519
0.44749114
0.44747847
0.44739762
0.44736809
0.44743374
INFO - Training [15][   80/  196]   Loss 0.638941   Top1 78.554688   Top5 97.905273   BatchTime 0.386795   LR 0.000208
0.44747126
0.44748288
0.44749478
0.44740814
0.44740513
0.44725645
0.44715741
0.44716796
0.44716847
0.44709876
0.44714248
0.44706300
0.44700900
0.44698417
0.44688547
0.44684261
0.44686887
0.44687817
0.44689587
0.44691482
0.44692338
0.44690302
INFO - Training [15][  100/  196]   Loss 0.634134   Top1 78.742188   Top5 97.917969   BatchTime 0.381204   LR 0.000206
0.44690955
0.44690922
0.44686288
0.44691256
0.44687298
0.44694793
0.44699627
0.44697392
0.44695827
0.44701871
0.44702041
0.44702661
0.44705915
0.44708937
0.44710159
0.44705662
0.44696233
INFO - Training [15][  120/  196]   Loss 0.626210   Top1 79.016927   Top5 98.033854   BatchTime 0.377582   LR 0.000205
0.44688740
0.44687217
0.44686976
0.44687575
0.44692945
0.44687277
0.44684657
0.44685429
0.44687870
0.44687760
0.44688421
0.44692042
0.44694883
0.44697317
0.44693699
0.44688657
0.44689134
0.44687164
0.44688073
0.44687811
0.44674498
0.44677952
INFO - Training [15][  140/  196]   Loss 0.624847   Top1 78.976004   Top5 98.113839   BatchTime 0.375391   LR 0.000203
0.44686452
0.44682217
0.44678000
0.44662297
0.44644359
0.44653475
0.44664541
0.44658849
0.44650543
0.44631436
0.44591847
0.44570589
0.44565398
0.44568381
0.44573104
0.44578937
0.44580331
0.44583124
0.44583383
0.44584182
0.44584605
0.44584975
INFO - Training [15][  160/  196]   Loss 0.628058   Top1 78.874512   Top5 98.081055   BatchTime 0.373846   LR 0.000201
0.44579992
0.44579831
0.44578028
0.44575158
0.44575107
0.44579369
0.44579563
0.44575384
0.44566849
0.44562027
0.44558758
0.44556895
0.44563603
0.44565907
0.44562671
0.44564945
0.44566804
INFO - Training [15][  180/  196]   Loss 0.628817   Top1 78.789062   Top5 98.062066   BatchTime 0.369779   LR 0.000200
0.44564369
0.44564989
0.44558811
0.44555014
0.44550860
0.44550920
0.44549808
0.44549954
0.44542757
0.44538000
0.44534341
0.44532472
0.44524154
0.44514710
0.44508672
0.44499171
0.44495198
0.44492969
0.44487113
0.44481590
INFO - ==> Top1: 78.840    Top5: 98.040    Loss: 0.628
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [15][   20/   40]   Loss 0.422953   Top1 85.605469   Top5 99.238281   BatchTime 0.124430
INFO - Validation [15][   40/   40]   Loss 0.410037   Top1 85.950000   Top5 99.400000   BatchTime 0.094486
INFO - ==> Top1: 85.950    Top5: 99.400    Loss: 0.410
INFO - ==> Sparsity : 0.702
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 85.950   Top5: 99.400]
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 85.720   Top5: 99.310]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  16
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.5449)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.1267)
features.2.conv.3 tensor(0.3449)
features.2.conv.6 tensor(0.7112)
features.3.conv.0 tensor(0.0854)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1339)
features.4.conv.0 tensor(0.2508)
features.4.conv.3 tensor(0.3229)
features.4.conv.6 tensor(0.6785)
features.5.conv.0 tensor(0.4762)
features.5.conv.3 tensor(0.4294)
features.5.conv.6 tensor(0.6903)
features.6.conv.0 tensor(0.0605)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0881)
features.7.conv.0 tensor(0.6021)
features.7.conv.3 tensor(0.4554)
features.7.conv.6 tensor(0.7805)
features.8.conv.0 tensor(0.6801)
features.8.conv.3 tensor(0.5321)
features.8.conv.6 tensor(0.8351)
features.9.conv.0 tensor(0.6700)
features.9.conv.3 tensor(0.5599)
features.9.conv.6 tensor(0.8508)
features.10.conv.0 tensor(0.0846)
features.10.conv.3 tensor(0.1085)
features.10.conv.6 tensor(0.1529)
features.11.conv.0 tensor(0.8020)
features.11.conv.3 tensor(0.6480)
features.11.conv.6 tensor(0.9264)
features.12.conv.0 tensor(0.7850)
features.12.conv.3 tensor(0.6819)
features.12.conv.6 tensor(0.9249)
features.13.conv.0 tensor(0.4927)
features.13.conv.3 tensor(0.4913)
features.13.conv.6 tensor(0.6254)
features.14.conv.0 tensor(0.9226)
features.14.conv.3 tensor(0.8399)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9175)
features.15.conv.3 tensor(0.8880)
features.15.conv.6 tensor(0.9772)
features.16.conv.0 tensor(0.7388)
features.16.conv.3 tensor(0.8105)
features.16.conv.6 tensor(0.9151)
conv.0 tensor(0.2685)
tensor(1536711.) 2188896.0
0.44481209
0.44486007
0.44485998
0.44488764
0.44487095
0.44484112
0.44483578
0.44484571
0.44490436
0.44489425
0.44492367
0.44494629
0.44500005
0.44499817
0.44494256
0.44493377
0.44495654
0.44494674
INFO - Training [16][   20/  196]   Loss 0.614770   Top1 79.179688   Top5 97.812500   BatchTime 0.442403   LR 0.000197
0.44496045
0.44502679
0.44501778
0.44505814
0.44510594
0.44515881
0.44516468
0.44528994
0.44542655
0.44663179
0.44701704
0.44696274
0.44693583
0.44684449
0.44680920
0.44674140
0.44668531
0.44661763
0.44654936
0.44656068
0.44657564
INFO - Training [16][   40/  196]   Loss 0.617927   Top1 78.945312   Top5 97.900391   BatchTime 0.412853   LR 0.000195
0.44660270
0.44653565
0.44647613
0.44644669
0.44637865
0.44626948
0.44614640
0.44613174
0.44611746
0.44607633
0.44602972
0.44606182
0.44616574
0.44618225
0.44620702
0.44619840
0.44618607
INFO - Training [16][   60/  196]   Loss 0.612472   Top1 79.218750   Top5 97.962240   BatchTime 0.394173   LR 0.000194
0.44610372
0.44600621
0.44594026
0.44589677
0.44580594
0.44578555
0.44581014
0.44585946
0.44582081
0.44584993
0.44589159
0.44592863
0.44590372
0.44597951
0.44601840
0.44606856
0.44614190
0.44614786
0.44613609
0.44609720
0.44608572
0.44600710
INFO - Training [16][   80/  196]   Loss 0.614839   Top1 79.252930   Top5 98.037109   BatchTime 0.385638   LR 0.000192
0.44597620
0.44593430
0.44594070
0.44601765
0.44604865
0.44608229
0.44599774
0.44599617
0.44592723
0.44589031
0.44580224
0.44578737
0.44581622
0.44578910
0.44581324
0.44571245
0.44566673
INFO - Training [16][  100/  196]   Loss 0.609056   Top1 79.468750   Top5 98.046875   BatchTime 0.380862   LR 0.000190
0.44561049
0.44552985
0.44554523
0.44558510
0.44560331
0.44559628
0.44559997
0.44565490
0.44560426
0.44552699
0.44550958
0.44553936
0.44553623
0.44559747
0.44564036
0.44561753
0.44569215
0.44566765
0.44566542
0.44565305
0.44563964
0.44563887
INFO - Training [16][  120/  196]   Loss 0.606487   Top1 79.593099   Top5 98.170573   BatchTime 0.376502   LR 0.000188
0.44574773
0.44578874
0.44570759
0.44569054
0.44571412
0.44569829
0.44572958
0.44566923
0.44567481
0.44568697
0.44573882
0.44577983
0.44578499
0.44576094
0.44573858
0.44572243
0.44566593
0.44561750
0.44557673
0.44555661
0.44563559
0.44565564
INFO - Training [16][  140/  196]   Loss 0.605820   Top1 79.709821   Top5 98.183594   BatchTime 0.374747   LR 0.000187
0.44571495
0.44572040
0.44573954
0.44568613
0.44570950
0.44567707
0.44560996
0.44562033
0.44554493
0.44555357
0.44555092
0.44551077
0.44553000
0.44546640
0.44541121
0.44538575
0.44528243
0.44518438
INFO - Training [16][  160/  196]   Loss 0.609061   Top1 79.553223   Top5 98.149414   BatchTime 0.370690   LR 0.000185
0.44512692
0.44513202
0.44506449
0.44509646
0.44505671
0.44500408
0.44493172
0.44483927
0.44474608
0.44473478
0.44475621
0.44465294
0.44465187
0.44468230
0.44463876
0.44459885
0.44452891
0.44448182
0.44445401
0.44449076
0.44451079
0.44456872
INFO - Training [16][  180/  196]   Loss 0.608871   Top1 79.596354   Top5 98.085938   BatchTime 0.370377   LR 0.000183
0.44460204
0.44457018
0.44458076
0.44454795
0.44442686
0.44442055
0.44433430
0.44435400
0.44437212
0.44421166
0.44421500
0.44419187
0.44411150
0.44400543
0.44403175
0.44391033
INFO - ==> Top1: 79.646    Top5: 98.108    Loss: 0.606
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
0.44377229
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [16][   20/   40]   Loss 0.432890   Top1 85.312500   Top5 99.355469   BatchTime 0.127627
INFO - Validation [16][   40/   40]   Loss 0.419156   Top1 85.740000   Top5 99.500000   BatchTime 0.105707
INFO - ==> Top1: 85.740    Top5: 99.500    Loss: 0.419
INFO - ==> Sparsity : 0.705
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 85.950   Top5: 99.400]
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 85.740   Top5: 99.500]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  17
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.5508)
features.1.conv.0 tensor(0.0553)
features.1.conv.3 tensor(0.0741)
features.1.conv.6 tensor(0.0742)
features.2.conv.0 tensor(0.1730)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.7159)
features.3.conv.0 tensor(0.0773)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1335)
features.4.conv.0 tensor(0.2531)
features.4.conv.3 tensor(0.3252)
features.4.conv.6 tensor(0.6729)
features.5.conv.0 tensor(0.4810)
features.5.conv.3 tensor(0.4271)
features.5.conv.6 tensor(0.6935)
features.6.conv.0 tensor(0.0653)
features.6.conv.3 tensor(0.0503)
features.6.conv.6 tensor(0.0909)
features.7.conv.0 tensor(0.6147)
features.7.conv.3 tensor(0.4552)
features.7.conv.6 tensor(0.7852)
features.8.conv.0 tensor(0.6788)
features.8.conv.3 tensor(0.5284)
features.8.conv.6 tensor(0.8356)
features.9.conv.0 tensor(0.6776)
features.9.conv.3 tensor(0.5608)
features.9.conv.6 tensor(0.8528)
features.10.conv.0 tensor(0.0801)
features.10.conv.3 tensor(0.1102)
features.10.conv.6 tensor(0.1107)
features.11.conv.0 tensor(0.8041)
features.11.conv.3 tensor(0.6514)
features.11.conv.6 tensor(0.9262)
features.12.conv.0 tensor(0.7917)
features.12.conv.3 tensor(0.6821)
features.12.conv.6 tensor(0.9260)
features.13.conv.0 tensor(0.4744)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.6340)
features.14.conv.0 tensor(0.9233)
features.14.conv.3 tensor(0.8400)
features.14.conv.6 tensor(0.9785)
features.15.conv.0 tensor(0.9193)
features.15.conv.3 tensor(0.8868)
features.15.conv.6 tensor(0.9771)
features.16.conv.0 tensor(0.7405)
features.16.conv.3 tensor(0.8103)
features.16.conv.6 tensor(0.9165)
conv.0 tensor(0.2825)
tensor(1543098.) 2188896.0
0.44344547
0.44337615
0.44334966
0.44326800
0.44317600
0.44310510
0.44302824
0.44298571
0.44302750
0.44296581
0.44293702
0.44291094
0.44291538
0.44289890
0.44291416
0.44306520
0.44352493
0.44410729
INFO - Training [17][   20/  196]   Loss 0.607855   Top1 79.160156   Top5 98.105469   BatchTime 0.395847   LR 0.000180
0.44447649
0.44466338
0.44474018
0.44470716
0.44460928
0.44450080
0.44433859
0.44414315
0.44402099
0.44394752
0.44385999
0.44376722
0.44366074
0.44362932
0.44357798
0.44345212
0.44340223
0.44332901
0.44320065
0.44311056
0.44300902
INFO - Training [17][   40/  196]   Loss 0.599940   Top1 79.648438   Top5 98.007812   BatchTime 0.348421   LR 0.000178
0.44292104
0.44284892
0.44279775
0.44275358
0.44267750
0.44260731
0.44255856
0.44244754
0.44238472
0.44230217
0.44225532
0.44221476
0.44216856
0.44205850
0.44197395
0.44188753
0.44178540
0.44174284
0.44170165
0.44169700
0.44167101
INFO - Training [17][   60/  196]   Loss 0.606654   Top1 79.544271   Top5 97.890625   BatchTime 0.360225   LR 0.000176
0.44164923
0.44164243
0.44162390
0.44160077
0.44168255
0.44169408
0.44168970
0.44167498
0.44163901
0.44162902
0.44162738
0.44157493
0.44155958
0.44154167
0.44147009
0.44144502
INFO - Training [17][   80/  196]   Loss 0.606812   Top1 79.545898   Top5 97.958984   BatchTime 0.364358   LR 0.000175
0.44140270
0.44139808
0.44138998
0.44139433
0.44140974
0.44142988
0.44142246
0.44139990
0.44135806
0.44135663
0.44134676
0.44137871
0.44134942
0.44133219
0.44130403
0.44124770
0.44125181
0.44130066
0.44131017
0.44130537
0.44127935
0.44132891
INFO - Training [17][  100/  196]   Loss 0.597510   Top1 79.875000   Top5 98.089844   BatchTime 0.362422   LR 0.000173
0.44133019
0.44132650
0.44131139
0.44131181
0.44131318
0.44132510
0.44136277
0.44133613
0.44131610
0.44131115
0.44128457
0.44129270
0.44122952
0.44119135
0.44113901
0.44112328
0.44115832
0.44114345
0.44108310
0.44102219
0.44094440
0.44093141
INFO - Training [17][  120/  196]   Loss 0.591672   Top1 80.104167   Top5 98.180339   BatchTime 0.362577   LR 0.000171
0.44092122
0.44088212
0.44083753
0.44082740
0.44081610
0.44081497
0.44081515
0.44079158
0.44079831
0.44084224
0.44086125
0.44086990
0.44087175
0.44086635
0.44082636
0.44081321
0.44079193
INFO - Training [17][  140/  196]   Loss 0.591311   Top1 80.139509   Top5 98.178013   BatchTime 0.362737   LR 0.000169
0.44078520
0.44078362
0.44076923
0.44076374
0.44074708
0.44074854
0.44073847
0.44070220
0.44067597
0.44064251
0.44068760
0.44067541
0.44065335
0.44060490
0.44056362
0.44055381
0.44057578
0.44055584
0.44054133
0.44049314
0.44048408
INFO - Training [17][  160/  196]   Loss 0.591020   Top1 80.083008   Top5 98.188477   BatchTime 0.365497   LR 0.000167
0.44051912
0.44049543
0.44051340
0.44050530
0.44045883
0.44041532
0.44041422
0.44045970
0.44047686
0.44050995
0.44050068
0.44050720
0.44050103
0.44053116
0.44052050
0.44059923
0.44062054
0.44063464
0.44059938
0.44055629
0.44054997
0.44059247
INFO - Training [17][  180/  196]   Loss 0.591686   Top1 80.060764   Top5 98.181424   BatchTime 0.364381   LR 0.000165
0.44055003
0.44056204
0.44052508
0.44051042
0.44043553
0.44033408
0.44031647
0.44031212
0.44021595
0.44017464
0.44013003
0.44004542
0.44001690
0.43993849
0.43986252
0.43984118
INFO - ==> Top1: 80.126    Top5: 98.194    Loss: 0.590
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation [17][   20/   40]   Loss 0.416814   Top1 85.839844   Top5 99.472656   BatchTime 0.115178
INFO - Validation [17][   40/   40]   Loss 0.401427   Top1 86.480000   Top5 99.520000   BatchTime 0.087130
INFO - ==> Top1: 86.480    Top5: 99.520    Loss: 0.401
INFO - ==> Sparsity : 0.708
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 86.480   Top5: 99.520]
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 85.950   Top5: 99.400]
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.5527)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0842)
features.2.conv.0 tensor(0.1502)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.7173)
features.3.conv.0 tensor(0.0720)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1447)
features.4.conv.0 tensor(0.2617)
features.4.conv.3 tensor(0.3247)
features.4.conv.6 tensor(0.6751)
features.5.conv.0 tensor(0.4925)
features.5.conv.3 tensor(0.4271)
features.5.conv.6 tensor(0.6986)
features.6.conv.0 tensor(0.0578)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0903)
features.7.conv.0 tensor(0.6147)
features.7.conv.3 tensor(0.4531)
features.7.conv.6 tensor(0.7883)
features.8.conv.0 tensor(0.6854)
features.8.conv.3 tensor(0.5286)
features.8.conv.6 tensor(0.8357)
features.9.conv.0 tensor(0.6792)
features.9.conv.3 tensor(0.5582)
features.9.conv.6 tensor(0.8535)
features.10.conv.0 tensor(0.0862)
features.10.conv.3 tensor(0.1073)
features.10.conv.6 tensor(0.1410)
features.11.conv.0 tensor(0.8052)
features.11.conv.3 tensor(0.6516)
features.11.conv.6 tensor(0.9290)
features.12.conv.0 tensor(0.7921)
features.12.conv.3 tensor(0.6817)
features.12.conv.6 tensor(0.9251)
features.13.conv.0 tensor(0.4949)
features.13.conv.3 tensor(0.4904)
features.13.conv.6 tensor(0.6417)
features.14.conv.0 tensor(0.9255)
features.14.conv.3 tensor(0.8400)
features.14.conv.6 tensor(0.9780)
features.15.conv.0 tensor(0.9214)
features.15.conv.3 tensor(0.8870)
features.15.conv.6 tensor(0.9776)
features.16.conv.0 tensor(0.7430)
features.16.conv.3 tensor(0.8101)
features.16.conv.6 tensor(0.9174)
conv.0 tensor(0.2858)
tensor(1549456.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  18
INFO - Training: 50000 samples (256 per mini-batch)
0.43982464
0.43978083
0.43976954
0.43966070
0.43960246
0.43963155
0.43959659
0.43956718
0.43957788
0.43956837
0.43953672
0.43955287
0.43955383
0.43953478
0.43946403
0.43947741
0.43949604
0.43951386
0.43954059
INFO - Training [18][   20/  196]   Loss 0.599912   Top1 80.175781   Top5 97.656250   BatchTime 0.396091   LR 0.000162
0.43950889
0.43945435
0.43943322
0.43941537
0.43936503
0.43935755
0.43929717
0.43928725
0.43931144
0.43929628
0.43927339
0.43927962
0.43927416
0.43927845
0.43919626
0.43919665
0.43919227
0.43911108
0.43908852
0.43908620
INFO - Training [18][   40/  196]   Loss 0.589962   Top1 80.380859   Top5 97.939453   BatchTime 0.346699   LR 0.000160
0.43909109
0.43910280
0.43904442
0.43903643
0.43903887
0.43896702
0.43895242
0.43896276
0.43895122
0.43892601
0.43892288
0.43890005
0.43890345
0.43890604
0.43892923
0.43890965
0.43887186
0.43886846
INFO - Training [18][   60/  196]   Loss 0.588960   Top1 80.364583   Top5 98.046875   BatchTime 0.348954   LR 0.000158
0.43890196
0.43896258
0.43893325
0.43896401
0.43896726
0.43899590
0.43894145
0.43894234
0.43890971
0.43889478
0.43886814
0.43884638
0.43880850
0.43878770
0.43875954
0.43883207
0.43887690
0.43888739
0.43888691
0.43890387
0.43891552
INFO - Training [18][   80/  196]   Loss 0.583594   Top1 80.395508   Top5 98.168945   BatchTime 0.354758   LR 0.000156
0.43895319
0.43888503
0.43889436
0.43889877
0.43889180
0.43892446
0.43891528
0.43896320
0.43895295
0.43891981
0.43885654
0.43877387
0.43876615
0.43873566
0.43872172
0.43865582
0.43863606
0.43862852
0.43862250
INFO - Training [18][  100/  196]   Loss 0.577350   Top1 80.640625   Top5 98.226562   BatchTime 0.369953   LR 0.000154
0.43865344
0.43864262
0.43864936
0.43867892
0.43869060
0.43870172
0.43871608
0.43870011
0.43873397
0.43872347
0.43870521
0.43875840
0.43875405
0.43869159
0.43864116
0.43858844
0.43854493
0.43848392
0.43843254
0.43842101
0.43839106
0.43831825
0.43825835
INFO - Training [18][  120/  196]   Loss 0.573028   Top1 80.738932   Top5 98.281250   BatchTime 0.381713   LR 0.000152
0.43825519
0.43820745
0.43815905
0.43811542
0.43805856
0.43804130
0.43801683
0.43799144
0.43794498
0.43795815
0.43795732
0.43794805
0.43791121
0.43786991
0.43784258
0.43777120
0.43769860
0.43766287
INFO - Training [18][  140/  196]   Loss 0.569728   Top1 80.895647   Top5 98.337054   BatchTime 0.389195   LR 0.000150
0.43761870
0.43757838
0.43758842
0.43757749
0.43754652
0.43754113
0.43754411
0.43753847
0.43753296
0.43751633
0.43753120
0.43755734
0.43760610
0.43760899
0.43765539
0.43767127
0.43765378
0.43768105
INFO - Training [18][  160/  196]   Loss 0.572511   Top1 80.759277   Top5 98.342285   BatchTime 0.396190   LR 0.000148
0.43771902
0.43771958
0.43768996
0.43772861
0.43776479
0.43778580
0.43780842
0.43783525
0.43784302
0.43788058
0.43791804
0.43793374
0.43791759
0.43785194
0.43784168
0.43785131
0.43788564
0.43792751
0.43798071
0.43801829
0.43801707
0.43801889
INFO - Training [18][  180/  196]   Loss 0.571066   Top1 80.776910   Top5 98.344184   BatchTime 0.403102   LR 0.000146
0.43807104
0.43810654
0.43812403
0.43815836
0.43817475
0.43818834
0.43816876
0.43812111
0.43811199
0.43809891
0.43807846
0.43806076
0.43804017
0.43802407
INFO - ==> Top1: 80.812    Top5: 98.352    Loss: 0.570
0.43801948
0.43796712
0.43794632
0.43796080
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [18][   20/   40]   Loss 0.416488   Top1 85.878906   Top5 99.394531   BatchTime 0.121476
INFO - Validation [18][   40/   40]   Loss 0.403952   Top1 86.170000   Top5 99.530000   BatchTime 0.086920
INFO - ==> Top1: 86.170    Top5: 99.530    Loss: 0.404
INFO - ==> Sparsity : 0.711
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 86.480   Top5: 99.520]
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 86.170   Top5: 99.530]
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 86.010   Top5: 99.480]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  19
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.5508)
features.1.conv.0 tensor(0.0475)
features.1.conv.3 tensor(0.0775)
features.1.conv.6 tensor(0.0803)
features.2.conv.0 tensor(0.1751)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.7144)
features.3.conv.0 tensor(0.0689)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1482)
features.4.conv.0 tensor(0.2829)
features.4.conv.3 tensor(0.3194)
features.4.conv.6 tensor(0.6766)
features.5.conv.0 tensor(0.4966)
features.5.conv.3 tensor(0.4271)
features.5.conv.6 tensor(0.6991)
features.6.conv.0 tensor(0.0594)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0888)
features.7.conv.0 tensor(0.6176)
features.7.conv.3 tensor(0.4549)
features.7.conv.6 tensor(0.7872)
features.8.conv.0 tensor(0.6964)
features.8.conv.3 tensor(0.5266)
features.8.conv.6 tensor(0.8378)
features.9.conv.0 tensor(0.6845)
features.9.conv.3 tensor(0.5616)
features.9.conv.6 tensor(0.8548)
features.10.conv.0 tensor(0.0800)
features.10.conv.3 tensor(0.1053)
features.10.conv.6 tensor(0.1547)
features.11.conv.0 tensor(0.8123)
features.11.conv.3 tensor(0.6530)
features.11.conv.6 tensor(0.9298)
features.12.conv.0 tensor(0.7943)
features.12.conv.3 tensor(0.6796)
features.12.conv.6 tensor(0.9247)
features.13.conv.0 tensor(0.4785)
features.13.conv.3 tensor(0.4902)
features.13.conv.6 tensor(0.6476)
features.14.conv.0 tensor(0.9267)
features.14.conv.3 tensor(0.8399)
features.14.conv.6 tensor(0.9776)
features.15.conv.0 tensor(0.9221)
features.15.conv.3 tensor(0.8861)
features.15.conv.6 tensor(0.9776)
features.16.conv.0 tensor(0.7466)
features.16.conv.3 tensor(0.8101)
features.16.conv.6 tensor(0.9188)
conv.0 tensor(0.2969)
tensor(1556457.) 2188896.0
0.43800625
0.43804124
0.43804017
0.43801403
0.43802300
0.43798944
0.43797514
0.43793795
0.43792355
0.43794334
0.43798313
0.43799120
0.43796864
0.43802834
0.43805161
0.43802273
0.43803877
0.43798083
0.43798256
0.43796185
INFO - Training [19][   20/  196]   Loss 0.578824   Top1 80.253906   Top5 97.792969   BatchTime 0.517445   LR 0.000143
0.43796813
0.43797749
0.43800867
0.43796900
0.43794414
0.43791202
0.43788305
0.43786323
0.43784750
0.43784374
0.43781951
0.43776014
0.43775716
0.43780014
0.43776637
0.43776223
0.43770340
0.43764508
INFO - Training [19][   40/  196]   Loss 0.577048   Top1 80.439453   Top5 97.861328   BatchTime 0.482973   LR 0.000141
0.43759388
0.43755728
0.43754292
0.43752247
0.43750581
0.43753484
0.43756592
0.43759820
0.43759224
0.43759027
0.43757463
0.43750370
0.43748376
0.43745938
0.43744996
0.43740201
0.43733898
0.43735716
0.43732452
0.43728867
INFO - Training [19][   60/  196]   Loss 0.566187   Top1 80.859375   Top5 98.020833   BatchTime 0.451811   LR 0.000139
0.43731341
0.43724892
0.43719152
0.43713939
0.43714657
0.43712467
0.43712965
0.43711531
0.43712518
0.43714753
0.43709883
0.43710586
0.43704993
0.43701795
0.43696696
0.43694434
0.43691137
0.43693227
0.43687758
0.43685687
0.43681487
INFO - Training [19][   80/  196]   Loss 0.568670   Top1 80.659180   Top5 98.095703   BatchTime 0.437408   LR 0.000137
0.43679059
0.43677592
0.43676701
0.43679425
0.43682322
0.43683210
0.43686485
0.43695894
0.43707541
0.43722525
0.43740240
0.43772244
0.43802530
0.43828201
0.43843710
0.43830135
0.43844879
0.43842679
INFO - Training [19][  100/  196]   Loss 0.566364   Top1 80.824219   Top5 98.195312   BatchTime 0.436257   LR 0.000135
0.43806639
0.43782046
0.43780488
0.43759543
0.43745148
0.43736312
0.43727273
0.43717936
0.43711427
0.43705246
0.43698412
0.43699676
0.43698922
0.43697694
0.43694919
0.43692175
0.43690369
0.43689856
0.43693006
0.43692884
INFO - Training [19][  120/  196]   Loss 0.562264   Top1 80.947266   Top5 98.287760   BatchTime 0.430092   LR 0.000133
0.43690473
0.43684831
0.43684655
0.43683937
0.43680686
0.43677577
0.43673241
0.43668965
0.43670610
0.43675742
0.43674868
0.43675882
0.43675438
0.43667924
0.43665504
0.43663269
0.43663338
0.43660250
0.43666023
0.43670464
0.43676040
0.43676832
0.43679401
INFO - Training [19][  140/  196]   Loss 0.559919   Top1 81.102121   Top5 98.370536   BatchTime 0.418460   LR 0.000131
0.43682316
0.43687132
0.43693256
0.43693560
0.43695381
0.43697560
0.43695617
0.43695292
0.43698674
0.43699983
0.43699557
0.43701002
0.43702546
0.43702328
0.43699658
0.43697408
0.43690071
0.43685427
INFO - Training [19][  160/  196]   Loss 0.561904   Top1 81.062012   Top5 98.352051   BatchTime 0.420638   LR 0.000129
0.43682507
0.43680328
0.43680730
0.43681780
0.43679532
0.43679562
0.43683854
0.43679386
0.43680453
0.43682849
0.43680981
0.43678993
0.43674648
0.43670800
0.43666881
0.43665293
0.43665379
0.43660578
0.43656647
0.43652558
0.43653160
0.43655863
INFO - Training [19][  180/  196]   Loss 0.560854   Top1 81.124132   Top5 98.342014   BatchTime 0.424862   LR 0.000127
0.43659189
0.43660063
0.43659517
0.43656826
0.43652263
0.43650359
0.43647921
0.43644837
0.43644053
0.43641040
0.43641815
0.43641159
0.43637541
INFO - ==> Top1: 81.188    Top5: 98.324    Loss: 0.560
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.43632743
0.43632558
0.43633869
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [19][   20/   40]   Loss 0.393075   Top1 86.640625   Top5 99.453125   BatchTime 0.121641
INFO - Validation [19][   40/   40]   Loss 0.378762   Top1 87.080000   Top5 99.590000   BatchTime 0.087407
INFO - ==> Top1: 87.080    Top5: 99.590    Loss: 0.379
INFO - ==> Sparsity : 0.715
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 87.080   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 86.480   Top5: 99.520]
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 86.170   Top5: 99.530]
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.5547)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0820)
features.2.conv.0 tensor(0.1664)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.7208)
features.3.conv.0 tensor(0.0732)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1576)
features.4.conv.0 tensor(0.2770)
features.4.conv.3 tensor(0.3206)
features.4.conv.6 tensor(0.6797)
features.5.conv.0 tensor(0.5073)
features.5.conv.3 tensor(0.4248)
features.5.conv.6 tensor(0.6989)
features.6.conv.0 tensor(0.0579)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0897)
features.7.conv.0 tensor(0.6270)
features.7.conv.3 tensor(0.4572)
features.7.conv.6 tensor(0.7879)
features.8.conv.0 tensor(0.6918)
features.8.conv.3 tensor(0.5318)
features.8.conv.6 tensor(0.8384)
features.9.conv.0 tensor(0.6803)
features.9.conv.3 tensor(0.5602)
features.9.conv.6 tensor(0.8545)
features.10.conv.0 tensor(0.0824)
features.10.conv.3 tensor(0.1047)
features.10.conv.6 tensor(0.1590)
features.11.conv.0 tensor(0.8140)
features.11.conv.3 tensor(0.6520)
features.11.conv.6 tensor(0.9319)
features.12.conv.0 tensor(0.7951)
features.12.conv.3 tensor(0.6802)
features.12.conv.6 tensor(0.9267)
features.13.conv.0 tensor(0.5066)
features.13.conv.3 tensor(0.4888)
features.13.conv.6 tensor(0.6558)
features.14.conv.0 tensor(0.9274)
features.14.conv.3 tensor(0.8397)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9244)
features.15.conv.3 tensor(0.8854)
features.15.conv.6 tensor(0.9780)
features.16.conv.0 tensor(0.7463)
features.16.conv.3 tensor(0.8097)
features.16.conv.6 tensor(0.9204)
conv.0 tensor(0.3101)
tensor(1565922.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  20
INFO - Training: 50000 samples (256 per mini-batch)
0.43627876
0.43626341
0.43625048
0.43625674
0.43621120
0.43620127
0.43614528
0.43611020
0.43610817
0.43606633
0.43602064
0.43597490
0.43592772
0.43589827
0.43586630
0.43584213
0.43581942
0.43581167
INFO - Training [20][   20/  196]   Loss 0.569899   Top1 80.839844   Top5 98.105469   BatchTime 0.572498   LR 0.000123
0.43578875
0.43575448
0.43567085
0.43560326
0.43557918
0.43552488
0.43545178
0.43541694
0.43536267
0.43529344
0.43520501
0.43518645
0.43513915
0.43508172
0.43503496
0.43497753
0.43491134
0.43483141
0.43473753
0.43466413
0.43463522
INFO - Training [20][   40/  196]   Loss 0.576121   Top1 80.566406   Top5 98.164062   BatchTime 0.518308   LR 0.000121
0.43460935
0.43458170
0.43458372
0.43455169
0.43454906
0.43452364
0.43447769
0.43444738
0.43443617
0.43435153
0.43427065
0.43427137
0.43427837
0.43430778
0.43428499
0.43430561
0.43431911
0.43434387
0.43434855
0.43435967
0.43437666
INFO - Training [20][   60/  196]   Loss 0.564204   Top1 80.970052   Top5 98.255208   BatchTime 0.477733   LR 0.000119
0.43438962
0.43440366
0.43439385
0.43437991
0.43438968
0.43437365
0.43436244
0.43435097
0.43435723
0.43436450
0.43435472
0.43432751
0.43433246
0.43432224
0.43425864
0.43417504
INFO - Training [20][   80/  196]   Loss 0.562045   Top1 81.166992   Top5 98.300781   BatchTime 0.450260   LR 0.000117
0.43412763
0.43410036
0.43408495
0.43401343
0.43396318
0.43392563
0.43387726
0.43382099
0.43374455
0.43367037
0.43361840
0.43355912
0.43352431
0.43348870
0.43343762
0.43341085
0.43336374
0.43335798
0.43333486
0.43335506
0.43332627
0.43337572
INFO - Training [20][  100/  196]   Loss 0.552603   Top1 81.589844   Top5 98.328125   BatchTime 0.452035   LR 0.000115
0.43341339
0.43344364
0.43346587
0.43345183
0.43342844
0.43338361
0.43331569
0.43332034
0.43332353
0.43335965
0.43338439
0.43338349
0.43338186
0.43333697
0.43329161
0.43329629
0.43328705
0.43328580
0.43326691
0.43324789
INFO - Training [20][  120/  196]   Loss 0.545636   Top1 81.861979   Top5 98.421224   BatchTime 0.444382   LR 0.000113
0.43326131
0.43324345
0.43321601
0.43320870
0.43324104
0.43326882
0.43332338
0.43333295
0.43337291
0.43336970
0.43339550
0.43337968
0.43340626
0.43341753
0.43337041
0.43334126
0.43337515
0.43339348
0.43341306
0.43346226
0.43350676
INFO - Training [20][  140/  196]   Loss 0.545182   Top1 81.891741   Top5 98.473772   BatchTime 0.433956   LR 0.000111
0.43354565
0.43357962
0.43358630
0.43358395
0.43362331
0.43363324
0.43363851
0.43363482
0.43364748
0.43363932
0.43364385
0.43370941
0.43375570
0.43375799
0.43373278
0.43371284
0.43365556
0.43370715
INFO - Training [20][  160/  196]   Loss 0.548003   Top1 81.760254   Top5 98.457031   BatchTime 0.435867   LR 0.000109
0.43370983
0.43364748
0.43365696
0.43368989
0.43373832
0.43370488
0.43370456
0.43369713
0.43366766
0.43364015
0.43363023
0.43358967
0.43356237
0.43355450
0.43350899
0.43352476
0.43350884
0.43351743
0.43351027
0.43347296
0.43342352
0.43339252
INFO - Training [20][  180/  196]   Loss 0.547734   Top1 81.770833   Top5 98.433160   BatchTime 0.437998   LR 0.000107
0.43335560
0.43332425
0.43332431
0.43332848
0.43328345
0.43325201
0.43322197
0.43320015
0.43316129
0.43312791
0.43307218
0.43302754
0.43296686
0.43293336
0.43294638
0.43289444
0.43282166
INFO - ==> Top1: 81.816    Top5: 98.442    Loss: 0.548
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [20][   20/   40]   Loss 0.398404   Top1 86.484375   Top5 99.531250   BatchTime 0.130521
INFO - Validation [20][   40/   40]   Loss 0.386927   Top1 86.960000   Top5 99.590000   BatchTime 0.092537
INFO - ==> Top1: 86.960    Top5: 99.590    Loss: 0.387
INFO - ==> Sparsity : 0.719
INFO - Scoreboard best 1 ==> Epoch [19][Top1: 87.080   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 86.960   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 86.480   Top5: 99.520]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  21
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.5566)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0846)
features.2.conv.0 tensor(0.1759)
features.2.conv.3 tensor(0.3511)
features.2.conv.6 tensor(0.7219)
features.3.conv.0 tensor(0.0674)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1552)
features.4.conv.0 tensor(0.2796)
features.4.conv.3 tensor(0.3223)
features.4.conv.6 tensor(0.6813)
features.5.conv.0 tensor(0.5132)
features.5.conv.3 tensor(0.4288)
features.5.conv.6 tensor(0.7013)
features.6.conv.0 tensor(0.0558)
features.6.conv.3 tensor(0.0498)
features.6.conv.6 tensor(0.0887)
features.7.conv.0 tensor(0.6223)
features.7.conv.3 tensor(0.4583)
features.7.conv.6 tensor(0.7904)
features.8.conv.0 tensor(0.6920)
features.8.conv.3 tensor(0.5307)
features.8.conv.6 tensor(0.8378)
features.9.conv.0 tensor(0.6850)
features.9.conv.3 tensor(0.5596)
features.9.conv.6 tensor(0.8555)
features.10.conv.0 tensor(0.0857)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.1633)
features.11.conv.0 tensor(0.8233)
features.11.conv.3 tensor(0.6522)
features.11.conv.6 tensor(0.9304)
features.12.conv.0 tensor(0.8072)
features.12.conv.3 tensor(0.6809)
features.12.conv.6 tensor(0.9285)
features.13.conv.0 tensor(0.4987)
features.13.conv.3 tensor(0.4890)
features.13.conv.6 tensor(0.6651)
features.14.conv.0 tensor(0.9281)
features.14.conv.3 tensor(0.8403)
features.14.conv.6 tensor(0.9777)
features.15.conv.0 tensor(0.9264)
features.15.conv.3 tensor(0.8858)
features.15.conv.6 tensor(0.9785)
features.16.conv.0 tensor(0.7502)
features.16.conv.3 tensor(0.8100)
features.16.conv.6 tensor(0.9217)
conv.0 tensor(0.3223)
tensor(1574409.) 2188896.0
0.43276888
0.43269393
0.43264157
0.43253008
0.43245825
0.43238276
0.43234479
0.43232340
0.43228790
0.43223405
0.43217450
0.43213314
0.43210119
0.43207252
0.43208477
0.43204230
0.43201250
INFO - Training [21][   20/  196]   Loss 0.556260   Top1 81.757812   Top5 98.105469   BatchTime 0.533967   LR 0.000104
0.43198729
0.43197727
0.43199185
0.43197492
0.43200397
0.43198666
0.43200985
0.43202406
0.43206927
0.43206301
0.43201801
0.43204650
0.43203887
0.43198341
0.43197528
0.43195158
0.43197289
0.43200269
0.43201229
0.43201411
0.43198007
0.43195438
0.43192106
INFO - Training [21][   40/  196]   Loss 0.561276   Top1 81.542969   Top5 98.046875   BatchTime 0.485301   LR 0.000102
0.43191472
0.43188500
0.43190086
0.43192235
0.43193781
0.43193364
0.43194076
0.43196964
0.43198115
0.43201378
0.43200275
0.43197313
0.43196321
0.43196705
0.43194082
0.43188176
0.43183848
INFO - Training [21][   60/  196]   Loss 0.552573   Top1 81.458333   Top5 98.203125   BatchTime 0.474405   LR 0.000100
0.43182459
0.43181637
0.43181387
0.43181548
0.43181255
0.43184942
0.43183765
0.43182167
0.43179923
0.43181375
0.43180344
0.43183333
0.43184236
0.43185055
0.43188331
0.43187049
0.43187407
0.43187612
0.43186244
0.43186167
INFO - Training [21][   80/  196]   Loss 0.547918   Top1 81.708984   Top5 98.281250   BatchTime 0.455809   LR 0.000098
0.43189931
0.43192449
0.43192315
0.43194064
0.43193763
0.43193558
0.43193254
0.43195963
0.43196824
0.43198752
0.43194699
0.43194512
0.43193036
0.43191132
0.43189287
0.43186453
0.43182638
0.43181992
0.43176678
0.43171054
0.43167081
INFO - Training [21][  100/  196]   Loss 0.543459   Top1 81.949219   Top5 98.308594   BatchTime 0.443436   LR 0.000096
0.43168774
0.43167332
0.43163082
0.43158612
0.43156034
0.43150699
0.43147400
0.43144605
0.43142965
0.43136290
0.43137309
0.43137637
0.43137789
0.43140802
0.43144035
0.43146753
0.43150494
0.43153715
0.43155155
0.43157518
0.43160206
0.43162751
INFO - Training [21][  120/  196]   Loss 0.535356   Top1 82.194010   Top5 98.369141   BatchTime 0.444847   LR 0.000094
0.43167061
0.43173549
0.43179265
0.43181542
0.43185362
0.43189028
0.43187672
0.43187001
0.43182105
0.43178609
0.43175247
0.43176010
0.43174118
0.43171066
0.43167767
0.43162432
0.43158486
0.43152931
0.43147725
0.43149507
0.43148473
INFO - Training [21][  140/  196]   Loss 0.532808   Top1 82.251674   Top5 98.426339   BatchTime 0.437451   LR 0.000092
0.43150455
0.43151072
0.43147293
0.43147734
0.43148014
0.43146685
0.43149391
0.43152243
0.43151587
0.43151841
0.43152678
0.43155116
0.43159974
0.43161240
0.43164042
0.43166429
0.43167099
0.43165329
0.43162465
INFO - Training [21][  160/  196]   Loss 0.533962   Top1 82.194824   Top5 98.432617   BatchTime 0.434872   LR 0.000090
0.43162987
0.43162778
0.43165198
0.43159974
0.43152645
0.43149281
0.43142632
0.43138868
0.43135145
0.43132770
0.43129426
0.43126550
0.43127543
0.43124449
0.43121648
0.43118694
0.43116829
0.43119219
0.43123329
0.43122745
INFO - Training [21][  180/  196]   Loss 0.533947   Top1 82.241753   Top5 98.398438   BatchTime 0.440347   LR 0.000088
0.43120766
0.43116152
0.43116948
0.43119204
0.43121150
0.43123513
0.43124273
0.43126532
0.43124720
0.43121374
0.43121651
0.43118829
0.43113035
INFO - ==> Top1: 82.286    Top5: 98.406    Loss: 0.532
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.43109533
0.43105629
0.43105233
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [21][   20/   40]   Loss 0.381865   Top1 86.816406   Top5 99.570312   BatchTime 0.117715
INFO - Validation [21][   40/   40]   Loss 0.372228   Top1 87.380000   Top5 99.590000   BatchTime 0.082772
INFO - ==> Top1: 87.380    Top5: 99.590    Loss: 0.372
INFO - ==> Sparsity : 0.722
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 87.380   Top5: 99.590]
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 87.080   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 86.960   Top5: 99.590]
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.5586)
features.1.conv.0 tensor(0.0508)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0838)
features.2.conv.0 tensor(0.1713)
features.2.conv.3 tensor(0.3519)
features.2.conv.6 tensor(0.7222)
features.3.conv.0 tensor(0.0680)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1565)
features.4.conv.0 tensor(0.2886)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.6787)
features.5.conv.0 tensor(0.5200)
features.5.conv.3 tensor(0.4288)
features.5.conv.6 tensor(0.7025)
features.6.conv.0 tensor(0.0592)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0871)
features.7.conv.0 tensor(0.6429)
features.7.conv.3 tensor(0.4549)
features.7.conv.6 tensor(0.7926)
features.8.conv.0 tensor(0.6966)
features.8.conv.3 tensor(0.5284)
features.8.conv.6 tensor(0.8378)
features.9.conv.0 tensor(0.6918)
features.9.conv.3 tensor(0.5599)
features.9.conv.6 tensor(0.8551)
features.10.conv.0 tensor(0.0824)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.1706)
features.11.conv.0 tensor(0.8247)
features.11.conv.3 tensor(0.6528)
features.11.conv.6 tensor(0.9303)
features.12.conv.0 tensor(0.8082)
features.12.conv.3 tensor(0.6806)
features.12.conv.6 tensor(0.9285)
features.13.conv.0 tensor(0.4951)
features.13.conv.3 tensor(0.4888)
features.13.conv.6 tensor(0.6742)
features.14.conv.0 tensor(0.9288)
features.14.conv.3 tensor(0.8402)
features.14.conv.6 tensor(0.9779)
features.15.conv.0 tensor(0.9283)
features.15.conv.3 tensor(0.8859)
features.15.conv.6 tensor(0.9787)
features.16.conv.0 tensor(0.7521)
features.16.conv.3 tensor(0.8102)
features.16.conv.6 tensor(0.9235)
conv.0 tensor(0.3287)
tensor(1580170.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  22
INFO - Training: 50000 samples (256 per mini-batch)
0.43105739
0.43106535
0.43107408
0.43108317
0.43109715
0.43109310
0.43106803
0.43106067
0.43106535
0.43107858
0.43111542
0.43112767
0.43114179
0.43112341
0.43112075
0.43111783
0.43110207
0.43107870
0.43106946
INFO - Training [22][   20/  196]   Loss 0.523215   Top1 81.953125   Top5 97.968750   BatchTime 0.513065   LR 0.000085
0.43108973
0.43106109
0.43104616
0.43107045
0.43107328
0.43106133
0.43102750
0.43100139
0.43097299
0.43095765
0.43095046
0.43094653
0.43092817
0.43094182
0.43089986
0.43084109
0.43080598
0.43074748
INFO - Training [22][   40/  196]   Loss 0.535942   Top1 81.904297   Top5 98.046875   BatchTime 0.483883   LR 0.000083
0.43072778
0.43071619
0.43070549
0.43069911
0.43067387
0.43066534
0.43065861
0.43065262
0.43063578
0.43064746
0.43064544
0.43063191
0.43060848
0.43059322
0.43056640
0.43055388
0.43052641
0.43049967
0.43047577
0.43045855
0.43041435
0.43037045
INFO - Training [22][   60/  196]   Loss 0.532216   Top1 82.031250   Top5 98.196615   BatchTime 0.479520   LR 0.000081
0.43033350
0.43036273
0.43036774
0.43033865
0.43032393
0.43027249
0.43023649
0.43023622
0.43021208
0.43020606
0.43020228
0.43021652
0.43017837
0.43014824
0.43015060
0.43017438
0.43018150
0.43021411
0.43020323
INFO - Training [22][   80/  196]   Loss 0.527647   Top1 82.290039   Top5 98.315430   BatchTime 0.462857   LR 0.000079
0.43021375
0.43022454
0.43025017
0.43029481
0.43033397
0.43035364
0.43034261
0.43031844
0.43028584
0.43024731
0.43021220
0.43018448
0.43018094
0.43019524
0.43017617
0.43014550
0.43009582
0.43007055
INFO - Training [22][  100/  196]   Loss 0.520999   Top1 82.585938   Top5 98.378906   BatchTime 0.456611   LR 0.000077
0.43003455
0.42999920
0.43000311
0.43000191
0.43002298
0.43003237
0.43000582
0.42998141
0.42995721
0.42991236
0.42990452
0.42986825
0.42988232
0.42991281
0.42993253
0.42990786
0.42987305
0.42985010
0.42983183
0.42982501
0.42983910
INFO - Training [22][  120/  196]   Loss 0.517817   Top1 82.669271   Top5 98.453776   BatchTime 0.459566   LR 0.000075
0.42980799
0.42979497
0.42977434
0.42970303
0.42969722
0.42969254
0.42969903
0.42969900
0.42968452
0.42968547
0.42965031
0.42963052
0.42960829
0.42959753
0.42958090
0.42955235
0.42952466
0.42948341
0.42942405
INFO - Training [22][  140/  196]   Loss 0.518621   Top1 82.636719   Top5 98.518415   BatchTime 0.454683   LR 0.000073
0.42938811
0.42932639
0.42925134
0.42919910
0.42911625
0.42904642
0.42899325
0.42891279
0.42883074
0.42879543
0.42874420
0.42869180
0.42866579
0.42862114
0.42863476
0.42862937
0.42861414
0.42857927
0.42855185
0.42854804
0.42852482
INFO - Training [22][  160/  196]   Loss 0.522675   Top1 82.468262   Top5 98.500977   BatchTime 0.445640   LR 0.000072
0.42849642
0.42852429
0.42857236
0.42862299
0.42870596
0.42870554
0.42871037
0.42872450
0.42874697
0.42874369
0.42874083
0.42872983
0.42871511
0.42872125
0.42871648
0.42874068
0.42876065
0.42876694
0.42877612
0.42875943
0.42876473
0.42876634
INFO - Training [22][  180/  196]   Loss 0.523041   Top1 82.456597   Top5 98.485243   BatchTime 0.447594   LR 0.000070
0.42876306
0.42874333
0.42873541
0.42870048
0.42866591
0.42861727
0.42859843
0.42858636
0.42855844
0.42853010
0.42851192
0.42848974
0.42850840
0.42853457
0.42852896
0.42856047
0.42855057
INFO - ==> Top1: 82.516    Top5: 98.496    Loss: 0.521
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [22][   20/   40]   Loss 0.361818   Top1 87.910156   Top5 99.589844   BatchTime 0.122759
INFO - Validation [22][   40/   40]   Loss 0.354214   Top1 88.100000   Top5 99.650000   BatchTime 0.087165
features.0.conv.0 tensor(0.2917)
features.0.conv.3 tensor(0.5586)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0846)
features.2.conv.0 tensor(0.1933)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.7234)
features.3.conv.0 tensor(0.0686)
features.3.conv.3 tensor(0.0810)
features.3.conv.6 tensor(0.1608)
features.4.conv.0 tensor(0.3182)
features.4.conv.3 tensor(0.3189)
features.4.conv.6 tensor(0.6789)
features.5.conv.0 tensor(0.5329)
features.5.conv.3 tensor(0.4329)
features.5.conv.6 tensor(0.7023)
features.6.conv.0 tensor(0.0568)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0887)
features.7.conv.0 tensor(0.6463)
features.7.conv.3 tensor(0.4534)
features.7.conv.6 tensor(0.7920)
features.8.conv.0 tensor(0.7010)
features.8.conv.3 tensor(0.5281)
features.8.conv.6 tensor(0.8394)
features.9.conv.0 tensor(0.7006)
features.9.conv.3 tensor(0.5587)
features.9.conv.6 tensor(0.8560)
features.10.conv.0 tensor(0.0855)
features.10.conv.3 tensor(0.1019)
features.10.conv.6 tensor(0.1778)
features.11.conv.0 tensor(0.8262)
features.11.conv.3 tensor(0.6530)
features.11.conv.6 tensor(0.9301)
features.12.conv.0 tensor(0.8068)
features.12.conv.3 tensor(0.6806)
features.12.conv.6 tensor(0.9297)
features.13.conv.0 tensor(0.4958)
features.13.conv.3 tensor(0.4900)
features.13.conv.6 tensor(0.6832)
features.14.conv.0 tensor(0.9299)
features.14.conv.3 tensor(0.8410)
features.14.conv.6 tensor(0.9782)
features.15.conv.0 tensor(0.9296)
features.15.conv.3 tensor(0.8852)
features.15.conv.6 tensor(0.9786)
features.16.conv.0 tensor(0.7519)
features.16.conv.3 tensor(0.8106)
features.16.conv.6 tensor(0.9243)
conv.0 tensor(0.3360)
tensor(1585863.) 2188896.0
INFO - ==> Top1: 88.100    Top5: 99.650    Loss: 0.354
INFO - ==> Sparsity : 0.725
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 88.100   Top5: 99.650]
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 87.380   Top5: 99.590]
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 87.080   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  23
INFO - Training: 50000 samples (256 per mini-batch)
0.42854038
0.42855394
0.42853704
0.42853588
0.42851797
0.42848280
0.42847222
0.42846236
0.42843455
0.42843735
0.42842263
0.42840603
0.42837417
0.42832303
0.42828086
0.42821273
INFO - Training [23][   20/  196]   Loss 0.533622   Top1 81.660156   Top5 98.125000   BatchTime 0.529850   LR 0.000067
0.42818528
0.42816889
0.42815018
0.42811510
0.42809036
0.42807129
0.42801875
0.42799944
0.42798975
0.42796308
0.42794505
0.42792681
0.42790899
0.42788407
0.42785424
0.42781582
0.42778167
0.42776507
0.42776355
0.42774704
0.42775983
0.42778620
INFO - Training [23][   40/  196]   Loss 0.536372   Top1 81.943359   Top5 98.193359   BatchTime 0.493690   LR 0.000065
0.42778796
0.42778721
0.42777562
0.42776898
0.42775086
0.42772892
0.42774078
0.42770296
0.42771855
0.42770302
0.42769253
0.42769122
0.42770264
0.42769858
0.42768484
0.42766830
0.42762941
0.42760429
INFO - Training [23][   60/  196]   Loss 0.528963   Top1 82.272135   Top5 98.352865   BatchTime 0.472387   LR 0.000063
0.42759770
0.42759594
0.42757982
0.42757046
0.42756078
0.42753041
0.42753091
0.42753160
0.42754510
0.42751905
0.42756870
0.42759064
0.42759258
0.42759788
0.42758903
0.42758405
0.42760351
0.42760742
0.42759261
0.42758125
INFO - Training [23][   80/  196]   Loss 0.529326   Top1 82.221680   Top5 98.500977   BatchTime 0.459202   LR 0.000061
0.42760250
0.42759869
0.42756799
0.42757198
0.42758107
0.42758968
0.42759120
0.42756519
0.42755640
0.42756081
0.42752728
0.42751911
0.42752212
0.42752576
0.42753562
0.42753488
0.42750761
0.42748362
0.42745748
0.42743525
0.42739704
0.42738357
INFO - Training [23][  100/  196]   Loss 0.518562   Top1 82.589844   Top5 98.488281   BatchTime 0.459384   LR 0.000060
0.42737240
0.42732695
0.42729273
0.42725024
0.42722204
0.42717925
0.42712659
0.42711702
0.42708474
0.42707539
0.42707083
0.42706507
0.42704773
0.42700621
0.42698416
0.42695194
0.42692834
0.42693064
0.42692596
0.42689756
0.42687121
INFO - Training [23][  120/  196]   Loss 0.510582   Top1 82.858073   Top5 98.583984   BatchTime 0.463500   LR 0.000058
0.42683232
0.42678666
0.42675686
0.42673555
0.42668006
0.42666301
0.42664954
0.42663008
0.42659009
0.42656463
0.42654893
0.42655230
0.42656213
0.42655018
0.42653519
0.42654532
0.42651972
0.42650649
INFO - Training [23][  140/  196]   Loss 0.508509   Top1 82.952009   Top5 98.627232   BatchTime 0.459120   LR 0.000056
0.42648461
0.42649150
0.42648411
0.42647734
0.42646313
0.42644450
0.42645124
0.42645118
0.42645290
0.42645600
0.42646885
0.42647037
0.42647576
0.42650387
0.42651141
0.42653114
0.42652181
0.42649880
0.42646757
0.42644629
INFO - Training [23][  160/  196]   Loss 0.511283   Top1 82.868652   Top5 98.593750   BatchTime 0.451948   LR 0.000055
0.42642072
0.42639351
0.42638186
0.42636651
0.42635918
0.42635849
0.42633781
0.42631540
0.42631724
0.42631155
0.42630219
0.42631489
0.42633024
0.42631119
0.42631009
0.42628565
0.42625973
0.42625183
0.42621377
0.42619908
0.42616582
0.42611596
INFO - Training [23][  180/  196]   Loss 0.509501   Top1 82.947049   Top5 98.559028   BatchTime 0.452223   LR 0.000053
0.42608091
0.42603892
0.42599607
0.42594513
0.42590091
0.42585889
0.42583460
0.42581365
0.42579135
0.42572907
0.42568776
0.42567253
0.42564914
0.42563975
INFO - ==> Top1: 83.034    Top5: 98.544    Loss: 0.509
0.42558777
0.42554793
0.42550042
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [23][   20/   40]   Loss 0.358041   Top1 88.164062   Top5 99.589844   BatchTime 0.125363
INFO - Validation [23][   40/   40]   Loss 0.347553   Top1 88.470000   Top5 99.620000   BatchTime 0.088281
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.5605)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.1875)
features.2.conv.3 tensor(0.3472)
features.2.conv.6 tensor(0.7251)
features.3.conv.0 tensor(0.0706)
features.3.conv.3 tensor(0.0795)
features.3.conv.6 tensor(0.1673)
features.4.conv.0 tensor(0.3188)
features.4.conv.3 tensor(0.3189)
features.4.conv.6 tensor(0.6810)
features.5.conv.0 tensor(0.5389)
features.5.conv.3 tensor(0.4317)
features.5.conv.6 tensor(0.7025)
features.6.conv.0 tensor(0.0592)
features.6.conv.3 tensor(0.0457)
features.6.conv.6 tensor(0.0902)
features.7.conv.0 tensor(0.6495)
features.7.conv.3 tensor(0.4537)
features.7.conv.6 tensor(0.7937)
features.8.conv.0 tensor(0.7089)
features.8.conv.3 tensor(0.5301)
features.8.conv.6 tensor(0.8405)
features.9.conv.0 tensor(0.7005)
features.9.conv.3 tensor(0.5599)
features.9.conv.6 tensor(0.8570)
features.10.conv.0 tensor(0.0830)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.1788)
features.11.conv.0 tensor(0.8276)
features.11.conv.3 tensor(0.6526)
features.11.conv.6 tensor(0.9316)
features.12.conv.0 tensor(0.8073)
features.12.conv.3 tensor(0.6813)
features.12.conv.6 tensor(0.9296)
features.13.conv.0 tensor(0.5233)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.6901)
features.14.conv.0 tensor(0.9301)
features.14.conv.3 tensor(0.8405)
features.14.conv.6 tensor(0.9782)
features.15.conv.0 tensor(0.9300)
features.15.conv.3 tensor(0.8843)
features.15.conv.6 tensor(0.9781)
features.16.conv.0 tensor(0.7566)
features.16.conv.3 tensor(0.8106)
features.16.conv.6 tensor(0.9258)
conv.0 tensor(0.3473)
tensor(1594500.) 2188896.0
INFO - ==> Top1: 88.470    Top5: 99.620    Loss: 0.348
INFO - ==> Sparsity : 0.728
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 88.470   Top5: 99.620]
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 88.100   Top5: 99.650]
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 87.380   Top5: 99.590]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  24
INFO - Training: 50000 samples (256 per mini-batch)
0.42548445
0.42545778
0.42541796
0.42538139
0.42535740
0.42534941
0.42535868
0.42537713
0.42538804
0.42540780
0.42541423
0.42543215
0.42543340
0.42546812
0.42550975
0.42551634
INFO - Training [24][   20/  196]   Loss 0.504983   Top1 83.046875   Top5 98.359375   BatchTime 0.531371   LR 0.000050
0.42551252
0.42552963
0.42550674
0.42549783
0.42547575
0.42547455
0.42546749
0.42545113
0.42544886
0.42544493
0.42543811
0.42542109
0.42540964
0.42539203
0.42537355
0.42536882
0.42536461
0.42534375
0.42531678
0.42530605
0.42530286
0.42531613
INFO - Training [24][   40/  196]   Loss 0.512794   Top1 82.783203   Top5 98.388672   BatchTime 0.506611   LR 0.000048
0.42530510
0.42529595
0.42527226
0.42525387
0.42523912
0.42522055
0.42519045
0.42514554
0.42510149
0.42507935
0.42505512
0.42502621
0.42500877
0.42498893
0.42494556
0.42491344
0.42485592
0.42482886
0.42483553
0.42479703
0.42477012
0.42475972
0.42473981
INFO - Training [24][   60/  196]   Loss 0.505995   Top1 82.975260   Top5 98.476562   BatchTime 0.481547   LR 0.000047
0.42473593
0.42472470
0.42471626
0.42466894
0.42463574
0.42462960
0.42461506
0.42461979
0.42462260
0.42462704
0.42464414
0.42464423
0.42464778
0.42462280
0.42459181
0.42454955
INFO - Training [24][   80/  196]   Loss 0.502932   Top1 83.032227   Top5 98.598633   BatchTime 0.454407   LR 0.000045
0.42453220
0.42453024
0.42449921
0.42447880
0.42445680
0.42441982
0.42439049
0.42436761
0.42434910
0.42434293
0.42430779
0.42431721
0.42433226
0.42432341
0.42434353
0.42434070
0.42433766
0.42433259
0.42430386
0.42427179
0.42425570
0.42421982
INFO - Training [24][  100/  196]   Loss 0.500174   Top1 83.074219   Top5 98.640625   BatchTime 0.453457   LR 0.000044
0.42420462
0.42416835
0.42412949
0.42408341
0.42403159
0.42399871
0.42396539
0.42391688
0.42387542
0.42381409
0.42376551
0.42371237
0.42368212
0.42365161
0.42363313
0.42363632
0.42361310
0.42361182
INFO - Training [24][  120/  196]   Loss 0.499285   Top1 83.089193   Top5 98.684896   BatchTime 0.453158   LR 0.000042
0.42357239
0.42354098
0.42351982
0.42351314
0.42348370
0.42347685
0.42346165
0.42342320
0.42338797
0.42336464
0.42337540
0.42334577
0.42331728
0.42328861
0.42325449
0.42323658
0.42321575
0.42321402
0.42319679
0.42319351
INFO - Training [24][  140/  196]   Loss 0.498673   Top1 83.228237   Top5 98.685826   BatchTime 0.443922   LR 0.000041
0.42319572
0.42319602
0.42317626
0.42313945
0.42311174
0.42308795
0.42307058
0.42307022
0.42307281
0.42304590
0.42304623
0.42303020
0.42302075
0.42301032
0.42297342
0.42296612
0.42296165
0.42296153
0.42296371
0.42296565
0.42296028
INFO - Training [24][  160/  196]   Loss 0.501989   Top1 83.127441   Top5 98.630371   BatchTime 0.438199   LR 0.000039
0.42294288
0.42292339
0.42291296
0.42289051
0.42286664
0.42286238
0.42284605
0.42285141
0.42283738
0.42280391
0.42277238
0.42271912
0.42266381
0.42264447
0.42263857
0.42265156
0.42266569
0.42268398
0.42265230
0.42264035
0.42262399
INFO - Training [24][  180/  196]   Loss 0.501026   Top1 83.192274   Top5 98.572049   BatchTime 0.440779   LR 0.000038
0.42263991
0.42262390
0.42263451
0.42265272
0.42265821
0.42265052
0.42266771
0.42268446
0.42269981
0.42269999
0.42271119
0.42271969
0.42274168
0.42274499
INFO - ==> Top1: 83.256    Top5: 98.582    Loss: 0.500
0.42274961
0.42275146
0.42276683
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [24][   20/   40]   Loss 0.348740   Top1 88.359375   Top5 99.628906   BatchTime 0.126560
INFO - Validation [24][   40/   40]   Loss 0.345112   Top1 88.480000   Top5 99.640000   BatchTime 0.090978
INFO - ==> Top1: 88.480    Top5: 99.640    Loss: 0.345
INFO - ==> Sparsity : 0.732
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 88.480   Top5: 99.640]
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 88.470   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 88.100   Top5: 99.650]
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.5586)
features.1.conv.0 tensor(0.0423)
features.1.conv.3 tensor(0.0868)
features.1.conv.6 tensor(0.0890)
features.2.conv.0 tensor(0.1910)
features.2.conv.3 tensor(0.3511)
features.2.conv.6 tensor(0.7257)
features.3.conv.0 tensor(0.0718)
features.3.conv.3 tensor(0.0802)
features.3.conv.6 tensor(0.1615)
features.4.conv.0 tensor(0.3215)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.6810)
features.5.conv.0 tensor(0.5412)
features.5.conv.3 tensor(0.4363)
features.5.conv.6 tensor(0.7041)
features.6.conv.0 tensor(0.0594)
features.6.conv.3 tensor(0.0434)
features.6.conv.6 tensor(0.0902)
features.7.conv.0 tensor(0.6541)
features.7.conv.3 tensor(0.4549)
features.7.conv.6 tensor(0.7943)
features.8.conv.0 tensor(0.7162)
features.8.conv.3 tensor(0.5295)
features.8.conv.6 tensor(0.8412)
features.9.conv.0 tensor(0.7028)
features.9.conv.3 tensor(0.5616)
features.9.conv.6 tensor(0.8576)
features.10.conv.0 tensor(0.0854)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.1834)
features.11.conv.0 tensor(0.8305)
features.11.conv.3 tensor(0.6528)
features.11.conv.6 tensor(0.9326)
features.12.conv.0 tensor(0.8090)
features.12.conv.3 tensor(0.6806)
features.12.conv.6 tensor(0.9301)
features.13.conv.0 tensor(0.5384)
features.13.conv.3 tensor(0.4871)
features.13.conv.6 tensor(0.6943)
features.14.conv.0 tensor(0.9300)
features.14.conv.3 tensor(0.8407)
features.14.conv.6 tensor(0.9778)
features.15.conv.0 tensor(0.9298)
features.15.conv.3 tensor(0.8832)
features.15.conv.6 tensor(0.9779)
features.16.conv.0 tensor(0.7591)
features.16.conv.3 tensor(0.8101)
features.16.conv.6 tensor(0.9271)
conv.0 tensor(0.3581)
tensor(1601772.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  25
INFO - Training: 50000 samples (256 per mini-batch)
0.42279467
0.42281696
0.42282423
0.42281717
0.42284113
0.42282605
0.42280126
0.42277527
0.42274296
0.42270592
0.42267355
0.42264399
0.42260286
0.42259493
0.42257792
0.42253312
0.42248601
0.42242593
0.42238057
0.42234409
INFO - Training [25][   20/  196]   Loss 0.520218   Top1 82.304688   Top5 97.851562   BatchTime 0.559571   LR 0.000035
0.42230505
0.42228428
0.42226326
0.42221177
0.42218944
0.42217571
0.42216408
0.42215291
0.42214945
0.42214417
0.42214510
0.42216423
0.42216548
0.42215952
0.42215964
0.42213312
0.42211002
0.42209777
INFO - Training [25][   40/  196]   Loss 0.508190   Top1 82.773438   Top5 98.134766   BatchTime 0.503417   LR 0.000034
0.42207766
0.42207226
0.42206818
0.42205906
0.42206612
0.42205104
0.42201290
0.42199808
0.42194739
0.42187956
0.42183074
0.42178717
0.42172983
0.42170748
0.42167747
0.42166904
0.42165372
0.42162025
0.42159232
INFO - Training [25][   60/  196]   Loss 0.502229   Top1 83.118490   Top5 98.287760   BatchTime 0.469801   LR 0.000033
0.42160174
0.42162597
0.42162672
0.42161974
0.42160815
0.42160523
0.42158711
0.42157748
0.42156789
0.42155546
0.42155769
0.42153201
0.42152438
0.42153472
0.42153054
0.42155075
0.42154408
0.42153800
0.42153189
INFO - Training [25][   80/  196]   Loss 0.499411   Top1 83.198242   Top5 98.437500   BatchTime 0.459119   LR 0.000031
0.42152593
0.42153955
0.42156652
0.42156580
0.42156550
0.42155305
0.42154962
0.42155507
0.42154491
0.42152482
0.42149556
0.42146933
0.42144877
0.42141864
0.42139509
0.42135644
0.42133078
0.42129657
0.42128077
0.42128828
0.42127839
0.42126504
INFO - Training [25][  100/  196]   Loss 0.491785   Top1 83.472656   Top5 98.476562   BatchTime 0.458268   LR 0.000030
0.42126212
0.42126584
0.42127138
0.42124858
0.42122233
0.42120942
0.42120636
0.42118683
0.42115423
0.42112452
0.42109299
0.42107412
0.42105570
0.42102316
0.42102125
0.42097610
0.42094347
0.42091265
INFO - Training [25][  120/  196]   Loss 0.491876   Top1 83.502604   Top5 98.512370   BatchTime 0.455019   LR 0.000029
0.42087218
0.42082214
0.42079341
0.42075899
0.42072162
0.42070231
0.42069098
0.42066282
0.42063671
0.42059979
0.42058784
0.42057779
0.42058855
0.42057160
0.42059571
0.42057768
0.42058596
0.42060629
0.42061844
0.42063057
INFO - Training [25][  140/  196]   Loss 0.489204   Top1 83.649554   Top5 98.568638   BatchTime 0.447631   LR 0.000027
0.42062551
0.42063883
0.42065519
0.42065194
0.42065927
0.42065614
0.42064413
0.42064068
0.42061234
0.42063385
0.42063561
0.42063248
0.42065117
0.42065054
0.42065695
0.42066348
0.42066529
0.42064455
0.42063153
0.42060420
0.42057398
0.42054823
0.42053750
0.42051387
INFO - Training [25][  160/  196]   Loss 0.494660   Top1 83.466797   Top5 98.527832   BatchTime 0.444969   LR 0.000026
0.42049530
0.42046720
0.42043418
0.42043248
0.42041239
0.42041531
0.42041117
0.42039633
0.42038378
0.42034766
0.42033395
0.42030850
0.42028853
0.42027307
0.42024535
0.42023510
0.42022878
INFO - Training [25][  180/  196]   Loss 0.495933   Top1 83.378906   Top5 98.506944   BatchTime 0.447043   LR 0.000025
0.42022616
0.42021039
0.42020148
0.42017889
0.42016497
0.42014447
0.42013198
0.42012569
0.42011756
0.42011088
0.42011440
0.42011788
0.42010128
0.42009157
0.42007935
0.42006424
0.42006814
INFO - ==> Top1: 83.408    Top5: 98.514    Loss: 0.495
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.42006707
0.42005166
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [25][   20/   40]   Loss 0.343029   Top1 88.574219   Top5 99.687500   BatchTime 0.131152
INFO - Validation [25][   40/   40]   Loss 0.337633   Top1 88.740000   Top5 99.700000   BatchTime 0.091327
INFO - ==> Top1: 88.740    Top5: 99.700    Loss: 0.338
INFO - ==> Sparsity : 0.734
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 88.480   Top5: 99.640]
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 88.470   Top5: 99.620]
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.5605)
features.1.conv.0 tensor(0.0482)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0864)
features.2.conv.0 tensor(0.2031)
features.2.conv.3 tensor(0.3503)
features.2.conv.6 tensor(0.7266)
features.3.conv.0 tensor(0.0758)
features.3.conv.3 tensor(0.0772)
features.3.conv.6 tensor(0.1667)
features.4.conv.0 tensor(0.3175)
features.4.conv.3 tensor(0.3183)
features.4.conv.6 tensor(0.6821)
features.5.conv.0 tensor(0.5436)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.7046)
features.6.conv.0 tensor(0.0581)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0889)
features.7.conv.0 tensor(0.6554)
features.7.conv.3 tensor(0.4540)
features.7.conv.6 tensor(0.7957)
features.8.conv.0 tensor(0.7195)
features.8.conv.3 tensor(0.5295)
features.8.conv.6 tensor(0.8417)
features.9.conv.0 tensor(0.7062)
features.9.conv.3 tensor(0.5593)
features.9.conv.6 tensor(0.8576)
features.10.conv.0 tensor(0.0854)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.1876)
features.11.conv.0 tensor(0.8330)
features.11.conv.3 tensor(0.6526)
features.11.conv.6 tensor(0.9325)
features.12.conv.0 tensor(0.8115)
features.12.conv.3 tensor(0.6804)
features.12.conv.6 tensor(0.9298)
features.13.conv.0 tensor(0.5499)
features.13.conv.3 tensor(0.4871)
features.13.conv.6 tensor(0.6992)
features.14.conv.0 tensor(0.9305)
features.14.conv.3 tensor(0.8410)
features.14.conv.6 tensor(0.9780)
features.15.conv.0 tensor(0.9304)
features.15.conv.3 tensor(0.8834)
features.15.conv.6 tensor(0.9780)
features.16.conv.0 tensor(0.7608)
features.16.conv.3 tensor(0.8102)
features.16.conv.6 tensor(0.9282)
conv.0 tensor(0.3659)
tensor(1607539.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  26
INFO - Training: 50000 samples (256 per mini-batch)
0.42005587
0.42004856
0.42004958
0.42004815
0.42005640
0.42004812
0.42004690
0.42004108
0.42004332
0.42003971
0.42003828
0.42002335
0.42000556
0.41997933
0.41997489
0.41997102
0.41994113
INFO - Training [26][   20/  196]   Loss 0.506581   Top1 83.066406   Top5 98.359375   BatchTime 0.509849   LR 0.000023
0.41991869
0.41990095
0.41987216
0.41986081
0.41986191
0.41985297
0.41984516
0.41983941
0.41981629
0.41977444
0.41975135
0.41972283
0.41969126
0.41965008
0.41962641
0.41962063
0.41962415
0.41961205
0.41961411
0.41959807
0.41959247
0.41959813
0.41959298
INFO - Training [26][   40/  196]   Loss 0.509964   Top1 82.910156   Top5 98.486328   BatchTime 0.473575   LR 0.000022
0.41959465
0.41958275
0.41957453
0.41955137
0.41954449
0.41954401
0.41953093
0.41953278
0.41953754
0.41954535
0.41952348
0.41952789
0.41953143
0.41953105
0.41955090
0.41954297
0.41953194
0.41953480
0.41951483
0.41951114
INFO - Training [26][   60/  196]   Loss 0.496984   Top1 83.235677   Top5 98.522135   BatchTime 0.447587   LR 0.000021
0.41950944
0.41951030
0.41950938
0.41949832
0.41949821
0.41949469
0.41949603
0.41947693
0.41945583
0.41942239
0.41941082
0.41938782
0.41936120
0.41935077
0.41932970
0.41933581
0.41933289
0.41932344
INFO - Training [26][   80/  196]   Loss 0.497845   Top1 83.330078   Top5 98.598633   BatchTime 0.450851   LR 0.000019
0.41931504
0.41931310
0.41932055
0.41930693
0.41930348
0.41929865
0.41928580
0.41927734
0.41928050
0.41927290
0.41926414
0.41925997
0.41925278
0.41924354
0.41925243
0.41925704
0.41925812
0.41925669
0.41924426
0.41923898
0.41925222
INFO - Training [26][  100/  196]   Loss 0.489139   Top1 83.617188   Top5 98.605469   BatchTime 0.453644   LR 0.000018
0.41924027
0.41925696
0.41923887
0.41924578
0.41925269
0.41924649
0.41924146
0.41924152
0.41924798
0.41923079
0.41921675
0.41919115
0.41919550
0.41918844
0.41917539
0.41918570
0.41919091
0.41918731
0.41917890
INFO - Training [26][  120/  196]   Loss 0.482461   Top1 83.902995   Top5 98.678385   BatchTime 0.448065   LR 0.000017
0.41916829
0.41916546
0.41915360
0.41912511
0.41911125
0.41909319
0.41907546
0.41906700
0.41905004
0.41904247
0.41904524
0.41906181
0.41905862
0.41903815
0.41903180
0.41903380
0.41903248
0.41904762
0.41903290
0.41902059
INFO - Training [26][  140/  196]   Loss 0.483232   Top1 83.903460   Top5 98.683036   BatchTime 0.442077   LR 0.000016
0.41899988
0.41899067
0.41899478
0.41897264
0.41896805
0.41895127
0.41893494
0.41893381
0.41892290
0.41890958
0.41889590
0.41890001
0.41887888
0.41885215
0.41883361
0.41881672
0.41880283
0.41878584
0.41877916
INFO - Training [26][  160/  196]   Loss 0.485845   Top1 83.815918   Top5 98.664551   BatchTime 0.440924   LR 0.000015
0.41876248
0.41874436
0.41873342
0.41873425
0.41871753
0.41870269
0.41868901
0.41869316
0.41868597
0.41867170
0.41866174
0.41866860
0.41865268
0.41864905
0.41864231
0.41863847
0.41864422
0.41865134
0.41863599
0.41863745
0.41864127
INFO - Training [26][  180/  196]   Loss 0.486541   Top1 83.815104   Top5 98.661024   BatchTime 0.444839   LR 0.000014
0.41864440
0.41866282
0.41866624
0.41866630
0.41868579
0.41867951
0.41866854
0.41867250
0.41866958
0.41865563
0.41865554
0.41864941
0.41863903
0.41862068
0.41861305
0.41860780
0.41858718
0.41859522
INFO - ==> Top1: 83.792    Top5: 98.660    Loss: 0.486
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [26][   20/   40]   Loss 0.342241   Top1 88.359375   Top5 99.570312   BatchTime 0.124319
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.5625)
features.1.conv.0 tensor(0.0469)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0881)
features.2.conv.0 tensor(0.2037)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.7274)
features.3.conv.0 tensor(0.0729)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1664)
features.4.conv.0 tensor(0.3219)
features.4.conv.3 tensor(0.3183)
features.4.conv.6 tensor(0.6829)
features.5.conv.0 tensor(0.5438)
features.5.conv.3 tensor(0.4323)
features.5.conv.6
INFO - Validation [26][   40/   40]   Loss 0.336381   Top1 88.530000   Top5 99.610000   BatchTime 0.087402
INFO - ==> Top1: 88.530    Top5: 99.610    Loss: 0.336
INFO - ==> Sparsity : 0.737
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 88.530   Top5: 99.610]
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 88.480   Top5: 99.640]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  27
features.5.conv.6 tensor(0.7044)
features.6.conv.0 tensor(0.0570)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0904)
features.7.conv.0 tensor(0.6587)
features.7.conv.3 tensor(0.4543)
features.7.conv.6 tensor(0.7962)
features.8.conv.0 tensor(0.7193)
features.8.conv.3 tensor(0.5301)
features.8.conv.6 tensor(0.8418)
features.9.conv.0 tensor(0.7077)
features.9.conv.3 tensor(0.5608)
features.9.conv.6 tensor(0.8580)
features.10.conv.0 tensor(0.0852)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.1876)
features.11.conv.0 tensor(0.8337)
features.11.conv.3 tensor(0.6520)
features.11.conv.6 tensor(0.9326)
features.12.conv.0 tensor(0.8126)
features.12.conv.3 tensor(0.6800)
features.12.conv.6 tensor(0.9295)
features.13.conv.0 tensor(0.5580)
features.13.conv.3 tensor(0.4869)
features.13.conv.6 tensor(0.7017)
features.14.conv.0 tensor(0.9308)
features.14.conv.3 tensor(0.8413)
features.14.conv.6 tensor(0.9782)
features.15.conv.0 tensor(0.9308)
features.15.conv.3 tensor(0.8834)
features.15.conv.6 tensor(0.9782)
features.16.conv.0 tensor(0.7625)
features.16.conv.3 tensor(0.8104)
features.16.conv.6 tensor(0.9288)
conv.0 tensor(0.3735)
tensor(1612224.) 2188896.0
0.41861886
0.41860980
0.41860497
0.41859543
0.41859221
0.41858584
0.41858029
0.41856316
0.41856366
0.41856745
0.41855890
0.41855818
0.41856664
0.41856575
0.41857296
0.41857356
0.41857478
0.41856551
INFO - Training [27][   20/  196]   Loss 0.475887   Top1 84.003906   Top5 98.632812   BatchTime 0.540256   LR 0.000013
0.41855603
0.41855791
0.41855171
0.41855127
0.41855395
0.41855231
0.41855481
0.41856825
0.41856357
0.41854769
0.41854343
0.41854641
0.41854444
0.41854852
0.41856036
0.41856748
0.41855565
0.41857392
0.41859394
0.41858554
0.41859034
0.41859519
INFO - Training [27][   40/  196]   Loss 0.482784   Top1 83.583984   Top5 98.574219   BatchTime 0.497205   LR 0.000012
0.41858533
0.41858310
0.41858464
0.41859511
0.41859129
0.41859049
0.41859227
0.41859633
0.41859505
0.41857699
0.41856694
0.41855812
0.41854796
0.41853759
0.41853726
0.41853893
0.41854131
0.41854084
0.41855308
0.41854471
0.41854218
INFO - Training [27][   60/  196]   Loss 0.484401   Top1 83.789062   Top5 98.580729   BatchTime 0.460361   LR 0.000011
0.41853979
0.41852596
0.41852668
0.41852608
0.41851723
0.41851309
0.41850784
0.41850713
0.41850466
0.41849372
0.41849440
0.41848162
0.41847605
0.41847390
0.41848099
0.41847438
0.41847587
0.41846448
INFO - Training [27][   80/  196]   Loss 0.482026   Top1 83.818359   Top5 98.642578   BatchTime 0.456596   LR 0.000010
0.41846067
0.41844642
0.41844487
0.41843808
0.41843283
0.41843155
0.41841474
0.41840431
0.41839531
0.41838318
0.41836861
0.41835058
0.41833180
0.41830316
0.41829789
0.41827706
0.41826224
0.41824570
0.41824448
0.41823718
0.41822222
INFO - Training [27][  100/  196]   Loss 0.480073   Top1 83.878906   Top5 98.660156   BatchTime 0.460442   LR 0.000009
0.41822389
0.41821805
0.41821879
0.41821435
0.41821238
0.41820368
0.41820946
0.41820118
0.41819489
0.41819707
0.41820583
0.41818947
0.41817969
0.41817948
0.41817257
0.41816589
0.41815820
INFO - Training [27][  120/  196]   Loss 0.476277   Top1 83.987630   Top5 98.727214   BatchTime 0.460507   LR 0.000009
0.41814733
0.41812742
0.41811961
0.41811818
0.41812602
0.41810825
0.41809678
0.41808113
0.41807079
0.41805762
0.41804296
0.41804531
0.41803551
0.41801688
0.41801319
0.41800833
0.41800892
0.41800767
0.41801205
0.41801241
0.41801578
0.41801015
0.41801077
INFO - Training [27][  140/  196]   Loss 0.475146   Top1 84.090402   Top5 98.775112   BatchTime 0.454682   LR 0.000008
0.41799518
0.41797930
0.41798556
0.41797811
0.41797534
0.41798064
0.41799477
0.41799957
0.41801301
0.41803116
0.41802889
0.41801402
0.41803110
0.41802523
0.41803095
0.41801658
INFO - Training [27][  160/  196]   Loss 0.477794   Top1 84.028320   Top5 98.752441   BatchTime 0.447094   LR 0.000007
0.41802168
0.41803220
0.41802645
0.41803321
0.41802996
0.41802189
0.41801170
0.41801447
0.41799927
0.41799781
0.41799104
0.41797659
0.41797572
0.41795939
0.41795501
0.41794890
0.41792279
0.41792247
0.41793004
0.41792735
0.41792226
0.41791454
0.41790944
0.41789368
INFO - Training [27][  180/  196]   Loss 0.478900   Top1 83.993056   Top5 98.719618   BatchTime 0.444821   LR 0.000007
0.41788366
0.41789028
0.41787896
0.41787264
0.41786814
0.41785055
0.41784653
0.41783860
0.41783458
0.41782606
0.41782209
0.41781604
0.41780674
INFO - ==> Top1: 84.062    Top5: 98.694    Loss: 0.478
0.41779944
0.41780242
0.41780689
********************pre-trained*****************
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [27][   20/   40]   Loss 0.341784   Top1 88.378906   Top5 99.550781   BatchTime 0.131509
INFO - Validation [27][   40/   40]   Loss 0.332009   Top1 88.710000   Top5 99.620000   BatchTime 0.092066
INFO - ==> Top1: 88.710    Top5: 99.620    Loss: 0.332
INFO - ==> Sparsity : 0.737
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 88.530   Top5: 99.610]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.5625)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0856)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.2049)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.7277)
features.3.conv.0 tensor(0.0747)
features.3.conv.3 tensor(0.0779)
features.3.conv.6 tensor(0.1671)
features.4.conv.0 tensor(0.3304)
features.4.conv.3 tensor(0.3189)
features.4.conv.6 tensor(0.6829)
features.5.conv.0 tensor(0.5444)
features.5.conv.3 tensor(0.4317)
features.5.conv.6 tensor(0.7044)
features.6.conv.0 tensor(0.0557)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0908)
features.7.conv.0 tensor(0.6591)
features.7.conv.3 tensor(0.4534)
features.7.conv.6 tensor(0.7965)
features.8.conv.0 tensor(0.7201)
features.8.conv.3 tensor(0.5298)
features.8.conv.6 tensor(0.8418)
features.9.conv.0 tensor(0.7081)
features.9.conv.3 tensor(0.5602)
features.9.conv.6 tensor(0.8581)
features.10.conv.0 tensor(0.0858)
features.10.conv.3 tensor(0.1050)
features.10.conv.6 tensor(0.1884)
features.11.conv.0 tensor(0.8339)
features.11.conv.3 tensor(0.6518)
features.11.conv.6 tensor(0.9328)
features.12.conv.0 tensor(0.8133)
features.12.conv.3 tensor(0.6796)
features.12.conv.6 tensor(0.9297)
features.13.conv.0 tensor(0.5639)
features.13.conv.3 tensor(0.4865)
features.13.conv.6 tensor(0.7033)
features.14.conv.0 tensor(0.9309)
features.14.conv.3 tensor(0.8410)
features.14.conv.6 tensor(0.9782)
features.15.conv.0 tensor(0.9309)
features.15.conv.3 tensor(0.8832)
features.15.conv.6 tensor(0.9782)
features.16.conv.0 tensor(0.7638)
features.16.conv.3 tensor(0.8105)
features.16.conv.6 tensor(0.9292)
conv.0 tensor(0.3748)
tensor(1613858.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  28
INFO - Training: 50000 samples (256 per mini-batch)
0.41780585
0.41780451
0.41781008
0.41781864
0.41782302
0.41782966
0.41784617
0.41785738
0.41786614
0.41785643
0.41784832
0.41785061
0.41785350
0.41786721
0.41786590
0.41785967
0.41786423
0.41785777
0.41785988
INFO - Training [28][   20/  196]   Loss 0.478941   Top1 84.121094   Top5 98.085938   BatchTime 0.532794   LR 0.000006
0.41784951
0.41784090
0.41783047
0.41782716
0.41781598
0.41781172
0.41782117
0.41780850
0.41780651
0.41780528
0.41780084
0.41780454
0.41781011
0.41780946
0.41781533
0.41780943
0.41780645
0.41780877
INFO - Training [28][   40/  196]   Loss 0.491453   Top1 83.925781   Top5 98.222656   BatchTime 0.492879   LR 0.000005
0.41781452
0.41780528
0.41780078
0.41781077
0.41780657
0.41780454
0.41781461
0.41780651
0.41780025
0.41780028
0.41780707
0.41779989
0.41780528
0.41780546
0.41780511
0.41780123
0.41780117
0.41779160
0.41777852
0.41777793
0.41777062
0.41777128
0.41775385
INFO - Training [28][   60/  196]   Loss 0.490788   Top1 83.912760   Top5 98.365885   BatchTime 0.466900   LR 0.000004
0.41776204
0.41776228
0.41776472
0.41776052
0.41774830
0.41774428
0.41774195
0.41775349
0.41773862
0.41774505
0.41774011
0.41773668
0.41772133
0.41771853
0.41772091
0.41772550
0.41771871
0.41772869
INFO - Training [28][   80/  196]   Loss 0.488807   Top1 83.925781   Top5 98.486328   BatchTime 0.461514   LR 0.000004
0.41773459
0.41772741
0.41774583
0.41774824
0.41773477
0.41774157
0.41773546
0.41772556
0.41772398
0.41771492
0.41770780
0.41770890
0.41770923
0.41770896
0.41770807
0.41770279
0.41770425
0.41770723
0.41770548
0.41769594
0.41768986
0.41769341
INFO - Training [28][  100/  196]   Loss 0.483582   Top1 84.062500   Top5 98.578125   BatchTime 0.463130   LR 0.000003
0.41769460
0.41768911
0.41768909
0.41767862
0.41769034
0.41769674
0.41769835
0.41771209
0.41770491
0.41769570
0.41770250
0.41770345
0.41770151
0.41768920
0.41768786
0.41767672
0.41767222
INFO - Training [28][  120/  196]   Loss 0.478518   Top1 84.163411   Top5 98.632812   BatchTime 0.464307   LR 0.000003
0.41767672
0.41767299
0.41767019
0.41767633
0.41767195
0.41767216
0.41766384
0.41767526
0.41767800
0.41767031
0.41767183
0.41768321
0.41768184
0.41767189
0.41765544
0.41765612
0.41766325
0.41766834
0.41766763
0.41767085
0.41767055
INFO - Training [28][  140/  196]   Loss 0.478712   Top1 84.182478   Top5 98.646763   BatchTime 0.464658   LR 0.000003
0.41767126
0.41766912
0.41766158
0.41765192
0.41765869
0.41765308
0.41765317
0.41766098
0.41767132
0.41768041
0.41768482
0.41768610
0.41767696
0.41766784
0.41767341
0.41766909
0.41767502
0.41767773
0.41767558
0.41767737
INFO - Training [28][  160/  196]   Loss 0.480620   Top1 84.108887   Top5 98.603516   BatchTime 0.456903   LR 0.000002
0.41766927
0.41767162
0.41766465
0.41767284
0.41765270
0.41765386
0.41765144
0.41764444
0.41764715
0.41764423
0.41763845
0.41764057
0.41764310
0.41764036
0.41763514
0.41763550
0.41763222
0.41764030
0.41764766
INFO - Training [28][  180/  196]   Loss 0.480427   Top1 84.064670   Top5 98.606771   BatchTime 0.454516   LR 0.000002
0.41765940
0.41765288
0.41764247
0.41763103
0.41762128
0.41762251
0.41761667
0.41760561
0.41759253
0.41759828
0.41759303
0.41759235
0.41758639
0.41759136
0.41760534
0.41760194
0.41760203
INFO - ==> Top1: 84.118    Top5: 98.634    Loss: 0.478
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.41759464
0.41760081
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [28][   20/   40]   Loss 0.341928   Top1 88.125000   Top5 99.628906   BatchTime 0.128339
INFO - Validation [28][   40/   40]   Loss 0.333715   Top1 88.580000   Top5 99.700000   BatchTime 0.093173
INFO - ==> Top1: 88.580    Top5: 99.700    Loss: 0.334
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 2 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 88.580   Top5: 99.700]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  29
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.5625)
features.1.conv.0 tensor(0.0540)
features.1.conv.3 tensor(0.0822)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.2057)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.7277)
features.3.conv.0 tensor(0.0752)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1669)
features.4.conv.0 tensor(0.3306)
features.4.conv.3 tensor(0.3189)
features.4.conv.6 tensor(0.6828)
features.5.conv.0 tensor(0.5448)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.7046)
features.6.conv.0 tensor(0.0558)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0907)
features.7.conv.0 tensor(0.6592)
features.7.conv.3 tensor(0.4534)
features.7.conv.6 tensor(0.7968)
features.8.conv.0 tensor(0.7203)
features.8.conv.3 tensor(0.5295)
features.8.conv.6 tensor(0.8419)
features.9.conv.0 tensor(0.7085)
features.9.conv.3 tensor(0.5599)
features.9.conv.6 tensor(0.8582)
features.10.conv.0 tensor(0.0857)
features.10.conv.3 tensor(0.1045)
features.10.conv.6 tensor(0.1891)
features.11.conv.0 tensor(0.8337)
features.11.conv.3 tensor(0.6518)
features.11.conv.6 tensor(0.9328)
features.12.conv.0 tensor(0.8135)
features.12.conv.3 tensor(0.6792)
features.12.conv.6 tensor(0.9296)
features.13.conv.0 tensor(0.5652)
features.13.conv.3 tensor(0.4867)
features.13.conv.6 tensor(0.7033)
features.14.conv.0 tensor(0.9310)
features.14.conv.3 tensor(0.8410)
features.14.conv.6 tensor(0.9782)
features.15.conv.0 tensor(0.9309)
features.15.conv.3 tensor(0.8831)
features.15.conv.6 tensor(0.9782)
features.16.conv.0 tensor(0.7644)
features.16.conv.3 tensor(0.8104)
features.16.conv.6 tensor(0.9294)
conv.0 tensor(0.3757)
tensor(1614485.) 2188896.0
0.41760531
0.41760886
0.41762319
0.41762570
0.41762784
0.41763416
0.41763428
0.41762790
0.41763657
0.41763213
0.41763440
0.41763660
0.41763243
0.41764092
0.41764846
0.41765207
0.41765168
INFO - Training [29][   20/  196]   Loss 0.510477   Top1 82.949219   Top5 98.066406   BatchTime 0.527620   LR 0.000001
0.41764086
0.41764265
0.41764012
0.41764480
0.41764334
0.41763234
0.41762802
0.41762376
0.41761330
0.41761443
0.41762188
0.41761711
0.41761595
0.41761810
0.41761234
0.41761139
0.41761360
0.41761488
0.41760525
0.41760471
0.41760838
INFO - Training [29][   40/  196]   Loss 0.499739   Top1 83.017578   Top5 98.203125   BatchTime 0.502262   LR 0.000001
0.41760567
0.41759804
0.41760814
0.41760021
0.41760877
0.41760966
0.41760704
0.41761503
0.41761428
0.41761839
0.41762924
0.41762847
0.41762957
0.41761819
0.41761425
0.41761109
0.41761264
0.41761309
0.41761211
INFO - Training [29][   60/  196]   Loss 0.492387   Top1 83.463542   Top5 98.372396   BatchTime 0.475338   LR 0.000001
0.41760498
0.41761407
0.41760948
0.41760793
0.41760072
0.41760984
0.41759568
0.41760799
0.41761407
0.41761556
0.41761202
0.41761571
0.41761366
0.41760409
0.41760299
0.41759482
0.41759297
0.41759413
0.41759208
0.41759020
0.41760024
0.41759565
INFO - Training [29][   80/  196]   Loss 0.488510   Top1 83.505859   Top5 98.505859   BatchTime 0.467769   LR 0.000001
0.41760221
0.41759974
0.41760352
0.41760108
0.41759530
0.41759905
0.41760257
0.41760162
0.41759449
0.41759455
0.41758475
0.41757998
0.41758323
0.41758129
0.41758442
0.41758832
0.41758016
0.41757190
INFO - Training [29][  100/  196]   Loss 0.479560   Top1 83.851562   Top5 98.566406   BatchTime 0.463792   LR 0.000000
0.41755986
0.41756195
0.41758177
0.41757810
0.41759470
0.41758820
0.41758651
0.41758755
0.41759092
0.41758391
0.41757590
0.41757879
0.41757494
0.41757250
0.41757071
0.41757342
0.41757208
0.41756323
0.41756874
0.41756880
0.41756570
0.41757894
INFO - Training [29][  120/  196]   Loss 0.474895   Top1 84.033203   Top5 98.613281   BatchTime 0.462509   LR 0.000000
0.41757417
0.41757211
0.41756895
0.41756958
0.41757375
0.41757649
0.41756037
0.41755632
0.41755202
0.41754806
0.41754866
0.41755173
0.41755182
0.41754994
0.41754740
0.41755798
0.41755733
0.41756102
INFO - Training [29][  140/  196]   Loss 0.472660   Top1 84.126674   Top5 98.708147   BatchTime 0.461113   LR 0.000000
0.41756040
0.41757596
0.41757140
0.41756079
0.41755509
0.41755763
0.41755289
0.41755664
0.41755962
0.41756251
0.41758430
0.41759205
0.41759354
0.41759548
0.41760144
0.41760445
0.41759670
0.41759983
0.41759774
INFO - Training [29][  160/  196]   Loss 0.477382   Top1 83.952637   Top5 98.666992   BatchTime 0.454067   LR 0.000000
0.41759968
0.41760242
0.41759980
0.41760862
0.41761616
0.41760910
0.41759825
0.41759428
0.41760293
0.41760248
0.41759065
0.41759294
0.41758937
0.41758287
0.41758239
0.41757381
0.41757444
0.41757557
0.41757190
0.41756871
0.41756883
0.41757438
0.41757169
INFO - Training [29][  180/  196]   Loss 0.476665   Top1 83.977865   Top5 98.663194   BatchTime 0.453228   LR 0.000000
0.41756704
0.41756096
0.41756877
0.41757554
0.41757333
0.41757607
0.41757432
0.41757125
0.41757831
0.41757604
0.41757417
0.41757920
0.41757119
0.41758046
0.41758850
0.41757724
0.41758144
********************pre-trained*****************
INFO - ==> Top1: 84.048    Top5: 98.672    Loss: 0.476
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [29][   20/   40]   Loss 0.342888   Top1 88.574219   Top5 99.648438   BatchTime 0.127494
features.0.conv.0 tensor(0.3021)
features.0.conv.3 tensor(0.5625)
features.1.conv.0 tensor(0.0547)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0872)
features.2.conv.0 tensor(0.2063)
features.2.conv.3 tensor(0.3488)
features.2.conv.6 tensor(0.7277)
features.3.conv.0 tensor(0.0752)
features.3.conv.3 tensor(0.0787)
features.3.conv.6 tensor(0.1669)
features.4.conv.0 tensor(0.3307)
features.4.conv.3 tensor(0.3189)
features.4.conv.6 tensor(0.6828)
features.5.conv.0 tensor(0.5448)
features.5.conv.3 tensor(0.4323)
features.5.conv.6 tensor(0.7046)
features.6.conv.0 tensor(0.0558)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0907)
features.7.conv.0 tensor(0.6592)
features.7.conv.3 tensor(0.4534)
features.7.conv.6 tensor(0.7968)
features.8.conv.0 tensor(0.7203)
features.8.conv.3 tensor(0.5295)
features.8.conv.6 tensor(0.8419)
features.9.conv.0
INFO - Validation [29][   40/   40]   Loss 0.335044   Top1 88.840000   Top5 99.670000   BatchTime 0.097560
INFO - ==> Top1: 88.840    Top5: 99.670    Loss: 0.335
INFO - ==> Sparsity : 0.738
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
features.9.conv.0 tensor(0.7085)
features.9.conv.3 tensor(0.5599)
features.9.conv.6 tensor(0.8582)
features.10.conv.0 tensor(0.0857)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.1891)
features.11.conv.0 tensor(0.8337)
features.11.conv.3 tensor(0.6518)
features.11.conv.6 tensor(0.9328)
features.12.conv.0 tensor(0.8136)
features.12.conv.3 tensor(0.6792)
features.12.conv.6 tensor(0.9297)
features.13.conv.0 tensor(0.5655)
features.13.conv.3 tensor(0.4865)
features.13.conv.6 tensor(0.7035)
features.14.conv.0 tensor(0.9310)
features.14.conv.3 tensor(0.8410)
features.14.conv.6 tensor(0.9782)
features.15.conv.0 tensor(0.9308)
features.15.conv.3 tensor(0.8831)
features.15.conv.6 tensor(0.9782)
features.16.conv.0 tensor(0.7644)
features.16.conv.3 tensor(0.8104)
features.16.conv.6 tensor(0.9294)
conv.0 tensor(0.3759)
tensor(1614600.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
                Best: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_best.pth.tar
save quantized models...
INFO - >>>>>> Epoch  30
INFO - Training: 50000 samples (256 per mini-batch)
0.41758686
0.41774735
0.41781524
0.41786548
0.41787553
0.41778058
0.41766080
0.41755450
0.41746485
0.41735902
0.41735321
0.41730958
0.41723597
0.41716927
0.41707370
0.41700396
0.41693237
0.41691640
0.41690889
0.41691488
0.41693255
INFO - Training [30][   20/  196]   Loss 0.511675   Top1 83.066406   Top5 98.125000   BatchTime 0.531058   LR 0.000125
0.41700336
0.41705826
0.41709322
0.41709298
0.41709220
0.41713467
0.41712776
0.41713730
0.41711593
0.41711909
0.41709295
0.41709256
0.41707107
0.41703928
0.41705251
0.41709355
0.41714334
INFO - Training [30][   40/  196]   Loss 0.517677   Top1 82.568359   Top5 98.242188   BatchTime 0.492085   LR 0.000125
0.41712505
0.41710940
0.41712916
0.41711763
0.41715607
0.41711348
0.41703415
0.41700059
0.41705284
0.41709647
0.41719124
0.41723794
0.41726369
0.41722375
0.41719511
0.41719887
0.41728190
0.41727123
0.41730729
0.41737214
INFO - Training [30][   60/  196]   Loss 0.516541   Top1 82.617188   Top5 98.404948   BatchTime 0.460704   LR 0.000125
0.41746414
0.41753381
0.41756520
0.41760921
0.41765618
0.41767767
0.41773617
0.41781259
0.41791394
0.41803488
0.41811168
0.41820472
0.41824183
0.41826957
0.41829124
0.41834158
0.41837850
0.41841081
0.41843149
0.41846234
0.41853285
0.41854197
0.41861597
INFO - Training [30][   80/  196]   Loss 0.521626   Top1 82.460938   Top5 98.486328   BatchTime 0.459041   LR 0.000125
0.41867021
0.41872546
0.41873908
0.41875529
0.41881409
0.41884950
0.41886431
0.41890264
0.41895121
0.41895661
0.41898876
0.41899484
0.41905501
0.41913140
0.41915831
0.41913190
0.41912571
INFO - Training [30][  100/  196]   Loss 0.518020   Top1 82.593750   Top5 98.417969   BatchTime 0.459149   LR 0.000125
0.41913369
0.41914481
0.41916588
0.41914287
0.41910514
0.41909435
0.41907200
0.41905537
0.41907001
0.41903222
0.41901672
0.41906810
0.41902405
0.41900986
0.41899979
0.41903767
0.41909423
0.41914570
0.41923630
0.41932753
0.41941845
0.41948420
INFO - Training [30][  120/  196]   Loss 0.516041   Top1 82.705078   Top5 98.457031   BatchTime 0.458305   LR 0.000125
0.41949815
0.41951048
0.41949520
0.41948164
0.41943052
0.41943353
0.41943219
0.41941196
0.41941229
0.41938427
0.41936779
0.41932365
0.41929203
0.41930756
0.41930288
0.41926911
0.41929749
0.41930619
0.41931584
0.41939029
INFO - Training [30][  140/  196]   Loss 0.515473   Top1 82.748326   Top5 98.510045   BatchTime 0.451194   LR 0.000125
0.41941518
0.41942090
0.41948083
0.41954228
0.41956213
0.41961995
0.41963425
0.41966525
0.41969407
0.41968256
0.41964424
0.41961911
0.41962272
0.41961747
0.41963366
0.41964239
0.41969138
0.41971004
INFO - Training [30][  160/  196]   Loss 0.518323   Top1 82.663574   Top5 98.510742   BatchTime 0.448322   LR 0.000125
0.41974258
0.41981092
0.41985825
0.41990307
0.41997635
0.41997731
0.42003891
0.42008325
0.42006269
0.42004269
0.42002508
0.41996312
0.41994539
0.41991633
0.41991434
0.41990554
0.41992170
0.41995129
0.41994584
0.41989028
0.41986504
INFO - Training [30][  180/  196]   Loss 0.516198   Top1 82.740885   Top5 98.496094   BatchTime 0.452423   LR 0.000125
0.41977495
0.41970849
0.41962072
0.41959152
0.41956174
0.41955751
0.41957623
0.41954842
0.41954255
0.41955385
0.41952640
0.41955182
0.41957080
0.41957170
0.41955516
0.41949594
0.41945475
********************pre-trained*****************
INFO - ==> Top1: 82.704    Top5: 98.500    Loss: 0.517
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [30][   20/   40]   Loss 0.390536   Top1 87.031250   Top5 99.355469   BatchTime 0.154009
features.0.conv.0 tensor(0.2951)
features.0.conv.3 tensor(0.5664)
features.1.conv.0 tensor(0.0534)
features.1.conv.3 tensor(0.0833)
features.1.conv.6 tensor(0.0877)
features.2.conv.0 tensor(0.2199)
features.2.conv.3 tensor(0.3526)
features.2.conv.6 tensor(0.7309)
features.3.conv.0 tensor(0.0761)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1712)
features.4.conv.0 tensor(0.3374)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.6805)
features.5.conv.0 tensor(0.5438)
INFO - Validation [30][   40/   40]   Loss 0.379027   Top1 87.280000   Top5 99.440000   BatchTime 0.103168
INFO - ==> Top1: 87.280    Top5: 99.440    Loss: 0.379
INFO - ==> Sparsity : 0.739
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  31
INFO - Training: 50000 samples (256 per mini-batch)
features.5.conv.3 tensor(0.4300)
features.5.conv.6 tensor(0.7018)
features.6.conv.0 tensor(0.0591)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0904)
features.7.conv.0 tensor(0.6640)
features.7.conv.3 tensor(0.4566)
features.7.conv.6 tensor(0.7937)
features.8.conv.0 tensor(0.7181)
features.8.conv.3 tensor(0.5298)
features.8.conv.6 tensor(0.8418)
features.9.conv.0 tensor(0.7118)
features.9.conv.3 tensor(0.5582)
features.9.conv.6 tensor(0.8562)
features.10.conv.0 tensor(0.0868)
features.10.conv.3 tensor(0.1016)
features.10.conv.6 tensor(0.1961)
features.11.conv.0 tensor(0.8324)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.9332)
features.12.conv.0 tensor(0.8146)
features.12.conv.3 tensor(0.6811)
features.12.conv.6 tensor(0.9302)
features.13.conv.0 tensor(0.5112)
features.13.conv.3 tensor(0.4848)
features.13.conv.6 tensor(0.6994)
features.14.conv.0 tensor(0.9328)
features.14.conv.3 tensor(0.8411)
features.14.conv.6 tensor(0.9781)
features.15.conv.0 tensor(0.9334)
features.15.conv.3 tensor(0.8814)
features.15.conv.6 tensor(0.9782)
features.16.conv.0 tensor(0.7651)
features.16.conv.3 tensor(0.8104)
features.16.conv.6 tensor(0.9284)
conv.0 tensor(0.3867)
tensor(1616523.) 2188896.0
0.41952085
0.41958502
0.41960305
0.41968796
0.41976178
0.41988257
0.41991413
0.41999772
0.42003837
0.42004025
0.42001462
0.42001969
0.42005926
0.42006347
0.42003968
0.42003024
0.42003828
0.41998550
0.41993046
0.41990802
INFO - Training [31][   20/  196]   Loss 0.540436   Top1 81.601562   Top5 98.183594   BatchTime 0.566063   LR 0.000125
0.41985896
0.41978666
0.41976348
0.41972357
0.41966897
0.41963315
0.41958198
0.41954821
0.41952425
0.41942725
0.41940334
0.41944012
0.41940504
0.41937521
0.41930985
0.41925350
0.41926232
0.41926149
INFO - Training [31][   40/  196]   Loss 0.546082   Top1 81.386719   Top5 98.251953   BatchTime 0.507036   LR 0.000125
0.41928747
0.41934198
0.41932663
0.41935226
0.41938648
0.41939253
0.41939792
0.41939563
0.41942799
0.41945893
0.41949683
0.41949993
0.41952798
0.41963139
0.41959959
0.41955978
0.41957739
0.41960874
0.41960171
0.41957188
INFO - Training [31][   60/  196]   Loss 0.540807   Top1 81.725260   Top5 98.320312   BatchTime 0.468872   LR 0.000125
0.41955301
0.41954163
0.41953310
0.41949698
0.41953024
0.41951561
0.41953361
0.41956052
0.41953227
0.41954848
0.41955104
0.41952154
0.41954556
0.41950536
0.41950619
0.41949272
0.41948834
0.41947225
0.41944370
0.41942117
0.41936719
0.41935512
INFO - Training [31][   80/  196]   Loss 0.538219   Top1 81.845703   Top5 98.437500   BatchTime 0.466863   LR 0.000125
0.41927192
0.41921628
0.41923010
0.41920292
0.41921732
0.41926119
0.41930845
0.41931057
0.41932857
0.41933563
0.41934615
0.41933268
0.41931331
0.41927662
0.41921881
0.41921383
0.41917828
0.41917357
INFO - Training [31][  100/  196]   Loss 0.527591   Top1 82.238281   Top5 98.464844   BatchTime 0.464568   LR 0.000125
0.41915545
0.41913158
0.41914636
0.41914380
0.41912755
0.41912639
0.41916856
0.41916427
0.41912669
0.41910997
0.41906264
0.41905227
0.41905871
0.41905713
0.41904774
0.41905239
0.41905519
0.41906661
0.41906655
0.41909859
0.41914439
0.41918239
INFO - Training [31][  120/  196]   Loss 0.523263   Top1 82.431641   Top5 98.518880   BatchTime 0.464561   LR 0.000125
0.41929388
0.41933006
0.41930905
0.41930822
0.41935858
0.41938528
0.41939238
0.41941807
0.41946104
0.41950992
0.41949004
0.41947195
0.41946515
0.41949776
0.41953138
0.41956037
0.41955304
0.41955948
0.41956070
INFO - Training [31][  140/  196]   Loss 0.519170   Top1 82.625558   Top5 98.563058   BatchTime 0.455893   LR 0.000124
0.41955450
0.41956618
0.41959214
0.41962847
0.41964644
0.41966370
0.41965559
0.41963679
0.41960618
0.41957673
0.41955790
0.41961643
0.41965732
0.41965899
0.41961232
0.41957888
0.41957691
0.41960585
0.41963366
INFO - Training [31][  160/  196]   Loss 0.522578   Top1 82.521973   Top5 98.552246   BatchTime 0.454727   LR 0.000124
0.41971377
0.41971767
0.41979393
0.41979179
0.41978443
0.41975343
0.41972786
0.41969922
0.41970131
0.41970399
0.41970709
0.41967967
0.41971886
0.41966718
0.41967469
0.41971907
0.41972053
0.41973141
0.41970670
0.41967890
0.41966775
INFO - Training [31][  180/  196]   Loss 0.519527   Top1 82.604167   Top5 98.489583   BatchTime 0.456597   LR 0.000124
0.41968924
0.41968653
0.41969007
0.41966343
0.41958827
0.41954580
0.41948038
0.41944960
0.41930923
0.41917378
0.41911411
0.41904432
0.41897491
0.41894722
0.41886234
0.41889271
0.41887066
INFO - ==> Top1: 82.644    Top5: 98.474    Loss: 0.519
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [31][   20/   40]   Loss 0.397848   Top1 86.523438   Top5 99.433594   BatchTime 0.130254
INFO - Validation [31][   40/   40]   Loss 0.382079   Top1 87.280000   Top5 99.580000   BatchTime 0.090508
INFO - ==> Top1: 87.280    Top5: 99.580    Loss: 0.382
INFO - ==> Sparsity : 0.741
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  32
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2778)
features.0.conv.3 tensor(0.5703)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0845)
features.1.conv.6 tensor(0.0833)
features.2.conv.0 tensor(0.2731)
features.2.conv.3 tensor(0.3534)
features.2.conv.6 tensor(0.7292)
features.3.conv.0 tensor(0.0703)
features.3.conv.3 tensor(0.0818)
features.3.conv.6 tensor(0.1799)
features.4.conv.0 tensor(0.3304)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.6818)
features.5.conv.0 tensor(0.5339)
features.5.conv.3 tensor(0.4277)
features.5.conv.6 tensor(0.6995)
features.6.conv.0 tensor(0.0581)
features.6.conv.3 tensor(0.0451)
features.6.conv.6 tensor(0.0895)
features.7.conv.0 tensor(0.6624)
features.7.conv.3 tensor(0.4523)
features.7.conv.6 tensor(0.7915)
features.8.conv.0 tensor(0.7259)
features.8.conv.3 tensor(0.5289)
features.8.conv.6 tensor(0.8413)
features.9.conv.0 tensor(0.7058)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.8588)
features.10.conv.0 tensor(0.0882)
features.10.conv.3 tensor(0.1036)
features.10.conv.6 tensor(0.1995)
features.11.conv.0 tensor(0.8321)
features.11.conv.3 tensor(0.6520)
features.11.conv.6 tensor(0.9339)
features.12.conv.0 tensor(0.8117)
features.12.conv.3 tensor(0.6815)
features.12.conv.6 tensor(0.9288)
features.13.conv.0 tensor(0.5083)
features.13.conv.3 tensor(0.4884)
features.13.conv.6 tensor(0.7008)
features.14.conv.0 tensor(0.9341)
features.14.conv.3 tensor(0.8405)
features.14.conv.6 tensor(0.9785)
features.15.conv.0 tensor(0.9349)
features.15.conv.3 tensor(0.8791)
features.15.conv.6 tensor(0.9780)
features.16.conv.0 tensor(0.7680)
features.16.conv.3 tensor(0.8104)
features.16.conv.6 tensor(0.9287)
conv.0 tensor(0.3956)
tensor(1621105.) 2188896.0
0.41893700
0.41898578
0.41905686
0.41908816
0.41912109
0.41914290
0.41915640
0.41915688
0.41913062
0.41913617
0.41913790
0.41914940
0.41912156
0.41910541
0.41910172
0.41910750
0.41913792
0.41915339
0.41914716
INFO - Training [32][   20/  196]   Loss 0.520985   Top1 82.812500   Top5 97.968750   BatchTime 0.520310   LR 0.000124
0.41910580
0.41909641
0.41905904
0.41902885
0.41899851
0.41894376
0.41890538
0.41887200
0.41882661
0.41882122
0.41880485
0.41878560
0.41877270
0.41879082
0.41872537
0.41869009
0.41873404
0.41867396
0.41868019
0.41872916
0.41876781
0.41879877
INFO - Training [32][   40/  196]   Loss 0.525503   Top1 82.490234   Top5 98.222656   BatchTime 0.485250   LR 0.000124
0.41882354
0.41884497
0.41886315
0.41884694
0.41883191
0.41878197
0.41880330
0.41887119
0.41894975
0.41897371
0.41897005
0.41894546
0.41891733
0.41888347
0.41879168
0.41881084
0.41880214
0.41875768
0.41869137
INFO - Training [32][   60/  196]   Loss 0.520207   Top1 82.669271   Top5 98.307292   BatchTime 0.460405   LR 0.000124
0.41860923
0.41851121
0.41843876
0.41836452
0.41831785
0.41826993
0.41826737
0.41825548
0.41823477
0.41821155
0.41817385
0.41817048
0.41815287
0.41817257
0.41815215
0.41811815
0.41811505
0.41812891
0.41814795
INFO - Training [32][   80/  196]   Loss 0.525551   Top1 82.524414   Top5 98.398438   BatchTime 0.454802   LR 0.000124
0.41818246
0.41822910
0.41831776
0.41841823
0.41848144
0.41853750
0.41857639
0.41862503
0.41862011
0.41863090
0.41863823
0.41871479
0.41870683
0.41873217
0.41870227
0.41864854
0.41862604
0.41861174
0.41858032
0.41858408
0.41856766
0.41850019
INFO - Training [32][  100/  196]   Loss 0.522043   Top1 82.734375   Top5 98.406250   BatchTime 0.455033   LR 0.000124
0.41846275
0.41841215
0.41837433
0.41836873
0.41836017
0.41830525
0.41837576
0.41834432
0.41839287
0.41838351
0.41839299
0.41839838
0.41841963
0.41837719
0.41837367
0.41834626
INFO - Training [32][  120/  196]   Loss 0.518713   Top1 82.851562   Top5 98.486328   BatchTime 0.459514   LR 0.000124
0.41837025
0.41837665
0.41838208
0.41842145
0.41843107
0.41846371
0.41846034
0.41851413
0.41850385
0.41849428
0.41849798
0.41844419
0.41841176
0.41840541
0.41837782
0.41835567
0.41827029
0.41823778
0.41816521
0.41812679
0.41808397
0.41795954
0.41793397
0.41790855
0.41789547
INFO - Training [32][  140/  196]   Loss 0.514186   Top1 83.018973   Top5 98.537946   BatchTime 0.451773   LR 0.000124
0.41793299
0.41794184
0.41791293
0.41792971
0.41792673
0.41793090
0.41794109
0.41792992
0.41796130
0.41797414
0.41794759
0.41792309
0.41785985
0.41785234
0.41783994
0.41779366
INFO - Training [32][  160/  196]   Loss 0.519978   Top1 82.824707   Top5 98.459473   BatchTime 0.453184   LR 0.000123
0.41776258
0.41773695
0.41770223
0.41767606
0.41767788
0.41762701
0.41760451
0.41759682
0.41757005
0.41753882
0.41748169
0.41743389
0.41741210
0.41741174
0.41738427
0.41734192
0.41730437
0.41723374
0.41719449
0.41711986
0.41708374
0.41702139
INFO - Training [32][  180/  196]   Loss 0.520677   Top1 82.714844   Top5 98.435330   BatchTime 0.454179   LR 0.000123
0.41699815
0.41701627
0.41703764
0.41704565
0.41704762
0.41709855
0.41710848
0.41712281
0.41714427
0.41717184
0.41723490
0.41727126
0.41732624
0.41736081
0.41734537
0.41735587
********************pre-trained*****************
INFO - ==> Top1: 82.680    Top5: 98.434    Loss: 0.521
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [32][   20/   40]   Loss 0.398520   Top1 86.562500   Top5 99.472656   BatchTime 0.130698
features.0.conv.0 tensor(0.2743)
features.0.conv.3 tensor(0.5684)
features.1.conv.0 tensor(0.0501)
features.1.conv.3 tensor(0.0706)
features.1.conv.6 tensor(0.0907)
features.2.conv.0 tensor(0.2943)
features.2.conv.3 tensor(0.3495)
features.2.conv.6 tensor(0.7248)
features.3.conv.0 tensor(0.0654)
features.3.conv.3 tensor(0.0756)
features.3.conv.6 tensor(0.1808)
features.4.conv.0 tensor(0.3423)
features.4.conv.3 tensor(0.3206)
features.4.conv.6 tensor(0.6836)
features.5.conv.0 tensor(0.5319)
features.5.conv.3 tensor(0.4421)
features.5.conv.6 tensor(0.7026)
features.6.conv.0 tensor(0.0578)
features.6.conv.3 tensor(0.0428)
features.6.conv.6 tensor(0.0885)
features.7.conv.0 tensor(0.6657)
features.7.conv.3 tensor(0.4528)
features.7.conv.6 tensor(0.7959)
features.8.conv.0 tensor(0.7198)
features.8.conv.3 tensor(0.5301)
features.8.conv.6 tensor(0.8428)
features.9.conv.0 tensor(0.7106)
features.9.conv.3 tensor(0.5576)
features.9.conv.6 tensor(0.8543)
features.10.conv.0 tensor(0.0839)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.2061)
features.11.conv.0 tensor(0.8320)
features.11.conv.3 tensor(0.6514)
features.11.conv.6 tensor(0.9322)
features.12.conv.0 tensor(0.8130)
features.12.conv.3 tensor(0.6819)
features.12.conv.6 tensor(0.9295)
features.13.conv.0 tensor(0.5082)
features.13.conv.3 tensor(0.4859)
features.13.conv.6 tensor(0.7041)
features.14.conv.0 tensor(0.9354)
features.14.conv.3 tensor(0.8406)
features.14.conv.6 tensor(0.9784)
features.15.conv.0 tensor(0.9351)
features.15.conv.3 tensor(0.8800)
features.15.conv.6 tensor(0.9786)
features.16.conv.0 tensor(0.7670)
features.16.conv.3 tensor(0.8113)
features.16.conv.6 tensor(0.9285)
conv.0 tensor(0.4020)
tensor(1624516.) 2188896.0
INFO - Validation [32][   40/   40]   Loss 0.382825   Top1 86.890000   Top5 99.530000   BatchTime 0.090935
INFO - ==> Top1: 86.890    Top5: 99.530    Loss: 0.383
INFO - ==> Sparsity : 0.742
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  33
INFO - Training: 50000 samples (256 per mini-batch)
0.41724569
0.41722628
0.41720071
0.41723374
0.41724244
0.41727999
0.41736063
0.41739252
0.41743401
0.41745821
0.41746372
0.41747347
0.41750419
0.41753846
0.41756719
0.41755643
0.41761413
0.41765270
INFO - Training [33][   20/  196]   Loss 0.534141   Top1 81.738281   Top5 98.046875   BatchTime 0.540795   LR 0.000123
0.41769174
0.41768765
0.41770557
0.41770914
0.41766655
0.41765171
0.41763800
0.41765013
0.41765517
0.41766962
0.41764501
0.41768393
0.41766760
0.41762143
0.41764376
0.41766059
0.41766208
0.41769034
0.41770515
0.41772842
0.41772759
0.41773078
INFO - Training [33][   40/  196]   Loss 0.538670   Top1 81.689453   Top5 98.193359   BatchTime 0.500319   LR 0.000123
0.41770899
0.41771245
0.41775349
0.41778740
0.41784164
0.41784471
0.41783997
0.41784430
0.41789263
0.41787529
0.41785869
0.41787905
0.41783544
0.41783130
0.41784683
0.41780320
0.41776323
0.41774666
0.41769835
0.41766787
INFO - Training [33][   60/  196]   Loss 0.527159   Top1 82.239583   Top5 98.333333   BatchTime 0.470918   LR 0.000123
0.41767901
0.41772553
0.41774681
0.41772914
0.41767326
0.41766170
0.41762057
0.41758335
0.41752782
0.41748518
0.41744316
0.41741204
0.41738880
0.41735563
0.41732588
0.41728693
0.41726533
0.41722718
INFO - Training [33][   80/  196]   Loss 0.521119   Top1 82.387695   Top5 98.476562   BatchTime 0.460741   LR 0.000123
0.41722792
0.41720757
0.41719320
0.41722354
0.41723785
0.41730893
0.41730231
0.41734046
0.41740474
0.41743359
0.41745725
0.41745657
0.41748989
0.41750214
0.41753015
0.41753632
0.41757932
0.41761839
0.41764197
0.41768819
0.41769648
INFO - Training [33][  100/  196]   Loss 0.514728   Top1 82.687500   Top5 98.519531   BatchTime 0.462500   LR 0.000123
0.41771448
0.41774145
0.41774940
0.41773999
0.41772881
0.41780147
0.41780779
0.41781318
0.41782942
0.41782773
0.41785049
0.41787910
0.41788739
0.41788626
0.41793349
0.41800117
0.41801032
0.41796234
0.41794834
0.41794482
0.41795412
INFO - Training [33][  120/  196]   Loss 0.510671   Top1 82.858073   Top5 98.626302   BatchTime 0.467526   LR 0.000123
0.41795635
0.41796422
0.41796154
0.41798905
0.41798040
0.41795984
0.41792369
0.41791484
0.41787839
0.41785535
0.41784012
0.41788068
0.41793182
0.41795737
0.41798058
0.41798666
0.41797638
0.41794613
0.41794744
0.41794878
INFO - Training [33][  140/  196]   Loss 0.509300   Top1 82.938058   Top5 98.663504   BatchTime 0.459102   LR 0.000122
0.41790950
0.41782653
0.41772708
0.41768989
0.41764474
0.41765097
0.41766551
0.41761321
0.41756579
0.41754147
0.41746795
0.41739061
0.41732663
0.41728133
0.41721734
0.41721591
0.41721490
INFO - Training [33][  160/  196]   Loss 0.511558   Top1 82.827148   Top5 98.635254   BatchTime 0.457699   LR 0.000122
0.41713390
0.41709173
0.41708925
0.41703421
0.41701469
0.41690037
0.41687921
0.41689017
0.41692492
0.41695973
0.41696241
0.41694885
0.41695359
0.41691092
0.41683373
0.41681913
0.41679421
0.41677901
0.41679013
0.41676259
0.41672629
0.41676381
0.41683343
0.41690326
INFO - Training [33][  180/  196]   Loss 0.513812   Top1 82.745226   Top5 98.572049   BatchTime 0.454038   LR 0.000122
0.41692182
0.41694042
0.41691363
0.41686380
0.41683123
0.41680834
0.41679132
0.41677928
0.41682246
0.41684946
0.41691476
0.41691995
0.41692871
0.41690838
INFO - ==> Top1: 82.704    Top5: 98.556    Loss: 0.515
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
0.41694042
********************pre-trained*****************
INFO - Validation: 10000 samples (256 per mini-batch)
validation quantized model on cpu
INFO - Validation [33][   20/   40]   Loss 0.392687   Top1 86.933594   Top5 99.414062   BatchTime 0.134857
INFO - Validation [33][   40/   40]   Loss 0.382503   Top1 87.170000   Top5 99.540000   BatchTime 0.092787
INFO - ==> Top1: 87.170    Top5: 99.540    Loss: 0.383
INFO - ==> Sparsity : 0.742
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
features.0.conv.0 tensor(0.2986)
features.0.conv.3 tensor(0.5742)
features.1.conv.0 tensor(0.0436)
features.1.conv.3 tensor(0.0694)
features.1.conv.6 tensor(0.0807)
features.2.conv.0 tensor(0.2946)
features.2.conv.3 tensor(0.3457)
features.2.conv.6 tensor(0.7277)
features.3.conv.0 tensor(0.0703)
features.3.conv.3 tensor(0.0733)
features.3.conv.6 tensor(0.1825)
features.4.conv.0 tensor(0.3407)
features.4.conv.3 tensor(0.3258)
features.4.conv.6 tensor(0.6810)
features.5.conv.0 tensor(0.5361)
features.5.conv.3 tensor(0.4387)
features.5.conv.6 tensor(0.6999)
features.6.conv.0 tensor(0.0620)
features.6.conv.3 tensor(0.0440)
features.6.conv.6 tensor(0.0903)
features.7.conv.0 tensor(0.6645)
features.7.conv.3 tensor(0.4537)
features.7.conv.6 tensor(0.7943)
features.8.conv.0 tensor(0.7155)
features.8.conv.3 tensor(0.5286)
features.8.conv.6 tensor(0.8410)
features.9.conv.0 tensor(0.7082)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.8538)
features.10.conv.0 tensor(0.0824)
features.10.conv.3 tensor(0.1024)
features.10.conv.6 tensor(0.2125)
features.11.conv.0 tensor(0.8337)
features.11.conv.3 tensor(0.6508)
features.11.conv.6 tensor(0.9331)
features.12.conv.0 tensor(0.8113)
features.12.conv.3 tensor(0.6809)
features.12.conv.6 tensor(0.9279)
features.13.conv.0 tensor(0.4978)
features.13.conv.3 tensor(0.4884)
features.13.conv.6 tensor(0.7039)
features.14.conv.0 tensor(0.9362)
features.14.conv.3 tensor(0.8410)
features.14.conv.6 tensor(0.9780)
features.15.conv.0 tensor(0.9369)
features.15.conv.3 tensor(0.8784)
features.15.conv.6 tensor(0.9790)
features.16.conv.0 tensor(0.7680)
features.16.conv.3 tensor(0.8111)
features.16.conv.6 tensor(0.9288)
conv.0 tensor(0.4027)
tensor(1624774.) 2188896.0
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  34
INFO - Training: 50000 samples (256 per mini-batch)
0.41697037
0.41696766
0.41696751
0.41693857
0.41690445
0.41689783
0.41691086
0.41692188
0.41692230
0.41691449
0.41685772
0.41683546
0.41675222
0.41672629
0.41668406
0.41663525
0.41662705
0.41659606
0.41653812
INFO - Training [34][   20/  196]   Loss 0.526991   Top1 82.285156   Top5 98.105469   BatchTime 0.545217   LR 0.000122
0.41649541
0.41648862
0.41646123
0.41641882
0.41638419
0.41638017
0.41638198
0.41635266
0.41632009
0.41631934
0.41635323
0.41638914
0.41635877
0.41631216
0.41634250
0.41639283
0.41640943
0.41638309
INFO - Training [34][   40/  196]   Loss 0.531839   Top1 82.363281   Top5 98.134766   BatchTime 0.496453   LR 0.000122
0.41636261
0.41634026
0.41635540
0.41636670
0.41635630
0.41633654
0.41628939
0.41625336
0.41627738
0.41634053
0.41636288
0.41635120
0.41635907
0.41641405
0.41642442
0.41646856
0.41647816
0.41648155
0.41650805
0.41656330
0.41661945
0.41667873
INFO - Training [34][   60/  196]   Loss 0.522477   Top1 82.695312   Top5 98.261719   BatchTime 0.481897   LR 0.000121
0.41669697
0.41669461
0.41668680
0.41673285
0.41675952
0.41677475
0.41676453
0.41680267
0.41682169
0.41680807
0.41678354
0.41677895
0.41680110
0.41679698
0.41676569
0.41679463
0.41679966
0.41675529
0.41679168
0.41676885
INFO - Training [34][   80/  196]   Loss 0.525271   Top1 82.500000   Top5 98.354492   BatchTime 0.462114   LR 0.000121
0.41671577
0.41668281
0.41670692
0.41669416
0.41665778
0.41665041
0.41667816
0.41665486
0.41664055
0.41666704
0.41670322
0.41676259
0.41683927
0.41686141
0.41686922
0.41687480
0.41688919
0.41698605
0.41701803
0.41705444
INFO - Training [34][  100/  196]   Loss 0.514368   Top1 82.910156   Top5 98.480469   BatchTime 0.448962   LR 0.000121
0.41707763
0.41709310
0.41711465
0.41712016
0.41710088
0.41712630
0.41710439
0.41712213
0.41713020
0.41712320
0.41717300
0.41722623
0.41723117
0.41724402
0.41721949
0.41720834
0.41721237
0.41719404
0.41720310
0.41717771
0.41713846
INFO - Training [34][  120/  196]   Loss 0.508848   Top1 83.089193   Top5 98.554688   BatchTime 0.452719   LR 0.000121
0.41712686
0.41710946
0.41711089
0.41709259
0.41709045
0.41702163
0.41702080
0.41699225
0.41692430
0.41684875
0.41679397
0.41679204
0.41678247
0.41675419
0.41667801
0.41661438
0.41652545
0.41644481
0.41642594
0.41637287
INFO - Training [34][  140/  196]   Loss 0.509302   Top1 83.013393   Top5 98.635603   BatchTime 0.446469   LR 0.000121
0.41632193
0.41629466
0.41626072
0.41621926
0.41620183
0.41621125
0.41622913
0.41622269
0.41621074
0.41620836
0.41616306
0.41616708
0.41616979
0.41616884
0.41613138
0.41608649
0.41607919
INFO - Training [34][  160/  196]   Loss 0.512736   Top1 82.844238   Top5 98.588867   BatchTime 0.449495   LR 0.000121
0.41609338
0.41607413
0.41609189
0.41610172
0.41613778
0.41617694
0.41620058
0.41618982
0.41619942
0.41619948
0.41622236
0.41622111
0.41625199
0.41625956
0.41626534
0.41630697
0.41636744
0.41639826
0.41643295
0.41640654
0.41634312
0.41636744
INFO - Training [34][  180/  196]   Loss 0.514042   Top1 82.751736   Top5 98.572049   BatchTime 0.450982   LR 0.000120
0.41632372
0.41623440
0.41619793
0.41618696
0.41617930
0.41618836
0.41624272
0.41625640
0.41626051
0.41622201
0.41617742
0.41613567
0.41611940
0.41609198
0.41612461
0.41614625
0.41616312
INFO - ==> Top1: 82.806    Top5: 98.588    Loss: 0.512
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [34][   20/   40]   Loss 0.382424   Top1 87.324219   Top5 99.433594   BatchTime 0.126448
features.0.conv.0 tensor(0.2708)
features.0.conv.3 tensor(0.5703)
features.1.conv.0 tensor(0.0462)
features.1.conv.3 tensor(0.0810)
features.1.conv.6 tensor(0.0920)
features.2.conv.0 tensor(0.3108)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.7208)
features.3.conv.0 tensor(0.0764)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1810)
features.4.conv.0 tensor(0.3296)
features.4.conv.3 tensor(0.3218)
features.4.conv.6 tensor(0.6815)
features.5.conv.0 tensor(0.5322)
features.5.conv.3 tensor(0.4300)
features.5.conv.6 tensor(0.7007)
features.6.conv.0 tensor(0.0579)
features.6.conv.3 tensor(0.0469)
features.6.conv.6 tensor(0.0898)
features.7.conv.0 tensor(0.6611)
features.7.conv.3 tensor(0.4534)
features.7.conv.6 tensor(0.7909)
features.8.conv.0 tensor(0.7124)
features.8.conv.3 tensor(0.5292)
features.8.conv.6 tensor(0.8415)
features.9.conv.0 tensor(0.7021)
features.9.conv.3 tensor(0.5605)
features.9.conv.6 tensor(0.8516)
features.10.conv.0 tensor(0.0822)
features.10.conv.3 tensor(0.1042)
features.10.conv.6 tensor(0.2173)
features.11.conv.0 tensor(0.8364)
features.11.conv.3 tensor(0.6505)
features.11.conv.6 tensor(0.9339)
features.12.conv.0 tensor(0.8093)
features.12.conv.3 tensor(0.6811)
features.12.conv.6 tensor(0.9285)
features.13.conv.0 tensor(0.5053)
features.13.conv.3 tensor(0.4888)
features.13.conv.6 tensor(0.7049)
features.14.conv.0 tensor(0.9368)
features.14.conv.3 tensor(0.8407)
features.14.conv.6 tensor(0.9782)
features.15.conv.0 tensor(0.9383)
features.15.conv.3 tensor(0.8753)
features.15.conv.6 tensor(0.9790)
features.16.conv.0 tensor(0.7695)
features.16.conv.3 tensor(0.8110)
features.16.conv.6 tensor(0.9287)
conv.0 tensor(0.4078)
tensor(1627674.) 2188896.0
INFO - Validation [34][   40/   40]   Loss 0.370860   Top1 87.540000   Top5 99.530000   BatchTime 0.089451
INFO - ==> Top1: 87.540    Top5: 99.530    Loss: 0.371
INFO - ==> Sparsity : 0.744
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  35
INFO - Training: 50000 samples (256 per mini-batch)
0.41621375
0.41629049
0.41631067
0.41632938
0.41635704
0.41636375
0.41639638
0.41640159
0.41635838
0.41636688
0.41640261
0.41634890
0.41628742
0.41624051
0.41616753
0.41615894
0.41615325
0.41615754
0.41612276
0.41611820
INFO - Training [35][   20/  196]   Loss 0.503283   Top1 83.222656   Top5 98.085938   BatchTime 0.515035   LR 0.000120
0.41615421
0.41615370
0.41614118
0.41616452
0.41615275
0.41614923
0.41611257
0.41606978
0.41610321
0.41615480
0.41617146
0.41624239
0.41622487
0.41629869
0.41631502
0.41632620
0.41640431
0.41642314
INFO - Training [35][   40/  196]   Loss 0.515536   Top1 82.890625   Top5 98.281250   BatchTime 0.475563   LR 0.000120
0.41642636
0.41645476
0.41649356
0.41649741
0.41648343
0.41650483
0.41652972
0.41657466
0.41659996
0.41659757
0.41661018
0.41660017
0.41653857
0.41649655
0.41645926
0.41645965
0.41641659
0.41642061
0.41640136
0.41636992
INFO - Training [35][   60/  196]   Loss 0.513052   Top1 82.975260   Top5 98.352865   BatchTime 0.481965   LR 0.000120
0.41634476
0.41631275
0.41627622
0.41621110
0.41615349
0.41611895
0.41615003
0.41612339
0.41613042
0.41612211
0.41614121
0.41611767
0.41607502
0.41605386
0.41604671
0.41603577
0.41599768
0.41594955
0.41594815
0.41592437
0.41589358
INFO - Training [35][   80/  196]   Loss 0.510219   Top1 83.037109   Top5 98.442383   BatchTime 0.480020   LR 0.000119
0.41587901
0.41585937
0.41585344
0.41581282
0.41580006
0.41575995
0.41576335
0.41572505
0.41569093
0.41569895
0.41569337
0.41567084
0.41563320
0.41563755
0.41563675
0.41564667
0.41568482
0.41570455
0.41571769
0.41572735
INFO - Training [35][  100/  196]   Loss 0.505298   Top1 83.187500   Top5 98.515625   BatchTime 0.466336   LR 0.000119
0.41577727
0.41583830
0.41586006
0.41588625
0.41588223
0.41588989
0.41592407
0.41597334
0.41595402
0.41598040
0.41602284
0.41603723
0.41604984
0.41607836
0.41613391
0.41615552
0.41620669
0.41628143
0.41630834
INFO - Training [35][  120/  196]   Loss 0.500192   Top1 83.365885   Top5 98.613281   BatchTime 0.460368   LR 0.000119
0.41633907
0.41639638
0.41644493
0.41642788
0.41640684
0.41635528
0.41634586
0.41632611
0.41634864
0.41633353
0.41635627
0.41633457
0.41631770
0.41631392
0.41633707
0.41635254
0.41633376
0.41631106
0.41629231
INFO - Training [35][  140/  196]   Loss 0.500158   Top1 83.353795   Top5 98.641183   BatchTime 0.451638   LR 0.000119
0.41633275
0.41632232
0.41627645
0.41623077
0.41621450
0.41612187
0.41599712
0.41589412
0.41589621
0.41585484
0.41581136
0.41583416
0.41584715
0.41583008
0.41581026
0.41578457
0.41579244
0.41576272
0.41573581
0.41571197
0.41571891
INFO - Training [35][  160/  196]   Loss 0.504967   Top1 83.203125   Top5 98.605957   BatchTime 0.444304   LR 0.000119
0.41570812
0.41570035
0.41564539
0.41562745
0.41559258
0.41559780
0.41562754
0.41560623
0.41556010
0.41556689
0.41559842
0.41559079
0.41563240
0.41567209
0.41567469
0.41571629
0.41569403
0.41567400
0.41564530
0.41563547
0.41563118
0.41569865
INFO - Training [35][  180/  196]   Loss 0.504472   Top1 83.170573   Top5 98.589410   BatchTime 0.446582   LR 0.000118
0.41572112
0.41576633
0.41578990
0.41583019
0.41584587
0.41585070
0.41582158
0.41583288
0.41581428
0.41582507
0.41577557
0.41574547
0.41573107
0.41572082
0.41570044
0.41563204
********************pre-trained*****************
INFO - ==> Top1: 83.160    Top5: 98.600    Loss: 0.504
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [35][   20/   40]   Loss 0.382803   Top1 87.265625   Top5 99.394531   BatchTime 0.127164
features.0.conv.0 tensor(0.2882)
features.0.conv.3 tensor(0.5625)
features.1.conv.0 tensor(0.0495)
features.1.conv.3 tensor(0.0799)
features.1.conv.6 tensor(0.0846)
features.2.conv.0 tensor(0.3032)
features.2.conv.3 tensor(0.3480)
features.2.conv.6 tensor(0.7193)
features.3.conv.0 tensor(0.0790)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1840)
features.4.conv.0 tensor(0.3577)
features.4.conv.3 tensor(0.3200)
features.4.conv.6 tensor(0.6792)
features.5.conv.0 tensor(0.5425)
features.5.conv.3 tensor(0.4311)
features.5.conv.6 tensor(0.7018)
features.6.conv.0 tensor(0.0557)
features.6.conv.3 tensor(0.0492)
features.6.conv.6 tensor(0.0905)
features.7.conv.0 tensor(0.6637)
features.7.conv.3 tensor(0.4543)
features.7.conv.6 tensor(0.7895)
features.8.conv.0 tensor(0.7120)
features.8.conv.3 tensor(0.5284)
features.8.conv.6 tensor(0.8414)
features.9.conv.0 tensor(0.7018)
features.9.conv.3 tensor(0.5584)
features.9.conv.6 tensor(0.8531)
features.10.conv.0 tensor(0.0874)
features.10.conv.3 tensor(0.1062)
features.10.conv.6 tensor(0.2199)
features.11.conv.0 tensor(0.8372)
features.11.conv.3 tensor(0.6508)
features.11.conv.6 tensor(0.9350)
features.12.conv.0 tensor(0.8096)
features.12.conv.3 tensor(0.6806)
features.12.conv.6 tensor(0.9274)
features.13.conv.0 tensor(0.5006)
features.13.conv.3 tensor(0.4892)
features.13.conv.6 tensor(0.7067)
features.14.conv.0 tensor(0.9386)
features.14.conv.3 tensor(0.8411)
features.14.conv.6 tensor(0.9786)
features.15.conv.0 tensor(0.9388)
features.15.conv.3 tensor(0.8727)
features.15.conv.6 tensor(0.9787)
features.16.conv.0 tensor(0.7696)
features.16.conv.3 tensor(0.8111)
features.16.conv.6 tensor(0.9288)
conv.0 tensor(0.4110)
tensor(1629807.) 2188896.0
INFO - Validation [35][   40/   40]   Loss 0.366945   Top1 87.620000   Top5 99.510000   BatchTime 0.090398
INFO - ==> Top1: 87.620    Top5: 99.510    Loss: 0.367
INFO - ==> Sparsity : 0.745
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  36
INFO - Training: 50000 samples (256 per mini-batch)
0.41559041
0.41558665
0.41555473
0.41551900
0.41548213
0.41545540
0.41542232
0.41538036
0.41534603
0.41530025
0.41520303
0.41514167
0.41512248
0.41510943
0.41508847
0.41504204
0.41498497
0.41495457
INFO - Training [36][   20/  196]   Loss 0.519984   Top1 82.656250   Top5 98.222656   BatchTime 0.541421   LR 0.000118
0.41487911
0.41483089
0.41479328
0.41474286
0.41474444
0.41476896
0.41477513
0.41475990
0.41477421
0.41478249
0.41481000
0.41482002
0.41481963
0.41487122
0.41493693
0.41492736
0.41495031
0.41495165
0.41498917
0.41496578
0.41498411
INFO - Training [36][   40/  196]   Loss 0.519919   Top1 82.578125   Top5 98.271484   BatchTime 0.514163   LR 0.000118
0.41494611
0.41486204
0.41481861
0.41478476
0.41474169
0.41470268
0.41466534
0.41461894
0.41459298
0.41460949
0.41461441
0.41459870
0.41456804
0.41455016
0.41452774
0.41450787
0.41447181
0.41442764
0.41439140
0.41436237
0.41435516
INFO - Training [36][   60/  196]   Loss 0.512310   Top1 82.682292   Top5 98.391927   BatchTime 0.501208   LR 0.000117
0.41431025
0.41424191
0.41418812
0.41414565
0.41405371
0.41401380
0.41400921
0.41398662
0.41398719
0.41392052
0.41393191
0.41391832
0.41390216
0.41392028
0.41398022
0.41396591
0.41401410
0.41404125
INFO - Training [36][   80/  196]   Loss 0.516415   Top1 82.709961   Top5 98.437500   BatchTime 0.490725   LR 0.000117
0.41406116
0.41411933
0.41411966
0.41408849
0.41407442
0.41408750
0.41407809
0.41407564
0.41407418
0.41403177
0.41399547
0.41395932
0.41394114
0.41391703
0.41391733
0.41392612
0.41395187
0.41402021
0.41404095
0.41405305
0.41404024
0.41406345
INFO - Training [36][  100/  196]   Loss 0.508653   Top1 82.957031   Top5 98.484375   BatchTime 0.475999   LR 0.000117
0.41407126
0.41412354
0.41416594
0.41420752
0.41422665
0.41425979
0.41427281
0.41426679
0.41423267
0.41423067
0.41423491
0.41423589
0.41422674
0.41420648
0.41418776
0.41418236
0.41421750
0.41422036
0.41416326
INFO - Training [36][  120/  196]   Loss 0.502687   Top1 83.199870   Top5 98.554688   BatchTime 0.469252   LR 0.000117
0.41416544
0.41416970
0.41423079
0.41422403
0.41424444
0.41427195
0.41432017
0.41431826
0.41431788
0.41432455
0.41432762
0.41430274
0.41427922
0.41429281
0.41429308
0.41427714
0.41427571
0.41429940
0.41433942
0.41434291
0.41436869
INFO - Training [36][  140/  196]   Loss 0.500289   Top1 83.264509   Top5 98.616071   BatchTime 0.468278   LR 0.000117
0.41437408
0.41436666
0.41436347
0.41439712
0.41441640
0.41442686
0.41444784
0.41439831
0.41441783
0.41446203
0.41447008
0.41448417
0.41447321
0.41445428
0.41440243
0.41441265
0.41442460
0.41440943
0.41442317
0.41446203
INFO - Training [36][  160/  196]   Loss 0.504701   Top1 83.098145   Top5 98.588867   BatchTime 0.460002   LR 0.000116
0.41447273
0.41451076
0.41451719
0.41458169
0.41461802
0.41467145
0.41467935
0.41467592
0.41465405
0.41463163
0.41464230
0.41466656
0.41468924
0.41471964
0.41473252
0.41475895
0.41477478
INFO - Training [36][  180/  196]   Loss 0.502665   Top1 83.198785   Top5 98.535156   BatchTime 0.459555   LR 0.000116
0.41478255
0.41480255
0.41479492
0.41481024
0.41479480
0.41478762
0.41473797
0.41467503
0.41465929
0.41460034
0.41454476
0.41448811
0.41445702
0.41444263
0.41442522
0.41441828
0.41437152
0.41440099
0.41438988
INFO - ==> Top1: 83.224    Top5: 98.540    Loss: 0.501
INFO - Created `MobileNetv2` model
          Use pre-trained model = False
********************pre-trained*****************
validation quantized model on cpu
INFO - Validation: 10000 samples (256 per mini-batch)
INFO - Validation [36][   20/   40]   Loss 0.364819   Top1 87.753906   Top5 99.628906   BatchTime 0.137631
INFO - Validation [36][   40/   40]   Loss 0.349746   Top1 88.310000   Top5 99.700000   BatchTime 0.095070
INFO - ==> Top1: 88.310    Top5: 99.700    Loss: 0.350
INFO - ==> Sparsity : 0.746
INFO - Scoreboard best 1 ==> Epoch [29][Top1: 88.840   Top5: 99.670]
INFO - Scoreboard best 2 ==> Epoch [25][Top1: 88.740   Top5: 99.700]
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 88.710   Top5: 99.620]
INFO - Saving checkpoint to:
            Current: /home/ilena7440/LSQ_FakeQuant/out/88_20221125-073313/_checkpoint.pth.tar
INFO - >>>>>> Epoch  37
INFO - Training: 50000 samples (256 per mini-batch)
features.0.conv.0 tensor(0.2847)
features.0.conv.3 tensor(0.5762)
features.1.conv.0 tensor(0.0488)
features.1.conv.3 tensor(0.0880)
features.1.conv.6 tensor(0.0851)
features.2.conv.0 tensor(0.3302)
features.2.conv.3 tensor(0.3465)
features.2.conv.6 tensor(0.7179)
features.3.conv.0 tensor(0.0810)
features.3.conv.3 tensor(0.0764)
features.3.conv.6 tensor(0.1877)
features.4.conv.0 tensor(0.3530)
features.4.conv.3 tensor(0.3252)
features.4.conv.6 tensor(0.6789)
features.5.conv.0 tensor(0.5436)
features.5.conv.3 tensor(0.4265)
features.5.conv.6 tensor(0.7028)
features.6.conv.0 tensor(0.0578)
features.6.conv.3 tensor(0.0475)
features.6.conv.6 tensor(0.0881)
features.7.conv.0 tensor(0.6497)
features.7.conv.3 tensor(0.4540)
features.7.conv.6 tensor(0.7915)
features.8.conv.0 tensor(0.7129)
features.8.conv.3 tensor(0.5295)
features.8.conv.6 tensor(0.8431)
features.9.conv.0 tensor(0.7031)
features.9.conv.3 tensor(0.5579)
features.9.conv.6 tensor(0.8535)
features.10.conv.0 tensor(0.0871)
features.10.conv.3 tensor(0.1030)
features.10.conv.6 tensor(0.2169)
features.11.conv.0 tensor(0.8341)
features.11.conv.3 tensor(0.6507)
features.11.conv.6 tensor(0.9342)
features.12.conv.0 tensor(0.8148)
features.12.conv.3 tensor(0.6796)
features.12.conv.6 tensor(0.9287)
features.13.conv.0 tensor(0.4927)
features.13.conv.3 tensor(0.4888)
features.13.conv.6 tensor(0.7081)
features.14.conv.0 tensor(0.9390)
features.14.conv.3 tensor(0.8413)
features.14.conv.6 tensor(0.9797)
features.15.conv.0 tensor(0.9394)
features.15.conv.3 tensor(0.8735)
features.15.conv.6 tensor(0.9796)
features.16.conv.0 tensor(0.7699)
features.16.conv.3 tensor(0.8109)
features.16.conv.6 tensor(0.9292)
conv.0 tensor(0.4163)
tensor(1632203.) 2188896.0
0.41439298
0.41441700
0.41444543
0.41448477
0.41447440
0.41445091
0.41443023
0.41443804
0.41445357
0.41445866
0.41442865
0.41441879
0.41441146
0.41438568
0.41437390
0.41434893
0.41432223
0.41425762
0.41423061
0.41416338
INFO - Training [37][   20/  196]   Loss 0.508345   Top1 83.320312   Top5 98.398438   BatchTime 0.577664   LR 0.000116
0.41413376
0.41411871
0.41414177
0.41428116
0.41429794
0.41423950
0.41416067
0.41416442
0.41414416
0.41412181
0.41409117
0.41405377
0.41408050
0.41404119
0.41395810
0.41388294
0.41382185
INFO - Training [37][   40/  196]   Loss 0.514214   Top1 82.812500   Top5 98.281250   BatchTime 0.526333   LR 0.000115
0.41381773
0.41379395
0.41377392
0.41375786
0.41371801
0.41370118
0.41366130
0.41360924
0.41358769
0.41354707
0.41349474
0.41348091
0.41346714
0.41342270
0.41338325
0.41330078
0.41322461
0.41317514
0.41310912
Traceback (most recent call last):
  File "main_slsq.py", line 91, in <module>
    optimizer, lr_scheduler, args.epochs, monitors, args, init_qparams = False, hard_pruning = True)
  File "main_slsq.py", line 77, in main
    logger.info(('Optimizer: %s' % optimizer).replace('\n', '\n' + ' ' * 11))
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 53, in train_qat_slsq
    t_top1, t_top5, t_loss = train_one_epoch_slsq(train_loader, qat_model,
  File "/home/ilena7440/LSQ_FakeQuant/trainer/process.py", line 154, in train_one_epoch_slsq
    outputs = qat_model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 140, in forward
    x = self.features(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ_FakeQuant/model/mobilenet_cifar10.py", line 93, in forward
    return self.skip_add.add(x, self.conv(x))
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1208, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 584, in forward
    return F.relu(ConvBn2d._forward(self, input))
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 101, in _forward
    return self._forward_approximate(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py", line 121, in _forward_approximate
    conv = self._conv_forward(input, scaled_weight, zero_bias)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt